Auffusion: Leveraging the Power of Diffusion and Large Language Models for
Text-to-Audio Generation
Jinlong Xue, Yayue Deng, Yingming Gao, Ya Li*
Beijing University of Posts and Telecommunications
{jinlong xue, yayue.deng, yingming.gao, yli01 }@bupt.edu.cn
Abstract
Recent advancements in diffusion models and large lan-
guage models (LLMs) have significantly propelled the field
of AIGC. Text-to-Audio (TTA), a burgeoning AIGC appli-
cation designed to generate audio from natural language
prompts, is attracting increasing attention. However, ex-
isting TTA studies often struggle with generation quality
and text-audio alignment, especially for complex textual
inputs. Drawing inspiration from state-of-the-art Text-to-
Image (T2I) diffusion models, we introduce Auffusion, a
TTA system adapting T2I model frameworks to TTA task,
by effectively leveraging their inherent generative strengths
and precise cross-modal alignment. Our objective and sub-
jective evaluations demonstrate that Auffusion surpasses
previous TTA approaches using limited data and computa-
tional resource. Furthermore, previous studies in T2I rec-
ognizes the significant impact of encoder choice on cross-
modal alignment, like fine-grained details and object bind-
ings, while similar evaluation is lacking in prior TTA works.
Through comprehensive ablation studies and innovative
cross-attention map visualizations, we provide insightful as-
sessments of text-audio alignment in TTA. Our findings re-
veal Auffusion’s superior capability in generating audios
that accurately match textual descriptions, which further
demonstrated in several related tasks, such as audio style
transfer, inpainting and other manipulations. Project page
is available at https://auffusion.github.io.
1. Introduction
Text-to-audio (TTA) generation is an emerging application
that focuses on synthesizing diverse audio outputs based
on text prompts. With the integration of artificial intelli-
gence into the realm of AIGC, the scope of TTA appli-
cations has expanded significantly, covering areas such as
movie dubbing and musical composition. Early TTA mod-
els, as referenced in [22, 29], primarily relied on one-hot
labels, leading to the generation of monotonous audio con-
*Corresponding author.strained by limited label space and generative capacity. In
contrast, natural descriptive text delivers more comprehen-
sive and fine-grained information. Thereby, the following
works [9, 16, 19, 27, 28, 55] develop their models based on
textual content.
Recent advancements in diffusion generative models
[15, 48] and large language models [6, 30, 39, 40, 47]
have showcased remarkable capabilities in content gener-
ation and understanding. Leveraging these advancements,
the first diffusion based TTA Diffsound [55] outperforms
previous TTA systems by generating discrete tokens quan-
tized from mel-spectrograms using a diffusion model. Later
Diffsound is surpassed by AudioGen [24] using an autore-
gressive model in a discrete space of waveform. Inspired
by [43], AudioLDM [27] is the first to utilize a contin-
uous latent diffusion model (LDM) [43], achieving better
quality and computational efficiency compared to other dis-
crete token based TTA systems [24, 55]. Similarly, many
well-performed TTA systems, including AudioLDM2 [28],
Tango [9], Make-an-Audio 1/2 [16, 19], integrate LDM into
their TTA frameworks to facilitate denoising processes in
the latent space. However, these models still require ex-
tensive computational resources and large-scale datasets for
training from scratch. Moreover, these models only em-
phasis coarse-grained performance and neglect fine-grained
text-audio alignment. Our work concentrates on addressing
these two critical challenges, providing effective solutions
and valuable insights.
In developing a powerful TTA model, two primary ob-
jectives are paramount: 1) mastering the distribution of
natural audio, and 2) achieving precise text-audio align-
ment. These competencies are also paralleled in T2I [2, 45]
tasks, where robust generative abilities and accurate text-
image alignment are similarly crucial. To this end, we in-
troduce our TTA system Auffusion that adapts Latent Dif-
fusion Model (LDM) originally pretrained for T2I tasks.
This adaptation enables Auffusion to leverage the LDM’s
inherent generative strength and transfer it alignment under-
standing effectively for text-audio alignment in TTA appli-
cation. The comprehensive subjective and objective evalua-
1arXiv:2401.01044v1  [cs.SD]  2 Jan 2024tion metrics show our proposed system Auffusion achieves
better quality and alignment. Moreover, Auffusion demon-
strates performance comparable to other baseline models
trained on datasets 60 times larger. Riffusion [7], an app
for 5-second music generation, also uses a pretrained LDM
[43]. However, it is specifically designed for music and does
not extend to broader TTA tasks. Additionally, Riffusion
simply quantizes audio signals into images, a non-reversible
process that leads to a great precision loss. In contrast, our
Auffusion model features a carefully designed feature space
transformation pipeline, enabling lossless audio conversion.
On the other hand, the text encoder serves as a critical
bridge between text and audio, representing a key compo-
nent in TTA systems. Different from the extensive stud-
ies [1, 17, 45] conducted in the T2I domain, where the im-
pact of text encoders on aspects such as fine-grained de-
tails and object bindings has been widely explored, their
influence in TTA has not been thoroughly explored. Gen-
erally, text encoders in existing TTA systems fall into two
categories: 1) multi-modal contrastive learning-based mod-
els, such as CLIP [39] and CLAP [6], and 2) text-only
large language models (LLMs) like BERT [4] and T5 [40].
However findings from previous studies can sometimes be
contradictory. Diffsound [55] employs CLIP, pretrained
on text-image pairs, claiming superior performance over
text-only model BERT. Conversely, AudioLDM [27] uses
CLAP model, pretrained on text-audio pairs, suggesting ad-
vantages using audio only features over using combined
audio-text features or text-only features. Building upon
the same LDM used in AudioLDM, Tango [9] owes dif-
ferent opinions. They advocate for instruction-tuned LLMs
(Flan-T5) [47] to better grasp textual descriptions and cross-
modal correlations, challenging the notion of embedding
audio and text in a shared space. However, comprehensive
ablation studies using these text encoders are lacking.
To address the debates outlined above, our study con-
ducts a thorough investigation into the performance of var-
ious text encoders and baseline models. Moving beyond
traditional evaluation metrics, we innovatively assess text-
audio alignment by visualizing the cross-attention map to
provide intuitive observation for the first time in TTA task.
We find that our model achieves better fine-grained text-
audio alignment. Additionally, we demonstrate versatile
audio manipulations enabled by our model’s generative ca-
pacity and clear text-audio alignment.
In summary, the contributions of our work are: 1) We
propose Auffusion, a TTA model that integrates a pow-
erful pretrained LDM from T2I in order to inherit gen-
erative strengths and enhance cross-modal alignment, and
our method demonstrates superior performance compared
to existing TTA systems; 2) We conduct an extensive in-
vestigation into the performance between different text en-
coders, and we provide a novel and insightful demonstrationin TTA task to assess text-audio alignment utilizing visual-
izations of cross-attention maps across different models.
2. Related Work
2.1. Text-to-Image Synthesis
Text-to-Image (T2I) synthesis, particularly through diffu-
sion models, has seen significant advancements in recent
years. Pioneering models like DALL-E [41] treats T2I as
a sequence-to-sequence translation task, encoding images
into discrete latent tokens using pretrained VQ-V AE [52].
DALL-E 2 [42] employs the CLIP text encoder and two
diffusion models, first predicting CLIP [39] visual features
from text and then synthesizing the image. Another famous
model Imagen [45] uses the T5 encoder [40] for text feature
extraction and a cascade of diffusion models for initial im-
age synthesis and subsequent super-resolution. Stable Dif-
fusion [43] optimizes computational efficiency by mapping
images from pixel to compressed latent space using a con-
tinuous V AE trained with discriminators, followed by dif-
fusion in this latent space. These models [36, 42, 43, 45]
demonstrate remarkable diversity and quality in image gen-
eration, guided by text prompts and operating either directly
in image space or within a latent space.
2.2. Text-to-Audio Synthesis
Text-to-Audio (TTA) synthesis has witnessed significant
advancements. Diffsound [55] leverages VQ-V AE [52]
model trained on mel-spectrograms and convert them into
discrete codes, where a non-autoregressive token based dif-
fusion model is then used to generate audio signals. Sim-
ilarly, AudioGen [24] uses a VQ-V AE based approach [3]
but focuses on encoding raw waveform data and employs
an autoregressive model for generation. Other studies
include the use of Latent Diffusion Models (LDMs), as
seen in AudioLDM 1/2 [27, 28], Make-An-Audio 1/2 [16,
19], and Tango [9]. AudioLDM [27], utilizes audio fea-
tures extracted by a pretrained contrastive text-audio model
CLAP [6] as a condition during training, while leveraging
text features during inference. This approach benefits from
CLAP’s ability to map audio and captions to a shared latent
space. AudioLDM2 [28] first employs an auto-regressive
model (AR) to generate AudioMAE [18] features from text,
then uses them to condition the LDM. These two methods
both alleviate the reliance of audio-text pair data. Other
methods [9, 16, 19], on the other hand, employ text features
in both training and inference stages. Make-An-Audio [19]’
LDM is similar to AudioLDM. Make-An-Audio2 [16] em-
phasize the temporal information by changing 2D spatial
structure to 1D temporal structure, and they additionally
replace U-Net design to transformer. However, neither of
these two models is open source.
2Figure 1. An overview of Auffusion architecture. The whole training and inference process include back-and-forth transformation between
four feature spaces: audio, spectrogram, pixel and latent space. Note that U-Net is initialized with pretrained text-to-image LDM.
3. Auffusion
3.1. Overview
Our proposed method Auffusion, as depicted in Fig. 1 has
four main components: 1) text encoder; 2) latent diffusion
model (LDM); 3) pixel V AE; 4) HiFi-GAN vocoder. In or-
der to achieve the TTA task and utilize the powerful pre-
trained model from T2I task, the whole process involves
conversion between four feature spaces: audio, spectro-
gram, pixel and latent space. The spectrogram feature is a
key proxy that bridges the audio space and pixel space. Dur-
ing training, audio is first converted into mel-spectrogram
and normalize for image space, then LDM is conditioned on
textual embeddings extracted by textual condition encoder
and trained in the pixel space learned by V AE. In inference,
this process is reversed: starting from standard Gaussian
noise, the latent representation is generated through reverse
diffusion process conditioned on text embeddings. There-
after the pixel V AE decoder reconstructs the pixel space and
generated image is denormalized into mel-spectrogram. Fi-
nally, the pretrained HiFi-GAN vocoder synthesizes the au-
dio from this mel-spectrogram.
3.2. Feature Space Transformation
Given an audio-text pair, we first convert the audio signal
xaudio∈RTinto mel-spectrogram xmel∈Rd×l, where d
andlrepresent the mel-channels and the number of frames
respectively. In order to transform mel-spectrogram into
image-like data without precision loss, we conduct nor-
malization by utilizing the mean µand variance δcalcu-
lated from the whole dataset rather than on individual mel-
spectrogram [7]. The normalized spectrogram xnorm canbe viewed as gray-scale image and then converted into RGB
image data x∈Rc×d×l, where c,dandlare referred to as
the image channel, height and width respectively.
3.3. Latent Diffusion Model
To guide the construction of the audio signal’s pixel dis-
tribution z0using a text prompt τ, we fine-tune the U-Net
diffusion module by minimizing mean squared error (MSE)
in the noise space. The objective function is defined as:
ℓθ=||ϵθ(zt, t, τ)−ϵ||2
2 (1)
Here, ϵ∼ N (0, I)represents Gaussian noise, tis a ran-
dom time step, and ϵθis the text-guided denoising network,
comprising a U-Net with a cross-attention component for
text guidance τ.
In this process, the V AE encoder processes the image-
like input xinto compressed latent vector z0. The diffusion
process then operates in this latent space, gradually trans-
forming z0into Gaussian noise zT. The model is trained
to reverse this transformation, recovering the original data
distribution from the noise. This process involves two key
steps: the forward process that converts z0intozTand the
reverse process that recovers z0from zT.
Forward process is defined by a fixed Markov chain
from data z0to the latent variable zT.
q(z1, . . . , z T|z0) :=TY
t=1q(zt|zt−1) (2)
The entire procedure transforms the initial latent data z0
into noise latent variables zTin accordance with a prede-
termined noise schedule β1, . . . , β T.
q(zt|zt−1) :=N(zt;p
1−βtzt−1, βtI) (3)
3where βtis a small positive constant. q(zt|zt−1)represents
a function where a small Gaussian noise is added to the dis-
tribution of zt−1.
Reverse process converts the latent variables from zTto
z0with learnable parameters θ, aimed at recovering samples
from Gaussian noise zT∼ N(0, I).
pθ(z0, . . . , z T−1|zT) :=TY
t=1pθ(zt−1|zt) (4)
pθ(zt−1|zt) :=N(zt−1;µθ(zt, t),Σθ(zt, t)) (5)
Note that µθtakes the diffusion step t∈Nand variable
ztas inputs and outputs zt−1for each iteration.
3.4. Conditioning Processes
The previous work AudioLDM [27] adopts concatenation
operation between pooled text embedding extracted from
CLAP and time embedding to guide the generation pro-
cess in LDM. By contrast, we turn diffusion model gener-
ation into more flexible and understandable by conducting
a cross-attention mechanism [53] between conditional em-
bedding sequence and latent vectors in the U-Net backbone.
More formally, we donates ϑi(zt)∈RN×di
ϵa interme-
diate representation of the i-thlayer of U-Net estimation
ϵθ. Then, a linear projection is applied to the deep spatial
features of the noisy data ϑi(zt).
Q=W(i)
q·ϑi(zt) (6)
The conditional embedding τis also projected via learned
linear projections.
K=W(i)
k·τ, V =W(i)
v·τ, (7)
where W(i)
q∈Rd×dτ,W(i)
v∈Rd×di
ϵandW(i)
k∈Rd×dτ
are learnable matrices. The attention value and attention
score are calculated as follows:
Attention (Q, K, V ) =score (Q, K)·V
score (Q, K) =softmax (QKT
√
d)(8)
The condition approach allows us to visualize the 2D at-
tention map [12, 35, 50] by reshaping attention score back
to latent image shape. Furthermore, it provides an intuitive
measurement to access the understanding ability of various
text encoders. We discuss the compared results in Sec. 4.3.
Meanwhile, based on the visualized attention map, we find
that the pretrained LDM is capable of adequately transfer-
ring cross-modal understanding ability from T2I to TTA
task, resulting in better alignment. Overall, we highlight
the importance of the conditioning process in enhancing the
audio-text model’s ability to extract key information from
text descriptions and accurately match the desired audio, as
demonstrated in Fig. 2.3.5. Text Encoder
Inspired by eDiff-I [1] who uses an ensemble of encoders
to provide multi-source information to LDM, we combine
CLAP and FlanT5 text encoders as conditions. We use ran-
dom dropout on each of these embeddings independently
during training. When all two embeddings are dropped, it
corresponds to unconditional training, which is useful for
performing classifier-free guidance [14]. We conduct com-
prehensive comparison for various text encoders and results
are shown in Sec 4.3.
3.6. Classifier-Free Guidance
To guide the reverse diffusion process, we utilize classifier-
free guidance [14] based on the text input τusing:
ˆϵθ(zt, t, τ) = (1 + w)·ϵ(zt, t, τ)−w·ϵ(zt, t)(9)
At the inference stage, the guidance scale wdetermines how
much the text input influences the noise estimation ˆϵθcom-
pared to the unconditional estimation. We randomly discard
the text condition at a rate of 10% during training.
4. Experiments
4.1. Experimental Setup
Dataset. We follow previous works [9, 16, 27] and use a va-
riety of different audio datasets with audio caption or audio
labels to train our model, including AudioCaps (AC) [20],
WavCaps [33], MACS [32], Clotho [5], ESC50 [38], Urban-
Sound [46], Music Instruments dataset1and GTZAN [51].
The WavCaps dataset consists of ChatGPT-assisted weakly-
labeled audio captions for the FreeSound2, BBC Sound Ef-
fects (SFX)3, SoundBible4and the AudioSet strongly la-
beled subset [11], containing 403,050 audio clips with an
average duration of 68 seconds. AudioCaps is a subset of
AudioSet (AS) [8] with handcrafted captions and it contains
about 46K ten-second audio clips. This results in a dataset
composed of 0.47 million audio text pairs, with a total du-
ration of approximately 7.7K hours.
It is noted that the duration of the audio samples in
AudioSet and AudioCaps is 10 seconds, while it is much
longer in FreeSound and BBC SFX datasets (86s and 115s
in average). To avoid the imbalance caused by longer au-
dio, which often contains repeated sounds like background
sounds, we only use the first thirty seconds of audios for
all datasets and randomly select ten-second segments dur-
ing training. Finally, we have in total 0.4M audio samples
with a total duration of around 2K hours for model training.
1https://www.kaggle.com/datasets/soumendraprasad/musical-
instruments-sound-dataset
2https://freesound.org/
3https://sound-effects.bbcrewind.co.uk/
4https://soundbible.com/
4Table 1. The comparison between our model Auffusion and baseline TTA models. Although our model Auffusion is only trained on a
much smaller dataset AC, our model outperforms other baselines on AC test set and has comparable zero-shot result in Clotho test set.
Model Pretrain Duration(h) ParamsAudioCaps Clotho
FD↓ FAD↓KL↓ IS↑ CLAP↑ FD↓ FAD↓KL↓ IS↑ CLAP↑
Riffusion " 1990 1.1B 26.28 4.68 1.57 7.21 47.6% 31.63 6.11 2.66 6.50 47.5%
AudioGen-v2-medium % 6824 1.5B 17.86 1.73 1.59 9.31 48.5% 23.26 2.55 2.56 7.19 46.7%
AudioLDM-S-full-v2 % 9031 421M 30.58 4.40 1.79 6.96 42.1% 26.51 3.54 2.62 6.58 49.9%
AudioLDM-L-full % 9031 975M 29.77 4.04 1.78 7.50 42.9% 24.13 3.02 2.56 7.49 51.0%
AudioLDM2 % 29510 1.1B 26.05 1.94 1.76 7.31 46.0% 23.53 3.06 2.47 9.05 48.1%
AudioLDM2-large % 29510 1.5B 25.59 2.19 1.70 7.83 47.9% 23.31 3.00 2.41 8.88 49.0%
Tango % 145 1.3B 24.82 1.77 1.43 7.20 55.0% 31.67 3.22 2.57 7.18 46.6%
Tango-Full % 3400 1.3B 30.68 3.67 1.63 4.79 51.9% 25.83 3.17 2.35 6.51 50.3%
Auffusion % 145 1.1B 24.45 2.25 1.39 10.14 54.7% 29.01 2.67 2.66 9.46 47.6%
Auffusion " 145 1.1B 21.99 1.63 1.36 10.57 55.3% 25.64 2.35 2.59 9.01 48.2%
Auffusion-Full % 1990 1.1B 24.11 1.67 1.46 8.39 51.6% 19.14 1.99 2.42 10.33 52.8%
Auffusion-Full " 1990 1.1B 23.08 1.76 1.36 10.28 55.6% 17.97 1.96 2.38 11.29 55.0%
Training Setup. We utilize the pretrained Stable Dif-
fusion v1.55, including its V AE and U-Net, and later fine-
tune the U-Net on audio datasets. All datasets are re-
sampled to 16kHz sampling rate and mono format, with
samples padded to 10.24 seconds. We then extract mel-
spectrograms from audios using parameters of 256 mel fil-
ter bands, 1024 window length, 2048 FFT, and 160 hop
size, resulting in (1,256,1024) mel-spectrograms, akin to
grayscale images with 256 height and 1024 width in 1 chan-
nels. These are normalized and channel-repeated to cre-
ate RGB-like images suitable for V AE encoder input. For
high-fidelity audio conversion, previous methods adopt neu-
ral vocoder [21, 25]. In order to match our need using our
specific mel-spectrogram parameters, we train a new HiFi-
GAN vocoder [21] using the same datasets described above.
This training employs the AdamW optimizer [31] with a 2e-
4 learning rate and 16 batch size on one A6000 GPU. Fi-
nally, we freeze the text encoder and finetune the pretrained
Stable Diffusion’s U-Net using AdamW optimizer with a
3e-5 learning rate, at a 20 batch size for 100K steps. Our
model can be trained only taking a total of 48 hours on one
A6000 GPU.
Objective Evaluation. In our experimental evalua-
tion, we follow previous evaluation methods [9, 16, 27]
and employ a suite of objective metrics to assess the
quality and fidelity of generated audio samples, includ-
ing Frechet Distance (FD), Frechet Audio Distance (FAD),
Kullback–Leibler (KL) divergence, Inception Score (IS)
and CLAP score. Analogous to the Frechet Inception Dis-
tance (FID) [13] used in image synthesis, the FD score in
audio domain quantifies the global similarity between cre-
ated audio samples and the target samples without the need
of using paired reference audio samples. The IS score is ef-
5https://huggingface.co/runwayml/stable-diffusion-v1-5fective at assessing both the quality and variety of samples.
The KL score is calculated using paired samples and it mea-
sures the divergence between two probability distributions.
These three metrics are all grounded in the advanced audio
classifier PANNs [23]. FAD score has a similar idea to FD
but it uses VGGish [10] as feature extractor. The evaluate
suite that we uses for FD, FAD, KL and IS is in project6. Be-
sides, we also use pretrained CLAP7model to compute the
similarity of the text caption and generated audio to evaluate
the text-audio alignment, similar to CLIP score.
Subjective Evaluation. Following previous evaluation
method [9, 24] in TTA field, we ask five human evaluators
to assess two aspects of the generated audio, including over-
all audio quality (OVL) and relevance to the text caption
(REL). We randomly select 30 audio samples from each of
the AC and Clotho test sets and ask participants to rate them
on a scale from 1 to 100 with 10-point intervals. Results are
shown in Table 3.
Baseline Models. To comprehensively compare our
models with others, our study employs five baseline mod-
els, including three diffusion based models Riffusion [7],
AudioLDM [27], AudioLDM2 [28], Tango [9], and one
auto-regressive generative model based on discrete audio
token AudioGen [24]. We re-implement Riffusion8, orig-
inally trained on music datasets for only 5s audio, to gen-
erate 10s audio using a 160 hop length and trained it on
our datasets. We use the other baseline models released
by authors on huggingface respectively. AudioLDM-S-
full-v29and AudioLDM-L-full10have 412M and 975M pa-
rameters and trained them on AudioCaps, AudioSet and
6https://github.com/haoheliu/audioldm eval
7https://huggingface.co/laion/clap-htsat-unfused
8https://huggingface.co/riffusion/riffusion-model-v1
9https://huggingface.co/cvssp/audioldm-s-full-v2
10https://huggingface.co/cvssp/audioldm-l-full
5other 2 datasets including 9031h audio data for more than
1.5M train steps. AudioLDM211and AudioLDM2-large12
have 1.1B and 1.5B parameters respectively and trained on
29510h diverse audio data. Tango is trained on Audio-
Caps dataset and Tango-Full is trained on datasets similar
with our datasets settings but using different preprocessing.
AudioGen-v2-medium13has 1.5B parameters and is trained
on AudioCaps, AudioSet, and eight other datasets around
4K hours data.
4.2. Results
Evaluation Setup. We compare our model Auffusion
trained on single dataset AudioCaps (AC) and Auffusion-
Full trained on whole datasets with other baselines in both
AudioCaps test set and Clotho test set. We also conduct ab-
lation studies on the impact of pretrained SD models. Both
Auffusion and Auffusion-Full use CLIP as default text en-
coder. We report our result in Table 1.
Objective Evaluation. When trained solely on AC
dataset, our model Auffusion-with-pretrained outperforms
the previous state-of-the-art Tango in AC test set, with 21.99
FD, 1.63 FAD, 1.36 KL, 10.57 IS and 55.3% in CLAP
score, and achieve comparable zero-shot Clotho test set re-
sults to other baseline models trained on datasets over 60
times larger. Notably, Tango and Auffusion-no-pretrained
both trained solely on AC dataset exhibit a huge drop on
Clotho test set, indicating a problem of overfitting. In con-
trast, our Auffusion-with-pretrained still maintain its perfor-
mance, demonstrating generalization ability. This suggests
that the generative capacity and cross-modal alignment of
the pretrained SD model can be effectively transferred to
mel-spectrogram domain, even with a small dataset.
When trained in much larger datasets using same training
steps, Auffusion-Full-with-pretrained achieves the state-
of-the-art performance in Clotho test set. Besides, it
shows a negligible decrease in AC test set compared to
Auffusion-with-pretrained, and records a slight increase in
CLAP score. This indicates the robustness and strong
generalization ability of the pretrained SD, even when
dealing with different data distributions. Additionally,
both our Auffusion-with-pretrained and Auffusion-Full-
with-pretrained models significantly outperform other base-
lines in terms of IS and CLAP scores. A higher IS score
implies that our model can generate mel-spectrograms with
both high fidelity and diversity. A higher CLAP score indi-
cates our model’s enhanced capability to adhere to textual
descriptions and produce more relevant audio. We also find
that our re-implement Riffusion yields much inferior re-
sults, indicating that the precision loss caused by the quan-
tization transform has a great impact.
11https://huggingface.co/cvssp/audioldm2
12https://huggingface.co/cvssp/audioldm2-large
13https://huggingface.co/facebook/audiogen-mediumSubjective Evaluation. Our subjective human evalua-
tion results are presented in Table 3. In the “All Event”
column, our model Auffusion-w-clip demonstrates superior
performance over other baseline models, achieving an OVL
score of 69.36 and a REL score of 70.25. Additionally, the
REL has significant gains compared to other models, show-
ing strong text-audio alignment. We delve deeper into the
impact of numbers of events and provide intuitive visualiza-
tion for text-audio alignment in Sec. 4.3.
4.3. Analysis
Effect of Text Encoder. To assess the performance of dif-
ferent text encoders and explore the effectiveness of a dual
text encoder approach in TTA applications, we compared
several encoder options including CLIP, CLAP, FlanT5-
base, FlanT5-large, and a combined CLAP and FlanT5-
large encoder. The original SDv1.5 uses the CLIP L/14
model, trained on text-image pairs, while AudioLDM em-
ploys the CLAP model, trained on text-audio pairs. Tango
suggests that FlanT5, an instruction-tuned LLM, enhances
textual understanding, but lacks an encoder ablation study.
Inspired by eDiff-I [1], which uses an ensemble of encoders
to provide multi-source information, we experimented with
a combined CLAP and FlanT5-large encoder by concatena-
tion. The results are shown in Table 2.
Our findings reveal that FlanT5-large surpasses FlanT5-
base in all evaluation metrics, underscoring the importance
of the text encoder’s size for understanding textual cap-
tions. FlanT5-large shows results comparable to CLIP, with
CLIP excelling in IS score and FlanT5-large in FAD score.
This mirrors Imagen [45] findings, where T5-XXL matches
CLIP in objective scores but exceeds smaller T5 models.
Notably, the CLAP model outperforms both text encoders,
especially in FAD and CLAP scores, demonstrating its ad-
vanced audio domain expertise. The elevated CLAP score
may be attributed to the use of same model during train-
ing. Combining CLAP and FlanT5-large encoders lever-
ages both acoustic and rich semantic knowledge, yielding
the best overall objective performance.
Text-Audio Alignment. In our investigation into the im-
pact of varying text encoders on TTA alignment—a crucial
aspect of TTA tasks—we are the first to examine the cross-
attention mechanisms between text and LDM outputs using
method [12]. This approach allows us to intuitively observe
the focal points of the LDM during the TTA process. How-
ever, due to the global conditioning approach employed by
AudioLDM and the use of GPT for generating AudioMAE
features in AudioLDM2, these models do not provide a
direct correlation between text and LDM. Therefore we
present a comparative visualization of cross-attention maps
using various encoders within the Auffusion framework,
and the Tango models who also adopt cross attention, as
depicted in Figure 2. For consistency, we standardize the
6Figure 2. The visualization of cross attention maps for Auffusion with different text encoders and Tango model. Auffusion-no-pretrain use
fixed CLIP encoder and LDM is trained from scratch. The LDMs in 2 to 4 rows are initialized with SDv1.5 with different encoders. The
last row shows the Tango’s cross attention map, and Tango uses FlanT5-large as condition encoder.
Table 2. The results evaluated on AudioCaps test set and Clotho test set with different settings of conditional encoder.
AudioCaps Clotho
Auffusion-Full-with FD↓ FAD↓KL↓ IS↑ CLAP↑ FD↓ FAD↓KL↓ IS↑ CLAP↑
CLIP 23.08 1.76 1.36 10.28 55.6% 17.97 1.96 2.38 11.29 55.0%
CLAP 21.92 1.57 1.35 10.01 58.3% 17.79 1.82 2.32 11.02 59.1%
FlanT5-base 23.00 1.55 1.50 10.11 53.0% 20.05 2.03 2.50 10.88 52.9%
FlanT5-large 22.31 1.41 1.42 9.37 54.6% 18.09 1.62 2.35 10.16 55.6%
Clap+FlanT5-large 22.55 1.50 1.32 10.34 57.4% 17.59 1.87 2.25 10.93 59.5%
diffusion steps to 50 and adjust all cross-attention maps to a
uniform square dimension for clear comparison.
Upon comparing the attention maps of the first and sec-
ond lines, it is evident that using pre-trained LDM exhibits
superior distinguishability, with clear attention across al-
most all tokens. Notably, the “gunshot” token within the
highlighted red area is prominent, and the “dogs” and “gun-
shot” sounds in the generated audio overlap in the latter
section. This suggests that the pre-trained LDM possesses
advanced prior knowledge, enabling it to effectively trans-
fer its text-image alignment capabilities to text-audio align-
ment tasks. Although the CLAP model achieves higher
objective scores, it surprisingly produces similar attention
maps for each token. Furthermore, the “dogs” and “gun-
shot” sounds are indistinguishable, occurring simultane-
ously in the generated audio, which indicates that the CLAP
model struggles to differentiate and isolate fine-grained
events or entities. We believe the reason is that CLAP model
can only gather global acoustic information and have is-sue capturing temporal and local information in the text.
Furthermore, all the objective evaluation also only focus
and compute global features, therefore such inconsistency
exists. Recent study [54] corroborates that current CLAP
models do not truly comprehend natural language, focusing
instead on global information.
In contrast, when comparing the attention maps of the
last two lines, where Tango also employs FlanT5-large as
a text encoder, the Tango’s attention maps appear muddled,
particularly for the “dog” token highlighted in blue area,
which results in the omission of the “dog” sound in the gen-
erated audio. Additionally, we find that Tango often pro-
duces extraneous sounds, such as unintended “bird” noises,
that are not present in the text captions. These findings high-
light that Auffusion, by leveraging the robust text-image
alignment capabilities of pre-trained LDMs, can generate
audio that more accurately reflects the given captions.
Performance against Number of Events. To better as-
sess fine-grained text-audio alignment, we evaluate perfor-
7Table 3. Subjective evaluation for all baseline models and different encoders used in Auffusion categorized by the number of events in the
text. OVL measures the overall quality and REL shows the relevance. ACC represents the mean accuracy of audio events matching the text
in multi-event conditions, indicating the fine-grained alignment between text and audio.
ModelAll Event Single Event Two Events Multi Events
OVL↑ REL↑ REL↑CLAP↑REL↑CLAP↑REL↑CLAP↑ACC↑
Groundtruth 71.56 74.01 73.70 50.9% 75.50 51.5% 72.85 48.7% 84.6%
AudioGen-v2-medium 63.86 59.80 59.75 47.2% 58.50 45.9% 61.15 48.3% 68.5%
AudioLDM-L-full 60.33 57.36 58.00 51.1% 60.40 50.8% 53.70 51.1% 53.2%
AudioLDM2-large 65.53 59.23 61.50 50.6% 60.20 48.3% 56.00 46.9% 61.4%
Tango-Full 67.78 65.05 63.75 48.2% 67.75 52.3% 62.65 53.6% 69.6%
Auffusion-w-clip 69.36 70.25 70.65 52.5% 73.45 53.2% 66.65 55.3% 73.9%
Auffusion-w-clap 69.76 67.76 68.95 55.6% 71.25 57.5% 63.10 58.9% 71.0%
Auffusion-w-flant5 70.13 70.65 69.55 51.1% 74.60 53.0% 67.80 54.3% 73.3%
Auffusion-w-clap-flant5 69.80 71.86 72.10 55.3% 73.90 56.8% 69.60 58.9% 74.1%
mance across varying event numbers in the AudioCaps and
Clotho test set. For instance, a sequence like A man talking
followed by plastic clacking then a power tool drilling com-
prises three distinct events. We categorize the test sets into
three groups: single event, two events, and multiple events
(three or more), randomly selecting 20 captions from each
category for generation. Human raters are asked to rate
the relevance between text and audio for each group, using
REL metric. Besides, we select an additional 80 samples
for multiple events. We ask raters to count the number of
events in the audios that accurately appear in the text, and
calculate the mean accuracy denoted as ACC to reflect the
fine-grained text-audio alignment. These results for base-
lines and Auffusion with various encoders trained on whole
datasets are presented in Table 3. Additionally, we used the
objective CLAP score for comparison.
We assume that human raters can directly and faithfully
represent true performance. We find that the CLAP score
can not accurately reflect detailed alignment ability, par-
ticularly in multi-event evaluation, compared with human
evaluation. Our analysis concludes that CLAP primarily
extracts global features, lacking in fine-grained evaluation
capacity. Additionally, we observe that AudioLDM, Au-
dioLDM2, and AudioGen exhibit inferior performance in
REL and we can tell from ACC score that they fail to gen-
erate matching audio in multi-event scenarios. This is at-
tributed to AudioLDM using globally pooled CLAP em-
beddings for conditioning, while AudioLDM2 first employs
an auto-regressive model (AR) to generate AudioMAE fea-
tures from text, then uses these to condition the LDM. Con-
sequently, fine-grained information is lost in AudioLDM,
and the AR model in AudioLDM2 introduces error accu-
mulation. In contrast, Tango and our Auffusion, which
adopt cross-attention between text embedding sequences
and LDM, demonstrate better alignment. Moreover, our
findings across various encoders align with the attention
map results illustrated in previous part. Despite Auffusion’scombination with the CLAP encoder yielding a higher
CLAP score, evaluations using ACC and REL especially
in multi-event scenarios reveal that the CLAP encoder cap-
tures less fine-grained information compared to CLIP and
FlanT5 encoders.
Applications. Leveraging our system’s exceptional text
comprehension capabilities and robust text-audio align-
ment, we demonstrate its versatile applications inspired by
T2I tasks [34, 44, 49, 56]. These include audio style trans-
fer, audio inpainting, and attention-based techniques such as
word swap and text attention re-weighting. We demonstrate
these capabilities in Appendix E. Our method offers a sig-
nificantly more controllable and fine-grained manipulation
compared to previous methods[9, 24, 27, 28].
5. Conclusion
In this study, we introduce Auffusion, a text-to-audio (TTA)
generation model that harnesses the robust generative capa-
bilities and precise cross-modal alignment abilities of pre-
trained text-to-image (T2I) models. Our extensive objec-
tive and subjective evaluations demonstrate that Auffusion
surpasses other state-of-the-art models, achieving superior
performance with limited data and computational resources.
Recognizing the significant impact of different encoders
on cross-modal alignment in T2I, we pioneer in the TTA
field by conducting comprehensive investigations and inno-
vatively adopting cross-attention map visualization. This
approach offers an intuitive evaluation of text-audio align-
ment. Our findings demonstrate that Auffusion exhibits an
exceptional ability to generate audio that accurately aligns
with text descriptions, surpassing existing methods, which
further evidenced in several audio manipulations, includ-
ing audio style transfer, inpainting, word swapping, and re-
weighting. In the future, we aim to delve into a broader
spectrum of innovative audio applications, based on the ro-
bust text-audio alignment capabilities of our system.
8References
[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,
Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,
Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu
Liu. ediff-i: Text-to-image diffusion models with an ensem-
ble of expert denoisers. CoRR , abs/2211.01324, 2022. 2, 4,
6
[2] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze
Xie, Yue Wu, Zhongdao Wang, James T. Kwok, Ping Luo,
Huchuan Lu, and Zhenguo Li. Pixart- α: Fast training of dif-
fusion transformer for photorealistic text-to-image synthesis.
CoRR , abs/2310.00426, 2023. 1
[3] Alexandre D ´efossez, Jade Copet, Gabriel Synnaeve, and
Yossi Adi. High fidelity neural audio compression. CoRR ,
abs/2210.13438, 2022. 2
[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: pre-training of deep bidirectional trans-
formers for language understanding. In NAACL-HLT (1) ,
pages 4171–4186. Association for Computational Linguis-
tics, 2019. 2
[5] Konstantinos Drossos, Samuel Lipping, and Tuomas Virta-
nen. Clotho: an audio captioning dataset. In 2020 IEEE
International Conference on Acoustics, Speech and Signal
Processing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020 ,
pages 736–740. IEEE, 2020. 4, 1
[6] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail,
and Huaming Wang. CLAP: learning audio concepts from
natural language supervision. CoRR , abs/2206.04769, 2022.
1, 2
[7] Seth* Forsgren and Hayk* Martiros. Riffusion - Stable dif-
fusion for real-time music generation. 2022. 2, 3, 5
[8] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren
Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal,
and Marvin Ritter. Audio set: An ontology and human-
labeled dataset for audio events. In 2017 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Process-
ing, ICASSP 2017, New Orleans, LA, USA, March 5-9, 2017 ,
pages 776–780. IEEE, 2017. 4
[9] Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish,
and Soujanya Poria. Text-to-audio generation using
instruction-tuned LLM and latent diffusion model. CoRR ,
abs/2304.13731, 2023. 1, 2, 4, 5, 8
[10] Shawn Hershey, Sourish Chaudhuri, Daniel P. W. Ellis,
Jort F. Gemmeke, Aren Jansen, R. Channing Moore, Manoj
Plakal, Devin Platt, Rif A. Saurous, Bryan Seybold, Mal-
colm Slaney, Ron J. Weiss, and Kevin W. Wilson. CNN ar-
chitectures for large-scale audio classification. In 2017 IEEE
International Conference on Acoustics, Speech and Signal
Processing, ICASSP 2017, New Orleans, LA, USA, March
5-9, 2017 , pages 131–135. IEEE, 2017. 5
[11] Shawn Hershey, Daniel P. W. Ellis, Eduardo Fonseca, Aren
Jansen, Caroline Liu, R. Channing Moore, and Manoj Plakal.
The benefit of temporally-strong labels in audio event clas-
sification. In IEEE International Conference on Acoustics,
Speech and Signal Processing, ICASSP 2021, Toronto, ON,
Canada, June 6-11, 2021 , pages 366–370. IEEE, 2021. 4, 1[12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-
age editing with cross-attention control. In ICLR . OpenRe-
view.net, 2023. 4, 6
[13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. In Advances in Neural Information Processing Systems
30: Annual Conference on Neural Information Processing
Systems 2017, December 4-9, 2017, Long Beach, CA, USA ,
pages 6626–6637, 2017. 5
[14] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. CoRR , abs/2207.12598, 2022. 4
[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In NeurIPS , 2020. 1
[16] Jiawei Huang, Yi Ren, Rongjie Huang, Dongchao Yang,
Zhenhui Ye, Chen Zhang, Jinglin Liu, Xiang Yin, Zejun Ma,
and Zhou Zhao. Make-an-audio 2: Temporal-enhanced text-
to-audio generation. CoRR , abs/2305.18474, 2023. 1, 2, 4,
5
[17] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xi-
hui Liu. T2i-compbench: A comprehensive benchmark for
open-world compositional text-to-image generation. CoRR ,
abs/2307.06350, 2023. 2
[18] Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski,
Michael Auli, Wojciech Galuba, Florian Metze, and
Christoph Feichtenhofer. Masked autoencoders that listen.
InNeurIPS , 2022. 2
[19] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren,
Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin,
and Zhou Zhao. Make-an-audio: Text-to-audio generation
with prompt-enhanced diffusion models. In ICML , pages
13916–13932. PMLR, 2023. 1, 2
[20] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and
Gunhee Kim. Audiocaps: Generating captions for audios
in the wild. In Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, NAACL-
HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume
1 (Long and Short Papers) , pages 119–132. Association for
Computational Linguistics, 2019. 4, 1
[21] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan:
Generative adversarial networks for efficient and high fi-
delity speech synthesis. In Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Infor-
mation Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual , 2020. 5, 1
[22] Qiuqiang Kong, Yong Xu, Turab Iqbal, Yin Cao, Wenwu
Wang, and Mark D. Plumbley. Acoustic scene generation
with conditional samplernn. In ICASSP , pages 925–929.
IEEE, 2019. 1
[23] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,
Wenwu Wang, and Mark D. Plumbley. Panns: Large-scale
pretrained audio neural networks for audio pattern recogni-
tion. IEEE ACM Trans. Audio Speech Lang. Process. , 28:
2880–2894, 2020. 5
[24] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer,
Alexandre D ´efossez, Jade Copet, Devi Parikh, Yaniv Taig-
9man, and Yossi Adi. Audiogen: Textually guided audio gen-
eration. In ICLR . OpenReview.net, 2023. 1, 2, 5, 8
[25] Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro,
and Sungroh Yoon. Bigvgan: A universal neural vocoder
with large-scale training. In The Eleventh International Con-
ference on Learning Representations, ICLR 2023, Kigali,
Rwanda, May 1-5, 2023 . OpenReview.net, 2023. 5
[26] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich,
Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu,
Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut,
and Daniel Haziza. xformers: A modular and hackable
transformer modelling library. https://github.com/
facebookresearch/xformers , 2022. 1
[27] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu,
Danilo P. Mandic, Wenwu Wang, and Mark D. Plumbley.
Audioldm: Text-to-audio generation with latent diffusion
models. In ICML , pages 21450–21474. PMLR, 2023. 1,
2, 4, 5, 8
[28] Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qi-
uqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang,
and Mark D. Plumbley. Audioldm 2: Learning holistic
audio generation with self-supervised pretraining. CoRR ,
abs/2308.05734, 2023. 1, 2, 5, 8
[29] Xubo Liu, Turab Iqbal, Jinzheng Zhao, Qiushi Huang,
Mark D. Plumbley, and Wenwu Wang. Conditional sound
generation using neural discrete time-frequency representa-
tion learning. In MLSP , pages 1–6. IEEE, 2021. 1
[30] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar
Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-
moyer, and Veselin Stoyanov. Roberta: A robustly optimized
BERT pretraining approach. CoRR , abs/1907.11692, 2019.
1
[31] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. In 7th International Conference on Learning
Representations, ICLR 2019, New Orleans, LA, USA, May
6-9, 2019 . OpenReview.net, 2019. 5
[32] Irene Mart ´ın-Morat ´o and Annamaria Mesaros. What is the
ground truth? reliability of multi-annotator data for audio
tagging. In 29th European Signal Processing Conference,
EUSIPCO 2021, Dublin, Ireland, August 23-27, 2021 , pages
76–80. IEEE, 2021. 4, 1
[33] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong,
Tom Ko, Chengqi Zhao, Mark D. Plumbley, Yuexian Zou,
and Wenwu Wang. Wavcaps: A chatgpt-assisted weakly-
labelled audio captioning dataset for audio-language multi-
modal research. CoRR , abs/2303.17395, 2023. 4
[34] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun
Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided im-
age synthesis and editing with stochastic differential equa-
tions. In The Tenth International Conference on Learn-
ing Representations, ICLR 2022, Virtual Event, April 25-29,
2022 . OpenReview.net, 2022. 8, 4
[35] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and
Daniel Cohen-Or. Null-text inversion for editing real images
using guided diffusion models. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition, CVPR 2023, Van-
couver, BC, Canada, June 17-24, 2023 , pages 6038–6047.
IEEE, 2023. 4[36] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,
Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya
Sutskever, and Mark Chen. GLIDE: towards photorealis-
tic image generation and editing with text-guided diffusion
models. In International Conference on Machine Learning,
ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA ,
pages 16784–16804. PMLR, 2022. 2
[37] Nathana ¨el Perraudin, P ´eter Bal ´azs, and Peter L.
Søndergaard. A fast griffin-lim algorithm. In IEEE
Workshop on Applications of Signal Processing to Audio
and Acoustics, WASPAA 2013, New Paltz, NY, USA, October
20-23, 2013 , pages 1–4. IEEE, 2013. 2
[38] Karol J. Piczak. ESC: dataset for environmental sound clas-
sification. In Proceedings of the 23rd Annual ACM Con-
ference on Multimedia Conference, MM ’15, Brisbane, Aus-
tralia, October 26 - 30, 2015 , pages 1015–1018. ACM, 2015.
4, 1
[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In ICML , pages
8748–8763. PMLR, 2021. 1, 2
[40] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J. Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. J. Mach. Learn. Res. , 21:
140:1–140:67, 2020. 1, 2
[41] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. In ICML , pages 8821–
8831. PMLR, 2021. 2
[42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with CLIP latents. CoRR , abs/2204.06125, 2022. 2
[43] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR , pages 10674–
10685. IEEE, 2022. 1, 2
[44] Chitwan Saharia, William Chan, Huiwen Chang, Chris A.
Lee, Jonathan Ho, Tim Salimans, David J. Fleet, and Mo-
hammad Norouzi. Palette: Image-to-image diffusion mod-
els. In SIGGRAPH ’22: Special Interest Group on Computer
Graphics and Interactive Techniques Conference, Vancouver,
BC, Canada, August 7 - 11, 2022 , pages 15:1–15:10. ACM,
2022. 8
[45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed
Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan,
Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad
Norouzi. Photorealistic text-to-image diffusion models with
deep language understanding. In NeurIPS , 2022. 1, 2, 6
[46] Justin Salamon, Christopher Jacoby, and Juan Pablo Bello. A
dataset and taxonomy for urban sound research. In Proceed-
ings of the ACM International Conference on Multimedia,
MM ’14, Orlando, FL, USA, November 03 - 07, 2014 , pages
1041–1044. ACM, 2014. 4, 1
10[47] Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Long-
pre, Jason Wei, Hyung Won Chung, Barret Zoph, William
Fedus, Xinyun Chen, Tu Vu, Yuexin Wu, Wuyang Chen, Al-
bert Webson, Yunxuan Li, Vincent Zhao, Hongkun Yu, Kurt
Keutzer, Trevor Darrell, and Denny Zhou. Flan-moe: Scal-
ing instruction-finetuned language models with sparse mix-
ture of experts. CoRR , abs/2305.14705, 2023. 1, 2
[48] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Mah-
eswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In ICML ,
pages 2256–2265. JMLR.org, 2015. 1
[49] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,
Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,
Naejin Kong, Harshith Goka, Kiwoong Park, and Victor
Lempitsky. Resolution-robust large mask inpainting with
fourier convolutions. In IEEE/CVF Winter Conference on
Applications of Computer Vision, WACV 2022, Waikoloa, HI,
USA, January 3-8, 2022 , pages 3172–3182. IEEE, 2022. 8
[50] Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang,
Gefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin,
and Ferhan Ture. What the DAAM: interpreting stable dif-
fusion using cross attention. In Proceedings of the 61st An-
nual Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), ACL 2023, Toronto, Canada,
July 9-14, 2023 , pages 5644–5659. Association for Compu-
tational Linguistics, 2023. 4
[51] George Tzanetakis and Perry R. Cook. Musical genre clas-
sification of audio signals. IEEE Trans. Speech Audio Pro-
cess. , 10(5):293–302, 2002. 4, 1
[52] A ¨aron van den Oord, Oriol Vinyals, and Koray
Kavukcuoglu. Neural discrete representation learning.
InNIPS , pages 6306–6315, 2017. 2
[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NIPS , pages 5998–
6008, 2017. 4
[54] Ho-Hsiang Wu, Oriol Nieto, Juan Pablo Bello, and Justin
Salamon. Audio-text models do not yet leverage natural
language. In IEEE International Conference on Acoustics,
Speech and Signal Processing ICASSP 2023, Rhodes Island,
Greece, June 4-10, 2023 , pages 1–5. IEEE, 2023. 7
[55] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao
Weng, Yuexian Zou, and Dong Yu. Diffsound: Discrete dif-
fusion model for text-to-sound generation. IEEE ACM Trans.
Audio Speech Lang. Process. , 31:1720–1733, 2023. 1, 2
[56] Lvmin Zhang and Maneesh Agrawala. Adding condi-
tional control to text-to-image diffusion models. CoRR ,
abs/2302.05543, 2023. 8
11Auffusion: Leveraging the Power of Diffusion and Large Language Models for
Text-to-Audio Generation
Supplementary Material
A. Dataset Details
Table 4. Statistics for the all datasets used in the this paper.
Dataset Samples Hours (Original) Hours (Processed) Source
AudioCaps 41K 110h 110h [20]
MACS 3930 11h 11h [32]
Clotho 5929 37h 37h [5]
ESC50 2000 3h 3h [38]
UrbanSound8K 8266 9h 9h [46]
Music Instruments 4587 11h 10h [9]
GTZAN 2997 8h 8h [51]
BBC Sound Effects 31K 997h 232h https://sound-effects.bbcrewind.co.uk/
FreeSound 206K 6246h 1283h https://freesound.org/
AudioSet SL 108K 296h 296h [11]
SoundBible 612 4h 3h https://soundbible.com/
Total 402K 7732h 1990h -
As indicated in Table 4, we collect a large-scale audio-text dataset comprising approximately 0.47M audio samples,
amounting to a total duration of about 7.7K hours. This dataset encompasses a diverse range of sounds, including musical
instruments, sound effects, human voices, and sounds from nature and everyday life. We only utilize the first 30 seconds
of each audio sample for long duration audio and exclude any samples shorter than 1 second. Consequently, our model is
trained on approximately 0.4M audio samples, collectively amounting to about 2K hours.
B. Experiment Details
B.1. Vocoder
In this work, we adopt HiFi-GAN vocoder [21] as a converter from V AE decoder output to finally generated audio. It is widely
used for speech waveform and audio sample reconstructed from mel-spectrogram. However, the default and pretrained HiFi-
GAN14use 80 mel filter bands, 256 hop size and trained on 22050 sample rate speech, resulting in (1,80,882) mel-spectogram
for 10-second audio, which does not suit V AE input requirements. Therefore we train our own HiFi-GAN vocoder using 256
mel filter bands, 1024 window length, 2048 FFT, and 160 hop size, resulting in (1,256,1024) mel-spectrograms. Then we
repeat by channel and convert grayscale image to RGB image to match V AE encoder input. The (3,256,1024) image is then
8x downsampled by V AE as (4,32,128) latent features sent to LDM. We train this vocoder using AdaW optimizer with 2e-4
learning rate and 16 batch size on one A6000 GPU. We release this pretrained vocoder in our open-source implementation.
B.2. Configuration
We utilize the pretrained Stable Diffusion v1.515, including its V AE and 860M-parameter U-Net. we freeze the text encoder
and finetune U-Net using AdamW optimizer with a 3e-5 learning rate and a constant scheduler, at a 20 batch size for 100K
steps in both Auffusion and Auffusion-Full setups. For GPU memory efficiency, we use xformer [26] and mixed precision,
and our model can be trained only taking a total of 48 hours on one A6000 GPU. In comparison, AudioGen [24] utilizes 64
A100 GPU with a batch size of 256. AudioLDM [27] use one A100 GPU for 1.5M train steps and AudioLDM2 [28] use 8
A100 GPU for the same steps. Tango use 4 A6000 GPU for 200K steps.
14https://github.com/jik876/hifi-gan
15https://huggingface.co/runwayml/stable-diffusion-v1-5
1B.3. Riffusion Re-implementation
Riffusion [7] is originally trained on music datasets for only 5-second sound clips using a 44100 sample rate, a 441 hop size,
and 512 mel filter bands. We changed the sample rate to 16000 and the hop size to 160 to match audio generation setup. After
converting audio to a mel-spectrogram, Riffusion simply applies min-max normalization to each individual mel-spectrogram
and quantizes it into an image. Therefore, it is a non-reversible process that leads to information loss. To convert back to
audio, Riffusion uses the Griffin-Lim [37] algorithm, which is not sensitive to initial data range and iteratively estimates the
missing phase information. However, the quality is not comparable to that of deep-learning-based vocoders. We use the same
training setup to re-implement Riffusion with same whole datasets.
C. Effect of Guidance Scale and Inference Steps
The number of inference steps and the classifier-free guidance scale are of crucial importance for sampling from latent dif-
fusion models. We report the effect of varying these parameters on audio generation using AudioCaps test set in Table 5. On
the left, with a fixed guidance scale of 7.5, we explore inference steps ranging from 10 to 200. Unlike previous studies [9, 27]
using 200 steps, we find that our Auffusion model have competent performance even with fewer inference steps, suggesting
strong generative capabilities. Therefore we choose using 100 inference steps for Auffusion model. On the right, we fix the
steps at 100 and adjust the guidance scale. We find that guidance 5 has the best FD and FAD score, and guidance 10 excels
in IS and CLAP score, therefore we choose balanced guidance scale 7.5 for Auffusion model to generate audio.
Table 5. Effect on the objective evaluation metrics with a varying number of inference steps and classifier-free guidance scale.
Varying Steps Varying Guidance
Guidance Steps FD ↓ FAD↓KL↓ IS↑ CLAP↑Steps Guidance FD ↓ FAD↓KL↓ IS↑ CLAP↑
7.510 21.15 2.48 1.54 7.89 48.9%
1001 32.32 5.17 2.47 4.56 32.2%
25 22.52 2.03 1.36 9.88 55.3% 2 23.44 2.86 1.63 6.4 47.1%
50 23.35 2.01 1.35 10.09 55.5% 5 21.24 1.72 1.37 9.29 54.9%
100 23.44 1.96 1.36 10.21 55.6% 7.5 23.44 1.96 1.36 10.21 55.6%
200 23.34 1.92 1.36 10.20 55.6% 10 24.69 2.31 1.36 10.50 55.9%
D. Evaluation
Despite many objective evaluation method exits, they can only assess global performance. Subjective method is a much more
direct approach and can be tailored to specific needs. We design three scoring tasks to evaluate performance. We first ask
human raters to evaluate the overall qaulity (OVL) from 0 to 100 with 10-point intervals. Then they need to rate the relevance
(REL) to the text caption, evaluate the global text-audio alignment. Finally, we ask testers to count the number of events that
appear in the audio to assess the fine-grained text-audio alignment. Our designed subjective evaluation is shown in Figure 3.
Figure 3. Screenshot of subjective evaluation.
2E. Demos
E.1. Text-to-Audio Generation
Figure 4. Demo of audio generation with the Auffusion-Full model.
3E.2. Audio Style Transfer Examples
We show the audio style transfer ability of our Auffusion. We adopt the similar image-to-image manipulation method first
introduced in T2I task by using shallow reverse process [34] to audio domain. In the figures below, we show the original
audio samples on the left, and six transferred audios guided by textual descriptions using different starting points of the
reverse process n. Asnincreases, more noise will be added to the original audio. Diffusion model will pay more attention
on text guidance and the generated audio will become less similar to the original one. When n= 1, the added noise is at its
maximum, and information from the original audio will not be retained. We gradually increase nand set n= 0.7for the last
generated sample. For instance, the original sound of a baby crying gradually transitions into the sound of a cat meowing
in Figure 5.
Figure 5. Audio style transfer gradually from baby crying tocat meowing .
Figure 6. Audio style transfer gradually from cat screaming tocar racing .
Figure 7. Audio style transfer gradually from bird chirping toambulance siren .
4E.3. Audio Inpainting Examples
In the figure below, we demonstrate the audio inpainting capability of our Auffusion model. Each audio sample is extended to
a duration of 10 seconds. In the first row of each example, we present the groundtruth samples. For the unprocessed samples,
we mask a segment from 2.5 to 7.5 seconds in the original audio and use these masked samples as input for inpainting. The
inpainting results are produced using the same text prompt as the original groundtruth sample. We observe that our model
can comprehend the textual description and audio context, thereby generating appropriate content for the masked segment.
Figure 8. The examples of audio inpainting ability of Auffusion.
5E.4. Word Swap Examples
Showing the replacement ability of attention map in TTA task. In below cases, we swap tokens in the original prompt with
others. By changing huge tosmall , we observe that the sound effect in the corresponding part changes to a less echoic and
clearer sound. By replacing gunshots tospeech , the corresponding sound is replaced with a human voice.
Figure 9. Demo of word swapping manipulation.
E.5. Attention Re-weighting Examples
Showing the re-weighting ability of attention map in TTA task. By increasing the cross attention of specific words (marked
with an arrow), we control the effect only on specific words without significant change the image. We find that increasing
the weight on the verb chopping enhances the frequency of the action sound, while amplifying the adjective huge affects the
sound effect.
Figure 10. Demo of attention re-weighting manipulation.
6VeCLIP: Improving CLIP Training via Visual-enriched Captions
Zhengfeng Lai1†, Haotian Zhang2†, Bowen Zhang2, Wentao Wu2, Haoping Bai2, Aleksei Timofeev2,
Xianzhi Du2, Zhe Gan2, Jiulong Shan2, Chen-Nee Chuah1, Yinfei Yang2‡, Meng Cao2
1University of California, Davis2Apple AI/ML
{lzhengfeng, chuah }@ucdavis.edu
{haotian zhang2, yinfeiy, mengcao }@apple.com
Abstract
Large-scale web-crawled datasets are fundamental for
the success of pre-training vision-language models, such as
CLIP . However, the inherent noise and potential irrelevance
of web-crawled AltTexts pose challenges in achieving pre-
cise image-text alignment. Existing methods utilizing large
language models (LLMs) for caption rewriting have shown
promise on small, curated datasets like CC3M and CC12M.
This study introduces a scalable pipeline for noisy caption
rewriting. Unlike recent LLM rewriting techniques, we em-
phasize the incorporation of visual concepts into captions,
termed as Visual- enriched Captions (VeCap). To ensure data
diversity, we propose a novel mixed training scheme that op-
timizes the utilization of AltTexts alongside newly generated
VeCap. We showcase the adaptation of this method for train-
ing CLIP on large-scale web-crawled datasets, termed Ve-
CLIP . Employing this cost-effective pipeline, we effortlessly
scale our dataset up to 300 million samples named VeCap
dataset. Our results show significant advantages in image-
text alignment and overall model performance. For example,
VeCLIP achieves up to +25.2% gain in COCO and Flickr30k
retrieval tasks under the 12M setting. For data efficiency,
VeCLIP achieves +3% gain while only using 14% of the data
employed in the vanilla CLIP and 11% in ALIGN. We also
note the VeCap data is complementary with other well cu-
rated datasets good for zero-shot classification tasks. When
combining VeCap and DFN, our model can achieve strong
performance on both of image-text retrieval and zero-shot
classification tasks, e.g.83.1% accuracy@1 on ImageNet
zero-shot for a H/14 model. We release the pre-trained mod-
els at https://github.com/apple/ml-veclip .
Work done during an internship at Apple.†Equal contribution.
‡Corresponding author.1. Introduction
Large-scale vision-language representation learning, exem-
plified by CLIP [ 32], has gained wide attention due to the
transferability of knowledge learned from image-text pairs
to diverse downstream tasks such as zero-shot image classi-
fication and image-text retrieval [ 17,19,20]. CLIP training
is straightforward via the image-text contrastive loss, but
involves a large-scale dataset of 400 million image-text pairs
crawled from the Web. Consequently, CLIP embeddings
lead to consistent improvement across various downstream
tasks compared to other vision pre-training methods such as
SimCLR [ 6] and MAE [ 15]. CLIP achieves success via two
scalable paradigms: data and computational resources. First,
the massive web-crawled data [ 35,36] enable the training
to be scalable and meet the requirements of data-hungry
backbones ( e.g., ViT [ 11]). Second, the simple image-text
contrastive loss grants favorable scaling properties to the
computational resources.
Despite the availability of large-scale web-crawled data,
their quality can be low or noisy. For example, AltTexts
suffer from two major issues: 1) they can be noisy, uninfor-
mative, or irrelevant to the images; 2) they may not describe
all visual contents in the image. For example, as shown
in Figure 1, in the first image, we observe a house with a
white roof and a porch. However, the corresponding caption
only describes the address, which proves overly abstract for
effective vision-language alignment in training. Our obser-
vations demonstrate that caption quality plays a pivotal role
in CLIP’s performance, as detailed in Table 6b and the Ap-
pendix ( e.g., CC3M vs. our web-crawled 3M). It is worth
noting that the captions in CC3M are derived from human an-
notations, which may require heavy resources when further
scaling up. This motivates the main open research question
addressed in this work: Can we devise a scalable and cost-
effective pipeline to improve captions within these noisy
datasets at scale (e.g., up to million or billion level)?
One natural direction is to deploy Large Language Mod-
els (LLMs) to rephrase captions [ 12]. However, the major
1arXiv:2310.07699v3  [cs.CV]  13 Mar 2024Figure 1. Noisy web-crawled data and the limitation of LLM rewrite. AltTexts can be noisy and uninformative; it may not describe all visual objects
present in the image. Simple LLM rewrite [ 12] on such raw and noisy captions cannot introduce new image-relevant information. After applying our proposed
VeCap, new captions are enriched with more image-specific concepts. We keep all image-text pairs for pre-training rather than filtering out those with noisy
AltTexts, as images of rich visual objects still contribute effectively to the training process.
limitation of such methods lies in the inability of LLMs to
generate and introduce new image-specific details. LLMs
can only modify sentence syntax in this scenario. For exam-
ple, we follow a recent work [ 12] and use LLM to rewrite the
raw captions from the Web: as shown in Fig. 1, LLM rewrite
cannot introduce any new information and thus the new cap-
tion remains similar to AltText. In other words, if the origi-
nal AltTexts are noisy, the benefits brought by LLM rewrite
might yield only trivial improvements. In essence, the re-
liance on high-quality captions within pre-training datasets
limits the effectiveness of simple LLM rewrite. However,
sourcing such high-quality datasets like manually curated
CC3M and CC12M [ 5] remains challenging, and further
scaling up to larger datasets becomes both time-consuming
and labor-intensive to meet the prerequisites for CLIP pre-
training. Therefore, in this work, we focus on building a
scalable and cost-effective pipeline tailored to raw and noisy
web-crawled data to improve CLIP.
In addition to data quality, the diversity of data signifi-
cantly impacts VLM pre-training [ 2,27]. Methods relying
on LLM-based rewriting may diminish data variety, given
that LLMs tend to apply a uniform style in their sentence
rephrasing. Moreover, existing works mainly focus on image
augmentations, while texts are disregarded and unaltered dur-
ing training without augmentation [ 12]. This may also incur
overfitting issues as the text encoders repeatedly encounter
the same texts in each epoch. Since these techniques have
exclusively undergone assessment on meticulously curated
datasets like CC3M and CC12M [ 5], their suitability for
extensive, uncensored web-crawled data remains uncertain.Consequently, there is a pressing need to build a scalable
approach to enhance data quality, diversity, and training
methodologies to improve pre-training for VLMs on both
model performance and data efficiency.
Concurrently, alongside the evolution of CLIP, there has
been substantial progress in the development of instruction
fine-tuned LLMs. These models and their multimodal ex-
tensions have demonstrated outstanding performance, sur-
passing human capabilities in various natural language and
vision tasks. Inspired by these models, we investigate the
potential of utilizing them to improve the noisy captions
gathered from the Internet. Specifically, we initially employ
LLaV A [ 24], a Language-Vision Assistant, to leverage visual
concepts extracted from the images. Given that AltTexts may
lack informativeness, our objective is to integrate the newly
derived visual concepts into the caption. However, it is worth
noting that LLaV A [ 24] fine-tuned its language decoder on
its own generated dataset, potentially losing its ability to ac-
commodate comprehensive instructions. Consequently, we
further propose to utilize an LLM to refine the sentence by
fusing the generated caption from LLaV A and the original
AltText. This process aims to maximize image-specific infor-
mation for optimal vision-language alignment. We denote
the caption generated from LLM as LLM Visual- enriched
Captions (VeCapap), or VeCap for short. For data variety,
we propose VeCLIP and introduce a mixed training scheme,
alternating between VeCap and the original AltText. This
strategy ensures that the model captures all pertinent informa-
tion without oversight. We generalize this scalable pipeline
to curate five pre-training datasets ranging from small-scale
2to large-scale up to 300M. Overall, our contributions are
summarized below:
•We present a visual-enriched re-captioning technique for
CLIP training. This marks the initial endeavor to leverage
visual concepts extracted from images and inject them into
the captioning process.
•Our pipeline is cost-effective and capable of processing
data at a scale exceeding 300M, named VeCap. Then, we
propose VeCLIP with a mixed training scheme that uses
VeCap to improve CLIP training on model performance.
•VeCLIP can achieve up to 25.2% improvement over CLIP
in retrival tasks. For training data efficiency, e.g., we use
only 5%data in training but achieve competitive results
in image-text retrieval tasks.
•VeCap data is also complementary with other well curated
datasets. A CLIP-H/14 model trained on the combination
of VeCap and DFN achieves strong performance on both of
image-text retrieval and zero-shot classification tasks, with
an impressive 83.1% zero-shot accuracy@1 on ImageNet.
2. Related Work
Contrastive language-image pre-training. CLIP [ 32] has
shown its effectiveness in acquiring transferable image repre-
sentations via text supervision after large-scale pre-training.
Similar models such as ALIGN [ 17], Florence [ 44], BA-
SIC [ 30] and OpenCLIP [ 8] have shown impressive zero-
shot image classification and image-text retrieval capabili-
ties. SLIP [ 26] and DeCLIP [ 21] incorporate self-supervised
training techniques to improve performance. CoCa [ 43]
introduces an additional decoder alongside the contrastive
loss. LiT [ 47] proposes to keep a pre-trained image encoder
frozen and fine-tune text encoders to improve the zero-shot
transferability. Nevertheless, the majority of these subse-
quent studies incorporate supplementary training inputs and
losses, potentially exerting adverse effects on both training
efficiency and memory usage.
Improving image-text datasets. Given the importance
of the pre-training data, many works focus on improving
the datasets, such as filtering less informative image-text
pairs [ 1,4,12,25]. However, these methods may disre-
gard a large amount of data even though some images have
rich visual concepts. An alternative approach is to rewrite
the caption to enhance the alignment between texts and im-
ages. For example, LaCLIP [ 12] employs LLMs to perform
rewriting. Nevertheless, their evaluation was conducted on
small-scale and meticulously curated datasets like CC3M
and CC12M [ 5], where the initial captions were already of
high quality. As shown in Fig. 1, the advantage of employing
LLM rewrite on noisy web-crawled data is marginal if the
AltText is noisy.3. Methodology
3.1. Preliminary
CLIP. The Contrastive Language-Image Pre-training (CLIP)
method has shown its effectiveness in training vision models
via language supervision. Specifically, a batch of Nimage-
text pairs {xI, xT}is sampled from the massive training data
during each training iteration. We apply data augmentations
to the images before inputting them into the vision encoder.
We denote fIandfTas the normalized features extracted
by the vision and text encoders, respectively. We use the
contrastive loss to train the model, where the paired images
and texts are treated as positive pairs and the remaining as
negative samples. The training loss iterating over images
can be formulated as follows:
LI=−NX
i=1logexp 
sim(fI(aug(xi
I)), fT(xi
T))/τ
PN
k=1exp 
sim(fI(aug(xi
I)), fT(xk
T))/τ,
(1)
where (xi
I, xi
T)is the ithimage-text pair in the batch, and
aug(·)refers to image augmentations. sim(·,·)is the simi-
larity measurement function. We set τas a learnable tem-
perature parameter that scales the logits in experiments. The
loss iterating over texts is symmetrical and denoted as LT.
Finally, the training loss is L= (LI+LT)/2.
3.2. Recaptioning with Visual Concept Exploitation
Web-crawled captions (AltTexts) can be noisy and uninfor-
mative to the images. LaCLIP [ 12] used LLM to rewrite
the caption. As shown in Fig. 1, this may not be applicable
if the captions are noisy as LLM can only reconstruct the
sentence but cannot introduce new information without any
information provided by the image. Given the inherent noise
in AltTexts, we advocate for the utilization of pre-trained
multimodal models to generate augmented captions with
richer visual concepts derived from the images. In this sub-
section, we use LLaV A [ 24] as one example and present a
scalable and cost-effective pipeline for scaling up.
LLaV A and image captioning for Visual-enriched Cap-
tions (VeCap). As a multimodal model, LLaV A connects
the open-set visual encoder of CLIP [ 32] with an LLM, such
as LLaMA [ 38], then fine-tune them on a visual instruction-
tuning dataset. LLaV A shows its effectiveness in leveraging
the capabilities of pre-trained LLM and vision foundation
models. Given an input image xI, we get fIfrom CLIP’s
vision encoder. Then, LLaV A applies a trainable projection
matrixWto convert fIinto language embedding tokens
to achieve the image-language alignment. To mitigate the
influence of AltText, we have devised AltText-independent
prompts tailored for LLaV A, ensuring the full exploitation
of visual concepts. We refrain from incorporating AltText
information into LLaV A, while acknowledging the potential
loss of pre-trained knowledge during fine-tuning of the LLM
3Figure 2. An overview of the scalable VeCap recaptioning piepline. First, we focus on exploiting visual concepts in images via leveraging a multimodal
LLM (LLaV A) to describe the image with a designed prompt independent of AltText to generate Visual-enriched Captions (VeC). Second, we leverage an
LLM to do ethical check and fuse the concepts from both AltText and VeC to generate the final caption, denoted as VeCap.
component on the generated dataset. This trade-off, however,
may limit its capacity to comprehend more intricate instruc-
tions. Thus, we adopt a straightforward yet potent prompt,
“Describe the image concisely, less than 20 words” , allowing
LLaV A to generate visual concepts directly from the image
autonomously. We denote this captions generated by LLaV A
asxTv. Subsequently, the image-text pair is converted as
(xI, xTv).
3.3. Scalable LLM Rewrite for Concept Fusion
Given the limited language capacity of LLaV A, we only
use LLaV A to extract all possible visual clues. Then, we
employ LLMs to refine the caption by fusing both the knowl-
edge from AltText xTand the novel visual concepts from
xTv. This step has three main advantages: 1) It ensures
the retention of information delineated in AltText, thereby
amplifying the informativeness of the caption; 2) It can serve
as a form of “strong augmentation” in textual data, character-
ized by a profound restructuring of sentence syntax instead
of focusing on word-level modifications used in existing lan-
guage augmentation techniques [ 37,40]; 3) It can mitigate
the “hallucination” issue arising from large vision-language
models (e.g., LLaV A) to ensure that the entity described in
the ultimate caption is present in the image.
Generating rewrites for a vast corpus of texts using closed-
source models like ChatGPT or Bard is impractical, consider-
ing the substantial financial costs and time incurred through
API utilization. Therefore, to facilitate the rewriting tasks ona large-scale dataset, we turn to open-source state-of-the-art
LLMs. Due to the license issue, we select Vicuna-1.1 [ 48],
renowned for its robust performance in text completion tasks,
as one example of LLM rewriting in this study. We formulate
a context input as the following three components. First, we
include a sentence designed to apprise the LLM of the task,
specifically, rewriting and fusing two attached sentences.
This serves as an initial contextual cue to orient the LLM
towards comprehending the overarching objective. Second,
we impose several constraints on the ultimate output. For in-
stance, our goal is to position attributes prior to noun entities,
all while refraining from introducing any novel semantic
interpretations. Furthermore, it is essential that the sentence
refrains from commencing with the phrase “The image” and
instead directly expounds upon all-encompassed concepts.
Finally, the last part of the context includes two sentences ( xv
andxTv) that require fusing and rewriting, followed by the
separation symbol. This ensures that the LLM is furnished
with the specific texts to be fused and rewritten as part of
its context input. By integrating these three components, we
establish an all-encompassing context that steers the LLM
towards proficiently crafting diverse and knowledge-fused
text rewrites.
Scalable batch-inference process. Employing the
crafted context input as a prompt, Vicuna showcases its profi-
ciency in executing text completion and producing rephrased
renditions of the associated text samples. However, single-
item inference may be time-consuming and not scalable for
4massive data. Therefore, we conduct this process in a batch-
inference process instead of a single-item inference as shown
in Fig 2: we group our data into batches and implement a
batch-inference process to achieve up to 64 times faster on
Nvidia A100. Specifically, we use Vicuna-1.1-13B model
to generate the final output as xTl. The final prompt is as
follows: [ Rephrase the following two sentences into one
short sentence while adhering to the provided instructions:
Place attributes before noun entities without introducing new
meaning. Do not start with “The image”. + 1. AltText; 2.
model generated caption. ] We denote the caption from LLM
as LLM-VeCap, or VeCap for short.
Potential ethics of LLM and failure cases processing.
While upscaling the LLM rewriting process, we identify two
scenarios in which LLM encounters difficulties in executing
the designated task: 1) Ethical Concerns . If the AltText
contains content either illegal or violent, LLM may reply,
“I am sorry that I cannot...”; 2) Length Constraint . In cases
where the AltText exceeds an optimal length, the processing
time of the LLM may be significantly prolonged, thus im-
peding large-scale rewriting. To address the first scenario,
we use the model generation captions as the only input to
be rewritten via LLM to form VeCap, thereby preemptively
excluding potentially unlawful or aggressive content. In the
second scenario, we mitigate this issue by preserving the
generated catpion but truncating the AltText to conform to
the maximum allowable length, thus we have more visual
concepts aligned with the image.
3.4. VeCLIP: Mixed Training Scheme with Visual-
enriched Captions for CLIP
As LLM rewriting may introduce a consistent style, there
could be a decline in data diversity for large-scale pre-
training, even if data quality is enhanced. To enhance data
diversity, we propose a mixed training scheme to serve as
additional text augmentations applied in pre-training:
mix(xt)∼Uniform ([xT, xTl]). (2)
Then, the training loss iterating over the images becomes:
LI=−PN
i=1logexp 
sim(fI(aug(xi
I)),fT(mix(xi
t)))/τ
PN
k=1exp 
sim(fI(aug(xi
I)),fT(mix(xk
t)))/τ.
The only difference with the original CLIP training is that
we alternate the AltTexts with our rephrased sentences, with
all other components remaining unaltered. This modifica-
tion does not incur additional computational complexity or
parameter overheads compared to the standard CLIP train-
ing process. Through the strategic alternation of AltTexts
and our captions, we improve both the quality and diversity
of the pre-training dataset without filtering any data points.
This approach empowers the model to glean insights from
both AltText and VeCap. This simple yet effective strategy
elevates the training regimen for CLIP, offering a scalableframework for optimizing other vision-language pre-training
efforts utilizing extensive web-crawled data.
4. Experiments
4.1. Pre-training Datasets and Downstream Tasks
Pre-training datasets and training setup. We conduct pre-
training experiments on four scales of our datasets (named
VeCap) to show the efficiency and scalability of our method.
Specifically, we set 3M as small scale, 12M as medium
scale, and 100M+ as large scale. We use ViT-B/16 [ 11]
as the vision encoder of CLIP training. Our batch size is
8,192 for small/medium scales (3M/12M), and 32,768 for
large scales (100M+). For efficiency purposes, we employ
JAX [3] and train models on 64 TPUs for the 3M/12M
settings, whereas we utilize 512 TPUs for the 100M/200M
pre-training configurations. All models are trained with the
AXLearn framework.1More details can be found in the
Appendix A. To show its generalizability and effectiveness,
we also evaluate it on well-curated CC3M/CC12M besides
our crawled noisy WIT data, as shown in our ablation studies
and Appendix C.2. We evaluate all pre-trained models on
the following three tasks.
Zero-shot image classification. We evaluate all the mod-
els on ImageNet [ 10], ImageNetV2 [ 33], and VTAB [ 46].
We select 9 tasks (6 from natural sets and 3 from special-
ized sets) that are suitable for zero-shot classification tasks
such as Flowers102 [ 28] and Caltech-101 [ 14] as zero-shot
classification tasks. We list the details in the Appendix.
Zero-shot image-text retrieval. We evaluate the pre-
trained models on COCO [ 23] and Flickr30k [ 31] cross-
modal retrieval tasks: Image-to-Text (denoted as I2T) and
Text-to-Image (T2I) retrieval. For Flickr30k, we evaluate
them on the standard 1K test set. We report the results in
terms of Recall@ kas R@1, R@5, and R@10.
Zero-shot image-to-image retrieval. We select
GPR1200 [ 34] for image-to-image retrieval. GPR1200 [ 34]
serves as a general-purpose benchmark for content-based
image retrieval, encompassing subsets drawn from six dif-
ferent domains. It includes 1200 categories (10 images per
category). Following [ 34], we do not split images as query
and index sets for evaluation. Instead, we perform retrieval
of the nearest neighbor for each image and utilize the remain-
ing images as the index set. We report the mean Average
Precision (mAP).
4.2. Results on Retrieval Tasks
I2T and T2I retrieval. We summarize the main results
in Table 1. We show consistent improvements across
Recall@ kmetrics in both COCO and Flickr30k datasets
for both I2T and T2I retrieval tasks. Specifically, for small
and medium scales (3M/12M), we attain an improvement
1https://github.com/apple/axlearn
5Table 1. Results (Recall@ k) on zero-shot image-to-text and text-to-image retrieval tasks on COCO and Flickr30k. 1.4B-CLIP denotes the
in-house CLIP pre-trained on 1.4B web-crawled image-text pairs. We use ViT-B/16 as the vision encoder of CLIP. (*) Denote FLIP uses
ViT-L/14.
Data ModelCOCO Flickr30k
Image-to-Text Text-to-Image Image-to-Text Text-to-Image
R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10
1.8B ALIGN [17] 58.60 83.00 89.70 45.60 69.80 78.60 88.60 98.70 99.70 75.70 93.80 96.80
400M FLIP* [22] 60.20 82.60 89.90 44.20 69.20 78.40 89.10 98.50 99.60 75.40 92.50 95.90
400M OpenAI CLIP 53.76 77.92 85.53 33.09 58.42 68.90 88.00 98.70 99.40 68.70 90.60 95.20
1.4B In-house CLIP 61.38 82.80 89.78 44.48 69.19 78.28 87.60 97.90 98.80 71.70 91.30 95.24
3MCLIP 5.46 15.34 22.42 3.28 10.44 15.96 12.20 27.80 37.50 6.36 19.16 27.58
VeCLIP 22.30 45.00 56.16 13.01 31.61 42.42 40.60 67.30 76.70 27.58 52.44 63.20
Performance Gain +16.84 +29.66 +33.74 +9.73 +21.17 +26.46 +28.40 +39.50 +39.20 +21.22 +33.28 +35.62
12MCLIP 24.52 48.28 59.82 14.28 34.52 46.29 44.70 71.80 80.40 29.06 58.62 70.00
VeCLIP 47.78 72.54 81.56 31.62 57.19 68.47 73.90 92.30 95.90 55.68 80.78 87.64
Performance Gain +23.26 +24.26 +21.74 +17.34 +22.67 +22.18 +29.20 +20.50 +15.50 +26.62 +22.16 +17.64
100MCLIP 47.24 72.34 81.56 30.61 56.49 67.91 74.40 93.20 96.70 57.16 88.12 88.98
VeCLIP 64.82 85.56 91.98 46.12 71.19 80.23 89.30 97.70 99.20 73.10 89.12 93.14
Performance Gain +17.58 +13.22 +10.42 +15.51 +14.70 +12.32 +14.90 +4.50 +2.50 +15.94 +1.00 +4.16
200MCLIP 52.20 76.22 85.04 34.97 60.42 71.08 80.90 94.90 97.60 63.26 86.58 92.26
VeCLIP 67.20 87.28 92.70 48.40 73.26 81.79 91.10 98.50 99.70 76.32 93.50 96.40
Performance Gain +15.00 +11.06 +7.66 +13.43 +12.84 +10.71 +10.20 +3.60 +2.10 +13.06 +6.92 +4.14
300MCLIP 54.24 78.14 86.48 36.98 62.32 72.70 81.30 95.80 97.80 65.80 88.28 93.16
VeCLIP 67.80 87.94 92.84 48.91 73.54 82.11 91.20 99.10 99.80 76.30 93.00 96.44
Performance Gain +13.56 +9.80 +6.36 +11.93 +11.22 +9.41 +9.90 +3.30 +2.00 +10.50 +4.72 +3.28
of +16.84%/+23.26% in Recall@1 for COCO image-to-
text retrieval, respectively. Notably, the strides made in
Flickr30k are particularly noteworthy, with a remarkable
+28.40%/+29.20% improvement in Recall@1. Subsequently,
we scale our approach to 100M and 200M, where we ob-
serve sustained and substantial improvements. Notably, we
achieve a noteworthy +17.58%/+15.00% enhancement in
COCO image-to-text retrieval performance using 100M and
200M, respectively. Furthermore, we observe a diminishing
improvement margin as we scale up the dataset. Initially, we
achieve a substantial 28.40% improvement in image-to-text
retrieval for Flickr30k with the 3M dataset, which subse-
quently decreases to 10.20% when employing the 200M
dataset. These findings show the advantages of our proposed
pipeline for enhancing CLIP pre-training. By demonstrating
its scalability from 3M to 300M, we provide compelling evi-
dence of its applicability in real-world scenarios, particularly
for training CLIP from scratch using WIT datasets.
Image-to-image retrieval. We use GPR1200 [ 34] with
6 domains for this setting: Google Landmarks V2 (natural
and architectural landmarks) denoted as Land, IMDB Faces
denoted as Faces, iNat (plants, animals, insects and fungi),
INSTRE (planar images and photographs of logos/toys) de-
noted as INST, ImageNet Sketch denoted as Sketch, and SOP
(products and objects, partly isolated). The results (mAP)
are summarized in Table 2. We attain a performance gain of
5.22%/3.92% under small/medium scales (3M/12M). Evenupon upscaling the dataset to 200M, we observe a notable
1.84% increase in average score across six domains. Notably,
our primary performance boost is derived from the Sketch do-
main, underlining the crucial role of visual concepts in zero-
shot transferability. Consequently, our visually-enriched
captions play a pivotal role in learning such transferability
towards downstream tasks.
Data efficiency for pre-training. To show the data ef-
ficiency of VeCLIP, we include ALIGN [ 17], pre-trained
on 1.8B data (denoted as 1.8B-ALIGN), and our in-house
CLIP [ 32] model trained on 1.4B data (denoted as 1.4B-
CLIP) as baselines trained at a significantly larger scale. We
use these models utilizing over tenfold more data compared
to our setting to show the data efficiency of VeCLIP training.
VeCLIP can outperform 1.4B-CLIP model when scaling up
to 100M, representing approximately 7% of its size, across
nearly all downstream tasks. Specifically, in COCO, we
achieve +3.44%/+1.64% gain in Recall@1 for both retrieval
tasks. Upon further scaling to 200M, the improvement be-
comes even more pronounced, reaching +5.82%/+3.92%.
Furthermore, we achieve a notable +8.60%/+2.80% gain in
COCO retrieval, as well as a +2.50%/+0.62% improvement
in Flickr30k, when compared to the 1.8B-ALIGN model.
Remarkably, these improvements are achieved with only
11.1% of the data utilized in the pre-training process. These
results show the data efficacy of VeCLIP. When we scale
it to 300M, the results are similar to 200M. The results on
6Table 2. Image-to-image retrieval results (mAP) on 6-domain GPR1200 [ 34].
Data ModelDomain Name
Land Faces iNat INST Sketch SOP All
3MCLIP 57.98 20.76 17.61 31.14 18.23 74.29 36.67
VeCLIP 66.55 23.51 20.43 38.63 24.59 77.65 41.89
12MCLIP 74.47 30.65 23.60 52.15 30.68 84.25 49.30
VeCLIP 79.30 31.72 25.53 56.65 41.42 84.69 53.22
100MCLIP 85.64 51.68 29.66 68.19 42.45 90.38 61.33
VeCLIP 85.59 42.83 30.72 71.96 52.59 90.54 62.37
200MCLIP 86.96 56.54 30.95 71.51 46.03 90.95 63.83
VeCLIP 86.40 48.48 31.72 73.74 56.52 91.16 65.67
Table 3. Zero-shot classification results (Top- kAccuracy) on ImageNet and
ImageNetV2 [33].
Data ModelImageNet ImageNetV2
Top-1 Top-5 Top-10 Top-1 Top-5 Top-10
3MCLIP 5.46 21.05 28.70 7.09 18.52 25.83
VeCLIP 15.98 34.11 43.23 13.51 30.03 38.93
12MCLIP 31.60 58.80 69.49 27.03 52.68 63.37
VeCLIP 38.11 66.74 76.36 32.53 60.16 70.50
100MCLIP 58.64 85.82 91.79 50.96 79.77 86.91
VeCLIP 60.77 87.77 93.16 54.17 82.51 89.24
200MCLIP 63.72 89.26 94.11 56.84 83.50 89.79
VeCLIP 64.62 90.27 94.90 57.67 85.24 91.62
Figure 3. Performance gain on downstream tasks across
different data scales.
Table 4. Zero-shot classification accuracy. Top-1 accuracies (% ) of VTAB [ 46] across 9 tasks (6 from natural and 3 from specialized sets)
are reported. Full table can be found in Appendix Table A7.
Data ModelNatural Sets Specialized SetsAverageCaltech101 CIFAR100 SVHN DTD OxPet Flowers102 EuroSAT RESISC45 Camelyon
3MCLIP 39.50 9.83 20.89 7.42 7.44 10.40 11.94 7.93 50.65 18.45
VeCLIP 54.30 17.74 18.74 11.23 10.09 22.75 7.35 16.54 52.52 23.48
12MCLIP 70.43 30.06 30.11 30.69 34.51 33.67 8.87 30.05 53.46 35.76
VeCLIP 70.58 45.10 23.61 30.90 36.22 43.94 27.46 38.09 55.54 41.27
100MCLIP 81.44 54.75 38.70 57.28 70.51 51.71 34.45 48.56 53.87 54.59
VeCLIP 81.64 64.62 46.49 57.51 64.81 66.41 46.23 51.75 58.51 59.78
200MCLIP 82.30 61.87 42.83 64.29 75.60 58.67 46.73 55.59 59.30 60.79
VeCLIP 83.14 68.14 44.93 61.95 72.61 68.51 47.36 55.10 62.59 62.70
300M can be found in Appendix. Therefore, we stop further
scaling up the dataset.
4.3. Results on Image Classification
ImageNet. We use the same prompt as CLIP (“A photo
of a [classname].”) for zero-shot evaluation on both Im-
ageNet [ 10] and ImageNetV2 [ 33]. The main results are
summarized in Table 3. We report Top-1, Top-5, and Top-
10 accuracies. In small and medium-scale settings, we ob-
serve a substantial improvement: +10.52%/+6.42% gains
in Top-1 accuracy on ImageNet/ImageNetV2 under the 3M
setting, and +6.51%/5.50% gains under the 12M setting.
While the improvement becomes marginal upon scaling to100M/200M, we still achieve noteworthy +2.07%/+3.21%
and +0.90%/+0.83% gains on 100M and 200M across Ima-
geNet and ImageNetV2, respectively. This shows the data
efficiency of our pre-training approach.
Visual Task Adaptation Benchmark (VTAB). Besides
ImageNet/ImageNetV2, we also select VTAB [ 46] for eval-
uation. Table 4 summarizes zero-shot image classification
results for both the original CLIP models and our models,
utilizing the identical prompt set from CLIP. Our approach
consistently achieves comparable or superior performance
to CLIP across the majority of datasets. For instance, we
observe an average accuracy gain of over 5% under settings
of 3M, 12M, and 100M. Even upon scaling up to 200M,
7we maintain a notable gain of +1.91%. These results show
great robustness on zero-shot classification tasks across dif-
ferent data distributions. We show the overall trend of the
performance gain over the data scale in Figure 3.
4.4. Performance trend across scales
Besides the performance gain, we also visualize the perfor-
mance trend across data scales in pre-training. As shown
in Figure 4, the performance of CLIP utilizing original Alt-
Texts exhibits a marked surge with the increased data size:
while its starting point is poor at 3M, it demonstrates swift
progression up to 12M and 100M. However, once scaled
beyond 100 million, the performance trend exhibits a grad-
ual and eventually saturated growth. On the other hand,
commencing with a higher baseline, VeCLIP employing Ve-
Cap demonstrates substantial improvement in comparison to
CLIP within small to medium scales (3M and 12M). As we
progress beyond 300M, the performance gains of VeCLIP
become relatively incremental but still noticeable in retrieval
tasks. Both CLIP and VeCLIP reach a saturation point when
scaled up to 100M: once over 100M, the performance gain
becomes gradual and marginal.
4.5. Complementary to other datasets to achieve
state-of-the-art performance
Our VeCap datasets with visual-enriched captions can also be
complementary to other well-curated dataset. For example,
DFN [ 13] has shown benefits on CLIP. To demonstrate that,
we train CLIP models with VeCap and DFN separately and
also a combination with them. All the models are trained
under same configuration for learning rate, maximum steps,
and so on.
We summarize the results in Table 5. The high-quality
descriptive captions from VeCap can achieve superior results
compared to DFN in retrieval tasks. However, the perfor-
mance on classification tasks are inferior. After we combine
DFN and VeCap for training, CLIP can achieve the most
improvements for all model sizes.
We also train a H/14 model with resolution 336x336,
and compare it with the state-of-the-art models like Meta-
CLIP [ 42] and DFN [ 13]. The results are summarized in
row 6 to 8 of table 5. Albeit trained on different resolutions
and recipes, the CLIP model with VeCap+DFN is compat-
ible with other models and provide yet another option for
downstream tasks2.
Our VeCLIP with DFN [ 13] can outperform FLIP [ 22]
and OpenAI CLIP with different backbones (as shown in
Table A10 in Appendix). Specifically, our ViT-H/14 model
achieves impressive 83.1% of accuracy on ImageNet. We
leave the further study of combing the synthetic data (VeCap)
with other data curation approaches as a future work.
2Note we took the DFN-H/14 model from its original paper, which is
trained 7 epochs, our model is only trained roughly around 2 epochs.4.6. Ablation Study
Importance of visual-enriched concepts. Different from
previous rewriting methods, our primary emphasis lies in fus-
ing visual-enriched concepts extracted from images. The ab-
lation findings are summarized in Table 6a. We use 3M/12M
as examples to show the performance gain in small/medium
scales. Original AltTexts shows its limitation in retrieval
tasks due to its noise and limited image-specific information.
VeC generated from LLaV A can boost the performance on
retrieval tasks but may hurt the performance on ImageNet
zero-shot task. Introducing VeCap can further improve Ve-
Cap in all settings. Intriguingly, the zero-shot ImageNet
results still lag behind the original AltText. In essence, our
VeCap exerts a profound influence on retrieval prowess yet
exerts a negative effect on classification tasks. We posit
that this phenomenon arises from the following two rea-
sons: 1) there can be a distributional shift in prompts from
pre-training to zero-shot inference in ImageNet, particularly
noteworthy given the extended length and augmented visual
content of VeCap; 2) the data diversity is hurt by LLM rewrit-
ing as LLM uses the same writing/paraphrasing style to fuse
VeCap and AltText.
Importance of mixed training strategies. To mitigate
the aforementioned issues, we propose a mixed training
scheme to alternate between AltTexts and VeCap to provide
more data variety during pre-training. We summarize the
ablation results of VeCLIP in Table 6b. First, we observe a
slight performance improvement by randomly selecting one
AltText in cases where multiple AltTexts are associated with
an image. This practice augments data diversity during pre-
training. Second, interchanging between AltText and VeCap
proves to be advantageous, not only in retaining substantial
performance gains in retrieval tasks but also in markedly
elevating zero-shot results on ImageNet. Lastly, leveraging
all AltTexts and VeCap within the mixed training approach in
VeCLIP achieves superior results across nearly all settings.
Larger backbone architecture. We also investigate a
larger backbone architecture, e.g., ViT-L/14 and ViT-H/14.
The detailed results can be found in both Table 5 and Ap-
pendix C.1. VeCLIP scaled up in backbone size can consis-
tently outperform the original CLIP in all downstream tasks.
Besides, a larger backbone (ViT-L/14) can also achieve up
to 5.87% improvement compared to ViT-B/16. These find-
ings support the effectiveness of VeCLIP in improving CLIP
pre-training, regardless of the specific underlying backbone
architecture.
Generalizability of VeCap on well-curated datasets.
Besides our WIT datasets, we evaluate VeCap on well-
curated CC3M/CC12M. Table 6b shows CLIP achieves bet-
ter performance when pre-trained on CC3M compared to
pre-trained on WIT-3M, indicating the importance of high-
quality captions for pre-training. With VeCap to further
improve the quality of CC3M’s captions, CLIP can achieve
8(a) CLIP (b) VeCLIP
(c) CLIP (d) VeCLIP
Figure 4. Performance trend with ViT-B/16 as the vision backbone. (a) and (c) show the trend of CLIP with original AltTexts while (b) and
(d) show the trend of VeCLIP with LLM-VeC. The performance is improved significantly when we scale pre-training data up to 100M. Once
over 100M, the performance gain becomes gradual and incremental.
Table 5. CLIP training with VeCap and DFN [13], and its comparison with the state-of-the-art models.
Model Resolution DataCOCO (R@1) Flickr30k (R@1)ImageNet
I2T T2I I2T T2I
B/16 224DFN [13] 63.0 43.2 87.1 70.4 76.2
VeCap+DFN 66.3 45.1 88.8 73.6 76.2
Comparison to other state-of-the-art models
DFN [13] 68.5 48.5 89.2 75.1 81.0
L/14 224 FLIP [22] 60.2 44.2 89.1 75.4 74.6
VeCap+DFN 70.8 49.5 92.4 78.4 81.1
224 MetaCLIP [42] 67.2 49.5 92.1 78.5 80.5
H/14 378 DFN [13] 71.8 55.6 94.0 82.1 84.4
336 VeCap+DFN 72.8 52.3 93.6 82.6 83.1
9Data CaptionPrompt
ConstraintCOCO (R@1) Flickr30k (R@1)ImageNet ImageNetV2I2T T2I I2T T2I
WIT-3MAltText - 5.18 3.40 10.50 6.88 8.02 6.88
VeC - 16.76 9.57 32.60 20.06 7.31 6.58
VeCap ✗ 17.34 9.52 37.30 21.62 8.12 6.83
VeCap ✓ 18.10 9.51 40.00 21.94 8.20 7.39
WIT-12MAltText - 22.58 14.23 44.40 30.90 31.14 25.91
VeC - 40.06 24.59 64.10 43.46 7.29 14.74
VeCap ✗ 44.52 27.46 70.90 50.46 21.05 18.11
VeCap ✓ 46.82 26.61 72.60 50.94 20.99 18.41
(a) Importance of visual-enriched concepts for data quality. We use the AltText with “Highest CLIP Score” (HCS) if multiple
AltTexts exist on the same image in all settings.
Data AltText VeCapTraining
SamplingCOCO (R@1) Flickr30k (R@1)ImageNet ImageNetV2I2T T2I I2T T2I
WIT-3M✓ ✗ HCS 5.18 3.40 10.50 6.88 8.02 6.88
✓ ✗ random 5.46 3.28 12.20 6.36 8.26 7.09
✗ ✓ HCS 18.10 9.51 40.00 21.94 8.20 7.39
✓ ✓ HCS&mixed 19.70 12.14 39.30 25.60 14.83 12.36
✓ ✓ random&mixed 22.30 13.01 40.60 27.58 15.98 13.51
WIT-12M✓ ✗ HCS 22.58 14.23 44.40 30.90 31.14 25.91
✓ ✗ random 23.32 14.28 44.70 29.06 31.60 27.03
✗ ✓ HCS 46.82 26.61 72.60 50.94 20.99 18.41
✓ ✓ HCS&mixed 46.00 31.10 72.50 56.82 37.45 32.41
✓ ✓ random&mixed 47.78 31.62 73.90 55.68 38.11 32.51
CC3M✓ ✗ - 13.88 9.64 26.30 18.04 14.59 12.52
✓ ✓ random&mixed 32.04 22.07 57.20 36.54 20.73 17.90
(b) Importance of the mixed training scheme for data variety. “HCS” refers to using the AltText with “Highest CLIP Score” while “random” refers to
randomly selecting one if multiple AltTexts exist.
Table 6. Ablation study of VeCLIP. The highest score is bold, and the second is underlined. “mixed” is our proposed mixed training scheme
to alternate among captions.
significant improvement, since the captions of CC3M are
of higher quality than our noisy WIT dataset. CC3M with
its original captions can outperform the performance of our
WIT-3M with AltTexts, indicating CC3M is of higher quality.
VeCap can significantly improve CLIP under CC3M settings,
e.g., +18.16% on the I2T task of COCO and +6.14% on Ima-
geNet, showing its generalizability on well-curated datasets.
More results are in Appendix C.2.
5. Discussion
Conclusion. We present a simple yet effective approach
to improve CLIP pre-training with leveraging LLaV A and
LLMs to rewrite the captions with more visual-enriched
concepts. VeCLIP is intentionally designed to be scalable
and adaptable for handling extensive image-text datasets
obtained from web crawling. We conduct a thorough evalua-
tion of VeCLIP on a diverse range of raw and noisy datasets,
spanning small, medium, and large scales. The results re-
veal a substantial performance boost, providing compelling
evidence for the effectiveness of our strategy in enhancing
large-scale VLM pre-training. VeCLIP can significantly re-
duce the computational cost and the size of training data forlarge models to reach competitive results as vanilla CLIP.
Future work. We employ CLIP as an illustrative in-
stance to highlight the importance of aligning text and im-
ages within the training dataset. For future work, we plan
to use the collected large-scale dataset to improve the pre-
training of other types of VLMs. Further, LLM can generate
outputs that encompass factual inaccuracies and hallucina-
tions. Thus, we also plan to delve into more sophisticated
filtering techniques to remove such descriptions.
Limitation. We only leverage LLaV A to exploit the
visual concepts. However, the quality measurement metric
for such generative AI is still under study.
References
[1]Amro Abbas, Kushal Tirumala, D ´aniel Simig, Surya Ganguli,
and Ari S Morcos. Semdedup: Data-efficient learning at
web-scale through semantic deduplication. arXiv preprint
arXiv:2303.09540 , 2023. 3
[2]James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng
Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee,
Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu,
Yunxin Jiao, and Aditya Ramesh. Improving image generation
with better captions. OpenAI , 2023. 2
10[3]James Bradbury, Roy Frostig, Peter Hawkins, Matthew James
Johnson, Chris Leary, Dougal Maclaurin, George Necula,
Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne,
and Qiao Zhang. JAX: composable transformations of
Python+NumPy programs. Github , 2018. 5, 1
[4]Liangliang Cao, Bowen Zhang, Chen Chen, Yinfei Yang,
Xianzhi Du, Wencong Zhang, Zhiyun Lu, and Yantao Zheng.
Less is more: Removing text-regions improves clip training
efficiency and robustness. arXiv preprint arXiv:2305.05095 ,
2023. 3
[5]Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
Soricut. Conceptual 12m: Pushing web-scale image-text pre-
training to recognize long-tail visual concepts. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3558–3568, 2021. 2, 3, 1
[6]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In International conference on
machine learning , pages 1597–1607. PMLR, 2020. 1
[7]Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing
image scene classification: Benchmark and state of the art.
Proceedings of the IEEE , 105(10):1865–1883, 2017. 2
[8]Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell
Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-
mann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scal-
ing laws for contrastive language-image learning. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 2818–2829, 2023. 3
[9]M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A.
Vedaldi. Describing textures in the wild. In Proceedings of
the IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR) , 2014. 2
[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li
Fei-Fei. Imagenet: A large-scale hierarchical image database.
In2009 IEEE conference on computer vision and pattern
recognition , pages 248–255. Ieee, 2009. 5, 7, 1
[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Transform-
ers for image recognition at scale. In International Conference
on Learning Representations , 2020. 1, 5
[12] Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yon-
glong Tian. Improving clip training with language rewrites.
arXiv preprint arXiv:2305.20088 , 2023. 1, 2, 3
[13] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig
Schmidt, Alexander T Toshev, and Vaishaal Shankar. Data fil-
tering networks. In NeurIPS 2023 Workshop on Distribution
Shifts: New Frontiers with Foundation Models , 2023. 8, 9, 2,
5
[14] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener-
ative visual models from few training examples: An incre-
mental bayesian approach tested on 101 object categories. In
2004 conference on computer vision and pattern recognition
workshop , pages 178–178. IEEE, 2004. 5, 2
[15] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scalablevision learners. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 16000–
16009, 2022. 1
[16] Patrick Helber, Benjamin Bischke, Andreas Dengel, and
Damian Borth. Introducing eurosat: A novel dataset and
deep learning benchmark for land use and land cover classifi-
cation. In IGARSS 2018-2018 IEEE International Geoscience
and Remote Sensing Symposium , pages 204–207. IEEE, 2018.
2
[17] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representation
learning with noisy text supervision. In International confer-
ence on machine learning , pages 4904–4916. PMLR, 2021.
1, 3, 6
[18] Alex Krizhevsky. Learning multiple layers of features from
tiny images. Canadian Institute for Advanced Research , 2009.
2
[19] Gukyeong Kwon, Zhaowei Cai, Avinash Ravichandran, Er-
han Bas, Rahul Bhotika, and Stefano Soatto. Masked vision
and language modeling for multi-modal representation learn-
ing. In The Eleventh International Conference on Learning
Representations , 2023. 1
[20] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip:
Bootstrapping language-image pre-training for unified vision-
language understanding and generation. In International Con-
ference on Machine Learning , pages 12888–12900. PMLR,
2022. 1
[21] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichten-
hofer, and Kaiming He. Scaling language-image pre-training
via masking. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 23390–
23400, 2023. 3
[22] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichten-
hofer, and Kaiming He. Scaling language-image pre-training
via masking. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 23390–
23400, 2023. 6, 8, 9, 2, 5
[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings, Part
V 13, pages 740–755. Springer, 2014. 5
[24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. arXiv preprint arXiv:2304.08485 ,
2023. 2, 3
[25] Pratyush Maini, Sachin Goyal, Zachary C Lipton, J Zico
Kolter, and Aditi Raghunathan. T-mars: Improving visual
representations by circumventing text feature learning. arXiv
preprint arXiv:2307.03132 , 2023. 3
[26] Norman Mu, Alexander Kirillov, David Wagner, and Sain-
ing Xie. Slip: Self-supervision meets language-image pre-
training. In European Conference on Computer Vision , pages
529–544. Springer, 2022. 3
[27] Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Se-
woong Oh, and Ludwig Schmidt. Improving multi-
11modal datasets with image captioning. arXiv preprint
arXiv:2307.10350 , 2023. 2
[28] Maria-Elena Nilsback and Andrew Zisserman. Automated
flower classification over a large number of classes. In 2008
Sixth Indian conference on computer vision, graphics & image
processing , pages 722–729. IEEE, 2008. 5, 2
[29] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and
CV Jawahar. Cats and dogs. In 2012 IEEE conference on
computer vision and pattern recognition , pages 3498–3505.
IEEE, 2012. 2
[30] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu,
Adams Wei Yu, Minh-Thang Luong, Mingxing Tan, and
Quoc V Le. Combined scaling for zero-shot transfer learning.
arXiv preprint arXiv:2111.10050 , 2021. 3
[31] Bryan A Plummer, Liwei Wang, Chris M Cervantes,
Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.
Flickr30k entities: Collecting region-to-phrase correspon-
dences for richer image-to-sentence models. In Proceedings
of the IEEE international conference on computer vision ,
pages 2641–2649, 2015. 5
[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervision.
InICML , pages 8748–8763, 2021. 1, 3, 6
[33] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and
Vaishaal Shankar. Do imagenet classifiers generalize to im-
agenet? In International conference on machine learning ,
pages 5389–5400. PMLR, 2019. 5, 7
[34] Konstantin Schall, Kai Uwe Barthel, Nico Hezel, and Klaus
Jung. Gpr1200: a benchmark for general-purpose content-
based image retrieval. In International Conference on Multi-
media Modeling , pages 205–216. Springer, 2022. 5, 6, 7
[35] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:
Open dataset of clip-filtered 400 million image-text pairs.
arXiv preprint arXiv:2111.02114 , 2021. 1
[36] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes,
Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.
Laion-5b: An open large-scale dataset for training next gen-
eration image-text models. Advances in Neural Information
Processing Systems , 35:25278–25294, 2022. 1
[37] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improv-
ing neural machine translation models with monolingual data.
arXiv preprint arXiv:1511.06709 , 2015. 4
[38] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-
tinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste
Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971 , 2023. 3
[39] Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Co-
hen, and Max Welling. Rotation equivariant cnns for digital
pathology. In Medical Image Computing and Computer As-
sisted Intervention–MICCAI 2018: 21st International Confer-
ence, Granada, Spain, September 16-20, 2018, Proceedings,
Part II 11 , pages 210–218. Springer, 2018. 2[40] Jason Wei and Kai Zou. Eda: Easy data augmentation tech-
niques for boosting performance on text classification tasks.
arXiv preprint arXiv:1901.11196 , 2019. 4
[41] Wentao Wu, Aleksei Timofeev, Chen Chen, Bowen Zhang,
Kun Duan, Shuangning Liu, Yantao Zheng, Jon Shlens, Xi-
anzhi Du, Zhe Gan, et al. Mofi: Learning image represen-
tations from noisy entity annotated images. arXiv preprint
arXiv:2306.07952 , 2023. 1
[42] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Rus-
sell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke
Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip
data. arXiv preprint arXiv:2309.16671 , 2023. 8, 9
[43] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-
jtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive
captioners are image-text foundation models. arXiv preprint
arXiv:2205.01917 , 2022. 3
[44] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,
Boxin Li, Chunyuan Li, et al. Florence: A new foundation
model for computer vision. arXiv preprint arXiv:2111.11432 ,
2021. 3
[45] Netzer Yuval. Reading digits in natural images with unsuper-
vised feature learning. In Proceedings of the NIPS Workshop
on Deep Learning and Unsupervised Feature Learning , 2011.
2
[46] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre
Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, An-
dre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al.
A large-scale study of representation learning with the visual
task adaptation benchmark. arXiv preprint arXiv:1910.04867 ,
2019. 5, 7, 1, 4
[47] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner,
Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit:
Zero-shot transfer with locked-image text tuning. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 18123–18133, 2022. 3
[48] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li,
Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez,
and Ion Stoica. Judging llm-as-a-judge with mt-bench and
chatbot arena, 2023. 4
121. Appendix
We provide additional details for datasets, experimental set-
tings, results, and analysis in the supplementary material.
A. Dataset details
Pre-training datasets. Instead of using well-curated
datasets, we use image-AltText pairs sampled from a web-
crawled dataset [ 41]. We collect 300M image-text pairs
from the Web and denote it as WIT-300M. Based on WIT-
300M, we build four subsets to cover from small to large
scales. Specifically, WIT-200M is a subset of WIT-300M.
WIT-100M is a subset of WIT-200M. WIT-12M is a subset
of WIT-100M. WIT-3M is a subset of WIT-12M.
VTAB datasets. We choose 9 classification datasets
suitable for zero-shot evaluation from VTAB [ 46]. Table A1
summarizes zero-shot image classification datasets. For both
original CLIP models and our models, we use the identi-
cal prompt set from CLIP. Every class label is expanded
using a collection of prompt templates, as defined by CLIP,
including examples like “A photo of a [classname].” The
class embedding is then computed by taking the average
of the embeddings of all such templates, followed by L2-
normalization.
B. Implementation details
Pre-training hyper-parameters. We summarize the pre-
training hyper-parameters for CLIP training in Table A2. We
pre-train models on up to 512 TPUs with JAX [3].
C. More experimental results
In this section, we present more detailed experimental results
and our ablation studies (e.g., generalization of VeCLIP with
a large backbone, public and well-curated datasets for pre-
training).
C.1. Larger backbone architectures
We also investigate the performance of VeCLIP using a larger
backbone architecture, ViT-L/14. The comparison results
are summarized in Table A3. First, VeCLIP shows a consis-
tent improvement over CLIP employing ViT-L/14 across all
downstream tasks. Second, VeCLIP utilizing ViT-L/14 sur-
passes its counterpart employing ViT-B/16, notably excelling
in image classification tasks, achieving a notable improve-
ment of over 5% on both ImageNet and ImageNetV2. This
shows that VeCLIP has the potential to be scalable with
larger backbone architectures and larger-scale datasets.
C.2. Generalization on well-curated datasets:
CC3M and CC12M
Besides our crawled noisy WIT datasets, we also use a well-
curated dataset, e.g., CC3M and CC12M [ 5], to show the ef-fectiveness and generalizability of our proposed approach on
well-curated datasets. CC3M and CC12M [ 5] were curated
via several rounds of comprehensive refining and filtering to
get high-quality image-caption pairs. We show high-quality
examples of CC3M and the comparison of CC3M’s captions
and WIT-3M’s AltTexts in Appendix D. We present an ex-
perimental comparison between our crawled WIT datasets
and well-curated CC3M/CC12M [5] in this subsection.
3M. As shown in Table A4, CC3M outperforms WIT-3M
when coupled with CLIP pre-training, yielding a notable
increase of +10.70% on the COCO I2T task. Additionally,
VeCLIP exhibits substantial improvement for both WIT-3M
and CC3M. Notably, we achieve a remarkable over 30%
improvement on the I2T task in Flickr30K, and an impressive
over 5% boost on ImageNet and ImageNetV2.
12M. Similar to 3M settings, CC12M exhibits superior
quality and attains better results in contrast to WIT-12M
when utilized with CLIP and original AltTexts. VeCLIP
demonstrates notable improvements for both WIT-12M and
CC12M. For instance, VeCLIP yields a remarkable +12.27%
increase in the I2T task of COCO, along with an impressive
over 5% improvement on both ImageNet and ImageNetV2.
These findings emphasize the effectiveness and generaliz-
ability of VeCLIP in both noisy web-crawled datasets and
meticulously curated datasets, where a richer set of visual
concepts is harnessed for pre-training.
C.3. Complete visual descriptions vs simplified en-
tity representations
In Table 6b of the main paper, we note that sole training on
VeCap might detriment zero-shot performance in compari-
son to the original AltText. Conversely, our mixed training
approach yields optimal outcomes. This intriguing finding
propels us toward a more profound investigation of zero-shot
classification tasks. Following established works [ 12,32],
we employ an identical set of prompting templates, such as
“a photo of a [CLS]” for ImageNet [ 10]. It is conceivable
that this direct and uncomplicated prompt may diverge sig-
nificantly from VeCap’s pre-training, which encompasses a
more extensive and intricate set of visual concepts. To ad-
dress this, we reformulate VeCap into a format as Simplified
Entity Representation (SER). Specifically, we employ the
NLTK package to extract entities from VeCap and subse-
quently apply filtering to retain only noun entities, denoted
as(A, B, C... )∈U. This transformation results in VeCap
being presented as “a photo of [ U]”, offering a concise repre-
sentation of all extracted entities. The results are summarized
in Table A5. Surprisingly, we find that even with SER-style
captions, the zero-shot performance remains inferior to that
achieved with the original AltText. We hypothesize that this
discrepancy may arise from a lack of data diversity. When
all sentences adhere to the same distribution, there exists
a risk of overfitting in the pre-trained model, resulting in
1Table A1. Details of 9 VTAB zero-shot classification datasets.
Dataset Metric Categories Train Size Test Size
CIFAR-100 [18] Accuracy 100 50,000 10,000
SVHN [45] Accuracy 10 73,257 26,032
DTD [9] Accuracy 47 3,760 1,880
Oxford Pets [29] Mean per class 37 3,680 3,669
Caltech101 [14] Mean per class 102 3,060 6,085
Flowers102 [28] Mean per class 102 2,040 6,149
EuroSAT [16] Accuracy 10 10,000 5,000
RESISC45 [7] Accuracy 45 25,200 6,300
Camelyon [39] Accuracy 2 262,144 32,768
Table A2. Details of the pre-training hyper-parameters for CLIP training on our web-crawled datasets.
(a)Pre-training hyper-parameters on 3M.
Config Value
Batch size 8,192
Optimizer AdamW
Learning rate 5×10−4
Weight decay 0.5
Adam β β1, β2= (0.9,0.98)
Adam ϵ 1×10−8
Total epochs 40
Warm up epochs 1
Learning rate schedule cosine decay(b)Pre-training hyper-parameters on 12M.
Config Value
Batch size 8,192
Optimizer AdamW
Learning rate 5×10−4
Weight decay 0.5
Adam β β1, β2= (0.9,0.98)
Adam ϵ 1×10−8
Total epochs 35
Warm up epochs 1
Learning rate schedule cosine decay
(c)Pre-training hyper-parameters on 100M.
Config Value
Batch size 32,768
Optimizer AdamW
Learning rate 5×10−4
Weight decay 0.2
Adam β β1, β2= (0.9,0.98)
Adam ϵ 1×10−6
Total epochs 32
Warm up iterations 2,000
Learning rate schedule cosine decay(d)Pre-training hyper-parameters on 200M.
Config Value
Batch size 32,768
Optimizer AdamW
Learning rate 5×10−4
Weight decay 0.2
Adam β β1, β2= (0.9,0.98)
Adam ϵ 1×10−6
Total epochs 32
Warm up iterations 2,000
Learning rate schedule cosine decay
suboptimal performance in downstream tasks.
C.4. Main results with WIT-300M
We show the detailed results with the Web-crawled Image-
Text 300M dataset (WIT-300M) here. We summarize the
results on various downstream tasks in Table A8. There are
two major observations. First, we observe that the results ob-
tained with a dataset size of 300M are close to those achieved
with 200M for both CLIP and VeCLIP models. This suggests
that a dataset scale of 200 million is sufficient for effectively
training a ViT-B/16-based CLIP model. Second, VeCLIP
achieves significant improvement on retrieval tasks even un-
der 300M settings. Nevertheless, the improvement observedin ImageNet/ImageNetV2 is marginal.
As shown in Table A9, our VeCLIP with DFN [ 13] can
outperform FLIP [ 22] and OpenAI CLIP with different back-
bones. Specifically, our ViT-H/14 model achieves impressive
83.1% of accuracy on ImageNet. We leave the further study
of combing the synthetic data (VeCap) with other data cura-
tion approaches as a future work.
D. Caption quality comparison between well-
curated Datasets and WIT datasets
In Appendix C.2, we find CLIP performs notably better
when pre-trained on CC3M compared to the case of being
pre-trained on noisy crawled WIT datasets due to several
2Table A3. Ablation studies on different backbones with VeCLIP. We use 200M as the pre-training dataset.
Model BackboneCOCO (R@1) Flickr30k (R@1)ImageNet ImageNetV2I2T T2I I2T T2I
CLIP ViT-B/16 52.20 34.97 80.90 63.23 63.72 56.84
VeCLIP ViT-B/16 67.20 48.40 91.10 76.32 64.62 57.67
Performance Gain +15.00 +13.43 +10.20 +13.06 +0.90 +0.81
CLIP ViT-L/14 53.92 37.86 84.60 66.78 68.51 61.13
VeCLIP ViT-L/14 69.92 51.32 92.60 79.04 69.85 63.54
Performance Gain +16.00 +13.46 +8.00 +12.26 +1.34 +2.41
VeCLIP ViT-L/14 vs B/16 +2.72 +2.92 +1.50 +2.72 +5.23 +5.87
Table A4. Ablation studies on well-curated datasets (CC3M and CC12M [ 5]) and the effect of data quality with ViT-B/16 as the vision
backbone.
Model ModelCOCO (R@1) Flickr30k (R@1)ImageNet ImageNetV2I2T T2I I2T T2I
WIT-3MCLIP 5.18 3.40 10.50 6.88 8.02 6.88
VeCLIP 22.30 13.01 40.60 27.58 15.98 13.51
Performance Gain +17.12 +9.61 +30.10 +20.70 +7.96 +6.63
CC3MCLIP 13.88 9.64 26.30 18.04 14.59 12.52
VeCLIP 32.04 22.07 57.20 36.54 20.73 17.90
Performance Gain +18.16 +12.43 +30.90 +18.50 +6.14 +5.38
WIT-12MCLIP 22.58 14.23 44.40 30.90 31.14 25.91
VeCLIP 47.78 31.62 73.90 55.68 38.11 32.51
Performance Gain +25.20 +17.39 +29.50 +24.78 +6.97 +6.60
CC12MCLIP 37.96 24.40 59.70 44.90 39.24 34.41
VeCLIP 53.23 36.90 75.20 62.10 45.32 40.21
Performance Gain +15.27 +12.50 +15.50 +17.20 +6.08 +5.80
rounds of filtering and refining involved in the curation
of CC3M and CC12M. In this section, we show detailed
captions from CC3M and compare them with AltTexts from
WIT datasets.
Here we provide more examples of AltText and LLM-
VeC from WIT-3M:
1.AltText: Ring Capri Pomellato — Pomellato Online
Boutique
VeCap: Pomellato’s Ring Capri features a delicate and
elegant white stone or possibly three pearls, set against a
white background.
2.AltText: Fiamma F45 L 450 Royal Blue Awning.
VeCap: The Fiamma F45 L 450 Royal Blue Awning is
featured on a white car with a visible red logo for perfect
closing, parked in a driveway under a tree, with a house
in the background.
3.AltText: Union votes for strike on pensions
VeCap: The man with white hair, dressed in a suit and
tie, exhibits a surprised or expressive look on his face,with his mouth open and hand near his face, creating a
dynamic and energetic expression.
4.AltText: r/reallifedoodles - I can show you the world
VeCap: The large orange and black drone hovers in the
air, carrying two small teddy bears attached to it, above a
patio area, as seen in the image.
5.AltText: 20 Amazon Skincare Products That Keep Sell-
ing Out
VeCap: 20 Amazon skincare products that keep selling
out feature a happy woman with dark skin, wearing a
white shirt and covering her face with her hands, with a
white spot or patch on her skin.
6.AltText: Durable White Arcane Dining Console Table
With 6 Hidden Chairs
VeCap: A durable white arcane dining console table with
6 hidden chairs is visually appealing and ready for use,
as seen in the image featuring a dining set with a white
table and two benches, surrounded by black chairs.
7.AltText: Peaceful apartment with wi fi internet access,
near old Quebec.
3Table A5. Ablation studies on VeCap and Simplied Entities Representation (SER). We use ViT-B/16 as the backbone and use 200M as the
pre-trained dataset.
Model CaptionCOCO (R@1) Flickr30k (R@1)ImageNet ImageNetV2I2T T2I I2T T2I
CLIP AltText 52.20 34.97 80.90 63.23 63.72 56.84
VeCLIP SER 65.88 49.04 89.20 75.96 58.58 52.89
VeCLIP VeCap 67.20 48.40 91.10 76.32 64.62 57.67
Table A6. Zero-shot classification accuracy. Top-1 Accuracies (% ) of VTAB [ 46] across 9 tasks (6 from natural and 3 from specialized sets)
are reported.
Data ModelNatural Sets Specialized SetsAverageCaltech101 CIFAR100 SVHN DTD OxPet Flowers102 EuroSAT RESISC45 Camelyon
Model Architecture: ViT-B/16
3MCLIP 39.50 9.83 20.89 7.42 7.44 10.40 11.94 7.93 50.65 18.45
VeCLIP 54.30 17.74 18.74 11.23 10.09 22.75 7.35 16.54 52.52 23.48
Performance Gain +14.80 +7.91 -2.15 +3.81 +2.65 +12.35 -4.59 +8.61 +1.87 +5.03
12MCLIP 70.43 30.06 30.11 30.69 34.51 33.67 8.87 30.05 53.46 35.76
VeCLIP 70.58 45.10 23.61 30.90 36.22 43.94 27.46 38.09 55.54 41.27
Performance Gain +0.15 +15.04 -6.50 +0.21 +1.71 +10.27 +18.59 +8.04 +2.08 +5.51
100MCLIP 81.44 54.75 38.70 57.28 70.51 51.71 34.45 48.56 53.87 54.59
VeCLIP 81.64 64.62 46.49 57.51 64.81 66.41 46.23 51.75 58.51 59.78
Performance Gain +0.20 +9.87 +7.79 +0.23 -5.70 +14.70 +11.78 +3.19 +4.64 +5.19
200MCLIP 82.30 61.87 42.83 64.29 75.60 58.67 46.73 55.59 59.30 60.79
VeCLIP 83.14 68.14 44.93 61.95 72.61 68.51 47.36 55.10 62.59 62.70
Performance Gain +0.84 +6.27 +2.10 -2.34 -2.99 +9.84 +0.63 -0.49 +3.29 +1.91
300MCLIP 83.58 63.36 50.04 66.16 74.30 61.81 39.95 56.44 53.94 61.06
VeCLIP 83.07 68.37 50.07 65.98 75.36 69.71 48.28 58.09 51.94 63.43
Performance Gain -0.51 +5.01 +0.03 -0.18 1.06 +7.90 +8.33 +1.65 -2.00 +2.37
VeCap: Experience a peaceful stay in a cozy apartment
with Wi-Fi internet access, located near historic Old Que-
bec, featuring a charming dining room with a set table
and chairs on a hardwood floor, complete with a white
refrigerator in the background.
8.AltText: CABLE BUJIA CHEVROLET CORSA 1.0 1.4
EFI FERRAZZI CABLE BUJIA CHEVROLET CORSA
1.0 1.4 EFI FERRAZZI
VeCap An array of cords and wires, comprising a black
rubber cable, is displayed on a pristine surface, featuring
diverse configurations and orientations, with some lying
horizontally and others positioned at angles.
Here we provide more examples of original caption and
VeCap from CC3M:
1.CC3M Caption: person runs with the ball during their
training session on friday.
VeCap: A group of soccer players, clad in red and black
jerseys, are energetically engaging in a game on a vast
field, with some running and others immersed in the
action, dispersed across the terrain.
2.CC3M Caption: a house with red roof with some bushes
and a lamp post in front.VeCap: A prominent two-story beige building with a
distinctive tile roof stands out in the area, illuminated by
a nearby lamp post. The building appears to be a complex
with several houses or apartments, adding a touch of
complexity to the surroundings.
3.CC3M Caption: eating a big sweet cupcake with choco-
late at cafe.
VeCap: A person holds a half-eaten blueberry muffin on
a plate, standing next to a dining table with a cup, while
eating a big sweet cupcake with chocolate at a cafe.
4.CC3M Caption: paper heart with red ribbon and a bow.
VeCap: A pink background showcases a heart-shaped
box with a bow, adorned in white with the message
“Happy Valentine’s Day,” positioned centrally within the
image.
5.CC3M Caption: person andactor at the premiere
VeCap: Two individuals, a man and a woman, are de-
picted standing together, both attired in formal attire. The
man is donning a tuxedo with a black bow tie, while the
woman is wearing a long dress. They seem to be posi-
tioning themselves for a photograph, possibly at a formal
event.
4Table A7. Image-to-Image retrieval results (mAP) on 6-domain
GPR1200.
Data ModelDomain Name
Land Faces iNat INST Sketch SOP All
3MCLIP 57.98 20.76 17.61 31.14 18.23 74.29 36.67
VeCLIP 66.55 23.51 20.43 38.63 24.59 77.65 41.89
12MCLIP 74.47 30.65 23.60 52.15 30.68 84.25 49.30
VeCLIP 79.30 31.72 25.53 56.65 41.42 84.69 53.22
100MCLIP 85.64 51.68 29.66 68.19 42.45 90.38 61.33
VeCLIP 85.59 42.83 30.72 71.96 52.59 90.54 62.37
200MCLIP 86.96 56.54 30.95 71.51 46.03 90.95 63.83
VeCLIP 86.40 48.48 31.72 73.74 56.52 91.16 65.67
300MCLIP 87.17 57.09 31.83 72.80 47.03 91.30 64.54
VeCLIP 86.22 48.51 32.05 75.29 56.18 91.25 66.91Table A8. Zero-shot classification results (Top- kAccuracy) on
ImageNet and ImageNetV2.
Data ModelImageNet ImageNetV2
Top-1 Top-5 Top-10 Top-1 Top-5 Top-10
3MCLIP 5.46 21.05 28.70 7.09 18.52 25.83
VeCLIP 15.98 34.11 43.23 13.51 30.03 38.93
12MCLIP 31.60 58.80 69.49 27.03 52.68 63.37
VeCLIP 38.11 66.74 76.36 32.53 60.16 70.50
100MCLIP 58.64 85.82 91.79 50.96 79.77 86.91
VeCLIP 60.77 87.77 93.16 54.17 82.51 89.24
200MCLIP 63.72 89.26 94.11 56.84 83.50 89.79
VeCLIP 64.62 90.27 94.90 57.67 85.24 91.62
300MCLIP 65.70 90.55 94.87 58.58 85.32 91.35
VeCLIP 65.71 91.15 95.36 58.76 86.31 91.95
Table A9. Comparison bwetween VeCLIP and other models.
Backbone Model DataCOCO (R@1) Flickr30k (R@1)ImageNetI2T T2I I2T T2I
ViT-B/16OpenAI CLIP OpenAI-400M 53.8 33.1 88.0 68.7 68.6
FLIP [22] LAION-400M - - - - 68.0
VeCLIP DFN [13] + VeCap 66.3 45.1 88.8 73.6 76.2
ViT-L/14OpenAI CLIP OpenAI-400M 58.4 37.8 88.0 68.7 75.3
FLIP [22] LAION-400M 60.2 44.2 89.1 75.4 74.6
VeCLIP DFN [13] + VeCap 71.1 51.1 93.1 81.0 82.0
ViT-H/14 VeCLIP DFN [13] + VeCap 72.8 52.3 93.6 82.6 83.1
6.CC3M Caption: wedding ceremony on the beach
VeCap: A picturesque wedding ceremony unfolds on a
stunning white sandy beach, where perfectly arranged
chairs accommodate guests in formal attire. The groom
and bride exude joy and love, basking in the warm sun-
light.
7.CC3M Caption: revenge is a dish best served cold ...
with lots of lettuce .
VeCap: A large, possibly turtle, tortoise with an angry
expression sits on rocks, displaying a saying or text mes-
sage that reads “Revenge is a dish best cold served with
lots of lettuce.”
8.CC3M Caption: interior of an abandoned factory
VeCap: The sunlit interior of an industrial building stands
in contrast to its darker exterior, with numerous windows
allowing natural light to flood the space, giving it an
empty and open appearance devoid of people or personal
touches.
Examining the aforementioned instances, it becomes evi-
dent that CC3M’s captions exhibit a notable level of preci-
sion and high quality, displaying a closer alignment with the
corresponding images. Conversely, WIT-3M’s AltTexts tend
to be more cluttered, signaling a comparatively subpar per-
formance in contrast to CC3M. Upon implementing VeCap,
even though CC3M’s captions are of high quality, they are
enhanced with more visual concepts leveraged via VeCap.Such integration of enriched visual concepts accounts for the
significant improvement we achieve in retrieval tasks (the
results are shown in Table A4).
E. More examples of WIT with VeCap
We conduct our scalable pipeline over 200 million image-
text pairs. We randomly select more examples below to
show the advantages of VeCap against the original AltText
in terms of visual concepts. The examples are visualized in
Figure A1.
5Figure A1. More examples of VeCap captions and AltTexts.
6TACOTRON : T OWARDS END-TO-ENDSPEECH SYN-
THESIS
Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weissy, Navdeep Jaitly,
Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengioy, Quoc Le, Yannis Agiomyrgiannakis,
Rob Clark, Rif A. Saurous
Google, Inc.
fyxwang,rjryan,rif g@google.com
ABSTRACT
A text-to-speech synthesis system typically consists of multiple stages, such as a
text analysis frontend, an acoustic model and an audio synthesis module. Build-
ing these components often requires extensive domain expertise and may contain
brittle design choices. In this paper, we present Tacotron, an end-to-end genera-
tive text-to-speech model that synthesizes speech directly from characters. Given
<text, audio >pairs, the model can be trained completely from scratch with ran-
dom initialization. We present several key techniques to make the sequence-to-
sequence framework perform well for this challenging task. Tacotron achieves a
3.82 subjective 5-scale mean opinion score on US English, outperforming a pro-
duction parametric system in terms of naturalness. In addition, since Tacotron
generates speech at the frame level, it’s substantially faster than sample-level au-
toregressive methods.
1 I NTRODUCTION
Modern text-to-speech (TTS) pipelines are complex (Taylor, 2009). For example, it is common for
statistical parametric TTS to have a text frontend extracting various linguistic features, a duration
model, an acoustic feature prediction model and a complex signal-processing-based vocoder (Zen
et al., 2009; Agiomyrgiannakis, 2015). These components are based on extensive domain expertise
and are laborious to design. They are also trained independently, so errors from each component
may compound. The complexity of modern TTS designs thus leads to substantial engineering efforts
when building a new system.
There are thus many advantages of an integrated end-to-end TTS system that can be trained on <text,
audio>pairs with minimal human annotation. First, such a system alleviates the need for laborious
feature engineering, which may involve heuristics and brittle design choices. Second, it more easily
allows for rich conditioning on various attributes, such as speaker or language, or high-level features
like sentiment. This is because conditioning can occur at the very beginning of the model rather
than only on certain components. Similarly, adaptation to new data might also be easier. Finally,
a single model is likely to be more robust than a multi-stage model where each component’s errors
can compound. These advantages imply that an end-to-end model could allow us to train on huge
amounts of rich, expressive yet often noisy data found in the real world.
TTS is a large-scale inverse problem: a highly compressed source (text) is “decompressed” into
audio. Since the same text can correspond to different pronunciations or speaking styles, this is a
particularly difﬁcult learning task for an end-to-end model: it must cope with large variations at the
signal level for a given input. Moreover, unlike end-to-end speech recognition (Chan et al., 2016)
These authors really like tacos.
yThese authors would prefer sushi.
1arXiv:1703.10135v2  [cs.CL]  6 Apr 2017Attention 
Pre-net CBHG 
Character embeddings Attention 
RNNDecoder 
RNN 
Pre-net Attention 
RNNDecoder 
RNN
Pre-net Attention 
RNNDecoder 
RNN
Pre-net CBHG Linear-scale 
spectrogram 
Seq2seq target 
with r=3 Griffin-Lim reconstruction 
Attention is applied 
to all decoder steps 
<GO> frame Figure 1: Model architecture. The model takes characters as input and outputs the corresponding
raw spectrogram, which is then fed to the Grifﬁn-Lim reconstruction algorithm to synthesize speech.
or machine translation (Wu et al., 2016), TTS outputs are continuous, and output sequences are
usually much longer than those of the input. These attributes cause prediction errors to accumulate
quickly. In this paper, we propose Tacotron, an end-to-end generative TTS model based on the
sequence-to-sequence (seq2seq) (Sutskever et al., 2014) with attention paradigm (Bahdanau et al.,
2014). Our model takes characters as input and outputs raw spectrogram, using several techniques
to improve the capability of a vanilla seq2seq model. Given <text, audio >pairs, Tacotron can
be trained completely from scratch with random initialization. It does not require phoneme-level
alignment, so it can easily scale to using large amounts of acoustic data with transcripts. With a
simple waveform synthesis technique, Tacotron produces a 3.82 mean opinion score (MOS) on an
US English eval set, outperforming a production parametric system in terms of naturalness1.
2 R ELATED WORK
WaveNet (van den Oord et al., 2016) is a powerful generative model of audio. It works well for TTS,
but is slow due to its sample-level autoregressive nature. It also requires conditioning on linguistic
features from an existing TTS frontend, and thus is not end-to-end: it only replaces the vocoder and
acoustic model. Another recently-developed neural model is DeepV oice (Arik et al., 2017), which
replaces every component in a typical TTS pipeline by a corresponding neural network. However,
each component is independently trained, and it’s nontrivial to change the system to train in an
end-to-end fashion.
To our knowledge, Wang et al. (2016) is the earliest work touching end-to-end TTS using seq2seq
with attention. However, it requires a pre-trained hidden Markov model (HMM) aligner to help the
seq2seq model learn the alignment. It’s hard to tell how much alignment is learned by the seq2seq
per se. Second, a few tricks are used to get the model trained, which the authors note hurts prosody.
Third, it predicts vocoder parameters hence needs a vocoder. Furthermore, the model is trained on
phoneme inputs and the experimental results seem to be somewhat limited.
Char2Wav (Sotelo et al., 2017) is an independently-developed end-to-end model that can be trained
on characters. However, Char2Wav still predicts vocoder parameters before using a SampleRNN
neural vocoder (Mehri et al., 2016), whereas Tacotron directly predicts raw spectrogram. Also, their
seq2seq and SampleRNN models need to be separately pre-trained, but our model can be trained
1Sound demos can be found at https://google.github.io/tacotron
2from scratch. Finally, we made several key modiﬁcations to the vanilla seq2seq paradigm. As
shown later, a vanilla seq2seq model does not work well for character-level inputs.
3 M ODEL ARCHITECTURE
The backbone of Tacotron is a seq2seq model with attention (Bahdanau et al., 2014; Vinyals et al.,
2015). Figure 1 depicts the model, which includes an encoder, an attention-based decoder, and a
post-processing net. At a high-level, our model takes characters as input and produces spectrogram
frames, which are then converted to waveforms. We describe these components below.
Conv1D layers Highway layers 
Conv1D bank + stacking Max-pool along time (stride=1) Bidirectional RNN 
Residual connection 
Conv1D projections 
Figure 2: The CBHG (1-D convolution bank + highway network + bidirectional GRU) module
adapted from Lee et al. (2016).
3.1 CBHG MODULE
We ﬁrst describe a building block dubbed CBHG, illustrated in Figure 2. CBHG consists of a
bank of 1-D convolutional ﬁlters, followed by highway networks (Srivastava et al., 2015) and a
bidirectional gated recurrent unit (GRU) (Chung et al., 2014) recurrent neural net (RNN). CBHG
is a powerful module for extracting representations from sequences. The input sequence is ﬁrst
convolved with Ksets of 1-D convolutional ﬁlters, where the k-th set contains Ckﬁlters of width
k(i.e.k= 1;2; : : : ; K ). These ﬁlters explicitly model local and contextual information (akin to
modeling unigrams, bigrams, up to K-grams). The convolution outputs are stacked together and
further max pooled along time to increase local invariances. Note that we use a stride of 1 to
preserve the original time resolution. We further pass the processed sequence to a few ﬁxed-width
1-D convolutions, whose outputs are added with the original input sequence via residual connections
(He et al., 2016). Batch normalization (Ioffe & Szegedy, 2015) is used for all convolutional layers.
The convolution outputs are fed into a multi-layer highway network to extract high-level features.
Finally, we stack a bidirectional GRU RNN on top to extract sequential features from both forward
and backward context. CBHG is inspired from work in machine translation (Lee et al., 2016),
where the main differences from Lee et al. (2016) include using non-causal convolutions, batch
normalization, residual connections, and stride=1 max pooling. We found that these modiﬁcations
improved generalization.
3.2 E NCODER
The goal of the encoder is to extract robust sequential representations of text. The input to the
encoder is a character sequence, where each character is represented as a one-hot vector and em-
3Table 1: Hyper-parameters and network architectures. “conv- k-c-ReLU” denotes 1-D convolution
with width kandcoutput channels with ReLU activation. FC stands for fully-connected.
Spectral analysis pre-emphasis : 0.97; frame length : 50 ms;
frame shift : 12.5 ms; window type : Hann
Character embedding 256-D
Encoder CBHG Conv1D bank :K=16, conv- k-128-ReLU
Max pooling : stride=1, width=2
Conv1D projections : conv-3-128-ReLU
!conv-3-128-Linear
Highway net : 4 layers of FC-128-ReLU
Bidirectional GRU : 128 cells
Encoder pre-net FC-256-ReLU!Dropout(0.5)!
FC-128-ReLU!Dropout(0.5)
Decoder pre-net FC-256-ReLU!Dropout(0.5)!
FC-128-ReLU!Dropout(0.5)
Decoder RNN 2-layer residual GRU (256 cells)
Attention RNN 1-layer GRU (256 cells)
Post-processing net Conv1D bank :K=8, conv-k-128-ReLU
CBHG Max pooling : stride=1, width=2
Conv1D projections : conv-3-256-ReLU
!conv-3-80-Linear
Highway net : 4 layers of FC-128-ReLU
Bidirectional GRU : 128 cells
Reduction factor ( r) 2
bedded into a continuous vector. We then apply a set of non-linear transformations, collectively
called a “pre-net”, to each embedding. We use a bottleneck layer with dropout as the pre-net in this
work, which helps convergence and improves generalization. A CBHG module transforms the pre-
net outputs into the ﬁnal encoder representation used by the attention module. We found that this
CBHG-based encoder not only reduces overﬁtting, but also makes fewer mispronunciations than a
standard multi-layer RNN encoder (see our linked page of audio samples).
3.3 D ECODER
We use a content-based tanh attention decoder (see e.g. Vinyals et al. (2015)), where a stateful recur-
rent layer produces the attention query at each decoder time step. We concatenate the context vector
and the attention RNN cell output to form the input to the decoder RNNs. We use a stack of GRUs
with vertical residual connections (Wu et al., 2016) for the decoder. We found the residual con-
nections speed up convergence. The decoder target is an important design choice. While we could
directly predict raw spectrogram, it’s a highly redundant representation for the purpose of learning
alignment between speech signal and text (which is really the motivation of using seq2seq for this
task). Because of this redundancy, we use a different target for seq2seq decoding and waveform syn-
thesis. The seq2seq target can be highly compressed as long as it provides sufﬁcient intelligibility
and prosody information for an inversion process, which could be ﬁxed or trained. We use 80-band
mel-scale spectrogram as the target, though fewer bands or more concise targets such as cepstrum
could be used. We use a post-processing network (discussed below) to convert from the seq2seq
target to waveform.
We use a simple fully-connected output layer to predict the decoder targets. An important trick we
discovered was predicting multiple, non-overlapping output frames at each decoder step. Predicting
rframes at once divides the total number of decoder steps by r, which reduces model size, training
time, and inference time. More importantly, we found this trick to substantially increase convergence
speed, as measured by a much faster (and more stable) alignment learned from attention. This is
likely because neighboring speech frames are correlated and each character usually corresponds to
multiple frames. Emitting one frame at a time forces the model to attend to the same input token for
multiple timesteps; emitting multiple frames allows the attention to move forward early in training.
A similar trick is also used in Zen et al. (2016) but mainly to speed up inference.
4The ﬁrst decoder step is conditioned on an all-zero frame, which represents a <GO>frame. In
inference, at decoder step t, the last frame of the rpredictions is fed as input to the decoder at step
t+ 1. Note that feeding the last prediction is an ad-hoc choice here – we could use all rpredictions.
During training, we always feed every r-th ground truth frame to the decoder. The input frame is
passed to a pre-net as is done in the encoder. Since we do not use techniques such as scheduled
sampling (Bengio et al., 2015) (we found it to hurt audio quality), the dropout in the pre-net is
critical for the model to generalize, as it provides a noise source to resolve the multiple modalities
in the output distribution.
3.4 P OST-PROCESSING NET AND WAVEFORM SYNTHESIS
As mentioned above, the post-processing net’s task is to convert the seq2seq target to a target that
can be synthesized into waveforms. Since we use Grifﬁn-Lim as the synthesizer, the post-processing
net learns to predict spectral magnitude sampled on a linear-frequency scale. Another motivation of
the post-processing net is that it can see the full decoded sequence. In contrast to seq2seq, which
always runs from left to right, it has both forward and backward information to correct the prediction
error for each individual frame. In this work, we use a CBHG module for the post-processing net,
though a simpler architecture likely works as well. The concept of a post-processing network is
highly general. It could be used to predict alternative targets such as vocoder parameters, or as a
WaveNet-like neural vocoder (van den Oord et al., 2016; Mehri et al., 2016; Arik et al., 2017) that
synthesizes waveform samples directly.
We use the Grifﬁn-Lim algorithm (Grifﬁn & Lim, 1984) to synthesize waveform from the predicted
spectrogram. We found that raising the predicted magnitudes by a power of 1.2 before feeding
to Grifﬁn-Lim reduces artifacts, likely due to its harmonic enhancement effect. We observed that
Grifﬁn-Lim converges after 50 iterations (in fact, about 30 iterations seems to be enough), which
is reasonably fast. We implemented Grifﬁn-Lim in TensorFlow (Abadi et al., 2016) hence it’s also
part of the model. While Grifﬁn-Lim is differentiable (it does not have trainable weights), we do not
impose any loss on it in this work. We emphasize that our choice of Grifﬁn-Lim is for simplicity;
while it already yields strong results, developing a fast and high-quality trainable spectrogram to
waveform inverter is ongoing work.
4 M ODEL DETAILS
Table 1 lists the hyper-parameters and network architectures. We use log magnitude spectrogram
with Hann windowing, 50 ms frame length, 12.5 ms frame shift, and 2048-point Fourier transform.
We also found pre-emphasis (0.97) to be helpful. We use 24 kHz sampling rate for all experiments.
We use r= 2 (output layer reduction factor) for the MOS results in this paper, though larger rvalues
(e.g.r= 5) also work well. We use the Adam optimizer (Kingma & Ba, 2015) with learning rate
decay, which starts from 0.001 and is reduced to 0.0005, 0.0003, and 0.0001 after 500K, 1M and 2M
global steps, respectively. We use a simple `1 loss for both seq2seq decoder (mel-scale spectrogram)
and post-processing net (linear-scale spectrogram). The two losses have equal weights.
We train using a batch size of 32, where all sequences are padded to a max length. It’s a com-
mon practice to train sequence models with a loss mask, which masks loss on zero-padded frames.
However, we found that models trained this way don’t know when to stop emitting outputs, causing
repeated sounds towards the end. One simple trick to get around this problem is to also reconstruct
the zero-padded frames.
5 E XPERIMENTS
We train Tacotron on an internal North American English dataset, which contains about 24.6 hours
of speech data spoken by a professional female speaker. The phrases are text normalized, e.g. “16”
is converted to “sixteen”.
50 50 100 150 200 250 300 350
Decoder timesteps020406080Encoder states
0.00.10.20.30.40.50.60.70.80.9
(a) Vanilla seq2seq + scheduled sampling
0 10 20 30 40 50 60 70
Decoder timesteps020406080Encoder states
0.00.10.20.30.40.50.60.70.80.9
(b) GRU encoder
0 10 20 30 40 50 60 70
Decoder timesteps020406080Encoder states
0.00.10.20.30.40.50.60.70.80.9
(c) Tacotron (proposed)
Figure 3: Attention alignments on a test phrase. The decoder length in Tacotron is shorter due to
the use of the output reduction factor r=5.
5.1 A BLATION ANALYSIS
We conduct a few ablation studies to understand the key components in our model. As is common
for generative models, it’s hard to compare models based on objective metrics, which often do not
correlate well with perception (Theis et al., 2015). We mainly rely on visual comparisons instead.
We strongly encourage readers to listen to the provided samples.
First, we compare with a vanilla seq2seq model. Both the encoder and decoder use 2 layers of
residual RNNs, where each layer has 256 GRU cells (we tried LSTM and got similar results). No
pre-net or post-processing net is used, and the decoder directly predicts linear-scale log magnitude
spectrogram. We found that scheduled sampling (sampling rate 0.5) is required for this model to
learn alignments and generalize. We show the learned attention alignment in Figure 3. Figure 3(a)
reveals that the vanilla seq2seq learns a poor alignment. One problem is that attention tends to
60 20 40 60 80 100 120 140
Frame02004006008001000DFT bin(a) Without post-processing net
0 20 40 60 80 100 120 140
Frame02004006008001000DFT bin
(b) With post-processing net
Figure 4: Predicted spectrograms with and without using the post-processing net.
get stuck for many frames before moving forward, which causes bad speech intelligibility in the
synthesized signal. The naturalness and overall duration are destroyed as a result. In contrast, our
model learns a clean and smooth alignment, as shown in Figure 3(c).
Second, we compare with a model with the CBHG encoder replaced by a 2-layer residual GRU
encoder. The rest of the model, including the encoder pre-net, remain exactly the same. Comparing
Figure 3(b) and 3(c), we can see that the alignment from the GRU encoder is noisier. Listening to
synthesized signals, we found that noisy alignment often leads to mispronunciations. The CBHG
encoder reduces overﬁtting and generalizes well to long and complex phrases.
Figures 4(a) and 4(b) demonstrate the beneﬁt of using the post-processing net. We trained a model
without the post-processing net while keeping all the other components untouched (except that the
decoder RNN predicts linear-scale spectrogram). With more contextual information, the prediction
from the post-processing net contains better resolved harmonics (e.g. higher harmonics between
bins 100 and 400) and high frequency formant structure, which reduces synthesis artifacts.
5.2 M EAN OPINION SCORE TESTS
We conduct mean opinion score tests, where the subjects were asked to rate the naturalness of the
stimuli in a 5-point Likert scale score. The MOS tests were crowdsourced from native speakers.
7100 unseen phrases were used for the tests and each phrase received 8 ratings. When computing
MOS, we only include ratings where headphones were used. We compare our model with a para-
metric (based on LSTM (Zen et al., 2016)) and a concatenative system (Gonzalvo et al., 2016),
both of which are in production. As shown in Table 2, Tacotron achieves an MOS of 3.82, which
outperforms the parametric system. Given the strong baselines and the artifacts introduced by the
Grifﬁn-Lim synthesis, this represents a very promising result.
Table 2: 5-scale mean opinion score evaluation.
mean opinion score
Tacotron 3.820.085
Parametric 3.690.109
Concatenative 4.090.119
6 D ISCUSSIONS
We have proposed Tacotron, an integrated end-to-end generative TTS model that takes a character
sequence as input and outputs the corresponding spectrogram. With a very simple waveform syn-
thesis module, it achieves a 3.82 MOS score on US English, outperforming a production parametric
system in terms of naturalness. Tacotron is frame-based, so the inference is substantially faster
than sample-level autoregressive methods. Unlike previous work, Tacotron does not need hand-
engineered linguistic features or complex components such as an HMM aligner. It can be trained
from scratch with random initialization. We perform simple text normalization, though recent ad-
vancements in learned text normalization (Sproat & Jaitly, 2016) may render this unnecessary in the
future.
We have yet to investigate many aspects of our model; many early design decisions have gone
unchanged. Our output layer, attention module, loss function, and Grifﬁn-Lim-based waveform
synthesizer are all ripe for improvement. For example, it’s well known that Grifﬁn-Lim outputs
may have audible artifacts. We are currently working on fast and high-quality neural-network-based
spectrogram inversion.
ACKNOWLEDGMENTS
The authors would like to thank Heiga Zen and Ziang Xie for constructive discussions and feedback.
REFERENCES
Mart ´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. TensorFlow: Large-scale machine
learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 , 2016.
Yannis Agiomyrgiannakis. V ocaine the vocoder and applications in speech synthesis. In Acoustics,
Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on , pp. 4230–
4234. IEEE, 2015.
Sercan Arik, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew Gibiansky, Yongguo
Kang, Xian Li, John Miller, Jonathan Raiman, Shubho Sengupta, and Mohammad Shoeybi. Deep
voice: Real-time neural text-to-speech. arXiv preprint arXiv:1702.07825 , 2017.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473 , 2014.
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence
prediction with recurrent neural networks. In Advances in Neural Information Processing Sys-
tems, pp. 1171–1179, 2015.
William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. Listen, attend and spell: A neural
network for large vocabulary conversational speech recognition. In Acoustics, Speech and Signal
Processing (ICASSP), 2016 IEEE International Conference on , pp. 4960–4964. IEEE, 2016.
8Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 , 2014.
Xavi Gonzalvo, Siamak Tazari, Chun-an Chan, Markus Becker, Alexander Gutkin, and Hanna Silen.
Recent advances in Google real-time HMM-driven unit selection synthesizer. In Proc. Inter-
speech , pp. 2238–2242, 2016.
Daniel Grifﬁn and Jae Lim. Signal estimation from modiﬁed short-time fourier transform. IEEE
Transactions on Acoustics, Speech, and Signal Processing , 32(2):236–243, 1984.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp.
770–778, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167 , 2015.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Proceedings of the
3rd International Conference on Learning Representations (ICLR) , 2015.
Jason Lee, Kyunghyun Cho, and Thomas Hofmann. Fully character-level neural machine translation
without explicit segmentation. arXiv preprint arXiv:1610.03017 , 2016.
Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo,
Aaron Courville, and Yoshua Bengio. SampleRNN: An unconditional end-to-end neural audio
generation model. arXiv preprint arXiv:1612.07837 , 2016.
Jose Sotelo, Soroush Mehri, Kundan Kumar, Jo ˜ao Felipe Santos, Kyle Kastner, Aaron Courville, and
Yoshua Bengio. Char2Wav: End-to-end speech synthesis. In ICLR2017 workshop submission ,
2017.
Richard Sproat and Navdeep Jaitly. RNN approaches to text normalization: A challenge. arXiv
preprint arXiv:1611.00068 , 2016.
Rupesh Kumar Srivastava, Klaus Greff, and J ¨urgen Schmidhuber. Highway networks. arXiv preprint
arXiv:1505.00387 , 2015.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
InAdvances in neural information processing systems , pp. 3104–3112, 2014.
Paul Taylor. Text-to-speech synthesis . Cambridge university press, 2009.
Lucas Theis, A ¨aron van den Oord, and Matthias Bethge. A note on the evaluation of generative
models. arXiv preprint arXiv:1511.01844 , 2015.
A¨aron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A generative model for
raw audio. arXiv preprint arXiv:1609.03499 , 2016.
Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. Gram-
mar as a foreign language. In Advances in Neural Information Processing Systems , pp. 2773–
2781, 2015.
Wenfu Wang, Shuang Xu, and Bo Xu. First step towards end-to-end parametric TTS synthesis:
Generating spectral parameters with neural attention. In Proceedings Interspeech , pp. 2243–2247,
2016.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine trans-
lation system: Bridging the gap between human and machine translation. arXiv preprint
arXiv:1609.08144 , 2016.
Heiga Zen, Keiichi Tokuda, and Alan W Black. Statistical parametric speech synthesis. Speech
Communication , 51(11):1039–1064, 2009.
9Heiga Zen, Yannis Agiomyrgiannakis, Niels Egberts, Fergus Henderson, and Przemysław Szczepa-
niak. Fast, compact, and high quality LSTM-RNN based statistical parametric speech synthesizers
for mobile devices. Proceedings Interspeech , 2016.
10Grounded Compositional Semantics
for Finding and Describing Images with Sentences
Richard Socher, Andrej Karpathy, Quoc V . Le*, Christopher D. Manning, Andrew Y. Ng
Stanford University, Computer Science Department, *Google Inc.
richard@socher.org, karpathy@cs.stanford.edu ,
qvl@google.com, manning@stanford.edu, ang@cs.stanford.edu
Abstract
Previous work on Recursive Neural Networks
(RNNs) shows that these models can produce
compositional feature vectors for accurately
representing and classifying sentences or im-
ages. However, the sentence vectors of previ-
ous models cannot accurately represent visu-
ally grounded meaning. We introduce the DT-
RNN model which uses dependency trees to
embed sentences into a vector space in order
to retrieve images that are described by those
sentences. Unlike previous RNN-based mod-
els which use constituency trees, DT-RNNs
naturally focus on the action and agents in
a sentence. They are better able to abstract
from the details of word order and syntactic
expression. DT-RNNs outperform other re-
cursive and recurrent neural networks, kernel-
ized CCA and a bag-of-words baseline on the
tasks of ﬁnding an image that ﬁts a sentence
description and vice versa. They also give
more similar representations to sentences that
describe the same image.
1 Introduction
Single word vector spaces are widely used (Turney
and Pantel, 2010) and successful at classifying sin-
gle words and capturing their meaning (Collobert
and Weston, 2008; Huang et al., 2012; Mikolov et
al., 2013). Since words rarely appear in isolation,
the task of learning compositional meaning repre-
sentations for longer phrases has recently received a
lot of attention (Mitchell and Lapata, 2010; Socher
et al., 2010; Socher et al., 2012; Grefenstette et al.,
2013). Similarly, classifying whole images into aﬁxed set of classes also achieves very high perfor-
mance (Le et al., 2012; Krizhevsky et al., 2012).
However, similar to words, objects in images are of-
ten seen in relationships with other objects which are
not adequately described by a single label.
In this work, we introduce a model, illustrated in
Fig. 1, which learns to map sentences and images
into a common embedding space in order to be able
to retrieve one from the other. We assume word and
image representations are ﬁrst learned in their re-
spective single modalities but ﬁnally mapped into a
jointly learned multimodal embedding space.
Our model for mapping sentences into this space
is based on ideas from Recursive Neural Networks
(RNNs) (Pollack, 1990; Costa et al., 2003; Socher
et al., 2011b). However, unlike all previous RNN
models which are based on constituency trees (CT-
RNNs), our model computes compositional vector
representations inside dependency trees. The com-
positional vectors computed by this new dependency
tree RNN (DT-RNN) capture more of the meaning
of sentences, where we deﬁne meaning in terms of
similarity to a “visual representation” of the textual
description. DT-RNN induced vector representa-
tions of sentences are more robust to changes in the
syntactic structure or word order than related mod-
els such as CT-RNNs or Recurrent Neural Networks
since they naturally focus on a sentence’s action and
its agents.
We evaluate and compare DT-RNN induced rep-
resentations on their ability to use a sentence such as
“A man wearing a helmet jumps on his bike near a
beach. ” to ﬁnd images that show such a scene. The
goal is to learn sentence representations that captureA man wearing a helmet jumps on his bike near a beach .
Compositional Sentence Vectors
Two airplanes parked in an airport .
A man jumping his downhill bike .
Image Vector Representation
A small child sits on a cement wall near white flower .
Multi -Modal 
RepresentationsFigure 1: The DT-RNN learns vector representations for sentences based on their dependency trees. We learn to
map the outputs of convolutional neural networks applied to images into the same space and can then compare both
sentences and images. This allows us to query images with a sentence and give sentence descriptions to images.
the visual scene described and to ﬁnd appropriate
images in the learned, multi-modal sentence-image
space. Conversely, when given a query image, we
would like to ﬁnd a description that goes beyond a
single label by providing a correct sentence describ-
ing it, a task that has recently garnered a lot of at-
tention (Farhadi et al., 2010; Ordonez et al., 2011;
Kuznetsova et al., 2012). We use the dataset intro-
duced by (Rashtchian et al., 2010) which consists of
1000 images, each with 5 descriptions. On all tasks,
our model outperforms baselines and related mod-
els.
2 Related Work
The presented model is connected to several areas of
NLP and vision research, each with a large amount
of related work to which we can only do some justice
given space constraints.
Semantic Vector Spaces and Their Composition-
ality. The dominant approach in semantic vec-
tor spaces uses distributional similarities of single
words. Often, co-occurrence statistics of a word and
its context are used to describe each word (Turney
and Pantel, 2010; Baroni and Lenci, 2010), such
as tf-idf. Most of the compositionality algorithms
and related datasets capture two-word compositions.
For instance, (Mitchell and Lapata, 2010) use two-
word phrases and analyze similarities computed by
vector addition, multiplication and others. Compo-
sitionality is an active ﬁeld of research with many
different models and representations being explored
(Grefenstette et al., 2013), among many others. We
compare to supervised compositional models thatcan learn task-speciﬁc vector representations such as
constituency tree recursive neural networks (Socher
et al., 2011b; Socher et al., 2011a), chain structured
recurrent neural networks and other baselines. An-
other alternative would be to use CCG trees as a
backbone for vector composition (K.M. Hermann,
2013).
Multimodal Embeddings. Multimodal embed-
ding methods project data from multiple sources
such as sound and video (Ngiam et al., 2011) or im-
ages and text. Socher et al. (Socher and Fei-Fei,
2010) project words and image regions into a com-
mon space using kernelized canonical correlation
analysis to obtain state of the art performance in an-
notation and segmentation. Similar to our work, they
use unsupervised large text corpora to learn seman-
tic word representations. Among other recent work
is that by Srivastava and Salakhutdinov (2012) who
developed multimodal Deep Boltzmann Machines.
Similar to their work, we use techniques from the
broad ﬁeld of deep learning to represent images and
words.
Recently, single word vector embeddings have
been used for zero shot learning (Socher et al.,
2013c). Mapping images to word vectors enabled
their system to classify images as depicting objects
such as ”cat” without seeing any examples of this
class. Related work has also been presented at NIPS
(Socher et al., 2013b; Frome et al., 2013). This work
moves zero-shot learning beyond single categories
per image and extends it to unseen phrases and full
length sentences, making use of similar ideas of se-
mantic spaces grounded in visual knowledge.Detailed Image Annotation. Interactions be-
tween images and texts is a growing research ﬁeld.
Early work in this area includes generating single
words or ﬁxed phrases from images (Duygulu et al.,
2002; Barnard et al., 2003) or using contextual in-
formation to improve recognition (Gupta and Davis,
2008; Torralba et al., 2010).
Apart from a large body of work on single object
image classiﬁcation (Le et al., 2012), there is also
work on attribute classiﬁcation and other mid-level
elements (Kumar et al., 2009), some of which we
hope to capture with our approach as well.
Our work is close in spirit with recent work in de-
scribing images with more detailed, longer textual
descriptions. In particular, Yao et al. (2010) describe
images using hierarchical knowledge and humans in
the loop. In contrast, our work does not require hu-
man interactions. Farhadi et al. (2010) and Kulkarni
et al. (2011), on the other hand, use a more automatic
method to parse images. For instance, the former ap-
proach uses a single triple of objects estimated for an
image to retrieve sentences from a collection written
to describe similar images. It forms representations
to describe 1 object, 1 action, and 1 scene. Kulkarni
et al. (2011) extends their method to describe an im-
age with multiple objects. None of these approaches
have used a compositional sentence vector repre-
sentation and they require speciﬁc language gener-
ation techniques and sophisticated inference meth-
ods. Since our model is based on neural networks in-
ference is fast and simple. Kuznetsova et al. (2012)
use a very large parallel corpus to connect images
and sentences. Feng and Lapata (2013) use a large
dataset of captioned images and experiments with
both extractive (search) and abstractive (generation)
models.
Most related is the very recent work of Hodosh et
al. (2013). They too evaluate using a ranking mea-
sure. In our experiments, we compare to kernelized
Canonical Correlation Analysis which is the main
technique in their experiments.
3 Dependency-Tree Recursive Neural
Networks
In this section we ﬁrst focus on the DT-RNN model
that computes compositional vector representations
for phrases and sentences of variable length and syn-tactic type. In section 5 the resulting vectors will
then become multimodal features by mapping im-
ages that show what the sentence describes to the
same space and learning both the image and sen-
tence mapping jointly.
The most common way of building representa-
tions for longer phrases from single word vectors is
to simply linearly average the word vectors. While
this bag-of-words approach can yield reasonable
performance in some tasks, it gives all the words the
same weight and cannot distinguish important dif-
ferences in simple visual descriptions such as The
bike crashed into the standing car. vs.The car
crashed into the standing bike. .
RNN models (Pollack, 1990; Goller and K ¨uchler,
1996; Socher et al., 2011b; Socher et al., 2011a) pro-
vided a novel way of combining word vectors for
longer phrases that moved beyond simple averag-
ing. They combine vectors with an RNN in binary
constituency trees which have potentially many hid-
den layers. While the induced vector representations
work very well on many tasks, they also inevitably
capture a lot of syntactic structure of the sentence.
However, the task of ﬁnding images from sentence
descriptions requires us to be more invariant to syn-
tactic differences. One such example are active-
passive constructions which can collapse words such
as “by” in some formalisms (de Marneffe et al.,
2006), relying instead on the semantic relationship
of “agent”. For instance, The mother hugged her
child. and The child was hugged by its mother.
should map to roughly the same visual space. Cur-
rent Recursive and Recurrent Neural Networks do
not exhibit this behavior and even bag of words rep-
resentations would be inﬂuenced by the words was
andby. The model we describe below focuses more
on recognizing actions and agents and has the po-
tential to learn representations that are invariant to
active-passive differences.
3.1 DT-RNN Inputs: Word Vectors and
Dependency Trees
In order for the DT-RNN to compute a vector repre-
sentation for an ordered list of mwords (a phrase or
sentence), we map the single words to a vector space
and then parse the sentence.
First, we map each word to a d-dimensional vec-
tor. We initialize these word vectors with the un-A man wearing a helmet jumps on his bike near a beachdetnsubj
partmod detdobjroot
prep posspobjprep
detpobj
Figure 2: Example of a full dependency tree for a longer sentence. The DT-RNN will compute vector representations
at every word that represents that word and an arbitrary number of child nodes. The ﬁnal representation is computed
at the root node, here at the verb jumps . Note that more important activity and object words are higher up in this tree
structure.
supervised model of Huang et al. (2012) which can
learn single word vector representations from both
local and global contexts. The idea is to construct a
neural network that outputs high scores for windows
and documents that occur in a large unlabeled corpus
and low scores for window-document pairs where
one word is replaced by a random word. When
such a network is optimized via gradient descent the
derivatives backpropagate into a word embedding
matrixAwhich stores word vectors as columns. In
order to predict correct scores the vectors in the ma-
trix capture co-occurrence statistics. We use d= 50
in all our experiments. The embedding matrix X
is then used by ﬁnding the column index iof each
word: [w] =iand retrieving the corresponding col-
umnxwfromX. Henceforth, we represent an input
sentencesas an ordered list of (word,vector) pairs:
s= ((w1;xw1);:::; (wm;xwm)).
Next, the sequence of words (w1;:::;wm)is
parsed by the dependency parser of de Marneffe
et al. (2006). Fig. 2 shows an example. We can
represent a dependency tree dof a sentence sas
an ordered list of (child,parent) indices: d(s) =
f(i;j)g, where every child word in the sequence
i= 1;:::;m is present and has any word j2
f1;:::;mg[f 0gas its parent. The root word has
as its parent 0and we notice that the same word can
be a parent between zero and mnumber of times.
Without loss of generality, we assume that these in-
dices form a tree structure. To summarize, the input
to the DT-RNN for each sentence is the pair (s;d):
the words and their vectors and the dependency tree.
3.2 Forward Propagation in DT-RNNs
Given these two inputs, we now illustrate how the
DT-RNN computes parent vectors. We will use the
following sentence as a running example: Students 1
ride 2bikes 3at4night 5.Fig. 3 shows its tree
and computed vector representations. The depen-
Students                 bikes           nightride 
at          x1x2
x3x4
x5h1h2
h3h4
h5Figure 3: Example of a DT-RNN tree structure for com-
puting a sentence representation in a bottom up fashion.
dency tree for this sentence can be summarized by
the following set of (child, parent) edges: d=
f(1;2);(2;0);(3;2);(4;2);(5;4)g.
The DT-RNN model will compute parent vectors
at each word that include all the dependent (chil-
dren) nodes in a bottom up fashion using a com-
positionality function gwhich is parameterized by
all the model parameters . To this end, the algo-
rithm searches for nodes in a tree that have either
(i) no children or (ii) whose children have already
been computed and then computes the correspond-
ing vector.
In our example, the words x1;x3;x5are leaf
nodes and hence, we can compute their correspond-
ing hidden nodes via:
hc=g(xc) =f(Wvxc)forc= 1;3;5;(1)
where we compute the hidden vector at position c
via our general composition function g. In the case
of leaf nodes, this composition function becomes
simply a linear layer, parameterized by Wv2Rnd,
followed by a nonlinearity. We cross-validate over
using no nonlinearity ( f= id ),tanh ,sigmoid or
rectiﬁed linear units ( f= max(0;x), but generally
ﬁndtanh to perform best.
The ﬁnal sentence representation we want to com-
pute is ath2, however, since we still do not have h4,we compute that one next:
h4=g(x4;h5) =f(Wvx4+Wr1h5);(2)
where we use the same Wvas before to map the
word vector into hidden space but we now also have
a linear layer that takes as input h5, the only child
of the fourth node. The matrix Wr12Rnnis used
because node 5 is the ﬁrst child node on the right
side of node 4. Generally, we have multiple matri-
ces for composing with hidden child vectors from
the right and left sides: Wr= (Wr1;:::;Wrkr)and
Wl= (Wl1;:::;Wlkl). The number of needed ma-
trices is determined by the data by simply ﬁnding
the maximum numbers of left kland rightkrchil-
dren any node has. If at test time a child appeared
at an even large distance (this does not happen in
our test set), the corresponding matrix would be the
identity matrix.
Now that all children of h2have their hidden vec-
tors, we can compute the ﬁnal sentence representa-
tion via:
h2=g(x2;h1;h3;h4) = (3)
f(Wvx2+Wl1h1+Wr1h3+Wr2h4):
Notice that the children are multiplied by matrices
that depend on their location relative to the current
node.
Another modiﬁcation that improves the mean
rank by approximately 6 in image search on the dev
set is to weight nodes by the number of words under-
neath them and normalize by the sum of words under
all children. This encourages the intuitive desidera-
tum that nodes describing longer phrases are more
important. Let `(i)be the number of leaf nodes
(words) under node iandC(i;y)be the set of child
nodes of node iin dependency tree y. The ﬁnal com-
position function for a node vector hibecomes:
hi=f0
@1
`(i)0
@Wvxi+X
j2C(i)`(j)Wpos(i;j)hj1
A1
A;
(4)
where by deﬁnition `(i) = 1 +P
j2C(i)`(j)and
pos(i;j)is the relative position of child jwith re-
spect to node i, e.g.l1orr2in Eq. 3.
3.3 Semantic Dependency Tree RNNs
An alternative is to condition the weight matrices
on the semantic relations given by the dependencyparser. We use the collapsed tree formalism of
the Stanford dependency parser (de Marneffe et al.,
2006). With such a semantic untying of the weights,
the DT-RNN makes better use of the dependency
formalism and could give active-passive reversals
similar semantic vector representation. The equation
for this semantic DT-RNN ( SDT-RNN ) is the same
as the one above except that the matrices Wpos(i;j)
are replaced with matrices based on the dependency
relationship. There are a total of 141 unique such
relationships in the dataset. However, most are very
rare. For examples of semantic relationships, see
Fig. 2 and the model analysis section 6.7.
This forward propagation can be used for com-
puting compositional vectors and in Sec. 5 we will
explain the objective function in which these are
trained.
3.4 Comparison to Previous RNN Models
The DT-RNN has several important differences to
previous RNN models of Socher et al. (2011a) and
(Socher et al., 2011b; Socher et al., 2011c). These
constituency tree RNNs (CT-RNNs) use the follow-
ing composition function to compute a hidden par-
ent vectorhfrom exactly two child vectors (c1;c2)
in a binary tree: h=f
Wc1
c2
, whereW2
Rd2dis the main parameter to learn. This can be
rewritten to show the similarity to the DT-RNN as
h=f(Wl1c1+Wr1c2). However, there are several
important differences.
Note ﬁrst that in previous RNN models the par-
ent vectors were of the same dimensionality to be
recursively compatible and be used as input to the
next composition. In contrast, our new model ﬁrst
maps single words into a hidden space and then par-
ent nodes are composed from these hidden vectors.
This allows a higher capacity representation which
is especially helpful for nodes that have many chil-
dren.
Secondly, the DT-RNN allows for n-ary nodes in
the tree. This is an improvement that is possible even
for constituency tree CT-RNNs but it has not been
explored in previous models.
Third, due to computing parent nodes in con-
stituency trees, previous models had the problem
that words that are merged last in the tree have a
larger weight or importance in the ﬁnal sentence rep-Figure 4: The architecture of the visual model. This model has 3 sequences of ﬁltering, pooling and local contrast
normalization layers. The learnable parameters are the ﬁltering layer. The ﬁlters are not shared, i.e., the network is
nonconvolutional.
resentation. This can be problematic since these are
often simple non-content words, such as a leading
‘But,’. While such single words can be important for
tasks such as sentiment analysis, we argue that for
describing visual scenes the DT-RNN captures the
more important effects: The dependency tree struc-
tures push the central content words such as the main
action or verb and its subject and object to be merged
last and hence, by construction, the ﬁnal sentence
representation is more robust to less important ad-
jectival modiﬁers, word order changes, etc.
Fourth, we allow some untying of weights de-
pending on either how far away a constituent is from
the current word or what its semantic relationship is.
Now that we can compute compositional vector
representations for sentences, the next section de-
scribes how we represent images.
4 Learning Image Representations with
Neural Networks
The image features that we use in our experiments
are extracted from a deep neural network, replicated
from the one described in (Le et al., 2012). The net-
work was trained using both unlabeled data (random
web images) and labeled data to classify 22,000 cat-
egories in ImageNet (Deng et al., 2009). We then
used the features at the last layer, before the classi-
ﬁer, as the feature representation in our experiments.
The dimension of the feature vector of the last layer
is 4,096. The details of the model and its training
procedures are as follows.
The architecture of the network can be seen in
Figure 4. The network takes 200x200 pixel images
as inputs and has 9 layers. The layers consist ofthree sequences of ﬁltering, pooling and local con-
trast normalization (Jarrett et al., 2009). The pooling
function is L2 pooling of the previous layer (taking
the square of the ﬁltering units, summing them up
in a small area in the image, and taking the square-
root). The local contrast normalization takes inputs
in a small area of the lower layer, subtracts the mean
and divides by the standard deviation.
The network was ﬁrst trained using an unsuper-
vised objective: trying to reconstruct the input while
keeping the neurons sparse. In this phase, the net-
work was trained on 20 million images randomly
sampled from the web. We resized a given image
so that its short dimension has 200 pixels. We then
cropped a ﬁxed size 200x200 pixel image right at the
center of the resized image. This means we may dis-
card a fraction of the long dimension of the image.
After unsupervised training, we used Ima-
geNet (Deng et al., 2009) to adjust the features in the
entire network. The ImageNet dataset has 22,000
categories and 14 million images. The number of
images in each category is equal across categories.
The 22,000 categories are extracted from WordNet.
To speed up the supervised training of this net-
work, we made a simple modiﬁcation to the algo-
rithm described in Le et al. (2012): adding a “bottle-
neck” layer in between the last layer and the classi-
ﬁer. to reduce the number of connections. We added
one “bottleneck” layer which has 4,096 units in be-
tween the last layer of the network and the softmax
layer. This newly-added layer is fully connected to
the previous layer and has a linear activation func-
tion. The total number of connections of this net-
work is approximately 1.36 billion.The network was trained again using the super-
vised objective of classifying the 22,000 classes in
ImageNet. Most features in the networks are local,
which allows model parallelism. Data parallelism
by asynchronous SGD was also employed as in Le
et al. (2012). The entire training, both unsupervised
and supervised, took 8 days on a large cluster of ma-
chines. This network achieves 18.3% precision@1
on the full ImageNet dataset (Release Fall 2011).
We will use the features at the bottleneck layer as
the feature vector zof an image. Each scaled and
cropped image is presented to our network. The net-
work then performs a feedforward computation to
compute the values of the bottleneck layer. This
means that every image is represented by a ﬁxed
length vector of 4,096 dimensions. Note that during
training, no aligned sentence-image data was used
and the ImageNet classes do not fully intersect with
the words used in our dataset.
5 Multimodal Mappings
The previous two sections described how we can
map sentences into a d= 50 -dimensional space and
how to extract high quality image feature vectors of
4096 dimensions. We now deﬁne our ﬁnal multi-
modal objective function for learning joint image-
sentence representations with these models. Our
training set consists of Nimages and their feature
vectorsziand each image has 5 sentence descrip-
tionssi1;:::;si5for which we use the DT-RNN to
compute vector representations. See Fig. 5 for ex-
amples from the dataset. For training, we use a max-
margin objective function which intuitively trains
pairs of correct image and sentence vectors to have
high inner products and incorrect pairs to have low
inner products. Let vi=WIzibe the mapped image
vector andyij=DTRNN (sij)the composed sen-
tence vector. We deﬁne Sto be the set of all sentence
indices andS(i)the set of sentence indices corre-
sponding to image i. Similarly,Iis the set of all im-
age indices andI(j)is the image index of sentence
j. The setPis the set of all correct image-sentence
training pairs (i;j). The ranking cost function to
minimize is then: J(WI;) =
X
(i;j)2PX
c2SnS (i)max(0; vT
iyj+vT
iyc)
+X
(i;j)2PX
c2InI (j)max(0; vT
iyj+vT
cyj);(5)whereare the language composition matrices,
and both second sums are over other sentences com-
ing from different images and vice versa. The hyper-
parameter is the margin. The margin is found via
cross validation on the dev set and usually around 1.
The ﬁnal objective also includes the regulariza-
tion term=left (kk2
2+kWIkF). Both the visual
model and the word vector learning require a very
large amount of training data and both have a huge
number of parameters. Hence, to prevent overﬁtting,
we assume their weights are ﬁxed and only train the
DT-RNN parameters WI. If larger training corpora
become available in the future, training both jointly
becomes feasible and would present a very promis-
ing direction. We use a modiﬁed version of Ada-
Grad (Duchi et al., 2011) for optimization of both
WIand the DT-RNN as well as the other baselines
(except kCCA). Adagrad has achieved good perfor-
mance previously in neural networks models (Dean
et al., 2012; Socher et al., 2013a). We modify it
by resetting all squared gradient sums to 1 every 5
epochs. With both images and sentences in the same
multimodal space, we can easily query the model for
similar images or sentences by ﬁnding the nearest
neighbors in terms of negative inner products.
An alternative objective function is based on the
squared loss J(WI;) =P
(i;j)2Pkvi yjk2
2:This
requires an alternating minimization scheme that
ﬁrst trains only WI, then ﬁxes WIand trains the
DT-RNN weights and then repeats this several
times. We ﬁnd that the performance with this ob-
jective function (paired with ﬁnding similar images
using Euclidean distances) is worse for all models
than the margin loss of Eq. 5. In addition kCCA
also performs much better using inner products in
the multimodal space.
6 Experiments
We use the dataset of Rashtchian et al. (2010) which
consists of 1000 images, each with 5 sentences. See
Fig. 5 for examples.
We evaluate and compare the DT-RNN in three
different experiments. First, we analyze how well
the sentence vectors capture similarity in visual
meaning. Then we analyze Image Search with
Query Sentences : to query each model with a sen-
tence in order to ﬁnd an image showing that sen-1. A woman and her dog watch the cameraman in their living with wooden floors .
2. A woman sitting on the couch while a black faced dog runs across the floor .
3. A woman wearing a backpack sits on a couch while a small dog runs on the hardwood floor next to her .
4. A women sitting on a sofa while a small Jack Russell walks towards the camera .
5. White and black small dog walks toward the camera while woman sits on couch , desk and computer seen 
    in the background as well as a pillow , teddy bear and moggie toy on the wood floor .
1. A man in a cowboy hat check approaches a small red sports car .
2. The back and left side of a red Ferrari and two men admiring it .
3. The sporty car is admired by passer by .
4. Two men next to a red sports car in a parking lot .
5. Two men stand beside a red sports car .Figure 5: Examples from the dataset of images and their sentence descriptions (Rashtchian et al., 2010). Sentence
length varies greatly and different objects can be mentioned ﬁrst. Hence, models have to be invariant to word ordering.
tence’s visual ‘meaning.’ The last experiment De-
scribing Images by Finding Suitable Sentences does
the reverse search where we query the model with an
image and try to ﬁnd the closest textual description
in the embedding space.
In our comparison to other methods we focus on
those models that can also compute ﬁxed, continu-
ous vectors for sentences. In particular, we compare
to the RNN model on constituency trees of Socher
et al. (2011a), a standard recurrent neural network;
a simple bag-of-words baseline which averages the
words. All models use the word vectors provided by
Huang et al. (2012) and do not update them as dis-
cussed above. Models are trained with their corre-
sponding gradients and backpropagation techniques.
A standard recurrent model is used where the hidden
vector at word index tis computed from the hidden
vector at the previous time step and the current word
vector:ht=f(Whht 1+Wxxt). During training,
we take the last hidden vector of the sentence chain
and propagate the error into that. It is also this vector
that is used to represent the sentence.
Other possible comparisons are to the very differ-
ent models mentioned in the related work section.
These models use a lot more task-speciﬁc engineer-
ing, such as running object detectors with bounding
boxes, attribute classiﬁers, scene classiﬁers, CRFs
for composing the sentences, etc. Another line of
work uses large sentence-image aligned resources
(Kuznetsova et al., 2012), whereas we focus on eas-
ily obtainable training data of each modality sepa-
rately and a rather small multimodal corpus.
In our experiments we split the data into 800 train-
ing, 100 development and 100 test images. Since
there are 5 sentences describing each image, wehave 4000 training sentences and 500 testing sen-
tences. The dataset has 3020 unique words, half of
which only appear once. Hence, the unsupervised,
pre-trained semantic word vector representations are
crucial. Word vectors are not ﬁne tuned during train-
ing. Hence, the main parameters are the DT-RNN’s
Wl;Wror the semantic matrices of which there are
141 and the image mapping WI. For both DT-RNNs
the weight matrices are initialized to block identity
matrices plus Gaussian noise. Word vectors and hid-
den vectors are set o length 50. Using the develop-
ment split, we found = 0:08and the learning rate
of AdaGrad to 0:0001 . The best model uses a mar-
gin of  = 3 .
Inspired by Socher and Fei-Fei (2010) and Ho-
dosh et al. (2013) we also compare to kernelized
Canonical Correlation Analysis (kCCA). We use the
average of word vectors for describing sentences and
the same powerful image vectors as before. We
use the code of Socher and Fei-Fei (2010). Tech-
nically, one could combine the recently introduced
deep CCA Andrew et al. (2013) and train the re-
cursive neural network architectures with the CCA
objective. We leave this to future work. With lin-
ear kernels, kCCA does well for image search but
is worse for sentence self similarity and describing
images with sentences close-by in embedding space.
All other models are trained by replacing the DT-
RNN function in Eq. 5.
6.1 Similarity of Sentences Describing the
Same Image
In this experiment, we ﬁrst map all 500 sentences
from the test set into the multi-modal space. Then
for each sentence, we ﬁnd the nearest neighbor sen-Sentences Similarity for Image
Model Mean Rank
Random 101.1
BoW 11.8
CT-RNN 15.8
Recurrent NN 18.5
kCCA 10.7
DT-RNN 11.1
SDT-RNN 10.5Image Search
Model Mean Rank
Random 52.1
BoW 14.6
CT-RNN 16.1
Recurrent NN 19.2
kCCA 15.9
DT-RNN 13.6
SDT-RNN 12.5Describing Images
Model Mean Rank
Random 92.1
BoW 21.1
CT-RNN 23.9
Recurrent NN 27.1
kCCA 18.0
DT-RNN 19.2
SDT-RNN 16.9
Table 1: Left: Comparison of methods for sentence similarity judgments. Lower numbers are better since they indicate
that sentences describing the same image rank more highly (are closer). The ranks are out of the 500 sentences in the
test set. Center: Comparison of methods for image search with query sentences. Shown is the average rank of the
single correct image that is being described. Right: Average rank of a correct sentence description for a query image.
tences in terms of inner products. We then sort
these neighbors and record the rank or position of
the nearest sentence that describes the same im-
age. If all the images were very unique and the vi-
sual descriptions close-paraphrases and consistent,
we would expect a very low rank. However, usually
a handful of images are quite similar (for instance,
there are various images of airplanes ﬂying, parking,
taxiing or waiting on the runway) and sentence de-
scriptions can vary greatly in detail and speciﬁcity
for the same image.
Table 1 (left) shows the results. We can see that
averaging the high quality word vectors already cap-
tures a lot of similarity. The chain structure of a
standard recurrent neural net performs worst since
its representation is dominated by the last words in
the sequence which may not be as important as ear-
lier words.
6.2 Image Search with Query Sentences
This experiment evaluates how well we can ﬁnd im-
ages that display the visual meaning of a given sen-
tence. We ﬁrst map a query sentence into the vector
space and then ﬁnd images in the same space using
simple inner products. As shown in Table 1 (center),
the new DT-RNN outperforms all other models.
6.3 Describing Images by Finding Suitable
Sentences
Lastly, we repeat the above experiments but with
roles reversed. For an image, we search for suitable
textual descriptions again simply by ﬁnding close-
by sentence vectors in the multi-modal embedding
space. Table 1 (right) shows that the DT-RNN again
outperforms related models. Fig. 2assigned to im-Image Search
Model mRank
BoW 24.7
CT-RNN 22.2
Recurrent NN 28.4
kCCA 13.7
DT-RNN 13.3
SDT-RNN 15.8Describing Images
Model mRank
BoW 30.7
CT-RNN 29.4
Recurrent NN 31.4
kCCA 38.0
DT-RNN 26.8
SDT-RNN 37.5
Table 2: Results of multimodal ranking when models are
trained with a squared error loss and using Euclidean dis-
tance in the multimodal space. Better performance is
reached for all models when trained in a max-margin loss
and using inner products as in the previous table.
ages. The average ranking of 25.3 for a correct sen-
tence description is out of 500 possible sentences. A
random assignment would give an average ranking
of 100.
6.4 Analysis: Squared Error Loss vs. Margin
Loss
We analyze the inﬂuence of the multimodal loss
function on the performance. In addition, we com-
pare using Euclidean distances instead of inner prod-
ucts. Table 2 shows that performance is worse for all
models in this setting.
6.5 Analysis: Recall at nvs Mean Rank
Hodosh et al. (2013) and other related work use re-
call atnas an evaluation measure. Recall at ncap-
tures how often one of the top nclosest vectors were
a correct image or sentence and gives a good intu-
ition of how a model would perform in a ranking
task that presents nsuch results to a user. Below, we
compare three commonly used and high performing
models: bag of words, kCCA and our SDT-RNN onA gray convertible sports car is parked in front of the trees .
A close -up view of the headlights of a blue old -fashioned car .
Black shiny sports car parked on concrete driveway .
Five cows grazing on a patch of grass between two roadways .
A jockey rides a brown and white horse in a dirt corral .
A young woman is riding a Bay hose in a dirt riding -ring.
A white bird pushes a miniature teal shopping cart .
A person rides a brown horse .
A motocross bike with rider flying through the air .
White propeller plane parked in middle of grassy field .
The white jet with its landing gear down flies in the blue sky .
An elderly woman catches a ride on the back of the bicycle .
A green steam train running down the tracks .
Steamy locomotive speeding thou the forest .
A steam engine comes down a train track near trees .
A double decker bus is driving by Big Ben in London .
People in an outrigger canoe sail on emerald green water .
Two people sailing a small white sail boat .
behind a cliff , a boat sails away
Tourist move in on Big Ben on a typical overcast London day .
A group of people sitting around a table on a porch .
A group of four people walking past a giant mushroom .
A man and women smiling for the camera in a kitchen .
A group of men sitting around a table drinking while a man behind 
stands pointing .
Figure 6: Images and their sentence descriptions assigned by the DT-RNN.
Image Search
Model mRank4 R@15 R@55 R@105
BoW 14.6 15.8 42.2 60.0
kCCA 15.9 16.4 41.4 58.0
SDT-RNN 12.5 16.4 46.6 65.6
Describing Images
BoW 21.1 19.0 38.0 57.0
kCCA 18.0 21.0 47.0 61.0
SDT-RNN 16.9 23.0 45.0 63.0
Table 3: Evaluation comparison between mean rank of
the closest correct image or sentence (lower is better 4)
with recall at different thresholds (higher is better, 5).
With one exception (R@5, bottom table), the SDT-RNN
outperforms the other two models and all other models
we did not include here.
this different metric. Table 3 shows that the mea-
sures do correlate well and the SDT-RNN also per-
forms best on the multimodal ranking tasks when
evaluated with this measure.
6.6 Error Analysis
In order to understand the main problems with the
composed sentence vectors, we analyze the sen-
tences that have the worst nearest neighbor rank be-
tween each other. We ﬁnd that the main failure mode
of the SDT-RNN occurs when a sentence that should
describe the same image does not use a verb but the
other sentences of that image do include a verb. For
example, the following sentence pair has vectors that
are very far apart from each other even though they
are supposed to describe the same image:
1. A blue and yellow airplane ﬂying straight down
while emitting white smoke
2. Airplane in dive positionGenerally, as long as both sentences either have a
verb or do not, the SDT-RNN is more robust to dif-
ferent sentence lengths than bag of words represen-
tations.
6.7 Model Analysis: Semantic Composition
Matrices
The best model uses composition matrices based on
semantic relationships from the dependency parser.
We give some insights into what the model learns
by listing the composition matrices with the largest
Frobenius norms. Intuitively, these matrices have
learned larger weights that are being multiplied with
the child vector in the tree and hence that child will
have more weight in the ﬁnal composed parent vec-
tor. In decreasing order of Frobenius norm, the re-
lationship matrices are: nominal subject, possession
modiﬁer (e.g. their), passive auxiliary, preposition
at, preposition in front of, passive auxiliary, passive
nominal subject, object of preposition, preposition
in and preposition on.
The model learns that nouns are very important as
well as their spatial prepositions and adjectives.
7 Conclusion
We introduced a new recursive neural network
model that is based on dependency trees. For eval-
uation, we use the challenging task of mapping sen-
tences and images into a common space for ﬁnding
one from the other. Our new model outperforms
baselines and other commonly used models that can
compute continuous vector representations for sen-
tences. In comparison to related models, the DT-
RNN is more invariant and robust to surface changes
such as word order.References
G. Andrew, R. Arora, K. Livescu, and J. Bilmes. 2013.
Deep canonical correlation analysis. In ICML , At-
lanta, Georgia.
K. Barnard, P. Duygulu, N. de Freitas, D. Forsyth,
D. Blei, and M. Jordan. 2003. Matching words and
pictures. JMLR .
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics , 36(4):673–721.
R. Collobert and J. Weston. 2008. A uniﬁed archi-
tecture for natural language processing: deep neural
networks with multitask learning. In Proceedings of
ICML , pages 160–167.
F. Costa, P. Frasconi, V . Lombardo, and G. Soda. 2003.
Towards incremental parsing of natural language using
recursive neural networks. Applied Intelligence .
M. de Marneffe, B. MacCartney, and C. D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC .
J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin,
Q. V . Le, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker,
K. Yang, and A.Y . Ng. 2012. Large scale distributed
deep networks. In NIPS .
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. 2009. ImageNet: A Large-Scale Hierarchical Im-
age Database. In CVPR .
J. Duchi, E. Hazan, and Y . Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic op-
timization. JMLR , 12, July.
P. Duygulu, K. Barnard, N. de Freitas, and D. Forsyth.
2002. Object recognition as machine translation. In
ECCV .
A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young,
C. Rashtchian, J. Hockenmaier, and D. Forsyth. 2010.
Every picture tells a story: Generating sentences from
images. In ECCV .
Y . Feng and M. Lapata. 2013. Automatic caption gen-
eration for news images. IEEE Trans. Pattern Anal.
Mach. Intell. , 35.
A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean,
M. Ranzato, and T. Mikolov. 2013. Devise: A deep
visual-semantic embedding model. In NIPS .
C. Goller and A. K ¨uchler. 1996. Learning task-
dependent distributed representations by backpropaga-
tion through structure. In Proceedings of the Interna-
tional Conference on Neural Networks .
E. Grefenstette, G. Dinu, Y .-Z. Zhang, M. Sadrzadeh, and
M. Baroni. 2013. Multi-step regression learning for
compositional distributional semantics. In IWCS .
A. Gupta and L. S. Davis. 2008. Beyond nouns: Exploit-
ing prepositions and comparative adjectives for learn-
ing visual classiﬁers. In ECCV .M. Hodosh, P. Young, and J. Hockenmaier. 2013. Fram-
ing image description as a ranking task: Data, mod-
els and evaluation metrics. J. Artif. Intell. Res. (JAIR) ,
47:853–899.
E. H. Huang, R. Socher, C. D. Manning, and A. Y . Ng.
2012. Improving Word Representations via Global
Context and Multiple Word Prototypes. In ACL.
K. Jarrett, K. Kavukcuoglu, M.A. Ranzato, and Y . Le-
Cun. 2009. What is the best multi-stage architecture
for object recognition? In ICCV .
P. Blunsom. K.M. Hermann. 2013. The role of syntax
in vector space models of compositional semantics. In
ACL.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. 2012.
Imagenet classiﬁcation with deep convolutional neural
networks. In NIPS .
G. Kulkarni, V . Premraj, S. Dhar, S. Li, Y . Choi, A. C.
Berg, and T. L. Berg. 2011. Baby talk: Understanding
and generating image descriptions. In CVPR .
N. Kumar, A. C. Berg, P. N. Belhumeur, , and S. K. Na-
yar. 2009. Attribute and simile classiﬁers for face ver-
iﬁcation. In ICCV .
P. Kuznetsova, V . Ordonez, A. C. Berg, T. L. Berg, and
Yejin Choi. 2012. Collective generation of natural
image descriptions. In ACL.
Q. V . Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen,
G.S. Corrado, J. Dean, and A. Y . Ng. 2012. Build-
ing high-level features using large scale unsupervised
learning. In ICML .
T. Mikolov, W. Yih, and G. Zweig. 2013. Linguistic
regularities in continuous spaceword representations.
InHLT-NAACL .
J. Mitchell and M. Lapata. 2010. Composition in dis-
tributional models of semantics. Cognitive Science ,
34(8):1388–1429.
J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A.Y .
Ng. 2011. Multimodal deep learning. In ICML .
V . Ordonez, G. Kulkarni, and T. L. Berg. 2011. Im2text:
Describing images using 1 million captioned pho-
tographs. In NIPS .
J. B. Pollack. 1990. Recursive distributed representa-
tions. Artiﬁcial Intelligence , 46, November.
C. Rashtchian, P. Young, M. Hodosh, and J. Hocken-
maier. 2010. Collecting image annotations using
Amazon’s Mechanical Turk. In Workshop on Creat-
ing Speech and Language Data with Amazon’s MTurk .
R. Socher and L. Fei-Fei. 2010. Connecting modalities:
Semi-supervised segmentation and annotation of im-
ages using unaligned text corpora. In CVPR .
R. Socher, C. D. Manning, and A. Y . Ng. 2010. Learning
continuous phrase representations and syntactic pars-
ing with recursive neural networks. In Proceedings of
the NIPS-2010 Deep Learning and Unsupervised Fea-
ture Learning Workshop .R. Socher, E. H. Huang, J. Pennington, A. Y . Ng, and
C. D. Manning. 2011a. Dynamic Pooling and Unfold-
ing Recursive Autoencoders for Paraphrase Detection.
InNIPS .
R. Socher, C. Lin, A. Y . Ng, and C.D. Manning. 2011b.
Parsing Natural Scenes and Natural Language with
Recursive Neural Networks. In ICML .
R. Socher, J. Pennington, E. H. Huang, A. Y . Ng, and
C. D. Manning. 2011c. Semi-Supervised Recursive
Autoencoders for Predicting Sentiment Distributions.
InEMNLP .
R. Socher, B. Huval, C. D. Manning, and A. Y . Ng.
2012. Semantic Compositionality Through Recursive
Matrix-Vector Spaces. In EMNLP .
R. Socher, J. Bauer, C. D. Manning, and A. Y . Ng. 2013a.
Parsing With Compositional Vector Grammars. In
ACL.
R. Socher, M. Ganjoo, C. D. Manning, and A. Y . Ng.
2013b. Zero-Shot Learning Through Cross-Modal
Transfer. In NIPS .
R. Socher, M. Ganjoo, H. Sridhar, O. Bastani, and
A. Y . Ng. C. D. Manning and. 2013c. Zero-shot learn-
ing through cross-modal transfer. In Proceedings of
the International Conference on Learning Representa-
tions (ICLR, Workshop Track) .
N. Srivastava and R. Salakhutdinov. 2012. Multimodal
learning with deep boltzmann machines. In NIPS .
A. Torralba, K. P. Murphy, and W. T. Freeman. 2010.
Using the forest to see the trees: exploiting context for
visual object detection and localization. Communica-
tions of the ACM .
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artiﬁcial Intelligence Research , 37:141–188.
B. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu. 2010.
I2t:image parsing to text description. IEEE Xplore .Chameleon: Mixed-Modal Early-Fusion Foundation
Models
Chameleon Team1,∗
1FAIR at Meta
∗See Contributions section for full author list.
We present Chameleon, a family of early-fusion token-based mixed-modal models capable of under-
standing and generating images and text in any arbitrary sequence. We outline a stable training
approach from inception, an alignment recipe, and an architectural parameterization tailored for the
early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range
of tasks, including visual question answering, image captioning, text generation, image generation, and
long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including
state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while
being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image
generation, all in a single model. It also matches or exceeds the performance of much larger models,
including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal
generation evaluation, where either the prompt or outputs contain mixed sequences of both images and
text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.
Date:May 17, 2024
1 Introduction
Recent multimodal foundation models are very widely adopted but still model different modalities separately,
often using modality specific encoders or decoders. This can limit their ability to integrate information across
modalities and generate multimodal documents that can contain arbitrary sequences of images and text. In
this paper, we present Chameleon , a family of mixed-modal foundation models capable of generating and
reasoning with mixed sequences of arbitrarily interleaved textual and image content (Figures 2-4). This
allows for full multimodal document modeling, which is a direct generalization of standard multimodal tasks
such as image generation, understanding and reasoning over images, and text-only LLMs. Chameleon is
instead designed to be mixed-model from inception and uses a uniform architecture trained from scratch in an
end-to-end fashion on an interleaved mixture of all modalities, i.e., images, text, and code.
Our unified approach uses fully token-based representations for both image and textual modalities (Figure 1).
By quantizing images into discrete tokens, analogous to words in text, we can apply the same transformer
architecture to sequences of both image and text tokens, without the need for separate image/text encoders
(Alayrac et al., 2022; Liu et al., 2023b; Laurençon et al., 2023) or domain-specific decoders (Ramesh et al.,
2022; Jin et al., 2023; Betker et al., 2023). This early-fusion approach, where all modalities are projected into
a shared representational space from the start, allows for seamless reasoning and generation across modalities.
However, it also presents significant technical challenges, particularly in terms of optimization stability and
scaling.
We address these challenges through a combination of architectural innovations and training techniques. We
introduce novel modifications to the transformer architecture, such as query-key normalization and revised
placement of layer norms, which we find to be crucial for stable training in the mixed-modal setting (Section
2.3). We further show how to adapt the supervised finetuning approaches used for text-only LLMs to the
mixed-modal setting, enabling strong alignment at scale (Section 3). Using these techniques, we successfully
train Chameleon-34B on 5x the number of tokens as Llama-2 – enabling new mixed-modal applications while
still matching or even outperforming existing LLMs on unimodal benchmarks.
1arXiv:2405.09818v1  [cs.CL]  16 May 2024Mixed-Modal Auto-Regressive LM 
TEXT PROMPT “What can I bake 
with this?” TEXT OUTPUT “Here is a recipe for 
banana bread.” 
Image Tokenizer Image De-Tokenizer 
IMAGE PROMPT 
IMAGE OUTPUT 
Start 
ImageStart 
Image End
Image
End
ImageStart 
Image 
Start 
ImageEnd
Image
Mixed Modal Auto-Regressive LM 
(a) Mixed-Modal Pre-Training (b) Mixed-Modal Generation Figure 1 Chameleon represents all modalities — images, text, and code, as discrete tokens and uses a uniform
transformer-based architecture that is trained from scratch in an end-to-end fashion on ∼10T tokens of interleaved
mixed-modal data. As a result, Chameleon can both reason over, as well as generate, arbitrary mixed-modal documents.
Text tokens are represented in green and image tokens are represented in blue.
Extensive evaluations demonstrate that Chameleon is a broadly capable model on a diverse set of tasks.
On visual question answering and image captioning benchmarks, Chameleon-34B achieves state-of-the-art
performance, outperforming models like Flamingo, IDEFICS and Llava-1.5 (Section 5.2). At the same time,
it maintains competitive performance on text-only benchmarks, matching models like Mixtral 8x7B and
Gemini-Pro on commonsense reasoning and reading comprehension tasks (Section 5.1). But perhaps most
impressively, Chameleon unlocks entirely new capabilities in terms of mixed-modal reasoning and generation.
As using only static, public benchmarks to evaluate model performance could be limited (Schaeffer, 2023), we
also conduct a carefully designed human evaluation experiment by measuring the quality of mixed-modal
long form responses to open-ended prompts. Chameleon-34B substantially outperforms strong baselines like
Gemini-Pro and GPT-4V (Section 4), achieving a 60.4% preference rate against Gemini-Pro and a 51.6%
preference rate against GPT-4V in pairwise comparisons.
In summary, we present the following contributions:
•We present Chameleon , a family of early-fusion token-based mixed-modal models capable of reasoning
over and generating interleaved image-text documents, setting a new bar for open multimodal foundation
models.
•We introduce architectural innovations and training techniques that enable the stable and scalable
training of early-fusion token-based models, addressing key challenges in mixed-modal learning.
•Through extensive evaluations, we demonstrate state-of-the-art performance across a diverse set of
vision-language benchmarks, while maintaining competitive performance on text-only tasks, and high
quality image generation, all in the same model.
•We conduct the first large-scale human evaluation on open-ended mixed-modal reasoning and generation,
demonstrating the unique capabilities of Chameleon in this new setting.
Chameleon represents a significant step towards realizing the vision of unified foundation models capable of
flexibly reasoning over and generating multimodal content.
2Prompt: I’m bored. Could you show me some cool, quirky-looking birds? Also tell me about them in 2–3
sentences.
Response: Absolutely! Here are three birds with some unique features that
you might find interesting:
Keel-Billed Toucan The Keel-Billed Toucan is a brightly colored bird
native to South American rainforests and woodlands. Its most distinctive
feature is its colorful beak, which can grow up to seven inches long and has a
distinctive, curved shape. The beak is used for plucking fruit from trees and
for social displays. Despite its large size, the Keel-Billed Toucan is a skilled
flyer and can often be seen flitting through the treetops. Here is an image of
this vibrant bird: <img>
PuffinPuffins are small, aquatic birds that are found in the northern hemisphere.
They have distinctive black and white feathers, as well as a brightly colored
beak that is used for attracting mates and defending their territory. Puffins
are known for their distinctive "puffin" call, which sounds like a loud, guttural
chuckle. They feed on fish, which they catch by diving into the water from the
air. Puffins are colonial birds, and they breed in large colonies on rocky cliffs
and islands. Here is an image of this adorable bird: <img>
Golden Pheasant The Golden Pheasant is a brightly colored bird native to the
forests of China. It is a member of the pheasant family and is known for its
striking plumage, which features a golden-yellow body, red face, and green tail
feathers. The Golden Pheasant is a ground-dwelling bird, and it feeds on a diet
of insects, seeds, and fruits. It is a popular game bird, and is often hunted for
its meat and feathers. In captivity, the Golden Pheasant is sometimes kept as
an ornamental bird, due to its striking appearance. Here is an image of this
beautiful bird: <img>
I hope these images and descriptions have helped to brighten your day! If you have any other questions
or requests, please don’t hesitate to ask.
Figure 2 Sample interleaved image and text generation from Chameleon. The corresponding images are generated in
locations marked by <img>.
32 Pre-Training
Chameleon represents images, in addition to text, as a series of discrete tokens and takes advantage of the
scaling properties of auto-regressive Transformers (Ramesh et al., 2021; Aghajanyan et al., 2022, 2023; Yu
et al., 2023). We present any ordering of images and text during training ranging from text-only, to single
text/image pairs to full interleaved text-image documents.
Prompt: <img> While chameleons are masters of camouflage in the natural
world, let’s see how they might fare in a man-made setting.
1. Analyze the colors, patterns, and textures in this image and identify the
most challenging spot for a chameleon to blend in.
2. Then generate an image of a chameleon with the optimal pattern to
camouflage itself.
3. Then, rate and explain how successful each attempt is on a scale from 1-10.
Response: The image is of a stop sign in a forested area, with some sunlight filtering through the trees.
The sign has a red octagonal shape with a white border and the word "STOP" written in white letters.
In the background, there are green leaves and trees.
Challenging Spot for a Chameleon: The stop sign would be the most challenging spot for a chameleon
to blend in due to its bright red color and bold white letters, which would stand out in the forested
environment.
Optimal Pattern for Camouflage: To camouflage itself on the stop sign, a chameleon would need
to have a pattern that would break up its outline and make it less conspicuous. A pattern with shades
of brown and red would help the chameleon blend in more effectively.
Additionally, a mottled or dappled pattern would help to scatter
light and further reduce the chameleon’s visibility.
<img>
Rating and Explanation: A rating of 2/10 would be given for
the chameleon’s ability to blend in on the stop sign. While the
chameleon’s green and brown pattern helps to break up its outline,
the bright red color of the stop sign will still make it stand out in
the forested environment.
Figure 3 Sample generation from Chameleon.
2.1 Tokenization
Image Tokenization We train a new image tokenizer based on Gafni et al. (2022), which encodes a 512×512
image into 1024discrete tokens from a codebook of size 8192. For training this tokenizer, we use only licensed
images. Given the importance of generating human faces, we up-sample the percentage of images with faces
during pre-training by 2 times. A core weakness of our tokenizer is in reconstructing images with a large
amount of text, therefore upper bounding the capability of our models, when it comes to heavy OCR-related
tasks.
Tokenizer We train a new BPE tokenizer (Sennrich et al., 2016) over a subset of the training data outlined
belowwithavocabularysizeof65,536, whichincludesthe8192imagecodebooktokens, usingthe sentencepiece
library (Kudo and Richardson, 2018).
4Prompt: What can I bake with this? Give me the full recipe and show me an
image with the final dish. <img>
Response: Bananas are a versatile fruit that can be used in various baked
goods, such as muffins, bread, cakes, and cookies. They add natural sweetness,
moisture, and a subtle banana flavor to the finished product. Additionally,
bananas are rich in potassium, fiber, and other nutrients, making them a
healthy addition to your baking repertoire.
Here is a simple recipe for banana bread that uses 3 ripe bananas:
Ingredients:
•3 ripe bananas, mashed
•1 1/2 cups all-purpose flour
•1 tsp baking powder
•1/2 tsp baking soda
•1/2 tsp salt
•1/4 cup unsalted butter, softened
•1 cup granulated sugar
•2 large eggs
•1 tsp vanilla extract
•1/2 cup chopped walnuts (optional)
Instructions:
•Preheat your oven to 350 °F (180 °C). Grease a 9x5-inch loaf pan and set it aside.
•In a medium bowl, whisk together the flour, baking powder, baking soda, and salt. Set aside.
•In a large mixing bowl, use an electric mixer to cream together the butter and sugar until light and
fluffy, about 2-3 minutes.
•Beat in the eggs one at a time, followed by the mashed bananas and vanilla extract.
•Gradually mix in the dry ingredients until just combined, being careful
not to overmix.
•If using walnuts, fold them into the batter.
•Pour the batter into the prepared loaf pan and smooth the top.
•Bake for 50-60 minutes, or until a toothpick inserted into the center of
the bread comes out clean.
•Remove the bread from the oven and let it cool in the pan for 10 minutes
before transferring it to a wire rack to cool completely. <img>
Figure 4 Sample Chameleon outputs.
2.2 Pre-Training Data
We delineate the pre-training stage into two separate stages. The first stage takes up the first 80% of training
while the second stage takes the last 20%. For all Text-To-Image pairs we rotate so that 50% of the time the
image comes before the text (i.e., captioning).
2.2.1 First Stage
In the first stage we use a data mixture consisting of the following very large scale completely unsupervised
datasets.
Text-Only: We use a variety of textual datasets, including a combination of the pre-training data used to
train LLaMa-2 (Touvron et al., 2023) and CodeLLaMa (Roziere et al., 2023) for a total of 2.9 trillion text-only
tokens.
50k 5k 10k 15k 20k 25k 30k
Step0.05.010.015.020.025.030.035.0Output Normw/ QK-norm and dropout
w/o dropout
w/o QK-norm or dropout(a)Uncontrolled growth of output
norms is a strong indicator of future
training divergence.
0k 25k 50k 75k 100k 125k 150k 175k
Step3.43.53.53.63.63.73.7Training Lossw/o QK-norm
w/ QK-norm(b)An ablation with Chameleon -7B
with and without QK-Norm .
0k 20k 40k 60k 80k
Step3.43.53.63.73.83.9Training Lossw/o dropout
w/ dropout(c)An ablation with Chameleon -7B
with and without dropout.
Figure 5 Output norm and training loss curves for Chameleon models under various settings.
Text-Image: The text-image data for pre-training is a combination of publicly available data sources and
licensed data. The images are then resized and center cropped into 512×512images for tokenization. In
total, we include 1.4 billion text-image pairs, which produces 1.5 trillion text-image tokens.
Text/Image Interleaved: We procure data from publicly available web sources, not including data from Meta’s
products or services, for a total of 400 billion tokens of interleaved text and image data similar to Laurençon
et al. (2023). We apply the same filtering for images, as was applied in Text-To-Image .
2.2.2 Second Stage
In the second stage, we lower the weight of the first stage data by 50% and mix in higher quality datasets
while maintaining a similar proportion of image text tokens.
We additionally include a filtered subset of the train sets from a large collection of instruction tuning sets.
2.3 Stability
It was challenging to maintain stable training when scaling the Chameleon models above 8B parameters
and 1T tokens, with instabilities often only arising very late in training. We adopted to following recipe for
architecture and optimization to achieve stability.
Architecture Our architecture largely follows LLaMa-2 (Touvron et al., 2023). For normalization, we continue
to use RMSNorm (Zhang and Sennrich, 2019); we use the SwiGLU (Shazeer, 2020) activation function and
rotary positional embeddings (RoPE) (Su et al., 2021).
We found that the standard LLaMa architecture showed complex divergences due to slow norm growth in the
mid-to-late stages of training. We narrowed down the cause of the divergence to the softmax operation being
problematic when training with multiple modalities of significantly varying entropy due to the translation
invariant property of softmax (i.e., softmax (z) =softmax (z+c)). Because we share all weights of the model
across modalities, each modality will try to “compete” with the other by increasing its norms slightly; while
not problematic at the beginning of training, it manifests in divergences once we get outside the effective
representation range of bf16 (In Figure 6b, we show that ablations without image generation did not diverge).
In a unimodal setting, this problem has also been named the logit drift problem (Wortsman et al., 2023). In
Figure 5a, we plot the norms of the output of the last transformer layer as training progresses and we find
that although training divergences can manifest after as much as even 20-30% of training progress, monitoring
uncontrolled growth of output norms is strongly correlated with predicting future loss divergence.
The softmax operation appears in two places in transformers: the core attention mechanism and the softmax
over the logits. As inspired by Dehghani et al. (2023) and Wortsman et al. (2023), we first deviate from
the Llama architecture by using query-key normalization (QK-Norm). QK-Norm directly controls the norm
growth of input to the softmax by applying layer norm to the query and key vectors within the attention.
60k 100k 200k 300k 400k 500k 600k
Step2.82.93.03.13.2Training Loss7b
34b(a)Training Curves for 600k steps for
Chameleon-7B and Chameleon-34B
over Mixed-Modal Data.
0k 50k 100k 150k 200k 250k
Step0.951.001.051.101.15Training Loss7B w/o image generation(b)Training loss curve with image gen-
eration disabled does not suffer from
instability issues.
0k 2k 4k 6k 8k 10k
Step3.54.04.55.05.56.0Training Lossw/o norm reordering
w/ norm reordering(c)For Chameleon-34B , using
dropout does not fix divergences,
both with and without norm-
reordering.
Figure 6 Training loss curves for Chameleon models under various settings.
In Figure 5b, we show training loss curves for Chameleon-7B with and without QK-Norm, and the latter
diverges after approximately 20% of a training epoch.
We found that to stabilize Chameleon-7B by controlling norm growth, it was necessary to introduce dropout
after the attention and feed-forward layers, in addition to QK-norm (see Figure 5c). However, this recipe was
not enough to stabilitize, Chameleon-34B , which required an additional re-ordering of the norms. Specifically,
we use the strategy of normalization proposed in Liu et al. (2021), within the transformer block. The benefit
of the Swin transformer normalization strategy is that it bounds the norm growth of the feedforward block,
which can become additionally problematic given the multiplicate nature of the SwiGLU activation function.
Ifhrepresents the hidden vector at time-step tafter self-attention is applied to input x,
Chameleon-34B: h=x+attention_norm (attention (x))
output =h+ffn_norm (feed_forward (h))
Llama2: h=x+attention (attention_norm (x))
output =h+feed_forward (ffn_norm (h))
There was no difference in perplexity when training a model from scratch with and without the normalization
re-ordering until the divergence of the LLaMa-2 parameterization. Additionally, we found that this type of
normalization did not work well in combination with dropout and therefore, we train Chameleon-34B without
dropout (Figure 6c). Furthermore, we retroactively found that Chameleon-7B can also be stably trained
without dropout, when using norm-reordering, but QK-norm is essential in both cases. We plot training
curves for the first 600k steps for both Chameleon-7B andChameleon-34B in Figure 6a.
Optimization Our training process uses the AdamW optimizer (Loshchilov and Hutter, 2017), with β1set
to 0.9 and β2to 0.95, with an ϵ= 10−5. We use a linear warm-up of 4000 steps with an exponential decay
schedule of the learning rate to 0. Additionally, we apply a weight decay of 0.1 and global gradient clipping at
a threshold of 1.0. We use a dropout of 0.1 (Srivastava et al., 2014) for Chameleon-7B for training stability,
but not for Chameleon-34B (see Figure 5c and 6c).
The application of QK-Norm while helping the inner softmaxes within the Transformer does not solve the
problem of logit shift in the final softmax. Following Chowdhery et al. (2022); Wortsman et al. (2023),
we apply z-lossregularization. Specifically, we regularize the partition function Zof the softmax function
σ(x)i=exi
Zwhere Z=P
iexiby adding 10−5log2Zto our loss function.
ForChameleon -7B it was important to use both dropout and z-lossto achieve stability, while Chameleon -34B
only required z-loss(Figure 6c).
Chameleon-7B was trained with a global batch size of 223(∼8M) tokens and Chameleon-34B was trained
with a global batch size of 3×222(∼12M) tokens. We do 2.1 epochs over our full training dataset for a total
7of 9.2 trillion tokens seen during training. We show the first 600k steps of training (55% for Chameleon-7B
and 80% for Chameleon-34B ) in Figure 6a.
Table 1Summary of core architecture and optimization decisions made in Chameleon in contrast to LLaMa-1 and
LLaMa-2.
Model Params Context Length GQA Tokens LR Epochs Dropout Zloss Qknorm
LLaMa-1 7B 2k ×1.0T 3.0×10−41.0 0.0 0.0 ×
33B 2k ×1.4T 1.5×10−41.0 0.0 0.0 ×
LLaMa-2 7B 4k ×2.0T 3.0×10−41.0 0.0 0.0 ×
34B 4k ✓2.0T 1.5×10−41.0 0.0 0.0 ×
Chameleon 7B 4k ×4.4T 1.0×10−42.1 0.1 10−5✓
34B 4k ✓4.4T 1.0×10−42.1 0.0 10−5✓
Pre-Training Hardware Our model pretraining was conducted on Meta’s Research Super Cluster (RSC) (Lee
and Sengupta, 2022), and our alignment was done on other internal research clusters. NVIDIA A100 80
GB GPUs power both environments. The primary distinction is the interconnect technology: RSC employs
NVIDIA Quantum InfiniBand, whereas our research cluster utilizes Elastic Fabric. We report our GPU usage
for pre-training in Table 2.
Table 2 Chameleon Model Pre-Training Resource Usage
Chameleon Concurrent GPUs GPU Hours
7B 1024 856481
34B 3072 4282407
2.4 Inference
To support alignment and evaluation, both automated and human, and to demonstrate the application-
readiness of our approach, we augment the inference strategy with respect to interleaved generation to improve
throughput and reduce latency.
Autoregressive, mixed-modal generation introduces unique performance-related challenges at inference time.
These include:
•Data-dependencies per-step — given that our decoding formulation changes depending on whether the
model is generating images or text at a particular step, tokens must be inspected at each step (i.e.
copied from the GPU to the CPU in a blocking fashion) to guide control flow.
•Masking for modality-constrained generation — to facilitate exclusive generation for a particular modality
(e.g. image-only generation), tokens that do not fall in a particular modality space must be masked and
ignored when de-tokenizing.
•Fixed-sized text units — unlike text-only generation, which is inherently variable-length, token-based
image generation produces fixed-size blocks of tokens corresponding to an image.
Given these unique challenges, we built a standalone inference pipeline based on PyTorch (Paszke et al., 2019)
supported with GPU kernels from xformers (Lefaudeux et al., 2022).
Our inference implementation supports streaming for both text and images. When generating in a streaming
fashion, token-dependent conditional logic is needed at each generation step. Without streaming, however,
blocks of image tokens can be generated in a fused fashion without conditional computation. In all cases,
token masking removes branching on the GPU. Even in the non-streaming setting, however, while generating
text, each output token must be inspected for image-start tokens to condition image-specific decoding
augmentations.
8Table 3Supervised Fine-Tuning Dataset Statistics
Category # of Samples # of Tokens # of Images
Chameleon-SFTText 1.6M 940.0M -
Code 14.1K 1.1M -
Visual Chat 15.6K 19.4M 16.7K
Image Generation 64.3K 68.0M 64.3K
Interleaved Generation 16.9K 35.8M 30.7K
Safety 95.3K 38.6M 1.6K
3 Alignment
We follow recent work in using a light weight alignment stage based on supervised fine tuning on carefully
curated high quality datasets (Zhou et al., 2023). We include a range of different types of data, targeting
both exposing model capabilities and improving safety.
3.1 Data
We separate our supervised fine-tuning (SFT) dataset into the following categories: Text,Code,Visual
Chat,Image Generation ,Interleaved Text/Image Generation , and Safety. We include examples from each
category from the Chameleon-SFT dataset in Figure 7.
We inherit the TextSFT dataset from LLaMa-2 (Touvron et al., 2023) and the CodeSFT from CodeLLaMa
(Roziere et al., 2023). For the Image Generation SFT dataset, we curate highly aesthetic images by applying
and filtering each image in our licensed data, with an aesthetic classifier from Schuhmann et al. (2022). We
first select images rated as at least six from the aesthetic classifier and then select the top 64K images closest
in size and aspect ratio to 512×512(the native resolution of our image tokenizer).
For both Visual Chat andInterleaved Text/Image Generation SFT data, we focused on very high-quality
data collection using third-party vendors following a similar strategy recommended by Touvron et al. (2023);
Zhou et al. (2023). We do not include any Meta user data. We present our dataset’s statistics in Table 3.
Safety Data We include a collection of prompts that can potentially provoke the model to produce unsafe
content, and match them with a refusal response (e.g. “I can’t help with that.”). These prompts cover a
wide variety of sensitive topics, such as violence, controlled substances, privacy, and sexual content. Our
collection of safety tuning data includes examples from LLaMa-2-Chat (Touvron et al., 2023), synthetic
text-based examples generated with Rainbow Teaming (Samvelyan et al., 2024), image generation prompts
manually selected from Pick-A-Pic (Kirstain et al., 2023) for safety testing, examples for cyber security
safety (Roziere et al., 2023), as well as mixed-modal prompts collected internally through manual annotation
and automatic expansion (Honovich et al., 2022). Collecting mixed-modal prompts is of particular importance,
since it addresses potential multi-modal attack vectors, which are outside the distribution of text-only and
text-to-image safety tuning datasets.
3.2 Fine-Tuning Strategy
Data Balancing We found that balancing modalities within the SFT stage is important for high quality
alignment. Specifically during the SFT stage, if there is a severe imbalance between pairings of modalities (or
when a specific modality should trigger), the model learns an unconditional prior of generating that modality
which can either mute or over exaggerate the generation of a single modality.
Optimization Our supervised fine-tuning strategy incorporates a cosine learning rate schedule, starting at an
initial rate of 1e-5, combined with a weight decay set at 0.1. We maintain a batch size of 128, accommodating
sequences up to 4096 tokens. During fine-tuning, each dataset instance comprises a paired prompt and its
corresponding answer. To improve efficiency, we pack as many prompts and answers as possible into each
9Prompt: Can you write a poem using
rhyming couplets?
Response: Sure, here’s a short poem
using rhyming couplets:
The sun sets in the sky,
A beautiful sight to the eye.
The stars come out to play,
A magical sight to behold.
The wind whispers through the
trees,
A gentle breeze that brings ease.
The birds sing their sweet melodies,
A soothing sound that calms the soul.
The world is full of wonder,
A place of beauty and thunder.
So let us cherish every moment,
And bask in the beauty all around us.
(a)TextPrompt: Given a list of integers,
find the first number that ap-
pears at least twice.
Response:
def find_duplicate(my_list):
seen = set()
for num in my_list:
if num in seen:
return num
seen.add(num)
return None
(b)CodePrompt: Renaissance square of
Stepanakert
Response:
(c)Image Generation
Prompt: Which elements contribute to this flower
arrangement’s distinctiveness, and what sets its sur-
roundings apart?
Response: The image features an autumnal-themed
floral arrangement in a classically designed vase.
The arrangement stands out with its striking com-
bination of red maple leaves, purple flowers, and
pink-red roses. The vase’s rosettes give it an antique
design. In the background, blurry green vegetation
is visible, creating a contrast to the autumnal theme
of the bouquet. The red maple leaves, both in the
arrangement and scattered around the vase, evoke
autumn, while the green vegetation suggests a more
summer-like setting. The outdoor setting is bright
and sunny, adding to the contrast between the ar-
rangement and its settings.
(d)Visual ChatGyumri is an urban municipal community and the
second-largest city in Armenia, serving as the ad-
ministrative center of Shirak Province in the north-
western part of the country.
Archaeological excavations conducted throughout
the Soviet period have shown that the area of
modern-day Gyumri has been populated since at
least the third millennium BC.
(e)Interleaved Text/Image Generation
Figure 7 Example alignment data for different categories.
sequence, inserting a distinct token to delineate the end of a prompt and the beginning of an answer. We
use an autoregressive training objective, selectively masking the loss for the prompt tokens. This targeted
approach allows us to optimize the model exclusively based on the answer tokens, which provides slight gains
overall. We also apply a dropout of 0.05. Additionally, we maintain the same zloss that was used during
10Advice: 10.2% 
How-to: 12.5% 
How do I properly clean my TV 
screen? I used Windex and now 
there are towel fibers and wipe 
marks all over. Show me some 
reference photos. 
Explanation: 14.4% 
I've been studying 
classical French art, 
and my favorite 
so far is his painting 
seen here: <img>  
Could you please 
give me a few images of other 
contemporary artworks that have 
this same aesthetic? 
Hypothetical: 5.6% 
What would the modern-day 
vehicle look like if oil had never 
been discovered? Brainstorming: 18.6% 
Show me a Middle Eastern alternative to 
these dishes. <img1> <img2> 
Reasoning: 2.1% Identification: 9.3 % 
Is the below image a 
Shetland Pony? If 
not, what is it, and 
can you show me a 
Shetland Pony? 
<img> 
Article: 3.1% 
Write me an introduction to a story about 
knick-knacks, and finish the story by 
shifting the focus with an image. Report: 5.4% 
Who designed the church in the image below, 
and what's the name of the 
Church? <img> Can you 
please provide me with 
additional photos of famous 
landmarks designed 
by the same architect? Story: 3.9% 
Can you create and illustrate a short story 
for children about an octopus that can't 
stop eating pizza? 
Other: 5.2% 
Create a decal for my truck that features 
running horses as well as the TRD insignia. Use 
black to gray gradients. Comparison: 9.6% 
Please tell me what the difference between 
these two creatures is, and show me some 
more examples. <img1> <img2> 
What does a meningitis rash look 
like? What are the other 
symptoms I should be on the 
lookout for? 
What is typically found at a construction site? 
Show me a construction site that has a crane. 
Figure 8 Task categories and examples of prompts. Image attributions: Seguin (2010); Agriflanders (2009); Tuszyński
(2015); Sokolov (2022).
pre-training. During supervised fine-tuning, images in the prompt are resized with border padding to ensure
that all the information is available in the image, whereas images in the answer are center-cropped to ensure
visually good image generation quality.
4 Human Evaluations and Safety Testing
Chameleon has significant new mixed modal understanding and generation abilities that cannot be measured
with existing benchmarks. In this section, we detail how we conduct human evaluations on large multi-modal
language models’ responses to a set of diverse prompts that regular users may ask daily. We first introduce
how we collect the prompts and then describe our baselines and evaluation methods, along with the evaluation
results and analysis. A safety study is also included in this section.
4.1 Prompts for Evaluation
We work with a third-party crowdsourcing vendor to collect a set of diverse and natural prompts from human
annotators. Specifically, we ask annotators to creatively think about what they want a multi-modal model
to generate for different real-life scenarios. For example, for the scenario of “imagine you are in a kitchen”,
annotators may come up with prompts like “How to cook pasta?” or “How should I design the layout of my
island? Show me some examples.” The prompts can be text-only or text with some images, and the expected
responses should be mixed-modal, containing both text and images.
After collecting an initial set of prompts, we ask three random annotators to evaluate whether the prompts
are clear and whether they expect the responses to contain images. We use a majority vote to filter unclear
prompts and prompts that don’t expect mixed-modal responses. In the end, our final evaluation set contains
1,048 prompts: 441 (42.1%) are mixed-modal (i.e., containing both text and images), and the remaining 607
(57.9%) are text-only.
To better understand the tasks users would like a multi-modal AI system to fulfill, we manually examine
11Fulfills Partially fulfills Does not fulfill
T ask Fulfillment Rate010203040506070Percentage (%)Model
Chameleon
Gemini+
GPT-4V+
Gemini
GPT-4V(a)The prompt task fulfillment rates.
0 20 40 60 80 100
Percent (%)GPT-4VGeminiGPT-4V+Gemini+
46.053.535.841.5
31.431.231.634.5
22.615.332.624.0Wins Ties Loses (b)Chameleon vs. the baselines: Gemini+, GPT-4V+,
Gemini, GPT-4V.
Figure 9 Performance of Chameleon vs baselines, on mixed-modal understanding and generation on a set of diverse
and natural prompts from human annotators.
the prompts and classify them into 12 categories. The description of these task categories1, as well as their
example prompts, can be found in Figure 8.
4.2 Baselines and Evaluations
We compare Chameleon 34B with OpenAI GPT-4V and Google Gemini Pro by calling their APIs. While these
models can take mixed-modal prompts as input, their responses are text-only. We create additional baselines
by augmenting GPT-4V and Gemini responses with images to have even stronger baselines. Specifically, we
instruct these models to generate image captions by adding the following sentence at the end of each original
input prompt: “If the question requires an image to be generated, then generate an image caption instead
and enclose the caption in a pair of ⟨caption ⟩ ⟨/caption ⟩tags.” We then use OpenAI DALL-E 3 to generate
images conditioned on these captions and replace the captions in the original responses with those generated
images. We denote the enhanced responses as GPT-4V+ and Gemini+ in this section. Working with the same
third-party crowdsourcing vendor, we conduct two types of evaluations to measure the model performance:
absolute andrelative.
4.2.1 Absolute Evaluation
For absolute evaluations, the output of each model is judged separately by asking three different annotators
a set of questions regarding the relevance and quality of the responses. Below, we give detailed results and
analysis on the most critical question, whether the response fulfills the task described in the prompt .
On task fulfillment, we ask annotators whether the response fulfills,partially fulfills , ordoes not fulfill the
task described in the prompt. As shown in Figure 9a, much more of Chameleon ’s responses are considered
to have completely fulfilled the tasks: 55.2% for Chameleon vs. 37.6% of Gemini+ and 44.7% of GPT-4V+.
When judging the original responses of Gemini and GPT-4V, the annotators consider much fewer prompts to
be fully fulfilled: Gemini completely fulfills 17.6% of the tasks and GPT-4V 23.1%. We suspect that because
all the prompts expect mixed-modal output, the text-only responses from Gemini and GPT-4V might be
viewed as only partially completing the tasks by the annotators.
The task fulfillment rates in each category and in each input modality can be found in Appendix B. The
task categories that Chameleon performs well include Brainstorming ,Comparison , andHypothetical , and the
1While not instructed specifically, certain image understanding tasks that require identifying the text in an image, such as
OCR (Optical character recognition), do not appear in our evaluation set of prompts.
12categories Chameleon needs to improve include Identification andReasoning . On the other hand, we don’t
see that the model performance differs a lot when comparing mixed-modality and text-only prompts, although
Chameleon seems to perform slightly better on text-only prompts, while Gemini+ and GPT-4V+ are slightly
better on mixed-modal ones. Figure 2 shows an example of Chameleon ’s response to a brainstorming prompt.
4.2.2 Relative Evaluation
For relative evaluations, we directly compare Chameleon with each baseline model by presenting their
responses to the same prompt in random order and asking human annotators which response they prefer. The
options include the firstresponse, the secondresponse, and about the same . Figure 9b shows Chameleon ’s
win rates2over the baselines. Compared with Gemini+, Chameleon ’s responses are better in 41.5% of the
cases, 34.5% are tie, and 24.0% are inferior. Annotators also think that Chameleon ’s responses are slightly
more often better than GPT-4V+, with 35.8% win, 31.6% tie, and 32.6% loss. Overall, Chameleon has win
rates of 60.4% and 51.6% over Gemini+ and GPT-4V+, respectively. When compared with the original
responses from Gemini without the augmented images, Chameleon ’s responses are considered better in 53.5%
of the cases, 31.2% are tied, and 15.3% are inferior. Chameleon ’s responses are also considered better than
GPT-4V more frequently, with 46.0% win, 31.4% tie, and 22.6% loss. Chameleon ’s win rates over Gemini
and GPT-4V are 69.1% and 61.7%, respectively.
4.3 Inter-annotator Agreement
Every question in our evaluation is answered by three different human annotators, and we take the majority
votes as the final answer. To understand the quality of the human annotators and whether the questions we
asked are reasonably designed, we examine the level of agreement between different annotators.
The levels of agreement on each question in the absolute evaluation are shown in Figure 10.
0
5001000 1500 2000 2500 3000
CountContaining images
Image quality
Image relevance
Language quality
Objectionable content
Relevance
T ask fulfillment
AccuracyAgreement
All
T wo
None
Figure 10 The inter-annotator agreement on the questions in the absolute evaluation.
For questions about simple, objective properties of the responses, we very rarely see three annotators disagree
with each other. For example, annotators have unanimous judgments on whether the model responses contain
objectionable content (e.g., hate speech); in this case, all models produce safe responses. For some questions,
such as whether the response fulfills the task or whether the model interprets the prompt correctly, when one
2The win rate is calculated by adding 1 point for a win and 0.5 points for a tie.
13annotator’s judgment differs from the other two’s, the decision is usually still close (e.g., fulfillsvs.partially
fulfills) rather than opposite (e.g., fulfillsvs.does not fulfill ).3
Table 4The inter-annotator agreement on relative evaluations.
All 3 annotators agree 2 of 3 annotators agree No Agreement
Chameleon vs. Gemini+ 331 (31.5%) 609 (58.1%) 108 (10.3%)
Chameleon vs. GPT-4V+ 371 (35.4%) 579 (55.2%) 98 (9.3%)
Chameleon vs. Gemini 317 (30.2%) 621 (59.3%) 110 (10.5%)
Chameleon vs. GPT-4V 300 (28.6%) 611 (58.3%) 137 (13.1%)
For the relative evaluation, Table 4 shows the numbers of cases where all three annotators agree, two annotators
agree, and there is no agreement. For each model pair, we have a bit higher than 10% of the cases where there
is no agreement among the three annotators (considered as a tie in our evaluation.) On about 28% to 35% of
the pairs, all annotators have unanimous judgments, and in about 55% to 60% of the pairs, one annotator
differs from other two. This may be interpreted as Chameleon performing similarly to other baselines in
many cases, making the relative evaluation challenging.4
4.4 Safety Testing
We crowdsource prompts that provoke the model to create unsafe content in predefined categories such as
self-harm, violence and hate, and criminal planning. These prompts cover both text and mixed-modal inputs,
as well as intents to produce unsafe text, images, or mixed-modal outputs. We generate the model’s response
to each prompt, and ask annotators to label whether the response is safeorunsafewith respect to each
category’s definition of safety; an unsureoption is also provided for borderline responses. Table 5 shows
that an overwhelming majority of Chameleon ’s responses are considered safe, with only 78 (0.39%) unsafe
responses for the 7B model and 19 (0.095%) for the 30B model.
We also evaluate the model’s ability to withstand adversarial prompting in an interactive session. For
that purpose, an internal red team probed the 30B model over 445 prompt-response interactions, including
multi-turn interactions. Table 5 shows that of those responses, 7 (1.6%) were considered unsafe and 20 (4.5%)
were labeled as unsure. While further safety tuning using RLHF/RLAIF has been shown to further harden
the model against jailbreaking and intentional malicious attacks, these results demonstrate that our current
safety tuning approach provides significant protection for reasonable, benign usage of this research artifact.
4.5 Discussion
Compared to Gemini and GPT-4V, Chameleon is very competitive when handling prompts that expect
interleaving, mixed-modal responses. The images generated by Chameleon are usually relevant to the context,
making the documents with interleaving text and images very appealing to users. However, readers should
be aware of the limitations of human evaluation. First, the prompts used in the evaluation came from
crowdsourcing instead of real users who interact with a model. While we certainly have a diverse set of
prompts, the coverage may still be limited, given the size of the dataset. Second, partially because our
prompts focus on the mixed-modal output, certain visual understanding tasks, such as OCR or Infographics
(i.e., interpreting a given chart or plot), are naturally excluded from our evaluation. Finally, at this moment,
the APIs of existing multi-modal LLMs provide only textual responses. While we strengthen the baselines by
augmenting their output with separately generated images, it is still preferred if we can compare Chameleon
to other native mixed-modal models.
3For the question of task fulfillment, the inter-rater reliability derived by Krippendorff’s Alpha (Krippendorff, 2018; Marzi
et al., 2024) is 0.338; the 95% confidence interval is [0.319 ,0.356], based on bootstrap sampling of 1,000 iterations.
4When comparing Chameleon with Gemini+ and GPT-4V+, the Krippendorff’s Alpha values are 0.337 [0 .293 ,0.378]and
0.396 [0 .353 ,0.435], respectively.
14Table 5Safety testing on 20,000 crowdsourced prompts and 445 red team interactions provoking the model to produce
unsafe content.
Dataset Params Safe Unsafe Unsure
Crowdsourced7B 99.2% 0.4% 0.4%
34B 99.7% 0.1% 0.2%
Red Team 34B 93.9% 1.6% 4.5%
5 Benchmark Evaluations
Given the general capabilities of Chameleon , there is not a single model that we can directly evaluate against;
therefore, we evaluate against the best models in every category within our capabilities.
5.1 Text
We evaluate the general text-only capabilities of our pre-trained (not SFT’d) model against other state-of-the-
art text-only large language models. We follow the evaluation protocol outlined by Touvron et al. (2023).
Specifically we evaluate all models, using an in-house evaluation platform on the areas of commonsense
reasoning, reading comprehension, math problems, and world knowledge. We report our results in Table 6.
Table 6Comparison of overall performance on collective academic benchmarks against open-source foundational models.
∗Evaluated using our framework/using API. For GSM8k/MATH, we report maj@1 unless mentioned otherwise.
∗∗From Gemini et al. (2023).
Chameleon Llama-2 Mistral Gemini
ProGPT-
4
7B 34B 7B 34B 70B 7B 8x7B — —
Commonsense Reasoning and Reading Comprehension
PIQA 79.6 83.3 78.8 81.9 82.8 83.0 83.6 — —
SIQA 57.0 63.3 48.3 50.9 50.7 — — — —
HellaSwag 74.2 82.7 77.2 83.3 85.3 81.3 84.4 — —
75.6
10-shot85.1
10-shot— — 87.1
10-shot83.9
10-shot86.7
10-shot84.7
10-shot95.3
10-shot
WinoGrande 70.4 78.5 69.2 76.7 80.2 75.3 77.2 — —
Arc-E 76.1 84.1 75.2 79.4 80.2 80.0 83.1 — —
Arc-C 46.5 59.7 45.9 54.5 57.4 55.5 59.7 — —
OBQA 51.0 54.0 58.6 58.2 60.2 — — — —
BoolQ 81.4 86.0 77.4 83.7 85.0 84.7∗— — —
Math and World Knowledge
GSM8k 41.6 61.4 14.6 42.2 56.8 52.1
maj@874.4
maj@886.5
maj@32
CoT92.0
SFT
CoT50.9
maj@877.0
maj@32— — — — 75.1∗
maj@32— —
MATH 11.5
maj@122.5
maj@12.5 6.24 13.5 13.1
maj@428.4
maj@432.6 52.9∗∗
12.9
maj@424.7
maj@4— — — — — — —
MMLU 52.1 65.8 45.3 62.6 68.9 60.1 70.6 71.8 86.4
•Commonsense Reasoning and Reading Comprehension: We report 0-shot performance on the following
benchmarks that measure commonsense reasoning and reading comprehension capabilities: PIQA(Bisk
et al., 2020), SIQA(Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al.,
2021), ARC-Easy (Clark et al., 2018), ARC-Challenge (Clark et al., 2018), OpenBookQA (Mihaylov
et al., 2018), and BoolQ(Clark et al., 2019). We score the prompt with each candidate answer and
compute accuracy using the candidate with the highest score. All baseline model performances except a
few are taken directly from the reported sources. We observe that Chameleon-7B andChameleon-34B
15are competitive with the corresponding Llama-2 models, with Chameleon-34B even outperforming
Llama-2 70B on 5/8tasks and performing on par with Mixtral 8x7B.
•MATH and World Knowledge We report 8-shot performance on GSM8K (Cobbe et al., 2021) i.e., grade
school math word problems and 4-shot performance on the MATH(Hendrycks et al., 2021) benchmark.
We report maj@N exact match accuracy for both benchmarks by sampling N generations from the model
(greedy sampling for N=1) and choosing the answer via majority voting. Despite training for additional
modalities, both Chameleon models demonstrate strong math capabilities. On GSM8k,Chameleon-7B
outperforms the corresponding Llama-2 models, with performance comparable to Mistral 7B (50.9 vs
52.1 maj@8). Furthermore, Chameleon-34B can outperform Llama2-70B on maj@1 (61.4 vs 56.8) and
Mixtral 8x7B on maj@32 (77.0 vs 75.1). Similarly, on MATH, Chameleon-7B outperforms Llama-2
and matches Mistral 7B on maj@4, while Chameleon-34B outperforms Llama2-70B, approaching the
performance of Mixtral 8x7B on maj@4 (24.7 vs 28.4).
We also report performance on MMLU (Hendrycks et al., 2020), which measures world/in-domain
knowledge and problem-solving abilities using 57 subjects, including elementary mathematics, US
history, computer science, and law. Both Chameleon models outperform their Llama-2 counterparts
with Chameleon-34B approaching the performance of Mixtral 8x7B/Gemini-Pro (65.8 vs 70.6/71.8).
Overall, Chameleon outperformsLLaMa-2acrosstheboard, withperformanceapproachingMistral7B/Mixtral
8x7B (Jiang et al., 2023, 2024) on some tasks. These gains are likely due to multiple factors. First, we do
two epochs over the LLaMa-2 pre-training data, and in general use more compute for pretraining. Second,
including code data significantly improves performance on text-only reasoning tasks. Lastly, having higher
quality data in the last 20% of pre-training significantly improves performance.
5.2 Image-To-Text
We next evaluate Chameleon on the segment of tasks that requires text generation conditioned on an image,
specifically on image captioning and visual question-answering tasks, and present results of Chameleon-34B
in Table 7. Together with our pre-trained model, we also present results with a model fine-tuned on all tasks
together ( Chameleon-34B -MultiTask), as well as models exclusively fine-tuned for the specific evaluation
tasks ( Chameleon-34B -SFT).
We evaluate against available open-source late-fusion models: specifically Flamingo 80B (Alayrac et al., 2022),
IDEFICS 80B (Laurençon et al., 2023), and Llava-1.5 (Liu et al., 2023a), as well as recent closed-source
models, such as Gemini (Gemini et al., 2023) and GPT4-V (OpenAI, 2023). We note that we did not take
any special care when formatting the pre-training data to ensure that 0-shot inference can be effectively done.
Therefore, we augment the input images or questions with the published prompts used by other models. This
was purposefully done to maintain the fidelity of the pre-training data.
•Image Captioning: For image captioning evaluations we report CiDER (Vedantam et al., 2015) scores
on the Karpathy test split of MS-COCO (Lin et al., 2014), and the Karpathy test split of Flickr30k
(Plummer et al., 2015) using the pycocoevalcap (Chen et al., 2020) package. For Chameleon models,
we restrict captions to 30tokens. We evaluated GPT-4V and Gemini models using several prompts and
generation lengths via their APIs and report the best performance that we were able to achieve.
In the open-source pre-trained category, Chameleon-34B (2-shot) outperforms the larger 80B models of
both Flamingo and IDEFICS on COCO with 32-shots, while matching their performance on Flickr30k.
With respect to fine-tuned/closed-source models, both multi-task and SFT variants of Chameleon-34B
outperform all other models on COCO, while for Flickr30k, the SFT model outperforms other models
with the multitask model being a close competitor.
•Visual Question Answering: For visual question answering (VQA) we report performance on the test-
dev split of VQA-v2 (Goyal et al., 2017). For VQA-v2, the pre-trained Chameleon-34B model with
2-shots matches the 32-shot performance of the larger Flamingo and IDEFICS models, while for fine-
tuned/closed models, Chameleon-34B -Multitask approaches the performance of IDEFICS-80B-Instruct
and Gemini Pro, but trails larger models such as Flamingo-80B-FT, GPT-4V, and Gemini Ultra.
Llava-1.5 outperforms Chameleon-34B on VQAv2 potentially owing to its additional fine-tuning on
16Table 7Model Performances on Image-to-Text Capabilities.∗Evaluated using API.
Model Model Size COCO Flickr30k VQAv2
Pre-trainedFlamingo-80B 80B 113.8
32-shot75.1
4-shot67.6
32-shot
IDEFICS-80B 80B 116.6
32-shot73.7
4-shot65.9
32-shot
ChameleonChameleon 34B 120.2
2-shot74.7
2-shot66.0
2-shot
Chameleon-SFT 34B 140.8
0-shot82.3
2-shot—
Chameleon-MultiTask 34B 139.1
2-shot76.2
2-shot69.6
Fine-tunedFlamingo-80B-FT 80B 138.1 — 82.0
IDEFICS-80B-Instruct 80B 123.2
32-shot78.4
32-shot68.8
32-shot
Closed Source
(finetuning
status unknown)GPT-4V — 78.5∗
8-shot55.3∗
8-shot77.2
Gemini Nano 2 — — — 67.5
Gemini Pro — 99.8∗
2-shot82.2∗
4-shot71.2
Gemini Ultra — — — 77.8
conversations from GPT-4, ShareGPT (ShareGPT, 2023), GQA (Hudson and Manning, 2019), and
region-level VQA datasets, but significantly trails behind on the other tasks.
In general, we find Chameleon is fairly competitive on both image captioning and VQA tasks. It rivals other
models by using much fewer in-context training examples and with smaller model sizes, in both pre-trained
and fine-tuned model evaluations.
6 Related Work
Chameleon builds upon the lineage of works exploring token-based approaches for multimodal learning. The
idea of using discrete tokens to represent continuous modalities like images was first explored in works like
BEiT (Bao et al., 2021), which proposed a self-supervised vision representation learning method based on
tokenized image patches. Aghajanyan et al. (2022) extended this idea to learning from mixed-modal documents
through interleaved image and text tokens, allowing for joint reasoning over both modalities within a unified
architecture. CM3Leon (Yu et al., 2023) further scaled up this approach to autoregressive text-to-image
generation, building on the initial proposal of token-based image generation in DALL-E (Ramesh et al., 2021).
As a fully token-based early-fusion model, Chameleon differs from late-fusion approaches like Flamingo
(Alayrac et al., 2022) which encode images and text separately before combining them at a later stage.
Other models like LLaVA (Liu et al., 2023a), IDEFICS (Laurençon et al., 2023), and VisualGPT (Chen
et al., 2022) also maintain separate image and text encoders. In contrast, Chameleon’s unified token space
allows it to seamlessly reason over and generate interleaved image and text sequences, without the need for
modality-specific components. This early-fusion approach, however, comes with significant challenges in terms
of representation learning and alignment, as discussed in Baltrušaitis et al. (2018).
The most similar model to Chameleon is Gemini (Gemini et al., 2023), which also uses an early-fusion
token-based approach. However, a key difference is that Gemini uses separate image decoders, whereas
Chameleon is an end-to-end dense model without any routing components. This makes Chameleon a more
general-purpose model for both multimodal understanding and generation tasks, similar in spirit to the
Perceiver (Jaegle et al., 2021) architecture which also aims for a unified model across modalities and tasks.
In summary, Chameleon builds on a rich history of work in multimodal learning and token-based architectures,
while pushing the boundaries in terms of model scale and architecture design. By demonstrating strong
performance across a wide range of vision-language tasks and enabling new capabilities in mixed-modal
reasoning and generation, Chameleon represents a significant step towards realizing the vision of general-
17purpose multimodal foundation models.
7 Conclusion
In this paper, we introduced Chameleon , a new family of early-fusion token-based foundation models that
set a new bar for multimodal machine learning. By learning a unified representation space over interleaved
image and text tokens, Chameleon is a single model that achieves strong performance across a wide range of
vision-language benchmarks while enabling new mixed-modal reasoning and generation capabilities.
The key to Chameleon ’s success is its fully token-based architecture, which allows for seamless information
integration across modalities. By quantizing images into discrete tokens and training on mixed-modal data
from scratch, Chameleon learns to jointly reason over image and text in a way that is impossible with
late-fusion architectures or models that maintain separate encoders for each modality. At the same time,
Chameleon introduces novel techniques for stable and scalable training of early-fusion models, addressing key
optimization and architectural design challenges that have previously limited the scale of such approaches. On
tasks such as image captioning and visual question answering, Chameleon-34B outperforms models such as
Flamingo and IDEFICS, while maintaining competitive performance on text-only benchmarks. Chameleon
also unlocks entirely new possibilities for multimodal interaction, as demonstrated by its strong performance
on our new benchmark for mixed-modal open-ended QA.
Acknowledgements
We thank Naren Briar for her invaluable contribution to manually curating safety prompts, which were crucial
for our safety tuning efforts. We also thank Pierre Fernandez for his indispensable support with the Chameleon
release, Shelly Sheynin for her work on the Chameleon image tokenizer, Puxin Xu and David for helping us
with datasets. Additionally, we thank Mitchell Wortsman for engaging in insightful discussions about stability
in large-scale language models and Mike Lewis for general discussions and advice throughout the project. We
thank Aaron Grattafiori, Firat Ozgenel, Divya Shah, Danny Livshits, Cristian Canton Ferrer, Saghar Hosseini,
Ramon Calderer, Joshua Saxe, Daniel Song and Manish Bhatt for their help with the safety and red teaming
efforts.
Contributors
We attribute credit separated by bucket of work. Additionally,∗indicates joint first authors,†indicates key
contributors,‡indicates workstream leads, and♯indicates project leads.
Pre-Training: Srinivasan Iyer∗, Bernie Huang∗, Lili Yu†, Arun Babu†, Chunting Zhou†, Kushal Tirumala, Xi
Victoria Lin, Hu Xu, Xian Li, Akshat Shrivastava, Omer Levy‡, Armen Aghajanyan∗‡
Alignment and Safety: Ram Pasunuru∗, Andrew Cohen†, Aram H. Markosyan†, Koustuv Sinha†, Xiaoqing
Ellen Tan†, Ivan Evtimov, Ping Yu, Tianlu Wang, Olga Golovneva, Asli Celikyilmaz‡
Inference and Evaluation: Pedro Rodriguez†, Leonid Shamis†, Vasu Sharma†, Christine Jou, Karthik Padthe†,
Ching-Feng Yeh, Mingda Chen, Bapi Akula, Jacob Kahn‡, Daniel Li‡, Scott Yih‡
Overall Project: Barlas Oguz, Morteza Behrooz, Benjamin Muller, Carleigh Wood, Mary Williamson, Ramya
Raghavendra, Barbara Usher, William Ngan, Nikolay Bashlykov, Lukas Blecher, Sony Theakanath (Lead
PM), Ammar Rizvi (Lead TPM), Gargi Ghosh♯, Luke Zettlemoyer♯
18References
Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko,
Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked multimodal model of the internet. arXiv
preprint arXiv:2201.07520 , 2022.
Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller,
Naman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling laws for generative mixed-modal language models. arXiv
preprint arXiv:2301.03728 , 2023.
Agriflanders. Miniatuurpaardjes prijskamp - Agriflanders 2009, 2009. https://en.wikipedia.org/wiki/File:
Miniatuurpaardje.jpg . CC-BY-SA 2.0, https://creativecommons.org/licenses/by/2.0/deed.en .
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in
Neural Information Processing Systems , 35:23716–23736, 2022.
Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning: A survey and
taxonomy. IEEE transactions on pattern analysis and machine intelligence , 41(2):423–443, 2018.
Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint
arXiv:2106.08254 , 2021.
James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce
Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai.
com/papers/dall-e-3. pdf , 2(3):8, 2023.
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural
language. In Proceedings of the AAAI conference on artificial intelligence , pages 7432–7439, 2020.
Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed Elhoseiny. Visualgpt: Data-efficient adaptation of pretrained
language models for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 18030–18040, 2022.
Xinlei Chen, Hao Fang, Tsung-Yi Lin, and Ramakrishna Vedantam. https://github.com/salaniz/pycocoevalcap , 2020.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,
Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways.
arXiv preprint arXiv:2204.02311 , 2022.
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq:
Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044 , 2019.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457 ,
2018.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,
Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint
arXiv:2110.14168 , 2021.
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter
Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion
parameters. In International Conference on Machine Learning , pages 7480–7512. PMLR, 2023.
Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based
text-to-image generation with human priors. arXiv preprint arXiv:2203.13131 , 2022.
Gemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan
Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv
preprint arXiv:2312.11805 , 2023.
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter:
Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 6904–6913, 2017.
19Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring
massive multitask language understanding. In International Conference on Learning Representations , 2020.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob
Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874 , 2021.
Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with
(almost) no human labor, 2022.
Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional
question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages
6700–6709, 2019.
Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda
Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A general architecture for structured
inputs & outputs. arXiv preprint arXiv:2107.14795 , 2021.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las
Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint
arXiv:2310.06825 , 2023.
Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh
Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint
arXiv:2401.04088 , 2024.
Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Bin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang Lei,
et al. Unified language-vision pretraining with dynamic discrete visual tokenization. arXiv preprint arXiv:2309.04669 ,
2023.
Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open
dataset of user preferences for text-to-image generation, 2023.
Klaus Krippendorff. Content analysis: An introduction to its methodology . Sage publications, 2018.
Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer
for neural text processing. arXiv preprint arXiv:1808.06226 , 2018.
Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang,
Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al. Obelisc: An open web-scale filtered dataset of
interleaved image-text documents. arXiv preprint arXiv:2306.16527 , 2023.
Kevin Lee and Shubho Sengupta. Introducing the ai research supercluster — meta’s cutting-edge ai supercomputer for
ai research. https://ai.facebook.com/blog/ai-rsc/ , 2022.
Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru
Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza, Luca Wehrstedt, Jeremy Reizenstein, and Grigory
Sizov. xformers: A modular and hackable transformer modelling library. https://github.com/facebookresearch/
xformers , 2022.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision , pages 740–755.
Springer, 2014.
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv
preprint arXiv:2310.03744 , 2023a.
HaotianLiu, ChunyuanLi, QingyangWu, andYongJaeLee. Visualinstructiontuning. arXiv preprint arXiv:2304.08485 ,
2023b.
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on
computer vision , pages 10012–10022, 2021.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 , 2017.
Giacomo Marzi, Marco Balzano, and Davide Marchiori. K-alpha calculator–krippendorff’s alpha calculator: A user-
friendly tool for computing krippendorff’s alpha inter-rater reliability coefficient. MethodsX , 12:102545, 2024. ISSN
202215-0161. doi: https://doi.org/10.1016/j.mex.2023.102545. https://www.sciencedirect.com/science/article/pii/
S2215016123005411 .
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new
dataset for open book question answering. arXiv preprint arXiv:1809.02789 , 2018.
OpenAI. GPTV System Card. https://cdn.openai.com/papers/GPTV_System_Card.pdf , 2023.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performance deep learning library.
InNeurIPS , 2019.
Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.
Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings
of the IEEE international conference on computer vision , pages 2641–2649, 2015.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092 , 2021.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image
generation with clip latents. arXiv preprint arXiv:2204.06125 , 2022.
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu,
Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 ,
2023.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd
schema challenge at scale. Communications of the ACM , 64(9):99–106, 2021.
Mikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram H. Markosyan, Manish Bhatt,
Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, Tim Rocktäschel, and Roberta Raileanu. Rainbow
teaming: Open-ended generation of diverse adversarial prompts, 2024.
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about
social interactions. arXiv preprint arXiv:1904.09728 , 2019.
Rylan Schaeffer. Pretraining on the test set is all you need. arXiv preprint arXiv:2309.08632 , 2023.
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for
training next generation image-text models. arXiv preprint arXiv:2210.08402 , 2022.
Georges Seguin. Mille-feuille, 2010. https://en.wikipedia.org/wiki/File:Mille-feuille_20100916.jpg . CC-BY-SA 3.0,
https://creativecommons.org/licenses/by-sa/3.0/deed.en .
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In
ACL, Berlin, Germany, 2016. https://aclanthology.org/P16-1162 .
ShareGPT. GPTV System Card. https://sharegpt.com/ , 2023.
Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 , 2020.
Maksim Sokolov. Sagrada Familia July 2022, 2022. https://en.wikipedia.org/wiki/File:Sagrada_Familia_%28July_
2022%29_08.jpg . CC-BY-SA-4.0, https://creativecommons.org/licenses/by-sa/4.0/deed.en .
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple
way to prevent neural networks from overfitting. The journal of machine learning research , 15(1):1929–1958, 2014.
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer
with rotary position embedding. arxiv e-prints, art. arXiv preprint arXiv:2104.09864 , 2021.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.
arXiv preprint arXiv:2307.09288 , 2023.
JarekTuszyński. Americantoad(Bufoamericanus)foundinFairfax, Virginia, 2015. https://en.wikipedia.org/wiki/File:
Miniatuurpaardje.jpg . CC-BY-SA-4.0, https://creativecommons.org/licenses/by-sa/4.0/deed.en .
Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation.
InProceedings of the IEEE conference on computer vision and pattern recognition , pages 4566–4575, 2015.
21Mitchell Wortsman, Peter J Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John D Co-Reyes, Izzeddin
Gur, Abhishek Kumar, Roman Novak, et al. Small-scale proxies for large-scale transformer training instabilities.
arXiv preprint arXiv:2309.14322 , 2023.
Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang,
Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models: Pretraining and instruction tuning.
arXiv preprint arXiv:2309.02591 , 2023.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your
sentence? arXiv preprint arXiv:1905.07830 , 2019.
Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing
Systems, 32, 2019.
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu,
et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206 , 2023.
22Appendix
A Samples
Prompt: My mom just adopted a dog called a mountain cur. I thought I
knew a lot about dogs but I have never heard of this breed. Can you give
me some basic information as well as show me a few pictures of what they
look like?
Response: Sure! Mountain Cur dogs are a type of hunting dog
that originated in the Appalachian Mountains. They are known for
their intelligence, loyalty, and strong prey drive. They are typically
medium-sized dogs, with a muscular build and a short, easy-to-maintain
coat that comes in a variety of colors including black, brown, tan, red,
and white. Here is an image of a black and white mountain cur dog: <img>
They have a long muzzle, floppy ears, and a bushy tail. They are generally
friendly and outgoing, but can be wary of strangers if not socialized
properly. They are highly energetic dogs that require regular exercise and
mental stimulation to prevent boredom and destructive behavior.
Here is another image of a mountain cur dog in the forest: <img>
Mountain Cur dogs are often used for hunting, particularly for small game
such as squirrels, raccoons, and rabbits. They are also popular family
pets due to their loyal and affectionate nature. However, they do require
regular exercise and training to ensure they remain happy and healthy.
With proper care and attention, Mountain Cur dogs can make wonderful
companions for active families or individuals who enjoy the outdoors.
Prompt: What color is a polar bear’s fur? Show me a photograph of the
polar bear in the wild.
Response: Polar bears have white fur, which helps them blend in with
their snowy and icy surroundings. This camouflage allows them to
hunt more effectively, as they can sneak up on their prey without being
detected. Additionally, the white fur reflects sunlight, keeping the polar
bear cool in the harsh Arctic environment. Here is an image of a polar
bear in the wild: <img>
23Prompt: <img> Myplantlookslikethis... Isthisaproblem? Ifso, howdoIfixit?
Response: The image you provided shows a pothos plant with yellowing leaves,
which can be caused by a variety of factors, including overwatering, underwater-
ing, lack of nutrients, too much direct sunlight, or pests. Here are some steps
you can take to troubleshoot and potentially fix the issue:
•Check the soil: Pothos plants prefer well-draining soil that is neither too
wet nor too dry. If the soil feels soggy or waterlogged, stop watering for a
while. Conversely, if the soil feels dry to the touch, it’s time to water.
•Adjust watering: Overwatering is a common cause of yellowing leaves on
pothos plants. Reduce the frequency of watering and allow the soil to dry
out slightly between waterings.
•Provide adequate light: Pothos plants prefer bright, indirect light but can tolerate low light
conditions. If your plant is not getting enough light, move it to a brighter location.
•Fertilize: Pothos plants benefit from regular fertilization. Use a balanced, water-soluble fertilizer
and follow the instructions on the label.
•Prune: Yellow leaves can be removed to improve the appearance of the plant. Cut back any yellow
leaves to the nearest healthy stem using clean pruning shears.
•Check for pests: Inspect the plant for pests such as mealybugs, spider mites, or scale. If present,
treat the plant with insecticidal soap or neem oil according to the product’s instructions.
•Repot: If the plant is pot-bound (meaning the roots have filled the container), consider repotting it
in a larger container with fresh, well-draining soil.
By following these steps, you may be able to identify and address the underlying cause of the yellowing
leaves on your pothos plant.
B Additional Information of Human Evaluations
Table 8Descriptions of the prompt task categories.
Task category Description
How-to Instructions on how to complete a task or accomplish a goal.
Advice Informed problem-solving.
Explanation A more detailed exposition of a topic, e.g., how batteries work, why animals hibernate, or
how to apply rules of composition to photography.
Hypothetical Responses to imaginative, “what if” questions.
Brainstorming Generating ideas, options, or possibilities.
Reasoning Deducing the answer to a question using commonsense or information provided in the prompt.
Comparison Describes the similarities / differences between multiple things, like products, places, foods,
etc.
Identification Identifying objects in the input image.
Article Asking for the creation of content such as blog posts.
Report Generating a summary of real events.
Story Creating fictional narratives.
Other Other miscellaneous requests.
For the twelve task categories of the prompts we collected for human evaluation, a short description of each
category can be found in Table 8.
The task fulfillment rates, broken down by each task category and modality are shown in Table 9 and Table 10.
Chameleon ’s win rates, broken down by task category and modality, are shown in Table 11, Table 12, Table 13
and Table 14.
24Table 9Task fulfillment breakdown.
Chameleon Gemini+ GPT-4V+
Task Type Fulfills Partially
fulfillsDoes not
fulfillFulfills Partially
fulfillsDoes not
fulfillFulfills Partially
fulfillsDoes not
fulfill
Advice 69.2% 26.2% 4.7% 42.1% 56.1% 1.9% 43.9% 48.6% 7.5%
Article 59.4% 37.5% 3.1% 40.6% 53.1% 6.3% 62.5% 37.5% 0.0%
Brainstorming 57.9% 36.4% 5.6% 33.3% 61.5% 5.1% 47.7% 47.2% 5.1%
Comparison 60.4% 34.7% 5.0% 47.5% 46.5% 5.9% 43.6% 44.6% 11.9%
Explanation 53.0% 37.7% 9.3% 33.8% 61.6% 4.6% 41.7% 50.3% 7.9%
How-to 52.7% 40.5% 6.9% 43.5% 52.7% 3.8% 48.1% 41.2% 10.7%
Hypothetical 55.9% 39.0% 5.1% 39.0% 47.5% 13.6% 42.4% 44.1% 13.6%
Identification 55.7% 33.0% 11.3% 33.0% 66.0% 1.0% 35.1% 55.7% 9.3%
Other 41.8% 40.0% 18.2% 38.2% 41.8% 20.0% 50.9% 40.0% 9.1%
Reasoning 50.0% 13.6% 36.4% 27.3% 59.1% 13.6% 31.8% 54.5% 13.6%
Report 49.1% 40.4% 10.5% 29.8% 61.4% 8.8% 38.6% 47.4% 14.0%
Story 31.7% 63.4% 4.9% 39.0% 56.1% 4.9% 53.7% 43.9% 2.4%
Gemini GPT-4V
Task Type Fulfills Partially
fulfillsDoes not
fulfillFulfills Partially
fulfillsDoes not
fulfill
Advice 21.5% 70.1% 8.4% 23.4% 75.7% 0.9%
Article 12.5% 84.4% 3.1% 9.4% 90.6% 0.0%
Brainstorming 18.5% 71.8% 9.7% 27.2% 66.7% 6.2%
Comparison 14.9% 76.2% 8.9% 19.8% 72.3% 7.9%
Explanation 15.2% 78.1% 6.6% 19.9% 77.5% 2.6%
How-to 19.8% 74.0% 6.1% 31.3% 67.2% 1.5%
Hypothetical 30.5% 49.2% 20.3% 32.2% 61.0% 6.8%
Identification 18.6% 75.3% 6.2% 22.7% 68.0% 9.3%
Other 14.5% 60.0% 25.5% 18.2% 67.3% 14.5%
Reasoning 9.1% 77.3% 13.6% 13.6% 81.8% 4.5%
Report 12.3% 77.2% 10.5% 22.8% 68.4% 8.8%
Story 9.8% 82.9% 7.3% 7.3% 90.2% 2.4%
Table 10 Modality fulfillment breakdown.
Chameleon Gemini+ GPT-4V+
Fulfills Partially
fulfillsDoes not
fulfillFulfills Partially
fulfillsDoes not
fulfillFulfills Partially
fulfillsDoes not
fulfill
Mixed-modality 55.3% 36.7% 7.9% 39.2% 57.8% 2.9% 42.6% 52.4% 5.0%
Text-only 57.7% 38.4% 4.0% 36.4% 55.5% 8.1% 46.1% 42.7% 11.2%
Gemini GPT-4V
Fulfills Partially
fulfillsDoes not
fulfillFulfills Partially
fulfillsDoes not
fulfill
Mixed-modality 19.7% 76.0% 4.3% 24.3% 72.6% 3.2%
Text-only 18.3% 72.7% 9.1% 23.6% 72.0% 4.4%
25Table 11Complete Win Rates: Chameleon vs. Gemini+.
Wins Ties Loses Win rate
Overall 435 362 251 58.8%
Advice 48 35 24 61.2%
Article 14 14 4 65.6%
Brainstorming 101 60 34 67.2%
Comparison 41 38 22 59.4%
Explanation 65 46 40 58.3%
How-to 53 51 27 59.9%
Hypothetical 17 24 18 49.2%
Identification 39 33 25 57.2%
Other 24 17 14 59.1%
Reasoning 7 8 7 50.0%
Report 16 22 19 47.4%
Story 10 14 17 41.5%
Mixed-modal Prompts 194 145 102 60.4%
Text-only Prompts 241 217 149 57.6%
Table 12 Complete Win Rates: Chameleon vs. GPT-4V+.
Wins Ties Loses Win rate
Overall 375 331 342 51.6%
Advice 54 27 26 63.1%
Article 9 11 12 45.3%
Brainstorming 78 57 60 54.6%
Comparison 35 35 31 52.0%
Explanation 53 56 42 53.6%
How-to 49 46 36 55.0%
Hypothetical 23 19 17 55.1%
Identification 31 26 40 45.4%
Other 16 13 26 40.9%
Reasoning 11 5 6 61.4%
Report 16 21 20 46.5%
Story 0 15 26 18.3%
Mixed-modal Prompts 149 119 173 47.3%
Text-only Prompts 226 212 169 54.7%
26Table 13 Complete Win Rates: Chameleon vs. Gemini.
Wins Ties Loses Win rate
Overall 561 327 160 69.1%
Advice 59 25 23 66.8%
Article 18 11 3 73.4%
Brainstorming 133 42 20 79.0%
Comparison 54 29 18 67.8%
Explanation 78 51 22 68.5%
How-to 65 42 24 65.6%
Hypothetical 27 26 6 67.8%
Identification 45 30 22 61.9%
Other 27 23 5 70.0%
Reasoning 11 6 5 63.6%
Report 30 21 6 71.1%
Story 14 21 6 59.8%
Mixed-modal Prompts 240 123 78 68.4%
Text-only Prompts 321 204 82 69.7%
Table 14 Complete Win Rates: Chameleon vs. GPT-4V.
Wins Ties Loses Win rate
Overall 482 329 237 61.7%
Advice 53 30 24 63.6%
Article 18 9 5 70.3%
Brainstorming 107 53 35 68.5%
Comparison 44 35 22 60.9%
Explanation 75 36 40 61.6%
How-to 51 49 31 57.6%
Hypothetical 20 25 14 55.1%
Identification 40 29 28 56.2%
Other 20 22 13 56.4%
Reasoning 10 6 6 59.1%
Report 25 18 14 59.6%
Story 19 17 5 67.1%
Mixed-modal Prompts 191 125 125 57.5%
Text-only Prompts 291 204 112 64.7%
27Jukebox: A Generative Model for Music
Prafulla Dhariwal* 1Heewoo Jun* 1Christine Payne* 1Jong Wook Kim1Alec Radford1Ilya Sutskever1
Abstract
We introduce Jukebox, a model that generates
music with singing in the raw audio domain. We
tackle the long context of raw audio using a multi-
scale VQ-V AE to compress it to discrete codes,
and modeling those using autoregressive Trans-
formers. We show that the combined model at
scale can generate high-ﬁdelity and diverse songs
with coherence up to multiple minutes. We can
condition on artist and genre to steer the musical
and vocal style, and on unaligned lyrics to make
the singing more controllable. We are releasing
thousands of non cherry-picked samples, along
with model weights and code.
1. Introduction
Music is an integral part of human culture, existing from the
earliest periods of human civilization and evolving into a
wide diversity of forms. It evokes a unique human spirit in
its creation, and the question of whether computers can ever
capture this creative process has fascinated computer scien-
tists for decades. We have had algorithms generating piano
sheet music (Hiller Jr & Isaacson, 1957; Moorer, 1972;
Hadjeres et al., 2017; Huang et al., 2017), digital vocoders
generating a singer’s voice (Bonada & Serra, 2007; Saino
et al., 2006; Blaauw & Bonada, 2017) and also synthesizers
producing timbres for various musical instruments (Engel
et al., 2017; 2019). Each captures a speciﬁc aspect of music
generation: melody, composition, timbre, and the human
voice singing. However, a single system to do it all remains
elusive.
The ﬁeld of generative models has made tremendous
progress in the last few years. One of the aims of gen-
erative modeling is to capture the salient aspects of the data
and to generate new instances indistinguishable from the
true data The hypothesis is that by learning to produce the
data we can learn the best features of the data1. We are
surrounded by highly complex distributions in the visual,
audio, and text domain, and in recent years we have devel-
*Equal contribution1OpenAI, San Francisco. Correspondence
to: <jukebox@openai.com>.oped advances in text generation (Radford et al.), speech
generation (Xie et al., 2017) and image generation (Brock
et al., 2019; Razavi et al., 2019). The rate of progress in
this ﬁeld has been rapid, where only a few years ago we
had algorithms producing blurry faces (Kingma & Welling,
2014; Goodfellow et al., 2014) but now we now can gener-
ate high-resolution faces indistinguishable from real ones
(Zhang et al., 2019b).
Generative models have been applied to the music genera-
tion task too. Earlier models generated music symbolically
in the form of a pianoroll, which speciﬁes the timing, pitch,
velocity, and instrument of each note to be played. (Yang
et al., 2017; Dong et al., 2018; Huang et al., 2019a; Payne,
2019; Roberts et al., 2018; Wu et al., 2019). The symbolic
approach makes the modeling problem easier by working
on the problem in the lower-dimensional space. However, it
constrains the music that can be generated to being a speciﬁc
sequence of notes and a ﬁxed set of instruments to render
with. In parallel, researchers have been pursuing the non-
symbolic approach, where they try to produce music directly
as a piece of audio. This makes the problem more challeng-
ing, as the space of raw audio is extremely high dimensional
with a high amount of information content to model. There
has been some success, with models producing piano pieces
either in the raw audio domain (Oord et al., 2016; Mehri
et al., 2017; Yamamoto et al., 2020) or in the spectrogram
domain (Vasquez & Lewis, 2019). The key bottleneck is
that modeling the raw audio directly introduces extremely
long-range dependencies, making it computationally chal-
lenging to learn the high-level semantics of music. A way to
reduce the difﬁculty is to learn a lower-dimensional encod-
ing of the audio with the goal of losing the less important
information but retaining most of the musical information.
This approach has demonstrated some success in generat-
ing short instrumental pieces restricted to a set of a few
instruments (Oord et al., 2017; Dieleman et al., 2018).
In this work, we show that we can use state-of-the-art deep
generative models to produce a single system capable of gen-
erating diverse high-ﬁdelity music in the raw audio domain,
with long-range coherence spanning multiple minutes. Our
approach uses a hierarchical VQ-V AE architecture (Razavi
1Richard Feynmann famously said, “What I cannot create, I
do not understand”arXiv:2005.00341v1  [eess.AS]  30 Apr 2020Jukebox: A Generative Model for Music
et al., 2019) to compress audio into a discrete space, with
a loss function designed to retain the maximum amount of
musical information, while doing so at increasing levels
of compression. We use an autoregressive Sparse Trans-
former (Child et al., 2019; Vaswani et al., 2017) trained with
maximum-likelihood estimation over this compressed space,
and also train autoregressive upsamplers to recreate the lost
information at each level of compression.
We show that our models can produce songs from highly
diverse genres of music like rock, hip-hop, and jazz. They
can capture melody, rhythm, long-range composition, and
timbres for a wide variety of instruments, as well as the
styles and voices of singers to be produced with the mu-
sic. We can also generate novel completions of existing
songs. Our approach allows the option to inﬂuence the
generation process: by swapping the top prior with a con-
ditional prior, we can condition on lyrics to tell the singer
what to sing, or on midi to control the composition. We
release our model weights and training and sampling code
at https://github.com/openai/jukebox.
2. Background
We consider music in the raw audio domain represented as
a continuous waveform x2[ 1;1]T, where the number
of samplesTis the product of the audio duration and the
sampling rate typically ranging from 16 kHz to 48 kHz. For
music, CD quality audio, 44.1 kHz samples stored in 16
bit precision, is typically enough to capture the range of
frequencies perceptible to humans. As an example, a four-
minute-long audio segment will have an input length of 10
million, where each position can have 16 bits of information.
In comparison, a high-resolution RGB image with 1024
1024 pixels has an input length of 3million, and each
position has 24 bits of information. This makes learning
a generative model for music extremely computationally
demanding with increasingly longer durations; we have to
capture a wide range of musical structures from timbre to
global coherence while simultaneously modeling a large
amount of diversity.
2.1. VQ-V AE
To make this task feasible, we use the VQ-V AE (Oord et al.,
2017; Dieleman et al., 2018; Razavi et al., 2019) to compress
raw audio to a lower-dimensional space. A one-dimensional
VQ-V AE learns to encode an input sequence x=hxtiT
t=1
using a sequence of discrete tokens z=hzs2[K]iS
s=1,
whereKdenotes the vocabulary size and we call the ratio
T=S the hop length. It consists of an encoder E(x)which
encodes xinto a sequence of latent vectors h=hhsiS
s=1,
a bottleneck that quantizes hs7!ezsby mapping each hs
to its nearest vector ezsfrom a codebook C=fekgK
k=1,
and a decoder D(e)that decodes the embedding vectorsback to the input space. It is thus an auto-encoder with a
discretization bottleneck. The VQ-V AE is trained using the
following objective:
L=Lrecons +Lcodebook +Lcommit (1)
Lrecons =1
TP
tkxt D(ezt)k2
2 (2)
Lcodebook =1
SP
sksg [hs] ezsk2
2 (3)
Lcommit =1
SP
skhs sg [ezs]k2
2 (4)
where sgdenotes the stop-gradient operation, which passes
zero gradient during backpropagation. The reconstruction
lossLrecons penalizes for the distance between the input x
and the reconstructed output bx=D(ez), andLcodebook pe-
nalizes the codebook for the distance between the encodings
hand their nearest neighbors ezfrom the codebook. To
stabilize the encoder, we also add Lcommit to prevent the
encodings from ﬂuctuating too much, where the weight 
controls the amount of contribution of this loss. To speed up
training, the codebook loss Lcodebook instead uses EMA up-
dates over the codebook variables. Razavi et al. (2019)
extends this to a hierarchical model where they train a sin-
gle encoder and decoder but break up the latent sequence
hinto a multi-level representation [h(1);;h(L)]with de-
creasing sequence lengths, each learning its own codebook
C(l). They use non-autoregressive encoder-decoders and
jointly train all levels with a simple mean-squared loss.
3. Music VQ-V AE
Inspired by the results from the hierarchical VQ-V AE model
(Razavi et al., 2019) for images, we consider applying the
same technique to model raw audio using three different
levels of abstraction, as illustrated in Figure 1. At each level,
we use residual networks consisting of WaveNet-style non-
causal 1-D dilated convolutions, interleaved with downsam-
pling and upsampling 1-D convolutions to match different
hop lengths. A detailed description of the architecture is
provided in Appendix B.1. We make a number of modiﬁca-
tions to our VQ-V AE compared to the ones in (Oord et al.,
2017; Razavi et al., 2019), as described in the following
subsections.
3.1. Random restarts for embeddings
VQ-V AEs are known to suffer from codebook collapse,
wherein all encodings get mapped to a single or few em-
bedding vectors while the other embedding vectors in the
codebook are not used, reducing the information capacity
of the bottleneck. To prevent this, we use random restarts:
when the mean usage of a codebook vector falls below a
threshold, we randomly reset it to one of the encoder out-
puts from the current batch. This ensures all vectors in theJukebox: A Generative Model for Music
V ector
QuantizationV ector
QuantizationV ector
Quantization
EncodeEncode
Encode
ht	=	E ( xt ) xt zt	=	 ar gmink	ǁ	 ht	–	 ek 	ǁ
DecodeDecodeDecode
ez t x ̂t	=	D ( ez t )
Codebook
LookupCodebook
LookupCodebook
LookupCodebook  ek
Figure 1. We ﬁrst train three separate VQ-V AE models with different temporal resolutions. At each level, the input audio is segmented
and encoded into latent vectors ht, which are then quantized to the closest codebook vectors ezt. The code ztis a discrete representation
of the audio that we later train our prior on. The decoder takes the sequence of codebook vectors and reconstructs the audio. The top
level learns the highest degree of abstraction, since it is encoding longer audio per token while keeping the codebook size the same.
Audio can be reconstructed using the codes at any one of the abstraction levels, where the least abstract bottom-level codes result in the
highest-quality audio, as shown in Figure 4. For the detailed structure of each component, see Figure 7.
codebook are being used and thus have a gradient to learn
from, mitigating codebook collapse.
3.2. Separated Autoencoders
When using the hierarchical VQ-V AE from (Razavi et al.,
2019) for raw audio, we observed that the bottlenecked top
level is utilized very little and sometimes experiences a com-
plete collapse, as the model decides to pass all information
through the less bottlenecked lower levels. To maximize
the amount of information stored at each level, we simply
train separate autoencoders with varying hop lengths. Dis-
crete codes from each level can be treated as independent
encodings of the input at different levels of compression.
3.3. Spectral Loss
When using only the sample-level reconstruction loss, the
model learns to reconstruct low frequencies only. To capture
mid-to-high frequencies, we add a spectral loss which is
deﬁned as
Lspec=kjSTFT (x)j jSTFT (bx)jk2
It encourages the model to match the spectral components
without paying attention to phase which is more difﬁcult
to learn. This is similar to the use of power loss (Oord
et al., 2018) and spectral convergence (Arık et al., 2018b)
when training parallel decoders for raw audio. One differ-
ence between the latter approach and ours is that we are no
longer optimizing the spectral signal-to-noise ratio; dividing
by the magnitude of the signal results in numerical insta-
bility for mostly silent inputs. To prevent the model from
overﬁtting to a particular choice of the STFT parameters,we use the sum of the spectral losses Lspeccalculated over
multiple STFT parameters that trade-off time and frequency
resolutions (Yamamoto et al., 2020).
4. Music Priors and Upsamplers
After training the VQ-V AE, we need to learn a prior p(z)
over the compressed space to generate samples. We break
up the prior model as
p(z) =p(ztop;zmiddle;zbottom) (5)
=p(ztop)p(zmiddlejztop)p(zbottomjzmiddle;ztop)(6)
and train separate models for the top-level prior p(ztop), and
upsamplers p(zmiddlejztop)andp(zbottomjzmiddle;ztop). Each
of these is an autoregressive modeling problem in the dis-
crete token space produced by the VQ-V AE. We use Trans-
formers with sparse attention (Vaswani et al., 2017; Child
et al., 2019) as they are currently the SOTA in autoregressive
modeling. We propose a simpliﬁed version which we call
the Scalable Transformer, that is easier to implement and
scale (see Appendix A for details).
For the upsamplers, we need to provide the autoregres-
sive Transformers with conditioning information from the
codes of the upper levels. To do so, we use a deep resid-
ual WaveNet (Xie et al., 2017) followed by an upsampling
strided convolution and a layer norm (Ba et al., 2016), and
add the output as extra positional information to the embed-
dings of the current level. We condition the lower levels
only on the chunk of upper level codes that correspond to
the same segment of raw audio.Jukebox: A Generative Model for Music
At each level, we use Transformers over the same context
length of discrete codes, which correspond to increasing
the raw audio length with larger hop lengths, and modeling
longer temporal dependencies at the higher levels while
keeping the same computational footprint for training each
level. As our VQ-V AE is convolutional, we can use the
same VQ-V AE to produce codes for arbitrary lengths of
audio.
4.1. Artist, Genre, and Timing Conditioning
Our generative model can be made more controllable by
providing additional conditioning signals while training. For
our ﬁrst models, we provide artist and genre labels for the
songs. This has two advantages: ﬁrst, it reduces the entropy
of the audio prediction, so the model is able to achieve
better quality in any particular style. Second, at generation
time, we are able to steer the model to generate in a style
of our choosing. Additionally, we attach a timing signal
for each segment at training time. This signal includes the
total duration of the piece, the start time of that particular
sample and how much fraction of the song that has elapsed.
This allows the model to learn audio patterns that depend
on the overall structure, such as spoken or instrumental
introductions and applause at the end of a piece.
4.2. Lyrics Conditioning
While the conditioned models above are able to generate
songs of diverse genres and artistic styles, singing voices
generated by those models, while often sung in a compelling
melody, are mostly composed of babbling, rarely producing
recognizable English words. In order to be able to control
the generative model with lyrics, we provide more context
at training time by conditioning the model on the lyrics
corresponding to each audio segment, allowing the model
to produce singing simultaneosly with the music.
Lyrics-to-singing (LTS) task : The conditioning signal
only includes the text of the lyrics, without timing or vocal-
isation information. We thus have to model the temporal
alignment of lyrics and singing, the artists voice and also
the diversity of ways one can sing a phrase depending on the
pitch, melody, rhythm and even genre of the song. The con-
ditioning data isn’t precise as the lyrics data often contains
textual references to repeated sections like “chorus” or mis-
matching portions of lyrics with the corresponding music.
There is also no separation between lead vocals, accompa-
nying vocals and the background music in target audio. This
makes the Lyrics-to-singing (LTS) task signiﬁcantly more
challenging than the corresponding Text-to-speech (TTS)
task.
Providing lyrics for chunks of audio : Our dataset includes
song-level lyrics, but to make the task easier we train on
shorter (24 sec) chunks of audio. To provide the lyrics cor-
Middle Upsampler
Bottom Upsampler
VQ-V AE DecoderTop-Level Prior
Conditioning
Information(a)Ancestral sampling : Priors for the VQ-V AE codes are trained
using a cascade of Transformer models, shown in blue. Each model
takes conditioning information such as genre, artist, timing, and
lyrics, and the upsampler models are also conditioned on the codes
from the upper levels. To generate music, the VQ-V AE codes are
sampled from top to bottom using the conditioning information
for control, after which the VQ-V AE decoder can convert the
bottom-level codes to audio.
time
new samples
(b)Windowed sampling : To generate music longer than the
model’s context length (12 in this ﬁgure), we repeatedly sample
continuations at each level, using overlapping windows of previous
codes as the context. The overlap amount is a hyperparameter, and
the ﬁgure shows an example of 75% overlap with hop length 3.
Primed Audio Generated AudioEncodeGenerateDecode
(c)Primed sampling : The model can generate continuations of
an existing audio signal by converting it into the VQ-V AE codes
and sampling the subsequent codes in each level.
Figure 2. Sampling methods for generating musicJukebox: A Generative Model for Music
responding to the audio during training, we began with a
simple heuristics of aligning the characters of the lyrics to
linearly span the duration of each song, and pass a ﬁxed-side
window of characters centered around the current segment
during training. While this simple strategy of linear align-
ment worked surprisingly well, we found that it fails for
certain genres such as hip-hop with fast lyrics. To address
this, we use Spleeter (Hennequin et al., 2019) to extract vo-
cals from each song and run NUS AutoLyricsAlign (Gupta
et al., 2020) on the extracted vocals to obtain a word-level
alignments of the lyrics, allowing us to more accurately
provide the lyrics for a given chunk of audio. We choose a
large enough window so that the actual lyrics have a high
probability of being inside the window.
Encoder-decoder model : We use an encoder-decoder style
model to condition on the characters of the lyrics, with
the encoder producing features from the lyrics which are
attended to by the decoder which produces the top level
music tokens. The lyrics encoder is a Transformer with an
autoregressive modeling loss for lyrics, and its last level is
used as features of the lyrics. In the music decoder, we inter-
leave a few additional layers with encoder-decoder attention
where the queries from the music tokens are only allowed
to attend to keys and values from the lyrics tokens. These
layers attend on the activation from the last layer of the
lyrics encoder (see Figure 8c). In Figure 3, we see that the
attention pattern learned by one of these layers corresponds
to the alignment between the lyrics and the singing.
4.3. Decoder Pretraining
To reduce computation required to train the lyrics condi-
tional model, we use a pretrained unconditional top-level
prior as our decoder and introduce the lyrics encoder using
model surgery (Berner et al., 2019). We initialize the output
projection weights in the MLP and the attention layers of
these residual blocks to zeros (Zhang et al., 2019a), so that
the added layers perform the identity function at initializa-
tion. Thus, at initialization the model behaves identically
as the pretrained decoder, but there is still a gradient with
respect to the encoder state and parameters2, allowing the
model to learn to use the encoder.
4.4. Sampling
After we have trained our VQ-V AE, upsamplers, and top
level priors, we can then use them to sample novel songs.
Ancestral sampling : We ﬁrst generate the top level codes
one token at a time by the usual ancestral sampling process
(see Figure 2a): generating the ﬁrst token, then passing all
2The gradient also needs to break symmetry with the encoder
output features, which is the case here since the weights of the
input projections in the attention are not zero.
0 1600 3200 4800 6400 8000
Music token position0100200300400500Lyrics token position
0.00.20.40.60.81.0Figure 3. Lyrics-singing alignment learned by one of the encoder-
decoder attention layers. The x-axis is the position of music
queries, and the y-axis is the position of lyric keys. The positions
attended to by the decoder correspond to the characters being sung.
previously generated tokens into the model as inputs and
outputting the next token conditioned on all previous tokens.
We then run our conditioning wavenet on the top level codes
to produce the conditioning information for the middle level
and sample ancestrally from it too, and do the same for the
bottom level.
Windowed sampling : To sample segments longer than the
context length, we use windowed sampling, where we move
ahead our sampling window by half our context and con-
tinue sampling conditioned on this half context (See Figure
2b). We can trade off speed for quality by using a smaller
hop length here.
Primed sampling : Instead of sampling the entire token
sequence from the model, we can also run a forward pass
of the VQ-V AE to obtain the top, middle, and bottom level
codes corresponding to a segment from an actual song, as
shown in Figure 2c. We can use these as the initial tokens in
our ancestral sampling process and continue sampling from
these to produce novel completions of the song.
5. Experiments
5.1. Dataset
We scraped a new dataset of 1.2 million songs (600k of
which in English), paired with the lyrics and metadata from
LyricWiki (LyricWiki). The metadata includes artist, album,
genre, and year of the release, along with common moods or
playlist keywords associated with each song. We train on 32
bit, 44.1 kHz raw audio and perform data augmentation by
randomly downmixing the right and left channels to produce
mono channel audio.
5.2. Training Details
For the music VQ-V AE, we use 3 levels of bottlenecks com-
pressing 44 kHz audio in dimensionality by 8x, 32x, andJukebox: A Generative Model for Music
128x respectively, with a codebook size of 2048 for each
level. The VQ-V AE has 2 million parameters and is trained
on 9-second audio clips on 256 V100 for 3 days. We used
exponential moving average to update the codebook fol-
lowing Razavi et al. (2019). For our prior and upsampler
models, we use a context of 8192 tokens of VQ-V AE codes,
which corresponds to approximately 24, 6, and 1.5 seconds
of raw audio at the top, middle, and bottom level, respec-
tively. The upsamplers have one billion parameters and are
trained on 128 V100s for 2 weeks, and the top-level prior
has 5 billion parameters and is trained on 512 V100s for 4
weeks. We use Adam with learning rate 0:00015 and weight
decay of 0:002. For lyrics conditioning, we reuse the prior
and add a small encoder, after which we train the model on
512 V100s for 2 weeks. The detailed hyperparameters for
our models and training are provided in Appendix B.3.
5.3. Samples
We trained a sequence of models with increasing sample
quality. Our ﬁrst model was trained on the MAESTRO
dataset using 22 kHz VQ-V AE codes and relatively small
prior models. We observed that this could generate high
ﬁdelity classical music samples with piano and occasional
violin. We then collected a larger and more diverse dataset
of songs with genre and artist labels. The same model when
trained on this new dataset was able to produce diverse sam-
ples other than classical music, and demonstrated musicality
and coherence over more than a minute.
Despite the novelty of being able to generate generally high
ﬁdelity and coherent songs, sample quality was still limited
by a number of factors. First, the use of 22 kHz sampling
rate along with small upsamplers introduced noise both in
the upsampling and decoding steps, which we hear as grainy
texture. We improved ﬁdelity by using 44 kHz VQ-V AE
and 1B parameter upsamplers in all subsequent experiments
at the expense of longer rendering time.
Second, the 1B top-level prior was not big enough to pro-
duce singing and diverse musical timbres. We ﬁrst explored
increasing the model size to 5 billion parameters. Larger
capacity allowed better modeling of the broader distribu-
tion of songs, resulting in samples with better musicality,
longer coherence and initial singing. While there is an over-
all qualitative improvement, the unconditional model still
struggled to sing recognizable words. Training a seq2seq
model with lyric conditioning and limiting the dataset only
to songs primarily in English made singing both intelligible
and controllable.
The ﬁnal model, which we call Jukebox, uses all these
improvements. Because everyone experiences music dif-
ferently, it is generally tricky and not very meaningful to
evaluate samples by the mean opinion score or FID-like
metrics. We manually evaluate coherence, musicality, diver-sity, and novelty of generated samples. The links to curated
examples are embedded in text.
Coherence: We ﬁnd the samples stay very coherent musi-
cally through the context length of the top-level prior (ap-
proximately 24 seconds), and they maintain similar har-
monies and textures as we slide the window to generate
longer samples. However, because the top-level does not
have the context of the entire song, we do not hear long
term musical patterns, and we would never hear choruses or
melodies that repeat.
The generations progress through beginnings of songs (for
example applause or slow instrumental warm-ups), through
sections that sound chorus-like, through instrumental inter-
ludes, and then fading or otherwise wrapping up at the end.
The top-level prior always knows what fraction of the song
is complete time-wise, so it is able to imitate appropriate
beginnings, middles and ends.
Musicality: The samples frequently imitate familiar mu-
sical harmonies and the lyrics are usually set in ways that
are very natural. Frequently the highest or longest notes of
the melody match words that a human singer would choose
to emphasize, and the lyrics are almost always rendered
in ways that capture the prosody of the phrases. This is
noticeable in hip hop generations, where the model reliably
captures the rhythm of spoken text. We do ﬁnd that the
generated melodies are usually less interesting than human
composed melodies. In particular, we do not hear the an-
tecedent and consequent pattern familiar to many human
melodies, and we rarely hear choruses that are melodically
memorable.
Diversity: Likelihood training encourages covering of all
modes, so we expect the model to produce diverse samples.
– Re-renditions: We generate multiple samples conditioned
on artist and lyrics combinations that exist in our training
data. While occasionally drum and bass lines or melodic
intervals echo the original versions, we ﬁnd that none of
the generated samples is noticeably similar to the original
songs.
We also generate multiple songs conditioned on the same
artist and lyrics as Sample 1 to obtain Samples 9–12. All ﬁve
sound interesting in their own ways with different moods
and melodies with Sample 10 playing a harmonic at 00:14
as part of a blues riff, showing that the model has learned a
wide range of singing and playing styles.
– Completions: We prime the model with 12 seconds of
existing songs and ask it to complete them in the same
styles. When the priming samples include singing, the con-
tinuations are more likely to imitate the original tunes and
rhythms. Songs primed with more generic or common intros
tend to be more diverse. Even generated samples that areJukebox: A Generative Model for Music
close to the originals early on deviate completely into new
musical material after about 30 seconds.
Re-renditions and completions are interesting and diverse,
but overall, there is still room for improvement in music
quality compared to the original songs.
– Full tree: To understand diversity in a more systematic
way, we generate multiple continuations from the same seg-
ment. We start with a one-minute sample and independently
sample four times per one-minute extension. By the three
minute mark, there are 16 completions. We can think of this
branching tree as exploring different possibilities obtained
by ancestral sampling. In the generated songs in the link,
we hear diversity in singing and development even when the
same initial segment is used. We note that this particular
sample follows the lyrics more successfully than many. For
certain genres like hip hop and rap, where linearly moving
the window does not yield good lyrics alignment, the chance
of obtaining plausible singing is lower.
Novelty: With the ability to condition on various styles,
lyrics, and raw audio, we would like Jukebox to be a useful
tool for both professional musicians and music enthusiasts
alike. In this section, we are interested in exploring capabil-
ities and applications of Jukebox.
– Novel styles: We generate songs in an unusual genre typi-
cally not associated with an artist. In general, we ﬁnd that
it is fairly difﬁcult to generalize to a novel style of singing
while using the same voice as the artist embedding overpow-
ers other information. In Joe Bonamassa and Frank Sinatra
samples, we hear a modest variation in instrumentation,
energy, and ambience depending on the genre embedding.
However, our attempts to mix country singer Alan Jackson
with unusual genres like hip hop and punk did not seem to
move the samples away from a country style in meaningful
ways.
– Novel voices: We pick artists whose voices are reproduced
reasonably well by the model, and interpolate their style
embeddings to synthesize new voices. Some blending, for
instance, between Frank Sinatra and Alan Jackson in Sample
4, still sounds similar to Frank Sinatra. In most cases, the
model renders in a vaguely recognizable but distinct voice
that preserves different vocal attributes. Samples 1 and
2 conditioned on the Céline Dion embeddings divided by
two have slightly different timbre and tone but capture her
unique vibrato.
We also experiment with changing the style embedding in
the middle of a song to create a duet (Sample 7). This is
another way of guiding generation during sampling. Con-
tinuing in another voice works best when the segment ends
in an interlude; otherwise, the model blends voices in the
middle of a word or a sentence.– Novel lyrics: We ask Jukebox to sing poems and novel
verses generated by GPT-2 (Radford et al.) to demonstrate
that it can indeed sing new lyrics. While the training data
consists of song lyrics with limited vocabulary and con-
strained structure, the model has learned to follow along
most prompts and sing even new words that are reasonably
pronounceable (including technical terms from the deep
learning literature). To get the best results, however, we ﬁnd
that it is useful to spell out difﬁcult words or acronyms as
they are spoken. The generations are noticeably higher qual-
ity if the text matches the distribution of lyrics for the given
artist, both in terms of length, and of rhyming or rhythmic
qualities. For example, hip hop lyrics tend to be longer than
most other genres, and the commonly emphasized syllables
easily form clear rhythms.
– Novel riffs: Another useful application of Jukebox is the
ability to record an incomplete idea and explore various
continuations without ever needing to tabulate in symbolic
representations, which would lose details of timbre and
mood. We curate recordings of novel riffs by our in-house
musicians and prime the model during sampling. Sample 6
starts with a musical style not widely used in Elton John’s
songs. The model still carries out the tune and develops
it further. Similarly, the beginning of Sample 1 is a pro-
gressive jazz piece with a 5/4 polymeter, which has never
been used in hip hop. Despite this novelty, the rhythm per-
sists throughout the song and is incorporated naturally with
rapping.
5.4. VQ-V AE Ablations
Spectral convergence (dB)
Level Hop length Without restart With restart
Bottom 8  21:1 23:0
Middle 32  12:4 12:4
Top 128  8:3 8:3
Table 1. Reconstruction ﬁdelity degrades with higher compression.
Restarting dead codes near random encoder outputs mitigates learn-
ing suboptimal codes.
Codebook size Spectral convergence (dB)
256  15:9
2048  23:0
No quantization  40:5
Table 2. Bottom-level VQ-V AE reconstruction results with differ-
ent codebook sizes. Using larger codebooks helps reconstruction
because it allows more information to be encoded at the bottleneck
layers. Removing the bottleneck entirely yields almost perfect
reconstruction.Jukebox: A Generative Model for Music
5001k2k4k8k16k
5001k2k4k8k16k
5001k2k4k8k16k
5001k2k4k8k16k
5001k2k4k8k16k
5001k2k4k8k16k
Figure 4. Comparison of reconstructions from different VQ-V AEs, x-axis is time and y-axis is frequency. The columns from left to
right are bottom-, middle-, and top-level reconstructions at hop lengths 8, 32, and 128 respectively, visualized as Mel spectrograms.
The ﬁrst row is the ground-truth, and the second row shows the spectrograms of audio outputs from our VQ-V AE. In the third row, we
remove the spectral loss, and see that the middle and top level lose high-frequency information. In the fourth row, we use a hierarchical
VQ-V AE (Razavi et al., 2019) instead of separate auto-encoders (Figure 1), and we see the middle and top levels are not used for encoding
pertinent information. Finally, the ﬁfth row shows a baseline with the Opus codec that encodes audio at constant bitrates comparable to
our VQ-V AE. It also fails to capture higher frequencies and adds noticeable artifacts at the highest level of compression.
Ablation Spectral convergence (dB)
None  8:3
Without spectral loss  6:3
With single autoencoder 2:9
Table 3. Top-level codes are generally difﬁcult to train well without
spectral loss or with a single hierarchical autoencoder. Resulting
reconstructions may lose some to most of information.
We compare raw audio VQ-V AEs when trained with varying
compression ratios, objectives, and architectures. As we
use nonautoregressive decoders with continuous represen-
tation for output, we report spectral convergence (Sturmel& Daudet, 2011), which measures the amount of spectral
error relative to signal, as test error and proxy for reconstruc-
tion ﬁdelity. We evaluate on 5000 held-out 3-second audio
segments and report the average in decibels. All models in
this section are trained with a batch size of 32, 3-second
audio clips sampled at 44 kHz. As before, we use hop
lengths of 8, 32, and 128 for the bottom, middle and top
level respectively.
In Table 1, we see that increasing the hop size results in
higher reconstruction error. Figure 4 indeed shows that a
signiﬁcant amount of information, especially higher frequen-
cies, is missing at middle and top levels across all ablations
we ran. This is expected as audio is compressed more withJukebox: A Generative Model for Music
0 100k 200k 300k 400k 500k
Number of training steps8910Codebook entropy (bits)with restart
without restart
Figure 5. Entropy of codebook with 2048 codes, i.e 11 bits, over
training. Reviving dead codes near random encoder outputs en-
sures good codebook utilization from the start of training.
larger hop sizes. To mitigate codebook collapse, we restart
dead codes near random encoder embeddings. In Figure 5,
we see that this yields higher codebook usage even from
early on in training. Models trained without random restarts
can converge to the same test error and codebook usage but
require more training steps. With poor initialization, these
models sometimes end up with suboptimal codes hurting
reconstruction ﬁdelity.
Codebook size also matters, as it sets a limit on channel ca-
pacity through the bottleneck layers. In Table 2, we ﬁnd that
reconstruction error increases considerably when the code-
book size is reduced from 2048 to 256. We also compare
with a model that uses continuous representations without
vector quantization. We can think of this model as using a
vastly large codebook with all encoder embeddings. This
achieves almost perfect reconstruction with negligible spec-
tral error.
When the model is trained with L2 loss only, reconstruc-
tions tend to sound muddy from missing high frequencies,
and this problem is exacerbated as hop size is increased. In
Figure 4, we see that top-level codes trained without spec-
tral loss do not capture much information beyond 2 kHz,
and obtain worse reconstructions (Table 3). However, we
observe that while spectral loss helps encode more infor-
mation, it also adds distortion artifacts which we hear as
scratchy noise.
Lastly, we train a raw audio hierarchical VQ-V AE (Razavi
et al., 2019) and ﬁnd that it is generally difﬁcult to push
information to higher levels. This model is trained twice as
long as the previous models, but middle and top-level recon-
structions as shown in Figure 4 are not capturing much. It is
possible that higher level codes may have collapsed before
bottom level starts to reconstruct the audio well. Making
the bottom layers explicitly model residuals pushed more
information to the top. But, we found separate autoencoders
to be cleaner and more effective.6. Related Work
Generative modeling in deep learning: Generative mod-
els aim to learn the distribution of data by either explicitly
by modeling the distribution or implicitly by constructing
means to sample from it (Goodfellow, 2016). Modeling
the interdependency within high-dimensional data was tra-
ditionally considered extremely difﬁcult, but starting with
Deep Boltzmann Machines (Salakhutdinov & Hinton, 2009),
various kinds of deep generative models have been intro-
duced. Generative Adversarial Networks (GANs) (Good-
fellow et al., 2014) use generator and discriminator net-
works that contest each other to make the generated samples
as indistinguishable as possible from the data, and they
are renowned for their ability to generate high-quality pic-
tures (Zhang et al., 2019b; Brock et al., 2019). Autoregres-
sive generative models such as NADE (Uria et al., 2016),
PixelCNN (Van den Oord et al., 2016), and Transformers
(Vaswani et al., 2017) use the chain rule of probability to
factorize the joint distribution of data into a product of
simpler distributions, and ﬂow-based models (Dinh et al.,
2015; 2017; Rezende & Mohamed, 2015; Kingma & Dhari-
wal, 2018) learn a series of invertible transformations that
maps the data distribution with a simpler one such as a
Gaussian distribution. Autoregressive ﬂows (Papamakarios
et al., 2017; Kingma et al., 2016) combine the two ideas to
achieve faster density estimation or data generation. Varia-
tional autoencoders (V AEs) (Rezende et al., 2014; Kingma
& Welling, 2014) impose a Gaussian prior on the latent
code in an encoder-decoder setup from which data can be
sampled.
Generative models for music: Generative modeling of
symbolic music dates back to more than half a century, when
Hiller Jr & Isaacson (1957) introduced the ﬁrst computer-
generated music based on Markov chains. There exists
a variety of earlier approaches using rule-based systems
(Moorer, 1972), chaos and self-similarity (Pressing, 1988),
cellular automata (Beyls, 1989), concatenative synthesis
(Jehan, 2005), and constraint programming (Anders & Mi-
randa, 2011). More recent data-driven approaches include
DeepBach (Hadjeres et al., 2017) and Coconet (Huang et al.,
2017) which use Gibbs sampling to produce notes in the
style of Bach chorals, MidiNet (Yang et al., 2017) and
MuseGAN (Dong et al., 2018) which use generative ad-
versarial networks, MusicV AE (Roberts et al., 2018) and
HRNN (Wu et al., 2019) which use hierarchical recurrent
networks, and Music Transformer (Huang et al., 2019a)
and MuseNet (Payne, 2019) which use Transformers to au-
toregressively predict MIDI note events. There also have
been a number of approaches for synthesizing music con-
ditioned on symbolic music information, such as NSynth
(Engel et al., 2017) which uses WaveNet-style autoen-
coder, Mel2Mel (Kim et al., 2019) and Wave2Midi2Wave
(Hawthorne et al., 2019) which synthesize music usingJukebox: A Generative Model for Music
WaveNet conditioned on a piano roll representation, and
GanSynth (Engel et al., 2019) which uses generative adver-
sarial networks to produce magnitude spectrograms together
with instananeous frequencies for easier spectrogram inver-
sion. Generative models for music can also be used for
music style transfer, as seen in Midi-V AE (Brunner et al.,
2018) which uses a variational autoencoder to transfer styles
between classical and jazz music, LakhNES (Donahue et al.,
2019) which uses a Transformer architecture to generate
chiptune music, and Universal Music Translator Network
(Mor et al., 2019) which uses a denoising autoencoder that
can disentangle musical style and content.
Sample-level generation of audio: In recent years, a vari-
ety of generative models for raw audio have been introduced.
WaveNet (Oord et al., 2016) performs autoregressive sample-
by-sample probabilistic modeling of raw waveform using a
series of dilated convolutions to exponentially increase the
context length. It can produce realistic audio either uncon-
ditionally or by conditioning on acoustic features or spec-
trograms. The autoregressive nature of WaveNet makes the
sampling notoriously slow, and it uses a categorical distribu-
tion for audio samples which introduces quantization noise.
Parallel WaveNet (Oord et al., 2018) improves upon this
by instead using a mixture of logistics distribution, a con-
tinuous probability distribution, and performing probabil-
ity density distillation which learns a parallel feed-forward
network from a pre-trained autoregressive model, allow-
ing faster sampling of high ﬁdelity audio. ClariNet (Ping
et al., 2019) achieves similar audio quality using a simple
Gaussian distribution instead and thus having a closed-form
loss function, eliminating the need for Monte-Carlo sam-
pling. SampleRNN (Mehri et al., 2017) uses a multi-scale,
hierarchical recurrent neural network with convolutional
upsampling to model long-range complex structures. Wa-
veRNN (Kalchbrenner et al., 2018) uses recurrent neural
networks that operate separately on the most signiﬁcant and
the least signiﬁcant bytes, which can be efﬁciently deployed
in mobile devices while having comparable audio quality to
WaveNet. WaveGlow (Prenger et al., 2019) is a ﬂow-based
model for parallel sample-level audio synthesis, which can
be trained with a straightforward maximum-likelihood esti-
mation and thus is advantageous to the two-stage training
process needed for distillation. Parallel WaveGAN (Ya-
mamoto et al., 2020) and MelGAN (Kumar et al., 2019)
are GAN-based approaches directly modeling audio wave-
forms, achieving similar quality as WaveNet and WaveGlow
models with signiﬁcantly fewer parameters. While the ap-
proaches above serve as sophisticated generative models for
raw audio to be conditioned on a compact and controllable
representation of audio such as Mel spectrograms, Mel-
Net (Vasquez & Lewis, 2019) takes a different approach of
hierarchically generating accurate high-resolution Mel spec-trograms, after which a simple gradient-based optimization
can produce high-ﬁdelity audio.
VQ-V AE: Oord et al. (2017) introduced VQ-V AE, an ap-
proach of downsampling extremely long context inputs to a
shorter-length discrete latent encoding using a vector quan-
tization, and they showed that it can generate both high-
quality images and audio, as well as learn unsupervized
representations of phonemes. Razavi et al. (2019) extended
the above model by introducing a hierarchy of discrete rep-
resentations for images and showed that the resulting model
can learn to separate high-level semantics into the highest
level of discrete codes which have the largest receptive ﬁeld,
while capturing local features like textures in the lower lev-
els with smaller receptive ﬁelds. They used the hierarchical
model to generate high-diversity and high-ﬁdelity images
for the conditional ImageNet and FFHQ datasets. Dieleman
et al. (2018) tried variants of this approach where instead
of a single encoder there are successive encoders that each
further compress the lossy discrete encodings from the previ-
ous levels. A downside of this approach is that information
is lost at each step and requires separate training for each
VQ-V AE level, and it leads to a hierarchy collapse problem.
De Fauw et al. (2019) used AR decoders which are known to
cause the problem of ignoring the latent variables, and they
suggested ways to mitigate it. The feed-forward decoders
from (Razavi et al., 2019) do not suffer from this issue, and
thus we use their approach.
Speech synthesis: Producing natural human voice entails
an understanding of linguistic features, mapping of sounds,
and steerability of expression. Many text-to-speech (TTS)
systems rely on highly engineered features (Klatt, 1980),
carefully curated sound segments (Hunt & Black, 1996),
statistical parametric modeling (Zen et al., 2009), and of-
ten complex pipelines as described in (Arık et al., 2017).
These approaches are fairly involved and produce unnatural
or inarticulate voices. More recent works like Deep V oice
3 (Ping et al., 2018), Tacotron 2 (Shen et al., 2018), and
Char2Wav (Sotelo et al., 2017) learn speech synthesis end-
to-end using sequence-to-sequence architecture (Sutskever
et al., 2014). The design space is vast, but in general, typical
approaches comprise of a bidirectional encoder, a decoder,
and a vocoder to build text representations, audio features,
and the ﬁnal raw waveforms. To generate multiple voices,
text-to-speech models can also condition on the speaker
identity (Oord et al., 2016; Gibiansky et al., 2017; Jia et al.,
2018) as well as text prompt. By learning and manipulat-
ing auxiliary embeddings, models can mimic a new voice
(Arık et al., 2018a; Taigman et al., 2018) at test time. These
methods, however, require labeled data. Ideas like clus-
tering (Dehak et al., 2011), priming (Wang et al., 2018),
and variational autoencoders (Hsu et al., 2019; Akuzawa
et al., 2018) have been used to learn broader styles of speech
and control expressivity in an unsupervised way. There areJukebox: A Generative Model for Music
also works on synthesizing singing by additionally con-
trolling pitch and timbre. Similar to TTS literature, early
works use concatenative methods (Bonada & Serra, 2007)
that join short segments of curated singing, and statistical
parametric methods (Saino et al., 2006; Oura et al., 2010)
which allow modeling of timbre from training data. Both
approaches impose fairly strong assumptions resulting in
noticeable artifacts. (Blaauw & Bonada, 2017) train a neural
TTS model with a parametric vocoder to separate pitch and
timbre which can be controlled at generation time.
7. Future work
While our approach represents a step forward in the ability
to generate coherent long raw audio music samples, we rec-
ognize several directions for future work. Great music gen-
eration should be high quality over all time scales: it should
have a developing musical and emotional structure across
the entire piece, local notes and harmonies that always make
sense, nuanced and appropriate small timbral and textural
details, and audio recording quality that balances and blends
the multiple voices well, and without unwanted noise. We
view our current model as stronger on the mid-range time
scales: often the model generates samples that locally sound
very good, with interesting and diverse harmonies, rhythms,
instruments, and voices. We have frequently been very
impressed how the melody and rhythm generated suits a
particular lyric extremely well. However, while the samples
stay consistent over longer time scales, we notice they don’t
have traditional larger music structures (such as choruses
that repeat, or melodies that have a question and answer
form). Additionally, on the smallest scale, we sometimes
hear audio noise or scratchiness.
Beyond the quality of the samples, we also would look
to diversify the languages and styles the model is able to
generate. Our current model has been trained only on songs
whose primary language as detected by (Sites, 2013) is
English. In the future, we would look to include other
languages and artists. We believe this will be of interest
both for generating strictly in those styles, and because
historically we have seen much creativity and development
coming from unusual blends of existing musical styles.
Finally, we consider it very important that computer music
generation also serves as a tool for human musicians, and
increasingly those interested in music but without formal
training. While we are able to steer our current model some-
what through lyric and midi conditioning, we can imagine
many other possible ways for humans to inﬂuence the gener-
ations, including indicating the mood or dynamic at various
sections, or controlling when drums, singers, or other instru-
ments should play.The current model takes around an hour to generate 1 minute
of top level tokens. The upsampling process is very slow,
as it proceeds sequentially through the sample. Currently it
takes around 8 hours to upsample one minute of top level
tokens. We can create a human-in-the-loop co-composition
process at the top level only, using the VQ-V AE decoders
to get a fast upsampling of the top level tokens to hear a
very rough sense of what the model generates. The top-level
model generates multiple samples, the person picks a fa-
vorite (listening to the rough VQ-V AE decoding), and then
the model continues generating multiple samples continuing
the favorite. This process would be signiﬁcantly improved
with faster generation and Transformer upsampling steps.
Our models have fast parallel evaluation of likelihood but
slow autoregressive sampling. We can instead use a model
with fast parallel sampling but slow autoregressive likeli-
hood evaluation (Kingma et al., 2016), and distill the infor-
mation from our current model into it (Oord et al., 2018).
The distillation works by generating samples from the paral-
lel sampler and evaluating it likelihood and entropy using
the parallel likelihood evaluator, and then optimising the
sampler by minimising the KL divergence of it from our
current model.
8. Conclusion
We have introduced Jukebox, a model that generates raw
audio music imitating many different styles and artists. We
can condition this music on speciﬁc artists and genres, and
can optionally specify the lyrics for the sample. We laid
out the details necessary to train a Hierarchical VQ-V AE to
compress the music effectively into tokens. While previous
work has generated raw audio music in the 20–30 second
range, our model is capable of generating pieces that are
multiple minutes long, and with recognizable singing in
natural-sounding voices.
9. Acknowledgement
We would like to thank John Schulman and Will Guss for
producing and performing novel riffs for our sampling ex-
periments, and Rewon Child, Aditya Ramesh, Ryan Lowe
and Jack Clark for providing feedback for initial drafts of
this paper.
References
Akuzawa, K., Iwasawa, Y ., and Matsuo, Y . Expressive
speech synthesis via modeling expressions with varia-
tional autoencoder. In INTERSPEECH , 2018.
Anders, T. and Miranda, E. R. Constraint programming
systems for modeling music theories and composition.
ACM Computing Surveys (CSUR) , 43(4):1–38, 2011.Jukebox: A Generative Model for Music
Arık, S. Ö., Chrzanowski, M., Coates, A., Diamos, G., Gib-
iansky, A., Kang, Y ., Li, X., Miller, J., Ng, A., Raiman,
J., Sengupta, S., and Shoeybi, M. Deep Voice: Real-time
neural text-to-speech. In International Conference on
Machine Learning , pp. 195–204, 2017.
Arık, S. Ö., Chen, J., Peng, K., Ping, W., and Zhou, Y .
Neural voice cloning with a few samples. In Advances
in Neural Information Processing Systems , pp. 10019–
10029. 2018a.
Arık, S. Ö., Jun, H., and Diamos, G. Fast spectrogram
inversion using multi-head convolutional neural networks.
IEEE Signal Processing Letters , 26(1):94–98, 2018b.
Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.
arXiv preprint arXiv:1607.06450 , 2016.
Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,
Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,
C., et al. Dota 2 with large scale deep reinforcement
learning. arXiv preprint arXiv:1912.06680 , 2019.
Beyls, P. The musical universe of cellular automata. In
International Computer Music Conference , pp. 34–41,
1989.
Blaauw, M. and Bonada, J. A neural parametric singing
synthesizer. In INTERSPEECH , 2017.
Bonada, J. and Serra, X. Synthesis of the singing voice by
performance sampling and spectral models. IEEE signal
processing magazine , 24(2):67–79, 2007.
Brock, A., Donahue, J., and Simonyan, K. Large scale
GAN training for high ﬁdelity natural image synthesis. In
International Conference on Learning Representations ,
2019.
Brunner, G., Konrad, A., Wang, Y ., and Wattenhofer, R.
MIDI-V AE: modeling dynamics and instrumentation of
music with applications to style transfer. In International
Society for Music Information Retrieval Conference , pp.
747–754, 2018.
Child, R., Gray, S., Radford, A., and Sutskever, I. Gen-
erating long sequences with sparse transformers. arXiv
preprint arXiv:1904.10509 , 2019.
De Fauw, J., Dieleman, S., and Simonyan, K. Hierarchi-
cal autoregressive image models with auxiliary decoders.
arXiv preprint arXiv:1903.04933 , 2019.
Dehak, N., Kenny, P. J., Dehak, R., Dumouchel, P., and
Ouellet, P. Front-end factor analysis for speaker veriﬁca-
tion. IEEE Transactions on Audio, Speech, and Language
Processing , 19(4):788–798, 2011.Dieleman, S., van den Oord, A., and Simonyan, K. The chal-
lenge of realistic music generation: modelling raw audio
at scale. In Advances in Neural Information Processing
Systems , pp. 7989–7999, 2018.
Dinh, L., Krueger, D., and Bengio, Y . NICE: Non-linear in-
dependent components estimation. In International Con-
ference in Learning Representations , Workshop, 2015.
Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density esti-
mation using Real NVP. In International Conference in
Learning Representations , 2017.
Donahue, C., Mao, H. H., Li, Y . E., Cottrell, G. W., and
McAuley, J. J. LakhNES: Improving multi-instrumental
music generation with cross-domain pre-training. In In-
ternational Society for Music Information Retrieval Con-
ference , pp. 685–692, 2019.
Dong, H.-W., Hsiao, W.-Y ., Yang, L.-C., and Yang, Y .-H.
MuseGAN: Multi-track sequential generative adversarial
networks for symbolic music generation and accompani-
ment. In Thirty-Second AAAI Conference on Artiﬁcial
Intelligence , 2018.
Engel, J., Resnick, C., Roberts, A., Dieleman, S., Norouzi,
M., Eck, D., and Simonyan, K. Neural audio synthesis
of musical notes with wavenet autoencoders. In Interna-
tional Conference on Machine Learning , pp. 1068–1077,
2017.
Engel, J., Agrawal, K. K., Chen, S., Gulrajani, I., Donahue,
C., and Roberts, A. GANSynth: Adversarial neural au-
dio synthesis. In International Conference on Learning
Representations , 2019.
Gibiansky, A., Arık, S. Ö., Diamos, G., Miller, J., Peng, K.,
Ping, W., Raiman, J., and Zhou, Y . Deep Voice 2: Multi-
speaker neural text-to-speech. In Advances in neural
information processing systems , pp. 2962–2970, 2017.
Goodfellow, I. NIPS 2016 tutorial: Generative adversarial
networks. In Neural Information Processing Systems ,
Tutorial, 2016.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S., Courville, A., and Bengio,
Y . Generative adversarial nets. In Advances in neural
information processing systems , pp. 2672–2680, 2014.
Gupta, C., Yılmaz, E., and Li, H. Automatic lyrics tran-
scription in polyphonic music: Does background music
help? In International Conference on Acoustics, Speech,
and Signal Processing , 2020.
Hadjeres, G., Pachet, F., and Nielsen, F. Deepbach: a steer-
able model for bach chorales generation. In International
Conference on Machine Learning , pp. 1362–1371. JMLR.
org, 2017.Jukebox: A Generative Model for Music
Hawthorne, C., Stasyuk, A., Roberts, A., Simon, I., Huang,
C.-Z. A., Dieleman, S., Elsen, E., Engel, J., and Eck, D.
Enabling factorized piano music modeling and generation
with the MAESTRO dataset. In International Conference
on Learning Representations , 2019.
Hennequin, R., Khlif, A., V oituret, F., and Moussallam, M.
Spleeter: A fast and state-of-the art music source separa-
tion tool with pre-trained models. Late-Breaking/Demo
ISMIR 2019, November 2019. Deezer Research.
Hiller Jr, L. A. and Isaacson, L. M. Musical composition
with a high speed digital computer. In Audio Engineering
Society Convention 9 . Audio Engineering Society, 1957.
Ho, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T.
Axial attention in multidimensional transformers. arXiv
preprint arXiv:1912.12180 , 2019.
Hsu, W.-N., Zhang, Y ., Weiss, R. J., Zen, H., Wu, Y ., Wang,
Y ., Cao, Y ., Jia, Y ., Chen, Z., Shen, J., Nguyen, P., and
Pang, R. Hierarchical generative modeling for control-
lable speech synthesis. In International Conference on
Learning Representations , 2019.
Huang, C. A., Cooijmans, T., Roberts, A., Courville, A. C.,
and Eck, D. Counterpoint by convolution. In Interna-
tional Society for Music Information Retrieval Confer-
ence, pp. 211–218, 2017.
Huang, C.-Z. A., Vaswani, A., Uszkoreit, J., Shazeer, N.,
Simon, I., Hawthorne, C., Dai, A. M., Hoffman, M. D.,
Dinculescu, M., and Eck, D. Music Transformer: Gen-
erating music with long-term structure. In International
Conference on Learning Representations , 2019a.
Huang, Y ., Cheng, Y ., Bapna, A., Firat, O., Chen, D., Chen,
M., Lee, H., Ngiam, J., Le, Q. V ., Wu, Y ., et al. Gpipe:
Efﬁcient training of giant neural networks using pipeline
parallelism. In Advances in Neural Information Process-
ing Systems , pp. 103–112, 2019b.
Hunt, A. J. and Black, A. W. Unit selection in a con-
catenative speech synthesis system using a large speech
database. In IEEE International Conference on Acoustics,
Speech, and Signal Processing Conference , pp. 373–376,
1996.
Jehan, T. Creating music by listening . PhD thesis, Mas-
sachusetts Institute of Technology, School of Architecture
and Planning, Program in Media Arts and Sciences, 2005.
Jia, Y ., Zhang, Y ., Weiss, R., Wang, Q., Shen, J., Ren, F.,
Chen, z., Nguyen, P., Pang, R., Lopez Moreno, I., and
Wu, Y . Transfer learning from speaker veriﬁcation to
multispeaker text-to-speech synthesis. In Advances in
Neural Information Processing Systems , pp. 4480–4490.
2018.Kalchbrenner, N., Elsen, E., Simonyan, K., Noury, S.,
Casagrande, N., Lockhart, E., Stimberg, F., Oord, A.,
Dieleman, S., and Kavukcuoglu, K. Efﬁcient neural au-
dio synthesis. In International Conference on Machine
Learning , pp. 2410–2419, 2018.
Kim, J. W., Bittner, R., Kumar, A., and Bello, J. P. Neural
music synthesis for ﬂexible timbre control. In IEEE In-
ternational Conference on Acoustics, Speech and Signal
Processing , pp. 176–180, 2019.
Kingma, D. P. and Dhariwal, P. Glow: Generative ﬂow
with invertible 1x1 convolutions. In Advances in Neural
Information Processing Systems , pp. 10215–10224, 2018.
Kingma, D. P. and Welling, M. Auto-encoding variational
bayes. In International Conference on Learning Repre-
sentations , 2014.
Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X.,
Sutskever, I., and Welling, M. Improved variational in-
ference with inverse autoregressive ﬂow. In Advances in
neural information processing systems , pp. 4743–4751,
2016.
Klatt, D. H. Software for a cascade/parallel formant synthe-
sizer. Journal of the Acoustical Society of America , 67
(3):971–995, 1980.
Kumar, K., Kumar, R., de Boissiere, T., Gestin, L., Teoh,
W. Z., Sotelo, J., de Brébisson, A., Bengio, Y ., and
Courville, A. C. MelGAN: Generative adversarial net-
works for conditional waveform synthesis. In Advances
in Neural Information Processing Systems , pp. 14881–
14892, 2019.
LyricWiki. URL https://lyrics.fandom.com/
wiki/LyricWiki .
Mehri, S., Kumar, K., Gulrajani, I., Kumar, R., Jain, S.,
Sotelo, J., Courville, A., and Bengio, Y . SampleRNN: An
unconditional end-to-end neural audio generation model.
InInternational Conference on Learning Representations ,
2017.
Moorer, J. A. Music and computer composition. Communi-
cations of the ACM , 15(2):104–113, 1972.
Mor, N., Wolf, L., Polyak, A., and Taigman, Y . Autoencoder-
based music translation. In International Conference on
Learning Representations , 2019.
Oord, A. v. d., Dieleman, S., Zen, H., Simonyan, K.,
Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A.,
and Kavukcuoglu, K. WaveNet: A generative model for
raw audio. arXiv preprint arXiv:1609.03499 , 2016.Jukebox: A Generative Model for Music
Oord, A. v. d., Vinyals, O., and Kavukcuoglu, K. Neural
discrete representation learning. In Neural Information
Processing Systems , 2017.
Oord, A. v. d., Li, Y ., Babuschkin, I., Simonyan, K., Vinyals,
O., Kavukcuoglu, K., van den Driessche, G., Lockhart, E.,
Cobo, L., Stimberg, F., Casagrande, N., Grewe, D., Noury,
S., Dieleman, S., Elsen, E., Kalchbrenner, N., Zen, H.,
Graves, A., King, H., Walters, T., Belov, D., and Hassabis,
D. Parallel WaveNet: Fast high-ﬁdelity speech synthesis.
InInternational Conference on Machine Learning , pp.
3918–3926, 2018.
Oura, K., Mase, A., Yamada, T., Muto, S., Nankaku, Y .,
and Tokuda, K. Recent development of the HMM-based
singing voice synthesis system – Sinsy. 2010.
Papamakarios, G., Pavlakou, T., and Murray, I. Masked
autoregressive ﬂow for density estimation. In Advances in
Neural Information Processing Systems , pp. 2338–2347,
2017.
Payne, C. Musenet. OpenAI blog , 2019. URL https:
//openai.com/blog/musenet .
Ping, W., Peng, K., Gibiansky, A., Arik, S. O., Kannan,
A., Narang, S., Raiman, J., and Miller, J. Deep Voice
3: 2000-speaker neural text-to-speech. In International
Conference on Learning Representations , 2018.
Ping, W., Peng, K., and Chen, J. Clarinet: Parallel wave
generation in end-to-end text-to-speech. In International
Conference on Learning Representations , 2019.
Prenger, R., Valle, R., and Catanzaro, B. WaveGlow: A
ﬂow-based generative network for speech synthesis. In
IEEE International Conference on Acoustics, Speech and
Signal Processing , pp. 3617–3621, 2019.
Pressing, J. Nonlinear maps as generators of musical design.
Computer Music Journal , 12(2):35–46, 1988.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and
Sutskever, I. Language models are unsupervised multitask
learners.
Razavi, A., van den Oord, A., and Vinyals, O. Generating
diverse high-ﬁdelity images with vq-vae-2. In Advances
in Neural Information Processing Systems , pp. 14837–
14847, 2019.
Rezende, D. and Mohamed, S. Variational inference with
normalizing ﬂows. In International Conference on Ma-
chine Learning , pp. 1530–1538, 2015.
Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic
backpropagation and approximate inference in deep gen-
erative models. In International Conference on Machine
Learning , pp. 1278–1286, 2014.Roberts, A., Engel, J., Raffel, C., Hawthorne, C., and Eck,
D. A hierarchical latent vector model for learning long-
term structure in music. In International Conference on
Machine Learning , pp. 4364–4373, 2018.
Saino, K., Zen, H., Nankaku, Y ., Lee, A., and Tokuda,
K. An HMM-based singing voice synthesis system. In
INTERSPEECH , 2006.
Salakhutdinov, R. and Hinton, G. Deep boltzmann machines.
InArtiﬁcial intelligence and statistics , pp. 448–455, 2009.
Shen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N.,
Yang, Z., Chen, Z., Zhang, Y ., Wang, Y ., Skerrv-Ryan,
R., Saurous, R. A., Agiomvrgiannakis, Y ., and Wu, Y .
Natural TTS synthesis by conditioning wavenet on mel
spectrogram predictions. In IEEE International Confer-
ence on Acoustics, Speech and Signal Processing , pp.
4779–4783, 2018.
Sites, D. Compact language detector 2. 2013. URL https:
//github.com/CLD2Owners/cld2 .
Sotelo, J., Mehri, S., Kumar, K., Santos, J. F., Kastner, K.,
Courville, A. C., and Bengio, Y . Char2Wav: End-to-
end speech synthesis. In International Conference on
Learning Representations , 2017.
Sturmel, N. and Daudet, L. Signal reconstruction from stft
magnitude: A state of the art. International Conference
on Digital Audio Effects, DAFx , 2011.
Sutskever, I., Vinyals, O., and Le, Q. V . Sequence to se-
quence learning with neural networks. In Advances in
neural information processing systems , pp. 3104–3112,
2014.
Taigman, Y ., Wolf, L., Polyak, A., and Nachmani, E.
V oiceLoop: V oice ﬁtting and synthesis via a phonological
loop. In International Conference on Learning Represen-
tations , 2018.
Uria, B., Côté, M.-A., Gregor, K., Murray, I., and
Larochelle, H. Neural autoregressive distribution esti-
mation. The Journal of Machine Learning Research , 17
(1):7184–7220, 2016.
Van den Oord, A., Kalchbrenner, N., Espeholt, L., Vinyals,
O., Graves, A., et al. Conditional image generation with
pixelcnn decoders. In Advances in neural information
processing systems , pp. 4790–4798, 2016.
Vasquez, S. and Lewis, M. MelNet: A generative model
for audio in the frequency domain. arXiv preprint
arXiv:1906.01083 , 2019.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-
tion is all you need. In Advances in Neural Information
Processing Systems , pp. 5998–6008, 2017.Jukebox: A Generative Model for Music
Wang, Y ., Stanton, D., Zhang, Y ., Skerry-Ryan, R., Batten-
berg, E., Shor, J., Xiao, Y ., Ren, F., Jia, Y ., and Saurous,
R. A. Style Tokens: Unsupervised style modeling, control
and transfer in end-to-end speech synthesis. In Interna-
tional Conference on Machine Learning , 2018.
Wu, J., Hu, C., Wang, Y ., Hu, X., and Zhu, J. A hierarchical
recurrent neural network for symbolic melody generation.
IEEE Transactions on Cybernetics , 2019.
Xie, S., Girshick, R., Dollár, P., Tu, Z., and He, K. Aggre-
gated residual transformations for deep neural networks.
InIEEE Conference on Computer Vision and Pattern
Recognition , pp. 1492–1500, 2017.
Yamamoto, R., Song, E., and Kim, J.-M. Parallel Wave-
GAN: A fast waveform generation model based on gener-
ative adversarial networks with multi-resolution spectro-
gram. In International Conference on Acoustics, Speech,
and Signal Processing , 2020.
Yang, L., Chou, S., and Yang, Y . Midinet: A convolutional
generative adversarial network for symbolic-domain mu-
sic generation. In International Society for Music Infor-
mation Retrieval Conference , pp. 324–331, 2017.
Zen, H., Tokuda, K., and Black, A. W. Review: Statistical
parametric speech synthesis. Speech Communication , 51
(11):1039–1064, 2009.
Zhang, H., Dauphin, Y . N., and Ma, T. Fixup initialization:
Residual learning without normalization. In International
Conference on Machine Learning , 2019a.
Zhang, H., Goodfellow, I., Metaxas, D., and Odena, A.
Self-attention generative adversarial networks. In Inter-
national Conference on Machine Learning , 2019b.Jukebox: A Generative Model for Music
A. Scalable Transformer
We make the Sparse Transformer (Child et al., 2019) more
scalable and easier to implement by a few small changes.
We implement a simpler attention pattern that has the same
performance without needing custom kernels to implement.
We simplify the initialization by using the same initalization
scale in the whole model without rescaling the weights
based on fan-in and depth, and we optimize the memory
footprint with fully half-precision training, i.e. storing the
model weights, gradients and the optimizer states in half
precision and performing computations in half precision as
well. To cope with the narrower dynamic range of the fp16
format, we use dynamic scaling of the gradient and Adam
optimizer states.
Axis-aligned attention patterns: The Sparse Transformer
(Child et al., 2019) sparsiﬁes the attention pattern by
reshaping the input sequence into a 2-D sequence of
shape (blocks, block length )to use factorized attention.
They observe that the strided attention pattern works
best for images and audio because it does not have the
state bottleneck of the ﬁxed attention. However, their
implementation require specialized CUDA kernels. We
can obtain a similar pattern by doing masked row, masked
column, and unmasked previous-row attention. While
the masked row captures the local context, the masked
column and unmasked previous-row attention captures
the context of all previous rows. We observe the same
computational speed as well as training loss with this
pattern. Each of these can be implemented directly as a
dense attention by transposing or slicing the input sequence
along appropriate axes, and thus do not require special
CUDA kernels to implement. This can be easily extended
to video too. Complementary to our work, a similar
pattern was introduced in (Ho et al., 2019) where they also
used axis-aligned attention but instead used a two-stream
architecture.
Half-precision parameters and optimizer state with dy-
namic scaling: To allow training large models, (Child et al.,
2019) uses recompute with gradient checkpointing, per-
forms computations using half precision activations and
gradients, and uses dynamic loss scaling. While this speeds
up training on V olta cores, one still has a high memory us-
age from storing the parameters and Adam state in full ﬂoat
precision. To scale our models further, we store our matmul
parameters and their Adam state in half precision, thus halv-
ing our memory usage. We use a single parameter sto set the
scale of all weights and initialize all matmul and input/out-
put embeddings3toN(0;s), and position embeddings to
N(0;2s). The initialization ensures all parameters are in a
similar dynamic range, and allows us to train in half preci-
3We share the input and output embedding
Masked
Row AttentionMasked
Column AttentionUnmasked
Previous-Row Attention(a) Three axis-aligned attention patterns are sparse attention pat-
terns that allow autoregressive generative modeling while only
using simple Python-level array manipulation. Masked row and
column attention patterns use autoregressive masks, whereas un-
masked previous-row attention is fully visible.
        
(b) Combining two of the attention patterns, each position can
attend to any of the previous positions, while not causing a state
bottleneck as in ﬁxed sparse attention (Child et al., 2019).
Figure 6. Axis-aligned attention patterns
sion completely without loss in training performance. For
the Adam state tensors (m_t, v_t) we do dynamic scal-
ing. For each iteration and for every parameter, we rescale
its state tensors before casting so that their maximum corre-
sponds to the maximum value of the ﬂoat16 range, thus max-
imizing the use of the ﬂoat16 range. Thus, we store the state
m_t as the tuple (scale, (m_t/scale).half()) ,
where scale = m_t.max()/float16.max() , and
similarly for v_t. The above lets us ﬁt models of size 1B
parameters into memory for our large context of 8192 to-
kens. To train even larger models, we use GPipe (Huang
et al., 2019b).Jukebox: A Generative Model for Music
B. Experimental details
B.1. Music VQ-V AE
We have three separate raw audio VQ-V AEs to produce dis-
crete codes at varying hop sizes for the bottom, middle, and
top priors. All autoencoders comprise non-causal, dilated
1-D convolutions, and are trained independently using non-
autoregressive reconstruction losses. Basic building blocks
in these networks share the same architecture, as shown in
Figure 7. Each encoder block consists of a downsampling
convolution, a residual network, and a 1D convolution with
a kernel size of 3. Dilation is grown by a factor of 3 in
these residual networks to increase the receptive ﬁeld. The
decoder block mirrors this exactly with a 1D convolution
with the kernel size of 3, a residual network with dilation
contracting across depth, and an upsampling transposed con-
volution. Here, all resampling convolutions use a kernel size
of 4 and stride 2 so that each building block changes the
hop length by a factor of 2. To get higher compression in
time, we simply stack more of these blocks. For example,
using seven blocks yields a hop length of 128 for the top
level autoencoder.
Each residual network has four residual blocks in the mid-
dle and top VQ-V AEs resulting in a receptive ﬁeld of 120
ms and 480 ms for the respective discrete tokens. Because
increasing the residual depth helped improve reconstruction
quality slightly, we doubled the number of residual blocks
for the bottom level. This dramatically increases the recep-
tive ﬁeld to about 2 seconds per code but the actual receptive
ﬁeld is mostly local.
We also experimented with having a single decoder and
modeling the residuals to separate out learned representa-
tions as in (Razavi et al., 2019), hoping upsampling priors
would simply ﬁll in local musical structure. However, push-
ing information to the top level was quite challenging as the
bottommost level reconstructs almost perfectly early on in
training. When we add auxiliary objectives to encourage
the top to be used more, the top-level codes add serious
distortions to the ﬁnal output. A similar challenge is shown
in (Dieleman et al., 2018).
B.2. Music Priors and Upsamplers
Architectural details of our music prior and upsampler mod-
els are depicted in Figure 8. They perform autoregressive
modeling of tokens at each level, conditioned on informa-
tion such as artist and genre, as well as the tokens from the
upper level in the case of the upsamplers (Figure 8a). Each
artist and genre are learned as embedding vectors, whose
sum is provided as the very ﬁrst token in each sequence.
In addition, positional embedding is learned as a function
of each position’s absolute and relative timing in the dura-
tion of the song. In upsampler models, upper-level tokens
×L
Conv1D×DDilated
Conv1DConv1D
+ x t h t(a) The encoder compresses the raw audio input into a sequence
of embeddings. The length of this latent representation relative
to the raw audio duration determines the amount of compression,
and is an important factor for the trade-off between ﬁdelity and
coherence.
Gradient PassthroughNearest-Neighbor
Searchz tCodebook
h t e z tCodebook
Lookup
(b) The bottleneck takes the sequence of embeddings from the
encoder and maps it into a sequence of code vectors from the
codebook. This sequence of code indices is used as a discrete
representation to be modeled by the priors. Larger codebooks
improve ﬁdelity but may be more difﬁcult to compress.
Conv1D×L
×DDilated
Conv1DConv1D
+T ransposed
Conv1De z t x ̂t
(c) The decoder reconstructs the raw audio from latent represen-
tations. It is a mirror of the encoder where dilations constracts
by a factor of 3 down to 1 at the last block. The ﬁnal Conv1D
projects to the desired number of audio channels and also acts as a
smoothing operation after a sequence of transposed convolutions.
Figure 7. Components of the VQ-V AE model
are upsampled by the conditioner network, using WaveNet-
style dilated convolutions followed by a transposed 1-D
convolutional layer (Figure 8b).
When the model is trained on lyrics, the top-level prior takes
lyrics data corresponding to each audio segment and uses
them to train an encoder-decoder Transformer as shown in
Figure 8c. All transformer stacks use sparse self-attention
layers with the three factorized attention types (row, column,
and previous-row) repeating, and encoder-decoder attention
layers, when present, are interleaved with the other attention
types. Each layer consists of residual connections of an
attention and an MLP feedforward network, each prepended
by layer normalization (see Figure 8d).Jukebox: A Generative Model for Music
Artist & GenreConditioner
z 1: TScalable T ransformerUpper-Level T okens
Time Embedding 
LyricsTiming DataNot Used in the T op Level
							z 1: T– 1
(a) The structure of our prior models, performing next-token prediction at each
level. The Transformer takes the embeddings of the tokens z1:T 1prepended by
the sum of the artist and genre embeddings, in addition to the time embedding
that encodes relative and absolute timing of the segments in the duration of the
song. The upsampler priors additionally take the tokens from the upper level,
which are fed to the conditioner network and added to the input sequence. The
top-level prior takes lyrics as conditioning information as well (see Figure 8c).
×D
Dilated Conv1D
Conv1D
+
Transposed Conv1DToken Embedding(b) The conditioner network takes the tokens from
the upper level, and their embedding vectors go
through non-causal WaveNet-like layers with in-
creasingly dilated convolutions. The transposed 1-D
convolution upsamples the sequence to the higher
temporal resolution of the current level.
Lyrics
Row Attention Layer
Column Attention Layer
Previous-Row Attention Layer
Row Attention Layer
Column Attention Layer
Previous-Row Attention Layer
⋮
Row Attention Layer
Column Attention Layer
Previous-Row Attention LayerLyrics T oken Embedding
Next-T oken PredictionLyrics T oken EmbeddingRow Attention Layer
Column Attention Layer
Previous-Row Attention LayerVQ Code Embedding
Next-T oken PredictionVQ Code EmbeddingVQ Codes
×6
Row Attention Layer
Column Attention Layer
Previous-Row Attention Layer×3
⋮Encoder-Decoder Attention Layer
Encoder-Decoder Attention Layer
Row Attention Layer
Column Attention Layer
Previous-Row Attention LayerEncoder-Decoder Attention Layer
×3⋮Only in the T op Level
Encoder-Decoder Attention Layer
(c) The Scalable Transformer architecture, shown with the lyrics Transformer used in the
top-level prior. The Transformer layers use the three factorized attention types alternatingly,
i.e. repeating row, column, and previous-row attentions. In the top-level prior, the VQ
Transformer additionally includes interleaved encoder-decoder attention layers that apply
lyrics conditioning by attending on the activation of the last encoder layer.
Layer Norm
Attention
Layer Norm
MLP+
+Encoder
Features(d) Each Transformer layer is a resid-
ual attention block, which performs
two residual operations, attention and
MLP, each prepended with layer nor-
malization. Depending on the layer’s
type, it uses either one of the three fac-
torized attentions or encoder-decoder
attention taking the lyrics features
from the encoder.
Figure 8. Detailed architecture of the music prior and upsampler modelsJukebox: A Generative Model for Music
B.3. Hyperparameters
For all Transformers’ residual blocks, we use MLP blocks
with the same width as the model width, and attention blocks
with queries, keys, and values with width 0.25 times the
model width. For all convolutional residual blocks, we use
convolutions with same channels as the model width.
Sample rate 44100
Sample length 393216
Hop lengths 8, 32, 128
Embedding width 64
Residual block width 64, 32, 32
Residual blocks (per 2x downsample) 8, 4, 4
Conv ﬁlter size 3
Conv channels 32
Dilation growth rate 3
Commit weight  0.02
Codebook EMA  0.99
Codebook size 2048
Spectral loss STFT bins 2048, 1024, 512
Spectral loss STFT hop length 240, 120, 50
Spectral loss STFT window size 1200, 600, 240
Initialization scale 0.02
Batch size 256
Training steps 384618
Learning rate 0.0003
Table 4. VQ-V AE hyperparameters
1B upsamplers
Sample length 262144, 65536
Context length 8192
Transformer width 1920
Transformer layers 72
Attention heads 1
Factorized attention shape (128, 64)
Conditioner residual block width 1024
Conditioner residual blocks 16
Conditioner conv ﬁlter size 3
Conditioner conv channels 1024
Conditioner dilation growth rate 3
Conditioner dilation cycle 8
Initialization scale 0.004, 0.008
Batch size 192, 184
Training steps 265000, 279000
Learning rate 0.0003
Adam2 0.95
Weight decay 0.01
Table 5. Middle- and bottom-level upsampler hyperparameters5B prior
Sample length 1048576
Context length 8192
Transformer width 4800
Transformer self-attention layers 72
Attention heads 8
Factorized attention shape (128, 64)
Lyrics encoder tokens 512
Lyrics encoder width 1280
Lyrics encoder layers 18
Lyrics encoder attention heads 4
Lyrics encoder factored attention shape (32, 16)
Encoder-Decoder attention layers 7
Initialization scale 0.002
Encoder initialization scale 0.014
Batch size 512
Training steps 310500
Learning rate 0.00015
Adam2 0.925
Weight decay 0.002
Table 6. Top-level prior hyperparametersJukebox: A Generative Model for Music
B.4.t-SNE Plot of Artists
The Beatles
George Harrison
Paul McCartney
John Lennon
Ringo Starr
Cheap Trick
Aerosmith
QueenStatus QuoELORod StewartFleetwood Mac
Neil Young
Eagles
Pearl JamThe Beatles
George Harrison
Paul McCartney
John Lennon
Ringo Starr
Cheap Trick
Aerosmith
QueenStatus QuoELORod StewartFleetwood Mac
Neil Young
Eagles
Pearl JamArianaGrande
P!nkEdSheeranTheWeeknd
TheBeatles
PinkFloyd
LinkinParkTheBeachBoysBuckOwens
EddyArnold
BillyRayCyrusShaggySeanPaulJimmyCliffBarringtonLevy
DeanMartin
MartyRobbinsHowardShoreTonyBennett
NeilDiamondCabCalloway
HenryMancini
VanMorrisonDinahWashingtonFatsDomino
HankSnow
TheSmashingPumpkinsRaminDjawadiNinaSimoneDonnaSummer
TheCureLouRawls
MigosVangelisNatalieCole
T.Pain
KanyeWestBessieSmithRayNoble
BobbyBlandTheMillsBrothers
AkonLouisPrima
O.S.T.R.LonnieJohnson
AndreaBocelliThePlattersBarryWhite
LutherVandross
YannTiersen
FranzSchubert
JohannSebastianBachGlennGould
Yo-YoMaGarrickOhlssonWalterGieseking
PopRock
ReggaeR&BR&B Soul
Hip Hop
 CountryBlues
ClassicalJazz
Soundtrack
Figure 9. t-SNE of (artist, genre) embedding. The overall clustering shows very clearly how genres are related to one another. The
broadest of all, pop, is situated in the middle of rock, country, blues, hip hop, and many more. Soundtrack and classical form their own
island. Within a genre, we see a similar trend among artists. John Lennon, Paul McCartney, George Harrison and Ringo Starr are clustered
around The Beatles. Cheap Trick which has a number of Beatles covers is also found near. Because we are showing only about 400 artists
here, not all neighboring artists may be related. For an interactive version, we point to our blog post.NEURAL AUDIO FINGERPRINT FOR HIGH-SPECIFIC AUDIO RETRIEV AL
BASED ON CONTRASTIVE LEARNING
Sungkyun Chang1, Donmoon Lee1,2, Jeongsoo Park1, Hyungui Lim1,
Kyogu Lee2, Karam Ko3, and Yoonchang Han1
1Cochlear.ai,2Seoul National University,3SK Telecom
ABSTRACT
Most of existing audio ﬁngerprinting systems have limitations to be
used for high-speciﬁc audio retrieval at scale. In this work, we gen-
erate a low-dimensional representation from a short unit segment of
audio, and couple this ﬁngerprint with a fast maximum inner-product
search. To this end, we present a contrastive learning framework
that derives from the segment-level search objective. Each update in
training uses a batch consisting of a set of pseudo labels, randomly
selected original samples, and their augmented replicas. These repli-
cas can simulate the degrading effects on original audio signals by
applying small time offsets and various types of distortions, such
as background noise and room/microphone impulse responses. In
the segment-level search task, where the conventional audio ﬁnger-
printing systems used to fail, our system using 10x smaller storage
has shown promising results. Our code and dataset are available at
https://mimbres.github.io/neural-audio-fp/ .
Index Terms —acoustic ﬁngerprint, self-supervised learning,
data augmentation, music information retrieval
1. INTRODUCTION
Audio ﬁngerprinting is a content summarization technique that links
short snippets of unlabeled audio contents to the same contents in the
database [1]. The most well-known application is the music ﬁnger-
printing system [1–7] that enables users to identify unknown songs
from the microphone or streaming audio input. Other applications
include detecting copyrights [3], deleting duplicated contents [8],
monitoring broadcasts [1, 9], and tracking advertisements [10].
General requirements for audio ﬁngerprinting system are dis-
criminability over a huge number of other ﬁngerprints, robustness
against various types of acoustic distortions, and computational efﬁ-
ciency for processing large-scale database. To achieve these require-
ments, most of conventional approaches [1–6, 11] employed a nov-
elty function to extract sparse representations of spectro-temporal
features from a pre-deﬁned audio window. These sparse represen-
tations, or acoustic landmarks [5], used to be coupled with binary
hashing algorithms [1, 2, 12] for scalable search in hamming space.
Still, the representation learning approach to audio ﬁngerprint-
ing has not been discovered well. Now-playing [7] has been pio-
neering work in the direction. They trained a neural network using
semi-hard triplet loss, which derived from face recognition [13]. In
their setup [7], Now-playing could identify songs within 44 h long
audio database. In our benchmark, we replicate this semi-hard triplet
approach and compare it with our work in a new setup: high-speciﬁc
audio retrieval in a 180 times larger database.
We present a neural audio ﬁngerprinter for robust high-speciﬁc
audio retrieval based on contrastive learning. Our ﬁngerprinting
model in Figure 1 differs from the prior works in three key aspects:
Fig. 1 . Overview of the neural audio ﬁngerprinter. We generate
segment-wise embeddings zt2Z that can represent a unit segment
of audio from the acoustic features Sat time step t. In our frame-
work, each segment can be searched by maximum inner-product.
• Prior works [1–7, 11] have focused on song-level audio retrieval
from a music excerpt; we challenge a high-speciﬁc audio search
by allowing miss-match less than 250 ms from a few seconds in-
put.
• We introduce the contrastive learning framework for simulating
maximum inner-product search (MIPS) in mini-batch.
• We employ various types of data augmentation methods for gen-
erating acoustic distractors and show their beneﬁts to training a
robust neural audio ﬁngerprinter.
2. NEURAL AUDIO FINGERPRINTER
Our neural audio ﬁngerprinter in Figure 1 transforms and maps the
segment-level acoustic features into L2-normalized space, where the
inner-product can measure similarities between segments. It consists
of a pre-processor and neural networks.
As a ﬁrst step, input audio Xis converted to time-frequency
representationS. It is then fed into convolutional encoder f(:)
which is based on the previous study [7]. Finally, L2-normalization
is applied to its output through a linear projection layer g(:). Thus,
we employgf:S7!Zdas a segment-wise encoder that trans-
formsSinto d-dimensional ﬁngerprint embedding space Zd. The
d-dimensional output space Zdalways belongs to Hilbert space
L2(Rd): the cosine similarity of a pair unit such as cos (za;zb)be-arXiv:2010.11910v4  [cs.SD]  10 Feb 2021Fig. 2 . Illustration of the contrastive prediction task in Section 2.1.
(left) Batch size N= 6. We prepare N=2pairs of original/replica.
The same shapes with solid/dashed lines represent the positive pair
of original/replica, respectively. (right) Each element in the matrix
represents pairwise similarity. In each row, a prediction task can
be deﬁned as classifying a positive pair (one of the orange squares)
against the negative pairs (green or purple squares) in the same row.
comes inner-product zT
azb, and due to its simplicity, L2projection
has been widely adopted in metric learning studies [7, 13, 14].
Thegf(:)described here can be interpreted as a reorganization
of the previous audio ﬁngerprinting networks [7] into the common
form employed in self-supervised learning (SSL) [14–17]. However,
our approach differs from the typical SSL that throws g(:)away be-
fore ﬁne-tuning for the target task: we maintain the self-supervised
g(:)up to the ﬁnal target task.
2.1. Contrastive learning framework
As mentioned earlier, we can use the inner-product as a measure of
similarity between zt2 Zdfor any time step t. Without losing
generality, searching the most similar point (*) of database V = fvig
for a given query qinZdspace can be formulated as maximum inner
product search (MIPS), v
i:=arg maxi(q>vi).
We simulate MIPS in a mini-batch setup that takes into account
various acoustic distortions and input frame mismatches occurring
in the ﬁngerprint task. A mini-batch with the size of Nconsists of
N=2pairs offsorg;srepg.sorgis the time-frequency representation
of sampled audio and srepis the augmented replica of sorg, where
srep=M(sorg).Mis an ordered augmentation chain that con-
sists of multiple augmentors with the random parameter set for
each replica. In this conﬁguration, the indices of original examples
are always odd, and that of replicas are even. Therefore, the batch-
wise output of fg(s)can befzorg
2k 1;zrep
2kg2=N
k=1.
We give each k-th example a chance to be an anchor (or a query
in MIPS) to be compared with all other examples excluding itself in
the batch. We calculate the pairwise inner-product matrix between
all elements in the batch fzigN
i=1asa(i;j) =zT
izjfor8i;j2
f1;2;:::;Ngas Figure 2. Then, we deﬁne the contrastive prediction
task for a positive pair of examples (i;j)as:
`(i;j) = logexp(ai;j=)PN
k=11(k6=i)exp(ai;j=): (1)
1(:)2f0;1gis an indicator function that returns 1iff(:)is true,
and >0denotes the temperature [18] parameter for softmax. We
employ Equation 1 to replace MIPS from the property: computing
the top-k(k=1 in our setup) predictions in the softmax function isAlgorithm 1: Training of neural audio ﬁngerprinter
Conﬁg: even number of batch size N, temperature 
Variables: inputs, representation z2Rd
Augmentor:M(:)with parameters 
Nets: encoderf(:),L2projection layer g(:)
1foreach sampled mini-batch fskgN=2
k=1do
2 for8k2f1;:::;N= 2gdo
3zorg
k=gf(sk)
4zrep
k=gf(M(sk))
5z=fzorg
1;zrep
1;:::;zorg
N=2;zrep
N=2g
6 for8i2f1;:::;Ngand8j2f1;:::;Ngdo
7ai;j=z>
izj /*Pairwise similarity */
8`(i;j) =NTxent (ai;j;) /*Eq.(1)*/
9 Updatef;gto minimizeL1
NNX
i=1`/*Eq.(2)*/
10return ﬁngerprinter gf(:)
equivalent to the MIPS. A similar approach is found in [19]. The
total lossLaverageslacross all positive pairs, both (i;j)and(j;i):
L=1
NNX
k=1[`(2k 1;2k);`(2k;2k 1)]: (2)
Updating rules are summarized in Algorithm 1.
It is worth comparing our approach to SimCLR [14] for visual
representation. Our approach differs from SimCLR on how to con-
struct positive pairs. We use foriginal, replicag, whereas SimCLR
usesfreplica, replicagfrom the same original source. In our case,
the anchor is already given because the database will always store
the clean source, so it can be more important to learn the consistent
relation between the original and its replica over all other negatives.
2.2. Sequence search
Our model trained by simulating MIPS is optimized for segment-
level search. In the case of searching for a query sequence fQL
i=0g
consisting of Lconsecutive segments: We ﬁrst gather the top k
segment-level search results indices Iqifor eachqifrom the DB.
The offset is then compensated by I0
qi=Iqi i. The set of candi-
date indices c2Cis determined by taking unique elements of I0
qi.
The sequence-level similarity score is the sum of all segment-level
similarities from the segment index range [c;c+L], and the index
with the highest score is the output of the system.
3. EXPERIMENTAL SETUP
3.1. Dataset
The main experiment in Table 3 is reproduceable with the following
three data sets, which are isolated from each other.
• Train (10K-30s): A subset of the fmamedium [20] consisting of
30 s audio clips from a total of 10K songs.
• Test-Dummy-DB (100K-full-db): a subset of the fmafull [20]
consisting of about 278 s audio clips from a total of 100K songs.
We scale the search experiment with this.
• Test-Query/DB (500-30s): Test-DB is another subset of the
fmamedium , which is 500 audio clips of 30 s each. Test-Query
was synthesized using Test-DB as directed in Section 3.5.Table 1 . Fingerprinter (FP) network structure in Section 3.3.
SCo i
ks(:) :=ReLUCLNCCo i
k0s0CReLUCLNCCo i
ks(:)
f(:) :=SCh h
32CSCh 4d
32CSC4d 4d
32CSC4d 2d
32C
SC2d 2d
32CSC2d d
32CSCd d
32CSCd 1
32(:)
g(:) :=L2CConcat CC1 u
11CELUCCu v
11CSplith=d(:)
FP:=gCf(input :=st)
3.2. Data pipeline with augmentation chain
A batch consists of fxorg;xrepgpairs. Eachxrepis generated from its
corresponding xorgthrough augmentation steps as following order:
• Time offset modulation: To simulate possible discrepancies in real
world search scenarios, we deﬁne positive examples as 1 s audio
clips with an offset of up to 200 ms. We ﬁrst sample 1.2 s of
audio and thenfxorg;xrepgare chosen by random start positions.
• Background mixing: A randomly selected noise in the SNR range
of [0, 10] dB is added to the audio to reﬂect the actual noise.
The noise dataset consists of 4.3 h of a subset of AudioSet [21]
and 2.3 h of pub and cafe noise recorded by us. The AudioSet
was crawled within subway ,metro , and underground tags with no
music-related tags. Each dataset is split into 8:2 for train/test.
• IR ﬁlters: To simulate the effect of diverse spatial and microphone
environments, microphone and room impulse response (IR) are
sequentially applied by convolution operation. Public microphone
[22] and spacial [23] IR dataset are split into 8:2 for train/test.
• Cutout [24] and Spec-augment [25] are applied after extracting
log-power Mel-spectrogram features, such that fsorg;srepg. Un-
like other augmentations, we uniformly apply a batch-wise ran-
dom mask to all examples in a batch including sorg. The size and
position of each rectangle/vertical/horizontal mask is random in
the range [1/10, 1/2] the length of each time/frequency axis.
3.3. Network structure
In Table 1, a space-saving notation Co i
ksdenotes Conv2d with input
channeli, output channel o, kernel size 1k, and stride 1s. The
k0ands0denote rotation as k1ands1.Splith=dsplits input
dimensionhintodparts of each output dimension v=h=d.gCf(:)
isg(f(:)). The network parameters fd;h;u;vgare in Table 2.
•Convolutional encoder f(:):f(:)takes as input a log-power
Mel-spectrogram stwith a time step trepresnting 1s audio cap-
tured by 50% overlapping window. f(:)consists of several blocks
containing spatially separable convolution (SC) [26] followed by
a layer normalization (LN) [27] and a ReLU activation.
•L2projection layer g(:): We take the split-head from the input
embeddings and pass it through the separate Linear-ELU-Linear
layers for each split as in previous studies [7, 28]. After concate-
nating the multi-head outputs, we apply L2-normalization.
3.4. Implementation details
The replication of Now-playing and our work shared the short-time
Fourier transform (STFT) settings listed in in Table 2. Note that, due
to ambiguity in the previous study [7], the STFT parameters were
set by us. We trained Now-playing using online semi-hard triplet
loss [13] with the margin m= 0:4and batch size N= 320 .Table 2 . Shared conﬁgurations for experiments
Parameter Value
Sampling rate 8,000 Hz
STFT window function Hann
STFT window length and hop 1024, 256
STFT spectrogram size FT 512T(T= 32)
log-power Mel-spectrogram size F0T 256T(T= 32)
Dynamic range 80 dB
Frequencyfmin, maxg f 300, 4,000gHz
Fingerprintfwindow length, hopg f 1s;0:5sgorf2s;1sg
Fingerprint dimension d 64 or 128
Network parameters fh; u; vg f 1024;32; h=dg
Batch size N 120 or 320 or 640
We trained our model using LAMB [29] optimizer, which per-
formed 2 pp better than Adam [30] with the 3 s query sequence for
batch sizeN320. In practice, Adam worked better only for
N240. The learning rate had an initial value of 1e-4 N=640with
cosine decay without warmup [31] or restarts [32], then it reached a
minimum value of 1-e7 in 100 epochs. The temperature in Eq.1 was
= 0:05, and we did not observe a meaningful performance change
in the range [0:01;0:1]. The training ﬁnished in about 30 h with a
single NVIDIA RTX 6000 GPU or v3-8 Cloud TPUs.
The search algorithm in Section 2.2 was implemented using an
open library [33]. We used the inverted ﬁle (IVF) index structure
with product quantizer (PQ) as a non-exhaustive MIPS. The IVF-PQ
had 200 centroids with the code size of 26, and 8-bits per index. In
this setting, the loss of recall remained below 0.1% compared to the
exhaustive search of 100K songs ( 56M segments) database.
3.5. Evaluation protocol
• Evaluation metric: To measure the performance in segment/song-
level search in Section 4, we use Top-1 hit rate(%) :
100(n of hits @Top-1)
(n of hits @Top-1) + (n of miss @Top-1 ); (3)
which is equivalent to recall . In Table 3, exact match is the case
when the system ﬁnds the correct index in database. We further
deﬁne the tolerance range for near match as500 ms.
• Test-Query generation: 2K query-sources for each f1, 2, 3, 5, 6,
10gs length are randomly cropped from Test-DB containing 500
clips of 30s each. Each query is synthesized through the random
augmentation pipeline as described in Section 3.2. Note that we
exclude Cutout and Spec-augment. The default SNR range is [0,
10] dB. We make sure that the data used for background mixing
and IR as unseen to our model by isolating them from training set.
4. RESULTS AND DISCUSSION
4.1. Experimental results
The main results are listed in Table 3. Using the same augmentation
method, Now-playing [7] based on semi-hard triplet [13] took 2 s as
a unit audio segment. The modiﬁed Now-playing with 1 s unit audio
segment could be more fairly compared with our works.
VS.Now-playing (semi-hard triplet) Modiﬁed Now-playing con-
sistently performed better than the replicated Now-playing . While
cutting the dimension in half, this trend was maintained. Consider-
ing that the DB size was the same when the number of ﬁngerprint
dimensions was half, it could be seen that constructing DB with 1Table 3 . Top-1 hit rate (%) of large-scale (total of 100K songs)
segment-level search. ddenotes the dimension of ﬁngerprint em-
bedding. exact match means that our system ﬁnds the exact index.
near match means a mismatch within 1 index or500 ms.
Method d matchQuery length in seconds
1 s 2 s 3 s 5 s 6 s 10 s
Now-playing
(replicated)128exact - 44.3 60.1 73.6 81.0 86.1
near - 46.8 63.5 75.2 81.6 86.3
Now-playing
(modiﬁed
for 1 s unit)64exact 25.8 58.5 69.3 78.5 81.4 87.7
near 30.9 61.3 71.2 79.5 82.2 88.3
128exact 26.3 58.2 69.5 78.4 81.4 87.8
near 30.9 61.1 71.8 79.8 83.0 89.2
This work
(N=640)64exact 54.6 78.9 85.4 90.4 92.0 94.9
near 61.3 81.7 86.7 90.9 92.7 95.1
128exact 62.2 83.2 87.4 92.0 93.3 95.6
near 68.3 84.9 88.7 92.7 94.1 95.8
This work
(N=320)128exact 61.0 82.2 87.1 91.8 93.1 95.2
near 67.1 84.1 88.1 92.5 93.9 95.5
This work
(N=120)128exact 55.9 78.8 84.9 90.9 92.2 95.3
near 62.3 80.9 86.3 91.5 92.8 95.5
This work
(no aug.)128exact 0.0 0.0 0.0 0.0 0.0 0.0
near 0.0 0.0 0.0 0.0 0.0 0.0
Table 4 . Effect of ﬁngerprint dimension din 1 s segment search.
Embedding dimension d=16 d=32 d=64 d=128
Top-1 hit rate@1 s (%) 11.6 40.2 54.6 62.2
s was more advantageous to segment search. The proposed model
with a 128-dimensional ﬁngerprint using batch size of 640 always
showed the best performance (highlighted in Table 3) for any query
length. This conﬁrmed that the proposed contrastive learning ap-
proach outperformed over the semi-hard triplet approach.
Embedding dimension In Table 2, increasing the embedding di-
mensiond: 64!128 for the modiﬁed Now-playing did not affect the
results signiﬁcantly. In contrast, increasing the embedding dimen-
siond: 64!128 for our best model gave us a larger improvement of
exact match performance as "7.6 (54.6!62.2%) pp for the 1 s query.
This reafﬁrmed the training beneﬁts of our contrastive learning over
the semi-hard triplet, fairly compared using the same network struc-
ture. In Table 4, we further investigated the effect of reducing dto
our model with 1 s query length. We could observed a rapid drop in
exact match performance while decreasing d: 64!32!16.
Performance of sequence search The longer the query sequence,
the better the performance in all experiments. In Table 3, segment-
level hit rate of our best model (highlighted) was increasing as
62.2!83.4!92.0!95.6% while increasing the query length by
almost double. Thus, the longer query length was useful. In Table 3,
the performance difference between near and exact match result of
our best model at 1 s query was 6.1 (62.2 and 68.3%) pp. This inter-
val decreased immediately as the query length became larger than 1.
These results showed that our sequence search method introduced
in Section 2.2 was quite effective.
Effect of batch size The larger the batch size, the better the per-
formance in all experiments. In Table 3, reducing the batch size
N: 640!120 from our best model degraded the exact match per-
formance by#6.3 (62.2!55.9%) pp at 1 s query length. Recent
works [14, 16, 17] on contrastive learning has been consistently re-
porting similar trends. Our result implicated that the diversity of
negative examples existing by large batch could play an important
role in the contrastive learning framework.VS.Dejavu We compared our work with the open-source project
Dejavu [34] based on the conventional method [1, 5] in the song-
level search task of smaller (10K-30s) scale. 69.6% of Top-1 hit rate
was achieved with Dejavu , a song-level search engine using a 6 s
query. Our best model achieved 99.5% for song-level hit rate, and
exact/near match was 98.9/99.1% at the 6 s query, respectively. Our
model also achieved f83.6, 95.4, 97.4g% exact match atf1,2,3gs
query. The capacity of ﬁngerprints from Dejavu was about 400
MB, while ours (quantized with 1/4 compression rate) was less than
40 MB ford=64. These results suggest that our method has advan-
tages over conventional methods in both performance and scalability.
4.2. Size of training set, search time and scalability
The models in Table 3 were trained with about 70 h dataset. This
size was less than 1% of the total 8K h DB for test. We assumed that
using the entire DB for training would be impractical–a huge num-
ber of new songs are produced every day. In additional experiment,
we used 10% of the Test-dummy-DB for training a d=64 model. It
achievedf58.3, 81.1, 86.5, 92.4, 93.4, 96.0 g% of Top-1 hit rate for
the query sequence of f1, 2, 3, 5, 6 ,10gs. This improved"3.7
(54.6!58.3%) pp at the 1 s query over the best model with d=64 in
Table 3, still lower than the result of d=128. Thus, both dand the
amount of training data were the factors affecting performance.
In our best model with d=128, the ﬁnal DB size was about 5.8
GB for 56M segments from total of 100K songs. We report about
1.5 s search time with i9-10980XE CPU (in-memory-search), and
0.02 s with GPU for parallel search of 19 segments (= 10 s query).
In case of using CPUs, we could observe on-disk-search using the
latest SSD with CPU was only twice as slow as in-memory-search.
We reserve the industry-level scalability issues for future work.
4.3. Transfer to down-stream task
We further investigated the generality of the learned embeddings by
performing a downstream task, as in the typical SSL [14–17] set-
tings. By ﬁxing f(:)and ﬁne-tuning a linear classiﬁer, we tried audio
genre classiﬁcation in GTZAN dataset with stratiﬁed 10-fold cross-
validation. Fine-tuning on the pre-trained embeddings for ﬁngerprint
achieved 59.2% accuracy, while training from scratch achieved only
32.0%. This showed that the features encoded by f(:)were linearly
interpretable, consistent with other SSL reports [14–17]. However,
our result was slightly lower than the baseline of 61.0% accuracy us-
ing MFCCs+GMM [35]. This might be due to the limitation of the
lightweight networks with the relatively short-time analysis window.
5. CONCLUSIONS AND FUTURE WORK
This study presented a neural audio ﬁngerprinter for high-speciﬁc
audio retrieval. Our model was trained to maximize the inner-
product between positive pairs of ﬁngerprints through a contrastive
prediction task. To this end, we explicitly sampled positive pairs to
have original–replica relations by applying various augmentations to
clean signals. We evaluated our model in the segment-level search
task with a public database of 100K songs. In the experiment, our
model performed better than the model with triplet embeddings. It
was also shown that our work, using 10 times less memory than an
existing work, outperformed in song-level search task. So far, these
results have implied that the audio ﬁngerprinting task would inher-
ently have self-supervised learning potentials. The future direction
of this study is to test neural audio ﬁngerprints in industry-scale
database and queries from a variety of user devices.6. ACKNOWLEDGEMENT
We would like to thank the TensorFlow Research Cloud (TFRC) pro-
gram that gave us access to Google Cloud TPUs.
7. REFERENCES
[1] J. Haitsma and T. Kalker, “A highly robust audio ﬁngerprinting
system.,” in Proc. of the Int. Society for Music Information
Retrieval (ISMIR) , 2002, vol. 2002, pp. 107–115.
[2] A. Wang et al., “An industrial strength audio search algo-
rithm.,” in Proc. of the Int. Society for Music Information Re-
trieval (ISMIR) , 2003, vol. 2003, pp. 7–13.
[3] P. Cano, E. Batlle, T. Kalker, et al., “A review of audio ﬁnger-
printing,” Journal of VLSI signal processing systems for signal,
image and video technology , vol. 41, no. 3, pp. 271–284, 2005.
[4] S. Baluja and M. Covell, “Waveprint: Efﬁcient wavelet-based
audio ﬁngerprinting,” Pattern recognition , vol. 41, no. 11, pp.
3467–3480, 2008.
[5] C. V . Cotton and D. P. Ellis, “Audio ﬁngerprinting to identify
multiple videos of an event,” in Proc. of the IEEE Int. Conf.
on Acoustics, Speech and Signal Processing (ICASSP) . IEEE,
2010, pp. 2386–2389.
[6] T.-K. Hon, L. Wang, J. D. Reiss, and A. Cavallaro, “Audio
ﬁngerprinting for multi-device self-localization,” IEEE/ACM
Transactions on Audio, Speech, and language processing , vol.
23, no. 10, pp. 1623–1636, 2015.
[7] B. Gfeller et al., “Now playing: Continuous low-power music
recognition,” in NeurIPS 2017 Workshop on Machine Learning
on the Phone and other Consumer Devices , 2017.
[8] C. J. Burges, D. Plastina, J. C. Platt, E. Renshaw, and H. S.
Malvar, “Using audio ﬁngerprinting for duplicate detection
and thumbnail generation,” in Proc. of the IEEE Int. Conf.
on Acoustics, Speech, and Signal Processing (ICASSP) . IEEE,
2005, vol. 3, pp. iii–9.
[9] E. Allamanche, “Audioid: Towards content-based identiﬁca-
tion of audio material,” in Proc. of the 100th AES Conv. , 2001.
[10] Y . Jiang, C. Wu, K. Deng, and Y . Wu, “An audio ﬁngerprint-
ing extraction algorithm based on lifting wavelet packet and
improved optimal-basis selection,” Multimedia Tools and Ap-
plications , vol. 78, no. 21, pp. 30011–30025, 2019.
[11] J. Six and M. Leman, “Panako - A Scalable Acoustic Finger-
printing System Handling Time-Scale and Pitch Modiﬁcation,”
inProc. of the Int. Society for Music Information Retrieval (IS-
MIR) , 2014, pp. 259–264.
[12] A. Gionis, P. Indyk, and R. Motwani, “Similarity search in
high dimensions via hashing,” in Proc. of the Int. Conf. on Very
Large Data Bases (VLDB) , 1999, VLDB ’99, pp. 518––529.
[13] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A uni-
ﬁed embedding for face recognition and clustering,” in Proc.
of the IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR) , 2015, pp. 815–823.
[14] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple
framework for contrastive learning of visual representations,”
arXiv preprint arXiv:2002.05709 , 2020.
[15] A. v. d. Oord, Y . Li, and O. Vinyals, “Representation
learning with contrastive predictive coding,” arXiv preprint
arXiv:1807.03748 , 2018.[16] T. Chen, S. Kornblith, K. Swersky, M. Norouzi, and G. Hin-
ton, “Big self-supervised models are strong semi-supervised
learners,” arXiv preprint arXiv:2006.10029 , 2020.
[17] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, “wav2vec
2.0: A framework for self-supervised learning of speech repre-
sentations,” arXiv preprint arXiv:2006.11477 , 2020.
[18] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge
in a neural network,” arXiv preprint arXiv:1503.02531 , 2015.
[19] P. H. Chen, S. Si, S. Kumar, Y . Li, and C.-J. Hsieh, “Learning
to screen for fast softmax inference on large vocabulary neural
networks,” arXiv preprint arXiv:1810.12406 , 2018.
[20] M. Defferrard, K. Benzi, P. Vandergheynst, and X. Bresson,
“Fma: A dataset for music analysis,” in Proc. of the Int. Society
for Music Information Retrieval (ISMIR) , 2017.
[21] J. F. Gemmeke, D. P. Ellis, and et al., “Audio set: An ontology
and human-labeled dataset for audio events,” in Proc. of the
IEEE Int. Conf. on Acoustics, Speech and Signal Processing
(ICASSP) . IEEE, 2017, pp. 776–780.
[22] Xaudia, “Microphone impulse response project,” 2017, [On-
line]. http://micirp.blogspot.com/.
[23] M. Jeub, M. Schafer, and P. Vary, “A binaural room impulse
response database for the evaluation of dereverberation algo-
rithms,” in Proc. of the Int. Conf. on Digital Signal Processing
(ICDSP) . IEEE, 2009, pp. 1–5.
[24] T. DeVries and G. W. Taylor, “Improved regularization of
convolutional neural networks with cutout,” arXiv preprint
arXiv:1708.04552 , 2017.
[25] D. S. Park, W. Chan, et al., “Specaugment: A simple data
augmentation method for automatic speech recognition,” in
Proc. of the Interspeech , 2019, pp. 2613–2617.
[26] F. Mamalet and C. Garcia, “Simplifying convnets for fast
learning,” in Proc. of the Int. Conf. on Artiﬁcial Neural Net-
works (ICANN) . Springer, 2012, pp. 58–65.
[27] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,”
arXiv preprint arXiv:1607.06450 , 2016.
[28] H. Lai, Y . Pan, Y . Liu, and S. Yan, “Simultaneous feature learn-
ing and hash coding with deep neural networks,” in Proc. of
the IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR) , 2015, pp. 3270–3278.
[29] Y . You, J. Li, et al., “Large batch optimization for deep learn-
ing: Training bert in 76 minutes,” in Proc. of the Int. Conf. on
Learning Representations (ICLR) , 2019.
[30] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-
mization,” arXiv preprint arXiv:1412.6980 , 2014.
[31] P. Goyal, P. Doll ´ar, R. Girshick, et al., “Accurate, large mini-
batch sgd: Training imagenet in 1 hour,” arXiv preprint
arXiv:1706.02677 , 2017.
[32] I. Loshchilov and F. Hutter, “Sgdr: Stochastic gradient descent
with warm restarts,” arXiv preprint arXiv:1608.03983 , 2016.
[33] J. Johnson, M. Douze, and H. J ´egou, “Billion-scale similarity
search with gpus,” IEEE Transactions on Big Data , 2019.
[34] W. Drevo, “Dejavu: open-source audio ﬁngerprinting project,”
2014, [Online]. https://pypi.org/project/PyDejavu/.
[35] G. Tzanetakis and P. Cook, “Musical genre classiﬁcation of
audio signals,” IEEE Transactions on speech and audio pro-
cessing , vol. 10, no. 5, pp. 293–302, 2002.Semantic Segmentation using Vision Transformers:
A survey
Hans Thisankea, Chamli Deshana, Kavindu Chamitha, Sachith
Seneviratneb,c, Rajith Vidanaarachchib,c, Damayanthi Heratha,
aDepartment of Computer Engineering, University of
Peradeniya, Peradeniya, 20400, Sri Lanka
bMelbourne School of Design, University of Melbourne, Parkville, VIC 3010, Australia
cFaculty of Engineering and IT, University of Melbourne, Parkville, VIC 3010, Australia
Abstract
Semantic segmentation has a broad range of applications in a variety of do-
mains including land coverage analysis, autonomous driving, and medical
image analysis. Convolutional neural networks (CNN) and Vision Trans-
formers (ViTs) provide the architecture models for semantic segmentation.
Even though ViTs have proven success in image classication, they cannot
be directly applied to dense prediction tasks such as image segmentation
and object detection since ViT is not a general purpose backbone due to
its patch partitioning scheme. In this survey, we discuss some of the dier-
ent ViT architectures that can be used for semantic segmentation and how
their evolution managed the above-stated challenge. The rise of ViT and
its performance with a high success rate motivated the community to slowly
replace the traditional convolutional neural networks in various computer
vision tasks. This survey aims to review and compare the performances of
ViT architectures designed for semantic segmentation using benchmarking
datasets. This will be worthwhile for the community to yield knowledge re-
garding the implementations carried out in semantic segmentation and to
discover more ecient methodologies using ViTs.
Keywords: vision transformer, semantic segmentation, review, survey,
convolution neural networks, self-supervised learning, deep learning
Corresponding author
Email addresses: e16368@eng.pdn.ac.lk (Hans Thisanke), e16076@eng.pdn.ac.lk
(Chamli Deshan), e16057@eng.pdn.ac.lk (Kavindu Chamith),
sachith.seneviratne@unimelb.edu.au (Sachith Seneviratne),
rajith.vidanaarachchi@unimelb.edu.au (Rajith Vidanaarachchi),
damayanthiherath@eng.pdn.ac.lk (Damayanthi Herath)
Preprint submitted to Engineering Applications of Articial Intelligence May 8, 2023arXiv:2305.03273v1  [cs.CV]  5 May 20231. Introduction
Transformers became the new state-of-the-art in natural language pro-
cessing (NLP) [1] after the tremendous success it achieved. This led to the
development of ViT [2] which was later adapted into the computer vision
tasks such as image classication [2, 3], semantic segmentation [4, 5] and
object detection [6, 7]. A typical Transformer encoder consists of a multi-
head self-attention (MSA) layer, a multi-layer perceptron (MLP), and a layer
norm (LN). The main driving force behind the ViT is the multi-head self-
attention mechanism. It helps ViT to capture long-range dependencies with
less inductive bias [8]. When trained on a sucient amount of data, ViT
shows remarkable performance, beating the performance of state-of-the-art
CNNs [2]. However, ViTs still have some drawbacks compared to CNNs such
as the need for very large datasets. Strategies such as self-supervised based
approaches can be used to alleviate some of these drawbacks and further
enhance ViTs [9].
Semantic segmentation is the process of assigning a class label to each
and every pixel of an image. This requires accurate predictions at the pixel
level. For segmentation, there exist both CNN-based models and Trans-
former based models. However, plain ViT models cannot be directly used
for segmentation tasks because they do not consist of segmentation heads
[10]. Instead SETR [5] and Swin Transformer [4] based architectures can be
utilized for segmentation tasks. Unlike image classication, dense prediction
tasks such as semantic segmentation and object detection come with a few
diculties due to the rich intra-class variation, context variation, occlusion
ambiguities, and low image resolution [11]. There have been many improve-
ments in the ViT domain in the last few years to overcome these challenges
while further developments are still in progress to make them ecient.
The review focuses specically on semantic segmentation using Vision
Transformers. The comparison of the ViT models specialized for semantic
segmentation is discussed with architecture-wise and tabulated specic sets
of model variants that can be compared with the same set of benchmark
datasets. The current surveys performed on ViTs have been structured with
a detailed historical evolution from NLP to the Vision Transformer domain.
[12] focuses on self-attention and its varieties with advantages and limitations
with existing methods for segmentation, object detection, classication, and
action recognition. The comparison follows between CNN and ViT back-
bones on the ImageNet dataset. The survey done by [13] is also considering
various vision tasks and surpasses CNN-based models with experimental re-
2sults on benchmark datasets. Even though several surveys have been done
[12, 13, 14], a comparison between segmentation models with several bench-
mark datasets to identify the best-performing model has not been performed.
In our survey, we provide a set of segmentation models, for each of which we
dene the best variant in each benchmark dataset category. This is useful in
the sense of identifying the most optimal parameters such as patch size, iter-
ations count for each variant of the model. By providing mIoU (%) of model
performance results over several semantic segmentation-related benchmark
datasets, overall evaluation and highest-performing model variants for each
dataset can be identied.
In Section 2 we discuss the applications of semantic segmentation, ViTs,
their challenges, and loss functions. Section 3 describes benchmark datasets
used in semantic segmentation. Section 4 describes the existing work done
in semantic segmentation using ViTs and presents a quantitative analysis.
Finally, Section 5 provides the discussions and Section 6 concludes the paper
with future directions.
2. Semantic Segmentation using Vision Transformers
This section aims to provide an in-depth analysis of the applications in
semantic segmentation, with a focus on recent advancements in ViTs. We
begin by exploring the principles and architecture of ViTs and their potential
for improving semantic segmentation performance. We then delve into vari-
ous application domains of semantic segmentation. We also devote a section
to practical approaches for overcoming the data limitations that often arise
in ViT models. Finally, we discuss various loss functions used in semantic
segmentation and their eectiveness in dierent scenarios.
2.1. Vision Transformers
Automatic segmentation techniques have been evolving and improving
throughout the years with the advancements of deep learning approaches
and the application of semantic segmentation in practical usage. For seman-
tic segmentation, the requirement is to locally identify the dierent classes
in the image with spatial location. For that, the fully connected layers in
the conventional CNN architecture were replaced with fully convolutional
layers combined with feature extraction. This was introduced as Fully Con-
volutional Networks (FCN) [15] to identify high-level semantic features from
images. These networks have shown to be faster compared to previous CNN-
based techniques and are also capable of generating segmentation maps for
images of any resolution. Some of the commonly known architectures are
3U-Net (state-of-the-art FCN) and more improved architectures with higher
accuracy and eciency are developed by [16, 17, 18].
One of the limitations identied with the FCN architecture is the low
resolution of the nal output segmentation image of the feature map due
to going through several convolutional and pooling layers. Furthermore, the
locality property of the FCN-based methods caused limitations to the capture
of long-range dependencies of the feature maps. To solve this, researchers also
looked into attention mechanisms to merge or replace these models. This has
led to trying out Transformer architectures in the computer vision domain
which were successful in NLP.
Self-attention-based architectures have taken priority in NLP by avoiding
the drawbacks such as vanishing gradients in sequence modeling and trans-
duction tasks. Specially designed for sequence modeling and transduction
tasks, Transformers with attention were able to model long-range sequences
of data. When training a NLP model, one of the best ways is to pre-train
on a large text corpus and then ne-tune on a small set of data which is for
the related task. But with deep neural networks, this was a challenging task.
As Transformers have high computational eciency and scalability, it was
easier to train on a large set of data [19].
With the success of using self-attention to enhance the input-output in-
teraction in NLP, works have been proposed to combine convolutional ar-
chitectures with self-attention, especially in object detection and semantic
segmentation where input-output interaction is highly needed [20]. But ap-
plying attention to convolutional architectures demands high computation
power, even though they are theoretically ecient [1].
Considering images, calculating self-attention is quadratic to the image
size as each pixel attends to every other pixel therefore it is a quadratic cost of
the pixel count [2]. Thus [2] proposed to divide the image into a sequence of
patches and treat them as tokens as it was done in NLP. Instead of pixel-wise
attention, patch-wise attention was used in the architecture which helped to
reduce the computational complexity compared to applying self-attention to
convolutional architecture.
This architecture showed promising results by surpassing all the state-
of-the-art convolution-based methods by reaching an accuracy of 88.55% on
ImageNet, 90.72% on ImageNet-ReaL, and 94.55% on CIFAR-100 datasets
[2]. A major characteristic of the ViT is that it needs more data for model
training. Experiments carried out by [2] ensure that with increasing data
size, ViT performs well.
4Figure 1: Architecture of the Vision Transformer. The model splits an image into a
number of xed-size patches and linearly embeds them with position embeddings (left).
Then the result is fed into a standard transformer encoder (right). Adapted from [2].
2.2. Applications of Semantic Segmentation
In this section, we discuss various application domains of semantic seg-
mentation, including remote sensing, medical imaging, and video processing.
For each of these domains, we highlight the unique challenges and opportuni-
ties that arise, as well as the current state-of-the-art methods and techniques.
2.2.1. Semantic Segmentation of Remote Sensing Images
Remote sensing is the process of getting information and monitoring the
characteristics of an area without having any physical contact. The two
main types of remote sensing techniques are the use of active sensors such
as RADAR, LiDAR and the use of passive sensors such as satellite imagery
[21]. These high-resolution earth surface images provide a wide range of use
cases such as world mapping updates [22], forest degradation analysis [23],
monitoring changes to the surface [24], etc.
Remote sensing imagery is widely used in combination with computer vi-
sion and Articial Intelligence (AI) for analyzing and processing the earth's
surface over large areas with complex feature distributions. The images col-
lected by satellites or unmanned aerial vehicles (UAV) provide a wide range
of information for applications such as urban planning, disaster management,
trac management, climate change, wildlife conservation, crop monitoring,
etc. The use of datasets containing these high-resolution images and their
respective segmented masks [25] have provided a base for remote sensing
5image analysis using computer vision and AI. The use of neural networks
provides the ability to process large amounts of image data for object de-
tection, semantic segmentation, and change detection tasks. The evolution
in the remote sensing domain has further improved satellite sensors and the
introduction of drone technology for aerial imagery has been vital to getting
ner details on the earth's surface. This has resulted in precise and accurate
data for processing using AI techniques [26].
Remote sensing images of the earth's surface provide land cover areas that
can be categorized into dierent segmented classes. Each of these classes is
assigned a label for each pixel while preserving the spatial resolution of the
image. Many datasets containing these remote sensing images and their seg-
mented masks are available [25, 27, 28] to use for dierent applications such
as change detection, land cover segmentation, and classication. Examples
of common land cover classes covered by the pixel-level classication are
forests, crops, buildings, water resources, grasslands, roads, etc. Research
has been conducted using ViT architecture models by adding layers and at-
tention mechanisms eciently and improvements in performance to process
high-resolution remote sensing images for semantic segmentation such as Ef-
cient Transformer [10] and Wide-Context Transformer [29].
Manual segmentation of these dierent environmental areas from a com-
plex satellite or aerial images is a dicult task which is time-consuming,
error-prone, and requires expertise in the remote sensing domain.
2.2.2. Semantic Segmentation of Medical Images
Medical image analysis has developed and incorporated scanning and vi-
sualization techniques. Segmentation techniques have been vital as it has the
ability to identify and segment medical imagery to assist in further diagnosis
and interventions. By identifying each region of interest (ROI) highlighted,
various important diagnoses are happening such as brain tumor boundary
detection from MRI images, pneumonia aections in X-rays, cancer detec-
tion from biopsy sample images, etc. The demand for this type of analy-
sis through image segmentation has emerged in the recent past with much
research being done in the scope to develop more precise, ecient models
and algorithms. These medical images that are used in image segmentation
tasks can be grouped based on modalities such as MRI, CT scan, X-ray,
ultrasound, microscopy, dermoscopy, etc. Each of these categories contains
datasets that were collected under medical supervision and some are made
publicly available.
Since there exist several modalities as mentioned above, the technological
systems that are used for medical imagery dier. Medical imagery system
development vendors built them as per the doctor's requirements. There-
6fore, the images generated are bound to the limitations of the technology
available and require medical personal intervention to examine them [30].
Therefore the segmentation of these images in dierent biological domains
requires experts in each eld to cope with these systems and spend a vast
amount of time examining them. To overcome these diculties, the capabil-
ity of automatic feature extraction has been introduced with deep learning
based techniques, which have been valuable in the sense of medical imagery.
With the advancements in segmentation analysis, better-performing models
have been introduced with the use of medical images by many researchers.
One such famous architecture is the U-Net [31] which was initially intro-
duced for medical image analysis. Based on this, several improved versions
have been followed up using medical imagery datasets from heart, lesion, and
liver segmentation [32, 33, 18]. This proves how benecial the improvement
of segmentation has been in the medical environment. In recent years, the
emerging new architectures of ViTs have also been applied to the medical
domain with TransUNet [34] and Swin-Unet [35]. They are hybrid Trans-
former architectures with the advantages of the U-Net. They performed with
better accuracy in cardiac and multi-organ segmentation applications.
Some limitations of medical images are the relatively less number of im-
ages available compared to natural image datasets (landscapes, people, an-
imals, and automobiles) with millions of images. In the medical domain,
there are several image modalities. For annotating medical images, expertise
in each medical eld is a must. Among them, MRI and microscopy images
are quite dicult to annotate [36]. Typically, these datasets contain fewer
images compared to ultrasound, X-ray, and lesion datasets which are ob-
tained with the existing scanning systems and are easier to annotate with
less complex structures and ne boundaries. But still, limitations exist due
to restrictions on privacy and other medical policies to obtain these images in
large quantities. To overcome these limitations with some datasets, several
image segmentation challenge competitions are taking place every year which
provide publicly available well-annotated medical image datasets. Most of
the improvements made through research in semantic segmentation models
have been based on these challenge datasets and most are taken as bench-
mark datasets for segmentation [37, 38, 39].
2.2.3. Video Semantic Segmentation
Human-Machine interaction [40], augmented reality [41], autonomous ve-
hicles [42], image search engines [43] are some applications in complete scene
understanding and for these type of applications, semantic segmentation con-
tributes more on complete scene understanding on videos. Usually, the idea
is to apply semantic segmentation on frames of a high-resolution video where
7the video is considered as a set of uncorrelated xed images [44]. The com-
mon challenge with this type of semantic segmentation is the computational
complexity of scaling the spatial dimension of the video using the temporal
frame rate. Removal of temporal features and only focusing on spatial frame-
by-frame features doesn't make sense in video segmentation. Since there is
a combined ow among frames of a video, considering the temporal context
of a video is an essential factor in video semantic segmentation, even though
it is computationally expensive.
Research has been conducted to reduce this high computation cost on
videos. Feature reuse and feature warping [45] have been proposed as a
solution. Cityscapes [46] and CamVid [47], are some largest video segmen-
tation datasets available for frame-by-frame approach of video segmentation
[48]. Recent papers have proposed segmentation methods such as selective
re-execution of feature extraction layers [49], optical ow-based feature warp-
ing [50], and LSTM-based, xed-budget keyframe selection policies [51]. The
main key problem in these approaches is that they have less attention to the
temporal context of a video. Researchers have shown that to satisfy both
spatial and temporal contexts, using an optical ow of video as temporal in-
formation to speed up uncertainty estimation makes good sense [52]. VisTR
[53], TeViT [54] and SeqFormer [55] are some of the Transformer models that
are used for video segmentation tasks.
2.3. Practical approaches to overcome the data limitation
Deep neural networks have performed well with supervised learning in
computer vision and NLP. But when it comes to the real world, supervised
learning faces a bottleneck in training a neural network as it needs lots of
labeled data. Collecting labeled data or manual labeling is dicult in every
aspect. Training a network from scratch is a somewhat costly task; as a
remedy for this, transfer learning comes into play. But when considering
specied downstream tasks such as satellite imagery semantic segmentation,
using pre-trained datasets is dicult as most of the architectures have been
trained on benchmark datasets where the data domain is dierent. Therefore,
getting good accuracy has been tricky.
Specially when considering Transformer architectures, self-supervised learn-
ing plays a great role as a remedy for data-hungry problems in deep learning.
In human vision, humans are fed with dierent things in the environment and
then are able to distinguish those things from other objects in the environ-
ment. There are no labeling mechanisms for these scenarios. Therefore, this
is the technique used in SSL which actually trains a neural network using
an unlabeled dataset where the labels are automatically provided through
the dataset itself. As the rst step, the network is set to solve a pretext
8task as described in Figure 2. A pretext task is a pre-designed task from
which the network can learn features and then using those trained weights
for dierent features, the network can be applied to solve some downstream
tasks. A downstream task is a specied task. Common downstream tasks in
computer vision are semantic segmentation, object detection, etc.
Figure 2: The general pipeline of self-supervised learning. The trained weights from solving
a pretext task are applied to solve some downstream tasks.
Rotating an image by a given angle and predicting the rotation, solving
jigsaw puzzles, lling a cut patch on an image, predicting the relative position
of a patch of an image, and separating images belonging to dierent clusters
can be considered as some of the pretext tasks in SSL [56]. By using these
methods, the network can learn dierent features in the dataset under the
given scope. No labels are used here and automatic labeling is achieved via
the image itself.
SSL has three general categories based on how the training happens.
•Generative: Train the encoder to encode the given input and using the
decoder get the input back
•Contrastive: Train the encoder to encode the given input and nd the
similarities
9•Generative-Contrastive (Adversarial): Train encoder to encode the given
input and create fake outputs and compare the features of the input
and output [57]
Semantic segmentation is one of the major downstream tasks that can
be performed using SSL. Pixel-wise labeling is essential in semantic segmen-
tation. If there are no properly annotated datasets, SSL is the best way to
train semantic segmentation architectures.
2.4. Loss functions in semantic segmentation
For segmentation, classication, and object detection models accuracy
improvement not only depends on the model architectures but also on the loss
functions used. The loss function calculates the overall error while training
batches and adjust the weights through back propagation. Numerous loss
functions have been created to cope with various domains, and some of them
are derived from existing loss functions. Additionally, these loss functions
take into account the imbalances in the dataset too.
In the case of semantic segmentation, the default choice and most com-
monly used is the cross-entropy loss which is applied pixel-wise. The loss
function independently evaluates the class predictions for each pixel and av-
erages over all the pixels.
CEloss(p;q) = nX
i=1pilog(qi) (1)
The equation 1 above computes the average loss for each pixel in an
image. Here in the equation piis the true probability of the ithclass and
qiis the predicted probability of the same class. This supports the model
to generate probability maps that closely resemble the actual segmentation
masks while penalizing inaccurate predictions more heavily. By minimizing
the cross-entropy loss function during training, the model becomes better at
precise image segmentation.
Even though the above method is widely used it can be biased with
dataset imbalance as the majority class will be dominant. To overcome this
when the dataset is skewed, a weighted cross entropy loss is introduced in
[31].
WCEloss(p;q) = nX
i=1piwilog(qi) (2)
Here as in equation 2, a weight factor as wifor theithclass is inserted to
the typical equation 1. But the issue was not signicantly solved as the cross
10entropy calculates the average per-pixel loss without considering the adjacent
pixels which can be boundaries.
As a further improvement for the cross-entropy loss, the focal loss tech-
nique [58] was introduced. This is implemented by altering the structure
of cross-entropy loss. When focal loss is applied to samples with accurate
classications, the scaling factor value is down-weighted. This ensures the
more harder samples are emphasized, therefore high class imbalance won't
bias toward the overall calculations.
Floss(pt) = t(1 pt)log(pt) (3)
In the equation 3, ptis the predicted probability of the true class, tis a
scaling factor that gives higher weight to the positive class, and is a focusing
parameter that controls how much the loss is focused on hard examples.
The cross-entropy loss is scaled in this loss function, with the scaling
factors decreasing to zero as the condence in the well-classied classes rises.
Therefore more attention is given to the pixel classes which are dicult to
predict.
Another set of loss calculation techniques is the overlapping between pre-
diction and actual segmentations. The models are trained to minimize the
loss such that the model outputs segmentations with higher overlaps.
Dice loss is one such widely used popular measure in computer vision
tasks to calculate the similarity between two images. It is based on the
dice coecient which was later developed as the dice loss function in the
segmentation domain. This loss was rst used in the computer vision domain
by [59] in medical image segmentation tasks.
Dloss(g;p) = 1 2Pn
i=1gipiPn
i=1gi+Pn
i=1pi+(4)
Here, in equation 4 gandpdescribes the ground truth and prediction
segmentations. The sum is calculated over the nnumber of pixels with 
small constant added to avoid division by zero. The dice coecient measures
the overlap between the samples (ground truth and prediction) and provides
a score ranging from 0 to 1, 1 means perfect overlap. Since this method
considered pixels in both global and local contexts, the accuracy is higher
than cross-entropy loss calculations.
Another similar method used to evaluate the metric of models is the
IoU (Intersection over Union) loss also known as the Jaccard index. It is
quite similar to the dice metric and measures the overlapping of the positive
instances between the considered samples. This method as shown in equation
115 diers from the dice loss with correctly classied segments relative to total
pixels in either the ground truth or predicted segments.
IoUloss(g;p) = 1 Pn
i=1gipiPn
i=1gi+Pn
i=1pi Pn
i=1gipi+(5)
For multi-class segmentation, the mean IoU is considered by taking the
average of each individual class IoU. This is widely used for performance
comparison and evaluation of dense prediction models [60].
3. Datasets
In this section, the common datasets used for the training and testing
of semantic segmentation models are considered. Factors aecting the cre-
ation of real datasets are lighting conditions, weather, and season. Based
on these factors, datasets can be classied into dierent groups. When data
is collected under normal daytime environmental conditions, those data are
categorized under no cross-domain datasets. If data is collected under some
deviated environmental conditions including rainy, cloudy, nighttime, snowy,
etc then such data are categorized under cross-domain datasets. Another
category is synthetic data, where the data is articially created and col-
lected for training purposes. These synthetic datasets are mostly created as
a cost-eective supplement for training purposes. Following are some of the
benchmark datasets specially made for semantic segmentation tasks, with a
summary presented in Table 1.
PASCAL-Context [61] This dataset was created by manually labeling
every pixel of PASCAL-VOC 2010 [62] dataset with semantic categories. The
domain of this dataset is not limited and its data contains dierent objects.
The semantic categories of this dataset can be divided into three main classes.
(i) objects, (ii) stu, and (iii) hybrids. Objects have dened categories such
as cups, keyboards, etc. Stu has classes without a specic shape and has
regions such as sky, water, etc. Hybrid contains intermediate objects such
as roads where roads have a clear boundary but shape cannot be predicted
correctly.
ADE20K [63] Annotations of this dataset are done on scenes, objects,
parts of objects. Many of the objects in the dataset are annotated with their
parts. Annotations in this dataset are made continuously. Therefore, this is
a growing dataset.
KITTI [64] This dataset contains both 2D and 3D images which have
been collected from urban and rural expressway incidents and trac sce-
narios. It is useful for robotics and autonomous driving. This dataset has
12dierent variants namely KITTI-2012, KITTI-2015 and they have some dif-
ferences in the ground truth.
Cityscapes [46] This contains large-scale pixel-level and instance-level
semantic segmentation annotations recorded from a set of stereo video se-
quences. Compared to other datasets, quality, data size, and annotations in
this dataset have a good rank and data have been collected from 50 dierent
cities in Germany and neighboring countries.
IDD [65] This is specially designed for road scene understanding and data
have been collected from 182 Indian road scenes. As these are taken from
Indian roads, there are some variations in the weather and lighting conditions
because of dust and air quality on roads. One key feature of this dataset is,
this contains some special classes such as auto-rickshaws and animals on the
roads.
Virtual KITTI [66] Except for dierent weather and imaging conditions,
most of the virtual vision datasets such as Virtual KITTI are similar to the
real vision datasets. Therefore virtual datasets are useful for pre-training
purposes. This dataset is created from 5 dierent urban scene videos from
the real-world KITTI dataset. Data have been automatically labeled and can
be used for object detection, semantic segmentation, instance segmentation,
etc.
IDDA [67] This contains 1 million frames generated from simulator
CARLA oriented on dierent 7 city models. This dataset can be used to
do semantic segmentation for more than 100 dierent visual domains and is
specially designed for autonomous driving models.
Dataset Classes Size Train Validation Test Resolution (pixels) Category
PASCAL-Context 540 19740 4998 5105 9637 387 470 No cross-domain
ADE20K 150 25210 20210 2000 3000 - No cross-domain
KITTI 5 252 140 - 112 1392 512 No cross-domain
Cityscapes 30 5K ne, 20K coarse 2975 500 1525 1024 2048 Cross-domain
IDD 34 10004 7003 1000 2001 1678 968 Cross-domain
Virtual KITTI 14 21260 - - - 1242 375 Synthetic
IDDA 24 1M - - - 1920 1080 Synthetic
Table 1: Summary of the datasets
Note: Both cross-domain and no-cross domain falls into the non-synthetic
category
4. Meta - analysis
In this section, we discuss some of the ViT models specialized for the
task of semantic segmentation. The models are selected upon considering the
datasets that they benchmarked (ADE20K, Cityscapes, PASCAL-Context).
13The intuition behind that is to compare all the models on a common basis.
The benchmark results are summarized in Table 2.
4.1.SEgmentation TRansformer (SETR)
SETR [5] proposes semantic segmentation as a sequence-to-sequence pre-
diction task. They adopt a pure Transformer as the encoder part of their
segmentation model without utilizing any convolution layers. In this model,
they replace the prevalent stacked convolution layer based encoder with a
pure Transformer which gradually reduces the spatial resolution.
Figure 3: SETR architecture and its variants adapted from [5]. (a) SETR consists of a
standard Transformer. (b) SETR-PUP with a progressive up-sampling design. (c) SETR-
MLA with a multi-level feature aggregation.
The SETR encoder (Figure 3a) which is a standard Transformer treats
an image as a sequence of patches followed by a linear projection. Then it
embeds these projections with patch embedding + position embedding to
feed them into a set of Transformer layers. SETR has no down-sampling
in spatial resolution at each layer of the encoder transformer while it only
provides global context modeling. They classify SETR into a few variants
depending on the decoder part of the model; SETR-PUP (Figure 3b) which
has a progressive up-sampling design and the SETR-MLA (Figure 3)which
has a multi-level feature aggregation.
SETR achieved state-of-the-art semantic segmentation results on ADE20K,
Pascal Context by the time of submission [5]. It has also been tested on the
Cityscapes dataset and has shown impressive results.
4.2. Swin Transformer
To address the issue of not having a general purpose Transformer back-
bone for computer vision tasks, [4] proposed Swin Transformer (Hierarchical
14Vision Transformer using Shifted Win dows) which can be served as a gen-
eral purpose backbone for computer vision tasks such as image classication
and dense prediction.
Figure 4: An overview of the Swin Transformer adapted from [4]. (a) Hierarchical feature
maps for reducing computational complexity. (b) Shifted window approach which was
used when calculating self-attention. (c) Two successive Swin Transformer Blocks which
presented at each stage. (d) Core architecture of the Swin.
Swin Transformer was able to bring down the quadratic computational
complexity of calculating self-attention in Transformers to linear complex-
ity by constructing hierarchical feature maps (Figure 4a). Also, the shifted
window approach illustrated in Figure 4b has much lower latency than the
earlier sliding window based approaches which were used to calculate the self-
attention. Swin Transformer showed great success over the previous state-
of-the-art in image classication (87.3% top-1 accuracy on ImageNet-1K),
semantic segmentation (53.5% mIoU on ADE20Kval) and object detection
(58.7 box AP and 51.1 mask AP on COCO test-dev) [4].
According to the architecture of a Swin Transformer, in the beginning,
it splits the given image into a sequence of non-overlapping patches (tokens)
by using the patch partitioning module (Figure 4d). Then a linear embed-
ding is applied to this sequence of patches to project them into an arbitrary
dimension. It is followed by several Swin Transformer blocks to apply self-
attention. The main responsibility of the patch merging module is to reduce
the number of tokens in deeper layers. It is noteworthy that the feature map
resolutions in the hierarchical stages are similar to those in typical convo-
15lution architectures such as ResNet [68]. Therefore Swin Transformer can
eciently replace ResNet backbone networks in computer vision tasks.
4.3. Segmenter
Segmenter [11] is a purely transformer-based approach for semantic seg-
mentation which consist of a ViT backbone pre-trained on ImageNet and
introduces a mask transformer as the decoder (Figure 5). Even though the
model was built for segmentation tasks, they take advantage of the mod-
els made for image classication to pre-train and then ne-tune them on
moderate-sized segmentation datasets.
Figure 5: Segmenter architecture adapted from [11]. It basically has a ViT backbone with
a mask transformer as the decoder.
CNN-based models are generally inecient when processing global image
context and ultimately result in a sub-optimal segmentation. The reason for
the sub-optimal segmentation of the convolution-based approaches is that
convolution is a local operation which poorly accesses the global information
of the image. But the global information is crucial where the global image
context usually inuences the local patch labeling. But modeling of global
interaction has a quadratic complexity to the image size because it needs
to model the interaction between each and every raw pixel of the image.
The architecture of the Segmenter especially captures the global context of
images, unlike the traditional CNN-based approaches.
Other than the semantic segmentation tasks, this Segmenter model also
can be applied to panoptic segmentation (semantic segmentation + instance
segmentation) tasks by altering the model architecture. The class embed-
dings of the model need to be replaced by object embeddings in such a case.
164.4. SegFormer
SegFormer [69] is an architecture for semantic segmentation which consist
of a hierarchical Transformer encoder with a lightweight multilayer percep-
tron (MLP) decoder (Figure 6). The MLP decoder is used for predicting the
nal mask. To obtain a precise segmentation, it uses a patch size of 4 4
in contrast to ViT which uses a patch size of 16 16. It has an overlapped
patch merging process to maintain the local continuity around the patches.
Figure 6: SegFormer architecture adapted from [69]. It has a hierarchical Transformer
encoder for feature extraction and a lightweight MLP decoder for predicting the nal
mask.
Generally, ViT has a xed resolution for positional encoding [70]. This
leads to a drop in accuracy since it needs to interpolate the positional en-
coding of testing images when they have a dierent resolution than training
images. Thus, SegFormer introduces a Positional-Encoding-Free design as a
key feature.
Moreover, the authors claim their architecture is more robust against
common corruptions and perturbations than current methods which make
SegFormer appropriate for safety-critical applications. SegFormer achieved
competitive results on ADE20K, Cityscapes, and COCO-Stu datasets as
shown in Table 2. SegFormer comes in several variants from SegFormer-B0
to SegFormer-B5, where the largest model is SegFormer-B5. This largest
model surpasses the SETR [5] on the ADE20K dataset achieving the highest
mIoU while being 4 faster than SETR. All of these SegFormer models have
trade-os between model size, accuracy, and runtime.
174.5. Pyramid Vision Transformer (PVT)
ViT couldn't be directly applicable to dense prediction tasks because its
output feature map is single scaled and it generally has a low resolution which
comes at a higher computational cost. PVT [71] overcomes the aforemen-
tioned concerns by introducing a progressive shrinking pyramid backbone
network to reduce the computational costs and simultaneously output more
ne-grained segmentation. PVT comes in two variants. PVT v1 [71] is the
rst work by the authors and PVT v2 [72] comes with some additional im-
provements to the previous version.
4.5.1. PVT v1
This initial version has some noteworthy changes compared to the ViT.
It takes 44 input patches in contrast to the 16 16 patches in ViT. This
improves the model's ability to learn high-resolution representations. It also
reduces the computational demand of traditional ViT by using a progressive
shrinking pyramid. This pyramid structure progressively shrinks the output
resolution from high to low in the stages which are responsible for generat-
ing the scaled feature maps (Figure 7). Another major dierence is that it
replaces the multi-head attention layer (MHA) in ViT with a novel spatial
reduction attention (SRA) layer which reduces the spatial scales before the
attention operation. This further reduces the computational and memory
demand because SRA has a low computational complexity than MHA.
Figure 7: PVT v1 architecture adapted from [71]. The pyramid structure of the stages
progressively shrinks the output resolution from high to low.
184.5.2. PVT v2
The former version has a few drawbacks. The computational demand
of the PVT v1 is relatively large when processing high-resolution images.
It loses the local continuity of the images when processing the image as a
sequence of non-overlapping patches. It cannot process variable-sized inputs
because of the xed-size position encoding. This new version has three major
improvements which circumvent the previous design issues. First one is linear
spatial reduction attention (LSRA) which reduces the spatial dimension of
the image to a xed size using average pooling (Figure 8). Unlike SRA
in the PVT v1, LSRA benets from linear complexity. Second one is the
overlapping patch embedding (Figure 9a). This is done by zero-padding the
border of the image and taking more enlarged patch windows which overlap
with the adjacent windows. It helps to capture more local continuity of the
images. The third one is the convolutional feed-forward network (Figure 9b)
which helps to process dierent sizes of input resolutions. With these major
improvements, PVT v2 was able to bring down the complexity of PVT v1
to linear complexity.
Figure 8: Comparison of spatial reduction attention (SRA) layers in PVT versions [72]
We can clearly see how the improvements of the PVT v2 contribute to
higher gains in the benchmark comparison in Table 2.
4.6. Twins
Twins [73] propose two modern Transformer designs for computer vision
named Twins-PCPVT and Twins-SVT by revisiting the work on the PVT
v1 [71] and Swin Transformer [4].
Twins-SVT uses a spatially separable self-attention (SSSA) mechanism
based on the depth-wise separable convolutions in neural networks. This
19Figure 9: Improved patch embedding and feed-forward networks in PVT v2 [72]
SSSA has two underlying attention mechanisms which are capable of cap-
turing local information as well as global information. Locally grouped self-
attention (LSA) and global sub-sampled attention (GSA) are the above-
mentioned attention mechanisms respectively. Those techniques greatly re-
duce the heavy computational demand in high-resolution image inputs while
keeping a ne-grained segmentation.
Figure 10: Twins-PCPVT architecture adapted from [73]. It uses conditional position
encoding with a positional encoding generator (PEG) to overcome some of the drawbacks
of xed-positional encoding.
As we discussed in the Pyramid Vision Transformer section, PVT v1
can only process xed-size image inputs due to its absolute positional en-
20coding. This hinders the performance of PVT. To alleviate this challenge
Twins-PCPVT uses a conditional position encoding (CPE) rst introduced
in Conditional Position encoding Vision Transformer (CPVT) [70]. This is
illustrated as the positional encoding generator (PEG) in Figure 10. It is
capable of alleviating some of the issues encountered in xed-position encod-
ing.
Twins architectures have shown outstanding performance on computer
vision tasks including image classication and semantic segmentation. The
semantic segmentation results achieved by the two Twins architectures are
highly competitive compared to the Swin Transformer [4] and PVT [71].
4.7. Dense Prediction Transformer (DPT)
DPT [74] architecture is introduced with a transformer backbone inside
the encoder-decoder design for ne-grained output segmentation predictions
compared to the fully convolutional networks. The transformer encoder
based on ViT [2] is capable of maintaining spatial resolution over all the
stages of the Transformer architecture which is important for dense predic-
tions.
Figure 11: DPT architecture adapted from [74]. (a) Non-overlapping image patches are fed
into the Transformer block. (b) Reassemble operation for assembling tokens into feature
maps. (c) Fusion blocks for combining feature maps.
In the paper, the authors have introduced several models based on the
used image embedding technique. The DPT-Base and DPT-Large models
use patch-based embedding where the input image is separated into non-
overlapping image patches. Then these are fed into the Transformer block
with a learnable position embedding to locate the spatial position of each in-
dividual token (Figure 11a). DPT-Base has 12 transformer layers compared
to the DPT-Large which has 24 layers with wide feature sizes. The other
model is the DPT-Hybrid, which uses the convolutional backbone ResNet-50
as a feature extractor and uses the pixel-based feature maps as token inputs
21to the 12-layer transformer block. The Transformer blocks reassemble the
tokens with multi-head self-attention (MSA) [1] sequential blocks for global
interaction between tokens. The tokens are reassembled into image-like fea-
ture representations in various resolutions (Figure 11b). Finally, these rep-
resentations are combined using residual convolutional units in the decoder
and fused together for the nal dense prediction (Figure 11c).
The experimental results of the dense prediction transformer have pro-
vided improved accuracy results over several benchmark dataset compar-
isons. The results show that for a large training dataset, the model has the
best performance. The comparisons were done for depth estimations and
semantic segmentation. ADE20K dataset is used for segmentation and the
DPT-Hybrid model has outperformed all the fully-convolutional models [74].
The DPT has the ability to identify precise boundaries of objects with less
distortion. The DPT model was also compared with the PASCAL-Context
dataset after ne-tuning.
4.8. High-Resolution Transformer (HRFormer)
HRFormer [75] is an architecture model that is built using a depth-wise
convolutional design with a Feed Forward Network (FFN) and a local win-
dow self-attention mechanism with a multi-resolution parallel transformer
module. This model is developed for dense prediction tasks focusing on pose
estimation and semantic segmentation. The model outperforms the conven-
tional ViT model which produces low-resolution outputs. The HRFormer is
designed to maintain the high-resolution using multi-resolution streams and
is more ecient in computational complexity and memory usage.
Figure 12: HRFormer architecture adapted from [75]. (a) Self-attention blocks. (b) FFN
with depth-wise convolutions.
HRFormer has been incorporated by using the HRNet [76], which is a
convolutional network consisting of a multi-scale parallel design. This ar-
chitecture helps to capture feature maps in variant resolutions while main-
taining high resolution. At each of these resolution blocks, partitioning is
22done by creating non-overlapping windows, and self-attention is performed
on each image window separately. This improved the eciency signicantly
compared to overlapping local window mechanisms introduced earlier in dif-
ferent studies [77]. The self-attention blocks (Figure 12a) are followed by
an FFN with depth-wise convolutions (Figure 12b) to increase the receptive
eld size by information exchange between local windows, which is vital in
dense prediction. By incorporating a multi-resolution parallel transformer
architecture with convolutional multi-scale fusions for the overall HRFormer
architecture, the information between dierent resolutions is exchanged re-
peatedly. This process creates a high-resolution output with both local and
global context information.
4.9. Masked-attention Mask Transformer (Mask2Former)
Mask2Former [78] is a new transformer architecture that can be leveraged
to do segmentation tasks including panoptic, instance, and semantic segmen-
tation. It is a successful attempt to introduce a universal architecture for the
segmentation tasks which outperforms the current specialized SOTA archi-
tectures for each of the segmentation tasks by the time of submission. Its key
components consist of a transformer decoder with masked attention. Gener-
ally, a standard Transformer attends to the full feature map. In contrast, the
masked attention operator in Mask2Former restricts the cross-attention to
the foreground region of the predicted mask and then extracts the localized
features. This makes the attention mechanism more ecient in this model.
Figure 13: Mask2Former architecture adapted from [78]. The model consists of a backbone
feature extractor, a pixel decoder, and a Transformer decoder.
23The architecture of Mask2Former is similar in design to the previous
MaskFormer [79] architecture. The main components are the backbone fea-
ture extractor, pixel decoder, and the Transformer decoder (Figure 13). The
backbone could be either a CNN-based model or a Transformer based model.
As the pixel decoder, they have used a more advanced multi-scale deformable
attention Transformer (MSDeformAttn) [6] in contrast to the feature pyra-
mid network [80] used in MaskFormer [79]. Masked attention has been used
to enhance the eectiveness of the Transformer decoder.
Despite being a universal architecture for segmentation, Mask2Former
still needs to be trained separately for each of the specic tasks. This is
a common limitation of the universal architectures for segmentation tasks.
Mask2Former has achieved new SOTA performance on all three segmentation
tasks (panoptic, instance, semantic) in popular datasets such as COCO and
ADE20K and Cityscapes. The semantic segmentation results are compared
for ADE20K and Cityscapes datasets in Table 2.
24Datasets
Model Variant Backbone #Params (M) ADE20K Cityscapes PASCAL-Context
SETR [5]SETR- Na ve (16,160k)ViT-Lz[2] 305.67 48.06 / 48.80 - -
SETR- PUP (16,160k) ViT-Lz318.31 48.58 / 50.09 - -
SETR- MLA (16,160k) ViT-Lz310.57 48.64 / 50.28 - -
SETR- PUP (16,40k) ViT-Lz318.31 - 78.39 / 81.57 -
SETR- PUP (16,80k) ViT-Lz318.31 - 79.34 / 82.15 -
SETR- Na ve (16,80k) ViT-Lz305.67 - - 52.89 / 53.61
SETR- PUP (16,80k) ViT-Lz318.31 - - 54.40 / 55.27
SETR- MLA (16,80k) ViT-Lz310.57 - - 54.87 / 55.83
Swin@[4]Swin-T 60 46.1 - -
Swin-S 81 49.3 - -
Swin-Bz121 51.6 - -
Swin-Lz234 53.5 - -
Segmenterx[11]Seg-B DeiT-By[81] 86 48.05 80.5 53.9
Seg-B/Mask DeiT-By86 50.08 80.6 55.0
Seg-L ViT-Lz307 52.25 80.7 56.5
Seg-L/Mask ViT-Lz307 53.63 81.3 59.0
SegFormer [69]MiT-B0y3.4 37.4 / 38.0 76.2 / 78.1 -
MiT-B1y13.1 42.2 / 43.1 78.5 / 80.0 -
MiT-B2y24.2 46.5 / 47.5 81.0 / 82.2 -
MiT-B3y44.0 49.4 / 50.0 81.7 / 83.3 -
MiT-B4y60.8 50.3 / 51.1 82.3 / 83.9 -
MiT-B5y81.4 51.0 / 51.8 82.4 /84.0 -
PVT@PVT-Tinyz17.0 35.7 - -
PVT-Smallz28.2 39.8 - -
PVT v1 [71] PVT-Mediumz48.0 41.6 - -
PVT-Largez65.1 42.1 - -
PVT-Largez* 65.1 44.8 - -
PVT v2-B0z7.6 37.2 - -
PVT v2-B1z17.8 42.5 - -
PVT v2 [72] PVT v2-B2z29.1 45.2 - -
PVT v2-B3z49.0 47.3 - -
PVT v2-B4z66.3 47.9 - -
PVT v2-B5z85.7 48.7 - -
Twins [73]Twins-PCPVT-Sy54.6 46.2 / 47.5 - -
Twins-PCPVT Twins-PCPVT-By74.3 47.1 / 48.4 - -
Twins-PCPVT-Ly91.5 48.6 / 49.8 - -
Twins-SVT-Sy54.4 46.2 / 47.1 - -
Twins-SVT Twins-SVT-By88.5 47.7 / 48.9 - -
Twins-SVT-Ly133 48.8 / 50.2 - -
DPTx[74]DPT-Hybrid ViT-Hybridz123 49.02 - 60.46
DPT-Large ViT-Lz343 47.63 - -
HRFormer [75]OCRNet(7,150k)HRFormer-S 13.5 44.0 / 45.1 - -
OCRNet(7,150k) HRFormer-B 50.3 46.3 / 47.6 - -
OCRNet(7,80k) HRFormer-S 13.5 - 80.0 / 81.0 -
OCRNet(7,80k) HRFormer-B 50.3 - 81.4 / 82.0 -
OCRNet(15,80k) HRFormer-B 50.3 - 81.9 / 82.6 57.6 / 58.5
OCRNet(7,60k) HRFormer-B 50.3 - - 56.3 / 57.1
OCRNet(7,60k) HRFormer-S 13.5 - - 53.8 / 54.6
Mask2Former [78]Swin-T - 47.7 / 49.6 - -
Swin-Lz216 56.1 / 57.3 - -
Swin-L-FaPNz- 56.4 / 57.7 - -
Swin-Lz216 - 83.3 / 84.3 -
Swin-Bz- - 83.3 / 84.5 -
Table 2: Comparison of the ViT models specialized for the task
of semantic segmentation according to mIoU (%) using dierent
benchmark datasets. The best-performing variant of each model
for a given dataset is highlighted. Overall top performing model
variant for each dataset is shaded in gray. "SS / MS" contains both
single-scale and multi-scale inferences. " @" - Single-scale inference only, " x"
- Multi-scale inference only, " " - (patch size, iterations), " †" - pre-trained
on ImageNet-1K, " ‡" - pre-trained on ImageNet-21K, " " - 320K training
iterations and multi-scale ip testing
255. Discussion
In this survey, we discussed how ViTs became a powerful alternative to
classical CNNs in various computer vision applications, their strengths as
well as limitations, and how ViT contributed to the semantic segmentation
of images with their usage across dierent domains such as remote sensing,
medical and video processing. Even though we included some of the CNN ar-
chitectures widely used in prior mentioned domains to provide a comparison
between the ViT and CNNs, an in-depth discussion about CNN architectures
is beyond the scope of this paper. We have summarized the dierent statis-
tics regarding popular datasets used for semantic segmentation tasks and the
results of dierent ViT architectures used for semantic segmentation to give
a clear and high-level overview for the reader around the region of semantic
segmentation.
6. Conclusions and Future Directions
Unlike mature convolutional neural networks, ViTs are still in the early
stage of development. Nevertheless, we observed how powerful and compet-
itive they are with their CNN counterparts. ViTs are progressing towards
excellence and it is expected that they will replace traditional CNN-based
methods widely used in the deep learning domain in the near future. Dier-
ent variants of ViTs can be used for experiments with domains such as big
data analytics that require a vast amount of data for processing. Exploring
research areas with less adaptation to ViT usage can create more ecient,
performance-increased outcomes for current implementation methods.
Even though ViTs have proven successful, they can be challenging to ex-
periment with due to their high computational demand. Thus improvements
to the ViT architecture are needed to make it lightweight and more ecient.
This will inspire the community to open new pathways using ViTs.
We believe there is a plethora of new research areas that ViT, along with
semantic segmentation can be applied to solve real-world problems.
References
[1] A. Vaswani, G. Brain, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez,  Lukasz Kaiser, I. Polosukhin, Attention is all you need,
Advances in Neural Information Processing Systems 30 (2017).
[2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.,
26An image is worth 16x16 words: Transformers for image recognition at
scale, arXiv preprint arXiv:2010.11929 (2020).
[3] C.-F. R. Chen, Q. Fan, R. Panda, Crossvit: Cross-attention multi-
scale vision transformer for image classication, in: Proceedings of the
IEEE/CVF international conference on computer vision, 2021, pp. 357{
366.
[4] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, B. Guo, Swin
transformer: Hierarchical vision transformer using shifted windows, in:
Proceedings of the IEEE/CVF International Conference on Computer
Vision, 2021, pp. 10012{10022.
[5] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng,
T. Xiang, P. H. Torr, et al., Rethinking semantic segmentation from a
sequence-to-sequence perspective with transformers, in: Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition,
2021, pp. 6881{6890.
[6] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, J. Dai, Deformable detr: De-
formable transformers for end-to-end object detection, arXiv preprint
arXiv:2010.04159 (2020).
[7] X. Dai, Y. Chen, J. Yang, P. Zhang, L. Yuan, L. Zhang, Dynamic detr:
End-to-end object detection with dynamic attention, in: Proceedings
of the IEEE/CVF International Conference on Computer Vision, 2021,
pp. 2988{2997.
[8] N. Park, S. Kim, How do vision transformers work?, arXiv preprint
arXiv:2202.06709 (2022).
[9] M. Caron, H. Touvron, I. Misra, H. J egou, J. Mairal, P. Bojanowski,
A. Joulin, Emerging properties in self-supervised vision transformers, in:
Proceedings of the IEEE/CVF International Conference on Computer
Vision, 2021, pp. 9650{9660.
[10] Z. Xu, W. Zhang, T. Zhang, Z. Yang, J. Li, Ecient transformer for re-
mote sensing image segmentation, Remote Sensing 13 (18) (2021) 3585.
[11] R. Strudel, R. Garcia, I. Laptev, C. Schmid, Segmenter: Transformer for
semantic segmentation, in: Proceedings of the IEEE/CVF International
Conference on Computer Vision, 2021, pp. 7262{7272.
27[12] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, M. Shah,
Transformers in vision: A survey, ACM computing surveys (CSUR)
54 (10s) (2022) 1{41.
[13] Y. Liu, Y. Zhang, Y. Wang, F. Hou, J. Yuan, J. Tian, Y. Zhang, Z. Shi,
J. Fan, Z. He, A survey of visual transformers, IEEE Transactions on
Neural Networks and Learning Systems (2023).
[14] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao,
C. Xu, Y. Xu, et al., A survey on vision transformer, IEEE transactions
on pattern analysis and machine intelligence (2022).
[15] J. Long, E. Shelhamer, T. Darrell, Fully convolutional networks for se-
mantic segmentation, in: Proceedings of the IEEE conference on com-
puter vision and pattern recognition, 2015, pp. 3431{3440.
[16] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Mis-
awa, K. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz, et al., At-
tention u-net: Learning where to look for the pancreas, arXiv preprint
arXiv:1804.03999 (2018).
[17] F. I. Diakogiannis, F. Waldner, P. Caccetta, C. Wu, Resunet-a: A deep
learning framework for semantic segmentation of remotely sensed data,
ISPRS Journal of Photogrammetry and Remote Sensing 162 (2020) 94{
114.
[18] Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, J. Liang, Unet++:
A nested u-net architecture for medical image segmentation, in: Deep
learning in medical image analysis and multimodal learning for clinical
decision support, Springer, 2018, pp. 3{11.
[19] S. Hochreiter, The vanishing gradient problem during learning recurrent
neural nets and problem solutions, International Journal of Uncertainty,
Fuzziness and Knowledge-Based Systems 6 (02) (1998) 107{116.
[20] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya,
J. Shlens, Stand-alone self-attention in vision models, Advances in Neu-
ral Information Processing Systems 32 (2019).
[21] L. Zhu, J. Suomalainen, J. Liu, J. Hyypp a, H. Kaartinen, H. Haggren,
et al., A review: Remote sensing sensors, Multi-purposeful application
of geospatial data (2018) 19{42.
28[22] M. Schmitt, J. Prexl, P. Ebel, L. Liebel, X. X. Zhu, Weakly super-
vised semantic segmentation of satellite images for land cover mapping{
challenges and opportunities, arXiv preprint arXiv:2002.08254 (2020).
[23] L. P. Olander, H. K. Gibbs, M. Steininger, J. J. Swenson, B. C. Murray,
Reference scenarios for deforestation and forest degradation in support
of redd: a review of data and methods, Environmental Research Letters
3 (2) (2008) 025011.
[24] F. Pacici, F. Del Frate, C. Solimini, W. J. Emery, An innovative
neural-net method to detect temporal changes in high-resolution op-
tical satellite imagery, IEEE Transactions on Geoscience and Remote
Sensing 45 (9) (2007) 2940{2952.
[25] A. Boguszewski, D. Batorski, N. Ziemba-Jankowska, T. Dziedzic,
A. Zambrzycka, Landcover. ai: Dataset for automatic mapping of build-
ings, woodlands, water and roads from aerial imagery, in: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition, 2021, pp. 1102{1110.
[26] L. P. Osco, J. M. Junior, A. P. M. Ramos, L. A. de Castro Jorge, S. N.
Fatholahi, J. de Andrade Silva, E. T. Matsubara, H. Pistori, W. N.
Gon calves, J. Li, A review on deep learning in uav remote sensing,
International Journal of Applied Earth Observation and Geoinformation
102 (2021) 102456.
[27] J. Wang, Z. Zheng, A. Ma, X. Lu, Y. Zhong, Loveda: A remote sensing
land-cover dataset for domain adaptive semantic segmentation, arXiv
preprint arXiv:2110.08733 (2021).
[28] I. Demir, K. Koperski, D. Lindenbaum, G. Pang, J. Huang, S. Basu,
F. Hughes, D. Tuia, R. Raskar, Deepglobe 2018: A challenge to parse the
earth through satellite images, in: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition Workshops, 2018, pp. 172{
181.
[29] L. Ding, D. Lin, S. Lin, J. Zhang, X. Cui, Y. Wang, H. Tang, L. Bruz-
zone, Looking outside the window: Wide-context transformer for the
semantic segmentation of high-resolution remote sensing images, IEEE
Transactions on Geoscience and Remote Sensing 60 (2022) 1{13.
[30] S. D. Olabarriaga, A. W. Smeulders, Interaction in the segmentation of
medical images: A survey, Medical image analysis 5 (2) (2001) 127{142.
29[31] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for
biomedical image segmentation, in: International Conference on Medical
image computing and computer-assisted intervention, Springer, 2015,
pp. 234{241.
[32] Z. Gu, J. Cheng, H. Fu, K. Zhou, H. Hao, Y. Zhao, T. Zhang, S. Gao,
J. Liu, Ce-net: Context encoder network for 2d medical image segmen-
tation, IEEE transactions on medical imaging 38 (10) (2019) 2281{2292.
[33] H. Huang, L. Lin, R. Tong, H. Hu, Q. Zhang, Y. Iwamoto, X. Han, Y.-
W. Chen, J. Wu, Unet 3+: A full-scale connected unet for medical image
segmentation, in: ICASSP 2020-2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2020, pp.
1055{1059.
[34] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille,
Y. Zhou, Transunet: Transformers make strong encoders for medical
image segmentation, arXiv preprint arXiv:2102.04306 (2021).
[35] H. Cao, Y. Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, M. Wang,
Swin-unet: Unet-like pure transformer for medical image segmentation,
in: Computer Vision{ECCV 2022 Workshops: Tel Aviv, Israel, October
23{27, 2022, Proceedings, Part III, Springer, 2023, pp. 205{218.
[36] A. I sn, C. Direko glu, M. S ah, Review of mri-based brain tumor image
segmentation using deep learning methods, Procedia Computer Science
102 (2016) 317{324.
[37] N. C. Codella, D. Gutman, M. E. Celebi, B. Helba, M. A. Marchetti,
S. W. Dusza, A. Kalloo, K. Liopyris, N. Mishra, H. Kittler, et al., Skin
lesion analysis toward melanoma detection: A challenge at the 2017
international symposium on biomedical imaging (isbi), hosted by the
international skin imaging collaboration (isic), in: 2018 IEEE 15th in-
ternational symposium on biomedical imaging (ISBI 2018), IEEE, 2018,
pp. 168{172.
[38] P. Bilic, P. F. Christ, E. Vorontsov, G. Chlebus, H. Chen, Q. Dou, C.-W.
Fu, X. Han, P.-A. Heng, J. Hesser, et al., The liver tumor segmentation
benchmark (lits), arXiv preprint arXiv:1901.04056 (2019).
[39] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani,
J. Kirby, Y. Burren, N. Porz, J. Slotboom, R. Wiest, et al., The multi-
modal brain tumor image segmentation benchmark (brats), IEEE trans-
actions on medical imaging 34 (10) (2014) 1993{2024.
30[40] D. Gorecky, M. Schmitt, M. Loskyll, D. Z uhlke, Human-machine-
interaction in the industry 4.0 era, in: 2014 12th IEEE international
conference on industrial informatics (INDIN), Ieee, 2014, pp. 289{294.
[41] R. T. Azuma, A survey of augmented reality, Presence: teleoperators &
virtual environments 6 (4) (1997) 355{385.
[42] J. Janai, F. G uney, A. Behl, A. Geiger, et al., Computer vision for au-
tonomous vehicles: Problems, datasets and state of the art, Foundations
and Trends ®in Computer Graphics and Vision 12 (1{3) (2020) 1{308.
[43] T. Gevers, A. Smeulders, Image search engines: An overview, Emerging
Topics in Computer Vision (2004) 1{54.
[44] S. Jain, X. Wang, J. E. Gonzalez, Accel: A corrective fusion network
for ecient semantic segmentation on video, in: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2019, pp. 8866{8875.
[45] M. Ding, Z. Wang, B. Zhou, J. Shi, Z. Lu, P. Luo, Every frame counts:
Joint learning of video segmentation and optical ow, in: Proceedings
of the AAAI Conference on Articial Intelligence, Vol. 34, 2020, pp.
10713{10720.
[46] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benen-
son, U. Franke, S. Roth, B. Schiele, The cityscapes dataset for semantic
urban scene understanding, in: Proceedings of the IEEE conference on
computer vision and pattern recognition, 2016, pp. 3213{3223.
[47] G. J. Brostow, J. Fauqueur, R. Cipolla, Semantic object classes in video:
A high-denition ground truth database, Pattern Recognition Letters
30 (2) (2009) 88{97.
[48] S. R. Richter, V. Vineet, S. Roth, V. Koltun, Playing for data: Ground
truth from computer games, in: European conference on computer vi-
sion, Springer, 2016, pp. 102{118.
[49] E. Shelhamer, K. Rakelly, J. Homan, T. Darrell, Clockwork convnets
for video semantic segmentation, in: European Conference on Computer
Vision, Springer, 2016, pp. 852{868.
[50] X. Zhu, Y. Xiong, J. Dai, L. Yuan, Y. Wei, Deep feature ow for video
recognition, in: Proceedings of the IEEE conference on computer vision
and pattern recognition, 2017, pp. 2349{2358.
31[51] B. Mahasseni, S. Todorovic, A. Fern, Budget-aware deep semantic video
segmentation, in: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2017, pp. 1029{1038.
[52] A. Garcia-Garcia, S. Orts-Escolano, S. Oprea, V. Villena-Martinez,
P. Martinez-Gonzalez, J. Garcia-Rodriguez, A survey on deep learn-
ing techniques for image and video semantic segmentation, Applied Soft
Computing 70 (2018) 41{65.
[53] Y. Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, H. Xia, End-to-
end video instance segmentation with transformers, in: Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition,
2021, pp. 8741{8750.
[54] S. Yang, X. Wang, Y. Li, Y. Fang, J. Fang, W. Liu, X. Zhao, Y. Shan,
Temporally ecient vision transformer for video instance segmentation,
in: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2022, pp. 2885{2895.
[55] J. Wu, Y. Jiang, S. Bai, W. Zhang, X. Bai, Seqformer: Sequential trans-
former for video instance segmentation, in: Computer Vision{ECCV
2022: 17th European Conference, Tel Aviv, Israel, October 23{27, 2022,
Proceedings, Part XXVIII, Springer, 2022, pp. 553{569.
[56] S. Gustavsson, Object detection and semantic segmentation using self-
supervised learning (2021).
[57] X. Liu, F. Zhang, Z. Hou, L. Mian, Z. Wang, J. Zhang, J. Tang, Self-
supervised learning: Generative or contrastive, IEEE Transactions on
Knowledge and Data Engineering (2021).
[58] T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Doll ar, Focal loss for dense
object detection, in: Proceedings of the IEEE international conference
on computer vision, 2017, pp. 2980{2988.
[59] F. Milletari, N. Navab, S.-A. Ahmadi, V-net: Fully convolutional neural
networks for volumetric medical image segmentation, in: 2016 fourth
international conference on 3D vision (3DV), IEEE, 2016, pp. 565{571.
[60] S. Jadon, A survey of loss functions for semantic segmentation, in: 2020
IEEE Conference on Computational Intelligence in Bioinformatics and
Computational Biology (CIBCB), IEEE, 2020, pp. 1{7.
32[61] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler, R. Ur-
tasun, A. Yuille, The role of context for object detection and semantic
segmentation in the wild, in: Proceedings of the IEEE conference on
computer vision and pattern recognition, 2014, pp. 891{898.
[62] M. Everingham, J. Winn, The pascal visual object classes challenge 2012
(voc2012) development kit, Pattern Anal. Stat. Model. Comput. Learn.,
Tech. Rep 2007 (2012) 1{45.
[63] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, A. Torralba, Scene
parsing through ade20k dataset, in: Proceedings of the IEEE conference
on computer vision and pattern recognition, 2017, pp. 633{641.
[64] S. Kuutti, R. Bowden, Y. Jin, P. Barber, S. Fallah, A survey of deep
learning applications to autonomous vehicle control, IEEE Transactions
on Intelligent Transportation Systems 22 (2) (2020) 712{733.
[65] G. Varma, A. Subramanian, A. Namboodiri, M. Chandraker, C. Jawa-
har, Idd: A dataset for exploring problems of autonomous navigation
in unconstrained environments, in: 2019 IEEE Winter Conference on
Applications of Computer Vision (WACV), IEEE, 2019, pp. 1743{1751.
[66] A. Gaidon, Q. Wang, Y. Cabon, E. Vig, Virtual worlds as proxy for
multi-object tracking analysis, in: Proceedings of the IEEE conference
on computer vision and pattern recognition, 2016, pp. 4340{4349.
[67] E. Alberti, A. Tavera, C. Masone, B. Caputo, Idda: a large-scale multi-
domain dataset for autonomous driving, IEEE Robotics and Automation
Letters 5 (4) (2020) 5526{5533.
[68] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image
recognition, in: Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770{778.
[69] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, P. Luo,
Segformer: Simple and ecient design for semantic segmentation with
transformers, Advances in Neural Information Processing Systems 34
(2021) 12077{12090.
[70] X. Chu, Z. Tian, B. Zhang, X. Wang, X. Wei, H. Xia, C. Shen, Con-
ditional positional encodings for vision transformers, arXiv preprint
arXiv:2102.10882 (2021).
33[71] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo,
L. Shao, Pyramid vision transformer: A versatile backbone for dense
prediction without convolutions, in: Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision, 2021, pp. 568{578.
[72] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo,
L. Shao, Pvt v2: Improved baselines with pyramid vision transformer,
Computational Visual Media 8 (3) (2022) 415{424.
[73] X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, C. Shen,
Twins: Revisiting the design of spatial attention in vision transformers,
Advances in Neural Information Processing Systems 34 (2021) 9355{
9366.
[74] R. Ranftl, A. Bochkovskiy, V. Koltun, Vision transformers for dense
prediction, in: Proceedings of the IEEE/CVF International Conference
on Computer Vision, 2021, pp. 12179{12188.
[75] Y. Yuan, R. Fu, L. Huang, W. Lin, C. Zhang, X. Chen, J. Wang,
Hrformer: High-resolution vision transformer for dense predict, Ad-
vances in Neural Information Processing Systems 34 (2021) 7281{7293.
[76] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu,
M. Tan, X. Wang, et al., Deep high-resolution representation learning for
visual recognition, IEEE transactions on pattern analysis and machine
intelligence 43 (10) (2020) 3349{3364.
[77] H. Hu, Z. Zhang, Z. Xie, S. Lin, Local relation networks for image
recognition, in: Proceedings of the IEEE/CVF International Conference
on Computer Vision, 2019, pp. 3464{3473.
[78] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, R. Girdhar, Masked-
attention mask transformer for universal image segmentation, in: Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2022, pp. 1290{1299.
[79] B. Cheng, A. Schwing, A. Kirillov, Per-pixel classication is not all
you need for semantic segmentation, Advances in Neural Information
Processing Systems 34 (2021) 17864{17875.
[80] T.-Y. Lin, P. Doll ar, R. Girshick, K. He, B. Hariharan, S. Belongie,
Feature pyramid networks for object detection, in: Proceedings of the
IEEE conference on computer vision and pattern recognition, 2017, pp.
2117{2125.
34[81] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, H. J egou,
Training data-ecient image transformers & distillation through atten-
tion, in: International Conference on Machine Learning, PMLR, 2021,
pp. 10347{10357.
35Relational inductive biases, deep learning, and graph networks
Peter W. Battaglia1,Jessica B. Hamrick1,Victor Bapst1,
Alvaro Sanchez-Gonzalez1,Vinicius Zambaldi1,Mateusz Malinowski1,
Andrea Tacchetti1,David Raposo1,Adam Santoro1,Ryan Faulkner1,
Caglar Gulcehre1,Francis Song1,Andrew Ballard1,Justin Gilmer2,
George Dahl2,Ashish Vaswani2,Kelsey Allen3,Charles Nash4,
Victoria Langston1,Chris Dyer1,Nicolas Heess1,
Daan Wierstra1,Pushmeet Kohli1,Matt Botvinick1,
Oriol Vinyals1,Yujia Li1,Razvan Pascanu1
1DeepMind;2Google Brain;3MIT;4University of Edinburgh
Abstract
Articial intelligence (AI) has undergone a renaissance recently, making major progress in
key domains such as vision, language, control, and decision-making. This has been due, in
part, to cheap data and cheap compute resources, which have t the natural strengths of deep
learning. However, many dening characteristics of human intelligence, which developed under
much dierent pressures, remain out of reach for current approaches. In particular, generalizing
beyond one's experiences|a hallmark of human intelligence from infancy|remains a formidable
challenge for modern AI.
The following is part position paper, part review, and part unication. We argue that
combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that
structured representations and computations are key to realizing this objective. Just as biology
uses nature and nurture cooperatively, we reject the false choice between \hand-engineering"
and \end-to-end" learning, and instead advocate for an approach which benets from their
complementary strengths. We explore how using relational inductive biases within deep learning
architectures can facilitate learning about entities, relations, and rules for composing them. We
present a new building block for the AI toolkit with a strong relational inductive bias|the graph
network |which generalizes and extends various approaches for neural networks that operate
on graphs, and provides a straightforward interface for manipulating structured knowledge and
producing structured behaviors. We discuss how graph networks can support relational reasoning
and combinatorial generalization, laying the foundation for more sophisticated, interpretable,
and exible patterns of reasoning. As a companion to this paper, we have also released an
open-source software library for building graph networks, with demonstrations of how to use
them in practice.
1 Introduction
A key signature of human intelligence is the ability to make \innite use of nite means" (Humboldt,
1836; Chomsky, 1965), in which a small set of elements (such as words) can be productively
composed in limitless ways (such as into new sentences). This reects the principle of combinatorial
generalization , that is, constructing new inferences, predictions, and behaviors from known building
blocks. Here we explore how to improve modern AI's capacity for combinatorial generalization by
Corresponding author: peterbattaglia@google.com
1arXiv:1806.01261v3  [cs.LG]  17 Oct 2018biasing learning towards structured representations and computations, and in particular, systems
that operate on graphs.
Humans' capacity for combinatorial generalization depends critically on our cognitive mecha-
nisms for representing structure and reasoning about relations. We represent complex systems as
compositions of entities and their interactions1(Navon, 1977; McClelland and Rumelhart, 1981;
Plaut et al., 1996; Marcus, 2001; Goodwin and Johnson-Laird, 2005; Kemp and Tenenbaum, 2008),
such as judging whether a haphazard stack of objects is stable (Battaglia et al., 2013). We use
hierarchies to abstract away from ne-grained dierences, and capture more general commonalities
between representations and behaviors (Botvinick, 2008; Tenenbaum et al., 2011), such as parts of
an object, objects in a scene, neighborhoods in a town, and towns in a country. We solve novel
problems by composing familiar skills and routines (Anderson, 1982), for example traveling to a
new location by composing familiar procedures and objectives, such as \travel by airplane", \to
San Diego", \eat at", and \an Indian restaurant". We draw analogies by aligning the relational
structure between two domains and drawing inferences about one based on corresponding knowledge
about the other (Gentner and Markman, 1997; Hummel and Holyoak, 2003).
Kenneth Craik's \The Nature of Explanation" (1943), connects the compositional structure of
the world to how our internal mental models are organized:
...[a human mental model] has a similar relation-structure to that of the process it
imitates. By `relation-structure' I do not mean some obscure non-physical entity which
attends the model, but the fact that it is a working physical model which works in the
same way as the process it parallels... physical reality is built up, apparently, from a few
fundamental types of units whose properties determine many of the properties of the
most complicated phenomena, and this seems to aord a sucient explanation of the
emergence of analogies between mechanisms and similarities of relation-structure among
these combinations without the necessity of any theory of objective universals. (Craik,
1943, page 51-55)
That is, the world is compositional, or at least, we understand it in compositional terms. When
learning, we either t new knowledge into our existing structured representations, or adjust the
structure itself to better accommodate (and make use of) the new and the old (Tenenbaum et al.,
2006; Griths et al., 2010; Ullman et al., 2017).
The question of how to build articial systems which exhibit combinatorial generalization has
been at the heart of AI since its origins, and was central to many structured approaches, including
logic, grammars, classic planning, graphical models, causal reasoning, Bayesian nonparametrics, and
probabilistic programming (Chomsky, 1957; Nilsson and Fikes, 1970; Pearl, 1986, 2009; Russell and
Norvig, 2009; Hjort et al., 2010; Goodman et al., 2012; Ghahramani, 2015). Entire sub-elds have
focused on explicit entity- and relation-centric learning, such as relational reinforcement learning
(D zeroski et al., 2001) and statistical relational learning (Getoor and Taskar, 2007). A key reason
why structured approaches were so vital to machine learning in previous eras was, in part, because
data and computing resources were expensive, and the improved sample complexity aorded by
structured approaches' strong inductive biases was very valuable.
In contrast with past approaches in AI, modern deep learning methods (LeCun et al., 2015;
Schmidhuber, 2015; Goodfellow et al., 2016) often follow an \end-to-end" design philosophy which
emphasizes minimal a priori representational and computational assumptions, and seeks to avoid
explicit structure and \hand-engineering". This emphasis has t well with|and has perhaps been
armed by|the current abundance of cheap data and cheap computing resources, which make
1Whether this entails a \language of thought" (Fodor, 1975) is beyond the scope of this work.
2trading o sample eciency for more exible learning a rational choice. The remarkable and rapid
advances across many challenging domains, from image classication (Krizhevsky et al., 2012;
Szegedy et al., 2017), to natural language processing (Sutskever et al., 2014; Bahdanau et al., 2015),
to game play (Mnih et al., 2015; Silver et al., 2016; Morav c k et al., 2017), are a testament to this
minimalist principle. A prominent example is from language translation, where sequence-to-sequence
approaches (Sutskever et al., 2014; Bahdanau et al., 2015) have proven very eective without using
explicit parse trees or complex relationships between linguistic entities.
Despite deep learning's successes, however, important critiques (Marcus, 2001; Shalev-Shwartz
et al., 2017; Lake et al., 2017; Lake and Baroni, 2018; Marcus, 2018a,b; Pearl, 2018; Yuille and
Liu, 2018) have highlighted key challenges it faces in complex language and scene understanding,
reasoning about structured data, transferring learning beyond the training conditions, and learning
from small amounts of experience. These challenges demand combinatorial generalization, and so it
is perhaps not surprising that an approach which eschews compositionality and explicit structure
struggles to meet them.
When deep learning's connectionist (Rumelhart et al., 1987) forebears were faced with analogous
critiques from structured, symbolic positions (Fodor and Pylyshyn, 1988; Pinker and Prince, 1988),
there was a constructive eort (Bobrow and Hinton, 1990; Marcus, 2001) to address the challenges
directly and carefully. A variety of innovative sub-symbolic approaches for representing and reasoning
about structured objects were developed in domains such as analogy-making, linguistic analysis,
symbol manipulation, and other forms of relational reasoning (Smolensky, 1990; Hinton, 1990;
Pollack, 1990; Elman, 1991; Plate, 1995; Eliasmith, 2013), as well as more integrative theories for
how the mind works (Marcus, 2001). Such work also helped cultivate more recent deep learning
advances which use distributed, vector representations to capture rich semantic content in text
(Mikolov et al., 2013; Pennington et al., 2014), graphs (Narayanan et al., 2016, 2017), algebraic and
logical expressions (Allamanis et al., 2017; Evans et al., 2018), and programs (Devlin et al., 2017;
Chen et al., 2018b).
We suggest that a key path forward for modern AI is to commit to combinatorial generalization
as a top priority, and we advocate for integrative approaches to realize this goal. Just as biology does
not choose between nature versus nurture|it uses nature and nurture jointly , to build wholes which
are greater than the sums of their parts|we, too, reject the notion that structure and exibility are
somehow at odds or incompatible, and embrace both with the aim of reaping their complementary
strengths. In the spirit of numerous recent examples of principled hybrids of structure-based methods
and deep learning (e.g., Reed and De Freitas, 2016; Garnelo et al., 2016; Ritchie et al., 2016; Wu
et al., 2017; Denil et al., 2017; Hudson and Manning, 2018), we see great promise in synthesizing
new techniques by drawing on the full AI toolkit and marrying the best approaches from today
with those which were essential during times when data and computation were at a premium.
Recently, a class of models has arisen at the intersection of deep learning and structured
approaches, which focuses on approaches for reasoning about explicitly structured data, in particular
graphs (e.g. Scarselli et al., 2009b; Bronstein et al., 2017; Gilmer et al., 2017; Wang et al., 2018c; Li
et al., 2018; Kipf et al., 2018; Gulcehre et al., 2018). What these approaches all have in common
is a capacity for performing computation over discrete entities and the relations between them.
What sets them apart from classical approaches is how the representations and structure of the
entities and relations|and the corresponding computations|can be learned, relieving the burden
of needing to specify them in advance. Crucially, these methods carry strong relational inductive
biases , in the form of specic architectural assumptions, which guide these approaches towards
learning about entities and relations (Mitchell, 1980), which we, joining many others (Spelke et al.,
1992; Spelke and Kinzler, 2007; Marcus, 2001; Tenenbaum et al., 2011; Lake et al., 2017; Lake and
Baroni, 2018; Marcus, 2018b), suggest are an essential ingredient for human-like intelligence.
3Box 1: Relational reasoning
We dene structure as the product of composing a set of known building blocks. \Structured
representations" capture this composition (i.e., the arrangement of the elements) and \structured
computations" operate over the elements and their composition as a whole. Relational reasoning,
then, involves manipulating structured representations of entities and relations , using rules
for how they can be composed. We use these terms to capture notions from cognitive science,
theoretical computer science, and AI, as follows:
Anentity is an element with attributes, such as a physical object with a size and mass.
Arelation is a property between entities. Relations between two objects might include
same size as ,heavier than , and distance from . Relations can have attributes as
well. The relation more than Xtimes heavier than takes an attribute, X, which
determines the relative weight threshold for the relation to be true vs.false . Relations
can also be sensitive to the global context. For a stone and a feather, the relation falls
with greater acceleration than depends on whether the context is in air vs.in a
vacuum . Here we focus on pairwise relations between entities.
Aruleis a function (like a non-binary logical predicate) that maps entities and relations
to other entities and relations, such as a scale comparison like is entity X large? and
is entity X heavier than entity Y? . Here we consider rules which take one or two
arguments (unary and binary), and return a unary property value.
As an illustrative example of relational reasoning in machine learning, graphical models (Pearl,
1988; Koller and Friedman, 2009) can represent complex joint distributions by making explicit
random conditional independences among random variables. Such models have been very
successful because they capture the sparse structure which underlies many real-world generative
processes and because they support ecient algorithms for learning and reasoning. For example,
hidden Markov models constrain latent states to be conditionally independent of others given
the state at the previous time step, and observations to be conditionally independent given the
latent state at the current time step, which are well-matched to the relational structure of many
real-world causal processes. Explicitly expressing the sparse dependencies among variables
provides for various ecient inference and reasoning algorithms, such as message-passing, which
apply a common information propagation procedure across localities within a graphical model,
resulting in a composable, and partially parallelizable, reasoning procedure which can be applied
to graphical models of dierent sizes and shape.
In the remainder of the paper, we examine various deep learning methods through the lens of
their relational inductive biases, showing that existing methods often carry relational assumptions
which are not always explicit or immediately evident. We then present a general framework for
entity- and relation-based reasoning|which we term graph networks |for unifying and extending
existing methods which operate on graphs, and describe key design principles for building powerful
architectures using graph networks as building blocks. We have also released an open-source library
for building graph networks, which can be found here: github.com/deepmind/graph nets .
2 Relational inductive biases
Many approaches in machine learning and AI which have a capacity for relational reasoning
4Box 2: Inductive biases
Learning is the process of apprehending useful knowledge by observing and interacting with the
world. It involves searching a space of solutions for one expected to provide a better explanation
of the data or to achieve higher rewards. But in many cases, there are multiple solutions which
are equally good (Goodman, 1955). An inductive bias allows a learning algorithm to prioritize
one solution (or interpretation) over another, independent of the observed data (Mitchell,
1980). In a Bayesian model, inductive biases are typically expressed through the choice and
parameterization of the prior distribution (Griths et al., 2010). In other contexts, an inductive
bias might be a regularization term (McClelland, 1994) added to avoid overtting, or it might
be encoded in the architecture of the algorithm itself. Inductive biases often trade exibility
for improved sample complexity and can be understood in terms of the bias-variance tradeo
(Geman et al., 1992). Ideally, inductive biases both improve the search for solutions without
substantially diminishing performance, as well as help nd solutions which generalize in a
desirable way; however, mismatched inductive biases can also lead to suboptimal performance
by introducing constraints that are too strong.
Inductive biases can express assumptions about either the data-generating process or the space
of solutions. For example, when tting a 1D function to data, linear least squares follows
the constraint that the approximating function be a linear model, and approximation errors
should be minimal under a quadratic penalty. This reects an assumption that the data-
generating process can be explained simply, as a line process corrupted by additive Gaussian
noise. Similarly, L2 regularization prioritizes solutions whose parameters have small values,
and can induce unique solutions and global structure to otherwise ill-posed problems. This can
be interpreted as an assumption about the learning process: that searching for good solutions
is easier when there is less ambiguity among solutions. Note, these assumptions need not be
explicit|they reect interpretations of how a model or algorithm interfaces with the world.
(Box 1) use a relational inductive bias . While not a precise, formal denition, we use this term to
refer generally to inductive biases (Box 2) which impose constraints on relationships and interactions
among entities in a learning process.
Creative new machine learning architectures have rapidly proliferated in recent years, with
(perhaps not surprisingly given the thesis of this paper) practitioners often following a design pattern
of composing elementary building blocks to form more complex, deep2computational hierarchies and
graphs3. Building blocks such as \fully connected" layers are stacked into \multilayer perceptrons"
(MLPs), \convolutional layers" are stacked into \convolutional neural networks" (CNNs), and a
standard recipe for an image processing network is, generally, some variety of CNN composed with
a MLP. This composition of layers provides a particular type of relational inductive bias|that
of hierarchical processing|in which computations are performed in stages, typically resulting in
increasingly long range interactions among information in the input signal. As we explore below, the
building blocks themselves also carry various relational inductive biases (Table 1). Though beyond
the scope of this paper, various non-relational inductive biases are used in deep learning as well: for
example, activation non-linearities, weight decay, dropout (Srivastava et al., 2014), batch and layer
normalization (Ioe and Szegedy, 2015; Ba et al., 2016), data augmentation, training curricula, and
optimization algorithms all impose constraints on the trajectory and outcome of learning.
2This pattern of composition in depth is ubiquitous in deep learning, and is where the \deep" comes from.
3Recent methods (Liu et al., 2018) even automate architecture construction via learned graph editing procedures.
5Component Entities Relations Rel. inductive bias Invariance
Fully connected Units All-to-all Weak -
Convolutional Grid elements Local Locality Spatial translation
Recurrent Timesteps Sequential Sequentiality Time translation
Graph network Nodes Edges Arbitrary Node, edge permutations
Table 1: Various relational inductive biases in standard deep learning components. See also Section 2.
To explore the relational inductive biases expressed within various deep learning methods, we
must identify several key ingredients, analogous to those in Box 1: what are the entities , what are
therelations , and what are the rules for composing entities and relations, and computing their
implications? In deep learning, the entities and relations are typically expressed as distributed
representations, and the rules as neural network function approximators; however, the precise forms
of the entities, relations, and rules vary between architectures. To understand these dierences
between architectures, we can further ask how each supports relational reasoning by probing:
The arguments to the rule functions (e.g., which entities and relations are provided as input).
How the rule function is reused , orshared , across the computational graph (e.g., across dierent
entities and relations, across dierent time or processing steps, etc.).
How the architecture denes interactions versus isolation among representations (e.g., by
applying rules to draw conclusions about related entities, versus processing them separately).
2.1 Relational inductive biases in standard deep learning building blocks
2.1.1 Fully connected layers
Perhaps the most common building block is a fully connected layer (Rosenblatt, 1961). Typically
implemented as a non-linear vector-valued function of vector inputs, each element, or \unit", of
the output vector is the dot product between a weight vector, followed by an added bias term, and
nally a non-linearity such as a rectied linear unit (ReLU). As such, the entities are the units in
the network, the relations are all-to-all (all units in layer iare connected to all units in layer j),
and the rules are specied by the weights and biases. The argument to the rule is the full input
signal, there is no reuse, and there is no isolation of information (Figure 1a). The implicit relational
inductive bias in a fully connected layer is thus very weak: all input units can interact to determine
any output unit's value, independently across outputs (Table 1).
2.1.2 Convolutional layers
Another common building block is a convolutional layer (Fukushima, 1980; LeCun et al., 1989). It is
implemented by convolving an input vector or tensor with a kernel of the same rank, adding a bias
term, and applying a point-wise non-linearity. The entities here are still individual units (or grid
elements, e.g. pixels), but the relations are sparser. The dierences between a fully connected layer
and a convolutional layer impose some important relational inductive biases: locality and translation
invariance (Figure 1b). Locality reects that the arguments to the relational rule are those entities in
close proximity with one another in the input signal's coordinate space, isolated from distal entities.
Translation invariance reects reuse of the same rule across localities in the input. These biases
are very eective for processing natural image data because there is high covariance within local
6(a) Fully connected
Sharing in space (b) Convolutional
Sharing in time (c) Recurrent
Figure 1: Reuse and sharing in common deep learning building blocks. (a) Fully connected layer,
in which all weights are independent, and there is no sharing. (b) Convolutional layer, in which
a local kernel function is reused multiple times across the input. Shared weights are indicated by
arrows with the same color. (c) Recurrent layer, in which the same function is reused across dierent
processing steps.
neighborhoods, which diminishes with distance, and because the statistics are mostly stationary
across an image (Table 1).
2.1.3 Recurrent layers
A third common building block is a recurrent layer (Elman, 1990), which is implemented over a
sequence of steps. Here, we can view the inputs and hidden states at each processing step as the
entities, and the Markov dependence of one step's hidden state on the previous hidden state and
the current input, as the relations. The rule for combining the entities takes a step's inputs and
hidden state as arguments to update the hidden state. The rule is reused over each step (Figure 1c),
which reects the relational inductive bias of temporal invariance (similar to a CNN's translational
invariance in space). For example, the outcome of some physical sequence of events should not
depend on the time of day. RNNs also carry a bias for locality in the sequence via their Markovian
structure (Table 1).
2.2 Computations over sets and graphs
While the standard deep learning toolkit contains methods with various forms of relational inductive
biases, there is no \default" deep learning component which operates on arbitrary relational structure.
We need models with explicit representations of entities and relations, and learning algorithms which
nd rules for computing their interactions, as well as ways of grounding them in data. Importantly,
entities in the world (such as objects and agents) do not have a natural order; rather, orderings
can be dened by the properties of their relations. For example, the relations between the sizes of
a set of objects can potentially be used to order them, as can their masses, ages, toxicities, and
prices. Invariance to ordering|except in the face of relations|is a property that should ideally be
reected by a deep learning component for relational reasoning.
Sets are a natural representation for systems which are described by entities whose order is
undened or irrelevant; in particular, their relational inductive bias does not come from the presence
of something, but rather from the absence . For illustration, consider the task of predicting the center
7H H
O
The brown 
dog jumped. The 
brown dog jumped (a) (b) 
(c) (d) 
(e) (f) 
Molecule Mass-Spring System 
n-body System Rigid Body System 
Sentence and Parse Tree Image and Fully-Connected Scene Graph Figure 2: Dierent graph representations. (a) A molecule, in which each atom is represented as a
node and edges correspond to bonds (e.g. Duvenaud et al., 2015). (b) A mass-spring system, in
which the rope is dened by a sequence of masses which are represented as nodes in the graph (e.g.
Battaglia et al., 2016; Chang et al., 2017). (c) A n-body system, in which the bodies are nodes and
the underlying graph is fully connected (e.g. Battaglia et al., 2016; Chang et al., 2017). (d) A rigid
body system, in which the balls and walls are nodes, and the underlying graph denes interactions
between the balls and between the balls and the walls (e.g. Battaglia et al., 2016; Chang et al., 2017).
(e) A sentence, in which the words correspond to leaves in a tree, and the other nodes and edges
could be provided by a parser (e.g. Socher et al., 2013). Alternately, a fully connected graph could
be used (e.g. Vaswani et al., 2017). (f) An image, which can be decomposed into image patches
corresponding to nodes in a fully connected graph (e.g. Santoro et al., 2017; Wang et al., 2018c).
of mass of a solar system comprised of nplanets, whose attributes (e.g., mass, position, velocity,
etc.) are denoted by fx1;x2;:::;xng. For such a computation, the order in which we consider the
planets does not matter because the state can be described solely in terms of aggregated, averaged
quantities. However, if we were to use a MLP for this task, having learned the prediction for a
particular input ( x1;x2;:::;xn) would not necessarily transfer to making a prediction for the same
inputs under a dierent ordering ( xn;x1;:::;x2). Since there are n! such possible permutations, in
the worst case, the MLP could consider each ordering as fundamentally dierent, and thus require
an exponential number of input/output training examples to learn an approximating function.
A natural way to handle such combinatorial explosion is to only allow the prediction to depend
on symmetric functions of the inputs' attributes. This might mean computing shared per-object
8featuresff(x1);:::;f (xn)gwhich are then aggregated in a symmetric way (for example, by taking
their mean). Such an approach is the essence of the Deep Sets and related models (Zaheer et al.,
2017; Edwards and Storkey, 2016; Pevn y and Somol, 2017), which we explore further in Section 4.2.3.
Of course, permutation invariance is not the only important form of underlying structure in
many problems. For example, each object in a set may be aected by pairwise interactions with
the other objects in the set (Hartford et al., 2018). In our planets scenario, consider now the
task of predicting each individual planet's position after a time interval,  t. In this case, using
aggregated, averaged information is not enough because the movement of each planet depends on
the forces the other planets are exerting on it. Instead, we could compute the state of each object
asx0
i=f(xi;P
jg(xi;xj)), wheregcould compute the force induced by the j-th planet on the
i-th planet, and fcould compute the future state of the i-th planet which results from the forces
and dynamics. The fact that we use the same geverywhere is again a consequence of the global
permutation invariance of the system; however, it also supports a dierent relational structure
becausegnow takes two arguments rather than one.4
The above solar system examples illustrate two relational structures: one in which there are
no relations, and one which consists of all pairwise relations. Many real-world systems (such as
in Figure 2) have a relational structure somewhere in between these two extremes, however, with
some pairs of entities possessing a relation and others lacking one. In our solar system example, if
the system instead consists of the planets and their moons, one may be tempted to approximate
it by neglecting the interactions between moons of dierent planets. In practice, this means
computing interactions only between some pairs of objects, i.e. x0
i=f(xi;P
j2(i)g(xi;xj)), where
(i)f1;:::;ngis a neighborhood around node i. This corresponds to a graph, in that the i-th
object only interacts with a subset of the other objects, described by its neighborhood. Note, the
updated states still do not depend in the order in which we describe the neighborhood.5
Graphs, generally, are a representation which supports arbitrary (pairwise) relational struc-
ture, and computations over graphs aord a strong relational inductive bias beyond that which
convolutional and recurrent layers can provide.
3 Graph networks
Neural networks that operate on graphs, and structure their computations accordingly, have been
developed and explored extensively for more than a decade under the umbrella of \graph neural
networks" (Gori et al., 2005; Scarselli et al., 2005, 2009a; Li et al., 2016), but have grown rapidly
in scope and popularity in recent years. We survey the literature on these methods in the next
sub-section (3.1). Then in the remaining sub-sections, we present our graph networks framework,
which generalizes and extends several lines of work in this area.
3.1 Background
Models in the graph neural network family (Gori et al., 2005; Scarselli et al., 2005, 2009a; Li et al.,
2016) have been explored in a diverse range of problem domains, across supervised, semi-supervised,
unsupervised, and reinforcement learning settings. They have been eective at tasks thought to
have rich relational structure, such as visual scene understanding tasks (Raposo et al., 2017; Santoro
4We could extend this same analysis to increasingly entangled structures that depend on relations among triplets
(i.e.,g(xi;xj;xk)), quartets, and so on. We note that if we restrict these functions to only operate on subsets of xi
which are spatially close, then we end back up with something resembling CNNs. In the most entangled sense, where
there is a single relation function g(x1; : : : ;xn), we end back up with a construction similar to a fully connected layer.
5The invariance which this model enforces is the invariance under isomorphism of the graph.
9et al., 2017) and few-shot learning (Garcia and Bruna, 2018). They have also been used to learn
the dynamics of physical systems (Battaglia et al., 2016; Chang et al., 2017; Watters et al., 2017;
van Steenkiste et al., 2018; Sanchez-Gonzalez et al., 2018) and multi-agent systems (Sukhbaatar
et al., 2016; Hoshen, 2017; Kipf et al., 2018), to reason about knowledge graphs (Bordes et al., 2013;
O~ noro-Rubio et al., 2017; Hamaguchi et al., 2017), to predict the chemical properties of molecules
(Duvenaud et al., 2015; Gilmer et al., 2017), to predict trac on roads (Li et al., 2017; Cui et al.,
2018), to classify and segment images and videos (Wang et al., 2018c; Hu et al., 2017) and 3D
meshes and point clouds (Wang et al., 2018d), to classify regions in images (Chen et al., 2018a), to
perform semi-supervised text classication (Kipf and Welling, 2017), and in machine translation
(Vaswani et al., 2017; Shaw et al., 2018; Gulcehre et al., 2018). They have been used within both
model-free (Wang et al., 2018b) and model-based (Hamrick et al., 2017; Pascanu et al., 2017;
Sanchez-Gonzalez et al., 2018) continuous control, for model-free reinforcement learning (Hamrick
et al., 2018; Zambaldi et al., 2018), and for more classical approaches to planning (Toyer et al.,
2017).
Many traditional computer science problems, which involve reasoning about discrete entities and
structure, have also been explored with graph neural networks, such as combinatorial optimization
(Bello et al., 2016; Nowak et al., 2017; Dai et al., 2017), boolean satisability (Selsam et al., 2018),
program representation and verication (Allamanis et al., 2018; Li et al., 2016), modeling cellular
automata and Turing machines (Johnson, 2017), and performing inference in graphical models
(Yoon et al., 2018). Recent work has also focused on building generative models of graphs (Li et al.,
2018; De Cao and Kipf, 2018; You et al., 2018; Bojchevski et al., 2018), and unsupervised learning of
graph embeddings (Perozzi et al., 2014; Tang et al., 2015; Grover and Leskovec, 2016; Garc a-Dur an
and Niepert, 2017).
The works cited above are by no means an exhaustive list, but provide a representative cross-
section of the breadth of domains for which graph neural networks have proven useful. We point
interested readers to a number of existing reviews which examine the body of work on graph neural
networks in more depth. In particular, Scarselli et al. (2009a) provides an authoritative overview of
early graph neural network approaches. Bronstein et al. (2017) provides an excellent survey of deep
learning on non-Euclidean data, and explores graph neural nets, graph convolution networks, and
related spectral approaches. Recently, Gilmer et al. (2017) introduced the message-passing neural
network (MPNN), which unied various graph neural network and graph convolutional network
approaches (Monti et al., 2017; Bruna et al., 2014; Hena et al., 2015; Deerrard et al., 2016;
Niepert et al., 2016; Kipf and Welling, 2017; Bronstein et al., 2017) by analogy to message-passing
in graphical models. In a similar vein, Wang et al. (2018c) introduced the non-local neural network
(NLNN), which unied various \self-attention"-style methods (Vaswani et al., 2017; Hoshen, 2017;
Veli ckovi c et al., 2018) by analogy to methods from computer vision and graphical models for
capturing long range dependencies in signals.
3.2 Graph network (GN) block
We now present our graph networks (GN) framework, which denes a class of functions for
relational reasoning over graph-structured representations. Our GN framework generalizes and
extends various graph neural network, MPNN, and NLNN approaches (Scarselli et al., 2009a; Gilmer
et al., 2017; Wang et al., 2018c), and supports constructing complex architectures from simple
building blocks. Note, we avoided using the term \neural" in the \graph network" label to reect
that they can be implemented with functions other than neural networks, though here our focus is
on neural network implementations.
The main unit of computation in the GN framework is the GN block , a \graph-to-graph" module
10Box 3: Our denition of \graph"
Attributesviek
<latexit sha1_base64="4ton1cC0/WpHTbJYOP5RCFkc+ww=">AAAB83icbVDLSsNAFL3xWeur6tLNYBG6KokIuiy4cVnBPqAJZTK9aYdOJmFmIpTQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJU8G1cd1vZ2Nza3tnt7JX3T84PDqunZx2dZIphh2WiET1Q6pRcIkdw43AfqqQxqHAXji9K/zeEyrNE/loZikGMR1LHnFGjZV8P6ZmEkY5zofTYa3uNt0FyDrxSlKHEu1h7csfJSyLURomqNYDz01NkFNlOBM4r/qZxpSyKR3jwFJJY9RBvsg8J5dWGZEoUfZJQxbq742cxlrP4tBOFhn1qleI/3mDzES3Qc5lmhmUbHkoygQxCSkKICOukBkxs4QyxW1WwiZUUWZsTVVbgrf65XXSvWp6btN7uK63GmUdFTiHC2iABzfQgntoQwcYpPAMr/DmZM6L8+58LEc3nHLnDP7A+fwBXXGRzQ==</latexit><latexit sha1_base64="4ton1cC0/WpHTbJYOP5RCFkc+ww=">AAAB83icbVDLSsNAFL3xWeur6tLNYBG6KokIuiy4cVnBPqAJZTK9aYdOJmFmIpTQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJU8G1cd1vZ2Nza3tnt7JX3T84PDqunZx2dZIphh2WiET1Q6pRcIkdw43AfqqQxqHAXji9K/zeEyrNE/loZikGMR1LHnFGjZV8P6ZmEkY5zofTYa3uNt0FyDrxSlKHEu1h7csfJSyLURomqNYDz01NkFNlOBM4r/qZxpSyKR3jwFJJY9RBvsg8J5dWGZEoUfZJQxbq742cxlrP4tBOFhn1qleI/3mDzES3Qc5lmhmUbHkoygQxCSkKICOukBkxs4QyxW1WwiZUUWZsTVVbgrf65XXSvWp6btN7uK63GmUdFTiHC2iABzfQgntoQwcYpPAMr/DmZM6L8+58LEc3nHLnDP7A+fwBXXGRzQ==</latexit><latexit sha1_base64="4ton1cC0/WpHTbJYOP5RCFkc+ww=">AAAB83icbVDLSsNAFL3xWeur6tLNYBG6KokIuiy4cVnBPqAJZTK9aYdOJmFmIpTQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJU8G1cd1vZ2Nza3tnt7JX3T84PDqunZx2dZIphh2WiET1Q6pRcIkdw43AfqqQxqHAXji9K/zeEyrNE/loZikGMR1LHnFGjZV8P6ZmEkY5zofTYa3uNt0FyDrxSlKHEu1h7csfJSyLURomqNYDz01NkFNlOBM4r/qZxpSyKR3jwFJJY9RBvsg8J5dWGZEoUfZJQxbq742cxlrP4tBOFhn1qleI/3mDzES3Qc5lmhmUbHkoygQxCSkKICOukBkxs4QyxW1WwiZUUWZsTVVbgrf65XXSvWp6btN7uK63GmUdFTiHC2iABzfQgntoQwcYpPAMr/DmZM6L8+58LEc3nHLnDP7A+fwBXXGRzQ==</latexit><latexit sha1_base64="4ton1cC0/WpHTbJYOP5RCFkc+ww=">AAAB83icbVDLSsNAFL3xWeur6tLNYBG6KokIuiy4cVnBPqAJZTK9aYdOJmFmIpTQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJU8G1cd1vZ2Nza3tnt7JX3T84PDqunZx2dZIphh2WiET1Q6pRcIkdw43AfqqQxqHAXji9K/zeEyrNE/loZikGMR1LHnFGjZV8P6ZmEkY5zofTYa3uNt0FyDrxSlKHEu1h7csfJSyLURomqNYDz01NkFNlOBM4r/qZxpSyKR3jwFJJY9RBvsg8J5dWGZEoUfZJQxbq742cxlrP4tBOFhn1qleI/3mDzES3Qc5lmhmUbHkoygQxCSkKICOukBkxs4QyxW1WwiZUUWZsTVVbgrf65XXSvWp6btN7uK63GmUdFTiHC2iABzfQgntoQwcYpPAMr/DmZM6L8+58LEc3nHLnDP7A+fwBXXGRzQ==</latexit>uvskvrkuviek
Here we use \graph" to mean a directed, attributed multi-graph with a global attribute. In our
terminology, a node is denoted as vi, an edge as ek, and the global attributes as u. We also use
skandrkto indicate the indices of the sender and receiver nodes (see below), respectively, for
edgek. To be more precise, we dene these terms as:
Directed : one-way edges, from a \sender" node to a \receiver" node.
Attribute : properties that can be encoded as a vector, set, or even another graph.
Attributed : edges and vertices have attributes associated with them.
Global attribute : a graph-level attribute.
Multi-graph : there can be more than one edge between vertices, including self-edges.
Figure 2 shows a variety of dierent types of graphs corresponding to real data that we may be
interested in modeling, including physical systems, molecules, images, and text.
which takes a graph as input, performs computations over the structure, and returns a graph as
output. As described in Box 3, entities are represented by the graph's nodes , relations by the edges ,
and system-level properties by global attributes. The GN framework's block organization emphasizes
customizability and synthesizing new architectures which express desired relational inductive biases.
The key design principles are: Flexible representations (see Section 4.1); Congurable within-block
structure (see Section 4.2); and Composable multi-block architectures (see Section 4.3).
We introduce a motivating example to help make the GN formalism more concrete. Consider
predicting the movements a set of rubber balls in an arbitrary gravitational eld, which, instead of
bouncing against one another, each have one or more springs which connect them to some (or all) of
the others. We will refer to this running example throughout the denitions below, to motivate the
graph representation and the computations operating over it. Figure 2 depicts some other common
scenarios that can be represented by graphs and reasoned over using graph networks.
3.2.1 Denition of \graph"
Within our GN framework, a graph is dened as a 3-tuple G=(u;V;E )(see Box 3 for details of
graph representations). The uis a global attribute; for example, umight represent the gravitational
eld. TheV=fvigi=1:Nvis the set of nodes (of cardinality Nv), where each viis a node's attribute.
For example, Vmight represent each ball, with attributes for position, velocity, and mass. The
E=f(ek;rk;sk)gk=1:Neis the set of edges (of cardinality Ne), where each ekis the edge's attribute,
rkis the index of the receiver node, and skis the index of the sender node. For example, Emight
represent the presence of springs between dierent balls, and their corresponding spring constants.
11Algorithm 1 Steps of computation in a full GN block.
function GraphNetwork (E,V,u)
fork2f1:::Negdo
e0
k e(ek;vrk;vsk;u) .1. Compute updated edge attributes
end for
fori2f1:::Nngdo
letE0
i=f(e0
k;rk;sk)grk=i;k=1:Ne
 e0
i e!v(E0
i) .2. Aggregate edge attributes per node
v0
i v( e0
i;vi;u) .3. Compute updated node attributes
end for
letV0=fv0gi=1:Nv
letE0=f(e0
k;rk;sk)gk=1:Ne
 e0 e!u(E0) .4. Aggregate edge attributes globally
 v0 v!u(V0) .5. Aggregate node attributes globally
u0 u( e0; v0;u) .6. Compute updated global attribute
return (E0;V0;u0)
end function
3.2.2 Internal structure of a GN block
A GN block contains three \update" functions, , and three \aggregation" functions, ,
e0
k=e(ek;vrk;vsk;u)
v0
i=v 
 e0
i;vi;u
u0=u 
 e0; v0;u e0
i=e!v 
E0
i
 e0=e!u 
E0
 v0=v!u 
V0(1)
whereE0
i=f(e0
k;rk;sk)grk=i;k=1:Ne,V0=fv0
igi=1:Nv, andE0=S
iE0
i=f(e0
k;rk;sk)gk=1:Ne.
Theeis mapped across all edges to compute per-edge updates, the vis mapped across all
nodes to compute per-node updates, and the uis applied once as the global update. The 
functions each take a set as input, and reduce it to a single element which represents the aggregated
information. Crucially, the functions must be invariant to permutations of their inputs, and should
take variable numbers of arguments (e.g., elementwise summation, mean, maximum, etc.).
3.2.3 Computational steps within a GN block
When a graph, G, is provided as input to a GN block, the computations proceed from the edge, to
the node, to the global level. Figure 3 shows a depiction of which graph elements are involved in
each of these computations, and Figure 4a shows a full GN block, with its update and aggregation
functions. Algorithm 1 shows the following steps of computation:
1.eis applied per edge, with arguments ( ek;vrk;vsk;u), and returns e0
k. In our springs example,
this might correspond to the forces or potential energies between two connected balls. The
set of resulting per-edge outputs for each node, i, is,E0
i=f(e0
k;rk;sk)grk=i;k=1:Ne. And
E0=S
iE0
i=f(e0
k;rk;sk)gk=1:Neis the set of all per-edge outputs.
2.e!vis applied to E0
i, and aggregates the edge updates for edges that project to vertex i, into
 e0
i, which will be used in the next step's node update. In our running example, this might
correspond to summing all the forces or potential energies acting on the ithball.
12vi
<latexit sha1_base64="UuhsKP3lpHlY+K0A8uvGImQtNkI=">AAAB83icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFNKJPppB06mYSZm0IJ/Q03LhRx68+482+ctllo64GBwzn3cs+cMJXCoOt+O6Wt7Z3dvfJ+5eDw6PikenrWMUmmGW+zRCa6F1LDpVC8jQIl76Wa0ziUvBtO7hd+d8q1EYl6wlnKg5iOlIgEo2gl348pjsMon84HYlCtuQ13CbJJvILUoEBrUP3yhwnLYq6QSWpM33NTDHKqUTDJ5xU/MzylbEJHvG+pojE3Qb7MPCdXVhmSKNH2KSRL9fdGTmNjZnFoJxcZzbq3EP/z+hlGd0EuVJohV2x1KMokwYQsCiBDoTlDObOEMi1sVsLGVFOGtqaKLcFb//Im6Vw3PLfhPd7UmvWijjJcwCXUwYNbaMIDtKANDFJ4hld4czLnxXl3PlajJafYOYc/cD5/AHRgkdw=</latexit><latexit sha1_base64="UuhsKP3lpHlY+K0A8uvGImQtNkI=">AAAB83icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFNKJPppB06mYSZm0IJ/Q03LhRx68+482+ctllo64GBwzn3cs+cMJXCoOt+O6Wt7Z3dvfJ+5eDw6PikenrWMUmmGW+zRCa6F1LDpVC8jQIl76Wa0ziUvBtO7hd+d8q1EYl6wlnKg5iOlIgEo2gl348pjsMon84HYlCtuQ13CbJJvILUoEBrUP3yhwnLYq6QSWpM33NTDHKqUTDJ5xU/MzylbEJHvG+pojE3Qb7MPCdXVhmSKNH2KSRL9fdGTmNjZnFoJxcZzbq3EP/z+hlGd0EuVJohV2x1KMokwYQsCiBDoTlDObOEMi1sVsLGVFOGtqaKLcFb//Im6Vw3PLfhPd7UmvWijjJcwCXUwYNbaMIDtKANDFJ4hld4czLnxXl3PlajJafYOYc/cD5/AHRgkdw=</latexit><latexit sha1_base64="UuhsKP3lpHlY+K0A8uvGImQtNkI=">AAAB83icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFNKJPppB06mYSZm0IJ/Q03LhRx68+482+ctllo64GBwzn3cs+cMJXCoOt+O6Wt7Z3dvfJ+5eDw6PikenrWMUmmGW+zRCa6F1LDpVC8jQIl76Wa0ziUvBtO7hd+d8q1EYl6wlnKg5iOlIgEo2gl348pjsMon84HYlCtuQ13CbJJvILUoEBrUP3yhwnLYq6QSWpM33NTDHKqUTDJ5xU/MzylbEJHvG+pojE3Qb7MPCdXVhmSKNH2KSRL9fdGTmNjZnFoJxcZzbq3EP/z+hlGd0EuVJohV2x1KMokwYQsCiBDoTlDObOEMi1sVsLGVFOGtqaKLcFb//Im6Vw3PLfhPd7UmvWijjJcwCXUwYNbaMIDtKANDFJ4hld4czLnxXl3PlajJafYOYc/cD5/AHRgkdw=</latexit><latexit sha1_base64="UuhsKP3lpHlY+K0A8uvGImQtNkI=">AAAB83icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFNKJPppB06mYSZm0IJ/Q03LhRx68+482+ctllo64GBwzn3cs+cMJXCoOt+O6Wt7Z3dvfJ+5eDw6PikenrWMUmmGW+zRCa6F1LDpVC8jQIl76Wa0ziUvBtO7hd+d8q1EYl6wlnKg5iOlIgEo2gl348pjsMon84HYlCtuQ13CbJJvILUoEBrUP3yhwnLYq6QSWpM33NTDHKqUTDJ5xU/MzylbEJHvG+pojE3Qb7MPCdXVhmSKNH2KSRL9fdGTmNjZnFoJxcZzbq3EP/z+hlGd0EuVJohV2x1KMokwYQsCiBDoTlDObOEMi1sVsLGVFOGtqaKLcFb//Im6Vw3PLfhPd7UmvWijjJcwCXUwYNbaMIDtKANDFJ4hld4czLnxXl3PlajJafYOYc/cD5/AHRgkdw=</latexit>u
<latexit sha1_base64="Wl/NKcf+4FQq41kPZqpr8GSpKP8=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GE/oUbF4q49W/c+TdO2iy09cDA4Zx7mXNPkAiujet+O6WNza3tnfJuZW//4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0j+WDmSXoR3QsecgZNVZ6HETUTIIwS+fDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzF+RpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslB/b2Q00noWBXYyT6hXvVz8z+unJrzxMy6T1KBky4/CVBATk/x8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZL+oowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucU/sD5/AHw2JD/</latexit><latexit sha1_base64="Wl/NKcf+4FQq41kPZqpr8GSpKP8=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GE/oUbF4q49W/c+TdO2iy09cDA4Zx7mXNPkAiujet+O6WNza3tnfJuZW//4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0j+WDmSXoR3QsecgZNVZ6HETUTIIwS+fDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzF+RpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslB/b2Q00noWBXYyT6hXvVz8z+unJrzxMy6T1KBky4/CVBATk/x8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZL+oowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucU/sD5/AHw2JD/</latexit><latexit sha1_base64="Wl/NKcf+4FQq41kPZqpr8GSpKP8=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GE/oUbF4q49W/c+TdO2iy09cDA4Zx7mXNPkAiujet+O6WNza3tnfJuZW//4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0j+WDmSXoR3QsecgZNVZ6HETUTIIwS+fDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzF+RpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslB/b2Q00noWBXYyT6hXvVz8z+unJrzxMy6T1KBky4/CVBATk/x8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZL+oowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucU/sD5/AHw2JD/</latexit><latexit sha1_base64="Wl/NKcf+4FQq41kPZqpr8GSpKP8=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GE/oUbF4q49W/c+TdO2iy09cDA4Zx7mXNPkAiujet+O6WNza3tnfJuZW//4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0j+WDmSXoR3QsecgZNVZ6HETUTIIwS+fDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzF+RpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslB/b2Q00noWBXYyT6hXvVz8z+unJrzxMy6T1KBky4/CVBATk/x8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZL+oowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucU/sD5/AHw2JD/</latexit>e0k
<latexit sha1_base64="a1hco1MShws4KpmpFnenOcfEqyc=">AAAB9HicdVDLSgMxFM34rPVVdekmWMSuhkxtsd0V3LisYB/QDiWT3mlDMw+TTKEM/Q43LhRx68e482/MtBVU9EDgcM693JPjxYIrTciHtba+sbm1ndvJ7+7tHxwWjo7bKkokgxaLRCS7HlUgeAgtzbWAbiyBBp6Ajje5zvzOFKTiUXinZzG4AR2F3OeMaiO5/YDqseenML8YTAaFIrGdCilXy5jY1XqdVOqG1KqXpEywY5MFimiF5qDw3h9GLAkg1ExQpXoOibWbUqk5EzDP9xMFMWUTOoKeoSENQLnpIvQcnxtliP1ImhdqvFC/b6Q0UGoWeGYyC6l+e5n4l9dLtF9zUx7GiYaQLQ/5icA6wlkDeMglMC1mhlAmucmK2ZhKyrTpKW9K+Pop/p+0y7ZjurqtFBulVR05dIrOUAk56Ao10A1qohZi6B49oCf0bE2tR+vFel2OrlmrnRP0A9bbJyRskkI=</latexit><latexit sha1_base64="a1hco1MShws4KpmpFnenOcfEqyc=">AAAB9HicdVDLSgMxFM34rPVVdekmWMSuhkxtsd0V3LisYB/QDiWT3mlDMw+TTKEM/Q43LhRx68e482/MtBVU9EDgcM693JPjxYIrTciHtba+sbm1ndvJ7+7tHxwWjo7bKkokgxaLRCS7HlUgeAgtzbWAbiyBBp6Ajje5zvzOFKTiUXinZzG4AR2F3OeMaiO5/YDqseenML8YTAaFIrGdCilXy5jY1XqdVOqG1KqXpEywY5MFimiF5qDw3h9GLAkg1ExQpXoOibWbUqk5EzDP9xMFMWUTOoKeoSENQLnpIvQcnxtliP1ImhdqvFC/b6Q0UGoWeGYyC6l+e5n4l9dLtF9zUx7GiYaQLQ/5icA6wlkDeMglMC1mhlAmucmK2ZhKyrTpKW9K+Pop/p+0y7ZjurqtFBulVR05dIrOUAk56Ao10A1qohZi6B49oCf0bE2tR+vFel2OrlmrnRP0A9bbJyRskkI=</latexit><latexit sha1_base64="a1hco1MShws4KpmpFnenOcfEqyc=">AAAB9HicdVDLSgMxFM34rPVVdekmWMSuhkxtsd0V3LisYB/QDiWT3mlDMw+TTKEM/Q43LhRx68e482/MtBVU9EDgcM693JPjxYIrTciHtba+sbm1ndvJ7+7tHxwWjo7bKkokgxaLRCS7HlUgeAgtzbWAbiyBBp6Ajje5zvzOFKTiUXinZzG4AR2F3OeMaiO5/YDqseenML8YTAaFIrGdCilXy5jY1XqdVOqG1KqXpEywY5MFimiF5qDw3h9GLAkg1ExQpXoOibWbUqk5EzDP9xMFMWUTOoKeoSENQLnpIvQcnxtliP1ImhdqvFC/b6Q0UGoWeGYyC6l+e5n4l9dLtF9zUx7GiYaQLQ/5icA6wlkDeMglMC1mhlAmucmK2ZhKyrTpKW9K+Pop/p+0y7ZjurqtFBulVR05dIrOUAk56Ao10A1qohZi6B49oCf0bE2tR+vFel2OrlmrnRP0A9bbJyRskkI=</latexit><latexit sha1_base64="a1hco1MShws4KpmpFnenOcfEqyc=">AAAB9HicdVDLSgMxFM34rPVVdekmWMSuhkxtsd0V3LisYB/QDiWT3mlDMw+TTKEM/Q43LhRx68e482/MtBVU9EDgcM693JPjxYIrTciHtba+sbm1ndvJ7+7tHxwWjo7bKkokgxaLRCS7HlUgeAgtzbWAbiyBBp6Ajje5zvzOFKTiUXinZzG4AR2F3OeMaiO5/YDqseenML8YTAaFIrGdCilXy5jY1XqdVOqG1KqXpEywY5MFimiF5qDw3h9GLAkg1ExQpXoOibWbUqk5EzDP9xMFMWUTOoKeoSENQLnpIvQcnxtliP1ImhdqvFC/b6Q0UGoWeGYyC6l+e5n4l9dLtF9zUx7GiYaQLQ/5icA6wlkDeMglMC1mhlAmucmK2ZhKyrTpKW9K+Pop/p+0y7ZjurqtFBulVR05dIrOUAk56Ao10A1qohZi6B49oCf0bE2tR+vFel2OrlmrnRP0A9bbJyRskkI=</latexit>(a) Edge update
u
<latexit sha1_base64="Wl/NKcf+4FQq41kPZqpr8GSpKP8=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GE/oUbF4q49W/c+TdO2iy09cDA4Zx7mXNPkAiujet+O6WNza3tnfJuZW//4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0j+WDmSXoR3QsecgZNVZ6HETUTIIwS+fDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzF+RpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslB/b2Q00noWBXYyT6hXvVz8z+unJrzxMy6T1KBky4/CVBATk/x8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZL+oowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucU/sD5/AHw2JD/</latexit><latexit sha1_base64="Wl/NKcf+4FQq41kPZqpr8GSpKP8=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GE/oUbF4q49W/c+TdO2iy09cDA4Zx7mXNPkAiujet+O6WNza3tnfJuZW//4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0j+WDmSXoR3QsecgZNVZ6HETUTIIwS+fDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzF+RpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslB/b2Q00noWBXYyT6hXvVz8z+unJrzxMy6T1KBky4/CVBATk/x8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZL+oowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucU/sD5/AHw2JD/</latexit><latexit sha1_base64="Wl/NKcf+4FQq41kPZqpr8GSpKP8=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GE/oUbF4q49W/c+TdO2iy09cDA4Zx7mXNPkAiujet+O6WNza3tnfJuZW//4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0j+WDmSXoR3QsecgZNVZ6HETUTIIwS+fDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzF+RpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslB/b2Q00noWBXYyT6hXvVz8z+unJrzxMy6T1KBky4/CVBATk/x8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZL+oowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucU/sD5/AHw2JD/</latexit><latexit sha1_base64="Wl/NKcf+4FQq41kPZqpr8GSpKP8=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GE/oUbF4q49W/c+TdO2iy09cDA4Zx7mXNPkAiujet+O6WNza3tnfJuZW//4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0j+WDmSXoR3QsecgZNVZ6HETUTIIwS+fDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzF+RpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslB/b2Q00noWBXYyT6hXvVz8z+unJrzxMy6T1KBky4/CVBATk/x8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZL+oowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucU/sD5/AHw2JD/</latexit>e0k
<latexit sha1_base64="TmBm7ikN3ChoJpDcsfwhm1T5rLk=">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvYVZkRQZcFNy4r2Ae0Q8mkd9rQTGZMMoUy9DvcuFDErR/jzr8x03ahrQcCh3Pu5Z6cIBFcG9f9dgobm1vbO8Xd0t7+weFR+fikpeNUMWyyWMSqE1CNgktsGm4EdhKFNAoEtoPxXe63J6g0j+WjmSboR3QoecgZNVbyexE1oyDMcHbZH/fLFbfmzkHWibckFVii0S9/9QYxSyOUhgmqdddzE+NnVBnOBM5KvVRjQtmYDrFrqaQRaj+bh56RC6sMSBgr+6Qhc/X3RkYjradRYCfzkHrVy8X/vG5qwls/4zJJDUq2OBSmgpiY5A2QAVfIjJhaQpniNithI6ooM7anki3BW/3yOmld1Ty35j1cV+rVZR1FOINzqIIHN1CHe2hAExg8wTO8wpszcV6cd+djMVpwljun8AfO5w/CAJH+</latexit><latexit sha1_base64="TmBm7ikN3ChoJpDcsfwhm1T5rLk=">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvYVZkRQZcFNy4r2Ae0Q8mkd9rQTGZMMoUy9DvcuFDErR/jzr8x03ahrQcCh3Pu5Z6cIBFcG9f9dgobm1vbO8Xd0t7+weFR+fikpeNUMWyyWMSqE1CNgktsGm4EdhKFNAoEtoPxXe63J6g0j+WjmSboR3QoecgZNVbyexE1oyDMcHbZH/fLFbfmzkHWibckFVii0S9/9QYxSyOUhgmqdddzE+NnVBnOBM5KvVRjQtmYDrFrqaQRaj+bh56RC6sMSBgr+6Qhc/X3RkYjradRYCfzkHrVy8X/vG5qwls/4zJJDUq2OBSmgpiY5A2QAVfIjJhaQpniNithI6ooM7anki3BW/3yOmld1Ty35j1cV+rVZR1FOINzqIIHN1CHe2hAExg8wTO8wpszcV6cd+djMVpwljun8AfO5w/CAJH+</latexit><latexit sha1_base64="TmBm7ikN3ChoJpDcsfwhm1T5rLk=">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvYVZkRQZcFNy4r2Ae0Q8mkd9rQTGZMMoUy9DvcuFDErR/jzr8x03ahrQcCh3Pu5Z6cIBFcG9f9dgobm1vbO8Xd0t7+weFR+fikpeNUMWyyWMSqE1CNgktsGm4EdhKFNAoEtoPxXe63J6g0j+WjmSboR3QoecgZNVbyexE1oyDMcHbZH/fLFbfmzkHWibckFVii0S9/9QYxSyOUhgmqdddzE+NnVBnOBM5KvVRjQtmYDrFrqaQRaj+bh56RC6sMSBgr+6Qhc/X3RkYjradRYCfzkHrVy8X/vG5qwls/4zJJDUq2OBSmgpiY5A2QAVfIjJhaQpniNithI6ooM7anki3BW/3yOmld1Ty35j1cV+rVZR1FOINzqIIHN1CHe2hAExg8wTO8wpszcV6cd+djMVpwljun8AfO5w/CAJH+</latexit><latexit sha1_base64="TmBm7ikN3ChoJpDcsfwhm1T5rLk=">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvYVZkRQZcFNy4r2Ae0Q8mkd9rQTGZMMoUy9DvcuFDErR/jzr8x03ahrQcCh3Pu5Z6cIBFcG9f9dgobm1vbO8Xd0t7+weFR+fikpeNUMWyyWMSqE1CNgktsGm4EdhKFNAoEtoPxXe63J6g0j+WjmSboR3QoecgZNVbyexE1oyDMcHbZH/fLFbfmzkHWibckFVii0S9/9QYxSyOUhgmqdddzE+NnVBnOBM5KvVRjQtmYDrFrqaQRaj+bh56RC6sMSBgr+6Qhc/X3RkYjradRYCfzkHrVy8X/vG5qwls/4zJJDUq2OBSmgpiY5A2QAVfIjJhaQpniNithI6ooM7anki3BW/3yOmld1Ty35j1cV+rVZR1FOINzqIIHN1CHe2hAExg8wTO8wpszcV6cd+djMVpwljun8AfO5w/CAJH+</latexit>v0i
<latexit sha1_base64="eeLXdOBZMDToGpT2JKCAlGanLL8=">AAAB9HicdVDLSgMxFL3js9ZX1aWbYBG7GjK1xXZXcOOygn1AO5RMmmlDMw+TTKEM/Q43LhRx68e482/MtBVU9EDgcM693JPjxYIrjfGHtba+sbm1ndvJ7+7tHxwWjo7bKkokZS0aiUh2PaKY4CFraa4F68aSkcATrONNrjO/M2VS8Si807OYuQEZhdznlGgjuf2A6LHnp9P5xYAPCkVsOxVcrpYRtqv1Oq7UDalVL3EZI8fGCxRhheag8N4fRjQJWKipIEr1HBxrNyVScyrYPN9PFIsJnZAR6xkakoApN12EnqNzowyRH0nzQo0W6veNlARKzQLPTGYh1W8vE//yeon2a27KwzjRLKTLQ34ikI5Q1gAacsmoFjNDCJXcZEV0TCSh2vSUNyV8/RT9T9pl2zFd3VaKjdKqjhycwhmUwIEraMANNKEFFO7hAZ7g2Zpaj9aL9bocXbNWOyfwA9bbJztsklE=</latexit><latexit sha1_base64="eeLXdOBZMDToGpT2JKCAlGanLL8=">AAAB9HicdVDLSgMxFL3js9ZX1aWbYBG7GjK1xXZXcOOygn1AO5RMmmlDMw+TTKEM/Q43LhRx68e482/MtBVU9EDgcM693JPjxYIrjfGHtba+sbm1ndvJ7+7tHxwWjo7bKkokZS0aiUh2PaKY4CFraa4F68aSkcATrONNrjO/M2VS8Si807OYuQEZhdznlGgjuf2A6LHnp9P5xYAPCkVsOxVcrpYRtqv1Oq7UDalVL3EZI8fGCxRhheag8N4fRjQJWKipIEr1HBxrNyVScyrYPN9PFIsJnZAR6xkakoApN12EnqNzowyRH0nzQo0W6veNlARKzQLPTGYh1W8vE//yeon2a27KwzjRLKTLQ34ikI5Q1gAacsmoFjNDCJXcZEV0TCSh2vSUNyV8/RT9T9pl2zFd3VaKjdKqjhycwhmUwIEraMANNKEFFO7hAZ7g2Zpaj9aL9bocXbNWOyfwA9bbJztsklE=</latexit><latexit sha1_base64="eeLXdOBZMDToGpT2JKCAlGanLL8=">AAAB9HicdVDLSgMxFL3js9ZX1aWbYBG7GjK1xXZXcOOygn1AO5RMmmlDMw+TTKEM/Q43LhRx68e482/MtBVU9EDgcM693JPjxYIrjfGHtba+sbm1ndvJ7+7tHxwWjo7bKkokZS0aiUh2PaKY4CFraa4F68aSkcATrONNrjO/M2VS8Si807OYuQEZhdznlGgjuf2A6LHnp9P5xYAPCkVsOxVcrpYRtqv1Oq7UDalVL3EZI8fGCxRhheag8N4fRjQJWKipIEr1HBxrNyVScyrYPN9PFIsJnZAR6xkakoApN12EnqNzowyRH0nzQo0W6veNlARKzQLPTGYh1W8vE//yeon2a27KwzjRLKTLQ34ikI5Q1gAacsmoFjNDCJXcZEV0TCSh2vSUNyV8/RT9T9pl2zFd3VaKjdKqjhycwhmUwIEraMANNKEFFO7hAZ7g2Zpaj9aL9bocXbNWOyfwA9bbJztsklE=</latexit><latexit sha1_base64="eeLXdOBZMDToGpT2JKCAlGanLL8=">AAAB9HicdVDLSgMxFL3js9ZX1aWbYBG7GjK1xXZXcOOygn1AO5RMmmlDMw+TTKEM/Q43LhRx68e482/MtBVU9EDgcM693JPjxYIrjfGHtba+sbm1ndvJ7+7tHxwWjo7bKkokZS0aiUh2PaKY4CFraa4F68aSkcATrONNrjO/M2VS8Si807OYuQEZhdznlGgjuf2A6LHnp9P5xYAPCkVsOxVcrpYRtqv1Oq7UDalVL3EZI8fGCxRhheag8N4fRjQJWKipIEr1HBxrNyVScyrYPN9PFIsJnZAR6xkakoApN12EnqNzowyRH0nzQo0W6veNlARKzQLPTGYh1W8vE//yeon2a27KwzjRLKTLQ34ikI5Q1gAacsmoFjNDCJXcZEV0TCSh2vSUNyV8/RT9T9pl2zFd3VaKjdKqjhycwhmUwIEraMANNKEFFO7hAZ7g2Zpaj9aL9bocXbNWOyfwA9bbJztsklE=</latexit> (b) Node update
u0
<latexit sha1_base64="RuJ/WOWmv0qWsx0aAsZGj4qvbr4=">AAAB8nicdVDJSgNBEO2JW4xb1KOXxiDmNMxk0cwt4MVjBLPAZAg9nZ6kSc9Cd40YhnyGFw+KePVrvPk3dhZBRR8UPN6roqqenwiuwLI+jNza+sbmVn67sLO7t39QPDzqqDiVlLVpLGLZ84ligkesDRwE6yWSkdAXrOtPruZ+945JxePoFqYJ80IyinjAKQEtuX1g9+AHWTo7HxRLllm5aFTqFrbMulOrNmqaNCpO1XGwbVoLlNAKrUHxvT+MaRqyCKggSrm2lYCXEQmcCjYr9FPFEkInZMRcTSMSMuVli5Nn+EwrQxzEUlcEeKF+n8hIqNQ09HVnSGCsfntz8S/PTSFoeBmPkhRYRJeLglRgiPH8fzzkklEQU00IlVzfiumYSEJBp1TQIXx9iv8nnYppW6Z9Uys1y6s48ugEnaIystElaqJr1EJtRFGMHtATejbAeDRejNdla85YzRyjHzDePgEJTZGr</latexit><latexit sha1_base64="RuJ/WOWmv0qWsx0aAsZGj4qvbr4=">AAAB8nicdVDJSgNBEO2JW4xb1KOXxiDmNMxk0cwt4MVjBLPAZAg9nZ6kSc9Cd40YhnyGFw+KePVrvPk3dhZBRR8UPN6roqqenwiuwLI+jNza+sbmVn67sLO7t39QPDzqqDiVlLVpLGLZ84ligkesDRwE6yWSkdAXrOtPruZ+945JxePoFqYJ80IyinjAKQEtuX1g9+AHWTo7HxRLllm5aFTqFrbMulOrNmqaNCpO1XGwbVoLlNAKrUHxvT+MaRqyCKggSrm2lYCXEQmcCjYr9FPFEkInZMRcTSMSMuVli5Nn+EwrQxzEUlcEeKF+n8hIqNQ09HVnSGCsfntz8S/PTSFoeBmPkhRYRJeLglRgiPH8fzzkklEQU00IlVzfiumYSEJBp1TQIXx9iv8nnYppW6Z9Uys1y6s48ugEnaIystElaqJr1EJtRFGMHtATejbAeDRejNdla85YzRyjHzDePgEJTZGr</latexit><latexit sha1_base64="RuJ/WOWmv0qWsx0aAsZGj4qvbr4=">AAAB8nicdVDJSgNBEO2JW4xb1KOXxiDmNMxk0cwt4MVjBLPAZAg9nZ6kSc9Cd40YhnyGFw+KePVrvPk3dhZBRR8UPN6roqqenwiuwLI+jNza+sbmVn67sLO7t39QPDzqqDiVlLVpLGLZ84ligkesDRwE6yWSkdAXrOtPruZ+945JxePoFqYJ80IyinjAKQEtuX1g9+AHWTo7HxRLllm5aFTqFrbMulOrNmqaNCpO1XGwbVoLlNAKrUHxvT+MaRqyCKggSrm2lYCXEQmcCjYr9FPFEkInZMRcTSMSMuVli5Nn+EwrQxzEUlcEeKF+n8hIqNQ09HVnSGCsfntz8S/PTSFoeBmPkhRYRJeLglRgiPH8fzzkklEQU00IlVzfiumYSEJBp1TQIXx9iv8nnYppW6Z9Uys1y6s48ugEnaIystElaqJr1EJtRFGMHtATejbAeDRejNdla85YzRyjHzDePgEJTZGr</latexit><latexit sha1_base64="RuJ/WOWmv0qWsx0aAsZGj4qvbr4=">AAAB8nicdVDJSgNBEO2JW4xb1KOXxiDmNMxk0cwt4MVjBLPAZAg9nZ6kSc9Cd40YhnyGFw+KePVrvPk3dhZBRR8UPN6roqqenwiuwLI+jNza+sbmVn67sLO7t39QPDzqqDiVlLVpLGLZ84ligkesDRwE6yWSkdAXrOtPruZ+945JxePoFqYJ80IyinjAKQEtuX1g9+AHWTo7HxRLllm5aFTqFrbMulOrNmqaNCpO1XGwbVoLlNAKrUHxvT+MaRqyCKggSrm2lYCXEQmcCjYr9FPFEkInZMRcTSMSMuVli5Nn+EwrQxzEUlcEeKF+n8hIqNQ09HVnSGCsfntz8S/PTSFoeBmPkhRYRJeLglRgiPH8fzzkklEQU00IlVzfiumYSEJBp1TQIXx9iv8nnYppW6Z9Uys1y6s48ugEnaIystElaqJr1EJtRFGMHtATejbAeDRejNdla85YzRyjHzDePgEJTZGr</latexit>e0k
<latexit sha1_base64="Iztn6Umi7rLG5lNF0JpW0x6J+s0=">AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120i7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAE/YgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3Sl/dfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23Sp+GsbalkM7V3xMZi4yZRIHtjBgOzbI3E//zOimGN34mVJIiKL5YFKaSYkxnCdC+0MBRTixhXAt7K+VDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IX/gfP4A6+WSGQ==</latexit><latexit sha1_base64="Iztn6Umi7rLG5lNF0JpW0x6J+s0=">AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120i7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAE/YgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3Sl/dfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23Sp+GsbalkM7V3xMZi4yZRIHtjBgOzbI3E//zOimGN34mVJIiKL5YFKaSYkxnCdC+0MBRTixhXAt7K+VDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IX/gfP4A6+WSGQ==</latexit><latexit sha1_base64="Iztn6Umi7rLG5lNF0JpW0x6J+s0=">AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120i7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAE/YgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3Sl/dfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23Sp+GsbalkM7V3xMZi4yZRIHtjBgOzbI3E//zOimGN34mVJIiKL5YFKaSYkxnCdC+0MBRTixhXAt7K+VDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IX/gfP4A6+WSGQ==</latexit><latexit sha1_base64="hP+6LrUf2d3tZaldqaQQvEKMXyw=">AAAB2XicbZDNSgMxFIXv1L86Vq1rN8EiuCozbnQpuHFZwbZCO5RM5k4bmskMyR2hDH0BF25EfC93vo3pz0JbDwQ+zknIvSculLQUBN9ebWd3b/+gfugfNfzjk9Nmo2fz0gjsilzl5jnmFpXU2CVJCp8LgzyLFfbj6f0i77+gsTLXTzQrMMr4WMtUCk7O6oyaraAdLMW2IVxDC9YaNb+GSS7KDDUJxa0dhEFBUcUNSaFw7g9LiwUXUz7GgUPNM7RRtRxzzi6dk7A0N+5oYkv394uKZ9bOstjdzDhN7Ga2MP/LBiWlt1EldVESarH6KC0Vo5wtdmaJNChIzRxwYaSblYkJN1yQa8Z3HYSbG29D77odBu3wMYA6nMMFXEEIN3AHD9CBLghI4BXevYn35n2suqp569LO4I+8zx84xIo4</latexit><latexit sha1_base64="ywz7v1q7Yrl4nBX/+QcnkaM0kGo=">AAAB6XicbZBLSwMxFIXv+Kz1Vd26CRbRVZlxo0vBjcsK9gHtUDLpnRqayYzJnWIZ+jvcuFDEP+TOf2P6WGjrgcDHOQn35kSZkpZ8/9tbW9/Y3Nou7ZR39/YPDitHe02b5kZgQ6QqNe2IW1RSY4MkKWxnBnkSKWxFw9tp3hqhsTLVDzTOMEz4QMtYCk7OCruEzxTFBU7Oe8NeperX/JnYKgQLqMJC9V7lq9tPRZ6gJqG4tZ3AzygsuCEpFE7K3dxixsWQD7DjUPMEbVjMlp6wM+f0WZwadzSxmfv7RcETa8dJ5G4mnB7tcjY1/8s6OcXXYSF1lhNqMR8U54pRyqYNsL40KEiNHXBhpNuViUduuCDXU9mVECx/eRWal7XArwX3PpTgBE7hAgK4ghu4gzo0QMATvMAbvHsj79X7mNe15i16O4Y/8j5/AKfckNQ=</latexit><latexit sha1_base64="ywz7v1q7Yrl4nBX/+QcnkaM0kGo=">AAAB6XicbZBLSwMxFIXv+Kz1Vd26CRbRVZlxo0vBjcsK9gHtUDLpnRqayYzJnWIZ+jvcuFDEP+TOf2P6WGjrgcDHOQn35kSZkpZ8/9tbW9/Y3Nou7ZR39/YPDitHe02b5kZgQ6QqNe2IW1RSY4MkKWxnBnkSKWxFw9tp3hqhsTLVDzTOMEz4QMtYCk7OCruEzxTFBU7Oe8NeperX/JnYKgQLqMJC9V7lq9tPRZ6gJqG4tZ3AzygsuCEpFE7K3dxixsWQD7DjUPMEbVjMlp6wM+f0WZwadzSxmfv7RcETa8dJ5G4mnB7tcjY1/8s6OcXXYSF1lhNqMR8U54pRyqYNsL40KEiNHXBhpNuViUduuCDXU9mVECx/eRWal7XArwX3PpTgBE7hAgK4ghu4gzo0QMATvMAbvHsj79X7mNe15i16O4Y/8j5/AKfckNQ=</latexit><latexit sha1_base64="Wxt2EGaSkqmyg6rX9KQvpR9rldE=">AAAB9HicbVA9TwJBEN3DL8Qv1NJmIzFSkTsbLUlsLDGRjwQuZG+Zgw17e+fuHJFc+B02Fhpj64+x89+4wBUKvmSSl/dmMjMvSKQw6LrfTmFjc2t7p7hb2ts/ODwqH5+0TJxqDk0ey1h3AmZACgVNFCihk2hgUSChHYxv5357AtqIWD3gNAE/YkMlQsEZWsnvITxhEGYwu+yP++WKW3MXoOvEy0mF5Gj0y1+9QczTCBRyyYzpem6CfsY0Ci5hVuqlBhLGx2wIXUsVi8D42eLoGb2wyoCGsbalkC7U3xMZi4yZRoHtjBiOzKo3F//zuimGN34mVJIiKL5cFKaSYkznCdCB0MBRTi1hXAt7K+UjphlHm1PJhuCtvrxOWlc1z615926lXs3jKJIzck6qxCPXpE7uSIM0CSeP5Jm8kjdn4rw4787HsrXg5DOn5A+czx/qpZIV</latexit><latexit sha1_base64="Iztn6Umi7rLG5lNF0JpW0x6J+s0=">AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120i7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAE/YgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3Sl/dfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23Sp+GsbalkM7V3xMZi4yZRIHtjBgOzbI3E//zOimGN34mVJIiKL5YFKaSYkxnCdC+0MBRTixhXAt7K+VDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IX/gfP4A6+WSGQ==</latexit><latexit sha1_base64="Iztn6Umi7rLG5lNF0JpW0x6J+s0=">AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120i7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAE/YgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3Sl/dfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23Sp+GsbalkM7V3xMZi4yZRIHtjBgOzbI3E//zOimGN34mVJIiKL5YFKaSYkxnCdC+0MBRTixhXAt7K+VDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IX/gfP4A6+WSGQ==</latexit><latexit sha1_base64="Iztn6Umi7rLG5lNF0JpW0x6J+s0=">AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120i7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAE/YgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3Sl/dfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23Sp+GsbalkM7V3xMZi4yZRIHtjBgOzbI3E//zOimGN34mVJIiKL5YFKaSYkxnCdC+0MBRTixhXAt7K+VDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IX/gfP4A6+WSGQ==</latexit><latexit sha1_base64="Iztn6Umi7rLG5lNF0JpW0x6J+s0=">AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120i7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAE/YgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3Sl/dfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23Sp+GsbalkM7V3xMZi4yZRIHtjBgOzbI3E//zOimGN34mVJIiKL5YFKaSYkxnCdC+0MBRTixhXAt7K+VDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IX/gfP4A6+WSGQ==</latexit><latexit sha1_base64="Iztn6Umi7rLG5lNF0JpW0x6J+s0=">AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120i7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAE/YgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3Sl/dfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23Sp+GsbalkM7V3xMZi4yZRIHtjBgOzbI3E//zOimGN34mVJIiKL5YFKaSYkxnCdC+0MBRTixhXAt7K+VDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IX/gfP4A6+WSGQ==</latexit><latexit sha1_base64="Iztn6Umi7rLG5lNF0JpW0x6J+s0=">AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120i7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAE/YgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3Sl/dfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23Sp+GsbalkM7V3xMZi4yZRIHtjBgOzbI3E//zOimGN34mVJIiKL5YFKaSYkxnCdC+0MBRTixhXAt7K+VDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IX/gfP4A6+WSGQ==</latexit>v0i
<latexit sha1_base64="PT7VlVtIO1b4RdkSG9z8jpkhSqk=">AAAB9HicbVBNS8NAEJ34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120y7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqxhsslrFuB9RwKRRvoEDJ24nmNAokbwWj25nfGnNtRKwecJJwP6IDJULBKFrJ7yJ/wiDMxtOLnuiVym7VnYOsEi8nZchR75W+uv2YpRFXyCQ1puO5CfoZ1SiY5NNiNzU8oWxEB7xjqaIRN342P3pKzq3SJ2GsbSkkc/X3REYjYyZRYDsjikOz7M3E/7xOiuGNnwmVpMgVWywKU0kwJrMESF9ozlBOLKFMC3srYUOqKUObU9GG4C2/vEqal1XPrXr3V+VaJY+jAKdwBhXw4BpqcAd1aACDR3iGV3hzxs6L8+58LFrXnHzmBP7A+fwBAvSSKA==</latexit><latexit sha1_base64="PT7VlVtIO1b4RdkSG9z8jpkhSqk=">AAAB9HicbVBNS8NAEJ34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120y7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqxhsslrFuB9RwKRRvoEDJ24nmNAokbwWj25nfGnNtRKwecJJwP6IDJULBKFrJ7yJ/wiDMxtOLnuiVym7VnYOsEi8nZchR75W+uv2YpRFXyCQ1puO5CfoZ1SiY5NNiNzU8oWxEB7xjqaIRN342P3pKzq3SJ2GsbSkkc/X3REYjYyZRYDsjikOz7M3E/7xOiuGNnwmVpMgVWywKU0kwJrMESF9ozlBOLKFMC3srYUOqKUObU9GG4C2/vEqal1XPrXr3V+VaJY+jAKdwBhXw4BpqcAd1aACDR3iGV3hzxs6L8+58LFrXnHzmBP7A+fwBAvSSKA==</latexit><latexit sha1_base64="6WjtAQy1eEki3DeLmUkkI9Sk/Os=">AAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0Xc+kvu/BunaRbaemDgcM65zL0nSAXXxnW/ncrG5tb2TnW3trd/cHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOB/WB6u/D7T6g0T+SDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzV+TpXhTOC8Nso0ppRN6RiHlkoao/bzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQ//OGmYlu/JzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3v/qrRbpZ1VOEMzqEJHlxDG+6gA11gEMIzvMKbw50X5935WEYrTjlzCn/gfP4AA6WMYA==</latexit><latexit sha1_base64="6WjtAQy1eEki3DeLmUkkI9Sk/Os=">AAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0Xc+kvu/BunaRbaemDgcM65zL0nSAXXxnW/ncrG5tb2TnW3trd/cHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOB/WB6u/D7T6g0T+SDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzV+TpXhTOC8Nso0ppRN6RiHlkoao/bzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQ//OGmYlu/JzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3v/qrRbpZ1VOEMzqEJHlxDG+6gA11gEMIzvMKbw50X5935WEYrTjlzCn/gfP4AA6WMYA==</latexit><latexit sha1_base64="hP+6LrUf2d3tZaldqaQQvEKMXyw=">AAAB2XicbZDNSgMxFIXv1L86Vq1rN8EiuCozbnQpuHFZwbZCO5RM5k4bmskMyR2hDH0BF25EfC93vo3pz0JbDwQ+zknIvSculLQUBN9ebWd3b/+gfugfNfzjk9Nmo2fz0gjsilzl5jnmFpXU2CVJCp8LgzyLFfbj6f0i77+gsTLXTzQrMMr4WMtUCk7O6oyaraAdLMW2IVxDC9YaNb+GSS7KDDUJxa0dhEFBUcUNSaFw7g9LiwUXUz7GgUPNM7RRtRxzzi6dk7A0N+5oYkv394uKZ9bOstjdzDhN7Ga2MP/LBiWlt1EldVESarH6KC0Vo5wtdmaJNChIzRxwYaSblYkJN1yQa8Z3HYSbG29D77odBu3wMYA6nMMFXEEIN3AHD9CBLghI4BXevYn35n2suqp569LO4I+8zx84xIo4</latexit><latexit sha1_base64="OfCCjkcIiyDbxmNKxve032U7QH4=">AAAB3HicbVBNS8NAEJ3Ur1qrVq9eFovgqSRe9Ch48VjBfkAbZLOZtEs3m7A7EUroH/DiQRF/lzf/jduPg7Y+GHi8N8PMvChX0pLvf3uVre2d3b3qfu2gfnh03Dipd21WGIEdkanM9CNuUUmNHZKksJ8b5GmksBdN7uZ+7xmNlZl+pGmOYcpHWiZScHJS+6nR9Fv+AmyTBCvShBVc/9cwzkSRoiahuLWDwM8pLLkhKRTOasPCYs7FhI9w4KjmKdqwXNw5YxdOiVmSGVea2EL9PVHy1NppGrnOlNPYrntz8T9vUFByE5ZS5wWhFstFSaEYZWz+NIulQUFq6ggXRrpbmRhzwwW5aGouhGD95U3SvWoFfit48KEKZ3AOlxDANdzCPbShAwJieIE3ePek9+p9LOOqeKvcTuEPvM8f/CqLKA==</latexit><latexit sha1_base64="OfCCjkcIiyDbxmNKxve032U7QH4=">AAAB3HicbVBNS8NAEJ3Ur1qrVq9eFovgqSRe9Ch48VjBfkAbZLOZtEs3m7A7EUroH/DiQRF/lzf/jduPg7Y+GHi8N8PMvChX0pLvf3uVre2d3b3qfu2gfnh03Dipd21WGIEdkanM9CNuUUmNHZKksJ8b5GmksBdN7uZ+7xmNlZl+pGmOYcpHWiZScHJS+6nR9Fv+AmyTBCvShBVc/9cwzkSRoiahuLWDwM8pLLkhKRTOasPCYs7FhI9w4KjmKdqwXNw5YxdOiVmSGVea2EL9PVHy1NppGrnOlNPYrntz8T9vUFByE5ZS5wWhFstFSaEYZWz+NIulQUFq6ggXRrpbmRhzwwW5aGouhGD95U3SvWoFfit48KEKZ3AOlxDANdzCPbShAwJieIE3ePek9+p9LOOqeKvcTuEPvM8f/CqLKA==</latexit><latexit sha1_base64="PwZ8GjhNs4EFPNOrNCQnWexiUCQ=">AAAB53icbVDLSgMxFL1TX7W+qi7dBIvQVZlxo8uCG5cV7APaQTKZTBuaZIbkjlCG/oAbF4q49Zfc+Tem7Sy0eiBwOOdccu+JMiks+v6XV9nY3Nreqe7W9vYPDo/qxyc9m+aG8S5LZWoGEbVcCs27KFDyQWY4VZHk/Wh6s/D7j9xYkep7nGU8VHSsRSIYRSd1HuoNv+UvQf6SoCQNKOHyn6M4ZbniGpmk1g4DP8OwoAYFk3xeG+WWZ5RN6ZgPHdVUcRsWyz3n5MIpMUlS455GslR/ThRUWTtTkUsqihO77i3E/7xhjsl1WAid5cg1W32U5JJgShZHk1gYzlDOHKHMCLcrYRNqKENXTc2VEKyf/Jf0LluB3wru/Ea7WdZRhTM4hyYEcAVtuIUOdIFBDE/wAq+e8J69N+99Fa145cwp/IL38Q0CZYxc</latexit><latexit sha1_base64="6WjtAQy1eEki3DeLmUkkI9Sk/Os=">AAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0Xc+kvu/BunaRbaemDgcM65zL0nSAXXxnW/ncrG5tb2TnW3trd/cHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOB/WB6u/D7T6g0T+SDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzV+TpXhTOC8Nso0ppRN6RiHlkoao/bzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQ//OGmYlu/JzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3v/qrRbpZ1VOEMzqEJHlxDG+6gA11gEMIzvMKbw50X5935WEYrTjlzCn/gfP4AA6WMYA==</latexit><latexit sha1_base64="6WjtAQy1eEki3DeLmUkkI9Sk/Os=">AAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0Xc+kvu/BunaRbaemDgcM65zL0nSAXXxnW/ncrG5tb2TnW3trd/cHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOB/WB6u/D7T6g0T+SDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzV+TpXhTOC8Nso0ppRN6RiHlkoao/bzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQ//OGmYlu/JzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3v/qrRbpZ1VOEMzqEJHlxDG+6gA11gEMIzvMKbw50X5935WEYrTjlzCn/gfP4AA6WMYA==</latexit><latexit sha1_base64="6WjtAQy1eEki3DeLmUkkI9Sk/Os=">AAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0Xc+kvu/BunaRbaemDgcM65zL0nSAXXxnW/ncrG5tb2TnW3trd/cHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOB/WB6u/D7T6g0T+SDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzV+TpXhTOC8Nso0ppRN6RiHlkoao/bzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQ//OGmYlu/JzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3v/qrRbpZ1VOEMzqEJHlxDG+6gA11gEMIzvMKbw50X5935WEYrTjlzCn/gfP4AA6WMYA==</latexit><latexit sha1_base64="6WjtAQy1eEki3DeLmUkkI9Sk/Os=">AAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0Xc+kvu/BunaRbaemDgcM65zL0nSAXXxnW/ncrG5tb2TnW3trd/cHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOB/WB6u/D7T6g0T+SDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzV+TpXhTOC8Nso0ppRN6RiHlkoao/bzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQ//OGmYlu/JzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3v/qrRbpZ1VOEMzqEJHlxDG+6gA11gEMIzvMKbw50X5935WEYrTjlzCn/gfP4AA6WMYA==</latexit><latexit sha1_base64="6WjtAQy1eEki3DeLmUkkI9Sk/Os=">AAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0Xc+kvu/BunaRbaemDgcM65zL0nSAXXxnW/ncrG5tb2TnW3trd/cHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOB/WB6u/D7T6g0T+SDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzV+TpXhTOC8Nso0ppRN6RiHlkoao/bzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQ//OGmYlu/JzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3v/qrRbpZ1VOEMzqEJHlxDG+6gA11gEMIzvMKbw50X5935WEYrTjlzCn/gfP4AA6WMYA==</latexit><latexit sha1_base64="6WjtAQy1eEki3DeLmUkkI9Sk/Os=">AAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0Xc+kvu/BunaRbaemDgcM65zL0nSAXXxnW/ncrG5tb2TnW3trd/cHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOB/WB6u/D7T6g0T+SDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzV+TpXhTOC8Nso0ppRN6RiHlkoao/bzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQ//OGmYlu/JzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3v/qrRbpZ1VOEMzqEJHlxDG+6gA11gEMIzvMKbw50X5935WEYrTjlzCn/gfP4AA6WMYA==</latexit> (c) Global update
Figure 3: Updates in a GN block. Blue indicates the element that is being updated, and black
indicates other elements which are involved in the update (note that the pre-update value of the
blue element is also used in the update). See Equation 1 for details on the notation.
3.vis applied to each node i, to compute an updated node attribute, v0
i. In our running
example,vmay compute something analogous to the updated position, velocity, and kinetic
energy of each ball. The set of resulting per-node outputs is, V0=fv0
igi=1:Nv.
4.e!uis applied to E0, and aggregates all edge updates, into  e0, which will then be used in the
next step's global update. In our running example, e!umay compute the summed forces
(which should be zero, in this case, due to Newton's third law) and the springs' potential
energies.
5.v!uis applied to V0, and aggregates all node updates, into  v0, which will then be used in
the next step's global update. In our running example, v!umight compute the total kinetic
energy of the system.
6.uis applied once per graph, and computes an update for the global attribute, u0. In our
running example, umight compute something analogous to the net forces and total energy
of the physical system.
Note, though we assume this sequence of steps here, the order is not strictly enforced: it is possible
to reverse the update functions to proceed from global, to per-node, to per-edge updates, for example.
Kearnes et al. (2016) computes edge updates from nodes in a similar manner.
3.2.4 Relational inductive biases in graph networks
Our GN framework imposes several strong relational inductive biases when used as components in
a learning process. First, graphs can express arbitrary relationships among entities, which means
the GN's input determines how representations interact and are isolated, rather than those choices
being determined by the xed architecture. For example, the assumption that two entities have a
relationship|and thus should interact|is expressed by an edge between the entities' corresponding
nodes. Similarly, the absence of an edge expresses the assumption that the nodes have no relationship
and should not inuence each other directly.
Second, graphs represent entities and their relations as sets, which are invariant to permutations.
This means GNs are invariant to the order of these elements6, which is often desirable. For example,
the objects in a scene do not have a natural ordering (see Sec. 2.2).
Third, a GN's per-edge and per-node functions are reused across all edges and nodes, respectively.
This means GNs automatically support a form of combinatorial generalization (see Section 5.1):
because graphs are composed of edges, nodes, and global features, a single GN can operate on
graphs of dierent sizes (numbers of edges and nodes) and shapes (edge connectivity).
6Note, an ordering can be imposed by encoding the indices in the node or edge attributes, or via the edges
themselves (e.g. by encoding a chain or partial ordering).
134 Design principles for graph network architectures
The GN framework can be used to implement a wide variety of architectures, in accordance with
the design principles listed above in Section 3.2, which also correspond to the sub-sections (4.1,
4.2, and 4.3) below. In general, the framework is agnostic to specic attribute representations and
functional forms. Here, however, we focus mainly on deep learning architectures, which allow GNs
to act as learnable graph-to-graph function approximators.
4.1 Flexible representations
Graph networks support highly exible graph representations in two ways: rst, in terms of the
representation of the attributes; and second, in terms of the structure of the graph itself.
4.1.1 Attributes
The global, node, and edge attributes of a GN block can use arbitrary representational formats. In
deep learning implementations, real-valued vectors and tensors are most common. However, other
data structures such as sequences, sets, or even graphs could also be used.
The requirements of the problem will often determine what representations should be used for
the attributes. For example, when the input data is an image, the attributes might be represented
as tensors of image patches; however, when the input data is a text document, the attributes might
be sequences of words corresponding to sentences.
For each GN block within a broader architecture, the edge and node outputs typically correspond
to lists of vectors or tensors, one per edge or node, and the global outputs correspond to a single
vector or tensor. This allows a GN's output to be passed to other deep learning building blocks
such as MLPs, CNNs, and RNNs.
The output of a GN block can also be tailored to the demands of the task. In particular,
Anedge-focused GN uses the edges as output, for example to make decisions about interactions
among entities (Kipf et al., 2018; Hamrick et al., 2018).
Anode-focused GN uses the nodes as output, for example to reason about physical systems
(Battaglia et al., 2016; Chang et al., 2017; Wang et al., 2018b; Sanchez-Gonzalez et al., 2018).
Agraph-focused GN uses the globals as output, for example to predict the potential energy of
a physical system (Battaglia et al., 2016), the properties of a molecule (Gilmer et al., 2017),
or answers to questions about a visual scene (Santoro et al., 2017).
The nodes, edges, and global outputs can also be mixed-and-matched depending on the task. For
example, Hamrick et al. (2018) used both the output edge and global attributes to compute a policy
over actions.
4.1.2 Graph structure
When dening how the input data will be represented as a graph, there are generally two scenarios:
rst, the input explicitly species the relational structure; and second, the relational structure must
be inferred or assumed. These are not hard distinctions, but extremes along a continuum.
Examples of data with more explicitly specied entities and relations include knowledge graphs,
social networks, parse trees, optimization problems, chemical graphs, road networks, and physical
systems with known interactions. Figures 2a-d illustrate how such data can be expressed as graphs.
Examples of data where the relational structure is not made explicit, and must be inferred or
assumed, include visual scenes, text corpora, programming language source code, and multi-agent
14systems. In these types of settings, the data may be formatted as a set of entities without relations,
or even just a vector or tensor (e.g., an image). If the entities are not specied explicitly, they might
be assumed, for instance, by treating each word in a sentence (Vaswani et al., 2017) or each local
feature vector in a CNN's output feature map, as a node (Watters et al., 2017; Santoro et al., 2017;
Wang et al., 2018c) (Figures 2e-f). Or, it might be possible to use a separate learned mechanism to
infer entities from an unstructured signal (Luong et al., 2015; Mnih et al., 2014; Eslami et al., 2016;
van Steenkiste et al., 2018). If relations are not available, the simplest approach is to instantiate all
possible directed edges between entities (Figure 2f). This can be prohibitive for large numbers of
entities, however, because the number of possible edges grows quadratically with the number of
nodes. Thus developing more sophisticated ways of inferring sparse structure from unstructured
data (Kipf et al., 2018) is an important future direction.
4.2 Congurable within-block structure
The structure and functions within a GN block can be congured in dierent ways, which oers
exibility in what information is made available as inputs to its functions, as well as how output edge,
node, and global updates are produced. In particular, each in Equation 1 must be implemented
with some function, f, wheref's argument signature determines what information it requires as
input; in Figure 4, the incoming arrows to each depict whether u,V, andEare taken as inputs.
Hamrick et al. (2018) and Sanchez-Gonzalez et al. (2018) used the full GN block shown in Figure 4a.
Theirimplementations used neural networks (denoted NNe,NNv, and NNubelow, to indicate that
they are dierent functions with dierent parameters). Their implementations used elementwise
summation, but averages and max/min could also be used,
e(ek;vrk;vsk;u):=fe(ek;vrk;vsk;u) = NNe([ek;vrk;vsk;u]) (2)
v 
 e0
i;vi;u:=fv 
 e0
i;vi;u
= NNv 
[ e0
i;vi;u]
u 
 e0; v0;u:=fu 
 e0; v0;u
= NNu 
[ e0; v0;u]
e!v 
E0
i:= =X
fk:rk=ige0
k
v!u 
V0:= =X
iv0
i
e!u 
E0:= =X
ke0
k
where [ x;y;z] indicates vector/tensor concatenation. For vector attributes, a MLP is often used for
, while for tensors such as image feature maps, CNNs may be more suitable.
Thefunctions can also use RNNs, which requires an additional hidden state as input and
output. Figure 4b shows a very simple version of a GN block with RNNs as functions: there is no
message-passing in this formulation, and this type of block might be used for recurrent smoothing of
some dynamic graph states. Of course, RNNs as functions could also be used in a full GN block
(Figure 4a).
A variety of other architectures can be expressed in the GN framework, often as dierent
function choices and within-block congurations. The remaining sub-sections explore how a GN's
within-block structure can be congured in dierent ways, with examples of published work which
uses such congurations. See the Appendix for details.
15(a) Full GN block
u0,u0hid
<latexit sha1_base64="W2hu4ghYojV0SmOcBbcdBDITOSo=">AAACDnicbVC7TsMwFHV4lvIKMLJYVBUMqEoQEgwMlVgYi0QfUhtFjuu0Vm0nsh2kKsoXsPArLAwgxMrMxt/gpBlKy5EsHZ9zr+69J4gZVdpxfqyV1bX1jc3KVnV7Z3dv3z447KgokZi0ccQi2QuQIowK0tZUM9KLJUE8YKQbTG5zv/tIpKKReNDTmHgcjQQNKUbaSL5dH3Ckx0GYJtnpOZz7+AWXPB3TYebbNafhFIDLxC1JDZRo+fb3YBjhhBOhMUNK9V0n1l6KpKaYkaw6SBSJEZ6gEekbKhAnykuLczJYN8oQhpE0T2hYqPMdKeJKTXlgKvMd1aKXi/95/USH115KRZxoIvBsUJgwqCOYZwOHVBKs2dQQhCU1u0I8RhJhbRKsmhDcxZOXSeei4ToN9/6y1rwp46iAY3ACzoALrkAT3IEWaAMMnsALeAPv1rP1an1Yn7PSFavsOQJ/YH39AtnQnJg=</latexit><latexit sha1_base64="W2hu4ghYojV0SmOcBbcdBDITOSo=">AAACDnicbVC7TsMwFHV4lvIKMLJYVBUMqEoQEgwMlVgYi0QfUhtFjuu0Vm0nsh2kKsoXsPArLAwgxMrMxt/gpBlKy5EsHZ9zr+69J4gZVdpxfqyV1bX1jc3KVnV7Z3dv3z447KgokZi0ccQi2QuQIowK0tZUM9KLJUE8YKQbTG5zv/tIpKKReNDTmHgcjQQNKUbaSL5dH3Ckx0GYJtnpOZz7+AWXPB3TYebbNafhFIDLxC1JDZRo+fb3YBjhhBOhMUNK9V0n1l6KpKaYkaw6SBSJEZ6gEekbKhAnykuLczJYN8oQhpE0T2hYqPMdKeJKTXlgKvMd1aKXi/95/USH115KRZxoIvBsUJgwqCOYZwOHVBKs2dQQhCU1u0I8RhJhbRKsmhDcxZOXSeei4ToN9/6y1rwp46iAY3ACzoALrkAT3IEWaAMMnsALeAPv1rP1an1Yn7PSFavsOQJ/YH39AtnQnJg=</latexit><latexit sha1_base64="W2hu4ghYojV0SmOcBbcdBDITOSo=">AAACDnicbVC7TsMwFHV4lvIKMLJYVBUMqEoQEgwMlVgYi0QfUhtFjuu0Vm0nsh2kKsoXsPArLAwgxMrMxt/gpBlKy5EsHZ9zr+69J4gZVdpxfqyV1bX1jc3KVnV7Z3dv3z447KgokZi0ccQi2QuQIowK0tZUM9KLJUE8YKQbTG5zv/tIpKKReNDTmHgcjQQNKUbaSL5dH3Ckx0GYJtnpOZz7+AWXPB3TYebbNafhFIDLxC1JDZRo+fb3YBjhhBOhMUNK9V0n1l6KpKaYkaw6SBSJEZ6gEekbKhAnykuLczJYN8oQhpE0T2hYqPMdKeJKTXlgKvMd1aKXi/95/USH115KRZxoIvBsUJgwqCOYZwOHVBKs2dQQhCU1u0I8RhJhbRKsmhDcxZOXSeei4ToN9/6y1rwp46iAY3ACzoALrkAT3IEWaAMMnsALeAPv1rP1an1Yn7PSFavsOQJ/YH39AtnQnJg=</latexit><latexit sha1_base64="W2hu4ghYojV0SmOcBbcdBDITOSo=">AAACDnicbVC7TsMwFHV4lvIKMLJYVBUMqEoQEgwMlVgYi0QfUhtFjuu0Vm0nsh2kKsoXsPArLAwgxMrMxt/gpBlKy5EsHZ9zr+69J4gZVdpxfqyV1bX1jc3KVnV7Z3dv3z447KgokZi0ccQi2QuQIowK0tZUM9KLJUE8YKQbTG5zv/tIpKKReNDTmHgcjQQNKUbaSL5dH3Ckx0GYJtnpOZz7+AWXPB3TYebbNafhFIDLxC1JDZRo+fb3YBjhhBOhMUNK9V0n1l6KpKaYkaw6SBSJEZ6gEekbKhAnykuLczJYN8oQhpE0T2hYqPMdKeJKTXlgKvMd1aKXi/95/USH115KRZxoIvBsUJgwqCOYZwOHVBKs2dQQhCU1u0I8RhJhbRKsmhDcxZOXSeei4ToN9/6y1rwp46iAY3ACzoALrkAT3IEWaAMMnsALeAPv1rP1an1Yn7PSFavsOQJ/YH39AtnQnJg=</latexit>E0,E0hid
<latexit sha1_base64="ZYHbDTDbKJNm66GNrt+dUMK6558=">AAAB/HicbVDLSgMxFM3UV62v0S7dBIvUhZQZEXThoiAFlxXsA9phyGQybWiSGZKMUIb6K25cKOLWD3Hn35hpZ6GtBwKHc+7lnpwgYVRpx/m2SmvrG5tb5e3Kzu7e/oF9eNRVcSox6eCYxbIfIEUYFaSjqWakn0iCeMBIL5jc5n7vkUhFY/GgpwnxOBoJGlGMtJF8u9qqn8NW3R9ypMeSZ2Mazny75jScOeAqcQtSAwXavv01DGOcciI0Zkipgesk2suQ1BQzMqsMU0UShCdoRAaGCsSJ8rJ5+Bk8NUoIo1iaJzScq783MsSVmvLATOYZ1bKXi/95g1RH115GRZJqIvDiUJQyqGOYNwFDKgnWbGoIwpKarBCPkURYm74qpgR3+curpHvRcJ2Ge39Za94UdZTBMTgBZ8AFV6AJ7kAbdAAGU/AMXsGb9WS9WO/Wx2K0ZBU7VfAH1ucPbBGT+A==</latexit><latexit sha1_base64="ZYHbDTDbKJNm66GNrt+dUMK6558=">AAAB/HicbVDLSgMxFM3UV62v0S7dBIvUhZQZEXThoiAFlxXsA9phyGQybWiSGZKMUIb6K25cKOLWD3Hn35hpZ6GtBwKHc+7lnpwgYVRpx/m2SmvrG5tb5e3Kzu7e/oF9eNRVcSox6eCYxbIfIEUYFaSjqWakn0iCeMBIL5jc5n7vkUhFY/GgpwnxOBoJGlGMtJF8u9qqn8NW3R9ypMeSZ2Mazny75jScOeAqcQtSAwXavv01DGOcciI0Zkipgesk2suQ1BQzMqsMU0UShCdoRAaGCsSJ8rJ5+Bk8NUoIo1iaJzScq783MsSVmvLATOYZ1bKXi/95g1RH115GRZJqIvDiUJQyqGOYNwFDKgnWbGoIwpKarBCPkURYm74qpgR3+curpHvRcJ2Ge39Za94UdZTBMTgBZ8AFV6AJ7kAbdAAGU/AMXsGb9WS9WO/Wx2K0ZBU7VfAH1ucPbBGT+A==</latexit><latexit sha1_base64="ZYHbDTDbKJNm66GNrt+dUMK6558=">AAAB/HicbVDLSgMxFM3UV62v0S7dBIvUhZQZEXThoiAFlxXsA9phyGQybWiSGZKMUIb6K25cKOLWD3Hn35hpZ6GtBwKHc+7lnpwgYVRpx/m2SmvrG5tb5e3Kzu7e/oF9eNRVcSox6eCYxbIfIEUYFaSjqWakn0iCeMBIL5jc5n7vkUhFY/GgpwnxOBoJGlGMtJF8u9qqn8NW3R9ypMeSZ2Mazny75jScOeAqcQtSAwXavv01DGOcciI0Zkipgesk2suQ1BQzMqsMU0UShCdoRAaGCsSJ8rJ5+Bk8NUoIo1iaJzScq783MsSVmvLATOYZ1bKXi/95g1RH115GRZJqIvDiUJQyqGOYNwFDKgnWbGoIwpKarBCPkURYm74qpgR3+curpHvRcJ2Ge39Za94UdZTBMTgBZ8AFV6AJ7kAbdAAGU/AMXsGb9WS9WO/Wx2K0ZBU7VfAH1ucPbBGT+A==</latexit><latexit sha1_base64="ZYHbDTDbKJNm66GNrt+dUMK6558=">AAAB/HicbVDLSgMxFM3UV62v0S7dBIvUhZQZEXThoiAFlxXsA9phyGQybWiSGZKMUIb6K25cKOLWD3Hn35hpZ6GtBwKHc+7lnpwgYVRpx/m2SmvrG5tb5e3Kzu7e/oF9eNRVcSox6eCYxbIfIEUYFaSjqWakn0iCeMBIL5jc5n7vkUhFY/GgpwnxOBoJGlGMtJF8u9qqn8NW3R9ypMeSZ2Mazny75jScOeAqcQtSAwXavv01DGOcciI0Zkipgesk2suQ1BQzMqsMU0UShCdoRAaGCsSJ8rJ5+Bk8NUoIo1iaJzScq783MsSVmvLATOYZ1bKXi/95g1RH115GRZJqIvDiUJQyqGOYNwFDKgnWbGoIwpKarBCPkURYm74qpgR3+curpHvRcJ2Ge39Za94UdZTBMTgBZ8AFV6AJ7kAbdAAGU/AMXsGb9WS9WO/Wx2K0ZBU7VfAH1ucPbBGT+A==</latexit>V0,V0hid
<latexit sha1_base64="WhBBytL7AOLCOPFKqkdbfTZ2xq4=">AAAB/HicbVBPS8MwHE3nvzn/VXf0EhwyDzJaEfTgYeDF4wTXDbZS0jTdwpK0JKkwyvwqXjwo4tUP4s1vY7r1oJsPAo/3fj9+Ly9MGVXacb6tytr6xuZWdbu2s7u3f2AfHnkqySQmXZywRPZDpAijgnQ11Yz0U0kQDxnphZPbwu89EqloIh70NCU+RyNBY4qRNlJg173mOfSawZAjPZY8H9NoFtgNp+XMAVeJW5IGKNEJ7K9hlOCME6ExQ0oNXCfVfo6kppiRWW2YKZIiPEEjMjBUIE6Un8/Dz+CpUSIYJ9I8oeFc/b2RI67UlIdmssiolr1C/M8bZDq+9nMq0kwTgReH4oxBncCiCRhRSbBmU0MQltRkhXiMJMLa9FUzJbjLX14l3kXLdVru/WWjfVPWUQXH4AScARdcgTa4Ax3QBRhMwTN4BW/Wk/VivVsfi9GKVe7UwR9Ynz+huZQa</latexit><latexit sha1_base64="WhBBytL7AOLCOPFKqkdbfTZ2xq4=">AAAB/HicbVBPS8MwHE3nvzn/VXf0EhwyDzJaEfTgYeDF4wTXDbZS0jTdwpK0JKkwyvwqXjwo4tUP4s1vY7r1oJsPAo/3fj9+Ly9MGVXacb6tytr6xuZWdbu2s7u3f2AfHnkqySQmXZywRPZDpAijgnQ11Yz0U0kQDxnphZPbwu89EqloIh70NCU+RyNBY4qRNlJg173mOfSawZAjPZY8H9NoFtgNp+XMAVeJW5IGKNEJ7K9hlOCME6ExQ0oNXCfVfo6kppiRWW2YKZIiPEEjMjBUIE6Un8/Dz+CpUSIYJ9I8oeFc/b2RI67UlIdmssiolr1C/M8bZDq+9nMq0kwTgReH4oxBncCiCRhRSbBmU0MQltRkhXiMJMLa9FUzJbjLX14l3kXLdVru/WWjfVPWUQXH4AScARdcgTa4Ax3QBRhMwTN4BW/Wk/VivVsfi9GKVe7UwR9Ynz+huZQa</latexit><latexit sha1_base64="WhBBytL7AOLCOPFKqkdbfTZ2xq4=">AAAB/HicbVBPS8MwHE3nvzn/VXf0EhwyDzJaEfTgYeDF4wTXDbZS0jTdwpK0JKkwyvwqXjwo4tUP4s1vY7r1oJsPAo/3fj9+Ly9MGVXacb6tytr6xuZWdbu2s7u3f2AfHnkqySQmXZywRPZDpAijgnQ11Yz0U0kQDxnphZPbwu89EqloIh70NCU+RyNBY4qRNlJg173mOfSawZAjPZY8H9NoFtgNp+XMAVeJW5IGKNEJ7K9hlOCME6ExQ0oNXCfVfo6kppiRWW2YKZIiPEEjMjBUIE6Un8/Dz+CpUSIYJ9I8oeFc/b2RI67UlIdmssiolr1C/M8bZDq+9nMq0kwTgReH4oxBncCiCRhRSbBmU0MQltRkhXiMJMLa9FUzJbjLX14l3kXLdVru/WWjfVPWUQXH4AScARdcgTa4Ax3QBRhMwTN4BW/Wk/VivVsfi9GKVe7UwR9Ynz+huZQa</latexit><latexit sha1_base64="WhBBytL7AOLCOPFKqkdbfTZ2xq4=">AAAB/HicbVBPS8MwHE3nvzn/VXf0EhwyDzJaEfTgYeDF4wTXDbZS0jTdwpK0JKkwyvwqXjwo4tUP4s1vY7r1oJsPAo/3fj9+Ly9MGVXacb6tytr6xuZWdbu2s7u3f2AfHnkqySQmXZywRPZDpAijgnQ11Yz0U0kQDxnphZPbwu89EqloIh70NCU+RyNBY4qRNlJg173mOfSawZAjPZY8H9NoFtgNp+XMAVeJW5IGKNEJ7K9hlOCME6ExQ0oNXCfVfo6kppiRWW2YKZIiPEEjMjBUIE6Un8/Dz+CpUSIYJ9I8oeFc/b2RI67UlIdmssiolr1C/M8bZDq+9nMq0kwTgReH4oxBncCiCRhRSbBmU0MQltRkhXiMJMLa9FUzJbjLX14l3kXLdVru/WWjfVPWUQXH4AScARdcgTa4Ax3QBRhMwTN4BW/Wk/VivVsfi9GKVe7UwR9Ynz+huZQa</latexit>E,Ehid
<latexit sha1_base64="DYZek5SmevKS8py25dx0aIqUbBY=">AAAB+nicbVDLSsNAFL3xWesr1aWbwSK4kJKIoAsXBSm4rGAf0IYwmUzboTNJmJkoJfZT3LhQxK1f4s6/cdJmoa0HBg7n3Ms9c4KEM6Ud59taWV1b39gsbZW3d3b39u3KQVvFqSS0RWIey26AFeUsoi3NNKfdRFIsAk47wfgm9zsPVCoWR/d6klBP4GHEBoxgbSTfrjTOUMPvC6xHUmQjFk59u+rUnBnQMnELUoUCTd/+6ocxSQWNNOFYqZ7rJNrLsNSMcDot91NFE0zGeEh7hkZYUOVls+hTdGKUEA1iaV6k0Uz9vZFhodREBGYyz6gWvVz8z+ulenDlZSxKUk0jMj80SDnSMcp7QCGTlGg+MQQTyUxWREZYYqJNW2VTgrv45WXSPq+5Ts29u6jWr4s6SnAEx3AKLlxCHW6hCS0g8AjP8Apv1pP1Yr1bH/PRFavYOYQ/sD5/AKXVk5Y=</latexit><latexit sha1_base64="DYZek5SmevKS8py25dx0aIqUbBY=">AAAB+nicbVDLSsNAFL3xWesr1aWbwSK4kJKIoAsXBSm4rGAf0IYwmUzboTNJmJkoJfZT3LhQxK1f4s6/cdJmoa0HBg7n3Ms9c4KEM6Ud59taWV1b39gsbZW3d3b39u3KQVvFqSS0RWIey26AFeUsoi3NNKfdRFIsAk47wfgm9zsPVCoWR/d6klBP4GHEBoxgbSTfrjTOUMPvC6xHUmQjFk59u+rUnBnQMnELUoUCTd/+6ocxSQWNNOFYqZ7rJNrLsNSMcDot91NFE0zGeEh7hkZYUOVls+hTdGKUEA1iaV6k0Uz9vZFhodREBGYyz6gWvVz8z+ulenDlZSxKUk0jMj80SDnSMcp7QCGTlGg+MQQTyUxWREZYYqJNW2VTgrv45WXSPq+5Ts29u6jWr4s6SnAEx3AKLlxCHW6hCS0g8AjP8Apv1pP1Yr1bH/PRFavYOYQ/sD5/AKXVk5Y=</latexit><latexit sha1_base64="DYZek5SmevKS8py25dx0aIqUbBY=">AAAB+nicbVDLSsNAFL3xWesr1aWbwSK4kJKIoAsXBSm4rGAf0IYwmUzboTNJmJkoJfZT3LhQxK1f4s6/cdJmoa0HBg7n3Ms9c4KEM6Ud59taWV1b39gsbZW3d3b39u3KQVvFqSS0RWIey26AFeUsoi3NNKfdRFIsAk47wfgm9zsPVCoWR/d6klBP4GHEBoxgbSTfrjTOUMPvC6xHUmQjFk59u+rUnBnQMnELUoUCTd/+6ocxSQWNNOFYqZ7rJNrLsNSMcDot91NFE0zGeEh7hkZYUOVls+hTdGKUEA1iaV6k0Uz9vZFhodREBGYyz6gWvVz8z+ulenDlZSxKUk0jMj80SDnSMcp7QCGTlGg+MQQTyUxWREZYYqJNW2VTgrv45WXSPq+5Ts29u6jWr4s6SnAEx3AKLlxCHW6hCS0g8AjP8Apv1pP1Yr1bH/PRFavYOYQ/sD5/AKXVk5Y=</latexit><latexit sha1_base64="DYZek5SmevKS8py25dx0aIqUbBY=">AAAB+nicbVDLSsNAFL3xWesr1aWbwSK4kJKIoAsXBSm4rGAf0IYwmUzboTNJmJkoJfZT3LhQxK1f4s6/cdJmoa0HBg7n3Ms9c4KEM6Ud59taWV1b39gsbZW3d3b39u3KQVvFqSS0RWIey26AFeUsoi3NNKfdRFIsAk47wfgm9zsPVCoWR/d6klBP4GHEBoxgbSTfrjTOUMPvC6xHUmQjFk59u+rUnBnQMnELUoUCTd/+6ocxSQWNNOFYqZ7rJNrLsNSMcDot91NFE0zGeEh7hkZYUOVls+hTdGKUEA1iaV6k0Uz9vZFhodREBGYyz6gWvVz8z+ulenDlZSxKUk0jMj80SDnSMcp7QCGTlGg+MQQTyUxWREZYYqJNW2VTgrv45WXSPq+5Ts29u6jWr4s6SnAEx3AKLlxCHW6hCS0g8AjP8Apv1pP1Yr1bH/PRFavYOYQ/sD5/AKXVk5Y=</latexit>V, Vhid
<latexit sha1_base64="brwuxF74R6OEOykh378as/RBfzE=">AAAB+nicbVBNS8NAFHzxs9avVI9eFovgQUoigh48FLx4rGDTQhvCZrNtl+4mYXejlNif4sWDIl79Jd78N27aHLR1YGGYeY83O2HKmdKO822trK6tb2xWtqrbO7t7+3btwFNJJgltk4QnshtiRTmLaVszzWk3lRSLkNNOOL4p/M4DlYol8b2epNQXeBizASNYGymwa94Z8oK+wHokRT5i0TSw607DmQEtE7ckdSjRCuyvfpSQTNBYE46V6rlOqv0cS80Ip9NqP1M0xWSMh7RnaIwFVX4+iz5FJ0aJ0CCR5sUazdTfGzkWSk1EaCaLjGrRK8T/vF6mB1d+zuI00zQm80ODjCOdoKIHFDFJieYTQzCRzGRFZIQlJtq0VTUluItfXibeecN1Gu7dRb15XdZRgSM4hlNw4RKacAstaAOBR3iGV3iznqwX6936mI+uWOXOIfyB9fkD20qTuA==</latexit><latexit sha1_base64="brwuxF74R6OEOykh378as/RBfzE=">AAAB+nicbVBNS8NAFHzxs9avVI9eFovgQUoigh48FLx4rGDTQhvCZrNtl+4mYXejlNif4sWDIl79Jd78N27aHLR1YGGYeY83O2HKmdKO822trK6tb2xWtqrbO7t7+3btwFNJJgltk4QnshtiRTmLaVszzWk3lRSLkNNOOL4p/M4DlYol8b2epNQXeBizASNYGymwa94Z8oK+wHokRT5i0TSw607DmQEtE7ckdSjRCuyvfpSQTNBYE46V6rlOqv0cS80Ip9NqP1M0xWSMh7RnaIwFVX4+iz5FJ0aJ0CCR5sUazdTfGzkWSk1EaCaLjGrRK8T/vF6mB1d+zuI00zQm80ODjCOdoKIHFDFJieYTQzCRzGRFZIQlJtq0VTUluItfXibeecN1Gu7dRb15XdZRgSM4hlNw4RKacAstaAOBR3iGV3iznqwX6936mI+uWOXOIfyB9fkD20qTuA==</latexit><latexit sha1_base64="brwuxF74R6OEOykh378as/RBfzE=">AAAB+nicbVBNS8NAFHzxs9avVI9eFovgQUoigh48FLx4rGDTQhvCZrNtl+4mYXejlNif4sWDIl79Jd78N27aHLR1YGGYeY83O2HKmdKO822trK6tb2xWtqrbO7t7+3btwFNJJgltk4QnshtiRTmLaVszzWk3lRSLkNNOOL4p/M4DlYol8b2epNQXeBizASNYGymwa94Z8oK+wHokRT5i0TSw607DmQEtE7ckdSjRCuyvfpSQTNBYE46V6rlOqv0cS80Ip9NqP1M0xWSMh7RnaIwFVX4+iz5FJ0aJ0CCR5sUazdTfGzkWSk1EaCaLjGrRK8T/vF6mB1d+zuI00zQm80ODjCOdoKIHFDFJieYTQzCRzGRFZIQlJtq0VTUluItfXibeecN1Gu7dRb15XdZRgSM4hlNw4RKacAstaAOBR3iGV3iznqwX6936mI+uWOXOIfyB9fkD20qTuA==</latexit><latexit sha1_base64="brwuxF74R6OEOykh378as/RBfzE=">AAAB+nicbVBNS8NAFHzxs9avVI9eFovgQUoigh48FLx4rGDTQhvCZrNtl+4mYXejlNif4sWDIl79Jd78N27aHLR1YGGYeY83O2HKmdKO822trK6tb2xWtqrbO7t7+3btwFNJJgltk4QnshtiRTmLaVszzWk3lRSLkNNOOL4p/M4DlYol8b2epNQXeBizASNYGymwa94Z8oK+wHokRT5i0TSw607DmQEtE7ckdSjRCuyvfpSQTNBYE46V6rlOqv0cS80Ip9NqP1M0xWSMh7RnaIwFVX4+iz5FJ0aJ0CCR5sUazdTfGzkWSk1EaCaLjGrRK8T/vF6mB1d+zuI00zQm80ODjCOdoKIHFDFJieYTQzCRzGRFZIQlJtq0VTUluItfXibeecN1Gu7dRb15XdZRgSM4hlNw4RKacAstaAOBR3iGV3iznqwX6936mI+uWOXOIfyB9fkD20qTuA==</latexit>u,uhid
<latexit sha1_base64="UO6spgXZarocGO1zBKohmEwCj+c=">AAACDHicbZDNSsNAFIVv6l+tf1WXbgaL4EJKIoIuXBTcuKxgW6ENZTKZtENnkjAzEUrIA7jxVdy4UMStD+DOt3GSBtTWAwMf597L3Hu8mDOlbfvLqiwtr6yuVddrG5tb2zv13b2uihJJaIdEPJJ3HlaUs5B2NNOc3sWSYuFx2vMmV3m9d0+lYlF4q6cxdQUehSxgBGtjDeuNgcB67AVpkp2gHx4WKEU6Zn5muuymXQgtglNCA0q1h/XPgR+RRNBQE46V6jt2rN0US80Ip1ltkCgaYzLBI9o3GGJBlZsWx2ToyDg+CiJpXqhR4f6eSLFQaio805nvqOZruflfrZ/o4MJNWRgnmoZk9lGQcKQjlCeDfCYp0XxqABPJzK6IjLHERJv8aiYEZ/7kReieNh276dycNVqXZRxVOIBDOAYHzqEF19CGDhB4gCd4gVfr0Xq23qz3WWvFKmf24Y+sj28GGpw2</latexit><latexit sha1_base64="UO6spgXZarocGO1zBKohmEwCj+c=">AAACDHicbZDNSsNAFIVv6l+tf1WXbgaL4EJKIoIuXBTcuKxgW6ENZTKZtENnkjAzEUrIA7jxVdy4UMStD+DOt3GSBtTWAwMf597L3Hu8mDOlbfvLqiwtr6yuVddrG5tb2zv13b2uihJJaIdEPJJ3HlaUs5B2NNOc3sWSYuFx2vMmV3m9d0+lYlF4q6cxdQUehSxgBGtjDeuNgcB67AVpkp2gHx4WKEU6Zn5muuymXQgtglNCA0q1h/XPgR+RRNBQE46V6jt2rN0US80Ip1ltkCgaYzLBI9o3GGJBlZsWx2ToyDg+CiJpXqhR4f6eSLFQaio805nvqOZruflfrZ/o4MJNWRgnmoZk9lGQcKQjlCeDfCYp0XxqABPJzK6IjLHERJv8aiYEZ/7kReieNh276dycNVqXZRxVOIBDOAYHzqEF19CGDhB4gCd4gVfr0Xq23qz3WWvFKmf24Y+sj28GGpw2</latexit><latexit sha1_base64="UO6spgXZarocGO1zBKohmEwCj+c=">AAACDHicbZDNSsNAFIVv6l+tf1WXbgaL4EJKIoIuXBTcuKxgW6ENZTKZtENnkjAzEUrIA7jxVdy4UMStD+DOt3GSBtTWAwMf597L3Hu8mDOlbfvLqiwtr6yuVddrG5tb2zv13b2uihJJaIdEPJJ3HlaUs5B2NNOc3sWSYuFx2vMmV3m9d0+lYlF4q6cxdQUehSxgBGtjDeuNgcB67AVpkp2gHx4WKEU6Zn5muuymXQgtglNCA0q1h/XPgR+RRNBQE46V6jt2rN0US80Ip1ltkCgaYzLBI9o3GGJBlZsWx2ToyDg+CiJpXqhR4f6eSLFQaio805nvqOZruflfrZ/o4MJNWRgnmoZk9lGQcKQjlCeDfCYp0XxqABPJzK6IjLHERJv8aiYEZ/7kReieNh276dycNVqXZRxVOIBDOAYHzqEF19CGDhB4gCd4gVfr0Xq23qz3WWvFKmf24Y+sj28GGpw2</latexit><latexit sha1_base64="UO6spgXZarocGO1zBKohmEwCj+c=">AAACDHicbZDNSsNAFIVv6l+tf1WXbgaL4EJKIoIuXBTcuKxgW6ENZTKZtENnkjAzEUrIA7jxVdy4UMStD+DOt3GSBtTWAwMf597L3Hu8mDOlbfvLqiwtr6yuVddrG5tb2zv13b2uihJJaIdEPJJ3HlaUs5B2NNOc3sWSYuFx2vMmV3m9d0+lYlF4q6cxdQUehSxgBGtjDeuNgcB67AVpkp2gHx4WKEU6Zn5muuymXQgtglNCA0q1h/XPgR+RRNBQE46V6jt2rN0US80Ip1ltkCgaYzLBI9o3GGJBlZsWx2ToyDg+CiJpXqhR4f6eSLFQaio805nvqOZruflfrZ/o4MJNWRgnmoZk9lGQcKQjlCeDfCYp0XxqABPJzK6IjLHERJv8aiYEZ/7kReieNh276dycNVqXZRxVOIBDOAYHzqEF19CGDhB4gCd4gVfr0Xq23qz3WWvFKmf24Y+sj28GGpw2</latexit>
Edge blockNode blockGlobal block u
<latexit sha1_base64="znt8hwWv6wryqwCugrweUa+jkM8=">AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubfaSNXu7y+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvFhxZqzvf3tLyyura+uFjeLm1vbObmlvPzQy04TWieRSN2NsKGeC1i2znDaVpjiNOW3Eg6ux33ik2jApbu1Q0SjFPcESRrB1UthWfXaXdUplv+JPgBZJMCPl6mH4dA8AtU7pq92VJEupsIRjY1qBr2yUY20Z4XRUbGeGKkwGuEdbjgqcUhPlk2tH6MQpXZRI7UpYNFF/T+Q4NWaYxq4zxbZv5r2x+J/XymxyEeVMqMxSQaaLkowjK9H4ddRlmhLLh45gopm7FZE+1phYF1DRhRDMv7xIwrNK4FeCG5fGJUxRgCM4hlMI4ByqcA01qAOBB3iGV3jzpPfivXsf09YlbzZzAH/gff4AqE6Qnw==</latexit><latexit sha1_base64="Nc0DXje6uYB+/0fHlXL99yCq0no=">AAAB7XicbVC7SgNBFL0bXzG+ooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0/kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/cD5+AGXTkqw=</latexit><latexit sha1_base64="Nc0DXje6uYB+/0fHlXL99yCq0no=">AAAB7XicbVC7SgNBFL0bXzG+ooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0/kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/cD5+AGXTkqw=</latexit><latexit sha1_base64="S5XnA5iYIAgqxiI+i0ptSwAiKP4=">AAAB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/CHrlSt+1Z+DrJIgJxXIUe+Vv7p9xbIEpWWCGtMJ/NSGE6otZwKnpW5mMKVsRAfYcVTSBE04mV87JWdO6ZNYaVfSkrn6e2JCE2PGSeQ6E2qHZtmbif95nczGV+GEyzSzKNliUZwJYhWZvU76XCOzYuwIZZq7WwkbUk2ZdQGVXAjB8surpHlRDfxqcOdXatd5HEU4gVM4hwAuoQa3UIcGMHiEZ3iFN095L96797FoLXj5zDH8gff5A53Djxw=</latexit>
<latexit sha1_base64="m8MJ1M94ujO0d0COo5n2Dsol6rc=">AAAB53icbVC7SgNBFL0bXzG+opY2g0GwCrs2phAM2FhGMA9IFpmdvZsMmZ1dZmaFsKS0sbFQxNZP8Rfs/AZ/wsmj0MQDFw7nnMt9BKng2rjul1NYWV1b3yhulra2d3b3yvsHLZ1kimGTJSJRnYBqFFxi03AjsJMqpHEgsB0MryZ++x6V5om8NaMU/Zj2JY84o8ZKjbtyxa26U5Bl4s1J5fLj+4EAgM1/9sKEZTFKwwTVuuu5qfFzqgxnAselXqYxpWxI+9i1VNIYtZ9P9xyTE6uEJEqULWnIVP3dkdNY61Ec2GRMzUAvehPxP6+bmajm51ymmUHJZoOiTBCTkMnRJOQKmREjSyhT3O5K2IAqyox9Tck+wVs8eZm0zqqeW/Vu3Er9AmYowhEcwyl4cA51uIYGNIFBCI/wDC8Od56cV+dtFi04855D+APn/Qd/sI8A</latexit><latexit sha1_base64="q1zM5ZsCNMJAtNUZI97B98ATlaA=">AAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo+1Si4xJrhRmAzUUgjX2DD71+O/MYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL96+7g7fqt81/tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhY/V3R0YjrQeRb5MRNT09643E/7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1+TtE7zZk+dJ/aTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvk+iCM+3Zgz9wXn8AGkeQ8w==</latexit><latexit sha1_base64="q1zM5ZsCNMJAtNUZI97B98ATlaA=">AAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo+1Si4xJrhRmAzUUgjX2DD71+O/MYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL96+7g7fqt81/tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhY/V3R0YjrQeRb5MRNT09643E/7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1+TtE7zZk+dJ/aTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvk+iCM+3Zgz9wXn8AGkeQ8w==</latexit><latexit sha1_base64="ioxb3woZF1oAlTScqds23PrgiMY=">AAAB53icbVC7SgNBFL3rM8ZX1NJmMAhWYdZGC4uAjWUE84BkkdnZ2WTI7Owyc1cIS37AxkIRW3/Jzr9xkmyhiQcGDuecy9x7wkxJi5R+e2vrG5tb25Wd6u7e/sFh7ei4Y9PccNHmqUpNL2RWKKlFGyUq0cuMYEmoRDcc38787pMwVqb6ASeZCBI21DKWnKGTWo+1Om3QOcgq8UtShxIu/zWIUp4nQiNXzNq+TzMMCmZQciWm1UFuRcb4mA1F31HNEmGDYr7nlJw7JSJxatzTSObq74mCJdZOktAlE4Yju+zNxP+8fo7xdVBIneUoNF98FOeKYEpmR5NIGsFRTRxh3Ei3K+EjZhhHV03VleAvn7xKOpcNnzb8e1pv3pR1VOAUzuACfLiCJtxBC9rAIYJneIU3T3ov3rv3sYiueeXMCfyB9/kDCGmMcA==</latexit> v
<latexit sha1_base64="HCiXjOq04H3f4Ed7vqyiRfd+2dI=">AAAB7XicbVA9SwNBEJ2LXzF+nQo2NotBsAp3NlpYBGwsI5hLIDnj3mYvWbO3e+zuRcKR/2BjoYit/8fOf+Pmo9DEBwOP92aYmRelnGnjed9OYWV1bX2juFna2t7Z3XP3DwItM0VonUguVTPCmnImaN0ww2kzVRQnEaeNaHA98RtDqjST4s6MUhomuCdYzAg2VgraaZ/dDztu2at4U6Bl4s9JuXoUPD0AQK3jfrW7kmQJFYZwrHXL91IT5lgZRjgdl9qZpikmA9yjLUsFTqgO8+m1Y3RqlS6KpbIlDJqqvydynGg9SiLbmWDT14veRPzPa2UmvgxzJtLMUEFmi+KMIyPR5HXUZYoSw0eWYKKYvRWRPlaYGBtQyYbgL768TILziu9V/FubxhXMUIRjOIEz8OECqnADNagDgUd4hld4c6Tz4rw7H7PWgjOfOYQ/cD5/AKnSkKA=</latexit><latexit sha1_base64="/voSHBGyFE5xYPVKPYM/GTIarLQ=">AAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQW/NZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMMp+HAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit><latexit sha1_base64="/voSHBGyFE5xYPVKPYM/GTIarLQ=">AAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQW/NZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMMp+HAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit><latexit sha1_base64="Fc8T4ygtia14k1z/CDji4ezWDqY=">AAAB7XicbVA9SwNBEJ3zM8avqKXNYhCswp2NFhYBG8sI5gOSM+xtNsmavd1jdy4QjvwHGwtFbP0/dv4bN8kVmvhg4PHeDDPzokQKi77/7a2tb2xubRd2irt7+weHpaPjhtWpYbzOtNSmFVHLpVC8jgIlbyWG0ziSvBmNbmd+c8yNFVo94CThYUwHSvQFo+ikRicZisdxt1T2K/4cZJUEOSlDjlq39NXpaZbGXCGT1Np24CcYZtSgYJJPi53U8oSyER3wtqOKxtyG2fzaKTl3So/0tXGlkMzV3xMZja2dxJHrjCkO7bI3E//z2in2r8NMqCRFrthiUT+VBDWZvU56wnCGcuIIZUa4WwkbUkMZuoCKLoRg+eVV0risBH4luPfL1Zs8jgKcwhlcQABXUIU7qEEdGDzBM7zCm6e9F+/d+1i0rnn5zAn8gff5A59Hjx0=</latexit> e
<latexit sha1_base64="gRKFy+QFytmwqWy0cvo5FmmPz8I=">AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubeaSNXu3x+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvDAVXBvX/XaWlldW19YLG8XNre2d3dLevq9lphjWmRRSNUOqUfAE64Ybgc1UIY1DgY1wcDX2G4+oNJfJrRmmGMS0l/CIM2qs5LfTPr/DTqnsVtwJyCLxZqRcPfSf7gGg1il9tbuSZTEmhgmqdctzUxPkVBnOBI6K7UxjStmA9rBlaUJj1EE+uXZETqzSJZFUthJDJurviZzGWg/j0HbG1PT1vDcW//NamYkugpwnaWYwYdNFUSaIkWT8OulyhcyIoSWUKW5vJaxPFWXGBlS0IXjzLy8S/6ziuRXvxqZxCVMU4AiO4RQ8OIcqXEMN6sDgAZ7hFd4c6bw4787HtHXJmc0cwB84nz+QDpCP</latexit><latexit sha1_base64="74MJShuZzxGyM2nLY3EY87InuhI=">AAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA==</latexit><latexit sha1_base64="74MJShuZzxGyM2nLY3EY87InuhI=">AAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA==</latexit><latexit sha1_base64="pLq6KB/1S9uyUeWp/G4byg43mK0=">AAAB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/AF75Ypf9ecgqyTISQVy1Hvlr25fsSxBaZmgxnQCP7XhhGrLmcBpqZsZTCkb0QF2HJU0QRNO5tdOyZlT+iRW2pW0ZK7+npjQxJhxErnOhNqhWfZm4n9eJ7PxVTjhMs0sSrZYFGeCWEVmr5M+18isGDtCmebuVsKGVFNmXUAlF0Kw/PIqaV5UA78a3PmV2nUeRxFO4BTOIYBLqMEt1KEBDB7hGV7hzVPei/fufSxaC14+cwx/4H3+AIWDjww=</latexit> (b) Independent recurrent block
Edge blockNode blockGlobal blockV0
<latexit sha1_base64="gAQ7qdt3IKvK5oBqK3uN1PHYi1k=">AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit><latexit sha1_base64="gAQ7qdt3IKvK5oBqK3uN1PHYi1k=">AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit><latexit sha1_base64="gAQ7qdt3IKvK5oBqK3uN1PHYi1k=">AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit><latexit sha1_base64="gAQ7qdt3IKvK5oBqK3uN1PHYi1k=">AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit>u0
<latexit sha1_base64="Z/n1gIms2/ONBt0R58c8NGdBbqU=">AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit><latexit sha1_base64="Z/n1gIms2/ONBt0R58c8NGdBbqU=">AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit><latexit sha1_base64="Z/n1gIms2/ONBt0R58c8NGdBbqU=">AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit><latexit sha1_base64="Z/n1gIms2/ONBt0R58c8NGdBbqU=">AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit> u
<latexit sha1_base64="znt8hwWv6wryqwCugrweUa+jkM8=">AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubfaSNXu7y+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvFhxZqzvf3tLyyura+uFjeLm1vbObmlvPzQy04TWieRSN2NsKGeC1i2znDaVpjiNOW3Eg6ux33ik2jApbu1Q0SjFPcESRrB1UthWfXaXdUplv+JPgBZJMCPl6mH4dA8AtU7pq92VJEupsIRjY1qBr2yUY20Z4XRUbGeGKkwGuEdbjgqcUhPlk2tH6MQpXZRI7UpYNFF/T+Q4NWaYxq4zxbZv5r2x+J/XymxyEeVMqMxSQaaLkowjK9H4ddRlmhLLh45gopm7FZE+1phYF1DRhRDMv7xIwrNK4FeCG5fGJUxRgCM4hlMI4ByqcA01qAOBB3iGV3jzpPfivXsf09YlbzZzAH/gff4AqE6Qnw==</latexit><latexit sha1_base64="Nc0DXje6uYB+/0fHlXL99yCq0no=">AAAB7XicbVC7SgNBFL0bXzG+ooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0/kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/cD5+AGXTkqw=</latexit><latexit sha1_base64="Nc0DXje6uYB+/0fHlXL99yCq0no=">AAAB7XicbVC7SgNBFL0bXzG+ooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0/kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/cD5+AGXTkqw=</latexit><latexit sha1_base64="S5XnA5iYIAgqxiI+i0ptSwAiKP4=">AAAB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/CHrlSt+1Z+DrJIgJxXIUe+Vv7p9xbIEpWWCGtMJ/NSGE6otZwKnpW5mMKVsRAfYcVTSBE04mV87JWdO6ZNYaVfSkrn6e2JCE2PGSeQ6E2qHZtmbif95nczGV+GEyzSzKNliUZwJYhWZvU76XCOzYuwIZZq7WwkbUk2ZdQGVXAjB8surpHlRDfxqcOdXatd5HEU4gVM4hwAuoQa3UIcGMHiEZ3iFN095L96797FoLXj5zDH8gff5A53Djxw=</latexit>⇢v!u
<latexit sha1_base64="8QVocR3pGD0i/QL+G1OhPZDl9fE=">AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWz3TRLN9mwO6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUEquAbH+bZKK6tr6xvlzcrW9s7unr1/0NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs8+8lQkH/KRp/ggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zop+DlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJ/XjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bH/PWklXMHKI/sD5/AAdVlig=</latexit><latexit sha1_base64="8QVocR3pGD0i/QL+G1OhPZDl9fE=">AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWz3TRLN9mwO6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUEquAbH+bZKK6tr6xvlzcrW9s7unr1/0NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs8+8lQkH/KRp/ggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zop+DlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJ/XjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bH/PWklXMHKI/sD5/AAdVlig=</latexit><latexit sha1_base64="8QVocR3pGD0i/QL+G1OhPZDl9fE=">AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWz3TRLN9mwO6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUEquAbH+bZKK6tr6xvlzcrW9s7unr1/0NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs8+8lQkH/KRp/ggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zop+DlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJ/XjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bH/PWklXMHKI/sD5/AAdVlig=</latexit><latexit sha1_base64="8QVocR3pGD0i/QL+G1OhPZDl9fE=">AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWz3TRLN9mwO6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUEquAbH+bZKK6tr6xvlzcrW9s7unr1/0NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs8+8lQkH/KRp/ggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zop+DlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJ/XjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bH/PWklXMHKI/sD5/AAdVlig=</latexit>
<latexit sha1_base64="m8MJ1M94ujO0d0COo5n2Dsol6rc=">AAAB53icbVC7SgNBFL0bXzG+opY2g0GwCrs2phAM2FhGMA9IFpmdvZsMmZ1dZmaFsKS0sbFQxNZP8Rfs/AZ/wsmj0MQDFw7nnMt9BKng2rjul1NYWV1b3yhulra2d3b3yvsHLZ1kimGTJSJRnYBqFFxi03AjsJMqpHEgsB0MryZ++x6V5om8NaMU/Zj2JY84o8ZKjbtyxa26U5Bl4s1J5fLj+4EAgM1/9sKEZTFKwwTVuuu5qfFzqgxnAselXqYxpWxI+9i1VNIYtZ9P9xyTE6uEJEqULWnIVP3dkdNY61Ec2GRMzUAvehPxP6+bmajm51ymmUHJZoOiTBCTkMnRJOQKmREjSyhT3O5K2IAqyox9Tck+wVs8eZm0zqqeW/Vu3Er9AmYowhEcwyl4cA51uIYGNIFBCI/wDC8Od56cV+dtFi04855D+APn/Qd/sI8A</latexit><latexit sha1_base64="q1zM5ZsCNMJAtNUZI97B98ATlaA=">AAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo+1Si4xJrhRmAzUUgjX2DD71+O/MYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL96+7g7fqt81/tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhY/V3R0YjrQeRb5MRNT09643E/7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1+TtE7zZk+dJ/aTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvk+iCM+3Zgz9wXn8AGkeQ8w==</latexit><latexit sha1_base64="q1zM5ZsCNMJAtNUZI97B98ATlaA=">AAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo+1Si4xJrhRmAzUUgjX2DD71+O/MYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL96+7g7fqt81/tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhY/V3R0YjrQeRb5MRNT09643E/7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1+TtE7zZk+dJ/aTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvk+iCM+3Zgz9wXn8AGkeQ8w==</latexit><latexit sha1_base64="ioxb3woZF1oAlTScqds23PrgiMY=">AAAB53icbVC7SgNBFL3rM8ZX1NJmMAhWYdZGC4uAjWUE84BkkdnZ2WTI7Owyc1cIS37AxkIRW3/Jzr9xkmyhiQcGDuecy9x7wkxJi5R+e2vrG5tb25Wd6u7e/sFh7ei4Y9PccNHmqUpNL2RWKKlFGyUq0cuMYEmoRDcc38787pMwVqb6ASeZCBI21DKWnKGTWo+1Om3QOcgq8UtShxIu/zWIUp4nQiNXzNq+TzMMCmZQciWm1UFuRcb4mA1F31HNEmGDYr7nlJw7JSJxatzTSObq74mCJdZOktAlE4Yju+zNxP+8fo7xdVBIneUoNF98FOeKYEpmR5NIGsFRTRxh3Ei3K+EjZhhHV03VleAvn7xKOpcNnzb8e1pv3pR1VOAUzuACfLiCJtxBC9rAIYJneIU3T3ov3rv3sYiueeXMCfyB9/kDCGmMcA==</latexit> v
<latexit sha1_base64="HCiXjOq04H3f4Ed7vqyiRfd+2dI=">AAAB7XicbVA9SwNBEJ2LXzF+nQo2NotBsAp3NlpYBGwsI5hLIDnj3mYvWbO3e+zuRcKR/2BjoYit/8fOf+Pmo9DEBwOP92aYmRelnGnjed9OYWV1bX2juFna2t7Z3XP3DwItM0VonUguVTPCmnImaN0ww2kzVRQnEaeNaHA98RtDqjST4s6MUhomuCdYzAg2VgraaZ/dDztu2at4U6Bl4s9JuXoUPD0AQK3jfrW7kmQJFYZwrHXL91IT5lgZRjgdl9qZpikmA9yjLUsFTqgO8+m1Y3RqlS6KpbIlDJqqvydynGg9SiLbmWDT14veRPzPa2UmvgxzJtLMUEFmi+KMIyPR5HXUZYoSw0eWYKKYvRWRPlaYGBtQyYbgL768TILziu9V/FubxhXMUIRjOIEz8OECqnADNagDgUd4hld4c6Tz4rw7H7PWgjOfOYQ/cD5/AKnSkKA=</latexit><latexit sha1_base64="/voSHBGyFE5xYPVKPYM/GTIarLQ=">AAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQW/NZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMMp+HAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit><latexit sha1_base64="/voSHBGyFE5xYPVKPYM/GTIarLQ=">AAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQW/NZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMMp+HAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit><latexit sha1_base64="Fc8T4ygtia14k1z/CDji4ezWDqY=">AAAB7XicbVA9SwNBEJ3zM8avqKXNYhCswp2NFhYBG8sI5gOSM+xtNsmavd1jdy4QjvwHGwtFbP0/dv4bN8kVmvhg4PHeDDPzokQKi77/7a2tb2xubRd2irt7+weHpaPjhtWpYbzOtNSmFVHLpVC8jgIlbyWG0ziSvBmNbmd+c8yNFVo94CThYUwHSvQFo+ikRicZisdxt1T2K/4cZJUEOSlDjlq39NXpaZbGXCGT1Np24CcYZtSgYJJPi53U8oSyER3wtqOKxtyG2fzaKTl3So/0tXGlkMzV3xMZja2dxJHrjCkO7bI3E//z2in2r8NMqCRFrthiUT+VBDWZvU56wnCGcuIIZUa4WwkbUkMZuoCKLoRg+eVV0risBH4luPfL1Zs8jgKcwhlcQABXUIU7qEEdGDzBM7zCm6e9F+/d+1i0rnn5zAn8gff5A59Hjx0=</latexit>⇢e!v
<latexit sha1_base64="s3/Cw/iD/Ic9TAit26LWmPV1hK0=">AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit><latexit sha1_base64="s3/Cw/iD/Ic9TAit26LWmPV1hK0=">AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit><latexit sha1_base64="s3/Cw/iD/Ic9TAit26LWmPV1hK0=">AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit><latexit sha1_base64="s3/Cw/iD/Ic9TAit26LWmPV1hK0=">AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit> e
<latexit sha1_base64="gRKFy+QFytmwqWy0cvo5FmmPz8I=">AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubeaSNXu3x+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvDAVXBvX/XaWlldW19YLG8XNre2d3dLevq9lphjWmRRSNUOqUfAE64Ybgc1UIY1DgY1wcDX2G4+oNJfJrRmmGMS0l/CIM2qs5LfTPr/DTqnsVtwJyCLxZqRcPfSf7gGg1il9tbuSZTEmhgmqdctzUxPkVBnOBI6K7UxjStmA9rBlaUJj1EE+uXZETqzSJZFUthJDJurviZzGWg/j0HbG1PT1vDcW//NamYkugpwnaWYwYdNFUSaIkWT8OulyhcyIoSWUKW5vJaxPFWXGBlS0IXjzLy8S/6ziuRXvxqZxCVMU4AiO4RQ8OIcqXEMN6sDgAZ7hFd4c6bw4787HtHXJmc0cwB84nz+QDpCP</latexit><latexit sha1_base64="74MJShuZzxGyM2nLY3EY87InuhI=">AAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA==</latexit><latexit sha1_base64="74MJShuZzxGyM2nLY3EY87InuhI=">AAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA==</latexit><latexit sha1_base64="pLq6KB/1S9uyUeWp/G4byg43mK0=">AAAB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/AF75Ypf9ecgqyTISQVy1Hvlr25fsSxBaZmgxnQCP7XhhGrLmcBpqZsZTCkb0QF2HJU0QRNO5tdOyZlT+iRW2pW0ZK7+npjQxJhxErnOhNqhWfZm4n9eJ7PxVTjhMs0sSrZYFGeCWEVmr5M+18isGDtCmebuVsKGVFNmXUAlF0Kw/PIqaV5UA78a3PmV2nUeRxFO4BTOIYBLqMEt1KEBDB7hGV7hzVPei/fufSxaC14+cwx/4H3+AIWDjww=</latexit>E<latexit sha1_base64="iJ/x8cSgmmYNbMN8WtCvsNrlH/U=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit><latexit sha1_base64="iJ/x8cSgmmYNbMN8WtCvsNrlH/U=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit><latexit sha1_base64="iJ/x8cSgmmYNbMN8WtCvsNrlH/U=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit><latexit sha1_base64="iJ/x8cSgmmYNbMN8WtCvsNrlH/U=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit>V<latexit sha1_base64="xc4uzoZiBSxZUZkArgltxczS6nM=">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64="xc4uzoZiBSxZUZkArgltxczS6nM=">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64="xc4uzoZiBSxZUZkArgltxczS6nM=">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64="xc4uzoZiBSxZUZkArgltxczS6nM=">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit>
(c) Message-passing neural network
Edge blockNode blockGlobal blockV0
<latexit sha1_base64="gAQ7qdt3IKvK5oBqK3uN1PHYi1k=">AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit><latexit sha1_base64="gAQ7qdt3IKvK5oBqK3uN1PHYi1k=">AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit><latexit sha1_base64="gAQ7qdt3IKvK5oBqK3uN1PHYi1k=">AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit><latexit sha1_base64="gAQ7qdt3IKvK5oBqK3uN1PHYi1k=">AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit>
<latexit sha1_base64="m8MJ1M94ujO0d0COo5n2Dsol6rc=">AAAB53icbVC7SgNBFL0bXzG+opY2g0GwCrs2phAM2FhGMA9IFpmdvZsMmZ1dZmaFsKS0sbFQxNZP8Rfs/AZ/wsmj0MQDFw7nnMt9BKng2rjul1NYWV1b3yhulra2d3b3yvsHLZ1kimGTJSJRnYBqFFxi03AjsJMqpHEgsB0MryZ++x6V5om8NaMU/Zj2JY84o8ZKjbtyxa26U5Bl4s1J5fLj+4EAgM1/9sKEZTFKwwTVuuu5qfFzqgxnAselXqYxpWxI+9i1VNIYtZ9P9xyTE6uEJEqULWnIVP3dkdNY61Ec2GRMzUAvehPxP6+bmajm51ymmUHJZoOiTBCTkMnRJOQKmREjSyhT3O5K2IAqyox9Tck+wVs8eZm0zqqeW/Vu3Er9AmYowhEcwyl4cA51uIYGNIFBCI/wDC8Od56cV+dtFi04855D+APn/Qd/sI8A</latexit><latexit sha1_base64="q1zM5ZsCNMJAtNUZI97B98ATlaA=">AAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo+1Si4xJrhRmAzUUgjX2DD71+O/MYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL96+7g7fqt81/tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhY/V3R0YjrQeRb5MRNT09643E/7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1+TtE7zZk+dJ/aTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvk+iCM+3Zgz9wXn8AGkeQ8w==</latexit><latexit sha1_base64="q1zM5ZsCNMJAtNUZI97B98ATlaA=">AAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo+1Si4xJrhRmAzUUgjX2DD71+O/MYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL96+7g7fqt81/tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhY/V3R0YjrQeRb5MRNT09643E/7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1+TtE7zZk+dJ/aTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvk+iCM+3Zgz9wXn8AGkeQ8w==</latexit><latexit sha1_base64="ioxb3woZF1oAlTScqds23PrgiMY=">AAAB53icbVC7SgNBFL3rM8ZX1NJmMAhWYdZGC4uAjWUE84BkkdnZ2WTI7Owyc1cIS37AxkIRW3/Jzr9xkmyhiQcGDuecy9x7wkxJi5R+e2vrG5tb25Wd6u7e/sFh7ei4Y9PccNHmqUpNL2RWKKlFGyUq0cuMYEmoRDcc38787pMwVqb6ASeZCBI21DKWnKGTWo+1Om3QOcgq8UtShxIu/zWIUp4nQiNXzNq+TzMMCmZQciWm1UFuRcb4mA1F31HNEmGDYr7nlJw7JSJxatzTSObq74mCJdZOktAlE4Yju+zNxP+8fo7xdVBIneUoNF98FOeKYEpmR5NIGsFRTRxh3Ei3K+EjZhhHV03VleAvn7xKOpcNnzb8e1pv3pR1VOAUzuACfLiCJtxBC9rAIYJneIU3T3ov3rv3sYiueeXMCfyB9/kDCGmMcA==</latexit> v
<latexit sha1_base64="HCiXjOq04H3f4Ed7vqyiRfd+2dI=">AAAB7XicbVA9SwNBEJ2LXzF+nQo2NotBsAp3NlpYBGwsI5hLIDnj3mYvWbO3e+zuRcKR/2BjoYit/8fOf+Pmo9DEBwOP92aYmRelnGnjed9OYWV1bX2juFna2t7Z3XP3DwItM0VonUguVTPCmnImaN0ww2kzVRQnEaeNaHA98RtDqjST4s6MUhomuCdYzAg2VgraaZ/dDztu2at4U6Bl4s9JuXoUPD0AQK3jfrW7kmQJFYZwrHXL91IT5lgZRjgdl9qZpikmA9yjLUsFTqgO8+m1Y3RqlS6KpbIlDJqqvydynGg9SiLbmWDT14veRPzPa2UmvgxzJtLMUEFmi+KMIyPR5HXUZYoSw0eWYKKYvRWRPlaYGBtQyYbgL768TILziu9V/FubxhXMUIRjOIEz8OECqnADNagDgUd4hld4c6Tz4rw7H7PWgjOfOYQ/cD5/AKnSkKA=</latexit><latexit sha1_base64="/voSHBGyFE5xYPVKPYM/GTIarLQ=">AAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQW/NZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMMp+HAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit><latexit sha1_base64="/voSHBGyFE5xYPVKPYM/GTIarLQ=">AAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQW/NZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMMp+HAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit><latexit sha1_base64="Fc8T4ygtia14k1z/CDji4ezWDqY=">AAAB7XicbVA9SwNBEJ3zM8avqKXNYhCswp2NFhYBG8sI5gOSM+xtNsmavd1jdy4QjvwHGwtFbP0/dv4bN8kVmvhg4PHeDDPzokQKi77/7a2tb2xubRd2irt7+weHpaPjhtWpYbzOtNSmFVHLpVC8jgIlbyWG0ziSvBmNbmd+c8yNFVo94CThYUwHSvQFo+ikRicZisdxt1T2K/4cZJUEOSlDjlq39NXpaZbGXCGT1Np24CcYZtSgYJJPi53U8oSyER3wtqOKxtyG2fzaKTl3So/0tXGlkMzV3xMZja2dxJHrjCkO7bI3E//z2in2r8NMqCRFrthiUT+VBDWZvU56wnCGcuIIZUa4WwkbUkMZuoCKLoRg+eVV0risBH4luPfL1Zs8jgKcwhlcQABXUIU7qEEdGDzBM7zCm6e9F+/d+1i0rnn5zAn8gff5A59Hjx0=</latexit>⇢e!v
<latexit sha1_base64="s3/Cw/iD/Ic9TAit26LWmPV1hK0=">AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit><latexit sha1_base64="s3/Cw/iD/Ic9TAit26LWmPV1hK0=">AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit><latexit sha1_base64="s3/Cw/iD/Ic9TAit26LWmPV1hK0=">AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit><latexit sha1_base64="s3/Cw/iD/Ic9TAit26LWmPV1hK0=">AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit> e
<latexit sha1_base64="gRKFy+QFytmwqWy0cvo5FmmPz8I=">AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubeaSNXu3x+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvDAVXBvX/XaWlldW19YLG8XNre2d3dLevq9lphjWmRRSNUOqUfAE64Ybgc1UIY1DgY1wcDX2G4+oNJfJrRmmGMS0l/CIM2qs5LfTPr/DTqnsVtwJyCLxZqRcPfSf7gGg1il9tbuSZTEmhgmqdctzUxPkVBnOBI6K7UxjStmA9rBlaUJj1EE+uXZETqzSJZFUthJDJurviZzGWg/j0HbG1PT1vDcW//NamYkugpwnaWYwYdNFUSaIkWT8OulyhcyIoSWUKW5vJaxPFWXGBlS0IXjzLy8S/6ziuRXvxqZxCVMU4AiO4RQ8OIcqXEMN6sDgAZ7hFd4c6bw4787HtHXJmc0cwB84nz+QDpCP</latexit><latexit sha1_base64="74MJShuZzxGyM2nLY3EY87InuhI=">AAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA==</latexit><latexit sha1_base64="74MJShuZzxGyM2nLY3EY87InuhI=">AAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA==</latexit><latexit sha1_base64="pLq6KB/1S9uyUeWp/G4byg43mK0=">AAAB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/AF75Ypf9ecgqyTISQVy1Hvlr25fsSxBaZmgxnQCP7XhhGrLmcBpqZsZTCkb0QF2HJU0QRNO5tdOyZlT+iRW2pW0ZK7+npjQxJhxErnOhNqhWfZm4n9eJ7PxVTjhMs0sSrZYFGeCWEVmr5M+18isGDtCmebuVsKGVFNmXUAlF0Kw/PIqaV5UA78a3PmV2nUeRxFO4BTOIYBLqMEt1KEBDB7hGV7hzVPei/fufSxaC14+cwx/4H3+AIWDjww=</latexit>E<latexit sha1_base64="iJ/x8cSgmmYNbMN8WtCvsNrlH/U=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit><latexit sha1_base64="iJ/x8cSgmmYNbMN8WtCvsNrlH/U=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit><latexit sha1_base64="iJ/x8cSgmmYNbMN8WtCvsNrlH/U=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit><latexit sha1_base64="iJ/x8cSgmmYNbMN8WtCvsNrlH/U=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit>V<latexit sha1_base64="xc4uzoZiBSxZUZkArgltxczS6nM=">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64="xc4uzoZiBSxZUZkArgltxczS6nM=">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64="xc4uzoZiBSxZUZkArgltxczS6nM=">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64="xc4uzoZiBSxZUZkArgltxczS6nM=">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit> (d) Non-local neural network
Edge blockNode blockGlobal blocku0
<latexit sha1_base64="Z/n1gIms2/ONBt0R58c8NGdBbqU=">AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit><latexit sha1_base64="Z/n1gIms2/ONBt0R58c8NGdBbqU=">AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit><latexit sha1_base64="Z/n1gIms2/ONBt0R58c8NGdBbqU=">AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit><latexit sha1_base64="Z/n1gIms2/ONBt0R58c8NGdBbqU=">AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit> u
<latexit sha1_base64="znt8hwWv6wryqwCugrweUa+jkM8=">AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubfaSNXu7y+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvFhxZqzvf3tLyyura+uFjeLm1vbObmlvPzQy04TWieRSN2NsKGeC1i2znDaVpjiNOW3Eg6ux33ik2jApbu1Q0SjFPcESRrB1UthWfXaXdUplv+JPgBZJMCPl6mH4dA8AtU7pq92VJEupsIRjY1qBr2yUY20Z4XRUbGeGKkwGuEdbjgqcUhPlk2tH6MQpXZRI7UpYNFF/T+Q4NWaYxq4zxbZv5r2x+J/XymxyEeVMqMxSQaaLkowjK9H4ddRlmhLLh45gopm7FZE+1phYF1DRhRDMv7xIwrNK4FeCG5fGJUxRgCM4hlMI4ByqcA01qAOBB3iGV3jzpPfivXsf09YlbzZzAH/gff4AqE6Qnw==</latexit><latexit sha1_base64="Nc0DXje6uYB+/0fHlXL99yCq0no=">AAAB7XicbVC7SgNBFL0bXzG+ooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0/kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/cD5+AGXTkqw=</latexit><latexit sha1_base64="Nc0DXje6uYB+/0fHlXL99yCq0no=">AAAB7XicbVC7SgNBFL0bXzG+ooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0/kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/cD5+AGXTkqw=</latexit><latexit sha1_base64="S5XnA5iYIAgqxiI+i0ptSwAiKP4=">AAAB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/CHrlSt+1Z+DrJIgJxXIUe+Vv7p9xbIEpWWCGtMJ/NSGE6otZwKnpW5mMKVsRAfYcVTSBE04mV87JWdO6ZNYaVfSkrn6e2JCE2PGSeQ6E2qHZtmbif95nczGV+GEyzSzKNliUZwJYhWZvU76XCOzYuwIZZq7WwkbUk2ZdQGVXAjB8surpHlRDfxqcOdXatd5HEU4gVM4hwAuoQa3UIcGMHiEZ3iFN095L96797FoLXj5zDH8gff5A53Djxw=</latexit>⇢e!u
<latexit sha1_base64="2suSYs2KtjHJeb1CIts1JrhYPII=">AAAB/nicbVBNS8NAEN34WetXVDx5WSyCp5KIoMeiF48V7Ac0sWy2m3bpZjfsTpQSCv4VLx4U8erv8Oa/cdvmoK0PBh7vzTAzL0oFN+B5387S8srq2nppo7y5tb2z6+7tN43KNGUNqoTS7YgYJrhkDeAgWDvVjCSRYK1oeD3xWw9MG67kHYxSFiakL3nMKQErdd3DQA/Ufc4CzfsDIFqrR5yNu27Fq3pT4EXiF6SCCtS77lfQUzRLmAQqiDEd30shzIkGTgUbl4PMsJTQIemzjqWSJMyE+fT8MT6xSg/HStuSgKfq74mcJMaMksh2JgQGZt6biP95nQziyzDnMs2ASTpbFGcCg8KTLHCPa0ZBjCwhVHN7K6YDogkFm1jZhuDPv7xImmdV36v6t+eV2lURRwkdoWN0inx0gWroBtVRA1GUo2f0it6cJ+fFeXc+Zq1LTjFzgP7A+fwB7JSWFw==</latexit><latexit sha1_base64="2suSYs2KtjHJeb1CIts1JrhYPII=">AAAB/nicbVBNS8NAEN34WetXVDx5WSyCp5KIoMeiF48V7Ac0sWy2m3bpZjfsTpQSCv4VLx4U8erv8Oa/cdvmoK0PBh7vzTAzL0oFN+B5387S8srq2nppo7y5tb2z6+7tN43KNGUNqoTS7YgYJrhkDeAgWDvVjCSRYK1oeD3xWw9MG67kHYxSFiakL3nMKQErdd3DQA/Ufc4CzfsDIFqrR5yNu27Fq3pT4EXiF6SCCtS77lfQUzRLmAQqiDEd30shzIkGTgUbl4PMsJTQIemzjqWSJMyE+fT8MT6xSg/HStuSgKfq74mcJMaMksh2JgQGZt6biP95nQziyzDnMs2ASTpbFGcCg8KTLHCPa0ZBjCwhVHN7K6YDogkFm1jZhuDPv7xImmdV36v6t+eV2lURRwkdoWN0inx0gWroBtVRA1GUo2f0it6cJ+fFeXc+Zq1LTjFzgP7A+fwB7JSWFw==</latexit><latexit sha1_base64="2suSYs2KtjHJeb1CIts1JrhYPII=">AAAB/nicbVBNS8NAEN34WetXVDx5WSyCp5KIoMeiF48V7Ac0sWy2m3bpZjfsTpQSCv4VLx4U8erv8Oa/cdvmoK0PBh7vzTAzL0oFN+B5387S8srq2nppo7y5tb2z6+7tN43KNGUNqoTS7YgYJrhkDeAgWDvVjCSRYK1oeD3xWw9MG67kHYxSFiakL3nMKQErdd3DQA/Ufc4CzfsDIFqrR5yNu27Fq3pT4EXiF6SCCtS77lfQUzRLmAQqiDEd30shzIkGTgUbl4PMsJTQIemzjqWSJMyE+fT8MT6xSg/HStuSgKfq74mcJMaMksh2JgQGZt6biP95nQziyzDnMs2ASTpbFGcCg8KTLHCPa0ZBjCwhVHN7K6YDogkFm1jZhuDPv7xImmdV36v6t+eV2lURRwkdoWN0inx0gWroBtVRA1GUo2f0it6cJ+fFeXc+Zq1LTjFzgP7A+fwB7JSWFw==</latexit><latexit sha1_base64="2suSYs2KtjHJeb1CIts1JrhYPII=">AAAB/nicbVBNS8NAEN34WetXVDx5WSyCp5KIoMeiF48V7Ac0sWy2m3bpZjfsTpQSCv4VLx4U8erv8Oa/cdvmoK0PBh7vzTAzL0oFN+B5387S8srq2nppo7y5tb2z6+7tN43KNGUNqoTS7YgYJrhkDeAgWDvVjCSRYK1oeD3xWw9MG67kHYxSFiakL3nMKQErdd3DQA/Ufc4CzfsDIFqrR5yNu27Fq3pT4EXiF6SCCtS77lfQUzRLmAQqiDEd30shzIkGTgUbl4PMsJTQIemzjqWSJMyE+fT8MT6xSg/HStuSgKfq74mcJMaMksh2JgQGZt6biP95nQziyzDnMs2ASTpbFGcCg8KTLHCPa0ZBjCwhVHN7K6YDogkFm1jZhuDPv7xImmdV36v6t+eV2lURRwkdoWN0inx0gWroBtVRA1GUo2f0it6cJ+fFeXc+Zq1LTjFzgP7A+fwB7JSWFw==</latexit> e
<latexit sha1_base64="gRKFy+QFytmwqWy0cvo5FmmPz8I=">AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubeaSNXu3x+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvDAVXBvX/XaWlldW19YLG8XNre2d3dLevq9lphjWmRRSNUOqUfAE64Ybgc1UIY1DgY1wcDX2G4+oNJfJrRmmGMS0l/CIM2qs5LfTPr/DTqnsVtwJyCLxZqRcPfSf7gGg1il9tbuSZTEmhgmqdctzUxPkVBnOBI6K7UxjStmA9rBlaUJj1EE+uXZETqzSJZFUthJDJurviZzGWg/j0HbG1PT1vDcW//NamYkugpwnaWYwYdNFUSaIkWT8OulyhcyIoSWUKW5vJaxPFWXGBlS0IXjzLy8S/6ziuRXvxqZxCVMU4AiO4RQ8OIcqXEMN6sDgAZ7hFd4c6bw4787HtHXJmc0cwB84nz+QDpCP</latexit><latexit sha1_base64="74MJShuZzxGyM2nLY3EY87InuhI=">AAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA==</latexit><latexit sha1_base64="74MJShuZzxGyM2nLY3EY87InuhI=">AAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA==</latexit><latexit sha1_base64="pLq6KB/1S9uyUeWp/G4byg43mK0=">AAAB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/AF75Ypf9ecgqyTISQVy1Hvlr25fsSxBaZmgxnQCP7XhhGrLmcBpqZsZTCkb0QF2HJU0QRNO5tdOyZlT+iRW2pW0ZK7+npjQxJhxErnOhNqhWfZm4n9eJ7PxVTjhMs0sSrZYFGeCWEVmr5M+18isGDtCmebuVsKGVFNmXUAlF0Kw/PIqaV5UA78a3PmV2nUeRxFO4BTOIYBLqMEt1KEBDB7hGV7hzVPei/fufSxaC14+cwx/4H3+AIWDjww=</latexit>E<latexit sha1_base64="iJ/x8cSgmmYNbMN8WtCvsNrlH/U=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit><latexit sha1_base64="iJ/x8cSgmmYNbMN8WtCvsNrlH/U=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit><latexit sha1_base64="iJ/x8cSgmmYNbMN8WtCvsNrlH/U=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit><latexit sha1_base64="iJ/x8cSgmmYNbMN8WtCvsNrlH/U=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit>V<latexit sha1_base64="xc4uzoZiBSxZUZkArgltxczS6nM=">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64="xc4uzoZiBSxZUZkArgltxczS6nM=">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64="xc4uzoZiBSxZUZkArgltxczS6nM=">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64="xc4uzoZiBSxZUZkArgltxczS6nM=">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit>
(e) Relation network
Edge blockNode blockGlobal blocku0
<latexit sha1_base64="Z/n1gIms2/ONBt0R58c8NGdBbqU=">AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit><latexit sha1_base64="Z/n1gIms2/ONBt0R58c8NGdBbqU=">AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit><latexit sha1_base64="Z/n1gIms2/ONBt0R58c8NGdBbqU=">AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit><latexit sha1_base64="Z/n1gIms2/ONBt0R58c8NGdBbqU=">AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit> u
<latexit sha1_base64="znt8hwWv6wryqwCugrweUa+jkM8=">AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubfaSNXu7y+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvFhxZqzvf3tLyyura+uFjeLm1vbObmlvPzQy04TWieRSN2NsKGeC1i2znDaVpjiNOW3Eg6ux33ik2jApbu1Q0SjFPcESRrB1UthWfXaXdUplv+JPgBZJMCPl6mH4dA8AtU7pq92VJEupsIRjY1qBr2yUY20Z4XRUbGeGKkwGuEdbjgqcUhPlk2tH6MQpXZRI7UpYNFF/T+Q4NWaYxq4zxbZv5r2x+J/XymxyEeVMqMxSQaaLkowjK9H4ddRlmhLLh45gopm7FZE+1phYF1DRhRDMv7xIwrNK4FeCG5fGJUxRgCM4hlMI4ByqcA01qAOBB3iGV3jzpPfivXsf09YlbzZzAH/gff4AqE6Qnw==</latexit><latexit sha1_base64="Nc0DXje6uYB+/0fHlXL99yCq0no=">AAAB7XicbVC7SgNBFL0bXzG+ooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0/kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/cD5+AGXTkqw=</latexit><latexit sha1_base64="Nc0DXje6uYB+/0fHlXL99yCq0no=">AAAB7XicbVC7SgNBFL0bXzG+ooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0/kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/cD5+AGXTkqw=</latexit><latexit sha1_base64="S5XnA5iYIAgqxiI+i0ptSwAiKP4=">AAAB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/CHrlSt+1Z+DrJIgJxXIUe+Vv7p9xbIEpWWCGtMJ/NSGE6otZwKnpW5mMKVsRAfYcVTSBE04mV87JWdO6ZNYaVfSkrn6e2JCE2PGSeQ6E2qHZtmbif95nczGV+GEyzSzKNliUZwJYhWZvU76XCOzYuwIZZq7WwkbUk2ZdQGVXAjB8surpHlRDfxqcOdXatd5HEU4gVM4hwAuoQa3UIcGMHiEZ3iFN095L96797FoLXj5zDH8gff5A53Djxw=</latexit>⇢v!u
<latexit sha1_base64="8QVocR3pGD0i/QL+G1OhPZDl9fE=">AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWz3TRLN9mwO6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUEquAbH+bZKK6tr6xvlzcrW9s7unr1/0NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs8+8lQkH/KRp/ggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zop+DlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJ/XjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bH/PWklXMHKI/sD5/AAdVlig=</latexit><latexit sha1_base64="8QVocR3pGD0i/QL+G1OhPZDl9fE=">AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWz3TRLN9mwO6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUEquAbH+bZKK6tr6xvlzcrW9s7unr1/0NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs8+8lQkH/KRp/ggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zop+DlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJ/XjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bH/PWklXMHKI/sD5/AAdVlig=</latexit><latexit sha1_base64="8QVocR3pGD0i/QL+G1OhPZDl9fE=">AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWz3TRLN9mwO6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUEquAbH+bZKK6tr6xvlzcrW9s7unr1/0NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs8+8lQkH/KRp/ggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zop+DlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJ/XjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bH/PWklXMHKI/sD5/AAdVlig=</latexit><latexit sha1_base64="8QVocR3pGD0i/QL+G1OhPZDl9fE=">AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWz3TRLN9mwO6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUEquAbH+bZKK6tr6xvlzcrW9s7unr1/0NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs8+8lQkH/KRp/ggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zop+DlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJ/XjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bH/PWklXMHKI/sD5/AAdVlig=</latexit>
<latexit sha1_base64="m8MJ1M94ujO0d0COo5n2Dsol6rc=">AAAB53icbVC7SgNBFL0bXzG+opY2g0GwCrs2phAM2FhGMA9IFpmdvZsMmZ1dZmaFsKS0sbFQxNZP8Rfs/AZ/wsmj0MQDFw7nnMt9BKng2rjul1NYWV1b3yhulra2d3b3yvsHLZ1kimGTJSJRnYBqFFxi03AjsJMqpHEgsB0MryZ++x6V5om8NaMU/Zj2JY84o8ZKjbtyxa26U5Bl4s1J5fLj+4EAgM1/9sKEZTFKwwTVuuu5qfFzqgxnAselXqYxpWxI+9i1VNIYtZ9P9xyTE6uEJEqULWnIVP3dkdNY61Ec2GRMzUAvehPxP6+bmajm51ymmUHJZoOiTBCTkMnRJOQKmREjSyhT3O5K2IAqyox9Tck+wVs8eZm0zqqeW/Vu3Er9AmYowhEcwyl4cA51uIYGNIFBCI/wDC8Od56cV+dtFi04855D+APn/Qd/sI8A</latexit><latexit sha1_base64="q1zM5ZsCNMJAtNUZI97B98ATlaA=">AAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo+1Si4xJrhRmAzUUgjX2DD71+O/MYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL96+7g7fqt81/tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhY/V3R0YjrQeRb5MRNT09643E/7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1+TtE7zZk+dJ/aTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvk+iCM+3Zgz9wXn8AGkeQ8w==</latexit><latexit sha1_base64="q1zM5ZsCNMJAtNUZI97B98ATlaA=">AAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo+1Si4xJrhRmAzUUgjX2DD71+O/MYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL96+7g7fqt81/tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhY/V3R0YjrQeRb5MRNT09643E/7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1+TtE7zZk+dJ/aTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvk+iCM+3Zgz9wXn8AGkeQ8w==</latexit><latexit sha1_base64="ioxb3woZF1oAlTScqds23PrgiMY=">AAAB53icbVC7SgNBFL3rM8ZX1NJmMAhWYdZGC4uAjWUE84BkkdnZ2WTI7Owyc1cIS37AxkIRW3/Jzr9xkmyhiQcGDuecy9x7wkxJi5R+e2vrG5tb25Wd6u7e/sFh7ei4Y9PccNHmqUpNL2RWKKlFGyUq0cuMYEmoRDcc38787pMwVqb6ASeZCBI21DKWnKGTWo+1Om3QOcgq8UtShxIu/zWIUp4nQiNXzNq+TzMMCmZQciWm1UFuRcb4mA1F31HNEmGDYr7nlJw7JSJxatzTSObq74mCJdZOktAlE4Yju+zNxP+8fo7xdVBIneUoNF98FOeKYEpmR5NIGsFRTRxh3Ei3K+EjZhhHV03VleAvn7xKOpcNnzb8e1pv3pR1VOAUzuACfLiCJtxBC9rAIYJneIU3T3ov3rv3sYiueeXMCfyB9/kDCGmMcA==</latexit> v
<latexit sha1_base64="HCiXjOq04H3f4Ed7vqyiRfd+2dI=">AAAB7XicbVA9SwNBEJ2LXzF+nQo2NotBsAp3NlpYBGwsI5hLIDnj3mYvWbO3e+zuRcKR/2BjoYit/8fOf+Pmo9DEBwOP92aYmRelnGnjed9OYWV1bX2juFna2t7Z3XP3DwItM0VonUguVTPCmnImaN0ww2kzVRQnEaeNaHA98RtDqjST4s6MUhomuCdYzAg2VgraaZ/dDztu2at4U6Bl4s9JuXoUPD0AQK3jfrW7kmQJFYZwrHXL91IT5lgZRjgdl9qZpikmA9yjLUsFTqgO8+m1Y3RqlS6KpbIlDJqqvydynGg9SiLbmWDT14veRPzPa2UmvgxzJtLMUEFmi+KMIyPR5HXUZYoSw0eWYKKYvRWRPlaYGBtQyYbgL768TILziu9V/FubxhXMUIRjOIEz8OECqnADNagDgUd4hld4c6Tz4rw7H7PWgjOfOYQ/cD5/AKnSkKA=</latexit><latexit sha1_base64="/voSHBGyFE5xYPVKPYM/GTIarLQ=">AAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQW/NZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMMp+HAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit><latexit sha1_base64="/voSHBGyFE5xYPVKPYM/GTIarLQ=">AAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQW/NZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMMp+HAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit><latexit sha1_base64="Fc8T4ygtia14k1z/CDji4ezWDqY=">AAAB7XicbVA9SwNBEJ3zM8avqKXNYhCswp2NFhYBG8sI5gOSM+xtNsmavd1jdy4QjvwHGwtFbP0/dv4bN8kVmvhg4PHeDDPzokQKi77/7a2tb2xubRd2irt7+weHpaPjhtWpYbzOtNSmFVHLpVC8jgIlbyWG0ziSvBmNbmd+c8yNFVo94CThYUwHSvQFo+ikRicZisdxt1T2K/4cZJUEOSlDjlq39NXpaZbGXCGT1Np24CcYZtSgYJJPi53U8oSyER3wtqOKxtyG2fzaKTl3So/0tXGlkMzV3xMZja2dxJHrjCkO7bI3E//z2in2r8NMqCRFrthiUT+VBDWZvU56wnCGcuIIZUa4WwkbUkMZuoCKLoRg+eVV0risBH4luPfL1Zs8jgKcwhlcQABXUIU7qEEdGDzBM7zCm6e9F+/d+1i0rnn5zAn8gff5A59Hjx0=</latexit>V<latexit sha1_base64="xc4uzoZiBSxZUZkArgltxczS6nM=">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64="xc4uzoZiBSxZUZkArgltxczS6nM=">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64="xc4uzoZiBSxZUZkArgltxczS6nM=">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64="xc4uzoZiBSxZUZkArgltxczS6nM=">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit>u<latexit sha1_base64="0coyYP26hzTYQyo/d27+M3N3DnU=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6MJFwY3LCvaBbSiT6aQdOpmEmRuhhP6FGxeKuPVv3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJre533ni2ohYPeA04X5ER0qEglG00mM/ojgOwiydDao1t+7OQVaJV5AaFGgOql/9YczSiCtkkhrT89wE/YxqFEzyWaWfGp5QNqEj3rNU0YgbP5snnpEzqwxJGGv7FJK5+nsjo5Ex0yiwk3lCs+zl4n9eL8Xw2s+ESlLkii0+ClNJMCb5+WQoNGcop5ZQpoXNStiYasrQllSxJXjLJ6+S9kXdc+ve/WWtcVPUUYYTOIVz8OAKGnAHTWgBAwXP8ApvjnFenHfnYzFacoqdY/gD5/MH9tyREw==</latexit><latexit sha1_base64="0coyYP26hzTYQyo/d27+M3N3DnU=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6MJFwY3LCvaBbSiT6aQdOpmEmRuhhP6FGxeKuPVv3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJre533ni2ohYPeA04X5ER0qEglG00mM/ojgOwiydDao1t+7OQVaJV5AaFGgOql/9YczSiCtkkhrT89wE/YxqFEzyWaWfGp5QNqEj3rNU0YgbP5snnpEzqwxJGGv7FJK5+nsjo5Ex0yiwk3lCs+zl4n9eL8Xw2s+ESlLkii0+ClNJMCb5+WQoNGcop5ZQpoXNStiYasrQllSxJXjLJ6+S9kXdc+ve/WWtcVPUUYYTOIVz8OAKGnAHTWgBAwXP8ApvjnFenHfnYzFacoqdY/gD5/MH9tyREw==</latexit><latexit sha1_base64="0coyYP26hzTYQyo/d27+M3N3DnU=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6MJFwY3LCvaBbSiT6aQdOpmEmRuhhP6FGxeKuPVv3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJre533ni2ohYPeA04X5ER0qEglG00mM/ojgOwiydDao1t+7OQVaJV5AaFGgOql/9YczSiCtkkhrT89wE/YxqFEzyWaWfGp5QNqEj3rNU0YgbP5snnpEzqwxJGGv7FJK5+nsjo5Ex0yiwk3lCs+zl4n9eL8Xw2s+ESlLkii0+ClNJMCb5+WQoNGcop5ZQpoXNStiYasrQllSxJXjLJ6+S9kXdc+ve/WWtcVPUUYYTOIVz8OAKGnAHTWgBAwXP8ApvjnFenHfnYzFacoqdY/gD5/MH9tyREw==</latexit><latexit sha1_base64="0coyYP26hzTYQyo/d27+M3N3DnU=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6MJFwY3LCvaBbSiT6aQdOpmEmRuhhP6FGxeKuPVv3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJre533ni2ohYPeA04X5ER0qEglG00mM/ojgOwiydDao1t+7OQVaJV5AaFGgOql/9YczSiCtkkhrT89wE/YxqFEzyWaWfGp5QNqEj3rNU0YgbP5snnpEzqwxJGGv7FJK5+nsjo5Ex0yiwk3lCs+zl4n9eL8Xw2s+ESlLkii0+ClNJMCb5+WQoNGcop5ZQpoXNStiYasrQllSxJXjLJ6+S9kXdc+ve/WWtcVPUUYYTOIVz8OAKGnAHTWgBAwXP8ApvjnFenHfnYzFacoqdY/gD5/MH9tyREw==</latexit> (f) Deep set
Figure 4: Dierent internal GN block congurations. See Section 3.2 for details on the notation,
and Section 4 for details about each variant. (a) A full GN predicts node, edge, and global output
attributes based on incoming node, edge, and global attributes. (b) An independent, recurrent
update block takes input and hidden graphs, and the functions are RNNs (Sanchez-Gonzalez
et al., 2018). (c) An MPNN (Gilmer et al., 2017) predicts node, edge, and global output attributes
based on incoming node, edge, and global attributes. Note that the global prediction does not
include aggregated edges. (d) A NLNN (Wang et al., 2018c) only predicts node output attributes.
(e) A relation network (Raposo et al., 2017; Santoro et al., 2017) only uses the edge predictions
to predict global attributes. (f) A Deep Set (Zaheer et al., 2017) bypasses the edge update and
predicts updated global attributes.
4.2.1 Message-passing neural network (MPNN)
Gilmer et al. (2017)'s MPNN generalizes a number of previous architectures and can be translated
naturally into the GN formalism. Following the MPNN paper's terminology (see Gilmer et al.
(2017), pages 2-4):
the message function, Mt, plays the role of the GN's e, but does not take uas input,
elementwise summation is used for the GN's e!v,
the update function, Ut, plays the role of the GN's v,
16V<latexit sha1_base64="xc4uzoZiBSxZUZkArgltxczS6nM=">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64="xc4uzoZiBSxZUZkArgltxczS6nM=">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64="xc4uzoZiBSxZUZkArgltxczS6nM=">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64="xc4uzoZiBSxZUZkArgltxczS6nM=">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit>↵e
<latexit sha1_base64="mjs48M94BmT64MLe6tKpQUPkNJM=">AAAB73icbVA9SwNBEJ3zM8avqKXNYRCswp0IWlgEbCwjmA9IzjC3mUuW7O2du3tCCPkTNhaK2Pp37Pw3bpIrNPHBwOO9GWbmhang2njet7Oyura+sVnYKm7v7O7tlw4OGzrJFKM6S0SiWiFqElxS3XAjqJUqwjgU1AyHN1O/+URK80Tem1FKQYx9ySPO0Fip1UGRDvCBuqWyV/FmcJeJn5My5Kh1S1+dXsKymKRhArVu+15qgjEqw5mgSbGTaUqRDbFPbUslxqSD8ezeiXtqlZ4bJcqWNO5M/T0xxljrURzazhjNQC96U/E/r52Z6CoYc5lmhiSbL4oy4ZrEnT7v9rgiZsTIEmSK21tdNkCFzNiIijYEf/HlZdI4r/hexb+7KFev8zgKcAwncAY+XEIVbqEGdWAg4Ble4c15dF6cd+dj3rri5DNH8AfO5w8AXY/p</latexit><latexit sha1_base64="mjs48M94BmT64MLe6tKpQUPkNJM=">AAAB73icbVA9SwNBEJ3zM8avqKXNYRCswp0IWlgEbCwjmA9IzjC3mUuW7O2du3tCCPkTNhaK2Pp37Pw3bpIrNPHBwOO9GWbmhang2njet7Oyura+sVnYKm7v7O7tlw4OGzrJFKM6S0SiWiFqElxS3XAjqJUqwjgU1AyHN1O/+URK80Tem1FKQYx9ySPO0Fip1UGRDvCBuqWyV/FmcJeJn5My5Kh1S1+dXsKymKRhArVu+15qgjEqw5mgSbGTaUqRDbFPbUslxqSD8ezeiXtqlZ4bJcqWNO5M/T0xxljrURzazhjNQC96U/E/r52Z6CoYc5lmhiSbL4oy4ZrEnT7v9rgiZsTIEmSK21tdNkCFzNiIijYEf/HlZdI4r/hexb+7KFev8zgKcAwncAY+XEIVbqEGdWAg4Ble4c15dF6cd+dj3rri5DNH8AfO5w8AXY/p</latexit><latexit sha1_base64="mjs48M94BmT64MLe6tKpQUPkNJM=">AAAB73icbVA9SwNBEJ3zM8avqKXNYRCswp0IWlgEbCwjmA9IzjC3mUuW7O2du3tCCPkTNhaK2Pp37Pw3bpIrNPHBwOO9GWbmhang2njet7Oyura+sVnYKm7v7O7tlw4OGzrJFKM6S0SiWiFqElxS3XAjqJUqwjgU1AyHN1O/+URK80Tem1FKQYx9ySPO0Fip1UGRDvCBuqWyV/FmcJeJn5My5Kh1S1+dXsKymKRhArVu+15qgjEqw5mgSbGTaUqRDbFPbUslxqSD8ezeiXtqlZ4bJcqWNO5M/T0xxljrURzazhjNQC96U/E/r52Z6CoYc5lmhiSbL4oy4ZrEnT7v9rgiZsTIEmSK21tdNkCFzNiIijYEf/HlZdI4r/hexb+7KFev8zgKcAwncAY+XEIVbqEGdWAg4Ble4c15dF6cd+dj3rri5DNH8AfO5w8AXY/p</latexit><latexit sha1_base64="mjs48M94BmT64MLe6tKpQUPkNJM=">AAAB73icbVA9SwNBEJ3zM8avqKXNYRCswp0IWlgEbCwjmA9IzjC3mUuW7O2du3tCCPkTNhaK2Pp37Pw3bpIrNPHBwOO9GWbmhang2njet7Oyura+sVnYKm7v7O7tlw4OGzrJFKM6S0SiWiFqElxS3XAjqJUqwjgU1AyHN1O/+URK80Tem1FKQYx9ySPO0Fip1UGRDvCBuqWyV/FmcJeJn5My5Kh1S1+dXsKymKRhArVu+15qgjEqw5mgSbGTaUqRDbFPbUslxqSD8ezeiXtqlZ4bJcqWNO5M/T0xxljrURzazhjNQC96U/E/r52Z6CoYc5lmhiSbL4oy4ZrEnT7v9rgiZsTIEmSK21tdNkCFzNiIijYEf/HlZdI4r/hexb+7KFev8zgKcAwncAY+XEIVbqEGdWAg4Ble4c15dF6cd+dj3rri5DNH8AfO5w8AXY/p</latexit> e
<latexit sha1_base64="EsSika+7GeuHZg/021QVL5D37Tg=">AAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoAcPAS8eI5gHJGuYnfQmQ2YfzPQKIeQjvHhQxKvf482/cZLsQRMLGoqqbrq7glRJQ6777RTW1jc2t4rbpZ3dvf2D8uFR0ySZFtgQiUp0O+AGlYyxQZIUtlONPAoUtoLR7cxvPaE2MokfaJyiH/FBLEMpOFmp1Q2Q+CP2yhW36s7BVomXkwrkqPfKX91+IrIIYxKKG9Px3JT8CdckhcJpqZsZTLkY8QF2LI15hMafzM+dsjOr9FmYaFsxsbn6e2LCI2PGUWA7I05Ds+zNxP+8TkbhtT+RcZoRxmKxKMwUo4TNfmd9qVGQGlvChZb2ViaGXHNBNqGSDcFbfnmVNC+qnlv17i8rtZs8jiKcwCmcgwdXUIM7qEMDBIzgGV7hzUmdF+fd+Vi0Fpx85hj+wPn8ATfaj3U=</latexit><latexit sha1_base64="EsSika+7GeuHZg/021QVL5D37Tg=">AAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoAcPAS8eI5gHJGuYnfQmQ2YfzPQKIeQjvHhQxKvf482/cZLsQRMLGoqqbrq7glRJQ6777RTW1jc2t4rbpZ3dvf2D8uFR0ySZFtgQiUp0O+AGlYyxQZIUtlONPAoUtoLR7cxvPaE2MokfaJyiH/FBLEMpOFmp1Q2Q+CP2yhW36s7BVomXkwrkqPfKX91+IrIIYxKKG9Px3JT8CdckhcJpqZsZTLkY8QF2LI15hMafzM+dsjOr9FmYaFsxsbn6e2LCI2PGUWA7I05Ds+zNxP+8TkbhtT+RcZoRxmKxKMwUo4TNfmd9qVGQGlvChZb2ViaGXHNBNqGSDcFbfnmVNC+qnlv17i8rtZs8jiKcwCmcgwdXUIM7qEMDBIzgGV7hzUmdF+fd+Vi0Fpx85hj+wPn8ATfaj3U=</latexit><latexit sha1_base64="EsSika+7GeuHZg/021QVL5D37Tg=">AAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoAcPAS8eI5gHJGuYnfQmQ2YfzPQKIeQjvHhQxKvf482/cZLsQRMLGoqqbrq7glRJQ6777RTW1jc2t4rbpZ3dvf2D8uFR0ySZFtgQiUp0O+AGlYyxQZIUtlONPAoUtoLR7cxvPaE2MokfaJyiH/FBLEMpOFmp1Q2Q+CP2yhW36s7BVomXkwrkqPfKX91+IrIIYxKKG9Px3JT8CdckhcJpqZsZTLkY8QF2LI15hMafzM+dsjOr9FmYaFsxsbn6e2LCI2PGUWA7I05Ds+zNxP+8TkbhtT+RcZoRxmKxKMwUo4TNfmd9qVGQGlvChZb2ViaGXHNBNqGSDcFbfnmVNC+qnlv17i8rtZs8jiKcwCmcgwdXUIM7qEMDBIzgGV7hzUmdF+fd+Vi0Fpx85hj+wPn8ATfaj3U=</latexit><latexit sha1_base64="EsSika+7GeuHZg/021QVL5D37Tg=">AAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoAcPAS8eI5gHJGuYnfQmQ2YfzPQKIeQjvHhQxKvf482/cZLsQRMLGoqqbrq7glRJQ6777RTW1jc2t4rbpZ3dvf2D8uFR0ySZFtgQiUp0O+AGlYyxQZIUtlONPAoUtoLR7cxvPaE2MokfaJyiH/FBLEMpOFmp1Q2Q+CP2yhW36s7BVomXkwrkqPfKX91+IrIIYxKKG9Px3JT8CdckhcJpqZsZTLkY8QF2LI15hMafzM+dsjOr9FmYaFsxsbn6e2LCI2PGUWA7I05Ds+zNxP+8TkbhtT+RcZoRxmKxKMwUo4TNfmd9qVGQGlvChZb2ViaGXHNBNqGSDcFbfnmVNC+qnlv17i8rtZs8jiKcwCmcgwdXUIM7qEMDBIzgGV7hzUmdF+fd+Vi0Fpx85hj+wPn8ATfaj3U=</latexit>⇢e!v
<latexit sha1_base64="s3/Cw/iD/Ic9TAit26LWmPV1hK0=">AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit><latexit sha1_base64="s3/Cw/iD/Ic9TAit26LWmPV1hK0=">AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit><latexit sha1_base64="s3/Cw/iD/Ic9TAit26LWmPV1hK0=">AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit><latexit sha1_base64="s3/Cw/iD/Ic9TAit26LWmPV1hK0=">AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit> e
<latexit sha1_base64="gRKFy+QFytmwqWy0cvo5FmmPz8I=">AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubeaSNXu3x+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvDAVXBvX/XaWlldW19YLG8XNre2d3dLevq9lphjWmRRSNUOqUfAE64Ybgc1UIY1DgY1wcDX2G4+oNJfJrRmmGMS0l/CIM2qs5LfTPr/DTqnsVtwJyCLxZqRcPfSf7gGg1il9tbuSZTEmhgmqdctzUxPkVBnOBI6K7UxjStmA9rBlaUJj1EE+uXZETqzSJZFUthJDJurviZzGWg/j0HbG1PT1vDcW//NamYkugpwnaWYwYdNFUSaIkWT8OulyhcyIoSWUKW5vJaxPFWXGBlS0IXjzLy8S/6ziuRXvxqZxCVMU4AiO4RQ8OIcqXEMN6sDgAZ7hFd4c6bw4787HtHXJmc0cwB84nz+QDpCP</latexit><latexit sha1_base64="74MJShuZzxGyM2nLY3EY87InuhI=">AAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA==</latexit><latexit sha1_base64="74MJShuZzxGyM2nLY3EY87InuhI=">AAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA==</latexit><latexit sha1_base64="pLq6KB/1S9uyUeWp/G4byg43mK0=">AAAB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/AF75Ypf9ecgqyTISQVy1Hvlr25fsSxBaZmgxnQCP7XhhGrLmcBpqZsZTCkb0QF2HJU0QRNO5tdOyZlT+iRW2pW0ZK7+npjQxJhxErnOhNqhWfZm4n9eJ7PxVTjhMs0sSrZYFGeCWEVmr5M+18isGDtCmebuVsKGVFNmXUAlF0Kw/PIqaV5UA78a3PmV2nUeRxFO4BTOIYBLqMEt1KEBDB7hGV7hzVPei/fufSxaC14+cwx/4H3+AIWDjww=</latexit><latexit sha1_base64="m8MJ1M94ujO0d0COo5n2Dsol6rc=">AAAB53icbVC7SgNBFL0bXzG+opY2g0GwCrs2phAM2FhGMA9IFpmdvZsMmZ1dZmaFsKS0sbFQxNZP8Rfs/AZ/wsmj0MQDFw7nnMt9BKng2rjul1NYWV1b3yhulra2d3b3yvsHLZ1kimGTJSJRnYBqFFxi03AjsJMqpHEgsB0MryZ++x6V5om8NaMU/Zj2JY84o8ZKjbtyxa26U5Bl4s1J5fLj+4EAgM1/9sKEZTFKwwTVuuu5qfFzqgxnAselXqYxpWxI+9i1VNIYtZ9P9xyTE6uEJEqULWnIVP3dkdNY61Ec2GRMzUAvehPxP6+bmajm51ymmUHJZoOiTBCTkMnRJOQKmREjSyhT3O5K2IAqyox9Tck+wVs8eZm0zqqeW/Vu3Er9AmYowhEcwyl4cA51uIYGNIFBCI/wDC8Od56cV+dtFi04855D+APn/Qd/sI8A</latexit><latexit sha1_base64="q1zM5ZsCNMJAtNUZI97B98ATlaA=">AAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo+1Si4xJrhRmAzUUgjX2DD71+O/MYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL96+7g7fqt81/tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhY/V3R0YjrQeRb5MRNT09643E/7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1+TtE7zZk+dJ/aTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvk+iCM+3Zgz9wXn8AGkeQ8w==</latexit><latexit sha1_base64="q1zM5ZsCNMJAtNUZI97B98ATlaA=">AAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo+1Si4xJrhRmAzUUgjX2DD71+O/MYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL96+7g7fqt81/tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhY/V3R0YjrQeRb5MRNT09643E/7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1+TtE7zZk+dJ/aTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvk+iCM+3Zgz9wXn8AGkeQ8w==</latexit><latexit sha1_base64="ioxb3woZF1oAlTScqds23PrgiMY=">AAAB53icbVC7SgNBFL3rM8ZX1NJmMAhWYdZGC4uAjWUE84BkkdnZ2WTI7Owyc1cIS37AxkIRW3/Jzr9xkmyhiQcGDuecy9x7wkxJi5R+e2vrG5tb25Wd6u7e/sFh7ei4Y9PccNHmqUpNL2RWKKlFGyUq0cuMYEmoRDcc38787pMwVqb6ASeZCBI21DKWnKGTWo+1Om3QOcgq8UtShxIu/zWIUp4nQiNXzNq+TzMMCmZQciWm1UFuRcb4mA1F31HNEmGDYr7nlJw7JSJxatzTSObq74mCJdZOktAlE4Yju+zNxP+8fo7xdVBIneUoNF98FOeKYEpmR5NIGsFRTRxh3Ei3K+EjZhhHV03VleAvn7xKOpcNnzb8e1pv3pR1VOAUzuACfLiCJtxBC9rAIYJneIU3T3ov3rv3sYiueeXMCfyB9/kDCGmMcA==</latexit> v
<latexit sha1_base64="HCiXjOq04H3f4Ed7vqyiRfd+2dI=">AAAB7XicbVA9SwNBEJ2LXzF+nQo2NotBsAp3NlpYBGwsI5hLIDnj3mYvWbO3e+zuRcKR/2BjoYit/8fOf+Pmo9DEBwOP92aYmRelnGnjed9OYWV1bX2juFna2t7Z3XP3DwItM0VonUguVTPCmnImaN0ww2kzVRQnEaeNaHA98RtDqjST4s6MUhomuCdYzAg2VgraaZ/dDztu2at4U6Bl4s9JuXoUPD0AQK3jfrW7kmQJFYZwrHXL91IT5lgZRjgdl9qZpikmA9yjLUsFTqgO8+m1Y3RqlS6KpbIlDJqqvydynGg9SiLbmWDT14veRPzPa2UmvgxzJtLMUEFmi+KMIyPR5HXUZYoSw0eWYKKYvRWRPlaYGBtQyYbgL768TILziu9V/FubxhXMUIRjOIEz8OECqnADNagDgUd4hld4c6Tz4rw7H7PWgjOfOYQ/cD5/AKnSkKA=</latexit><latexit sha1_base64="/voSHBGyFE5xYPVKPYM/GTIarLQ=">AAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQW/NZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMMp+HAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit><latexit sha1_base64="/voSHBGyFE5xYPVKPYM/GTIarLQ=">AAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQW/NZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMMp+HAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit><latexit sha1_base64="Fc8T4ygtia14k1z/CDji4ezWDqY=">AAAB7XicbVA9SwNBEJ3zM8avqKXNYhCswp2NFhYBG8sI5gOSM+xtNsmavd1jdy4QjvwHGwtFbP0/dv4bN8kVmvhg4PHeDDPzokQKi77/7a2tb2xubRd2irt7+weHpaPjhtWpYbzOtNSmFVHLpVC8jgIlbyWG0ziSvBmNbmd+c8yNFVo94CThYUwHSvQFo+ikRicZisdxt1T2K/4cZJUEOSlDjlq39NXpaZbGXCGT1Np24CcYZtSgYJJPi53U8oSyER3wtqOKxtyG2fzaKTl3So/0tXGlkMzV3xMZja2dxJHrjCkO7bI3E//z2in2r8NMqCRFrthiUT+VBDWZvU56wnCGcuIIZUa4WwkbUkMZuoCKLoRg+eVV0risBH4luPfL1Zs8jgKcwhlcQABXUIU7qEEdGDzBM7zCm6e9F+/d+1i0rnn5zAn8gff5A59Hjx0=</latexit>V0
<latexit sha1_base64="gAQ7qdt3IKvK5oBqK3uN1PHYi1k=">AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit><latexit sha1_base64="gAQ7qdt3IKvK5oBqK3uN1PHYi1k=">AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit><latexit sha1_base64="gAQ7qdt3IKvK5oBqK3uN1PHYi1k=">AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit><latexit sha1_base64="gAQ7qdt3IKvK5oBqK3uN1PHYi1k=">AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit>Figure 5: NLNNs as GNs. A schematic showing how NLNNs (Wang et al., 2018c) are implemented
by theeande!vunder the GN framework. Typically, NLNNs assume that dierent regions of
an image (or words in a sentence) correspond to nodes in a fully connected graph, and the attention
mechanism denes a weighted sum over nodes during the aggregation step.
the readout function, R, plays the role of the GN's u, but does not take uorE0as input,
and thus an analog to the GN's e!uis not required;
dmaster serves a roughly similar purpose to the GN's u, but is dened as an extra node
connected to all others, and thus does not inuence the edge and global updates directly. It
can then be represented in the GN's V.
Figure 4c shows how an MPNN is structured, according to the GN framework. For details and
various MPNN architectures, see the Appendix.
4.2.2 Non-local neural networks (NLNN)
Wang et al. (2018c)'s NLNN, which unies various \intra-/self-/vertex-/graph-attention" approaches
(Lin et al., 2017; Vaswani et al., 2017; Hoshen, 2017; Veli ckovi c et al., 2018; Shaw et al., 2018),
can also be translated into the GN formalism. The label \attention" refers to how the nodes are
updated: each node update is based on a weighted sum of (some function of) the node attributes of
its neighbors, where the weight between a node and one of its neighbors is computed by a scalar
pairwise function between their attributes (and then normalized across neighbors). The published
NLNN formalism does not explicitly include edges, and instead computes pairwise attention weights
between all nodes. But various NLNN-compliant models, such as the vertex attention interaction
network (Hoshen, 2017) and graph attention network (Veli ckovi c et al., 2018), are able to handle
explicit edges by eectively setting to zero the weights between nodes which do not share an edge.
As Figures 4d and 5 illustrate, the eis factored into the scalar pairwise-interaction function
which returns the unnormalized attention term, denoted e(vrk;vsk)=a0
k, and a vector-valued
non-pairwise term, denoted e(vsk)=b0
k. In thee!vaggregation, the a0
kterms are normalized
across each receiver's edges, b0
k, and elementwise summed:
e(ek;vrk;vsk;u):=fe(vrk;vsk) = ( e(vrk;vsk); e(vsk)) = (a0
k;b0
k) =e0
k
v e0
i;vi;u:=fv(e0
i)
e!v 
E0
i:=1P
fk:rk=iga0
kX
fk:rk=iga0
kb0
k
In the NLNN paper's terminology (see Wang et al. (2018c), pages 2-4):
theirfplays the role of the above ,
17theirgplays the role of the above .
This formulation may be helpful for focusing only on those interactions which are most relevant for
the downstream task, especially when the input entities were a set, from which a graph was formed
by adding all possible edges between them.
Vaswani et al. (2017)'s multi-headed self-attention mechanism adds an interesting feature, where
theeande!vare implemented by a parallel set of functions, whose results are concatenated
together as the nal step of e!v. This can be interpreted as using typed edges, where the dierent
types index into dierent ecomponent functions, analogous to Li et al. (2016).
For details and various NLNN architectures, see the Appendix.
4.2.3 Other graph network variants
The full GN (Equation 2) can be used to predict a full graph, or any subset of (u0;V0;E0), as
outlined in Section 4.1.1. For example, to predict a global property of a graph, V0andE0can just
be ignored. Similarly, if global, node, or edge attributes are unspecied in the inputs, those vectors
can be zero-length, i.e., not taken as explicit input arguments. The same idea applies for other GN
variants which do not use the full set of mapping ( ) and reduction ( ) functions. For instance,
Interaction Networks (Battaglia et al., 2016; Watters et al., 2017) and the Neural Physics Engine
(Chang et al., 2017) use a full GN but for the absence of the global to update the edge properties
(see Appendix for details).
Various models, including CommNet (Sukhbaatar et al., 2016), structure2vec (Dai et al., 2016)
(in the version of (Dai et al., 2017)), and Gated Graph Sequence Neural Networks (Li et al., 2016)
have used a ewhich does not directly compute pairwise interactions, but instead ignore the receiver
node, operating only on the sender node and in some cases an edge attribute. This can be expressed
by implementations of ewith the following signatures, such as:
e(ek;vrk;vsk;u):=fe(vsk)
ore(ek;vrk;vsk;u):=vsk+fe(ek)
ore(ek;vrk;vsk;u):=fe(ek;vsk):
See the Appendix for further details.
Relation Networks (Raposo et al., 2017; Santoro et al., 2017) bypass the node update entirely
and predict the global output from pooled edge information directly (see also Figure 4e),
e(ek;vrk;vsk;u):=fe(vrk;vsk) = NNe([vrk;vsk])
u 
 e0; v0;u:=fu 
 e0
= NNu 
 e0
e!u 
E0:= =X
ke0
k
Deep Sets (Zaheer et al., 2017) bypass the edges update completely and predict the global output
from pooled nodes information directly (Figure 4f),
v(ei;vi;u):=fv(vi;u) = NNv([vi;u])
u 
 e0; v0;u:=fu 
 v0
= NNu 
 v0
v!u 
V0:= =X
iv0
i
PointNet (Qi et al., 2017) use similar update rule, with a max-aggregation for v!uand a two-step
node update.
18GM
<latexit sha1_base64="vjTCpRgsPEJfhljVzwQb7AFhV5c=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOf+MmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd6+8f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUe+Vv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf+8TmrCSz/jMkkNSjZfFKaCmJhM/yZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuz+v1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5A+mdjYU=</latexit><latexit sha1_base64="vjTCpRgsPEJfhljVzwQb7AFhV5c=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOf+MmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd6+8f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUe+Vv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf+8TmrCSz/jMkkNSjZfFKaCmJhM/yZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuz+v1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5A+mdjYU=</latexit><latexit sha1_base64="vjTCpRgsPEJfhljVzwQb7AFhV5c=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOf+MmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd6+8f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUe+Vv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf+8TmrCSz/jMkkNSjZfFKaCmJhM/yZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuz+v1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5A+mdjYU=</latexit><latexit sha1_base64="vjTCpRgsPEJfhljVzwQb7AFhV5c=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOf+MmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd6+8f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUe+Vv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf+8TmrCSz/jMkkNSjZfFKaCmJhM/yZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuz+v1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5A+mdjYU=</latexit>GN1
<latexit sha1_base64="oAmr7/S238q10w2wEvXkfEGmAr8=">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIUBcuCi50JRXsA9qhZNJMG5pkxiRTKEO/w40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3md+eUKVZJB/NNKa+wEPJQkawsZLfE9iMlEhv72d9r1+uuFV3DrRKvJxUIEejX/7qDSKSCCoN4VjrrufGxk+xMoxwOiv1Ek1jTMZ4SLuWSiyo9tN56Bk6s8oAhZGyTxo0V39vpFhoPRWBncxC6mUvE//zuokJr/yUyTgxVJLFoTDhyEQoawANmKLE8KklmChmsyIywgoTY3sq2RK85S+vktZF1XOr3sNlpX6d11GEEziFc/CgBnW4gwY0gcATPMMrvDkT58V5dz4WowUn3zmGP3A+fwCg3ZH4</latexit><latexit sha1_base64="oAmr7/S238q10w2wEvXkfEGmAr8=">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIUBcuCi50JRXsA9qhZNJMG5pkxiRTKEO/w40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3md+eUKVZJB/NNKa+wEPJQkawsZLfE9iMlEhv72d9r1+uuFV3DrRKvJxUIEejX/7qDSKSCCoN4VjrrufGxk+xMoxwOiv1Ek1jTMZ4SLuWSiyo9tN56Bk6s8oAhZGyTxo0V39vpFhoPRWBncxC6mUvE//zuokJr/yUyTgxVJLFoTDhyEQoawANmKLE8KklmChmsyIywgoTY3sq2RK85S+vktZF1XOr3sNlpX6d11GEEziFc/CgBnW4gwY0gcATPMMrvDkT58V5dz4WowUn3zmGP3A+fwCg3ZH4</latexit><latexit sha1_base64="oAmr7/S238q10w2wEvXkfEGmAr8=">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIUBcuCi50JRXsA9qhZNJMG5pkxiRTKEO/w40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3md+eUKVZJB/NNKa+wEPJQkawsZLfE9iMlEhv72d9r1+uuFV3DrRKvJxUIEejX/7qDSKSCCoN4VjrrufGxk+xMoxwOiv1Ek1jTMZ4SLuWSiyo9tN56Bk6s8oAhZGyTxo0V39vpFhoPRWBncxC6mUvE//zuokJr/yUyTgxVJLFoTDhyEQoawANmKLE8KklmChmsyIywgoTY3sq2RK85S+vktZF1XOr3sNlpX6d11GEEziFc/CgBnW4gwY0gcATPMMrvDkT58V5dz4WowUn3zmGP3A+fwCg3ZH4</latexit><latexit sha1_base64="oAmr7/S238q10w2wEvXkfEGmAr8=">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIUBcuCi50JRXsA9qhZNJMG5pkxiRTKEO/w40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3md+eUKVZJB/NNKa+wEPJQkawsZLfE9iMlEhv72d9r1+uuFV3DrRKvJxUIEejX/7qDSKSCCoN4VjrrufGxk+xMoxwOiv1Ek1jTMZ4SLuWSiyo9tN56Bk6s8oAhZGyTxo0V39vpFhoPRWBncxC6mUvE//zuokJr/yUyTgxVJLFoTDhyEQoawANmKLE8KklmChmsyIywgoTY3sq2RK85S+vktZF1XOr3sNlpX6d11GEEziFc/CgBnW4gwY0gcATPMMrvDkT58V5dz4WowUn3zmGP3A+fwCg3ZH4</latexit>GN2
<latexit sha1_base64="pet508CCa1uIZM8cv8xqNGylB9w=">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqswUQRcuCi50JRXsA9qhZNJMG5pkxiRTKEO/w40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3md+eUKVZJB/NNKa+wEPJQkawsZLfE9iMlEhv72f9Wr9ccavuHGiVeDmpQI5Gv/zVG0QkEVQawrHWXc+NjZ9iZRjhdFbqJZrGmIzxkHYtlVhQ7afz0DN0ZpUBCiNlnzRorv7eSLHQeioCO5mF1MteJv7ndRMTXvkpk3FiqCSLQ2HCkYlQ1gAaMEWJ4VNLMFHMZkVkhBUmxvZUsiV4y19eJa1a1XOr3sNFpX6d11GEEziFc/DgEupwBw1oAoEneIZXeHMmzovz7nwsRgtOvnMMf+B8/gCiYZH5</latexit><latexit sha1_base64="pet508CCa1uIZM8cv8xqNGylB9w=">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqswUQRcuCi50JRXsA9qhZNJMG5pkxiRTKEO/w40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3md+eUKVZJB/NNKa+wEPJQkawsZLfE9iMlEhv72f9Wr9ccavuHGiVeDmpQI5Gv/zVG0QkEVQawrHWXc+NjZ9iZRjhdFbqJZrGmIzxkHYtlVhQ7afz0DN0ZpUBCiNlnzRorv7eSLHQeioCO5mF1MteJv7ndRMTXvkpk3FiqCSLQ2HCkYlQ1gAaMEWJ4VNLMFHMZkVkhBUmxvZUsiV4y19eJa1a1XOr3sNFpX6d11GEEziFc/DgEupwBw1oAoEneIZXeHMmzovz7nwsRgtOvnMMf+B8/gCiYZH5</latexit><latexit sha1_base64="pet508CCa1uIZM8cv8xqNGylB9w=">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqswUQRcuCi50JRXsA9qhZNJMG5pkxiRTKEO/w40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3md+eUKVZJB/NNKa+wEPJQkawsZLfE9iMlEhv72f9Wr9ccavuHGiVeDmpQI5Gv/zVG0QkEVQawrHWXc+NjZ9iZRjhdFbqJZrGmIzxkHYtlVhQ7afz0DN0ZpUBCiNlnzRorv7eSLHQeioCO5mF1MteJv7ndRMTXvkpk3FiqCSLQ2HCkYlQ1gAaMEWJ4VNLMFHMZkVkhBUmxvZUsiV4y19eJa1a1XOr3sNFpX6d11GEEziFc/DgEupwBw1oAoEneIZXeHMmzovz7nwsRgtOvnMMf+B8/gCiYZH5</latexit><latexit sha1_base64="pet508CCa1uIZM8cv8xqNGylB9w=">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqswUQRcuCi50JRXsA9qhZNJMG5pkxiRTKEO/w40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3md+eUKVZJB/NNKa+wEPJQkawsZLfE9iMlEhv72f9Wr9ccavuHGiVeDmpQI5Gv/zVG0QkEVQawrHWXc+NjZ9iZRjhdFbqJZrGmIzxkHYtlVhQ7afz0DN0ZpUBCiNlnzRorv7eSLHQeioCO5mF1MteJv7ndRMTXvkpk3FiqCSLQ2HCkYlQ1gAaMEWJ4VNLMFHMZkVkhBUmxvZUsiV4y19eJa1a1XOr3sNFpX6d11GEEziFc/DgEupwBw1oAoEneIZXeHMmzovz7nwsRgtOvnMMf+B8/gCiYZH5</latexit>GNM
<latexit sha1_base64="1uUQuLnXmq2FrQq5fvsHBbzm7v8=">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoAsXBRe6USrYB7RDyaSZNjTJjEmmUIZ+hxsXirj1Y9z5N2baWWjrgcDhnHu5JyeIOdPGdb+dwsrq2vpGcbO0tb2zu1feP2jqKFGENkjEI9UOsKacSdowzHDajhXFIuC0FYyuM781pkqzSD6aSUx9gQeShYxgYyW/K7AZKpHe3E97d71yxa26M6Bl4uWkAjnqvfJXtx+RRFBpCMdadzw3Nn6KlWGE02mpm2gaYzLCA9qxVGJBtZ/OQk/RiVX6KIyUfdKgmfp7I8VC64kI7GQWUi96mfif10lMeOmnTMaJoZLMD4UJRyZCWQOozxQlhk8swUQxmxWRIVaYGNtTyZbgLX55mTTPqp5b9R7OK7WrvI4iHMExnIIHF1CDW6hDAwg8wTO8wpszdl6cd+djPlpw8p1D+APn8wfLTZIU</latexit><latexit sha1_base64="1uUQuLnXmq2FrQq5fvsHBbzm7v8=">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoAsXBRe6USrYB7RDyaSZNjTJjEmmUIZ+hxsXirj1Y9z5N2baWWjrgcDhnHu5JyeIOdPGdb+dwsrq2vpGcbO0tb2zu1feP2jqKFGENkjEI9UOsKacSdowzHDajhXFIuC0FYyuM781pkqzSD6aSUx9gQeShYxgYyW/K7AZKpHe3E97d71yxa26M6Bl4uWkAjnqvfJXtx+RRFBpCMdadzw3Nn6KlWGE02mpm2gaYzLCA9qxVGJBtZ/OQk/RiVX6KIyUfdKgmfp7I8VC64kI7GQWUi96mfif10lMeOmnTMaJoZLMD4UJRyZCWQOozxQlhk8swUQxmxWRIVaYGNtTyZbgLX55mTTPqp5b9R7OK7WrvI4iHMExnIIHF1CDW6hDAwg8wTO8wpszdl6cd+djPlpw8p1D+APn8wfLTZIU</latexit><latexit sha1_base64="1uUQuLnXmq2FrQq5fvsHBbzm7v8=">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoAsXBRe6USrYB7RDyaSZNjTJjEmmUIZ+hxsXirj1Y9z5N2baWWjrgcDhnHu5JyeIOdPGdb+dwsrq2vpGcbO0tb2zu1feP2jqKFGENkjEI9UOsKacSdowzHDajhXFIuC0FYyuM781pkqzSD6aSUx9gQeShYxgYyW/K7AZKpHe3E97d71yxa26M6Bl4uWkAjnqvfJXtx+RRFBpCMdadzw3Nn6KlWGE02mpm2gaYzLCA9qxVGJBtZ/OQk/RiVX6KIyUfdKgmfp7I8VC64kI7GQWUi96mfif10lMeOmnTMaJoZLMD4UJRyZCWQOozxQlhk8swUQxmxWRIVaYGNtTyZbgLX55mTTPqp5b9R7OK7WrvI4iHMExnIIHF1CDW6hDAwg8wTO8wpszdl6cd+djPlpw8p1D+APn8wfLTZIU</latexit><latexit sha1_base64="1uUQuLnXmq2FrQq5fvsHBbzm7v8=">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoAsXBRe6USrYB7RDyaSZNjTJjEmmUIZ+hxsXirj1Y9z5N2baWWjrgcDhnHu5JyeIOdPGdb+dwsrq2vpGcbO0tb2zu1feP2jqKFGENkjEI9UOsKacSdowzHDajhXFIuC0FYyuM781pkqzSD6aSUx9gQeShYxgYyW/K7AZKpHe3E97d71yxa26M6Bl4uWkAjnqvfJXtx+RRFBpCMdadzw3Nn6KlWGE02mpm2gaYzLCA9qxVGJBtZ/OQk/RiVX6KIyUfdKgmfp7I8VC64kI7GQWUi96mfif10lMeOmnTMaJoZLMD4UJRyZCWQOozxQlhk8swUQxmxWRIVaYGNtTyZbgLX55mTTPqp5b9R7OK7WrvI4iHMExnIIHF1CDW6hDAwg8wTO8wpszdl6cd+djPlpw8p1D+APn8wfLTZIU</latexit>...<latexit sha1_base64="Gj7yv98SlyD93Ghofp+NnyXvd2c=">AAAB7HicbVBNS8NAFHypX7V+VT16WSyCp5KIUA8eCl48VjBtoQ1ls920SzebsPsilNDf4MWDIl79Qd78N27bHLR1YGGYecO+N2EqhUHX/XZKG5tb2zvl3cre/sHhUfX4pG2STDPus0QmuhtSw6VQ3EeBkndTzWkcSt4JJ3dzv/PEtRGJesRpyoOYjpSIBKNoJb8/TNAMqjW37i5A1olXkBoUaA2qXzbHspgrZJIa0/PcFIOcahRM8lmlnxmeUjahI96zVNGYmyBfLDsjF1YZkijR9ikkC/V3IqexMdM4tJMxxbFZ9ebif14vw+gmyIVKM+SKLT+KMkkwIfPLyVBozlBOLaFMC7srYWOqKUPbT8WW4K2evE7aV3XPrXsP17XmbVFHGc7gHC7BgwY04R5a4AMDAc/wCm+Ocl6cd+djOVpyiswp/IHz+QPvmo68</latexit><latexit sha1_base64="Gj7yv98SlyD93Ghofp+NnyXvd2c=">AAAB7HicbVBNS8NAFHypX7V+VT16WSyCp5KIUA8eCl48VjBtoQ1ls920SzebsPsilNDf4MWDIl79Qd78N27bHLR1YGGYecO+N2EqhUHX/XZKG5tb2zvl3cre/sHhUfX4pG2STDPus0QmuhtSw6VQ3EeBkndTzWkcSt4JJ3dzv/PEtRGJesRpyoOYjpSIBKNoJb8/TNAMqjW37i5A1olXkBoUaA2qXzbHspgrZJIa0/PcFIOcahRM8lmlnxmeUjahI96zVNGYmyBfLDsjF1YZkijR9ikkC/V3IqexMdM4tJMxxbFZ9ebif14vw+gmyIVKM+SKLT+KMkkwIfPLyVBozlBOLaFMC7srYWOqKUPbT8WW4K2evE7aV3XPrXsP17XmbVFHGc7gHC7BgwY04R5a4AMDAc/wCm+Ocl6cd+djOVpyiswp/IHz+QPvmo68</latexit><latexit sha1_base64="Gj7yv98SlyD93Ghofp+NnyXvd2c=">AAAB7HicbVBNS8NAFHypX7V+VT16WSyCp5KIUA8eCl48VjBtoQ1ls920SzebsPsilNDf4MWDIl79Qd78N27bHLR1YGGYecO+N2EqhUHX/XZKG5tb2zvl3cre/sHhUfX4pG2STDPus0QmuhtSw6VQ3EeBkndTzWkcSt4JJ3dzv/PEtRGJesRpyoOYjpSIBKNoJb8/TNAMqjW37i5A1olXkBoUaA2qXzbHspgrZJIa0/PcFIOcahRM8lmlnxmeUjahI96zVNGYmyBfLDsjF1YZkijR9ikkC/V3IqexMdM4tJMxxbFZ9ebif14vw+gmyIVKM+SKLT+KMkkwIfPLyVBozlBOLaFMC7srYWOqKUPbT8WW4K2evE7aV3XPrXsP17XmbVFHGc7gHC7BgwY04R5a4AMDAc/wCm+Ocl6cd+djOVpyiswp/IHz+QPvmo68</latexit><latexit sha1_base64="Gj7yv98SlyD93Ghofp+NnyXvd2c=">AAAB7HicbVBNS8NAFHypX7V+VT16WSyCp5KIUA8eCl48VjBtoQ1ls920SzebsPsilNDf4MWDIl79Qd78N27bHLR1YGGYecO+N2EqhUHX/XZKG5tb2zvl3cre/sHhUfX4pG2STDPus0QmuhtSw6VQ3EeBkndTzWkcSt4JJ3dzv/PEtRGJesRpyoOYjpSIBKNoJb8/TNAMqjW37i5A1olXkBoUaA2qXzbHspgrZJIa0/PcFIOcahRM8lmlnxmeUjahI96zVNGYmyBfLDsjF1YZkijR9ikkC/V3IqexMdM4tJMxxbFZ9ebif14vw+gmyIVKM+SKLT+KMkkwIfPLyVBozlBOLaFMC7srYWOqKUPbT8WW4K2evE7aV3XPrXsP17XmbVFHGc7gHC7BgwY04R5a4AMDAc/wCm+Ocl6cd+djOVpyiswp/IHz+QPvmo68</latexit>G1
<latexit sha1_base64="YNShseMoKm2HdChKvcjMRmoBu5o=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9r1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvy2NaQ==</latexit><latexit sha1_base64="YNShseMoKm2HdChKvcjMRmoBu5o=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9r1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvy2NaQ==</latexit><latexit sha1_base64="YNShseMoKm2HdChKvcjMRmoBu5o=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9r1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvy2NaQ==</latexit><latexit sha1_base64="YNShseMoKm2HdChKvcjMRmoBu5o=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9r1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvy2NaQ==</latexit>G0
<latexit sha1_base64="vj48jMMQe2f55rU6zb6RZp+K9y4=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9t1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvamNaA==</latexit><latexit sha1_base64="vj48jMMQe2f55rU6zb6RZp+K9y4=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9t1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvamNaA==</latexit><latexit sha1_base64="vj48jMMQe2f55rU6zb6RZp+K9y4=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9t1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvamNaA==</latexit><latexit sha1_base64="vj48jMMQe2f55rU6zb6RZp+K9y4=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9t1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvamNaA==</latexit>GM
<latexit sha1_base64="vjTCpRgsPEJfhljVzwQb7AFhV5c=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOf+MmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd6+8f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUe+Vv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf+8TmrCSz/jMkkNSjZfFKaCmJhM/yZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuz+v1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5A+mdjYU=</latexit><latexit sha1_base64="vjTCpRgsPEJfhljVzwQb7AFhV5c=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOf+MmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd6+8f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUe+Vv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf+8TmrCSz/jMkkNSjZfFKaCmJhM/yZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuz+v1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5A+mdjYU=</latexit><latexit sha1_base64="vjTCpRgsPEJfhljVzwQb7AFhV5c=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOf+MmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd6+8f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUe+Vv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf+8TmrCSz/jMkkNSjZfFKaCmJhM/yZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuz+v1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5A+mdjYU=</latexit><latexit sha1_base64="vjTCpRgsPEJfhljVzwQb7AFhV5c=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOf+MmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd6+8f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUe+Vv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf+8TmrCSz/jMkkNSjZfFKaCmJhM/yZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuz+v1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5A+mdjYU=</latexit>G0
<latexit sha1_base64="vj48jMMQe2f55rU6zb6RZp+K9y4=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9t1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvamNaA==</latexit><latexit sha1_base64="vj48jMMQe2f55rU6zb6RZp+K9y4=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9t1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvamNaA==</latexit><latexit sha1_base64="vj48jMMQe2f55rU6zb6RZp+K9y4=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9t1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvamNaA==</latexit><latexit sha1_base64="vj48jMMQe2f55rU6zb6RZp+K9y4=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9t1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvamNaA==</latexit>GNcore
<latexit sha1_base64="sfcetjjriA53KVhP8LRkSGs9KNA=">AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit><latexit sha1_base64="sfcetjjriA53KVhP8LRkSGs9KNA=">AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit><latexit sha1_base64="sfcetjjriA53KVhP8LRkSGs9KNA=">AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit><latexit sha1_base64="sfcetjjriA53KVhP8LRkSGs9KNA=">AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit>⇥M
<latexit sha1_base64="xCEPSgjeJaAOppNxwTZXrwRukIg=">AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit><latexit sha1_base64="xCEPSgjeJaAOppNxwTZXrwRukIg=">AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit><latexit sha1_base64="xCEPSgjeJaAOppNxwTZXrwRukIg=">AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit><latexit sha1_base64="xCEPSgjeJaAOppNxwTZXrwRukIg=">AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit>(a) Composition of GN blocks
GNenc
<latexit sha1_base64="KZY5NxgXVEC/q8QikcRvAbEAxBs=">AAAB+nicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6/cRKz0NYDA4dz7mXOPWHMqNKu+2UtLa+srq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3M/e49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfc+NtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De/2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa9/4wuWeXOPvyB9fENyBqUTg==</latexit><latexit sha1_base64="KZY5NxgXVEC/q8QikcRvAbEAxBs=">AAAB+nicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6/cRKz0NYDA4dz7mXOPWHMqNKu+2UtLa+srq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3M/e49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfc+NtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De/2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa9/4wuWeXOPvyB9fENyBqUTg==</latexit><latexit sha1_base64="KZY5NxgXVEC/q8QikcRvAbEAxBs=">AAAB+nicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6/cRKz0NYDA4dz7mXOPWHMqNKu+2UtLa+srq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3M/e49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfc+NtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De/2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa9/4wuWeXOPvyB9fENyBqUTg==</latexit><latexit sha1_base64="KZY5NxgXVEC/q8QikcRvAbEAxBs=">AAAB+nicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6/cRKz0NYDA4dz7mXOPWHMqNKu+2UtLa+srq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3M/e49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfc+NtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De/2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa9/4wuWeXOPvyB9fENyBqUTg==</latexit>GNdec
<latexit sha1_base64="79QHnx4t/4kSfeuqQfRiz+zDMfI=">AAAB+nicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79Enf+jZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr+10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk+5p03Wa7t1Zo3VZ1lEFh+AInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQB/YH3+ALjdlEQ=</latexit><latexit sha1_base64="79QHnx4t/4kSfeuqQfRiz+zDMfI=">AAAB+nicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79Enf+jZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr+10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk+5p03Wa7t1Zo3VZ1lEFh+AInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQB/YH3+ALjdlEQ=</latexit><latexit sha1_base64="79QHnx4t/4kSfeuqQfRiz+zDMfI=">AAAB+nicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79Enf+jZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr+10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk+5p03Wa7t1Zo3VZ1lEFh+AInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQB/YH3+ALjdlEQ=</latexit><latexit sha1_base64="79QHnx4t/4kSfeuqQfRiz+zDMfI=">AAAB+nicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79Enf+jZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr+10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk+5p03Wa7t1Zo3VZ1lEFh+AInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQB/YH3+ALjdlEQ=</latexit>GNcore
<latexit sha1_base64="sfcetjjriA53KVhP8LRkSGs9KNA=">AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit><latexit sha1_base64="sfcetjjriA53KVhP8LRkSGs9KNA=">AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit><latexit sha1_base64="sfcetjjriA53KVhP8LRkSGs9KNA=">AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit><latexit sha1_base64="sfcetjjriA53KVhP8LRkSGs9KNA=">AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit>⇥M
<latexit sha1_base64="xCEPSgjeJaAOppNxwTZXrwRukIg=">AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit><latexit sha1_base64="xCEPSgjeJaAOppNxwTZXrwRukIg=">AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit><latexit sha1_base64="xCEPSgjeJaAOppNxwTZXrwRukIg=">AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit><latexit sha1_base64="xCEPSgjeJaAOppNxwTZXrwRukIg=">AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit>Gout
<latexit sha1_base64="TKn8tu9S9/KYM9INqkcELhgYcuA=">AAAB+XicbVDLSsNAFJ3UV62vqEs3wSK4KokIunBRcKHLCvYBbQiT6aQdOo8wc1MooX/ixoUibv0Td/6NkzYLrR4YOJxzL/fMiVPODPj+l1NZW9/Y3Kpu13Z29/YP3MOjjlGZJrRNFFe6F2NDOZO0DQw47aWaYhFz2o0nt4XfnVJtmJKPMEtpKPBIsoQRDFaKXPcuygcCw1iLXGUwn0du3W/4C3h/SVCSOirRitzPwVCRTFAJhGNj+oGfQphjDYxwOq8NMkNTTCZ4RPuWSiyoCfNF8rl3ZpWhlyhtnwRvof7cyLEwZiZiO1mENKteIf7n9TNIrsOcyTQDKsnyUJJxD5RX1OANmaYE+MwSTDSzWT0yxhoTsGXVbAnB6pf/ks5FI/AbwcNlvXlT1lFFJ+gUnaMAXaEmukct1EYETdETekGvTu48O2/O+3K04pQ7x+gXnI9vWWaUGA==</latexit><latexit sha1_base64="TKn8tu9S9/KYM9INqkcELhgYcuA=">AAAB+XicbVDLSsNAFJ3UV62vqEs3wSK4KokIunBRcKHLCvYBbQiT6aQdOo8wc1MooX/ixoUibv0Td/6NkzYLrR4YOJxzL/fMiVPODPj+l1NZW9/Y3Kpu13Z29/YP3MOjjlGZJrRNFFe6F2NDOZO0DQw47aWaYhFz2o0nt4XfnVJtmJKPMEtpKPBIsoQRDFaKXPcuygcCw1iLXGUwn0du3W/4C3h/SVCSOirRitzPwVCRTFAJhGNj+oGfQphjDYxwOq8NMkNTTCZ4RPuWSiyoCfNF8rl3ZpWhlyhtnwRvof7cyLEwZiZiO1mENKteIf7n9TNIrsOcyTQDKsnyUJJxD5RX1OANmaYE+MwSTDSzWT0yxhoTsGXVbAnB6pf/ks5FI/AbwcNlvXlT1lFFJ+gUnaMAXaEmukct1EYETdETekGvTu48O2/O+3K04pQ7x+gXnI9vWWaUGA==</latexit><latexit sha1_base64="TKn8tu9S9/KYM9INqkcELhgYcuA=">AAAB+XicbVDLSsNAFJ3UV62vqEs3wSK4KokIunBRcKHLCvYBbQiT6aQdOo8wc1MooX/ixoUibv0Td/6NkzYLrR4YOJxzL/fMiVPODPj+l1NZW9/Y3Kpu13Z29/YP3MOjjlGZJrRNFFe6F2NDOZO0DQw47aWaYhFz2o0nt4XfnVJtmJKPMEtpKPBIsoQRDFaKXPcuygcCw1iLXGUwn0du3W/4C3h/SVCSOirRitzPwVCRTFAJhGNj+oGfQphjDYxwOq8NMkNTTCZ4RPuWSiyoCfNF8rl3ZpWhlyhtnwRvof7cyLEwZiZiO1mENKteIf7n9TNIrsOcyTQDKsnyUJJxD5RX1OANmaYE+MwSTDSzWT0yxhoTsGXVbAnB6pf/ks5FI/AbwcNlvXlT1lFFJ+gUnaMAXaEmukct1EYETdETekGvTu48O2/O+3K04pQ7x+gXnI9vWWaUGA==</latexit><latexit sha1_base64="TKn8tu9S9/KYM9INqkcELhgYcuA=">AAAB+XicbVDLSsNAFJ3UV62vqEs3wSK4KokIunBRcKHLCvYBbQiT6aQdOo8wc1MooX/ixoUibv0Td/6NkzYLrR4YOJxzL/fMiVPODPj+l1NZW9/Y3Kpu13Z29/YP3MOjjlGZJrRNFFe6F2NDOZO0DQw47aWaYhFz2o0nt4XfnVJtmJKPMEtpKPBIsoQRDFaKXPcuygcCw1iLXGUwn0du3W/4C3h/SVCSOirRitzPwVCRTFAJhGNj+oGfQphjDYxwOq8NMkNTTCZ4RPuWSiyoCfNF8rl3ZpWhlyhtnwRvof7cyLEwZiZiO1mENKteIf7n9TNIrsOcyTQDKsnyUJJxD5RX1OANmaYE+MwSTDSzWT0yxhoTsGXVbAnB6pf/ks5FI/AbwcNlvXlT1lFFJ+gUnaMAXaEmukct1EYETdETekGvTu48O2/O+3K04pQ7x+gXnI9vWWaUGA==</latexit>Ginp
<latexit sha1_base64="mvYjY6mgtt6w2Efm7YMP3XaOQ7I=">AAAB+XicbVDLSgMxFL1TX7W+Rl26CRbBVZkRQRcuCi50WcE+oB2GTJppQ5PMkGQKZeifuHGhiFv/xJ1/Y6adhVYPBA7n3EvOPVHKmTae9+VU1tY3Nreq27Wd3b39A/fwqKOTTBHaJglPVC/CmnImadsww2kvVRSLiNNuNLkt/O6UKs0S+WhmKQ0EHkkWM4KNlULXvQvzgcBmrETOZDqfh27da3gLoL/EL0kdSrRC93MwTEgmqDSEY637vpeaIMfKMMLpvDbINE0xmeAR7VsqsaA6yBfJ5+jMKkMUJ8o+adBC/bmRY6H1TER2sgipV71C/M/rZya+DoqLMkMlWX4UZxyZBBU1oCFTlBg+swQTxWxWRMZYYWJsWTVbgr968l/SuWj4XsN/uKw3b8o6qnACp3AOPlxBE+6hBW0gMIUneIFXJ3eenTfnfTlaccqdY/gF5+MbP22UBw==</latexit><latexit sha1_base64="mvYjY6mgtt6w2Efm7YMP3XaOQ7I=">AAAB+XicbVDLSgMxFL1TX7W+Rl26CRbBVZkRQRcuCi50WcE+oB2GTJppQ5PMkGQKZeifuHGhiFv/xJ1/Y6adhVYPBA7n3EvOPVHKmTae9+VU1tY3Nreq27Wd3b39A/fwqKOTTBHaJglPVC/CmnImadsww2kvVRSLiNNuNLkt/O6UKs0S+WhmKQ0EHkkWM4KNlULXvQvzgcBmrETOZDqfh27da3gLoL/EL0kdSrRC93MwTEgmqDSEY637vpeaIMfKMMLpvDbINE0xmeAR7VsqsaA6yBfJ5+jMKkMUJ8o+adBC/bmRY6H1TER2sgipV71C/M/rZya+DoqLMkMlWX4UZxyZBBU1oCFTlBg+swQTxWxWRMZYYWJsWTVbgr968l/SuWj4XsN/uKw3b8o6qnACp3AOPlxBE+6hBW0gMIUneIFXJ3eenTfnfTlaccqdY/gF5+MbP22UBw==</latexit><latexit sha1_base64="mvYjY6mgtt6w2Efm7YMP3XaOQ7I=">AAAB+XicbVDLSgMxFL1TX7W+Rl26CRbBVZkRQRcuCi50WcE+oB2GTJppQ5PMkGQKZeifuHGhiFv/xJ1/Y6adhVYPBA7n3EvOPVHKmTae9+VU1tY3Nreq27Wd3b39A/fwqKOTTBHaJglPVC/CmnImadsww2kvVRSLiNNuNLkt/O6UKs0S+WhmKQ0EHkkWM4KNlULXvQvzgcBmrETOZDqfh27da3gLoL/EL0kdSrRC93MwTEgmqDSEY637vpeaIMfKMMLpvDbINE0xmeAR7VsqsaA6yBfJ5+jMKkMUJ8o+adBC/bmRY6H1TER2sgipV71C/M/rZya+DoqLMkMlWX4UZxyZBBU1oCFTlBg+swQTxWxWRMZYYWJsWTVbgr968l/SuWj4XsN/uKw3b8o6qnACp3AOPlxBE+6hBW0gMIUneIFXJ3eenTfnfTlaccqdY/gF5+MbP22UBw==</latexit><latexit sha1_base64="mvYjY6mgtt6w2Efm7YMP3XaOQ7I=">AAAB+XicbVDLSgMxFL1TX7W+Rl26CRbBVZkRQRcuCi50WcE+oB2GTJppQ5PMkGQKZeifuHGhiFv/xJ1/Y6adhVYPBA7n3EvOPVHKmTae9+VU1tY3Nreq27Wd3b39A/fwqKOTTBHaJglPVC/CmnImadsww2kvVRSLiNNuNLkt/O6UKs0S+WhmKQ0EHkkWM4KNlULXvQvzgcBmrETOZDqfh27da3gLoL/EL0kdSrRC93MwTEgmqDSEY637vpeaIMfKMMLpvDbINE0xmeAR7VsqsaA6yBfJ5+jMKkMUJ8o+adBC/bmRY6H1TER2sgipV71C/M/rZya+DoqLMkMlWX4UZxyZBBU1oCFTlBg+swQTxWxWRMZYYWJsWTVbgr968l/SuWj4XsN/uKw3b8o6qnACp3AOPlxBE+6hBW0gMIUneIFXJ3eenTfnfTlaccqdY/gF5+MbP22UBw==</latexit> (b) Encode-process-decode
GNenc
<latexit sha1_base64="KZY5NxgXVEC/q8QikcRvAbEAxBs=">AAAB+nicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6/cRKz0NYDA4dz7mXOPWHMqNKu+2UtLa+srq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3M/e49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfc+NtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De/2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa9/4wuWeXOPvyB9fENyBqUTg==</latexit><latexit sha1_base64="KZY5NxgXVEC/q8QikcRvAbEAxBs=">AAAB+nicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6/cRKz0NYDA4dz7mXOPWHMqNKu+2UtLa+srq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3M/e49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfc+NtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De/2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa9/4wuWeXOPvyB9fENyBqUTg==</latexit><latexit sha1_base64="KZY5NxgXVEC/q8QikcRvAbEAxBs=">AAAB+nicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6/cRKz0NYDA4dz7mXOPWHMqNKu+2UtLa+srq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3M/e49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfc+NtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De/2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa9/4wuWeXOPvyB9fENyBqUTg==</latexit><latexit sha1_base64="KZY5NxgXVEC/q8QikcRvAbEAxBs=">AAAB+nicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6/cRKz0NYDA4dz7mXOPWHMqNKu+2UtLa+srq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3M/e49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfc+NtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De/2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa9/4wuWeXOPvyB9fENyBqUTg==</latexit>GNdec
<latexit sha1_base64="79QHnx4t/4kSfeuqQfRiz+zDMfI=">AAAB+nicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79Enf+jZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr+10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk+5p03Wa7t1Zo3VZ1lEFh+AInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQB/YH3+ALjdlEQ=</latexit><latexit sha1_base64="79QHnx4t/4kSfeuqQfRiz+zDMfI=">AAAB+nicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79Enf+jZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr+10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk+5p03Wa7t1Zo3VZ1lEFh+AInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQB/YH3+ALjdlEQ=</latexit><latexit sha1_base64="79QHnx4t/4kSfeuqQfRiz+zDMfI=">AAAB+nicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79Enf+jZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr+10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk+5p03Wa7t1Zo3VZ1lEFh+AInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQB/YH3+ALjdlEQ=</latexit><latexit sha1_base64="79QHnx4t/4kSfeuqQfRiz+zDMfI=">AAAB+nicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79Enf+jZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr+10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk+5p03Wa7t1Zo3VZ1lEFh+AInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQB/YH3+ALjdlEQ=</latexit>GNcore
<latexit sha1_base64="sfcetjjriA53KVhP8LRkSGs9KNA=">AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit><latexit sha1_base64="sfcetjjriA53KVhP8LRkSGs9KNA=">AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit><latexit sha1_base64="sfcetjjriA53KVhP8LRkSGs9KNA=">AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit><latexit sha1_base64="sfcetjjriA53KVhP8LRkSGs9KNA=">AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit>⇥M
<latexit sha1_base64="xCEPSgjeJaAOppNxwTZXrwRukIg=">AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit><latexit sha1_base64="xCEPSgjeJaAOppNxwTZXrwRukIg=">AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit><latexit sha1_base64="xCEPSgjeJaAOppNxwTZXrwRukIg=">AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit><latexit sha1_base64="xCEPSgjeJaAOppNxwTZXrwRukIg=">AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit>Gthid
<latexit sha1_base64="Vr1kZJh7jApVU4cGmtPimK4PPic=">AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIoAsXBRe6rGAf0MYwmUzaoTOTMDMRagj+ihsXirj1P9z5N07aLLT1wMDhnHu5Z06QMKq043xblaXlldW16nptY3Nre8fe3euoOJWYtHHMYtkLkCKMCtLWVDPSSyRBPGCkG4yvCr/7QKSisbjTk4R4HA0FjShG2ki+fXB9n+nczwYc6ZHk2YiGee7bdafhTAEXiVuSOijR8u2vQRjjlBOhMUNK9V0n0V6GpKaYkbw2SBVJEB6jIekbKhAnysum6XN4bJQQRrE0T2g4VX9vZIgrNeGBmSxCqnmvEP/z+qmOLryMiiTVRODZoShlUMewqAKGVBKs2cQQhCU1WSEeIYmwNoXVTAnu/JcXSee04ToN9/as3rws66iCQ3AEToALzkET3IAWaAMMHsEzeAVv1pP1Yr1bH7PRilXu7IM/sD5/AI1Vlec=</latexit><latexit sha1_base64="Vr1kZJh7jApVU4cGmtPimK4PPic=">AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIoAsXBRe6rGAf0MYwmUzaoTOTMDMRagj+ihsXirj1P9z5N07aLLT1wMDhnHu5Z06QMKq043xblaXlldW16nptY3Nre8fe3euoOJWYtHHMYtkLkCKMCtLWVDPSSyRBPGCkG4yvCr/7QKSisbjTk4R4HA0FjShG2ki+fXB9n+nczwYc6ZHk2YiGee7bdafhTAEXiVuSOijR8u2vQRjjlBOhMUNK9V0n0V6GpKaYkbw2SBVJEB6jIekbKhAnysum6XN4bJQQRrE0T2g4VX9vZIgrNeGBmSxCqnmvEP/z+qmOLryMiiTVRODZoShlUMewqAKGVBKs2cQQhCU1WSEeIYmwNoXVTAnu/JcXSee04ToN9/as3rws66iCQ3AEToALzkET3IAWaAMMHsEzeAVv1pP1Yr1bH7PRilXu7IM/sD5/AI1Vlec=</latexit><latexit sha1_base64="Vr1kZJh7jApVU4cGmtPimK4PPic=">AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIoAsXBRe6rGAf0MYwmUzaoTOTMDMRagj+ihsXirj1P9z5N07aLLT1wMDhnHu5Z06QMKq043xblaXlldW16nptY3Nre8fe3euoOJWYtHHMYtkLkCKMCtLWVDPSSyRBPGCkG4yvCr/7QKSisbjTk4R4HA0FjShG2ki+fXB9n+nczwYc6ZHk2YiGee7bdafhTAEXiVuSOijR8u2vQRjjlBOhMUNK9V0n0V6GpKaYkbw2SBVJEB6jIekbKhAnysum6XN4bJQQRrE0T2g4VX9vZIgrNeGBmSxCqnmvEP/z+qmOLryMiiTVRODZoShlUMewqAKGVBKs2cQQhCU1WSEeIYmwNoXVTAnu/JcXSee04ToN9/as3rws66iCQ3AEToALzkET3IAWaAMMHsEzeAVv1pP1Yr1bH7PRilXu7IM/sD5/AI1Vlec=</latexit><latexit sha1_base64="Vr1kZJh7jApVU4cGmtPimK4PPic=">AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIoAsXBRe6rGAf0MYwmUzaoTOTMDMRagj+ihsXirj1P9z5N07aLLT1wMDhnHu5Z06QMKq043xblaXlldW16nptY3Nre8fe3euoOJWYtHHMYtkLkCKMCtLWVDPSSyRBPGCkG4yvCr/7QKSisbjTk4R4HA0FjShG2ki+fXB9n+nczwYc6ZHk2YiGee7bdafhTAEXiVuSOijR8u2vQRjjlBOhMUNK9V0n0V6GpKaYkbw2SBVJEB6jIekbKhAnysum6XN4bJQQRrE0T2g4VX9vZIgrNeGBmSxCqnmvEP/z+qmOLryMiiTVRODZoShlUMewqAKGVBKs2cQQhCU1WSEeIYmwNoXVTAnu/JcXSee04ToN9/as3rws66iCQ3AEToALzkET3IAWaAMMHsEzeAVv1pP1Yr1bH7PRilXu7IM/sD5/AI1Vlec=</latexit>Gt 1hid
<latexit sha1_base64="TTYaLKTJF7FnWkmBKBRepTsjYHc=">AAAB/3icbVDLSsNAFJ34rPUVFdy4GSyCG0sigi5cFFzosoJ9QBvDZDJph85MwsxEKDELf8WNC0Xc+hvu/BsnbRbaemDgcM693DMnSBhV2nG+rYXFpeWV1cpadX1jc2vb3tltqziVmLRwzGLZDZAijArS0lQz0k0kQTxgpBOMrgq/80CkorG40+OEeBwNBI0oRtpIvr1/fZ/pEzf3sz5Heih5NqRhnvt2zak7E8B54pakBko0ffurH8Y45URozJBSPddJtJchqSlmJK/2U0UShEdoQHqGCsSJ8rJJ/hweGSWEUSzNExpO1N8bGeJKjXlgJouQatYrxP+8XqqjCy+jIkk1EXh6KEoZ1DEsyoAhlQRrNjYEYUlNVoiHSCKsTWVVU4I7++V50j6tu07dvT2rNS7LOirgAByCY+CCc9AAN6AJWgCDR/AMXsGb9WS9WO/Wx3R0wSp39sAfWJ8/b2WWWQ==</latexit><latexit sha1_base64="TTYaLKTJF7FnWkmBKBRepTsjYHc=">AAAB/3icbVDLSsNAFJ34rPUVFdy4GSyCG0sigi5cFFzosoJ9QBvDZDJph85MwsxEKDELf8WNC0Xc+hvu/BsnbRbaemDgcM693DMnSBhV2nG+rYXFpeWV1cpadX1jc2vb3tltqziVmLRwzGLZDZAijArS0lQz0k0kQTxgpBOMrgq/80CkorG40+OEeBwNBI0oRtpIvr1/fZ/pEzf3sz5Heih5NqRhnvt2zak7E8B54pakBko0ffurH8Y45URozJBSPddJtJchqSlmJK/2U0UShEdoQHqGCsSJ8rJJ/hweGSWEUSzNExpO1N8bGeJKjXlgJouQatYrxP+8XqqjCy+jIkk1EXh6KEoZ1DEsyoAhlQRrNjYEYUlNVoiHSCKsTWVVU4I7++V50j6tu07dvT2rNS7LOirgAByCY+CCc9AAN6AJWgCDR/AMXsGb9WS9WO/Wx3R0wSp39sAfWJ8/b2WWWQ==</latexit><latexit sha1_base64="TTYaLKTJF7FnWkmBKBRepTsjYHc=">AAAB/3icbVDLSsNAFJ34rPUVFdy4GSyCG0sigi5cFFzosoJ9QBvDZDJph85MwsxEKDELf8WNC0Xc+hvu/BsnbRbaemDgcM693DMnSBhV2nG+rYXFpeWV1cpadX1jc2vb3tltqziVmLRwzGLZDZAijArS0lQz0k0kQTxgpBOMrgq/80CkorG40+OEeBwNBI0oRtpIvr1/fZ/pEzf3sz5Heih5NqRhnvt2zak7E8B54pakBko0ffurH8Y45URozJBSPddJtJchqSlmJK/2U0UShEdoQHqGCsSJ8rJJ/hweGSWEUSzNExpO1N8bGeJKjXlgJouQatYrxP+8XqqjCy+jIkk1EXh6KEoZ1DEsyoAhlQRrNjYEYUlNVoiHSCKsTWVVU4I7++V50j6tu07dvT2rNS7LOirgAByCY+CCc9AAN6AJWgCDR/AMXsGb9WS9WO/Wx3R0wSp39sAfWJ8/b2WWWQ==</latexit><latexit sha1_base64="TTYaLKTJF7FnWkmBKBRepTsjYHc=">AAAB/3icbVDLSsNAFJ34rPUVFdy4GSyCG0sigi5cFFzosoJ9QBvDZDJph85MwsxEKDELf8WNC0Xc+hvu/BsnbRbaemDgcM693DMnSBhV2nG+rYXFpeWV1cpadX1jc2vb3tltqziVmLRwzGLZDZAijArS0lQz0k0kQTxgpBOMrgq/80CkorG40+OEeBwNBI0oRtpIvr1/fZ/pEzf3sz5Heih5NqRhnvt2zak7E8B54pakBko0ffurH8Y45URozJBSPddJtJchqSlmJK/2U0UShEdoQHqGCsSJ8rJJ/hweGSWEUSzNExpO1N8bGeJKjXlgJouQatYrxP+8XqqjCy+jIkk1EXh6KEoZ1DEsyoAhlQRrNjYEYUlNVoiHSCKsTWVVU4I7++V50j6tu07dvT2rNS7LOirgAByCY+CCc9AAN6AJWgCDR/AMXsGb9WS9WO/Wx3R0wSp39sAfWJ8/b2WWWQ==</latexit>Gtout
<latexit sha1_base64="Fm4Iz64LvWn082kf2+Dq6sd1g9I=">AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNBlBfuANobJdNIOnWTCzI1YQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0gE1+A439bK6tr6xmZlq7q9s7u3bx/UOlqmirI2lUKqXkA0EzxmbeAgWC9RjESBYN1gcl343UemNJfxPUwT5kVkFPOQUwJG8u3azQP42SAiMFZRJlPIc9+uOw1nBrxM3JLUUYmWb38NhpKmEYuBCqJ133US8DKigFPB8uog1SwhdEJGrG9oTCKmvWyWPccnRhniUCrzYsAz9fdGRiKtp1FgJouQetErxP+8fgrhpZfxOEmBxXR+KEwFBomLIvCQK0ZBTA0hVHGTFdMxUYSCqatqSnAXv7xMOmcN12m4d+f15lVZRwUdoWN0ilx0gZroFrVQG1H0hJ7RK3qzcuvFerc+5qMrVrlziP7A+vwB7zuU/g==</latexit><latexit sha1_base64="Fm4Iz64LvWn082kf2+Dq6sd1g9I=">AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNBlBfuANobJdNIOnWTCzI1YQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0gE1+A439bK6tr6xmZlq7q9s7u3bx/UOlqmirI2lUKqXkA0EzxmbeAgWC9RjESBYN1gcl343UemNJfxPUwT5kVkFPOQUwJG8u3azQP42SAiMFZRJlPIc9+uOw1nBrxM3JLUUYmWb38NhpKmEYuBCqJ133US8DKigFPB8uog1SwhdEJGrG9oTCKmvWyWPccnRhniUCrzYsAz9fdGRiKtp1FgJouQetErxP+8fgrhpZfxOEmBxXR+KEwFBomLIvCQK0ZBTA0hVHGTFdMxUYSCqatqSnAXv7xMOmcN12m4d+f15lVZRwUdoWN0ilx0gZroFrVQG1H0hJ7RK3qzcuvFerc+5qMrVrlziP7A+vwB7zuU/g==</latexit><latexit sha1_base64="Fm4Iz64LvWn082kf2+Dq6sd1g9I=">AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNBlBfuANobJdNIOnWTCzI1YQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0gE1+A439bK6tr6xmZlq7q9s7u3bx/UOlqmirI2lUKqXkA0EzxmbeAgWC9RjESBYN1gcl343UemNJfxPUwT5kVkFPOQUwJG8u3azQP42SAiMFZRJlPIc9+uOw1nBrxM3JLUUYmWb38NhpKmEYuBCqJ133US8DKigFPB8uog1SwhdEJGrG9oTCKmvWyWPccnRhniUCrzYsAz9fdGRiKtp1FgJouQetErxP+8fgrhpZfxOEmBxXR+KEwFBomLIvCQK0ZBTA0hVHGTFdMxUYSCqatqSnAXv7xMOmcN12m4d+f15lVZRwUdoWN0ilx0gZroFrVQG1H0hJ7RK3qzcuvFerc+5qMrVrlziP7A+vwB7zuU/g==</latexit><latexit sha1_base64="Fm4Iz64LvWn082kf2+Dq6sd1g9I=">AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNBlBfuANobJdNIOnWTCzI1YQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0gE1+A439bK6tr6xmZlq7q9s7u3bx/UOlqmirI2lUKqXkA0EzxmbeAgWC9RjESBYN1gcl343UemNJfxPUwT5kVkFPOQUwJG8u3azQP42SAiMFZRJlPIc9+uOw1nBrxM3JLUUYmWb38NhpKmEYuBCqJ133US8DKigFPB8uog1SwhdEJGrG9oTCKmvWyWPccnRhniUCrzYsAz9fdGRiKtp1FgJouQetErxP+8fgrhpZfxOEmBxXR+KEwFBomLIvCQK0ZBTA0hVHGTFdMxUYSCqatqSnAXv7xMOmcN12m4d+f15lVZRwUdoWN0ilx0gZroFrVQG1H0hJ7RK3qzcuvFerc+5qMrVrlziP7A+vwB7zuU/g==</latexit>Gtinp
<latexit sha1_base64="KNSYXpe+JERsrbP/+QxnbJ2F1T0=">AAAB+3icbVDLSsNAFL3xWesr1qWbYBFclUQEXbgouNBlBfuANobJdNIOnZmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvcy5J0wYVdp1v62V1bX1jc3KVnV7Z3dv3z6odVScSkzaOGax7IVIEUYFaWuqGeklkiAeMtINJ9eF330kUtFY3OtpQnyORoJGFCNtpMCu3QTZgCM9ljyjIsnzBx3YdbfhzuAsE68kdSjRCuyvwTDGKSdCY4aU6ntuov0MSU0xI3l1kCqSIDxBI9I3VCBOlJ/NsufOiVGGThRL84R2ZurvjQxxpaY8NJNFTLXoFeJ/Xj/V0aVf3JRqIvD8oyhljo6doghnSCXBmk0NQVhSk9XBYyQR1qauqinBWzx5mXTOGp7b8O7O682rso4KHMExnIIHF9CEW2hBGzA8wTO8wpuVWy/Wu/UxH12xyp1D+APr8wfV6pTt</latexit><latexit sha1_base64="KNSYXpe+JERsrbP/+QxnbJ2F1T0=">AAAB+3icbVDLSsNAFL3xWesr1qWbYBFclUQEXbgouNBlBfuANobJdNIOnZmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvcy5J0wYVdp1v62V1bX1jc3KVnV7Z3dv3z6odVScSkzaOGax7IVIEUYFaWuqGeklkiAeMtINJ9eF330kUtFY3OtpQnyORoJGFCNtpMCu3QTZgCM9ljyjIsnzBx3YdbfhzuAsE68kdSjRCuyvwTDGKSdCY4aU6ntuov0MSU0xI3l1kCqSIDxBI9I3VCBOlJ/NsufOiVGGThRL84R2ZurvjQxxpaY8NJNFTLXoFeJ/Xj/V0aVf3JRqIvD8oyhljo6doghnSCXBmk0NQVhSk9XBYyQR1qauqinBWzx5mXTOGp7b8O7O682rso4KHMExnIIHF9CEW2hBGzA8wTO8wpuVWy/Wu/UxH12xyp1D+APr8wfV6pTt</latexit><latexit sha1_base64="KNSYXpe+JERsrbP/+QxnbJ2F1T0=">AAAB+3icbVDLSsNAFL3xWesr1qWbYBFclUQEXbgouNBlBfuANobJdNIOnZmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvcy5J0wYVdp1v62V1bX1jc3KVnV7Z3dv3z6odVScSkzaOGax7IVIEUYFaWuqGeklkiAeMtINJ9eF330kUtFY3OtpQnyORoJGFCNtpMCu3QTZgCM9ljyjIsnzBx3YdbfhzuAsE68kdSjRCuyvwTDGKSdCY4aU6ntuov0MSU0xI3l1kCqSIDxBI9I3VCBOlJ/NsufOiVGGThRL84R2ZurvjQxxpaY8NJNFTLXoFeJ/Xj/V0aVf3JRqIvD8oyhljo6doghnSCXBmk0NQVhSk9XBYyQR1qauqinBWzx5mXTOGp7b8O7O682rso4KHMExnIIHF9CEW2hBGzA8wTO8wpuVWy/Wu/UxH12xyp1D+APr8wfV6pTt</latexit><latexit sha1_base64="KNSYXpe+JERsrbP/+QxnbJ2F1T0=">AAAB+3icbVDLSsNAFL3xWesr1qWbYBFclUQEXbgouNBlBfuANobJdNIOnZmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvcy5J0wYVdp1v62V1bX1jc3KVnV7Z3dv3z6odVScSkzaOGax7IVIEUYFaWuqGeklkiAeMtINJ9eF330kUtFY3OtpQnyORoJGFCNtpMCu3QTZgCM9ljyjIsnzBx3YdbfhzuAsE68kdSjRCuyvwTDGKSdCY4aU6ntuov0MSU0xI3l1kCqSIDxBI9I3VCBOlJ/NsufOiVGGThRL84R2ZurvjQxxpaY8NJNFTLXoFeJ/Xj/V0aVf3JRqIvD8oyhljo6doghnSCXBmk0NQVhSk9XBYyQR1qauqinBWzx5mXTOGp7b8O7O682rso4KHMExnIIHF9CEW2hBGzA8wTO8wpuVWy/Wu/UxH12xyp1D+APr8wfV6pTt</latexit> (c) Recurrent GN architecture
Figure 6: (a) An example composing multiple GN blocks in sequence to form a GN \core". Here,
the GN blocks can use shared weights, or they could be independent. (b) The encode-process-decode
architecture, which is a common choice for composing GN blocks (see Section 4.3). Here, a GN
encodes an input graph, which is then processed by a GN core. The output of the core is decoded
by a third GN block into an output graph, whose nodes, edges, and/or global attributes would be
used for task-specic purposes. (c) The encode-process-decode architecture applied in a sequential
setting in which the core is also unrolled over time (potentially using a GRU or LSTM architecture),
in addition to being repeated within each time step. Here, merged lines indicate concatenation, and
split lines indicate copying.
4.3 Composable multi-block architectures
A key design principle of graph networks is constructing complex architectures by composing GN
blocks. We dened a GN block as always taking a graph comprised of edge, node, and global
elements as input, and returning a graph with the same constituent elements as output (simply
passing through the input elements to the output when those elements are not explicitly updated).
This graph-to-graph input/output interface ensures that the output of one GN block can be passed
as input to another, even if their internal congurations are dierent, similar to the tensor-to-tensor
interface of the standard deep learning toolkit. In the most basic form, two GN blocks, GN1and
GN2, can be composed as GN1GN2by passing the output of the rst as input to the second:
G0= GN 2(GN 1(G)).
Arbitrary numbers of GN blocks can be composed, as show in Figure 6a. The blocks can
be unshared (dierent functions and/or parameters, analogous to layers of a CNN), GN16=
GN26=6=GNM, or shared (reused functions and parameters, analogous to an unrolled RNN),
GN1=GN2==GNM. The white box around the GNcorein Figure 6a represents Mrepeated
internal processing sub-steps, with either shared or unshared GN blocks. Shared congurations
are analogous to message-passing (Gilmer et al., 2017), where the same local update procedure is
applied iteratively to propagate information across the structure (Figure 7). If we exclude the global
u(which aggregates information from across the nodes and edges), the information that a node
has access to after msteps of propagation is determined by the set of nodes and edges that are at
mostmhops away. This can be interpreted as breaking down a complex computation into smaller
elementary steps. The steps can also be used to capture sequentiality in time. In our ball-spring
example, if each propagation step predicts the physical dynamics over one time step of duration  t,
then theMpropagation steps result in a total simulation time of, Mt.
A common architecture design is what we call the encode-process-decode conguration (Hamrick
et al. (2018); also see Figure 6ba): an input graph, Ginpis transformed into a latent representation,
G0, by an encoder, GNenc; a shared core block, GNcore, is applied Mtimes to return GM; and
nally an output graph, Gout, is decoded by GNdec. For example, in our running example, the
encoder might compute the initial forces and interaction energies between the balls, the core might
19m=0
<latexit sha1_base64="ic6WezPV7TWar890N1QnpVOPjNA=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79Im/+jZNkDxotaCiquunuilIpLPr+l1dYW9/Y3Cpul3Z29/YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324/cWJHoB5ykPFR0qEUsGEUn3atrv1+u+DV/AfKXBDmpQI5Gv/zZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4v/ed0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMv+B9fAO9341Y</latexit><latexit sha1_base64="ic6WezPV7TWar890N1QnpVOPjNA=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79Im/+jZNkDxotaCiquunuilIpLPr+l1dYW9/Y3Cpul3Z29/YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324/cWJHoB5ykPFR0qEUsGEUn3atrv1+u+DV/AfKXBDmpQI5Gv/zZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4v/ed0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMv+B9fAO9341Y</latexit><latexit sha1_base64="ic6WezPV7TWar890N1QnpVOPjNA=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79Im/+jZNkDxotaCiquunuilIpLPr+l1dYW9/Y3Cpul3Z29/YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324/cWJHoB5ykPFR0qEUsGEUn3atrv1+u+DV/AfKXBDmpQI5Gv/zZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4v/ed0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMv+B9fAO9341Y</latexit><latexit sha1_base64="ic6WezPV7TWar890N1QnpVOPjNA=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79Im/+jZNkDxotaCiquunuilIpLPr+l1dYW9/Y3Cpul3Z29/YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324/cWJHoB5ykPFR0qEUsGEUn3atrv1+u+DV/AfKXBDmpQI5Gv/zZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4v/ed0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMv+B9fAO9341Y</latexit>m=1
<latexit sha1_base64="Zv5ybfKo/H8QvXQaOWqQVyOFtg4=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64="Zv5ybfKo/H8QvXQaOWqQVyOFtg4=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64="Zv5ybfKo/H8QvXQaOWqQVyOFtg4=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64="hP+6LrUf2d3tZaldqaQQvEKMXyw=">AAAB2XicbZDNSgMxFIXv1L86Vq1rN8EiuCozbnQpuHFZwbZCO5RM5k4bmskMyR2hDH0BF25EfC93vo3pz0JbDwQ+zknIvSculLQUBN9ebWd3b/+gfugfNfzjk9Nmo2fz0gjsilzl5jnmFpXU2CVJCp8LgzyLFfbj6f0i77+gsTLXTzQrMMr4WMtUCk7O6oyaraAdLMW2IVxDC9YaNb+GSS7KDDUJxa0dhEFBUcUNSaFw7g9LiwUXUz7GgUPNM7RRtRxzzi6dk7A0N+5oYkv394uKZ9bOstjdzDhN7Ga2MP/LBiWlt1EldVESarH6KC0Vo5wtdmaJNChIzRxwYaSblYkJN1yQa8Z3HYSbG29D77odBu3wMYA6nMMFXEEIN3AHD9CBLghI4BXevYn35n2suqp569LO4I+8zx84xIo4</latexit><latexit sha1_base64="41wuc+jmacSzA8ipzMzk3JQ30Cs=">AAAB33icbZDNSgMxFIXv1L9aq1a3boJFcFUybnQjCG5cVrS10A4lk95pQ5PMkGSEMvQR3LhQxLdy59uY/iy09UDg45yE3HviTArrKP0OShubW9s75d3KXnX/4LB2VG3bNDccWzyVqenEzKIUGltOOImdzCBTscSneHw7y5+e0ViR6kc3yTBSbKhFIjhz3npQ12G/VqcNOhdZh3AJdViq2a999QYpzxVqxyWzthvSzEUFM05widNKL7eYMT5mQ+x61EyhjYr5qFNy5p0BSVLjj3Zk7v5+UTBl7UTF/qZibmRXs5n5X9bNXXIVFUJnuUPNFx8luSQuJbO9yUAY5E5OPDBuhJ+V8BEzjDvfTsWXEK6uvA7ti0ZIG+E9hTKcwCmcQwiXcAN30IQWcBjCC7zBeyCD1+BjUVcpWPZ2DH8UfP4Arc6MHg==</latexit><latexit sha1_base64="41wuc+jmacSzA8ipzMzk3JQ30Cs=">AAAB33icbZDNSgMxFIXv1L9aq1a3boJFcFUybnQjCG5cVrS10A4lk95pQ5PMkGSEMvQR3LhQxLdy59uY/iy09UDg45yE3HviTArrKP0OShubW9s75d3KXnX/4LB2VG3bNDccWzyVqenEzKIUGltOOImdzCBTscSneHw7y5+e0ViR6kc3yTBSbKhFIjhz3npQ12G/VqcNOhdZh3AJdViq2a999QYpzxVqxyWzthvSzEUFM05widNKL7eYMT5mQ+x61EyhjYr5qFNy5p0BSVLjj3Zk7v5+UTBl7UTF/qZibmRXs5n5X9bNXXIVFUJnuUPNFx8luSQuJbO9yUAY5E5OPDBuhJ+V8BEzjDvfTsWXEK6uvA7ti0ZIG+E9hTKcwCmcQwiXcAN30IQWcBjCC7zBeyCD1+BjUVcpWPZ2DH8UfP4Arc6MHg==</latexit><latexit sha1_base64="c5O5JuZLnHr2YYnYcAr/a2hwAn0=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoOQKtzZaCMEbCwjmg9IjrC3mSRLdveO3T0hHPkJNhaK2PqL7Pw3bpIrNPHBwOO9GWbmRYngxvr+t1fY2Nza3inulvb2Dw6PyscnLROnmmGTxSLWnYgaFFxh03IrsJNopDIS2I4mt3O//YTa8Fg92mmCoaQjxYecUeukB3kT9MsVv+YvQNZJkJMK5Gj0y1+9QcxSicoyQY3pBn5iw4xqy5nAWamXGkwom9ARdh1VVKIJs8WpM3LhlAEZxtqVsmSh/p7IqDRmKiPXKakdm1VvLv7ndVM7vA4zrpLUomLLRcNUEBuT+d9kwDUyK6aOUKa5u5WwMdWUWZdOyYUQrL68TlqXtcCvBfd+pV7N4yjCGZxDFQK4gjrcQQOawGAEz/AKb57wXrx372PZWvDymVP4A+/zB74jjVU=</latexit><latexit sha1_base64="Zv5ybfKo/H8QvXQaOWqQVyOFtg4=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64="Zv5ybfKo/H8QvXQaOWqQVyOFtg4=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64="Zv5ybfKo/H8QvXQaOWqQVyOFtg4=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64="Zv5ybfKo/H8QvXQaOWqQVyOFtg4=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64="Zv5ybfKo/H8QvXQaOWqQVyOFtg4=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64="Zv5ybfKo/H8QvXQaOWqQVyOFtg4=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit>m=2
<latexit sha1_base64="oVNcNejQAmVb9Nn6VIeeV7f50ls=">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3+CFw+KePUXefPfmLZ70NYHA4/3ZpiZFyaCG+t536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5Ux+UK17NWwCvEz8nFcjRHJS/+sOYppIpSwUxpud7iQ0yoi2ngs1K/dSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F/7xeaqPrIOMqSS1TdLkoSgW2MZ7/jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZp/f1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541a</latexit><latexit sha1_base64="oVNcNejQAmVb9Nn6VIeeV7f50ls=">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3+CFw+KePUXefPfmLZ70NYHA4/3ZpiZFyaCG+t536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5Ux+UK17NWwCvEz8nFcjRHJS/+sOYppIpSwUxpud7iQ0yoi2ngs1K/dSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F/7xeaqPrIOMqSS1TdLkoSgW2MZ7/jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZp/f1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541a</latexit><latexit sha1_base64="oVNcNejQAmVb9Nn6VIeeV7f50ls=">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3+CFw+KePUXefPfmLZ70NYHA4/3ZpiZFyaCG+t536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5Ux+UK17NWwCvEz8nFcjRHJS/+sOYppIpSwUxpud7iQ0yoi2ngs1K/dSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F/7xeaqPrIOMqSS1TdLkoSgW2MZ7/jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZp/f1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541a</latexit><latexit sha1_base64="oVNcNejQAmVb9Nn6VIeeV7f50ls=">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3+CFw+KePUXefPfmLZ70NYHA4/3ZpiZFyaCG+t536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5Ux+UK17NWwCvEz8nFcjRHJS/+sOYppIpSwUxpud7iQ0yoi2ngs1K/dSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F/7xeaqPrIOMqSS1TdLkoSgW2MZ7/jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZp/f1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541a</latexit>m=3
<latexit sha1_base64="+jOsYdJ9SNJymWDVPaXJffNRSPM=">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpT/BiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9/Y3Cpul3Z29/YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6Je/eoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E/7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezX//rJSr+ZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4Q/Q5w/Ca41b</latexit><latexit sha1_base64="+jOsYdJ9SNJymWDVPaXJffNRSPM=">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpT/BiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9/Y3Cpul3Z29/YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6Je/eoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E/7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezX//rJSr+ZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4Q/Q5w/Ca41b</latexit><latexit sha1_base64="+jOsYdJ9SNJymWDVPaXJffNRSPM=">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpT/BiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9/Y3Cpul3Z29/YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6Je/eoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E/7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezX//rJSr+ZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4Q/Q5w/Ca41b</latexit><latexit sha1_base64="+jOsYdJ9SNJymWDVPaXJffNRSPM=">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpT/BiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9/Y3Cpul3Z29/YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6Je/eoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E/7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezX//rJSr+ZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4Q/Q5w/Ca41b</latexit>
m=0
<latexit sha1_base64="ic6WezPV7TWar890N1QnpVOPjNA=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79Im/+jZNkDxotaCiquunuilIpLPr+l1dYW9/Y3Cpul3Z29/YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324/cWJHoB5ykPFR0qEUsGEUn3atrv1+u+DV/AfKXBDmpQI5Gv/zZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4v/ed0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMv+B9fAO9341Y</latexit><latexit sha1_base64="ic6WezPV7TWar890N1QnpVOPjNA=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79Im/+jZNkDxotaCiquunuilIpLPr+l1dYW9/Y3Cpul3Z29/YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324/cWJHoB5ykPFR0qEUsGEUn3atrv1+u+DV/AfKXBDmpQI5Gv/zZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4v/ed0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMv+B9fAO9341Y</latexit><latexit sha1_base64="ic6WezPV7TWar890N1QnpVOPjNA=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79Im/+jZNkDxotaCiquunuilIpLPr+l1dYW9/Y3Cpul3Z29/YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324/cWJHoB5ykPFR0qEUsGEUn3atrv1+u+DV/AfKXBDmpQI5Gv/zZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4v/ed0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMv+B9fAO9341Y</latexit><latexit sha1_base64="ic6WezPV7TWar890N1QnpVOPjNA=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79Im/+jZNkDxotaCiquunuilIpLPr+l1dYW9/Y3Cpul3Z29/YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324/cWJHoB5ykPFR0qEUsGEUn3atrv1+u+DV/AfKXBDmpQI5Gv/zZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4v/ed0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMv+B9fAO9341Y</latexit>m=1
<latexit sha1_base64="Zv5ybfKo/H8QvXQaOWqQVyOFtg4=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64="Zv5ybfKo/H8QvXQaOWqQVyOFtg4=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64="Zv5ybfKo/H8QvXQaOWqQVyOFtg4=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64="hP+6LrUf2d3tZaldqaQQvEKMXyw=">AAAB2XicbZDNSgMxFIXv1L86Vq1rN8EiuCozbnQpuHFZwbZCO5RM5k4bmskMyR2hDH0BF25EfC93vo3pz0JbDwQ+zknIvSculLQUBN9ebWd3b/+gfugfNfzjk9Nmo2fz0gjsilzl5jnmFpXU2CVJCp8LgzyLFfbj6f0i77+gsTLXTzQrMMr4WMtUCk7O6oyaraAdLMW2IVxDC9YaNb+GSS7KDDUJxa0dhEFBUcUNSaFw7g9LiwUXUz7GgUPNM7RRtRxzzi6dk7A0N+5oYkv394uKZ9bOstjdzDhN7Ga2MP/LBiWlt1EldVESarH6KC0Vo5wtdmaJNChIzRxwYaSblYkJN1yQa8Z3HYSbG29D77odBu3wMYA6nMMFXEEIN3AHD9CBLghI4BXevYn35n2suqp569LO4I+8zx84xIo4</latexit><latexit sha1_base64="41wuc+jmacSzA8ipzMzk3JQ30Cs=">AAAB33icbZDNSgMxFIXv1L9aq1a3boJFcFUybnQjCG5cVrS10A4lk95pQ5PMkGSEMvQR3LhQxLdy59uY/iy09UDg45yE3HviTArrKP0OShubW9s75d3KXnX/4LB2VG3bNDccWzyVqenEzKIUGltOOImdzCBTscSneHw7y5+e0ViR6kc3yTBSbKhFIjhz3npQ12G/VqcNOhdZh3AJdViq2a999QYpzxVqxyWzthvSzEUFM05widNKL7eYMT5mQ+x61EyhjYr5qFNy5p0BSVLjj3Zk7v5+UTBl7UTF/qZibmRXs5n5X9bNXXIVFUJnuUPNFx8luSQuJbO9yUAY5E5OPDBuhJ+V8BEzjDvfTsWXEK6uvA7ti0ZIG+E9hTKcwCmcQwiXcAN30IQWcBjCC7zBeyCD1+BjUVcpWPZ2DH8UfP4Arc6MHg==</latexit><latexit sha1_base64="41wuc+jmacSzA8ipzMzk3JQ30Cs=">AAAB33icbZDNSgMxFIXv1L9aq1a3boJFcFUybnQjCG5cVrS10A4lk95pQ5PMkGSEMvQR3LhQxLdy59uY/iy09UDg45yE3HviTArrKP0OShubW9s75d3KXnX/4LB2VG3bNDccWzyVqenEzKIUGltOOImdzCBTscSneHw7y5+e0ViR6kc3yTBSbKhFIjhz3npQ12G/VqcNOhdZh3AJdViq2a999QYpzxVqxyWzthvSzEUFM05widNKL7eYMT5mQ+x61EyhjYr5qFNy5p0BSVLjj3Zk7v5+UTBl7UTF/qZibmRXs5n5X9bNXXIVFUJnuUPNFx8luSQuJbO9yUAY5E5OPDBuhJ+V8BEzjDvfTsWXEK6uvA7ti0ZIG+E9hTKcwCmcQwiXcAN30IQWcBjCC7zBeyCD1+BjUVcpWPZ2DH8UfP4Arc6MHg==</latexit><latexit sha1_base64="c5O5JuZLnHr2YYnYcAr/a2hwAn0=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoOQKtzZaCMEbCwjmg9IjrC3mSRLdveO3T0hHPkJNhaK2PqL7Pw3bpIrNPHBwOO9GWbmRYngxvr+t1fY2Nza3inulvb2Dw6PyscnLROnmmGTxSLWnYgaFFxh03IrsJNopDIS2I4mt3O//YTa8Fg92mmCoaQjxYecUeukB3kT9MsVv+YvQNZJkJMK5Gj0y1+9QcxSicoyQY3pBn5iw4xqy5nAWamXGkwom9ARdh1VVKIJs8WpM3LhlAEZxtqVsmSh/p7IqDRmKiPXKakdm1VvLv7ndVM7vA4zrpLUomLLRcNUEBuT+d9kwDUyK6aOUKa5u5WwMdWUWZdOyYUQrL68TlqXtcCvBfd+pV7N4yjCGZxDFQK4gjrcQQOawGAEz/AKb57wXrx372PZWvDymVP4A+/zB74jjVU=</latexit><latexit sha1_base64="Zv5ybfKo/H8QvXQaOWqQVyOFtg4=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64="Zv5ybfKo/H8QvXQaOWqQVyOFtg4=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64="Zv5ybfKo/H8QvXQaOWqQVyOFtg4=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64="Zv5ybfKo/H8QvXQaOWqQVyOFtg4=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64="Zv5ybfKo/H8QvXQaOWqQVyOFtg4=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64="Zv5ybfKo/H8QvXQaOWqQVyOFtg4=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit>m=2
<latexit sha1_base64="oVNcNejQAmVb9Nn6VIeeV7f50ls=">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3+CFw+KePUXefPfmLZ70NYHA4/3ZpiZFyaCG+t536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5Ux+UK17NWwCvEz8nFcjRHJS/+sOYppIpSwUxpud7iQ0yoi2ngs1K/dSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F/7xeaqPrIOMqSS1TdLkoSgW2MZ7/jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZp/f1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541a</latexit><latexit sha1_base64="oVNcNejQAmVb9Nn6VIeeV7f50ls=">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3+CFw+KePUXefPfmLZ70NYHA4/3ZpiZFyaCG+t536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5Ux+UK17NWwCvEz8nFcjRHJS/+sOYppIpSwUxpud7iQ0yoi2ngs1K/dSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F/7xeaqPrIOMqSS1TdLkoSgW2MZ7/jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZp/f1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541a</latexit><latexit sha1_base64="oVNcNejQAmVb9Nn6VIeeV7f50ls=">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3+CFw+KePUXefPfmLZ70NYHA4/3ZpiZFyaCG+t536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5Ux+UK17NWwCvEz8nFcjRHJS/+sOYppIpSwUxpud7iQ0yoi2ngs1K/dSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F/7xeaqPrIOMqSS1TdLkoSgW2MZ7/jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZp/f1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541a</latexit><latexit sha1_base64="oVNcNejQAmVb9Nn6VIeeV7f50ls=">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3+CFw+KePUXefPfmLZ70NYHA4/3ZpiZFyaCG+t536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5Ux+UK17NWwCvEz8nFcjRHJS/+sOYppIpSwUxpud7iQ0yoi2ngs1K/dSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F/7xeaqPrIOMqSS1TdLkoSgW2MZ7/jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZp/f1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541a</latexit>m=3
<latexit sha1_base64="+jOsYdJ9SNJymWDVPaXJffNRSPM=">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpT/BiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9/Y3Cpul3Z29/YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6Je/eoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E/7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezX//rJSr+ZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4Q/Q5w/Ca41b</latexit><latexit sha1_base64="+jOsYdJ9SNJymWDVPaXJffNRSPM=">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpT/BiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9/Y3Cpul3Z29/YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6Je/eoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E/7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezX//rJSr+ZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4Q/Q5w/Ca41b</latexit><latexit sha1_base64="+jOsYdJ9SNJymWDVPaXJffNRSPM=">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpT/BiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9/Y3Cpul3Z29/YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6Je/eoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E/7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezX//rJSr+ZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4Q/Q5w/Ca41b</latexit><latexit sha1_base64="+jOsYdJ9SNJymWDVPaXJffNRSPM=">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpT/BiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9/Y3Cpul3Z29/YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6Je/eoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E/7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezX//rJSr+ZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4Q/Q5w/Ca41b</latexit>Figure 7: Example of message passing. Each row highlights the information that diuses through
the graph starting from a particular node. In the top row, the node of interest is in the upper
right; in the bottom row, the node of interest is in the bottom right. Shaded nodes indicate how far
information from the original node can travel in msteps of message passing; bolded edges indicate
which edges that information has the potential to travel across. Note that during the full message
passing procedure, this propagation of information happens simultaneously for all nodes and edges
in the graph (not just the two shown here).
apply an elementary dynamics update, and the decoder might read out the nal positions from the
updated graph state.
Similar to the encode-process-decode design, recurrent GN-based architectures can be built by
maintaining a hidden graph, Gt
hid, taking as input an observed graph, Gt
inp, and returning an output
graph,Gt
out, on each step (see Figure 6c). This type of architecture can be particularly useful for
predicting sequences of graphs, such as predicting the trajectory of a dynamical system over time
(e.g. Sanchez-Gonzalez et al., 2018). The encoded graph, output by GNenc, must have the same
structure as Gt
hid, and they can be easily combined by concatenating their corresponding ek,vi,
anduvectors (where the upward arrow merges into the left-hand horizontal arrow in Figure 6c),
before being passed to GNcore. For the output, the Gt
hidis copied (where the right-hand horizontal
arrow splits into the downward arrow in Figure 6c) and decoded by GNdec. This design reuses GN
blocks in several ways: GNenc,GNdec, and GNcoreare shared across each step, t; and within each
step, GN coremay perform multiple shared sub-steps.
Various other techniques for designing GN-based architectures can be useful. Graph skip
connections, for example, would concatenate a GN block's input graph, Gm, with its output graph,
Gm+1, before proceeding to further computations. Merging and smoothing input and hidden graph
information, as in Figure 6c, can use LSTM- or GRU-style gating schemes, instead of simple
concatenation (Li et al., 2016). Or distinct, recurrent GN blocks (e.g. Figure 4b) can be composed
before and/or after other GN blocks, to improve stability in the representations over multiple
propagation steps (Sanchez-Gonzalez et al., 2018).
4.4 Implementing graph networks in code
Similar to CNNs (see Figure 1), which are naturally parallelizable (e.g. on GPUs), GNs have a
natural parallel structure: since the eandvfunctions in Equation 1 are shared over the edges
and nodes, respectively, they can be computed in parallel. In practice, this means that with respect
20Box 4: Graph Nets open-source software library: github.com/deepmind/graph nets
We have released an open-source library for building GNs in Tensorow/Sonnet. It includes
demos of how to create, manipulate, and train GNs to reason about graph-structured data, on
a shortest path-nding task, a sorting task, and a physical prediction task. Each demo uses the
same GN architecture, which highlights the exibility of the approach.
Shortest path demo: tinyurl.com/gn-shortest-path-demo
This demo creates random graphs, and trains a GN to label the nodes and edges on the shortest
path between any two nodes. Over a sequence of message-passing steps (as depicted by each
step's plot), the model renes its prediction of the shortest path.
Sort demo: tinyurl.com/gn-sort-demo
This demo creates lists of random numbers, and trains a GN to sort the list. After a sequence
of message-passing steps, the model makes an accurate prediction of which elements (columns
in the gure) come next after each other (rows).
Physics demo: tinyurl.com/gn-physics-demo
This demo creates random mass-spring physical systems, and trains a GN to predict the state of
the system on the next timestep. The model's next-step predictions can be fed back in as input
to create a rollout of a future trajectory. Each subplot below shows the true and predicted
mass-spring system states over 50 timesteps. This is similar to the model and experiments in
(Battaglia et al., 2016)'s \interaction networks".
21toeandv, the nodes and edges can be treated like the batch dimension in typical mini-batch
training regimes. Moreover, several graphs can be naturally batched together by treating them as
disjoint components of a larger graph. With some additional bookkeeping, this allows batching
together the computations made on several independent graphs.
Reusingeandvalso improves GNs' sample eciency. Again, analogous to a convolutional
kernel, the number of samples which are used to optimize a GN's eandvfunctions is the number
of edges and nodes, respectively, across all training graphs. For example, in the balls example
from Sec. 3.2, a scene with four balls which are all connected by springs will provide twelve (4 3)
examples of the contact interaction between them.
We have released an open-source software library for building GNs, which can be found here:
github.com/deepmind/graph nets . See Box 4 for an overview.
4.5 Summary
In this section, we have discussed the design principles behind graph networks: exible representa-
tions, congurable within-block structure, and composable multi-block architectures. These three
design principles combine in our framework which is extremely exible and applicable to a wide range
of domains ranging from perception, language, and symbolic reasoning. And, as we will see in the
remainder of this paper, the strong relational inductive bias possessed by graph networks supports
combinatorial generalization, thus making it a powerful tool both in terms of implementation and
theory.
5 Discussion
In this paper, we analyzed the extent to which relational inductive bias exists in deep learning
architectures like MLPs, CNNs, and RNNs, and concluded that while CNNs and RNNs do contain
relational inductive biases, they cannot naturally handle more structured representations such as
sets or graphs. We advocated for building stronger relational inductive biases into deep learning
architectures by highlighting an underused deep learning building block called a graph network ,
which performs computations over graph-structured data. Our graph network framework unies
existing approaches that also operate over graphs, and provides a straightforward interface for
assembling graph networks into complex, sophisticated architectures.
5.1 Combinatorial generalization in graph networks
The structure of GNs naturally supports combinatorial generalization because they do not perform
computations strictly at the system level, but also apply shared computations across the entities and
across the relations as well. This allows never-before-seen systems to be reasoned about, because
they are built from familiar components, in a way that reects von Humboldt's \innite use of nite
means" (Humboldt, 1836; Chomsky, 1965).
A number of studies have explored GNs' capacity for combinatorial generalization. Battaglia
et al. (2016) found that GNs trained to make one-step physical state predictions could simulate
thousands of future time steps, and also exhibit accurate zero-shot transfer to physical systems
with double, or half, the number of entities experienced during training. Sanchez-Gonzalez et al.
(2018) found similar results in more complex physical control settings, including that GNs trained as
forward models on simulated multi-joint agents could generalize to agents with new numbers of joints.
Hamrick et al. (2018) and Wang et al. (2018b) each found that GN-based decision-making policies
could transfer to novel numbers of entities as well. In combinatorial optimization problems, Bello
22et al. (2016); Nowak et al. (2017); Dai et al. (2017); Kool and Welling (2018) showed that GNs could
generalize well to problems of much dierent sizes than they had been trained on. Similarly, Toyer
et al. (2017) showed generalization to dierent sizes of planning problems, and Hamilton et al. (2017)
showed generalization to producing useful node embeddings for previously unseen data. On boolean
SAT problems, Selsam et al. (2018) demonstrated generalization both to dierent problem sizes and
across problem distributions: their model retained good performance upon strongly modifying the
distribution of the input graphs and its typical local structure.
These striking examples of combinatorial generalization are not entirely surprising, given GNs'
entity- and relation-centric organization, but nonetheless provide important support for the view
that embracing explicit structure and exible learning is a viable approach toward realizing better
sample eciency and generalization in modern AI.
5.2 Limitations of graph networks
One limitation of GNs' and MPNNs' form of learned message-passing (Shervashidze et al., 2011)
is that it cannot be guaranteed to solve some classes of problems, such as discriminating between
certain non-isomorphic graphs. Kondor et al. (2018) suggested that covariance7(Cohen and Welling,
2016; Kondor and Trivedi, 2018), rather than invariance to permutations of the nodes and edges
is preferable, and proposed \covariant compositional networks" which can preserve structural
information, and allow it to be ignored only if desired.
More generally, while graphs are a powerful way of representing structure information, they
have limits. For example, notions like recursion, control ow, and conditional iteration are not
straightforward to represent with graphs, and, minimally, require additional assumptions (e.g., in
interpreting abstract syntax trees). Programs and more \computer-like" processing can oer greater
representational and computational expressivity with respect to these notions, and some have argued
they are an important component of human cognition (Tenenbaum et al., 2011; Lake et al., 2015;
Goodman et al., 2015).
5.3 Open questions
Although we are excited about the potential impacts that graph networks can have, we caution that
these models are only one step forward. Realizing the full potential of graph networks will likely be
far more challenging than organizing their behavior under one framework, and indeed, there are a
number of unanswered questions regarding the best ways to use graph networks.
One pressing question is: where do the graphs come from that graph networks operate over?
One of the hallmarks of deep learning has been its ability to perform complex computations over
raw sensory data, such as images and text, yet it is unclear the best ways to convert sensory data
into more structured representations like graphs. One approach (which we have already discussed)
assumes a fully connected graph structure between spatial or linguistic entities, such as in the
literature on self-attention (Vaswani et al., 2017; Wang et al., 2018c). However, such representations
may not correspond exactly to the \true" entities (e.g., convolutional features do not directly
correspond to objects in a scene). Moreover, many underlying graph structures are much more
sparse than a fully connected graph, and it is an open question how to induce this sparsity. Several
lines of active research are exploring these issues (Watters et al., 2017; van Steenkiste et al., 2018;
Li et al., 2018; Kipf et al., 2018) but as of yet there is no single method which can reliably extract
discrete entities from sensory data. Developing such a method is an exciting challenge for future
7Covariance means, roughly, that the activations vary in a predictable way as a function of the ordering of the
incoming edges.
23research, and once solved will likely open the door for much more powerful and exible reasoning
algorithms.
A related question is how to adaptively modify graph structures during the course of computation.
For example, if an object fractures into multiple pieces, a node representing that object also ought
to split into multiple nodes. Similarly, it might be useful to only represent edges between objects
that are in contact, thus requiring the ability to add or remove edges depending on context. The
question of how to support this type of adaptivity is also actively being researched, and in particular,
some of the methods used for identifying the underlying structure of a graph may be applicable (e.g.
Li et al., 2018; Kipf et al., 2018).
Human cognition makes the strong assumption that the world is composed of objects and
relations (Spelke and Kinzler, 2007), and because GNs make a similar assumption, their behavior
tends to be more interpretable. The entities and relations that GNs operate over often correspond
to things that humans understand (such as physical objects), thus supporting more interpretable
analysis and visualization (e.g., as in Selsam et al., 2018). An interesting direction for future work
is to further explore the interpretability of the behavior of graph networks.
5.4 Integrative approaches for learning and structure
While our focus here has been on graphs, one takeaway from this paper is less about graphs
themselves and more about the approach of blending powerful deep learning approaches with
structured representations. We are excited by related approaches which have explored this idea for
other types of structured representations and computations, such as linguistic trees (Socher et al.,
2011a,b, 2012, 2013; Tai et al., 2015; Andreas et al., 2016), partial tree traversals in a state-action
graph (Guez et al., 2018; Farquhar et al., 2018), hierarchical action policies (Andreas et al., 2017),
multi-agent communication channels (Foerster et al., 2016), \capsules" (Sabour et al., 2017), and
programs (Parisotto et al., 2017). Other methods have attempted to capture dierent types of
structure by mimicking key hardware and software components in computers and how they transfer
information between each other, such as persistent slotted storage, registers, memory I/O controllers,
stacks, and queues (e.g. Dyer et al., 2015; Grefenstette et al., 2015; Joulin and Mikolov, 2015;
Sukhbaatar et al., 2015; Kurach et al., 2016; Graves et al., 2016).
5.5 Conclusion
Recent advances in AI, propelled by deep learning, have been transformative across many important
domains. Despite this, a vast gap between human and machine intelligence remains, especially with
respect to ecient, generalizable learning. We argue for making combinatorial generalization a top
priority for AI, and advocate for embracing integrative approaches which draw on ideas from human
cognition, traditional computer science, standard engineering practice, and modern deep learning.
Here we explored exible learning-based approaches which implement strong relational inductive
biases to capitalize on explicitly structured representations and computations, and presented a
framework called graph networks , which generalize and extend various recent approaches for neural
networks applied to graphs. Graph networks are designed to promote building complex architectures
using customizable graph-to-graph building blocks, and their relational inductive biases promote
combinatorial generalization and improved sample eciency over other standard machine learning
building blocks.
Despite their benets and potential, however, learnable models which operate on graphs are
only a stepping stone on the path toward human-like intelligence. We are optimistic about a
number of other relevant, and perhaps underappreciated, research directions, including marrying
24learning-based approaches with programs (Ritchie et al., 2016; Andreas et al., 2016; Gaunt et al.,
2016; Evans and Grefenstette, 2018; Evans et al., 2018), developing model-based approaches with an
emphasis on abstraction (Kansky et al., 2017; Konidaris et al., 2018; Zhang et al., 2018; Hay et al.,
2018), investing more heavily in meta-learning (Wang et al., 2016, 2018a; Finn et al., 2017), and
exploring multi-agent learning and interaction as a key catalyst for advanced intelligence (Nowak,
2006; Ohtsuki et al., 2006). These directions each involve rich notions of entities, relations, and
combinatorial generalization, and can potentially benet, and benet from, greater interaction with
approaches for learning relational reasoning over explicitly structured representations.
Acknowledgements
We thank Tobias Pfa, Danilo Rezende, Nando de Freitas, Murray Shanahan, Thore Graepel, John
Jumper, Demis Hassabis, and the broader DeepMind and Google communities for valuable feedback
and support.
References
Allamanis, M., Brockschmidt, M., and Khademi, M. (2018). Learning to represent programs with
graphs. In Proceedings of the International Conference on Learning Representations (ICLR) .
Allamanis, M., Chanthirasegaran, P., Kohli, P., and Sutton, C. (2017). Learning continuous
semantic representations of symbolic expressions. In Proceedings of the International Conference
on Machine Learning (ICML) .
Anderson, J. R. (1982). Acquisition of cognitive skill. Psychological Review , 89(4):369.
Andreas, J., Klein, D., and Levine, S. (2017). Modular multitask reinforcement learning with policy
sketches. In Proceedings of the International Conference on Machine Learning (ICML) .
Andreas, J., Rohrbach, M., Darrell, T., and Klein, D. (2016). Neural module networks. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 39{48.
Ba, J. L., Kiros, J. R., and Hinton, G. E. (2016). Layer normalization. arXiv preprint
arXiv:1607.06450 .
Bahdanau, D., Cho, K., and Bengio, Y. (2015). Neural machine translation by jointly learning to
align and translate. In Proceedings of the International Conference on Learning Representations
(ICLR) .
Battaglia, P., Pascanu, R., Lai, M., Rezende, D. J., et al. (2016). Interaction networks for learning
about objects, relations and physics. In Advances in Neural Information Processing Systems ,
pages 4502{4510.
Battaglia, P. W., Hamrick, J. B., and Tenenbaum, J. B. (2013). Simulation as an engine of physical
scene understanding. Proceedings of the National Academy of Sciences , 110(45):18327{18332.
Bello, I., Pham, H., Le, Q. V., Norouzi, M., and Bengio, S. (2016). Neural combinatorial optimization
with reinforcement learning. arXiv preprint arXiv:1611.09940 .
Bobrow, D. G. and Hinton, G. E., editors (1990). Articial Intelligence , volume 46. Elsevier Science
Publishers Ltd., Essex, UK. Special Issue 1-2: On Connectionist Symbol Processing.
25Bojchevski, A., Shchur, O., Z ugner, D., and G unnemann, S. (2018). Netgan: Generating graphs via
random walks. arXiv preprint arXiv:1803.00816 .
Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., and Yakhnenko, O. (2013). Translating
embeddings for modeling multi-relational data. In Advances in Neural Information Processing
Systems , pages 2787{2795.
Botvinick, M. M. (2008). Hierarchical models of behavior and prefrontal function. Trends in
Cognitive Sciences , 12(5):201{208.
Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., and Vandergheynst, P. (2017). Geometric deep
learning: going beyond euclidean data. IEEE Signal Processing Magazine , 34(4):18{42.
Bruna, J., Zaremba, W., Szlam, A., and LeCun, Y. (2014). Spectral networks and locally connected
networks on graphs. In Proceedings of the International Conference on Learning Representations
(ICLR) .
Chang, M. B., Ullman, T., Torralba, A., and Tenenbaum, J. B. (2017). A compositional object-
based approach to learning physical dynamics. In Proceedings of the International Conference on
Learning Representations (ICLR) .
Chen, X., Li, L., Fei-Fei, L., and Gupta, A. (2018a). Iterative visual reasoning beyond convolutions.
InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .
Chen, X., Liu, C., and Song, D. (2018b). Tree-to-tree neural networks for program translation. In
Workshops of the International Conference on Learning Representations (ICLR) .
Cho, K., Van Merri enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio,
Y. (2014). Learning phrase representations using rnn encoder-decoder for statistical machine
translation. In Proceeding of the Conference on Empirical Methods in Natural Language Processing
(EMNLP) .
Chomsky, N. (1957). Syntactic Structures . Mouton & Co.
Chomsky, N. (1965). Aspects of the Theory of Syntax . MIT Press.
Cohen, T. and Welling, M. (2016). Group equivariant convolutional networks. In International
Conference on Machine Learning , pages 2990{2999.
Craik, K. J. W. (1943). The Nature of Explanation . Cambridge University Press.
Cui, Z., Henrickson, K., Ke, R., and Wang, Y. (2018). High-order graph convolutional recurrent
neural network: A deep learning framework for network-scale trac learning and forecasting.
arXiv preprint arXiv:1802.07007 .
Dai, H., Dai, B., and Song, L. (2016). Discriminative embeddings of latent variable models for
structured data. In Proceedings of the International Conference on Machine Learning (ICML) .
Dai, H., Khalil, E. B., Zhang, Y., Dilkina, B., and Song, L. (2017). Learning combinatorial
optimization algorithms over graphs. In Advances in Neural Information Processing Systems .
De Cao, N. and Kipf, T. (2018). MolGAN: An implicit generative model for small molecular graphs.
arXiv preprint arXiv:1805.11973 .
26Deerrard, M., Bresson, X., and Vandergheynst, P. (2016). Convolutional neural networks on graphs
with fast localized spectral ltering. In Advances in Neural Information Processing Systems , pages
3844{3852.
Denil, M., Colmenarejo, S. G., Cabi, S., Saxton, D., and de Freitas, N. (2017). Programmable
agents. arXiv preprint arXiv:1706.06383 .
Devlin, J., Uesato, J., Singh, R., and Kohli, P. (2017). Semantic code repair using neuro-symbolic
transformation networks. arXiv preprint arXiv:1710.11054 .
Duvenaud, D. K., Maclaurin, D., Iparraguirre, J., Bombarell, R., Hirzel, T., Aspuru-Guzik, A., and
Adams, R. P. (2015). Convolutional networks on graphs for learning molecular ngerprints. In
Advances in Neural Information Processing Systems , pages 2224{2232.
Dyer, C., Ballesteros, M., Ling, W., Matthews, A., and Smith, N. A. (2015). Transition-based
dependency parsing with stack long short-term memory. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics (ACL) .
D zeroski, S., De Raedt, L., and Driessens, K. (2001). Relational reinforcement learning. Machine
Learning , 43(1-2):7{52.
Edwards, H. and Storkey, A. (2016). Towards a neural statistician. arXiv preprint arXiv:1606.02185 .
Eliasmith, C. (2013). How to build a brain: A neural architecture for biological cognition . Oxford
University Press.
Elman, J. L. (1990). Finding structure in time. Cognitive Science , 14(2):179{211.
Elman, J. L. (1991). Distributed representations, simple recurrent networks, and grammatical
structure. Machine Learning , 7(2-3):195{225.
Eslami, S. A., Heess, N., Weber, T., Tassa, Y., Szepesvari, D., Hinton, G. E., et al. (2016). Attend,
infer, repeat: Fast scene understanding with generative models. In Advances in Neural Information
Processing Systems , pages 3225{3233.
Evans, R. and Grefenstette, E. (2018). Learning explanatory rules from noisy data. Journal of
Articial Intelligence Research , 61:1{64.
Evans, R., Saxton, D., Amos, D., Kohli, P., and Grefenstette, E. (2018). Can neural networks
understand logical entailment? In Proceedings of the International Conference on Learning
Representations (ICLR) .
Farquhar, G., Rockt aschel, T., Igl, M., and Whiteson, S. (2018). TreeQN and ATreeC: Dierentiable
tree planning for deep reinforcement learning. In Proceedings of the International Conference on
Learning Representations (ICLR) .
Finn, C., Abbeel, P., and Levine, S. (2017). Model-agnostic meta-learning for fast adaptation of
deep networks. arXiv preprint arXiv:1703.03400 .
Fodor, J. A. (1975). The Language of Thought . Harvard University Press.
Fodor, J. A. and Pylyshyn, Z. W. (1988). Connectionism and cognitive architecture: A critical
analysis. Cognition , 28(1-2):3{71.
27Foerster, J., Assael, I. A., de Freitas, N., and Whiteson, S. (2016). Learning to communicate with
deep multi-agent reinforcement learning. In Advances in Neural Information Processing Systems ,
pages 2137{2145.
Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of
pattern recognition unaected by shift in position. Biological Cybernetics , 36:193{202.
Garcia, V. and Bruna, J. (2018). Few-shot learning with graph neural networks. In Proceedings of
the International Conference on Learning Representations (ICLR) .
Garc a-Dur an, A. and Niepert, M. (2017). Learning graph representations with embedding propaga-
tion. arXiv preprint arXiv:1710.03059 .
Garnelo, M., Arulkumaran, K., and Shanahan, M. (2016). Towards deep symbolic reinforcement
learning. arXiv preprint arXiv:1609.05518 .
Gaunt, A. L., Brockschmidt, M., Kushman, N., and Tarlow, D. (2016). Dierentiable programs
with neural libraries. arXiv preprint arXiv:1611.02109 .
Geman, S., Bienenstock, E., and Doursat, R. (1992). Neural networks and the bias/variance dilemma.
Neural Computation , 4(1):1{58.
Gentner, D. and Markman, A. B. (1997). Structure mapping in analogy and similarity. American
Psychologist , 52(1):45.
Getoor, L. and Taskar, B. (2007). Introduction to Statistical Relational Learning . MIT press.
Ghahramani, Z. (2015). Probabilistic machine learning and articial intelligence. Nature ,
521(7553):452.
Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. (2017). Neural message
passing for quantum chemistry. arXiv preprint arXiv:1704.01212 .
Goodfellow, I., Bengio, Y., Courville, A., and Bengio, Y. (2016). Deep Learning . MIT Press.
Goodman, N. (1955). The new riddle of induction. In Fact, Fiction, and Forecast , pages 59{83.
Harvard University Press.
Goodman, N., Mansinghka, V., Roy, D. M., Bonawitz, K., and Tenenbaum, J. B. (2012). Church: a
language for generative models. arXiv preprint arXiv:1206.3255 .
Goodman, N. D., Tenenbaum, J. B., and Gerstenberg, T. (2015). Concepts in a probabilistic
language of thought. In Margolis, E. and Laurence, S., editors, The Conceptual Mind: New
Directions in the Study of Concepts . MIT Press.
Goodwin, G. P. and Johnson-Laird, P. (2005). Reasoning about relations. Psychological Review ,
112(2):468.
Gori, M., Monfardini, G., and Scarselli, F. (2005). A new model for learning in graph domains. In
Proceedings of the International Joint Conference on Neural Networks (IJCNN) , volume 2, pages
729{734. IEEE.
Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwi nska, A., Colmenarejo,
S. G., Grefenstette, E., Ramalho, T., Agapiou, J., et al. (2016). Hybrid computing using a neural
network with dynamic external memory. Nature , 538(7626):471.
28Grefenstette, E., Hermann, K. M., Suleyman, M., and Blunsom, P. (2015). Learning to transduce
with unbounded memory. In Advances in Neural Information Processing Systems , pages 1828{1836.
Griths, T. L., Chater, N., Kemp, C., Perfors, A., and Tenenbaum, J. B. (2010). Probabilistic
models of cognition: Exploring representations and inductive biases. Trends in Cognitive Sciences ,
14(8):357{364.
Grover, A. and Leskovec, J. (2016). node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining ,
pages 855{864. ACM.
Guez, A., Weber, T., Antonoglou, I., Simonyan, K., Vinyals, O., Wierstra, D., Munos, R., and
Silver, D. (2018). Learning to search with MCTSnets. arXiv preprint arXiv:1802.04697 .
Gulcehre, C., Denil, M., Malinowski, M., Razavi, A., Pascanu, R., Hermann, K. M., Battaglia, P.,
Bapst, V., Raposo, D., Santoro, A., and de Freitas, N. (2018). Hyperbolic attention networks.
arXiv preprint arXiv:1805.09786 .
Hamaguchi, T., Oiwa, H., Shimbo, M., and Matsumoto, Y. (2017). Knowledge transfer for out-of-
knowledge-base entities: A graph neural network approach. In Proceedings of the International
Joint Conference on Articial Intelligence (IJCAI) .
Hamilton, W., Ying, Z., and Leskovec, J. (2017). Inductive representation learning on large graphs.
InAdvances in Neural Information Processing Systems , pages 1025{1035.
Hamrick, J., Allen, K., Bapst, V., Zhu, T., McKee, K., Tenenbaum, J., and Battaglia, P. (2018).
Relational inductive bias for physical construction in humans and machines. In Proceedings of the
40th Annual Conference of the Cognitive Science Society .
Hamrick, J. B., Ballard, A. J., Pascanu, R., Vinyals, O., Heess, N., and Battaglia, P. W. (2017).
Metacontrol for adaptive imagination-based optimization. In Proceedings of the International
Conference on Learning Representations (ICLR) .
Hartford, J., Graham, D. R., Leyton-Brown, K., and Ravanbakhsh, S. (2018). Deep models of
interactions across sets. arXiv preprint arXiv:1803.02879 .
Hay, N., Stark, M., Schlegel, A., Wendelken, C., Park, D., Purdy, E., Silver, T., Phoenix, D. S.,
and George, D. (2018). Behavior is everything{towards representing concepts with sensorimotor
contingencies. In Proceedings of the AAAI Conference on Articial Intelligence (AAAI) .
Hena, M., Bruna, J., and LeCun, Y. (2015). Deep convolutional networks on graph-structured
data. arXiv preprint arXiv:1506.05163 .
Hinton, G. E. (1990). Mapping part-whole hierarchies into connectionist networks. Articial
Intelligence , 46(1-2):47{75.
Hjort, N. L., Holmes, C., M uller, P., and Walker, S. G. (2010). Bayesian Nonparametrics . Cambridge
University Press.
Hoshen, Y. (2017). Vain: Attentional multi-agent predictive modeling. In Advances in Neural
Information Processing Systems , pages 2698{2708.
Hu, H., Gu, J., Zhang, Z., Dai, J., and Wei, Y. (2017). Relation networks for object detection.
arXiv preprint arXiv:1711.11575 .
29Hudson, D. A. and Manning, C. D. (2018). Compositional attention networks for machine reasoning.
InProceedings of the International Conference on Learning Representations (ICLR) .
Humboldt, W. (1999/1836). On Language: On the diversity of human language construction and its
inuence on the mental development of the human species . Cambridge University Press.
Hummel, J. E. and Holyoak, K. J. (2003). A symbolic-connectionist theory of relational inference
and generalization. Psychological Review , 110(2):220.
Ioe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In Proceedings of the 32nd International Conference on International
Conference on Machine Learning (ICML) .
Johnson, D. D. (2017). Learning graphical state transitions. Proceedings of the International
Conference on Learning Representations (ICLR) .
Joulin, A. and Mikolov, T. (2015). Inferring algorithmic patterns with stack-augmented recurrent
nets. In Advances in Neural Information Processing Systems , pages 190{198.
Kansky, K., Silver, T., M ely, D. A., Eldawy, M., L azaro-Gredilla, M., Lou, X., Dorfman, N., Sidor,
S., Phoenix, S., and George, D. (2017). Schema networks: Zero-shot transfer with a generative
causal model of intuitive physics. In Proceedings of the International Conference on Machine
Learning (ICML) .
Kearnes, S., McCloskey, K., Berndl, M., Pande, V., and Riley, P. (2016). Molecular graph
convolutions: moving beyond ngerprints. Journal of computer-aided molecular design , 30(8):595{
608.
Kemp, C. and Tenenbaum, J. B. (2008). The discovery of structural form. Proceedings of the
National Academy of Sciences , 105(31):10687{10692.
Kipf, T., Fetaya, E., Wang, K.-C., Welling, M., and Zemel, R. (2018). Neural relational inference
for interacting systems. In Proceedings of the International Conference on Machine Learning
(ICML) .
Kipf, T. N. and Welling, M. (2017). Semi-supervised classication with graph convolutional networks.
InProceedings of the International Conference on Learning Representations (ICLR) .
Koller, D. and Friedman, N. (2009). Probabilistic Graphical Models: Principles and Techniques .
MIT press.
Kondor, R., Son, H. T., Pan, H., Anderson, B., and Trivedi, S. (2018). Covariant compositional
networks for learning graphs. arXiv preprint arXiv:1801.02144 .
Kondor, R. and Trivedi, S. (2018). On the generalization of equivariance and convolution in neural
networks to the action of compact groups. arXiv preprint arXiv:1802.03690 .
Konidaris, G., Kaelbling, L. P., and Lozano-Perez, T. (2018). From skills to symbols: Learning
symbolic representations for abstract high-level planning. Journal of Articial Intelligence
Research , 61:215{289.
Kool, W. and Welling, M. (2018). Attention solves your TSP. arXiv preprint arXiv:1803.08475 .
30Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet classication with deep convolu-
tional neural networks. In Advances in Neural Information Processing Systems , pages 1097{1105.
Kurach, K., Andrychowicz, M., and Sutskever, I. (2016). Neural random-access machines. In
Proceedings of the International Conference on Learning Representations (ICLR) .
Lake, B. M. and Baroni, M. (2018). Still not systematic after all these years: On the compositional
skills of sequence-to-sequence recurrent networks. In Proceedings of the International Conference
on Machine Learning (ICML) .
Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning
through probabilistic program induction. Science , 350(6266):1332{1338.
Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. (2017). Building machines that
learn and think like people. Behavioral and Brain Sciences , 40.
LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature , 521(7553):436.
LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel,
L. D. (1989). Backpropagation applied to handwritten zip code recognition. Neural computation ,
1(4):541{551.
Li, Y., Tarlow, D., Brockschmidt, M., and Zemel, R. (2016). Gated graph sequence neural networks.
InProceedings of the International Conference on Learning Representations (ICLR) .
Li, Y., Vinyals, O., Dyer, C., Pascanu, R., and Battaglia, P. (2018). Learning deep generative
models of graphs. In Workshops at the International Conference on Learning Representations
(ICLR) .
Li, Y., Yu, R., Shahabi, C., and Liu, Y. (2017). Diusion convolutional recurrent neural network:
Data-driven trac forecasting. arXiv preprint arXiv:1707.01926 .
Lin, Z., Feng, M., Santos, C. N. d., Yu, M., Xiang, B., Zhou, B., and Bengio, Y. (2017). A structured
self-attentive sentence embedding. In Proceedings of the International Conference on Learning
Representations (ICLR) .
Liu, H., Simonyan, K., Vinyals, O., Fernando, C., and Kavukcuoglu, K. (2018). Hierarchical
representations for ecient architecture search. In Proceedings of the International Conference on
Learning Representations (ICLR) .
Luong, M.-T., Pham, H., and Manning, C. D. (2015). Eective approaches to attention-based neural
machine translation. arXiv preprint arXiv:1508.04025 .
Marcus, G. (2001). The algebraic mind.
Marcus, G. (2018a). Deep learning: A critical appraisal. arXiv preprint arXiv:1801.00631 .
Marcus, G. (2018b). Innateness, alphazero, and articial intelligence. arXiv preprint
arXiv:1801.05667 .
McClelland, J. L. (1994). The interaction of nature and nurture in development: A parallel
distributed processing perspective. International perspectives on psychological science , 1:57{88.
31McClelland, J. L. and Rumelhart, D. E. (1981). An interactive activation model of context eects
in letter perception: I. an account of basic ndings. Psychological Review , 88(5):375.
Mikolov, T., Yih, W.-t., and Zweig, G. (2013). Linguistic regularities in continuous space word
representations. In Proceedings of the 2013 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies , pages 746{751.
Mitchell, T. M. (1980). The need for biases in learning generalizations . Department of Computer
Science, Laboratory for Computer Science Research, Rutgers Univ. New Jersey.
Mnih, V., Heess, N., Graves, A., et al. (2014). Recurrent models of visual attention. In Advances in
neural information processing systems , pages 2204{2212.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A.,
Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. (2015). Human-level control through deep
reinforcement learning. Nature , 518(7540):529.
Monti, F., Boscaini, D., Masci, J., Rodola, E., Svoboda, J., and Bronstein, M. M. (2017). Geometric
deep learning on graphs and manifolds using mixture model cnns. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) .
Morav c k, M., Schmid, M., Burch, N., Lis y, V., Morrill, D., Bard, N., Davis, T., Waugh, K.,
Johanson, M., and Bowling, M. (2017). Deepstack: Expert-level articial intelligence in heads-up
no-limit poker. Science , 356(6337):508{513.
Narayanan, A., Chandramohan, M., Chen, L., Liu, Y., and Saminathan, S. (2016). subgraph2vec:
Learning distributed representations of rooted sub-graphs from large graphs. In Workshops at the
20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining .
Narayanan, A., Chandramohan, M., Venkatesan, R., Chen, L., Liu, Y., and Jaiswal, S. (2017).
graph2vec: Learning distributed representations of graphs. arXiv preprint arXiv:1707.05005 .
Navon, D. (1977). Forest before trees: The precedence of global features in visual perception.
Cognitive Psychology , 9(3):353{383.
Niepert, M., Ahmed, M., and Kutzkov, K. (2016). Learning convolutional neural networks for
graphs. In Proceedings of the International Conference on Machine Learning (ICML) , pages
2014{2023.
Nilsson, N. J. and Fikes, R. E. (1970). Strips: A new approach to the application of theorem proving
to problem solving. Technical report, SRI International, Menlo Park, CA Articial Intelligence
Center.
Nowak, A., Villar, S., Bandeira, A. S., and Bruna, J. (2017). A note on learning algorithms for
quadratic assignment with graph neural networks. In Proceedings of the Principled Approaches to
Deep Learning Workshop (PADL) at the International Conference of Machine Learning (ICML) .
Nowak, M. A. (2006). Five rules for the evolution of cooperation. science , 314(5805):1560{1563.
Ohtsuki, H., Hauert, C., Lieberman, E., and Nowak, M. A. (2006). A simple rule for the evolution
of cooperation on graphs and social networks. Nature , 441(7092):502.
32O~ noro-Rubio, D., Niepert, M., Garc a-Dur an, A., Gonz alez-S anchez, R., and L opez-Sastre,
R. J. (2017). Representation learning for visual-relational knowledge graphs. arXiv preprint
arXiv:1709.02314 .
Parisotto, E., Mohamed, A.-r., Singh, R., Li, L., Zhou, D., and Kohli, P. (2017). Neuro-symbolic
program synthesis. In Proceedings of the International Conference on Learning Representations
(ICLR) .
Pascanu, R., Li, Y., Vinyals, O., Heess, N., Buesing, L., Racani ere, S., Reichert, D., Weber, T.,
Wierstra, D., and Battaglia, P. (2017). Learning model-based planning from scratch. arXiv
preprint arXiv:1707.06170 .
Pearl, J. (1986). Fusion, propagation, and structuring in belief networks. Articial intelligence ,
29(3):241{288.
Pearl, J. (1988). Probabilistic reasoning in intelligent systems: Networks of plausible inference.
Morgan Kaufmann.
Pearl, J. (2009). Causality: Models, Reasoning and Inference . Cambridge University Press, New
York, NY, USA, 2nd edition.
Pearl, J. (2018). Theoretical impediments to machine learning with seven sparks from the causal
revolution. arXiv preprint arXiv:1801.04016 .
Pennington, J., Socher, R., and Manning, C. (2014). Glove: Global vectors for word representation.
InProceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) ,
pages 1532{1543.
Perozzi, B., Al-Rfou, R., and Skiena, S. (2014). Deepwalk: Online learning of social representations.
InProceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and
data mining , pages 701{710. ACM.
Pevn y, T. and Somol, P. (2017). Using neural network formalism to solve multiple-instance problems.
InInternational Symposium on Neural Networks , pages 135{142. Springer.
Pinker, S. and Prince, A. (1988). On language and connectionism: Analysis of a parallel distributed
processing model of language acquisition. Cognition , 28(1-2):73{193.
Plate, T. A. (1995). Holographic reduced representations. IEEE Transactions on Neural Networks ,
6(3):623{641.
Plaut, D. C., McClelland, J. L., Seidenberg, M. S., and Patterson, K. (1996). Understanding normal
and impaired word reading: computational principles in quasi-regular domains. Psychological
Review , 103(1):56.
Pollack, J. B. (1990). Recursive distributed representations. Articial Intelligence , 46(1-2):77{105.
Qi, C. R., Su, H., Mo, K., and Guibas, L. J. (2017). Pointnet: Deep learning on point sets for 3d
classication and segmentation. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) .
Raposo, D., Santoro, A., Barrett, D., Pascanu, R., Lillicrap, T., and Battaglia, P. (2017). Discovering
objects and their relations from entangled scene representations. In Workshops at the International
Conference on Learning Representations (ICLR) .
33Reed, S. and De Freitas, N. (2016). Neural programmer-interpreters. In Proceedings of the
International Conference on Learning Representations (ICLR) .
Ritchie, D., Horsfall, P., and Goodman, N. D. (2016). Deep amortized inference for probabilistic
programs. arXiv preprint arXiv:1610.05735 .
Rosenblatt, F. (1961). Principles of neurodynamics. perceptrons and the theory of brain mechanisms.
Technical report, Cornell Aeronautical Lab Inc., Bualo, NY.
Rumelhart, D. E., McClelland, J. L., Group, P. R., et al. (1987). Parallel Distributed Processing ,
volume 1. MIT Press.
Russell, S. J. and Norvig, P. (2009). Articial Intelligence: A Modern Approach (3rd Edition) .
Pearson.
Sabour, S., Frosst, N., and Hinton, G. E. (2017). Dynamic routing between capsules. In Advances
in Neural Information Processing Systems , pages 3859{3869.
Sanchez-Gonzalez, A., Heess, N., Springenberg, J. T., Merel, J., Riedmiller, M., Hadsell, R., and
Battaglia, P. (2018). Graph networks as learnable physics engines for inference and control. In
Proceedings of the 35th International Conference on Machine Learning (ICLR) .
Santoro, A., Raposo, D., Barrett, D. G., Malinowski, M., Pascanu, R., Battaglia, P., and Lillicrap,
T. (2017). A simple neural network module for relational reasoning. In Advances in Neural
Information Processing Systems .
Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G. (2009a). Computational
capabilities of graph neural networks. IEEE Transactions on Neural Networks , 20(1):81{102.
Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G. (2009b). The graph
neural network model. IEEE Transactions on Neural Networks , 20(1):61{80.
Scarselli, F., Yong, S. L., Gori, M., Hagenbuchner, M., Tsoi, A. C., and Maggini, M. (2005). Graph
neural networks for ranking web pages. In Proceedings of the 2005 IEEE/WIC/ACM International
Conference on Web Intelligence , pages 666{672. IEEE.
Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks , 61:85{117.
Selsam, D., Lamm, M., Bunz, B., Liang, P., de Moura, L., and Dill, D. L. (2018). Learning a sat
solver from single-bit supervision. arXiv preprint arXiv:1802.03685 .
Shalev-Shwartz, S., Shamir, O., and Shammah, S. (2017). Failures of gradient-based deep learning.
arXiv preprint arXiv:1703.07950 .
Shaw, P., Uszkoreit, J., and Vaswani, A. (2018). Self-attention with relative position representations.
InProceedings of the 16th Annual Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies .
Shervashidze, N., Schweitzer, P., Leeuwen, E. J. v., Mehlhorn, K., and Borgwardt, K. M. (2011).
Weisfeiler-lehman graph kernels. Journal of Machine Learning Research , 12(Sep):2539{2561.
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J.,
Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. (2016). Mastering the game of go with
deep neural networks and tree search. Nature , 529(7587):484{489.
34Smolensky, P. (1990). Tensor product variable binding and the representation of symbolic structures
in connectionist systems. Articial Intelligence , 46(1-2):159{216.
Socher, R., Huval, B., Manning, C. D., and Ng, A. Y. (2012). Semantic compositionality through
recursive matrix-vector spaces. In Proceedings of the Joint Conference on Empirical Methods in
Natural Language Processing (EMNLP) and Computational Natural Language Learning (CNLL) ,
pages 1201{1211. Association for Computational Linguistics.
Socher, R., Lin, C. C., Manning, C., and Ng, A. Y. (2011a). Parsing natural scenes and natural
language with recursive neural networks. In Proceedings of the 28th International Conference on
Machine Learning (ICML) , pages 129{136.
Socher, R., Pennington, J., Huang, E. H., Ng, A. Y., and Manning, C. D. (2011b). Semi-supervised
recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing (EMNLP) , pages 151{161. Association for
Computational Linguistics.
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., and Potts, C. (2013).
Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of
the Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 1631{1642.
Spelke, E. S., Breinlinger, K., Macomber, J., and Jacobson, K. (1992). Origins of knowledge.
Psychological review , 99(4):605.
Spelke, E. S. and Kinzler, K. D. (2007). Core knowledge. Developmental Science , 10(1):89{96.
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout:
A simple way to prevent neural networks from overtting. The Journal of Machine Learning
Research , 15(1):1929{1958.
Sukhbaatar, S., Fergus, R., et al. (2016). Learning multiagent communication with backpropagation.
InAdvances in Neural Information Processing Systems , pages 2244{2252.
Sukhbaatar, S., Weston, J., Fergus, R., et al. (2015). End-to-end memory networks. In Advances in
Neural Information Processing Systems , pages 2440{2448.
Sutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to sequence learning with neural networks.
InAdvances in Neural Information Processing Systems , pages 3104{3112.
Szegedy, C., Ioe, S., Vanhoucke, V., and Alemi, A. A. (2017). Inception-v4, inception-resnet
and the impact of residual connections on learning. In Proceedings of the AAAI Conference on
Articial Intelligence (AAAI) , volume 4, page 12.
Tai, K. S., Socher, R., and Manning, C. D. (2015). Improved semantic representations from
tree-structured long short-term memory networks. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL) .
Tang, J., Qu, M., Wang, M., Zhang, M., Yan, J., and Mei, Q. (2015). Line: Large-scale information
network embedding. In Proceedings of the 24th International Conference on World Wide Web ,
pages 1067{1077. International World Wide Web Conferences Steering Committee.
Tenenbaum, J. B., Griths, T. L., and Kemp, C. (2006). Theory-based bayesian models of inductive
learning and reasoning. Trends in Cognitive Sciences , 10(7):309{318.
35Tenenbaum, J. B., Kemp, C., Griths, T. L., and Goodman, N. D. (2011). How to grow a mind:
Statistics, structure, and abstraction. Science , 331(6022):1279{1285.
Toyer, S., Trevizan, F., Thiebaux, S., and Xie, L. (2017). Action schema networks: Generalised
policies with deep learning. In Proceedings of the AAAI Conference on Articial Intelligence
(AAAI) .
Ullman, T. D., Spelke, E., Battaglia, P., and Tenenbaum, J. B. (2017). Mind games: Game engines
as an architecture for intuitive physics. Trends in Cognitive Sciences , 21(9):649{665.
van Steenkiste, S., Chang, M., Gre, K., and Schmidhuber, J. (2018). Relational neural expectation
maximization: Unsupervised discovery of objects and their interactions. Proceedings of the
International Conference on Learning Representations (ICLR) .
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and
Polosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing
Systems .
Veli ckovi c, P., Cucurull, G., Casanova, A., Romero, A., Li o, P., and Bengio, Y. (2018). Graph
attention networks. In Proceedings of the International Conference on Learning Representations
(ICLR) .
Wang, J. X., Kurth-Nelson, Z., Kumaran, D., Tirumala, D., Soyer, H., Leibo, J. Z., Hassabis, D.,
and Botvinick, M. (2018a). Prefrontal cortex as a meta-reinforcement learning system. Nature
neuroscience , page 1.
Wang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z., Munos, R., Blundell, C.,
Kumaran, D., and Botvinick, M. (2016). Learning to reinforcement learn. arXiv preprint
arXiv:1611.05763 .
Wang, T., Liao, R., Ba, J., and Fidler, S. (2018b). Nervenet: Learning structured policy with graph
neural networks. In Proceedings of the International Conference on Learning Representations
(ICLR) .
Wang, X., Girshick, R., Gupta, A., and He, K. (2018c). Non-local neural networks. In Proceedings
of the Conference on Computer Vision and Pattern Recognition (CVPR) .
Wang, Y., Sun, Y., Liu, Z., Sarma, S. E., Bronstein, M. M., and Solomon, J. M. (2018d). Dynamic
graph cnn for learning on point clouds. arXiv preprint arXiv:1801.07829 .
Watters, N., Zoran, D., Weber, T., Battaglia, P., Pascanu, R., and Tacchetti, A. (2017). Visual
interaction networks: Learning a physics simulator from video. In Advances in Neural Information
Processing Systems , pages 4542{4550.
Wu, J., Lu, E., Kohli, P., Freeman, B., and Tenenbaum, J. (2017). Learning to see physics via
visual de-animation. In Advances in Neural Information Processing Systems , pages 152{163.
Yoon, K., Liao, R., Xiong, Y., Zhang, L., Fetaya, E., Urtasun, R., Zemel, R., and Pitkow, X.
(2018). Inference in probabilistic graphical models by graph neural networks. In Workshops at
the International Conference on Learning Representations (ICLR) .
You, J., Ying, R., Ren, X., Hamilton, W. L., and Leskovec, J. (2018). GraphRNN: A deep generative
model for graphs. arXiv preprint arXiv:1802.08773 .
36Yuille, A. L. and Liu, C. (2018). Deep nets: What have they ever done for vision? arXiv preprint
arXiv:1805.04025 .
Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., and Smola, A. J. (2017).
Deep sets. In Advances in Neural Information Processing Systems , pages 3394{3404.
Zambaldi, V., Raposo, D., Santoro, A., Bapst, V., Li, Y., Babuschkin, I., Tuyls, K., Reichert, D.,
Lillicrap, T., Lockhart, E., Shanahan, M., Langston, V., Pascanu, R., Botvinick, M., Vinyals, O.,
and Battaglia, P. (2018). Relational deep reinforcement learning. arXiv preprint arXiv .
Zhang, A., Lerer, A., Sukhbaatar, S., Fergus, R., and Szlam, A. (2018). Composable planning with
attributes. arXiv preprint arXiv:1803.00512 .
Z ugner, D., Akbarnejad, A., and G unnemann, S. (2018). Adversarial Attacks on Neural Networks
for Graph Data. arXiv preprint arXiv:1805.07984 .
37Appendix: Formulations of additional models
In this appendix we give more examples of how published networks can t in the frame dened by
Equation 1.
Interaction networks
Interaction Networks (Battaglia et al., 2016; Watters et al., 2017) and the Neural Physics Engine
Chang et al. (2017) use a full GN but for the absence of the global to update the edge properties:
e(ek;vrk;vsk;u):=fe(ek;vrk;vsk) = NNe([ek;vrk;vsk])
v 
 e0
i;vi;u:=fv 
 e0
i;vi;u
= NNv 
[ e0
i;vi;u]
e!v 
E0
i:= =X
fk:rk=ige0
k
That work also included an extension to the above formulation which output global, rather than
per-node, predictions:
e(ek;vrk;vsk;u):=fe(ek;vrk;vsk) = NNe([ek;vrk;vsk])
v 
 e0
i;vi;u:=fv 
 e0
i;vi;u
= NNv 
[ e0
i;vi;u]
u 
 e0; v0;u:=fu 
 v0;u
= NNu 
[ v0;u]
v!g 
V0:= =X
iv0
i
Non-pairwise interactions
Gated Graph Sequence Neural Networks (GGS-NN) (Li et al., 2016) use a slightly generalized
formulation where each edge has an attached type tk2f1;::;Tg, and the updates are:
e((ek;tk);vrk;vsk;u):=fe(ek;vsk) = NNe;tk(vsk)
v 
 e0
i;vi;u:=fv 
 e0
i;vi
= NNv 
[ e0
i;vi]
e!v 
E0
i:= =X
fk:rk=ige0
k
These updates are applied recurrently (the NNvis a GRU (Cho et al., 2014)), followed by a global
decoder which computes a weighted sum of embedded nal node states. Here each NNe;tkis a neural
network with specic parameters.
CommNet (Sukhbaatar et al., 2016) (in the slightly more general form described by (Hoshen,
2017)) uses:
e(ek;vrk;vsk;u):=fe(vsk) = NN e(vsk)
v 
 e0
i;vi;u:=fv 
 e0
i;vi
= NNv 
[ e0
i;NNv0(vi)]
e!v 
E0
i:= =1
jE0
ijX
fk:rk=ige0
k
38Attention-based approaches
The various attention-based approaches use a ewhich is factored into a scalar pairwise-interaction
function which returns the unnormalized attention term, denoted e(vrk;vsk)=a0
k, and a vector-
valued non-pairwise term, denoted e(vsk) =b0
k,
e(ek;vrk;vsk;u):=fe(vrk;vsk) = (e(vrk;vsk); e(vsk)) = (a0
k;b0
k) =e0
k
The single-headed self-attention (SA) in the Transformer architecture (Vaswani et al., 2017),
implements the non-local formulation as:
e(vrk;vsk) = exp (NN query(vrk)|NNkey(vsk))
e(vsk) = NN (vsk)
v 
 e0
i;vi;u:=fv 
 e0
i
= NNv 
 e0
i
where NNquery,NNkey, and NNare again neural network functions with dierent parameters and
possibly dierent architectures. They also use a multi-headed version which computes Nhparallel
 e0h
iusing dierent NNquery
h,NNkey
h,NNh, wherehindexes the dierent parameters. These are
passed tofvand concatenated:
fv
f e0h
igh=1:::Nh
= NNv
[ e01
i;:::; e0Nh
i]
Vertex Attention Interaction Networks (Hoshen, 2017) are very similar to single-headed SA,
but use Euclidean distance for the attentional similarity metric, with shared parameters across the
attention inputs' embeddings, and also use the input node feature in the node update function,
e(vrk;vsk) = exp 
 kNN(vrk) NN(vsk)k2
e(vsk) = NN (vsk)
v 
 e0
i;vi;u:=fv 
 e0
i
= NNv 
[ e0
i;vi]
Graph Attention Networks (Veli ckovi c et al., 2018) are also similar to multi-headed SA, but use
a neural network as the attentional similarity metric, with shared parameters across the attention
inputs' embeddings:
e(vrk;vsk) = exp (NN 0([NN(vrk);NN(vsk)))
e(vsk) = NN (vsk)
v 
 e0
i;vi;u:=fv
f e0h
igh=1:::Nh
= NNv
[ e01
i;:::; e0Nh
i]
Stretching beyond the specic non-local formulation, Shaw et al. (2018) extended multi-headed
SA with relative position encodings. \Relative" refers to an encoding of the spatial distance between
nodes in a sequence or other signal in a metric space. This can be expressed in GN language as an
edge attribute ek, and replacing the e(vsk) from multi-headed SA above with:
e(ek;vsk) = NNe(vsk) +ek
39Belief Propagation embeddings
Finally, we briey summarize how the general \structure2vec" algorithm of Dai et al. (2016) can t
into our framework. In order to do so, we need to slightly modify our main Equation 1, i.e.:
k=
felgsl=rk
rl6=sk
:=X
rl=sk
sl6=rkel
e0
k=e(k) :=f(k) = NN( k)
 e0
i= 
fe0
kgrk=i:=X
fk:rk=igek
v0
i=v 
 e0
i:=f( e0
i) = NN(  e0
i)
Edges' features now takes the meaning of \message" between their receiver and sender; note that
there is only one set of parameters to learn for both the edges and nodes updates.
40Diffusion Models Beat GANs on Image Synthesis
Prafulla Dhariwal
OpenAI
prafulla@openai.comAlex Nichol
OpenAI
alex@openai.com
Abstract
We show that diffusion models can achieve image sample quality superior to the
current state-of-the-art generative models. We achieve this on unconditional im-
age synthesis by ﬁnding a better architecture through a series of ablations. For
conditional image synthesis, we further improve sample quality with classiﬁer guid-
ance: a simple, compute-efﬁcient method for trading off diversity for ﬁdelity using
gradients from a classiﬁer. We achieve an FID of 2.97 on ImageNet 128 128,
4.59 on ImageNet 256 256, and 7.72 on ImageNet 512 512, and we match
BigGAN-deep even with as few as 25 forward passes per sample, all while main-
taining better coverage of the distribution. Finally, we ﬁnd that classiﬁer guidance
combines well with upsampling diffusion models, further improving FID to 3.94
on ImageNet 256256 and 3.85 on ImageNet 512 512. We release our code at
https://github.com/openai/guided-diffusion .
1 Introduction
Figure 1: Selected samples from our best ImageNet 512 512 model (FID 3.85)
Over the past few years, generative models have gained the ability to generate human-like natural
language [ 6], inﬁnite high-quality synthetic images [ 5,28,51] and highly diverse human speech and
music [ 64,13]. These models can be used in a variety of ways, such as generating images from text
prompts [ 72,50] or learning useful feature representations [ 14,7]. While these models are already
Equal contributionarXiv:2105.05233v4  [cs.LG]  1 Jun 2021capable of producing realistic images and sound, there is still much room for improvement beyond
the current state-of-the-art, and better generative models could have wide-ranging impacts on graphic
design, games, music production, and countless other ﬁelds.
GANs [ 19] currently hold the state-of-the-art on most image generation tasks [ 5,68,28] as measured
by sample quality metrics such as FID [ 23], Inception Score [ 54] and Precision [ 32]. However, some
of these metrics do not fully capture diversity, and it has been shown that GANs capture less diversity
than state-of-the-art likelihood-based models [ 51,43,42]. Furthermore, GANs are often difﬁcult to
train, collapsing without carefully selected hyperparameters and regularizers [5, 41, 4].
While GANs hold the state-of-the-art, their drawbacks make them difﬁcult to scale and apply to
new domains. As a result, much work has been done to achieve GAN-like sample quality with
likelihood-based models [ 51,25,42,9]. While these models capture more diversity and are typically
easier to scale and train than GANs, they still fall short in terms of visual sample quality. Furthermore,
except for V AEs, sampling from these models is slower than GANs in terms of wall-clock time.
Diffusion models are a class of likelihood-based models which have recently been shown to produce
high-quality images [ 56,59,25] while offering desirable properties such as distribution coverage,
a stationary training objective, and easy scalability. These models generate samples by gradually
removing noise from a signal, and their training objective can be expressed as a reweighted variational
lower-bound [ 25]. This class of models already holds the state-of-the-art [ 60] on CIFAR-10 [ 31], but
still lags behind GANs on difﬁcult generation datasets like LSUN and ImageNet. Nichol and Dhariwal
[43] found that these models improve reliably with increased compute, and can produce high-quality
samples even on the difﬁcult ImageNet 256 256 dataset using an upsampling stack. However, the
FID of this model is still not competitive with BigGAN-deep [5], the current state-of-the-art on this
dataset.
We hypothesize that the gap between diffusion models and GANs stems from at least two factors:
ﬁrst, that the model architectures used by recent GAN literature have been heavily explored and
reﬁned; second, that GANs are able to trade off diversity for ﬁdelity, producing high quality samples
but not covering the whole distribution. We aim to bring these beneﬁts to diffusion models, ﬁrst by
improving model architecture and then by devising a scheme for trading off diversity for ﬁdelity.
With these improvements, we achieve a new state-of-the-art, surpassing GANs on several different
metrics and datasets.
The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion
models based on Ho et al. [ 25] and the improvements from Nichol and Dhariwal [ 43] and Song
et al. [ 57], and we describe our evaluation setup. In Section 3, we introduce simple architecture
improvements that give a substantial boost to FID. In Section 4, we describe a method for using
gradients from a classiﬁer to guide a diffusion model during sampling. We ﬁnd that a single
hyperparameter, the scale of the classiﬁer gradients, can be tuned to trade off diversity for ﬁdelity,
and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial
examples [ 61]. Finally, in Section 5 we show that models with our improved architecture achieve
state-of-the-art on unconditional image synthesis tasks, and with classiﬁer guidance achieve state-of-
the-art on conditional image synthesis. When using classiﬁer guidance, we ﬁnd that we can sample
with as few as 25 forward passes while maintaining FIDs comparable to BigGAN. We also compare
our improved models to upsampling stacks, ﬁnding that the two approaches give complementary
improvements and that combining them gives the best results on ImageNet 256 256 and 512512.
2 Background
In this section, we provide a brief overview of diffusion models. For a more detailed mathematical
description, we refer the reader to Appendix B.
On a high level, diffusion models sample from a distribution by reversing a gradual noising process. In
particular, sampling starts with noise xTand produces gradually less-noisy samples xT 1;xT 2;:::
until reaching a ﬁnal sample x0. Each timestep tcorresponds to a certain noise level, and xtcan be
thought of as a mixture of a signal x0with some noise where the signal to noise ratio is determined
by the timestep t. For the remainder of this paper, we assume that the noise is drawn from a diagonal
Gaussian distribution, which works well for natural images and simpliﬁes various derivations.
2A diffusion model learns to produce a slightly more “denoised” xt 1fromxt. Ho et al. [ 25]
parameterize this model as a function (xt;t)which predicts the noise component of a noisy sample
xt. To train these models, each sample in a minibatch is produced by randomly drawing a data sample
x0, a timestep t, and noise, which together give rise to a noised sample xt(Equation 17). The
training objective is then jj(xt;t) jj2, i.e. a simple mean-squared error loss between the true
noise and the predicted noise (Equation 26).
It is not immediately obvious how to sample from a noise predictor (xt;t). Recall that diffusion
sampling proceeds by repeatedly predicting xt 1fromxt, starting from xT. Ho et al. [ 25] show
that, under reasonable assumptions, we can model the distribution p(xt 1jxt)ofxt 1givenxtas
a diagonal Gaussian N(xt 1;(xt;t);(xt;t)), where the mean (xt;t)can be calculated as a
function of(xt;t)(Equation 27). The variance (xt;t)of this Gaussian distribution can be ﬁxed
to a known constant [ 25] or learned with a separate neural network head [ 43], and both approaches
yield high-quality samples when the total number of diffusion steps Tis large enough.
Ho et al. [ 25] observe that the simple mean-sqaured error objective, Lsimple , works better in practice
than the actual variational lower bound Lvlbthat can be derived from interpreting the denoising diffu-
sion model as a V AE. They also note that training with this objective and using their corresponding
sampling procedure is equivalent to the denoising score matching model from Song and Ermon [ 58],
who use Langevin dynamics to sample from a denoising model trained with multiple noise levels to
produce high quality image samples. We often use “diffusion models” as shorthand to refer to both
classes of models.
2.1 Improvements
Following the breakthrough work of Song and Ermon [ 58] and Ho et al. [ 25], several recent papers
have proposed improvements to diffusion models. Here we describe a few of these improvements,
which we employ for our models.
Nichol and Dhariwal [ 43] ﬁnd that ﬁxing the variance (xt;t)to a constant as done in Ho et al.
[25] is sub-optimal for sampling with fewer diffusion steps, and propose to parameterize (xt;t)as
a neural network whose output vis interpolated as:
(xt;t) = exp(vlogt+ (1 v) log ~t) (1)
Here,tand~t(Equation 19) are the variances in Ho et al. [ 25] corresponding to upper and lower
bounds for the reverse process variances. Additionally, Nichol and Dhariwal [ 43] propose a hybrid
objective for training both (xt;t)and(xt;t)using the weighted sum Lsimple +Lvlb. Learning
the reverse process variances with their hybrid objective allows sampling with fewer steps without
much drop in sample quality. We adopt this objective and parameterization, and use it throughout our
experiments.
Song et al. [ 57] propose DDIM, which formulates an alternative non-Markovian noising process
that has the same forward marginals as DDPM, but allows producing different reverse samplers by
changing the variance of the reverse noise. By setting this noise to 0, they provide a way to turn any
model(xt;t)into a deterministic mapping from latents to images, and ﬁnd that this provides an
alternative way to sample with fewer steps. We adopt this sampling approach when using fewer than
50 sampling steps, since Nichol and Dhariwal [43] found it to be beneﬁcial in this regime.
2.2 Sample Quality Metrics
For comparing sample quality across models, we perform quantitative evaluations using the following
metrics. While these metrics are often used in practice and correspond well with human judgement,
they are not a perfect proxy, and ﬁnding better metrics for sample quality evaluation is still an open
problem.
Inception Score (IS) was proposed by Salimans et al. [ 54], and it measures how well a model captures
the full ImageNet class distribution while still producing individual samples that are convincing
examples of a single class. One drawback of this metric is that it does not reward covering the
whole distribution or capturing diversity within a class, and models which memorize a small subset
of the full dataset will still have high IS [ 3]. To better capture diversity than IS, Fréchet Inception
Distance (FID) was proposed by Heusel et al. [ 23], who argued that it is more consistent with human
3Channels Depth HeadsAttention BigGAN Rescale FID FID
resolutions up/downsample resblock 700K 1200K
160 2 1 16 7 7 15.33 13.21
128 4 -0.21 -0.48
4 -0.54 -0.82
32,16,8 -0.72 -0.66
3 -1.20 -1.21
3 0.16 0.25
160 2 4 32,16,8 3 7 -3.14 -3.00
Table 1: Ablation of various architecture changes, evaluated at 700K and 1200K iterations
judgement than Inception Score. FID provides a symmetric measure of the distance between two
image distributions in the Inception-V3 [ 62] latent space. Recently, sFID was proposed by Nash
et al. [ 42] as a version of FID that uses spatial features rather than the standard pooled features.
They ﬁnd that this metric better captures spatial relationships, rewarding image distributions with
coherent high-level structure. Finally, Kynkäänniemi et al. [ 32] proposed Improved Precision and
Recall metrics to separately measure sample ﬁdelity as the fraction of model samples which fall into
the data manifold (precision), and diversity as the fraction of data samples which fall into the sample
manifold (recall).
We use FID as our default metric for overall sample quality comparisons as it captures both diversity
and ﬁdelity and has been the de facto standard metric for state-of-the-art generative modeling work
[27,28,5,25]. We use Precision or IS to measure ﬁdelity, and Recall to measure diversity or
distribution coverage. When comparing against other methods, we re-compute these metrics using
public samples or models whenever possible. This is for two reasons: ﬁrst, some papers [ 27,28,
25] compare against arbitrary subsets of the training set which are not readily available; and second,
subtle implementation differences can affect the resulting FID values [ 45]. To ensure consistent
comparisons, we use the entire training set as the reference batch [ 23,5], and evaluate metrics for all
models using the same codebase.
3 Architecture Improvements
In this section we conduct several architecture ablations to ﬁnd the model architecture that provides
the best sample quality for diffusion models.
Ho et al. [ 25] introduced the UNet architecture for diffusion models, which Jolicoeur-Martineau
et al. [ 26] found to substantially improve sample quality over the previous architectures [ 58,33] used
for denoising score matching. The UNet model uses a stack of residual layers and downsampling
convolutions, followed by a stack of residual layers with upsampling colvolutions, with skip con-
nections connecting the layers with the same spatial size. In addition, they use a global attention
layer at the 1616 resolution with a single head, and add a projection of the timestep embedding into
each residual block. Song et al. [ 60] found that further changes to the UNet architecture improved
performance on the CIFAR-10 [ 31] and CelebA-64 [ 34] datasets. We show the same result on
ImageNet 128128, ﬁnding that architecture can indeed give a substantial boost to sample quality on
much larger and more diverse datasets at a higher resolution.
We explore the following architectural changes:
• Increasing depth versus width, holding model size relatively constant.
• Increasing the number of attention heads.
• Using attention at 32 32, 1616, and 88 resolutions rather than only at 16 16.
•Using the BigGAN [ 5] residual block for upsampling and downsampling the activations,
following [60].
• Rescaling residual connections with1p
2, following [60, 27, 28].
For all comparisons in this section, we train models on ImageNet 128 128 with batch size 256, and
sample using 250 sampling steps. We train models with the above architecture changes and compare
4Number of heads Channels per head FID
1 14.08
2 -0.50
4 -0.97
8 -1.17
32 -1.36
64 -1.03
128 -1.08
Table 2: Ablation of various attention conﬁgurations. More heads or lower channels per heads both
lead to improved FID.
40 60 80 100 120 140 160 180
time (hrs)14161820222426FIDch=128, res=4
ch=160, res=2
ch=160, res=2, heads=4
ch=160, res=2, multi-res attn
ch=160, res=2, biggan up/down
ch=160, res=2, skip rescale
ch=160, res=2, heads=4, multi-res attn, biggan up/down
20 40 60 80 100
time (hrs)1416182022242628FID1 head
2 heads
4 heads
8 heads
32 head channels
64 head channels
128 head channels
Figure 2: Ablation of various architecture changes, showing FID as a function of wall-clock time.
FID evaluated over 10k samples instead of 50k for efﬁciency.
Operation FID
AdaGN 13.06
Addition + GroupNorm 15.08
Table 3: Ablating the element-wise operation used when projecting timestep and class embeddings
into each residual block. Replacing AdaGN with the Addition + GroupNorm layer from Ho et al.
[25] makes FID worse.
them on FID, evaluated at two different points of training, in Table 1. Aside from rescaling residual
connections, all of the other modiﬁcations improve performance and have a positive compounding
effect. We observe in Figure 2 that while increased depth helps performance, it increases training
time and takes longer to reach the same performance as a wider model, so we opt not to use this
change in further experiments.
We also study other attention conﬁgurations that better match the Transformer architecture [ 66]. To
this end, we experimented with either ﬁxing attention heads to a constant, or ﬁxing the number of
channels per head. For the rest of the architecture, we use 128 base channels, 2 residual blocks
per resolution, multi-resolution attention, and BigGAN up/downsampling, and we train the models
for 700K iterations. Table 2 shows our results, indicating that more heads or fewer channels per
head improves FID. In Figure 2, we see 64 channels is best for wall-clock time, so we opt to use 64
channels per head as our default. We note that this choice also better matches modern transformer
architectures, and is on par with our other conﬁgurations in terms of ﬁnal FID.
53.1 Adaptive Group Normalization
We also experiment with a layer [ 43] that we refer to as adaptive group normalization (AdaGN), which
incorporates the timestep and class embedding into each residual block after a group normalization
operation [ 69], similar to adaptive instance norm [ 27] and FiLM [ 48]. We deﬁne this layer as
AdaGN (h;y) =ysGroupNorm (h)+yb, wherehis the intermediate activations of the residual block
following the ﬁrst convolution, and y= [ys;yb]is obtained from a linear projection of the timestep
and class embedding.
We had already seen AdaGN improve our earliest diffusion models, and so had it included by
default in all our runs. In Table 3, we explicitly ablate this choice, and ﬁnd that the adaptive group
normalization layer indeed improved FID. Both models use 128 base channels and 2 residual blocks
per resolution, multi-resolution attention with 64 channels per head, and BigGAN up/downsampling,
and were trained for 700K iterations.
In the rest of the paper, we use this ﬁnal improved model architecture as our default: variable width
with 2 residual blocks per resolution, multiple heads with 64 channels per head, attention at 32, 16 and
8 resolutions, BigGAN residual blocks for up and downsampling, and adaptive group normalization
for injecting timestep and class embeddings into residual blocks.
4 Classiﬁer Guidance
In addition to employing well designed architectures, GANs for conditional image synthesis [ 39,5]
make heavy use of class labels. This often takes the form of class-conditional normalization statistics
[16,11] as well as discriminators with heads that are explicitly designed to behave like classiﬁers
p(yjx)[40]. As further evidence that class information is crucial to the success of these models,
Lucic et al. [ 36] ﬁnd that it is helpful to generate synthetic labels when working in a label-limited
regime.
Given this observation for GANs, it makes sense to explore different ways to condition diffusion
models on class labels. We already incorporate class information into normalization layers (Section
3.1). Here, we explore a different approach: exploiting a classiﬁer p(yjx)to improve a diffusion
generator. Sohl-Dickstein et al. [ 56] and Song et al. [ 60] show one way to achieve this, wherein a
pre-trained diffusion model can be conditioned using the gradients of a classiﬁer. In particular, we
can train a classiﬁer p(yjxt;t)on noisy images xt, and then use gradients rxtlogp(yjxt;t)to
guide the diffusion sampling process towards an arbitrary class label y.
In this section, we ﬁrst review two ways of deriving conditional sampling processes using classiﬁers.
We then describe how we use such classiﬁers in practice to improve sample quality. We choose the
notationp(yjxt;t) =p(yjxt)and(xt;t) =(xt)for brevity, noting that they refer to separate
functions for each timestep tand at training time the models must be conditioned on the input t.
4.1 Conditional Reverse Noising Process
We start with a diffusion model with an unconditional reverse noising process p(xtjxt+1). To
condition this on a label y, it sufﬁces to sample each transition2according to
p;(xtjxt+1;y) =Zp(xtjxt+1)p(yjxt) (2)
whereZis a normalizing constant (proof in Appendix H). It is typically intractable to sample from
this distribution exactly, but Sohl-Dickstein et al. [ 56] show that it can be approximated as a perturbed
Gaussian distribution. Here, we review this derivation.
Recall that our diffusion model predicts the previous timestep xtfrom timestep xt+1using a Gaussian
distribution:
p(xtjxt+1) =N(;) (3)
logp(xtjxt+1) = 1
2(xt )T 1(xt ) +C (4)
2We must also sample xTconditioned on y, but a noisy enough diffusion process causes xTto be nearly
Gaussian even in the conditional case.
6Algorithm 1 Classiﬁer guided diffusion sampling, given a diffusion model ((xt);(xt)), classi-
ﬁerp(yjxt), and gradient scale s.
Input: class label y, gradient scale s
xT sample fromN(0;I)
for alltfromTto 1do
; (xt);(xt)
xt 1 sample fromN(+srxtlogp(yjxt);)
end for
returnx0
Algorithm 2 Classiﬁer guided DDIM sampling, given a diffusion model (xt), classiﬁerp(yjxt),
and gradient scale s.
Input: class label y, gradient scale s
xT sample fromN(0;I)
for alltfromTto 1do
^ (xt) p1 trxtlogp(yjxt)
xt 1 pt 1
xt p1 t^pt
+p1 t 1^
end for
returnx0
We can assume that logp(yjxt)has low curvature compared to  1. This assumption is reasonable
in the limit of inﬁnite diffusion steps, where jjjj! 0. In this case, we can approximate logp(yjxt)
using a Taylor expansion around xt=as
logp(yjxt)logp(yjxt)jxt=+ (xt )rxtlogp(yjxt)jxt= (5)
= (xt )g+C1 (6)
Here,g=rxtlogp(yjxt)jxt=, andC1is a constant. This gives
log(p(xtjxt+1)p(yjxt)) 1
2(xt )T 1(xt ) + (xt )g+C2 (7)
= 1
2(xt  g)T 1(xt  g) +1
2gTg+C2 (8)
= 1
2(xt  g)T 1(xt  g) +C3 (9)
= logp(z) +C4;zN(+ g;) (10)
We can safely ignore the constant term C4, since it corresponds to the normalizing coefﬁcient Zin
Equation 2. We have thus found that the conditional transition operator can be approximated by a
Gaussian similar to the unconditional transition operator, but with its mean shifted by g. Algorithm
1 summaries the corresponding sampling algorithm. We include an optional scale factor sfor the
gradients, which we describe in more detail in Section 4.3.
4.2 Conditional Sampling for DDIM
The above derivation for conditional sampling is only valid for the stochastic diffusion sampling
process, and cannot be applied to deterministic sampling methods like DDIM [ 57]. To this end, we
use a score-based conditioning trick adapted from Song et al. [ 60], which leverages the connection
between diffusion models and score matching [ 59]. In particular, if we have a model (xt)that
predicts the noise added to a sample, then this can be used to derive a score function:
rxtlogp(xt) = 1p1 t(xt) (11)
7Figure 3: Samples from an unconditional diffusion model with classiﬁer guidance to condition
on the class "Pembroke Welsh corgi". Using classiﬁer scale 1.0 (left; FID: 33.0) does not produce
convincing samples in this class, whereas classiﬁer scale 10.0 (right; FID: 12.0) produces much more
class-consistent images.
We can now substitute this into the score function for p(xt)p(yjxt):
rxtlog(p(xt)p(yjxt)) =rxtlogp(xt) +rxtlogp(yjxt) (12)
= 1p1 t(xt) +rxtlogp(yjxt) (13)
Finally, we can deﬁne a new epsilon prediction ^(xt)which corresponds to the score of the joint
distribution:
^(xt):=(xt) p
1 trxtlogp(yjxt) (14)
We can then use the exact same sampling procedure as used for regular DDIM, but with the modiﬁed
noise predictions ^(xt)instead of(xt). Algorithm 2 summaries the corresponding sampling
algorithm.
4.3 Scaling Classiﬁer Gradients
To apply classiﬁer guidance to a large scale generative task, we train classiﬁcation models on
ImageNet. Our classiﬁer architecture is simply the downsampling trunk of the UNet model with
an attention pool [ 49] at the 8x8 layer to produce the ﬁnal output. We train these classiﬁers on the
same noising distribution as the corresponding diffusion model, and also add random crops to reduce
overﬁtting. After training, we incorporate the classiﬁer into the sampling process of the diffusion
model using Equation 10, as outlined by Algorithm 1.
In initial experiments with unconditional ImageNet models, we found it necessary to scale the
classiﬁer gradients by a constant factor larger than 1. When using a scale of 1, we observed that the
classiﬁer assigned reasonable probabilities (around 50%) to the desired classes for the ﬁnal samples,
but these samples did not match the intended classes upon visual inspection. Scaling up the classiﬁer
gradients remedied this problem, and the class probabilities from the classiﬁer increased to nearly
100%. Figure 3 shows an example of this effect.
To understand the effect of scaling classiﬁer gradients, note that srxlogp(yjx) =rxlog1
Zp(yjx)s,
whereZis an arbitrary constant. As a result, the conditioning process is still theoretically grounded
in a re-normalized classiﬁer distribution proportional to p(yjx)s. Whens > 1, this distribution
becomes sharper than p(yjx), since larger values are ampliﬁed by the exponent. In other words, using
a larger gradient scale focuses more on the modes of the classiﬁer, which is potentially desirable for
producing higher ﬁdelity (but less diverse) samples.
In the above derivations, we assumed that the underlying diffusion model was unconditional, modeling
p(x). It is also possible to train conditional diffusion models, p(xjy), and use classiﬁer guidance in
the exact same way. Table 4 shows that the sample quality of both unconditional and conditional
models can be greatly improved by classiﬁer guidance. We see that, with a high enough scale, the
guided unconditional model can get quite close to the FID of an unguided conditional model, although
training directly with the class labels still helps. Guiding a conditional model further improves FID.
Table 4 also shows that classiﬁer guidance improves precision at the cost of recall, thus introducing
a trade-off in sample ﬁdelity versus diversity. We explicitly evaluate how this trade-off varies with
8Conditional Guidance Scale FID sFID IS Precision Recall
7 7 26.21 6.35 39.70 0.61 0.63
7 3 1.0 33.03 6.99 32.92 0.56 0.65
7 3 10.0 12.00 10.40 95.41 0.76 0.44
3 7 10.94 6.02 100.98 0.69 0.63
3 3 1.0 4.59 5.25 186.70 0.82 0.52
3 3 10.0 9.11 10.93 283.92 0.88 0.32
Table 4: Effect of classiﬁer guidance on sample quality. Both conditional and unconditional models
were trained for 2M iterations on ImageNet 256 256 with batch size 256.
0 2 4 6 8 10
gradient scale46810121416FID sFID
0 2 4 6 8 10
gradient scale100150200250300IS
0 2 4 6 8 10
gradient scale0.30.40.50.60.70.80.9precision recall
Figure 4: Change in sample quality as we vary scale of the classiﬁer gradients for a class-conditional
ImageNet 128128 model.
0.70 0.75 0.80 0.85 0.90 0.95
Precision0.00.10.20.30.40.50.6RecallBigGAN-deep
Classifier  guidance (ours)
100125150175200225250275
IS51015202530FIDBigGAN-deep
Classifier guidance (ours)
Figure 5: Trade-offs when varying truncation for BigGAN-deep and gradient scale for classiﬁer
guidance. Models are evaluated on ImageNet 128 128. The BigGAN-deep results were produced
using the TFHub model [12] at truncation levels [0:1;0:2;0:3;:::;1:0].
the gradient scale in Figure 4. We see that scaling the gradients beyond 1.0 smoothly trades off
recall (a measure of diversity) for higher precision and IS (measures of ﬁdelity). Since FID and sFID
depend on both diversity and ﬁdelity, their best values are obtained at an intermediate point. We also
compare our guidance with the truncation trick from BigGAN in Figure 5. We ﬁnd that classiﬁer
guidance is strictly better than BigGAN-deep when trading off FID for Inception Score. Less clear
cut is the precision/recall trade-off, which shows that classiﬁer guidance is only a better choice up
until a certain precision threshold, after which point it cannot achieve better precision.
5 Results
To evaluate our improved model architecture on unconditional image generation, we train separate
diffusion models on three LSUN [ 71] classes: bedroom, horse, and cat. To evaluate classiﬁer
guidance, we train conditional diffusion models on the ImageNet [ 52] dataset at 128128, 256256,
and 512512 resolution.
9Model FID sFID Prec Rec
LSUN Bedrooms 256 256
DCTransformery[42] 6.40 6.66 0.44 0.56
DDPM [25] 4.89 9.07 0.60 0.45
IDDPM [43] 4.24 8.21 0.62 0.46
StyleGAN [27] 2.35 6.62 0.59 0.48
ADM (dropout) 1.90 5.59 0.66 0.51
LSUN Horses 256256
StyleGAN2 [28] 3.84 6.46 0.63 0.48
ADM 2.95 5.94 0.69 0.55
ADM (dropout) 2.57 6.81 0.71 0.55
LSUN Cats 256256
DDPM [25] 17.1 12.4 0.53 0.48
StyleGAN2 [28] 7.25 6.33 0.58 0.43
ADM (dropout) 5.57 6.69 0.63 0.52
ImageNet 6464
BigGAN-deep* [5] 4.06 3.96 0.79 0.48
IDDPM [43] 2.92 3.79 0.74 0.62
ADM 2.61 3.77 0.73 0.63
ADM (dropout) 2.07 4.29 0.74 0.63Model FID sFID Prec Rec
ImageNet 128128
BigGAN-deep [5] 6.02 7.18 0.86 0.35
LOGANy[68] 3.36
ADM 5.91 5.09 0.70 0.65
ADM-G (25 steps) 5.98 7.04 0.78 0.51
ADM-G 2.97 5.09 0.78 0.59
ImageNet 256256
DCTransformery[42] 36.51 8.24 0.36 0.67
VQ-V AE-2yz[51] 31.11 17.38 0.36 0.57
IDDPMz[43] 12.26 5.42 0.70 0.62
SR3yz[53] 11.30
BigGAN-deep [5] 6.95 7.36 0.87 0.28
ADM 10.94 6.02 0.69 0.63
ADM-G (25 steps) 5.44 5.32 0.81 0.49
ADM-G 4.59 5.25 0.82 0.52
ImageNet 512512
BigGAN-deep [5] 8.43 8.13 0.88 0.29
ADM 23.24 10.19 0.73 0.60
ADM-G (25 steps) 8.41 9.67 0.83 0.47
ADM-G 7.72 6.57 0.87 0.42
Table 5: Sample quality comparison with state-of-the-art generative models for each task. ADM
refers to our ablated diffusion model, and ADM-G additionally uses classiﬁer guidance. LSUN
diffusion models are sampled using 1000 steps (see Appendix J). ImageNet diffusion models are
sampled using 250 steps, except when we use the DDIM sampler with 25 steps. *No BigGAN-deep
model was available at this resolution, so we trained our own.yValues are taken from a previous
paper, due to lack of public models or samples.zResults use two-resolution stacks.
5.1 State-of-the-art Image Synthesis
Table 5 summarizes our results. Our diffusion models can obtain the best FID on each task, and
the best sFID on all but one task. With the improved architecture, we already obtain state-of-the-art
image generation on LSUN and ImageNet 64 64. For higher resolution ImageNet, we observe that
classiﬁer guidance allows our models to substantially outperform the best GANs. These models
obtain perceptual quality similar to GANs, while maintaining a higher coverage of the distribution as
measured by recall, and can even do so using only 25 diffusion steps.
Figure 6 compares random samples from the best BigGAN-deep model to our best diffusion model.
While the samples are of similar perceptual quality, the diffusion model contains more modes than the
GAN, such as zoomed ostrich heads, single ﬂamingos, different orientations of cheeseburgers, and a
tinca ﬁsh with no human holding it. We also check our generated samples for nearest neighbors in
the Inception-V3 feature space in Appendix C, and we show additional samples in Appendices K-M.
5.2 Comparison to Upsampling
We also compare guidance to using a two-stage upsampling stack. Nichol and Dhariwal [ 43] and
Saharia et al. [ 53] train two-stage diffusion models by combining a low-resolution diffusion model
with a corresponding upsampling diffusion model. In this approach, the upsampling model is
trained to upsample images from the training set, and conditions on low-resolution images that are
concatenated channel-wise to the model input using a simple interpolation (e.g. bilinear). During
sampling, the low-resolution model produces a sample, and then the upsampling model is conditioned
on this sample. This greatly improves FID on ImageNet 256 256, but does not reach the same
performance as state-of-the-art models like BigGAN-deep [43, 53], as seen in Table 5.
In Table 6, we show that guidance and upsampling improve sample quality along different axes.
While upsampling improves precision while keeping a high recall, guidance provides a knob to trade
10Figure 6: Samples from BigGAN-deep with truncation 1.0 (FID 6.95, left) vs samples from our
diffusion model with guidance (FID 4.59, middle) and samples from the training set (right).
Model Sbase Supsample FID sFID IS Precision Recall
ImageNet 256256
ADM 250 10.94 6.02 100.98 0.69 0.63
ADM-U 250 250 7.49 5.13 127.49 0.72 0.63
ADM-G 250 4.59 5.25 186.70 0.82 0.52
ADM-G, ADM-U 250 250 3.94 6.14 215.84 0.83 0.53
ImageNet 512512
ADM 250 23.24 10.19 58.06 0.73 0.60
ADM-U 250 250 9.96 5.62 121.78 0.75 0.64
ADM-G 250 7.72 6.57 172.71 0.87 0.42
ADM-G, ADM-U 25 25 5.96 12.10 187.87 0.81 0.54
ADM-G, ADM-U 250 25 4.11 9.57 219.29 0.83 0.55
ADM-G, ADM-U 250 250 3.85 5.86 221.72 0.84 0.53
Table 6: Comparing our single, upsampling and classiﬁer guided models. For upsampling, we use
theupsampling stack from Nichol and Dhariwal [ 43] combined with our architecture improvements,
which we refer to as ADM-U. The base resolution for the two-stage upsampling models is 64and
128for the 256and512models, respectively. When combining classiﬁer guidance with upsampling,
we only guide the lower resolution model.
off diversity for much higher precision. We achieve the best FIDs by using guidance at a lower
resolution before upsampling to a higher resolution, indicating that these approaches complement
one another.
6 Related Work
Score based generative models were introduced by Song and Ermon [ 59] as a way of modeling a
data distribution using its gradients, and then sampling using Langevin dynamics [ 67]. Ho et al. [ 25]
found a connection between this method and diffusion models [ 56], and achieved excellent sample
quality by leveraging this connection. After this breakthrough work, many works followed up with
more promising results: Kong et al. [ 30] and Chen et al. [ 8] demonstrated that diffusion models
11work well for audio; Jolicoeur-Martineau et al. [ 26] found that a GAN-like setup could improve
samples from these models; Song et al. [ 60] explored ways to leverage techniques from stochastic
differential equations to improve the sample quality obtained by score-based models; Song et al. [ 57]
and Nichol and Dhariwal [ 43] proposed methods to improve sampling speed; Nichol and Dhariwal
[43] and Saharia et al. [ 53] demonstrated promising results on the difﬁcult ImageNet generation task
using upsampling diffusion models. Also related to diffusion models, and following the work of
Sohl-Dickstein et al. [ 56], Goyal et al. [ 21] described a technique for learning a model with learned
iterative generation steps, and found that it could achieve good image samples when trained with a
likelihood objective.
One missing element from previous work on diffusion models is a way to trade off diversity for ﬁdelity.
Other generative techniques provide natural levers for this trade-off. Brock et al. [ 5] introduced the
truncation trick for GANs, wherein the latent vector is sampled from a truncated normal distribution.
They found that increasing truncation naturally led to a decrease in diversity but an increase in ﬁdelity.
More recently, Razavi et al. [ 51] proposed to use classiﬁer rejection sampling to ﬁlter out bad samples
from an autoregressive likelihood-based model, and found that this technique improved FID. Most
likelihood-based models also allow for low-temperature sampling [ 1], which provides a natural way
to emphasize modes of the data distribution (see Appendix G).
Other likelihood-based models have been shown to produce high-ﬁdelity image samples. VQ-V AE
[65] and VQ-V AE-2 [ 51] are autoregressive models trained on top of quantized latent codes, greatly
reducing the computational resources required to train these models on large images. These models
produce diverse and high quality images, but still fall short of GANs without expensive rejection
sampling and special metrics to compensate for blurriness. DCTransformer [ 42] is a related method
which relies on a more intelligent compression scheme. V AEs are another promising class of
likelihood-based models, and recent methods such as NV AE [ 63] and VDV AE [ 9] have successfully
been applied to difﬁcult image generation domains. Energy-based models are another class of
likelihood-based models with a rich history [ 1,10,24]. Sampling from the EBM distribution is
challenging, and Xie et al. [ 70] demonstrate that Langevin dynamics can be used to sample coherent
images from these models. Du and Mordatch [ 15] further improve upon this approach, obtaining
high quality images. More recently, Gao et al. [ 18] incorporate diffusion steps into an energy-based
model, and ﬁnd that doing so improves image samples from these models.
Other works have controlled generative models with a pre-trained classiﬁer. For example, an emerging
body of work [ 17,47,2] aims to optimize GAN latent spaces for text prompts using pre-trained CLIP
[49] models. More similar to our work, Song et al. [ 60] uses a classiﬁer to generate class-conditional
CIFAR-10 images with a diffusion model. In some cases, classiﬁers can act as stand-alone generative
models. For example, Santurkar et al. [ 55] demonstrate that a robust image classiﬁer can be used as a
stand-alone generative model, and Grathwohl et al. [ 22] train a model which is jointly a classiﬁer and
an energy-based model.
7 Limitations and Future Work
While we believe diffusion models are an extremely promising direction for generative modeling,
they are still slower than GANs at sampling time due to the use of multiple denoising steps (and
therefore forward passes). One promising work in this direction is from Luhman and Luhman [ 37],
who explore a way to distill the DDIM sampling process into a single step model. The samples
from the single step model are not yet competitive with GANs, but are much better than previous
single-step likelihood-based models. Future work in this direction might be able to completely close
the sampling speed gap between diffusion models and GANs without sacriﬁcing image quality.
Our proposed classiﬁer guidance technique is currently limited to labeled datasets, and we have
provided no effective strategy for trading off diversity for ﬁdelity on unlabeled datasets. In the future,
our method could be extended to unlabeled data by clustering samples to produce synthetic labels
[36] or by training discriminative models to predict when samples are in the true data distribution or
from the sampling distribution.
The effectiveness of classiﬁer guidance demonstrates that we can obtain powerful generative models
from the gradients of a classiﬁcation function. This could be used to condition pre-trained models
in a plethora of ways, for example by conditioning an image generator with a text caption using a
noisy version of CLIP [ 49], similar to recent methods that guide GANs using text prompts [ 17,47,
122]. It also suggests that large unlabeled datasets could be leveraged in the future to pre-train powerful
diffusion models that can later be improved by using a classiﬁer with desirable properties.
8 Conclusion
We have shown that diffusion models, a class of likelihood-based models with a stationary training
objective, can obtain better sample quality than state-of-the-art GANs. Our improved architecture
is sufﬁcient to achieve this on unconditional image generation tasks, and our classiﬁer guidance
technique allows us to do so on class-conditional tasks. In the latter case, we ﬁnd that the scale
of the classiﬁer gradients can be adjusted to trade off diversity for ﬁdelity. These guided diffusion
models can reduce the sampling time gap between GANs and diffusion models, although diffusion
models still require multiple forward passes during sampling. Finally, by combining guidance with
upsampling, we can further improve sample quality on high-resolution conditional image synthesis.
9 Acknowledgements
We thank Alec Radford, Mark Chen, Pranav Shyam and Raul Puri for providing feedback on this
work.
References
[1]David Ackley, Geoffrey Hinton, and Terrence Sejnowski. A learning algorithm for boltzmann
machines. Cognitive science, 9(1):147-169 , 1985.
[2]Adverb. The big sleep. https://twitter.com/advadnoun/status/
1351038053033406468 , 2021.
[3] Shane Barratt and Rishi Sharma. A note on the inception score. arXiv:1801.01973 , 2018.
[4]Andrew Brock, Theodore Lim, J. M. Ritchie, and Nick Weston. Neural photo editing with
introspective adversarial networks. arXiv:1609.07093 , 2016.
[5]Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high ﬁdelity
natural image synthesis. arXiv:1809.11096 , 2018.
[6]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners.
arXiv:2005.14165 , 2020.
[7]Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya
Sutskever. Generative pretraining from pixels. In International Conference on Machine
Learning , pages 1691–1703. PMLR, 2020.
[8]Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan.
Wavegrad: Estimating gradients for waveform generation. arXiv:2009.00713 , 2020.
[9]Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on
images. arXiv:2011.10650 , 2021.
[10] Peter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S Zemel. The helmholtz
machine. Neural computation , 7(5):889–904, 1995.
[11] Harm de Vries, Florian Strub, Jérémie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron
Courville. Modulating early visual processing by language. arXiv:1707.00683 , 2017.
[12] DeepMind. Biggan-deep 128x128 on tensorﬂow hub. https://tfhub.dev/deepmind/
biggan-deep-128/1 , 2018.
13[13] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya
Sutskever. Jukebox: A generative model for music. arXiv:2005.00341 , 2020.
[14] Jeff Donahue and Karen Simonyan. Large scale adversarial representation learning.
arXiv:1907.02544 , 2019.
[15] Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models.
arXiv:1903.08689 , 2019.
[16] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. A learned representation for
artistic style. arXiv:1610.07629 , 2017.
[17] Federico A. Galatolo, Mario G. C. A. Cimino, and Gigliola Vaglini. Generating images from
caption and vice versa via clip-guided generative latent space search. arXiv:2102.01645 , 2021.
[18] Ruiqi Gao, Yang Song, Ben Poole, Ying Nian Wu, and Diederik P. Kingma. Learning energy-
based models by diffusion recovery likelihood. arXiv:2012.08125 , 2020.
[19] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv:1406.2661 ,
2014.
[20] Google. Cloud tpus. https://cloud.google.com/tpu/ , 2018.
[21] Anirudh Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. Variational walkback:
Learning a transition operator as a stochastic recurrent net. arXiv:1711.02282 , 2017.
[22] Will Grathwohl, Kuan-Chieh Wang, Jörn-Henrik Jacobsen, David Duvenaud, Mohammad
Norouzi, and Kevin Swersky. Your classiﬁer is secretly an energy based model and you should
treat it like one. arXiv:1912.03263 , 2019.
[23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in
Neural Information Processing Systems 30 (NIPS 2017) , 2017.
[24] Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural
computation , 14(8):1771–1800, 2002.
[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models.
arXiv:2006.11239 , 2020.
[26] Alexia Jolicoeur-Martineau, Rémi Piché-Taillefer, Rémi Tachet des Combes, and Ioan-
nis Mitliagkas. Adversarial score matching and improved sampling for image generation.
arXiv:2009.05475 , 2020.
[27] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. arXiv:arXiv:1812.04948 , 2019.
[28] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.
Analyzing and improving the image quality of stylegan. arXiv:1912.04958 , 2019.
[29] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
arXiv:1412.6980 , 2014.
[30] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile
diffusion model for audio synthesis. arXiv:2009.09761 , 2020.
[31] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. CIFAR-10 (Canadian Institute for Advanced
Research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html .
[32] Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved
precision and recall metric for assessing generative models. arXiv:1904.06991 , 2019.
[33] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian Reid. Reﬁnenet: Multi-path reﬁnement
networks for high-resolution semantic segmentation. arXiv:1611.06612 , 2016.
14[34] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the
wild. In Proceedings of International Conference on Computer Vision (ICCV) , December 2015.
[35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv:1711.05101 ,
2017.
[36] Mario Lucic, Michael Tschannen, Marvin Ritter, Xiaohua Zhai, Olivier Bachem, and Sylvain
Gelly. High-ﬁdelity image generation with fewer labels. arXiv:1903.02271 , 2019.
[37] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for
improved sampling speed. arXiv:2101.02388 , 2021.
[38] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,
Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed
precision training. arXiv:1710.03740 , 2017.
[39] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv:1411.1784 ,
2014.
[40] Takeru Miyato and Masanori Koyama. cgans with projection discriminator. arXiv:1802.05637 ,
2018.
[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. arXiv:1802.05957 , 2018.
[42] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W. Battaglia. Generating images with
sparse representations. arXiv:2103.03841 , 2021.
[43] Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.
arXiv:2102.09672 , 2021.
[44] NVIDIA. Stylegan2. https://github.com/NVlabs/stylegan2 , 2019.
[45] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On buggy resizing libraries and surprising
subtleties in ﬁd calculation. arXiv:2104.11222 , 2021.
[46] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
style, high-performance deep learning library. arXiv:1912.01703 , 2019.
[47] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip:
Text-driven manipulation of stylegan imagery. arXiv:2103.17249 , 2021.
[48] Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. Film:
Visual reasoning with a general conditioning layer. arXiv:1709.07871 , 2017.
[49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini
Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger,
and Ilya Sutskever. Learning transferable visual models from natural language supervision.
arXiv:2103.00020 , 2021.
[50] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark
Chen, and Ilya Sutskever. Zero-shot text-to-image generation. arXiv:2102.12092 , 2021.
[51] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-ﬁdelity images
with VQ-V AE-2. arXiv:1906.00446 , 2019.
[52] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
Imagenet large scale visual recognition challenge. arXiv:1409.0575 , 2014.
[53] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad
Norouzi. Image super-resolution via iterative reﬁnement. arXiv:arXiv:2104.07636 , 2021.
15[54] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. arXiv:1606.03498 , 2016.
[55] Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Andrew Ilyas, Logan Engstrom, and
Aleksander Madry. Image synthesis with a single (robust) classiﬁer. arXiv:1906.09453 , 2019.
[56] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep
unsupervised learning using nonequilibrium thermodynamics. arXiv:1503.03585 , 2015.
[57] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models.
arXiv:2010.02502 , 2020.
[58] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.
arXiv:2006.09011 , 2020.
[59] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data
distribution. arXiv:arXiv:1907.05600 , 2020.
[60] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon,
and Ben Poole. Score-based generative modeling through stochastic differential equations.
arXiv:2011.13456 , 2020.
[61] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Good-
fellow, and Rob Fergus. Intriguing properties of neural networks. arXiv:1312.6199 , 2013.
[62] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.
Rethinking the inception architecture for computer vision. arXiv:1512.00567 , 2015.
[63] Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder.
arXiv:2007.03898 , 2020.
[64] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex
Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative
model for raw audio. arXiv:1609.03499 , 2016.
[65] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation
learning. arXiv:1711.00937 , 2017.
[66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv:1706.03762 , 2017.
[67] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics.
InProceedings of the 28th international conference on machine learning (ICML-11) , pages
681–688. Citeseer, 2011.
[68] Yan Wu, Jeff Donahue, David Balduzzi, Karen Simonyan, and Timothy Lillicrap. Logan: Latent
optimisation for generative adversarial networks. arXiv:1912.00953 , 2019.
[69] Yuxin Wu and Kaiming He. Group normalization. arXiv:1803.08494 , 2018.
[70] Jianwen Xie, Yang Lu, Song-Chun Zhu, and Ying Nian Wu. A theory of generative convnet.
arXiv:1602.03264 , 2016.
[71] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao.
Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop.
arXiv:1506.03365 , 2015.
[72] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and
Dimitris Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative
adversarial networks. arXiv:1612.03242 , 2016.
[73] Ligeng Zhu. Thop. https://github.com/Lyken17/pytorch-OpCounter , 2018.
16A Computational Requirements
Compute is essential to modern machine learning applications, and more compute typically yields
better results. It is thus important to compare our method’s compute requirements to competing
methods. In this section, we demonstrate that we can achieve results better than StyleGAN2 and
BigGAN-deep with the same or lower compute budget.
A.1 Throughput
We ﬁrst benchmark the throughput of our models in Table 7. For the theoretical throughput, we
measure the theoretical FLOPs for our model using THOP [ 73], and assume 100% utilization of an
NVIDIA Tesla V100 (120 TFLOPs), while for the actual throughput we use measured wall-clock
time. We include communication time across two machines whenever our training batch size doesn’t
ﬁt on a single machine, where each of our machines has 8 V100s.
We ﬁnd that a naive implementation of our models in PyTorch 1.7 is very inefﬁcient, utilizing only
20-30% of the hardware. We also benchmark our optimized version, which use larger per-GPU batch
sizes, fused GroupNorm-Swish and fused Adam CUDA ops. For our ImageNet 128 128 model in
particular, we ﬁnd that we can increase the per-GPU batch size from 4 to 32 while still ﬁtting in GPU
memory, and this makes a large utilization difference. Our implementation is still far from optimal,
and further optimizations should allow us to reach higher levels of utilization.
Model ImplementationBatch Size ThroughputUtilizationper GPU Imgs per V100-sec
6464Theoretical - 182.3 100%
Naive 32 37.0 20%
Optimized 96 74.1 41%
128128Theoretical - 65.2 100%
Naive 4 11.5 18%
Optimized 32 24.8 38%
256256Theoretical - 17.9 100%
Naive 4 4.4 25%
Optimized 8 6.4 36%
64!256Theoretical - 31.7 100%
Naive 4 6.3 20%
Optimized 12 9.5 30%
128!512Theoretical - 8.0 100%
Naive 2 1.9 24%
Optimized 2 2.3 29%
Table 7: Throughput of our ImageNet models, measured in Images per V100-sec.
A.2 Early stopping
In addition, we can train for many fewer iterations while maintaining sample quality superior to
BigGAN-deep. Table 8 and 9 evaluate our ImageNet 128 128 and 256256 models throughout
training. We can see that the ImageNet 128 128 model beats BigGAN-deep’s FID (6.02) after 500K
training iterations, only one eighth of the way through training. Similarly, the ImageNet 256 256
model beats BigGAN-deep after 750K iterations, roughly a third of the way through training.
Iterations FID sFID Precision Recall
250K 7.97 6.48 0.80 0.50
500K 5.31 5.97 0.83 0.49
1000K 4.10 5.80 0.81 0.51
2000K 3.42 5.69 0.83 0.53
4360K 3.09 5.59 0.82 0.54
Table 8: Evaluating an ImageNet 128 128 model throughout training (classiﬁer scale 1.0).
17Iterations FID sFID Precision Recall
250K 12.21 6.15 0.78 0.50
500K 7.95 5.51 0.81 0.50
750K 6.49 5.39 0.81 0.50
1000K 5.74 5.29 0.81 0.52
1500K 5.01 5.20 0.82 0.52
1980K 4.59 5.25 0.82 0.52
Table 9: Evaluating an ImageNet 256 256 model throughout training (classiﬁer scale 1.0).
A.3 Compute comparison
Finally, in Table 10 we compare the compute of our models with StyleGAN2 and BigGAN-deep, and
show we can obtain better FIDs with a similar compute budget. For BigGAN-deep, Brock et al. [ 5] do
not explicitly describe the compute requirements for training their models, but rather provide rough
estimates in terms of days on a Google TPUv3 pod [ 20]. We convert their TPU-v3 estimates to V100
days according to 2 TPU-v3 day = 1 V100 day. For StyleGAN2, we use the reported throughput of
25M images over 32 days 13 hour on one V100 for conﬁg-f [ 44]. We note that our classiﬁer training
is relatively lightweight compared to training the generative model.
Model Generator Classiﬁer Total FID sFID Precision Recall
Compute Compute Compute
LSUN Horse 256256
StyleGAN2 [28] 130 3.84 6.46 0.63 0.48
ADM (250K) 116 - 116 2.95 5.94 0.69 0.55
ADM (dropout, 250K) 116 - 116 2.57 6.81 0.71 0.55
LSUN Cat 256256
StyleGAN2 [28] 115 7.25 6.33 0.58 0.43
ADM (dropout, 200K) 92 - 92 5.57 6.69 0.63 0.52
ImageNet 128128
BigGAN-deep [5] 64-128 6.02 7.18 0.86 0.35
ADM-G (4360K) 521 9 530 3.09 5.59 0.82 0.54
ADM-G (450K) 54 9 63 5.67 6.19 0.82 0.49
ImageNet 256256
BigGAN-deep [5] 128-256 6.95 7.36 0.87 0.28
ADM-G (1980K) 916 46 962 4.59 5.25 0.82 0.52
ADM-G (750K) 347 46 393 6.49 5.39 0.81 0.50
ADM-G (750K) 347 14y361 6.68 5.34 0.81 0.51
ADM-G (540K), ADM-U (500K) 329 30 359 3.85 5.86 0.84 0.53
ADM-G (540K), ADM-U (150K) 219 30 249 4.15 6.14 0.82 0.54
ADM-G (200K), ADM-U (150K) 110 10z126 4.93 5.82 0.82 0.52
ImageNet 512512
BigGAN-deep [5] 256-512 8.43 8.13 0.88 0.29
ADM-G (4360K), ADM-U (1050K) 1878 36 1914 3.85 5.86 0.84 0.53
ADM-G (500K), ADM-U (100K) 189 9* 198 7.59 6.84 0.84 0.53
Table 10: Training compute requirements for our diffusion models compared to StyleGAN2 and
BigGAN-deep. Training iterations for each diffusion model are mentioned in parenthesis. Compute
is measured in V100-days.yImageNet 256256 classiﬁer with 150K iterations (instead of 500K).
zImageNet 6464 classiﬁer with batch size 256 (instead of 1024). *ImageNet 128 128 classiﬁer
with batch size 256 (instead of 1024).
18B Detailed Formulation of DDPM
Here, we provide a detailed review of the formulation of Gaussian diffusion models from Ho et al.
[25]. We start by deﬁning our data distribution x0q(x0)and a Markovian noising process qwhich
gradually adds noise to the data to produce noised samples x1throughxT. In particular, each step of
the noising process adds Gaussian noise according to some variance schedule given by t:
q(xtjxt 1):=N(xt;p
1 txt 1;tI) (15)
Ho et al. [ 25] note that we need not apply qrepeatedly to sample from xtq(xtjx0). Instead,
q(xtjx0)can be expressed as a Gaussian distribution. With t:= 1 tandt:=Qt
s=0s
q(xtjx0) =N(xt;ptx0;(1 t)I) (16)
=ptx0+p
1 t; N(0;I) (17)
Here, 1 ttells us the variance of the noise for an arbitrary timestep, and we could equivalently
use this to deﬁne the noise schedule instead of t.
Using Bayes theorem, one ﬁnds that the posterior q(xt 1jxt;x0)is also a Gaussian with mean
~t(xt;x0)and variance ~tdeﬁned as follows:
~t(xt;x0):=pt 1t
1 tx0+pt(1 t 1)
1 txt (18)
~t:=1 t 1
1 tt (19)
q(xt 1jxt;x0) =N(xt 1; ~(xt;x0);~tI) (20)
If we wish to sample from the data distribution q(x0), we can ﬁrst sample from q(xT)and then sample
reverse steps q(xt 1jxt)until we reach x0. Under reasonable settings for tandT, the distribution
q(xT)is nearly an isotropic Gaussian distribution, so sampling xTis trivial. All that is left is to
approximate q(xt 1jxt)using a neural network, since it cannot be computed exactly when the data
distribution is unknown. To this end, Sohl-Dickstein et al. [ 56] note thatq(xt 1jxt)approaches a
diagonal Gaussian distribution as T!1 and correspondingly t!0, so it is sufﬁcient to train a
neural network to predict a mean and a diagonal covariance matrix :
p(xt 1jxt):=N(xt 1;(xt;t);(xt;t)) (21)
To train this model such that p(x0)learns the true data distribution q(x0), we can optimize the
following variational lower-bound Lvlbforp(x0):
Lvlb:=L0+L1+:::+LT 1+LT (22)
L0:= logp(x0jx1) (23)
Lt 1:=DKL(q(xt 1jxt;x0)jjp(xt 1jxt)) (24)
LT:=DKL(q(xTjx0)jjp(xT)) (25)
While the above objective is well-justiﬁed, Ho et al. [ 25] found that a different objective produces
better samples in practice. In particular, they do not directly parameterize (xt;t)as a neural
network, but instead train a model (xt;t)to predictfrom Equation 17. This simpliﬁed objective
is deﬁned as follows:
Lsimple :=Et[1;T];x0q(x0);N(0;I)[jj (xt;t)jj2] (26)
During sampling, we can use substitution to derive (xt;t)from(xt;t):
(xt;t) =1pt
xt 1 tp1 t(xt;t)
(27)
Note thatLsimple does not provide any learning signal for (xt;t). Ho et al. [ 25] ﬁnd that instead of
learning (xt;t), they can ﬁx it to a constant, choosing either tIor~tI. These values correspond
to upper and lower bounds for the true reverse step variance.
19C Nearest Neighbors for Samples
Figure 7: Nearest neighbors for samples from a classiﬁer guided model on ImageNet 256 256. For
each image, the top row is a sample, and the remaining rows are the top 3 nearest neighbors from the
dataset. The top samples were generated with classiﬁer scale 1 and 250 diffusion sampling steps (FID
4.59). The bottom samples were generated with classiﬁer scale 2.5 and 25 DDIM steps (FID 5.44).
Our models achieve their best FID when using a classiﬁer to reduce the diversity of the generations.
One might fear that such a process could cause the model to recall existing images from the training
dataset, especially as the classiﬁer scale is increased. To test this, we looked at the nearest neighbors
(in InceptionV3 [ 62] feature space) for a handful of samples. Figure 7 shows our results, revealing
that the samples are indeed unique and not stored in the training set.
D Effect of Varying the Classiﬁer Scale
Figure 8: Samples when increasing the classiﬁer scale from 0.0 (left) to 5.5 (right). Each row
corresponds to a ﬁxed noise seed. We observe that the classiﬁer drastically changes some images,
while leaving others relatively unaffected.
20E LSUN Diversity Comparison
Figure 9: Samples from StyleGAN2 (or StyleGAN for bedrooms) with truncation 1.0 (left) vs
samples from our diffusion models (middle) and samples from the training set (right).
21F Interpolating Between Dataset Images Using DDIM
The DDIM [ 57] sampling process is deterministic given the initial noise xT, thus giving rise to an
implicit latent space. It corresponds to integrating an ODE in the forward direction, and we can run
the process in reverse to get the latents that produce a given real image. Here, we experiment with
encoding real images into this latent space and then interpolating between them.
Equation 13 for the generative pass in DDIM looks like
xt 1 xt=pt 1hp
1=t p
1=t 1
xt+p
1=t 1 1 p
1=t 1
(xt)i
Thus, in the limit of small steps, we can expect the reversal of this ODE in the forward direction
looks like
xt+1 xt=pt+1hp
1=t p
1=t+1
xt+p
1=t+1 1 p
1=t 1
(xt)i
We found that this reverse ODE approximation gives latents with reasonable reconstructions, even
with as few as 250 reverse steps. However, we noticed some noise artifacts when reversing all 250
steps, and ﬁnd that reversing the ﬁrst 249 steps gives much better reconstructions. To interpolate
the latents, class embeddings, and classiﬁer log probabilities, we use cos()x0+sin()x1where
sweeps linearly from 0 to
2.
Figures 10athrough 10cshow DDIM latent space interpolations on a class-conditional 256 256
model, while varying the classiﬁer scale. The left and rightmost images are ground truth dataset
examples, and between them are reconstructed interpolations in DDIM latent space (including both
endpoints). We see that the model with no guidance has almost perfect reconstructions due to its high
recall, whereas raising the guidance scale to 2.5 only ﬁnds approximately similar reconstructions.
Figure 10a: DDIM latent reconstructions and interpolations on real images with no classiﬁer guidance.
22Figure 10b: DDIM latent reconstructions and interpolations on real images with classiﬁer scale 1.0.
Figure 10c: DDIM latent reconstructions and interpolations on real images with classiﬁer scale 2.5.
23G Reduced Temperature Sampling
We achieved our best ImageNet samples by reducing the diversity of our models using classiﬁer
guidance. For many classes of generative models, there is a much simpler way to reduce diversity:
reducing the temperature [ 1]. The temperature parameter is typically setup so that = 1:0corre-
sponds to standard sampling, and  <1:0focuses more on high-density samples. We experimented
with two ways of implementing this for diffusion models: ﬁrst, by scaling the Gaussian noise used
for each transition by , and second by dividing (xt)by. The latter implementation makes
sense when thinking about as a re-scaled score function (see Section 4.2), and scaling up the score
function is similar to scaling up classiﬁer gradients.
To measure how temperature scaling affects samples, we experimented with our ImageNet 128 128
model, evaluating FID, Precision, and Recall across different temperatures (Figure 11). We ﬁnd
that two techniques behave similarly, and neither technique provides any substantial improvement in
our evaluation metrics. We also ﬁnd that low temperatures have both low precision and low recall,
indicating that the model is not focusing on modes of the real data distribution. Figure 12 highlights
this effect, indicating that reducing temperature produces blurry, smooth images.
10−310−2
1 - temperature05101520FIDnoise temperature
epsilon temperature
10−310−2
1 - temperature0.40.50.60.7Precision
noise temperature
epsilon temperature
10−310−2
1 - temperature0.450.500.550.600.65Recall
noise temperature
epsilon temperature
Figure 11: The effect of changing temperature for an ImageNet 128 128 model.
Figure 12: Samples at temperature 0.98 with epsilon scaling (left) and noise scaling (right).
24H Conditional Diffusion Process
In this section, we show that conditional sampling can be achieved with a transition operator
proportional to p(xtjxt+1)p(yjxt), wherep(xtjxt+1)approximates q(xtjxt+1)andp(yjxt)
approximates the label distribution for a noised sample xt.
We start by deﬁning a conditional Markovian noising process ^qsimilar toq, and assume that ^q(yjx0)
is a known and readily available label distribution for each sample.
^q(x0):=q(x0) (28)
^q(yjx0):=Known labels per sample (29)
^q(xt+1jxt;y):=q(xt+1jxt) (30)
^q(x1:Tjx0;y):=TY
t=1^q(xtjxt 1;y) (31)
While we deﬁned the noising process ^qconditioned on y, we can prove that ^qbehaves exactly like
qwhen not conditioned on y. Along these lines, we ﬁrst derive the unconditional noising operator
^q(xt+1jxt):
^q(xt+1jxt) =Z
y^q(xt+1;yjxt)dy (32)
=Z
y^q(xt+1jxt;y)^q(yjxt)dy (33)
=Z
yq(xt+1jxt)^q(yjxt)dy (34)
=q(xt+1jxt)Z
y^q(yjxt)dy (35)
=q(xt+1jxt) (36)
= ^q(xt+1jxt;y) (37)
Following similar logic, we ﬁnd the joint distribution ^q(x1:Tjx0):
^q(x1:Tjx0) =Z
y^q(x1:T;yjx0)dy (38)
=Z
y^q(yjx0)^q(x1:Tjx0;y)dy (39)
=Z
y^q(yjx0)TY
t=1^q(xtjxt 1;y)dy (40)
=Z
y^q(yjx0)TY
t=1q(xtjxt 1)dy (41)
=TY
t=1q(xtjxt 1)Z
y^q(yjx0)dy (42)
=TY
t=1q(xtjxt 1) (43)
=q(x1:Tjx0) (44)
25Using Equation 44, we can now derive ^q(xt):
^q(xt) =Z
x0:t 1^q(x0;:::;xt)dx0:t 1 (45)
=Z
x0:t 1^q(x0)^q(x1;:::;xtjx0)dx0:t 1 (46)
=Z
x0:t 1q(x0)q(x1;:::;xtjx0)dx0:t 1 (47)
=Z
x0:t 1q(x0;:::;xt)dx0:t 1 (48)
=q(xt) (49)
(50)
Using the identities ^q(xt) =q(xt)and^q(xt+1jxt) =q(xt+1jxt), it is trivial to show via Bayes rule
that the unconditional reverse process ^q(xtjxt+1) =q(xtjxt+1).
One observation about ^qis that it gives rise to a noisy classiﬁcation function, ^q(yjxt). We can show
that this classiﬁcation distribution does not depend on xt+1(a noisier version of xt), a fact which we
will later use:
^q(yjxt;xt+1) = ^q(xt+1jxt;y)^q(yjxt)
^q(xt+1jxt)(51)
= ^q(xt+1jxt)^q(yjxt)
^q(xt+1jxt)(52)
= ^q(yjxt) (53)
(54)
We can now derive the conditional reverse process:
^q(xtjxt+1;y) =^q(xt;xt+1;y)
^q(xt+1;y)(55)
=^q(xt;xt+1;y)
^q(yjxt+1)^q(xt+1)(56)
=^q(xtjxt+1)^q(yjxt;xt+1)^q(xt+1)
^q(yjxt+1)^q(xt+1)(57)
=^q(xtjxt+1)^q(yjxt;xt+1)
^q(yjxt+1)(58)
=^q(xtjxt+1)^q(yjxt)
^q(yjxt+1)(59)
=q(xtjxt+1)^q(yjxt)
^q(yjxt+1)(60)
(61)
The^q(yjxt+1)term can be treated as a constant since it does not depend on xt. We thus want to
sample from the distribution Zq(xtjxt+1)^q(yjxt)whereZis a normalizing constant. We already
have a neural network approximation of q(xtjxt+1), calledp(xtjxt+1), so all that is left is an
approximation of ^q(yjxt). This can be obtained by training a classiﬁer p(yjxt)on noised images xt
derived by sampling from q(xt).
26I Hyperparameters
When choosing optimal classiﬁer scales for our sampler, we swept over [0:5;1;2]for ImageNet
128128 and ImageNet 256 256, and [1;2;3;3:5;4;4:5;5]for ImageNet 512512. For DDIM,
we swept over values [0:5;0:75;1:0;1:25;2]for ImageNet 128128,[0:5;1;1:5;2;2:5;3;3:5]for
ImageNet 256256, and [3;4;5;6;7;9;11]for ImageNet 512512.
Hyperparameters for training the diffusion and classiﬁcation models are in Table 11 and Table 12
respectively. Hyperparameters for guided sampling are in Table 14. Hyperparameters used to train
upsampling models are in Table 13. We train all of our models using Adam [ 29] or AdamW [ 35]
with1= 0:9and2= 0:999. We train in 16-bit precision using loss-scaling [ 38], but maintain
32-bit weights, EMA, and optimizer state. We use an EMA rate of 0.9999 for all experiments. We
use PyTorch [46], and train on NVIDIA Tesla V100s.
For all architecture ablations, we train with batch size 256, and sample using 250 sampling steps.
For our attention heads ablations, we use 128 base channels, 2 residual blocks per resolution, multi-
resolution attention, and BigGAN up/downsampling, and we train the models for 700K iterations. By
default, all of our experiments use adaptive group normalization, except when explicitly ablating for
it.
When sampling with 1000 timesteps, we use the same noise schedule as for training. On ImageNet,
we use the uniform stride from Nichol and Dhariwal [ 43] for 250 step samples and the slightly
different uniform stride from Song et al. [57] for 25 step DDIM.
LSUN ImageNet 64 ImageNet 128 ImageNet 256 ImageNet 512
Diffusion steps 1000 1000 1000 1000 1000
Noise Schedule linear cosine linear linear linear
Model size 552M 296M 422M 554M 559M
Channels 256 192 256 256 256
Depth 2 3 2 2 2
Channels multiple 1,1,2,2,4,4 1,2,3,4 1,1,2,3,4 1,1,2,2,4,4 0.5,1,1,2,2,4,4
Heads 4
Heads Channels 64 64 64 64
Attention resolution 32,16,8 32,16,8 32,16,8 32,16,8 32,16,8
BigGAN up/downsample 3 3 3 3 3
Dropout 0.1 0.1 0.0 0.0 0.0
Batch size 256 2048 256 256 256
Iterations varies* 540K 4360K 1980K 1940K
Learning Rate 1e-4 3e-4 1e-4 1e-4 1e-4
Table 11: Hyperparameters for diffusion models. *We used 200K iterations for LSUN cat, 250K for
LSUN horse, and 500K for LSUN bedroom.
ImageNet 64 ImageNet 128 ImageNet 256 ImageNet 512
Diffusion steps 1000 1000 1000 1000
Noise Schedule cosine linear linear linear
Model size 65M 43M 54M 54M
Channels 128 128 128 128
Depth 4 2 2 2
Channels multiple 1,2,3,4 1,1,2,3,4 1,1,2,2,4,4 0.5,1,1,2,2,4,4
Heads Channels 64 64 64 64
Attention resolution 32,16,8 32,16,8 32,16,8 32,16,8
BigGAN up/downsample 3 3 3 3
Attention pooling 3 3 3 3
Weight decay 0.2 0.05 0.05 0.05
Batch size 1024 256* 256 256
Iterations 300K 300K 500K 500K
Learning rate 6e-4 3e-4* 3e-4 3e-4
Table 12: Hyperparameters for classiﬁcation models. *For our ImageNet 128 128!512512
upsamples, we used a different classiﬁer for the base model, with batch size 1024 and learning rate
6e-5.
27ImageNet 64!256 ImageNet 128!512
Diffusion steps 1000 1000
Noise Schedule linear linear
Model size 312M 309M
Channels 192 192
Depth 2 2
Channels multiple 1,1,2,2,4,4 1,1,2,2,4,4*
Heads 4
Heads Channels 64
Attention resolution 32,16,8 32,16,8
BigGAN up/downsample 3 3
Dropout 0.0 0.0
Batch size 256 256
Iterations 500K 1050K
Learning Rate 1e-4 1e-4
Table 13: Hyperparameters for upsampling diffusion models. *We chose this as an optimization, with
the intuition that a lower-resolution path should be unnecessary for upsampling 128x128 images.
ImageNet 64 ImageNet 128 ImageNet 256 ImageNet 512
Gradient Scale (250 steps) 1.0 0.5 1.0 4.0
Gradient Scale (DDIM, 25 steps) - 1.25 2.5 9.0
Table 14: Hyperparameters for classiﬁer-guided sampling.
28J Using Fewer Sampling Steps on LSUN
We initially found that our LSUN models achieved much better results when sampling with 1000
steps rather than 250 steps, contrary to previous results from Nichol and Dhariwal [ 43]. To address
this, we conducted a sweep over sampling-time noise schedules, ﬁnding that an improved schedule
can largely close the gap. We swept over schedules on LSUN bedrooms, and selected the schedule
with the best FID for use on the other two datasets. Table 15 details the ﬁndings of this sweep, and
Table 16 applies this schedule to three LSUN datasets.
While sweeping over sampling schedules is not as expensive as re-training models from scratch, it
does require a signiﬁcant amount of sampling compute. As a result, we did not conduct an exhaustive
sweep, and superior schedules are likely to exist.
Schedule FID
50;50;50;50;50 2.31
70;60;50;40;30 2.17
90;50;40;40;30 2.10
90;60;50;30;20 2.09
80;60;50;30;30 2.09
90;50;50;30;30 2.07
100;50;40;30;30 2.03
90;60;60;20;20 2.02
Table 15: Results of sweeping over 250 step sampling schedules on LSUN bedrooms. The schedule
is expressed as a sequence of ﬁve integers, where each integer is the number of steps allocated to
one ﬁfth of the diffusion process. The ﬁrst integer corresponding to t2[0;199] and the last to
t2[T 200;T 1]. Thus, 50;50;50;50;50is a uniform schedule, and 250;0;0;0;0is a schedule
where all timesteps are spent near t= 0.
Schedule FID sFID Prec Rec
LSUN Bedrooms 256 256
1000 steps 1.90 5.59 0.66 0.51
250 steps (uniform) 2.31 6.12 0.65 0.50
250 steps (sweep) 2.02 6.12 0.67 0.50
LSUN Horses 256256
1000 steps 2.57 6.81 0.71 0.55
250 steps (uniform) 3.45 7.55 0.68 0.56
250 steps (sweep) 2.83 7.08 0.69 0.56
LSUN Cat 256256
1000 steps 5.57 6.69 0.63 0.52
250 steps (uniform) 7.03 8.24 0.60 0.53
250 steps (sweep) 5.94 7.43 0.62 0.52
Table 16: Evaluations on LSUN bedrooms, horses, and cats using different sampling schedules. We
ﬁnd that the sweep schedule produces better results than the uniform 250 step schedule on all three
datasets, and mostly bridges the gap to the 1000 step schedule.
29K Samples from ImageNet 512 512
Figure 13: Samples from our best 512 512 model (FID: 3.85). Classes are 1: goldﬁsh, 279: arctic
fox, 323: monarch butterﬂy, 386: african elephant, 130: ﬂamingo, 852: tennis ball.
30Figure 14: Samples from our best 512 512 model (FID: 3.85). Classes are 933: cheeseburger, 562:
fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric.
31Figure 15: Difﬁcult class samples from our best 512 512 model (FID: 3.85). Classes are 432:
bassoon, 468: cab, 424: barbershop, 444: bicycle-built-for-two, 981: ballplayer, 550: espresso maker.
32Figure 16: Samples from our guided 512 512 model using 250 steps with classiﬁer scale 4.0 (FID
7.72). Classes are 1: goldﬁsh, 279: arctic fox, 323: monarch butterﬂy, 386: african elephant, 130:
ﬂamingo, 852: tennis ball.
33Figure 17: Samples from our guided 512 512 model using 250 steps with classiﬁer scale 4.0 (FID
7.72). Classes are 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992:
agaric.
34Figure 18: Random samples from our best ImageNet 512 512 model (FID 3.85).
35Figure 19: Random samples from our guided 512 512 model using 250 steps with classiﬁer scale
4.0 (FID 7.72).
36L Samples from ImageNet 256 256
Figure 20: Samples using our best 256 256 model (FID 3.94). Classes are 1: goldﬁsh, 279: arctic
fox, 323: monarch butterﬂy, 386: african elephant, 130: ﬂamingo, 852: tennis ball, 933: cheeseburger,
562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric
37Figure 21: Samples from our guided 256 256 model using 250 steps with classiﬁer scale 1.0 (FID
4.59). Classes are 1: goldﬁsh, 279: arctic fox, 323: monarch butterﬂy, 386: african elephant, 130:
ﬂamingo, 852: tennis ball, 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90:
lorikeet, 992: agaric
38Figure 22: Samples from our guided 256 256 model using 25 DDIM steps with classiﬁer scale 2.5
(FID 5.44). Classes are 1: goldﬁsh, 279: arctic fox, 323: monarch butterﬂy, 386: african elephant,
130: ﬂamingo, 852: tennis ball, 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90:
lorikeet, 992: agaric
39Figure 23: Random samples from our best 256 256 model (FID 3.94).
40Figure 24: Random samples from our guided 256 256 model using 250 steps with classiﬁer scale
1.0 (FID 4.59).
41M Samples from LSUN
Figure 25: Random samples from our LSUN bedroom model using 1000 sampling steps. (FID 1.90)
42Figure 26: Random samples from our LSUN horse model using 1000 sampling steps. (FID 2.57)
43Figure 27: Random samples from our LSUN cat model using 1000 sampling steps. (FID 5.57)
44Mixtral of Experts
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch,
Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour,
Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux,
Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao,
Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed
Abstract
We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language
model. Mixtral has the same architecture as Mistral 7B, with the difference
that each layer is composed of 8 feedforward blocks (i.e. experts). For every
token, at each layer, a router network selects two experts to process the current
state and combine their outputs. Even though each token only sees two experts,
the selected experts can be different at each timestep. As a result, each token
has access to 47B parameters, but only uses 13B active parameters during
inference. Mixtral was trained with a context size of 32k tokens and it outperforms
or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In
particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code
generation, and multilingual benchmarks. We also provide a model fine-
tuned to follow instructions, Mixtral 8x7B – Instruct, that surpasses GPT-3.5
Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B – chat model on human bench-
marks. Both the base and instruct models are released under the Apache 2.0 license.
Code: https://github.com/mistralai/mistral-src
Webpage: https://mistral.ai/news/mixtral-of-experts/
1 Introduction
In this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights,
licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As
it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low
batch-sizes, and higher throughput at large batch-sizes.
Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward
block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router
network chooses two of these groups (the “experts”) to process the token and combine their output
additively. This technique increases the number of parameters of a model while controlling cost and
latency, as the model only uses a fraction of the total set of parameters per token.
Mixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches
or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,arXiv:2401.04088v1  [cs.LG]  8 Jan 2024Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The
layer’s output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard
feedforward block as in a vanilla transformer architecture.
Mixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require
multilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments
show that Mixtral is able to successfully retrieve information from its context window of 32k tokens,
regardless of the sequence length and the location of the information in the sequence.
We also present Mixtral 8x7B – Instruct, a chat model fine-tuned to follow instructions using
supervised fine-tuning and Direct Preference Optimization [ 25]. Its performance notably surpasses
that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B – chat model on human evaluation
benchmarks. Mixtral – Instruct also demonstrates reduced biases, and a more balanced sentiment
profile in benchmarks such as BBQ, and BOLD.
We release both Mixtral 8x7B and Mixtral 8x7B – Instruct under the Apache 2.0 license1, free for
academic and commercial usage, ensuring broad accessibility and potential for diverse applications.
To enable the community to run Mixtral with a fully open-source stack, we submitted changes to
the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also
allows the deployment of vLLM endpoints on any instance in the cloud.
2 Architectural details
Parameter Value
dim 4096
n_layers 32
head_dim 128
hidden_dim 14336
n_heads 32
n_kv_heads 8
context_len 32768
vocab_size 32000
num_experts 8
top_k_experts 2
Table 1: Model architecture.Mixtral is based on a transformer architecture [ 31] and uses the same
modifications as described in [ 18], with the notable exceptions that Mix-
tral supports a fully dense context length of 32k tokens, and the feed-
forward blocks are replaced by Mixture-of-Expert layers (Section 2.1).
The model architecture parameters are summarized in Table 1.
2.1 Sparse Mixture of Experts
We present a brief overview of the Mixture of Experts layer (Figure 1).
For a more in-depth overview, see [ 12]. The output of the MoE module
for a given input xis determined by the weighted sum of the outputs
of the expert networks, where the weights are given by the gating
network’s output. i.e. given nexpert networks {E0, Ei, ..., E n−1}, the
output of the expert layer is given by:
n−1X
i=0G(x)i·Ei(x).
Here, G(x)idenotes the n-dimensional output of the gating network for the i-th expert, and Ei(x)
is the output of the i-th expert network. If the gating vector is sparse, we can avoid computing
the outputs of experts whose gates are zero. There are multiple alternative ways of implementing
G(x)[6,15,35], but a simple and performant one is implemented by taking the softmax over the
Top-K logits of a linear layer [28]. We use
G(x) := Softmax (TopK (x·Wg)),
where (TopK (ℓ))i:=ℓiifℓiis among the top-K coordinates of logits ℓ∈Rnand(TopK (ℓ))i:=−∞
otherwise. The value of K – the number of experts used per token – is a hyper-parameter that modu-
lates the amount of compute used to process each token. If one increases nwhile keeping Kfixed, one
1https://mistral.ai/news/mixtral-of-experts/
2can increase the model’s parameter count while keeping its computational cost effectively constant.
This motivates a distinction between the model’s total parameter count (commonly referenced as the
sparse parameter count), which grows with n, and the number of parameters used for processing an
individual token (called the active parameter count), which grows with Kup to n.
MoE layers can be run efficiently on single GPUs with high performance specialized kernels. For
example, Megablocks [ 13] casts the feed-forward network (FFN) operations of the MoE layer as large
sparse matrix multiplications, significantly enhancing the execution speed and naturally handling
cases where different experts get a variable number of tokens assigned to them. Moreover, the
MoE layer can be distributed to multiple GPUs through standard Model Parallelism techniques, and
through a particular kind of partitioning strategy called Expert Parallelism (EP) [ 28]. During the MoE
layer’s execution, tokens meant to be processed by a specific expert are routed to the corresponding
GPU for processing, and the expert’s output is returned to the original token location. Note that EP
introduces challenges in load balancing, as it is essential to distribute the workload evenly across the
GPUs to prevent overloading individual GPUs or hitting computational bottlenecks.
In a Transformer model, the MoE layer is applied independently per token and replaces the
feed-forward (FFN) sub-block of the transformer block. For Mixtral we use the same SwiGLU
architecture as the expert function Ei(x)and set K= 2. This means each token is routed to two
SwiGLU sub-blocks with different sets of weights. Taking this all together, the output yfor an input
token xis computed as:
y=n−1X
i=0Softmax (Top2 (x·Wg))i·SwiGLU i(x).
This formulation is similar to the GShard architecture [ 21], with the exceptions that we replace all
FFN sub-blocks by MoE layers while GShard replaces every other block, and that GShard uses a
more elaborate gating strategy for the second expert assigned to each token.
3 Results
We compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair
comparison. We measure performance on a wide variety of tasks categorized as follow:
•Commonsense Reasoning (0-shot): Hellaswag [ 32], Winogrande [ 26], PIQA [ 3], SIQA [ 27],
OpenbookQA [22], ARC-Easy, ARC-Challenge [8], CommonsenseQA [30]
•World Knowledge (5-shot): NaturalQuestions [20], TriviaQA [19]
•Reading Comprehension (0-shot): BoolQ [7], QuAC [5]
•Math: GSM8K [9] (8-shot) with maj@8 and MATH [17] (4-shot) with maj@4
•Code: Humaneval [4] (0-shot) and MBPP [1] (3-shot)
•Popular aggregated results: MMLU [ 16] (5-shot), BBH [ 29] (3-shot), and AGI Eval [ 34]
(3-5-shot, English multiple-choice questions only)
Figure 2: Performance of Mixtral and different Llama models on a wide range of benchmarks . All models
were re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mixtral outperforms or
matches Llama 2 70B on all benchmarks. In particular, it is vastly superior in mathematics and code generation.
3ModelActive
ParamsMMLU HellaS WinoG PIQA Arc-e Arc-c NQ TriQA HumanE MBPP Math GSM8K
LLaMA 2 7B 7B 44.4% 77.1% 69.5% 77.9% 68.7% 43.2% 17.5% 56.6% 11.6% 26.1% 3.9% 16.0%
LLaMA 2 13B 13B 55.6% 80.7% 72.9% 80.8% 75.2% 48.8% 16.7% 64.0% 18.9% 35.4% 6.0% 34.3%
LLaMA 1 33B 33B 56.8% 83.7% 76.2% 82.2% 79.6% 54.4% 24.1% 68.5% 25.0% 40.9% 8.4% 44.1%
LLaMA 2 70B 70B 69.9% 85.4% 80.4% 82.6% 79.9% 56.5% 25.4% 73.0% 29.3% 49.8% 13.8% 69.6%
Mistral 7B 7B 62.5% 81.0% 74.2% 82.2% 80.5% 54.9% 23.2% 62.5% 26.2% 50.2% 12.7% 50.0%
Mixtral 8x7B 13B 70.6% 84.4% 77.2% 83.6% 83.1% 59.7% 30.6% 71.5% 40.2% 60.7% 28.4% 74.4%
Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on
almost all popular benchmarks while using 5x fewer active parameters during inference.
Figure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension,
math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B) . Mixtral largely outperforms Llama 2 70B
on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It
is also vastly superior to Llama 2 70B on code and math.
Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2are reported
in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different
categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a
superior performance in code and mathematics benchmarks.
Size and Efficiency. We compare our performance to the Llama 2 family, aiming to understand
Mixtral models’ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture-
of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active
parameters, Mixtral is able to outperform Llama 2 70B across most categories.
Note that this analysis focuses on the active parameter count (see Section 2.1), which is directly
proportional to the inference compute cost, but does not consider the memory costs and hardware
utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count,
47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer
introduces additional overhead due to the routing mechanism and due to the increased memory loads
when running more than one expert per device. They are more suitable for batched workloads where
one can reach a good degree of arithmetic intensity.
Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B
compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the
two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller
capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest
GPT-3.5-Turbo model available, gpt-3.5-turbo-1106 .
2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.
4LLaMA 2 70B GPT-3.5 Mixtral 8x7B
MMLU
(MCQ in 57 subjects)69.9% 70.0% 70.6%
HellaSwag
(10-shot)87.1% 85.5% 86.7%
ARC Challenge
(25-shot)85.1% 85.2% 85.8%
WinoGrande
(5-shot)83.2% 81.6% 81.2%
MBPP
(pass@1)49.8% 52.2% 60.7%
GSM-8K
(5-shot)53.6% 57.1% 58.4%
MT Bench
(for Instruct Models)6.86 8.32 8.30
Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2
70B and GPT-3.5 performance on most metrics.
Evaluation Differences. On some benchmarks, there are some differences between our evaluation
protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2)
on TriviaQA, we do not provide Wikipedia contexts.
3.1 Multilingual benchmarks
Compared to Mistral 7B, we significantly upsample the proportion of multilingual data during
pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while
maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B
in French, German, Spanish, and Italian, as shown in Table 4.
Active
ParamsFrench German Spanish Italian
Model Arc-c HellaS MMLU Arc-c HellaS MMLU Arc-c HellaS MMLU Arc-c HellaS MMLU
LLaMA 1 33B 33B 39.3% 68.1% 49.9% 41.1% 63.3% 48.7% 45.7% 69.8% 52.3% 42.9% 65.4% 49.0%
LLaMA 2 70B 70B 49.9% 72.5% 64.3% 47.3% 68.7% 64.2% 50.5% 74.5% 66.0% 49.4% 70.9% 65.1%
Mixtral 8x7B 13B 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9%
Table 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On ARC Challenge, Hellaswag,
and MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and Italian.
3.2 Long range performance
To assess the capabilities of Mixtral to tackle long context, we evaluate it on the passkey retrieval
task introduced in [ 23], a synthetic task designed to measure the ability of the model to retrieve a
passkey inserted randomly in a long prompt. Results in Figure 4 (Left) show that Mixtral achieves a
100% retrieval accuracy regardless of the context length or the position of passkey in the sequence.
Figure 4 (Right) shows that the perplexity of Mixtral on a subset of the proof-pile dataset [ 2] decreases
monotonically as the size of the context increases.
Figure 4: Long range performance of Mixtral. (Left) Mixtral has 100% retrieval accuracy of the Passkey task
regardless of the location of the passkey and length of the input sequence. (Right) The perplexity of Mixtral on
the proof-pile dataset decreases monotonically as the context length increases.
53.3 Bias Benchmarks
Llama 2 70B Mixtral 8x7B
BBQ accuracy 51.5% 56.0%
BOLD sentiment score (avg ±std)
gender 0.293 ±0.073 0.323 ±0.045
profession 0.218 ±0.073 0.243 ±0.087
religious_ideology 0.188 ±0.133 0.144 ±0.089
political_ideology 0.149 ±0.140 0.186 ±0.146
race 0.232 ±0.049 0.232 ±0.052
Figure 5: Bias Benchmarks. Compared Llama 2 70B,
Mixtral presents less bias (higher accuracy on BBQ, lower
std on BOLD) and displays more positive sentiment (higher
avg on BOLD).To identify possible flaws to be corrected
by fine-tuning / preference modeling, we
measure the base model performance on
Bias Benchmark for QA (BBQ) [ 24] and
Bias in Open-Ended Language Generation
Dataset (BOLD) [ 10]. BBQ is a dataset
of hand-written question sets that target
attested social biases against nine differ-
ent socially-relevant categories: age, dis-
ability status, gender identity, nationality,
physical appearance, race/ethnicity, religion,
socio-economic status, sexual orientation.
BOLD is a large-scale dataset that consists
of 23,679 English text generation prompts
for bias benchmarking across five domains.
We benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report
the results in Table 5. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark
(56.0% vs 51.5%). For each group in BOLD, a higher average sentiment score means more positive
sentiments and a lower standard deviation indicates less bias within the group. Overall, Mixtral
displays more positive sentiments than Llama 2, with similar variances within each group.
4 Instruction Fine-tuning
We train Mixtral – Instruct using supervised fine-tuning (SFT) on an instruction dataset followed by
Direct Preference Optimization (DPO) [ 25] on a paired feedback dataset. Mixtral – Instruct reaches a
score of 8.30 on MT-Bench [ 33] (see Table 2), making it the best open-weights model as of December
2023. Independent human evaluation conducted by LMSys is reported in Figure 63and shows that
Mixtral – Instruct outperforms GPT-3.5-Turbo, Gemini Pro, Claude-2.1, and Llama 2 70B chat.
Figure 6: LMSys Leaderboard. (Screenshot from Dec 22, 2023) Mixtral 8x7B Instruct v0.1 achieves an Arena
Elo rating of 1121 outperforming Claude-2.1 (1117), all versions of GPT-3.5-Turbo (1117 best), Gemini Pro
(1111), and Llama-2-70b-chat (1077). Mixtral is currently the best open-weights model by a large margin.
3https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard
65 Routing analysis
In this section, we perform a small analysis on the expert selection by the router. In particular,
we are interested to see if during training some experts specialized to some specific domains (e.g.
mathematics, biology, philosophy, etc.).
To investigate this, we measure the distribution of selected experts on different subsets of The Pile
validation dataset [ 14]. Results are presented in Figure 7, for layers 0, 15, and 31 (layers 0 and 31
respectively being the first and the last layers of the model). Surprisingly, we do not observe obvious
patterns in the assignment of experts based on the topic. For instance, at all layers, the distribution of
expert assignment is very similar for ArXiv papers (written in Latex), for biology (PubMed Abstracts),
and for Philosophy (PhilPapers) documents.
Only for DM Mathematics we note a marginally different distribution of experts. This divergence is
likely a consequence of the dataset’s synthetic nature and its limited coverage of the natural language
spectrum, and is particularly noticeable at the first and last layers, where the hidden states are very
correlated to the input and output embeddings respectively.
This suggests that the router does exhibit some structured syntactic behavior. Figure 8 shows
examples of text from different domains (Python code, mathematics, and English), where each token
is highlighted with a background color corresponding to its selected expert. The figure shows that
words such as ‘self’ in Python and ‘Question’ in English often get routed through the same expert
even though they involve multiple tokens. Similarly, in code, the indentation tokens are always
assigned to the same experts, particularly at the first and last layers where the hidden states are more
correlated to the input and output of the model.
We also note from Figure 8 that consecutive tokens are often assigned the same experts. In fact, we
observe some degree of positional locality in The Pile datasets. Table 5 shows the proportion of con-
secutive tokens that get the same expert assignments per domain and layer. The proportion of repeated
00.050.100.150.20layer: 0
00.050.100.150.20layer: 15
0 1 2 3 4 5 6 700.050.100.150.20layer: 31
Expert IDSelection proportion
ArXiv
DM MathematicsGithub
GutenbergPhilPapers
PubMed AbstractsStackExchange
Wikipedia (en)
Figure 7: Proportion of tokens assigned to each expert on different domains from The Pile dataset for
layers 0, 15, and 31. The gray dashed vertical line marks 1/8, i.e. the proportion expected with uniform
sampling. Here, we consider experts that are either selected as a first or second choice by the router. A
breakdown of the proportion of assignments done in each case cane be seen in Figure 9 in the Appendix.
7First choice First or second choice
Layer 0 Layer 15 Layer 31 Layer 0 Layer 15 Layer 31
ArXiv 14.0% 27.9% 22.7% 46.5% 62.3% 52.9%
DM Mathematics 14.1% 28.4% 19.7% 44.9% 67.0% 44.5%
Github 14.9% 28.1% 19.7% 49.9% 66.9% 49.2%
Gutenberg 13.9% 26.1% 26.3% 49.5% 63.1% 52.2%
PhilPapers 13.6% 25.3% 22.1% 46.9% 61.9% 51.3%
PubMed Abstracts 14.2% 24.6% 22.0% 48.6% 61.6% 51.8%
StackExchange 13.6% 27.2% 23.6% 48.2% 64.6% 53.6%
Wikipedia (en) 14.4% 23.6% 25.3% 49.8% 62.1% 51.8%
Table 5: Percentage of expert assignment repetitions. We evaluate the proportion of times the same expert is
assigned to a token iand its following token i+1. We report whether the first chosen expert is the same, or whether
the same expert is observed as first or second choice in consecutive tokens. For reference, the expected proportion
of repetitions in the case of random assignments is1
8= 12.5%for “First choice” and 1−6
85
7≈46% for “First
and second choice”. Repetitions at the first layer are close to random, but are significantly higher at layers 15
and 31. The high number of repetitions shows that expert choice exhibits high temporal locality at these layers.
consecutive assignments is significantly higher than random for higher layers. This has implications
in how one might optimize the model for fast training and inference. For example, cases with high
locality are more likely to cause over-subscription of certain experts when doing Expert Parallelism.
Conversely, this locality can be leveraged for caching, as is done in [ 11]. A more complete view of
these same expert frequency is provided for all layers and across datasets in Figure 10 in the Appendix.
6 Conclusion
In this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the-
art performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem-
ini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each
time step, Mixtral only uses 13B active parameters per token while outperforming the previous best
model using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned mod-
els publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de-
velopment of new techniques and applications that can benefit a wide range of industries and domains.
Figure 8: Text samples where each token is colored with the first expert choice. The selection of experts
appears to be more aligned with the syntax rather than the domain, especially at the initial and final layers.
8Acknowledgements
We thank the CoreWeave and Scaleway teams for technical support as we trained our models. We
are grateful to NVIDIA for supporting us in integrating TensorRT-LLM and Triton and working
alongside us to make a sparse mixture of experts compatible with TensorRT-LLM.
References
[1]Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David
Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large
language models. arXiv preprint arXiv:2108.07732 , 2021.
[2]Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,
Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language
model for mathematics. arXiv preprint arXiv:2310.10631 , 2023.
[3]Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys-
ical commonsense in natural language. In Proceedings of the AAAI conference on artificial
intelligence , pages 7432–7439, 2020.
[4]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374 , 2021.
[5]Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and
Luke Zettlemoyer. Quac: Question answering in context. arXiv preprint arXiv:1808.07036 ,
2018.
[6]Aidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan
Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified
scaling laws for routed language models. In International Conference on Machine Learning ,
pages 4057–4086. PMLR, 2022.
[7]Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and
Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.
arXiv preprint arXiv:1905.10044 , 2019.
[8]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,
and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning
challenge. arXiv preprint arXiv:1803.05457 , 2018.
[9]Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to
solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.
[10] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei
Chang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended
language generation. In Proceedings of the 2021 ACM conference on fairness, accountability,
and transparency , pages 862–872, 2021.
[11] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with
offloading. arXiv preprint arXiv:2312.17238 , 2023.
[12] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning.
arXiv preprint arXiv:2209.01667 , 2022.
[13] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse
training with mixture-of-experts. arXiv preprint arXiv:2211.15841 , 2022.
[14] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse
text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.
[15] Hussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen,
Rahul Mazumder, Lichan Hong, and Ed Chi. Dselect-k: Differentiable selection in the mixture
of experts with applications to multi-task learning. Advances in Neural Information Processing
Systems , 34:29335–29347, 2021.
9[16] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and
Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint
arXiv:2009.03300 , 2020.
[17] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn
Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.
arXiv preprint arXiv:2103.03874 , 2021.
[18] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile
Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825 , 2023.
[19] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large
scale distantly supervised challenge dataset for reading comprehension. arXiv preprint
arXiv:1705.03551 , 2017.
[20] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris
Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a
benchmark for question answering research. Transactions of the Association for Computational
Linguistics , pages 453–466, 2019.
[21] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with condi-
tional computation and automatic sharding. arXiv preprint arXiv:2006.16668 , 2020.
[22] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789 ,
2018.
[23] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context
length for transformers. arXiv preprint arXiv:2305.16300 , 2023.
[24] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thomp-
son, Phu Mon Htut, and Samuel R Bowman. Bbq: A hand-built bias benchmark for question
answering. arXiv preprint arXiv:2110.08193 , 2021.
[25] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and
Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model.
arXiv preprint arXiv:2305.18290 , 2023.
[26] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An
adversarial winograd schema challenge at scale. Communications of the ACM , pages 99–106,
2021.
[27] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Com-
monsense reasoning about social interactions. arXiv preprint arXiv:1904.09728 , 2019.
[28] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts
layer. arXiv preprint arXiv:1701.06538 , 2017.
[29] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won
Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei.
Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint
arXiv:2210.09261 , 2022.
[30] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A ques-
tion answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937 ,
2018.
[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems , 30, 2017.
[32] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a
machine really finish your sentence? arXiv preprint arXiv:1905.07830 , 2019.
[33] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and
chatbot arena. arXiv preprint arXiv:2306.05685 , 2023.
10[34] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,
Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation
models. arXiv preprint arXiv:2304.06364 , 2023.
[35] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai,
Quoc V Le, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in
Neural Information Processing Systems , 35:7103–7114, 2022.
1100.10.20.3Layer 0 -- Either choice
00.10.20.3Layer 0 -- First choice
00.10.20.3Layer 0 -- Second choice
00.10.20.3Layer 15 -- Either choice
00.10.20.3Layer 15 -- First choice
00.10.20.3Layer 15 -- Second choice
00.10.20.3Layer 31 -- Either choice
00.10.20.3Layer 31 -- First choice
0 1 2 3 4 5 6 700.10.20.3Layer 31 -- Second choice
Expert IDSelection proportion
ArXiv
DM MathematicsGithub
GutenbergPhilPapers
PubMed AbstractsStackExchange
Wikipedia (en)Figure 9: Proportion of tokens assigned to each expert on different subsets from The Pile dataset, separated
by whether the expert was selected as first or second choice, or either. The “Either choice” case is equivalent
to Figure 7. The gray dashed vertical line marks1
8, i.e. the proportion expected with uniform sampling.
120.150.200.250.300.35First choice
0 10 20 300.50.60.7First or second choice
LayerProportion of repeated assignmentssource
ArXiv
DM Mathematics
Github
Gutenberg
PhilPapers
PubMed Abstracts
StackExchange
Wikipedia (en)Figure 10: Repeated consecutive assignments per MoE layer. Repeated assignments occur a lot more
often than they would with uniform assignments (materialized by the dashed lines). Patterns are similar across
datasets with less repetitions for DM Mathematics.
13Show and Tell: A Neural Image Caption Generator
Oriol Vinyals
Google
vinyals@google.comAlexander Toshev
Google
toshev@google.comSamy Bengio
Google
bengio@google.comDumitru Erhan
Google
dumitru@google.com
Abstract
Automatically describing the content of an image is a
fundamental problem in artiﬁcial intelligence that connects
computer vision and natural language processing. In this
paper, we present a generative model based on a deep re-
current architecture that combines recent advances in com-
puter vision and machine translation and that can be used
to generate natural sentences describing an image. The
model is trained to maximize the likelihood of the target de-
scription sentence given the training image. Experiments
on several datasets show the accuracy of the model and the
ﬂuency of the language it learns solely from image descrip-
tions. Our model is often quite accurate, which we verify
both qualitatively and quantitatively. For instance, while
the current state-of-the-art BLEU-1 score (the higher the
better) on the Pascal dataset is 25, our approach yields 59,
to be compared to human performance around 69. We also
show BLEU-1 score improvements on Flickr30k, from 56 to
66, and on SBU, from 19 to 28. Lastly, on the newly released
COCO dataset, we achieve a BLEU-4 of 27.7, which is the
current state-of-the-art.
1. Introduction
Being able to automatically describe the content of an
image using properly formed English sentences is a very
challenging task, but it could have great impact, for instance
by helping visually impaired people better understand the
content of images on the web. This task is signiﬁcantly
harder, for example, than the well-studied image classiﬁ-
cation or object recognition tasks, which have been a main
focus in the computer vision community [27]. Indeed, a
description must capture not only the objects contained in
an image, but it also must express how these objects relate
to each other as well as their attributes and the activities
they are involved in. Moreover, the above semantic knowl-
edge has to be expressed in a natural language like English,
which means that a language model is needed in addition to
visual understanding.
Most previous attempts have proposed to stitch together
A group of people shopping at an outdoor market. !There are many vegetables at the fruit stand.Vision!Deep CNNLanguage !Generating!RNN
Figure 1. NIC, our model, is based end-to-end on a neural net-
work consisting of a vision CNN followed by a language gener-
ating RNN. It generates complete sentences in natural language
from an input image, as shown on the example above.
existing solutions of the above sub-problems, in order to go
from an image to its description [6, 16]. In contrast, we
would like to present in this work a single joint model that
takes an image Ias input, and is trained to maximize the
likelihoodp(SjI)of producing a target sequence of words
S=fS1;S2;:::gwhere each word Stcomes from a given
dictionary, that describes the image adequately.
The main inspiration of our work comes from recent ad-
vances in machine translation, where the task is to transform
a sentenceSwritten in a source language, into its transla-
tionTin the target language, by maximizing p(TjS). For
many years, machine translation was also achieved by a se-
ries of separate tasks (translating words individually, align-
ing words, reordering, etc), but recent work has shown that
translation can be done in a much simpler way using Re-
current Neural Networks (RNNs) [3, 2, 30] and still reach
state-of-the-art performance. An “encoder” RNN reads the
source sentence and transforms it into a rich ﬁxed-length
vector representation, which in turn in used as the initial
hidden state of a “decoder” RNN that generates the target
sentence.
Here, we propose to follow this elegant recipe, replac-
ing the encoder RNN by a deep convolution neural network
(CNN). Over the last few years it has been convincingly
shown that CNNs can produce a rich representation of the
input image by embedding it to a ﬁxed-length vector, such
that this representation can be used for a variety of vision
1arXiv:1411.4555v2  [cs.CV]  20 Apr 2015tasks [28]. Hence, it is natural to use a CNN as an image
“encoder”, by ﬁrst pre-training it for an image classiﬁcation
task and using the last hidden layer as an input to the RNN
decoder that generates sentences (see Fig. 1). We call this
model the Neural Image Caption, or NIC.
Our contributions are as follows. First, we present an
end-to-end system for the problem. It is a neural net which
is fully trainable using stochastic gradient descent. Second,
our model combines state-of-art sub-networks for vision
and language models. These can be pre-trained on larger
corpora and thus can take advantage of additional data. Fi-
nally, it yields signiﬁcantly better performance compared
to state-of-the-art approaches; for instance, on the Pascal
dataset, NIC yielded a BLEU score of 59, to be compared to
the current state-of-the-art of 25, while human performance
reaches 69. On Flickr30k, we improve from 56 to 66, and
on SBU, from 19 to 28.
2. Related Work
The problem of generating natural language descriptions
from visual data has long been studied in computer vision,
but mainly for video [7, 32]. This has led to complex sys-
tems composed of visual primitive recognizers combined
with a structured formal language, e.g. And-Or Graphs or
logic systems, which are further converted to natural lan-
guage via rule-based systems. Such systems are heav-
ily hand-designed, relatively brittle and have been demon-
strated only on limited domains, e.g. trafﬁc scenes or sports.
The problem of still image description with natural text
has gained interest more recently. Leveraging recent ad-
vances in recognition of objects, their attributes and loca-
tions, allows us to drive natural language generation sys-
tems, though these are limited in their expressivity. Farhadi
et al. [6] use detections to infer a triplet of scene elements
which is converted to text using templates. Similarly, Li
et al. [19] start off with detections and piece together a ﬁ-
nal description using phrases containing detected objects
and relationships. A more complex graph of detections
beyond triplets is used by Kulkani et al. [16], but with
template-based text generation. More powerful language
models based on language parsing have been used as well
[23, 1, 17, 18, 5]. The above approaches have been able to
describe images “in the wild”, but they are heavily hand-
designed and rigid when it comes to text generation.
A large body of work has addressed the problem of rank-
ing descriptions for a given image [11, 8, 24]. Such ap-
proaches are based on the idea of co-embedding of images
and text in the same vector space. For an image query, de-
scriptions are retrieved which lie close to the image in the
embedding space. Most closely, neural networks are used to
co-embed images and sentences together [29] or even image
crops and subsentences [13] but do not attempt to generate
novel descriptions. In general, the above approaches cannotdescribe previously unseen compositions of objects, even
though the individual objects might have been observed in
the training data. Moreover, they avoid addressing the prob-
lem of evaluating how good a generated description is.
In this work we combine deep convolutional nets for im-
age classiﬁcation [12] with recurrent networks for sequence
modeling [10], to create a single network that generates de-
scriptions of images. The RNN is trained in the context of
this single “end-to-end” network. The model is inspired by
recent successes of sequence generation in machine trans-
lation [3, 2, 30], with the difference that instead of starting
with a sentence, we provide an image processed by a con-
volutional net. The closest works are by Kiros et al. [15]
who use a neural net, but a feedforward one, to predict the
next word given the image and previous words. A recent
work by Mao et al. [21] uses a recurrent NN for the same
prediction task. This is very similar to the present proposal
but there are a number of important differences: we use a
more powerful RNN model, and provide the visual input to
the RNN model directly, which makes it possible for the
RNN to keep track of the objects that have been explained
by the text. As a result of these seemingly insigniﬁcant dif-
ferences, our system achieves substantially better results on
the established benchmarks. Lastly, Kiros et al. [14] pro-
pose to construct a joint multimodal embedding space by
using a powerful computer vision model and an LSTM that
encodes text. In contrast to our approach, they use two sepa-
rate pathways (one for images, one for text) to deﬁne a joint
embedding, and, even though they can generate text, their
approach is highly tuned for ranking.
3. Model
In this paper, we propose a neural and probabilistic
framework to generate descriptions from images. Recent
advances in statistical machine translation have shown that,
given a powerful sequence model, it is possible to achieve
state-of-the-art results by directly maximizing the proba-
bility of the correct translation given an input sentence in
an “end-to-end” fashion – both for training and inference.
These models make use of a recurrent neural network which
encodes the variable length input into a ﬁxed dimensional
vector, and uses this representation to “decode” it to the de-
sired output sentence. Thus, it is natural to use the same ap-
proach where, given an image (instead of an input sentence
in the source language), one applies the same principle of
“translating” it into its description.
Thus, we propose to directly maximize the probability of
the correct description given the image by using the follow-
ing formulation:
?= arg max
X
(I;S)logp(SjI;) (1)
whereare the parameters of our model, Iis an image, andSits correct transcription. Since Srepresents any sentence,
its length is unbounded. Thus, it is common to apply the
chain rule to model the joint probability over S0;:::;SN,
whereNis the length of this particular example as
logp(SjI) =NX
t=0logp(StjI;S 0;:::;St 1) (2)
where we dropped the dependency on for convenience.
At training time, (S;I)is a training example pair, and we
optimize the sum of the log probabilities as described in (2)
over the whole training set using stochastic gradient descent
(further training details are given in Section 4).
It is natural to model p(StjI;S 0;:::;St 1)with a Re-
current Neural Network (RNN), where the variable number
of words we condition upon up to t 1is expressed by a
ﬁxed length hidden state or memory ht. This memory is
updated after seeing a new input xtby using a non-linear
functionf:
ht+1=f(ht;xt): (3)
To make the above RNN more concrete two crucial design
choices are to be made: what is the exact form of fand
how are the images and words fed as inputs xt. Forfwe
use a Long-Short Term Memory (LSTM) net, which has
shown state-of-the art performance on sequence tasks such
as translation. This model is outlined in the next section.
For the representation of images, we use a Convolutional
Neural Network (CNN). They have been widely used and
studied for image tasks, and are currently state-of-the art
for object recognition and detection. Our particular choice
of CNN uses a novel approach to batch normalization and
yields the current best performance on the ILSVRC 2014
classiﬁcation competition [12]. Furthermore, they have
been shown to generalize to other tasks such as scene clas-
siﬁcation by means of transfer learning [4]. The words are
represented with an embedding model.
3.1. LSTM-based Sentence Generator
The choice of fin (3) is governed by its ability to deal
with vanishing and exploding gradients [10], the most com-
mon challenge in designing and training RNNs. To address
this challenge, a particular form of recurrent nets, called
LSTM, was introduced [10] and applied with great success
to translation [3, 30] and sequence generation [9].
The core of the LSTM model is a memory cell cencod-
ing knowledge at every time step of what inputs have been
observed up to this step (see Figure 2) . The behavior of the
cell is controlled by “gates” – layers which are applied mul-
tiplicatively and thus can either keep a value from the gated
layer if the gate is 1or zero this value if the gate is 0. In
particular, three gates are being used which control whether
to forget the current cell value (forget gate f), if it should
hσσσc
inputLSTMmemory blockword predictionsoftmax
inputgate ioutputgate fforgetgate f
updatingtermct-1ctmt
xFigure 2. LSTM: the memory block contains a cell cwhich is
controlled by three gates. In blue we show the recurrent connec-
tions – the output mat time t 1is fed back to the memory at
time tvia the three gates; the cell value is fed back via the forget
gate; the predicted word at time t 1is fed back in addition to the
memory output mat time tinto the Softmax for word prediction.
read its input (input gate i) and whether to output the new
cell value (output gate o). The deﬁnition of the gates and
cell update and output are as follows:
it=(Wixxt+Wimmt 1) (4)
ft=(Wfxxt+Wfmmt 1) (5)
ot=(Woxxt+Wommt 1) (6)
ct=ftct 1+ith(Wcxxt+Wcmmt 1)(7)
mt=otct (8)
pt+1 =Softmax (mt) (9)
whererepresents the product with a gate value, and the
variousWmatrices are trained parameters. Such multi-
plicative gates make it possible to train the LSTM robustly
as these gates deal well with exploding and vanishing gra-
dients [10]. The nonlinearities are sigmoid ()and hyper-
bolic tangent h(). The last equation mtis what is used to
feed to a Softmax, which will produce a probability distri-
butionptover all words.
Training The LSTM model is trained to predict each
word of the sentence after it has seen the image as well
as all preceding words as deﬁned by p(StjI;S 0;:::;St 1).
For this purpose, it is instructive to think of the LSTM in un-
rolled form – a copy of the LSTM memory is created for theLSTMLSTMLSTMWeS1WeSN-1p1pNp2log p1(S1) log p2(S2) log pN(SN) ...LSTMWeS0S1SN-1S0
imageFigure 3. LSTM model combined with a CNN image embedder
(as deﬁned in [12]) and word embeddings. The unrolled connec-
tions between the LSTM memories are in blue and they corre-
spond to the recurrent connections in Figure 2. All LSTMs share
the same parameters.
image and each sentence word such that all LSTMs share
the same parameters and the output mt 1of the LSTM at
timet 1is fed to the LSTM at time t(see Figure 3). All
recurrent connections are transformed to feed-forward con-
nections in the unrolled version. In more detail, if we denote
byIthe input image and by S= (S0;:::;SN)a true sen-
tence describing this image, the unrolling procedure reads:
x 1=CNN (I) (10)
xt=WeSt; t2f0:::N 1g (11)
pt+1 =LSTM (xt); t2f0:::N 1g (12)
where we represent each word as a one-hot vector Stof
dimension equal to the size of the dictionary. Note that we
denote byS0a special start word and by SNa special stop
word which designates the start and end of the sentence. In
particular by emitting the stop word the LSTM signals that a
complete sentence has been generated. Both the image and
the words are mapped to the same space, the image by using
a vision CNN, the words by using word embedding We.
The imageIis only input once, at t= 1, to inform the
LSTM about the image contents. We empirically veriﬁed
that feeding the image at each time step as an extra input
yields inferior results, as the network can explicitly exploit
noise in the image and overﬁts more easily.
Our loss is the sum of the negative log likelihood of the
correct word at each step as follows:
L(I;S) = NX
t=1logpt(St): (13)
The above loss is minimized w.r.t. all the parameters of the
LSTM, the top layer of the image embedder CNN and word
embeddings We.Inference There are multiple approaches that can be used
to generate a sentence given an image, with NIC. The ﬁrst
one is Sampling where we just sample the ﬁrst word ac-
cording top1, then provide the corresponding embedding
as input and sample p2, continuing like this until we sample
the special end-of-sentence token or some maximum length.
The second one is BeamSearch : iteratively consider the set
of thekbest sentences up to time tas candidates to generate
sentences of size t+ 1, and keep only the resulting best k
of them. This better approximates S= arg max S0p(S0jI).
We used the BeamSearch approach in the following experi-
ments, with a beam of size 20. Using a beam size of 1 (i.e.,
greedy search) did degrade our results by 2 BLEU points on
average.
4. Experiments
We performed an extensive set of experiments to assess
the effectiveness of our model using several metrics, data
sources, and model architectures, in order to compare to
prior art.
4.1. Evaluation Metrics
Although it is sometimes not clear whether a description
should be deemed successful or not given an image, prior
art has proposed several evaluation metrics. The most re-
liable (but time consuming) is to ask for raters to give a
subjective score on the usefulness of each description given
the image. In this paper, we used this to reinforce that some
of the automatic metrics indeed correlate with this subjec-
tive score, following the guidelines proposed in [11], which
asks the graders to evaluate each generated sentence with a
scale from 1 to 41.
For this metric, we set up an Amazon Mechanical Turk
experiment. Each image was rated by 2 workers. The typ-
ical level of agreement between workers is 65%. In case
of disagreement we simply average the scores and record
the average as the score. For variance analysis, we perform
bootstrapping (re-sampling the results with replacement and
computing means/standard deviation over the resampled re-
sults). Like [11] we report the fraction of scores which are
larger or equal than a set of predeﬁned thresholds.
The rest of the metrics can be computed automatically
assuming one has access to groundtruth, i.e. human gen-
erated descriptions. The most commonly used metric so
far in the image description literature has been the BLEU
score [25], which is a form of precision of word n-grams
between generated and reference sentences2. Even though
1The raters are asked whether the image is described without any er-
rors, described with minor errors, with a somewhat related description, or
with an unrelated description, with a score of 4 being the best and 1 being
the worst.
2In this literature, most previous work report BLEU-1, i.e., they only
compute precision at the unigram level, whereas BLEU-n is a geometric
average of precision over 1- to n-grams.this metric has some obvious drawbacks, it has been shown
to correlate well with human evaluations. In this work,
we corroborate this as well, as we show in Section 4.3.
An extensive evaluation protocol, as well as the generated
outputs of our system, can be found at http://nic.
droppages.com/ .
Besides BLEU, one can use the perplexity of the model
for a given transcription (which is closely related to our
objective function in (1)). The perplexity is the geometric
mean of the inverse probability for each predicted word. We
used this metric to perform choices regarding model selec-
tion and hyperparameter tuning in our held-out set, but we
do not report it since BLEU is always preferred3. A much
more detailed discussion regarding metrics can be found in
[31], and research groups working on this topic have been
reporting other metrics which are deemed more appropriate
for evaluating caption. We report two such metrics - ME-
TEOR and Cider - hoping for much more discussion and
research to arise regarding the choice of metric.
Lastly, the current literature on image description has
also been using the proxy task of ranking a set of avail-
able descriptions with respect to a given image (see for in-
stance [14]). Doing so has the advantage that one can use
known ranking metrics like recall@k. On the other hand,
transforming the description generation task into a ranking
task is unsatisfactory: as the complexity of images to de-
scribe grows, together with its dictionary, the number of
possible sentences grows exponentially with the size of the
dictionary, and the likelihood that a predeﬁned sentence will
ﬁt a new image will go down unless the number of such
sentences also grows exponentially, which is not realistic;
not to mention the underlying computational complexity
of evaluating efﬁciently such a large corpus of stored sen-
tences for each image. The same argument has been used in
speech recognition, where one has to produce the sentence
corresponding to a given acoustic sequence; while early at-
tempts concentrated on classiﬁcation of isolated phonemes
or words, state-of-the-art approaches for this task are now
generative and can produce sentences from a large dictio-
nary.
Now that our models can generate descriptions of rea-
sonable quality, and despite the ambiguities of evaluating
an image description (where there could be multiple valid
descriptions not in the groundtruth) we believe we should
concentrate on evaluation metrics for the generation task
rather than for ranking.
4.2. Datasets
For evaluation we use a number of datasets which consist
of images and sentences in English describing these images.
3Even though it would be more desirable, optimizing for BLEU score
yields a discrete optimization problem. In general, perplexity and BLEU
scores are fairly correlated.The statistics of the datasets are as follows:
Dataset namesize
train valid. test
Pascal VOC 2008 [6] - - 1000
Flickr8k [26] 6000 1000 1000
Flickr30k [33] 28000 1000 1000
MSCOCO [20] 82783 40504 40775
SBU [24] 1M - -
With the exception of SBU, each image has been annotated
by labelers with 5 sentences that are relatively visual and
unbiased. SBU consists of descriptions given by image
owners when they uploaded them to Flickr. As such they
are not guaranteed to be visual or unbiased and thus this
dataset has more noise.
The Pascal dataset is customary used for testing only af-
ter a system has been trained on different data such as any of
the other four dataset. In the case of SBU, we hold out 1000
images for testing and train on the rest as used by [18]. Sim-
ilarly, we reserve 4K random images from the MSCOCO
validation set as test, called COCO-4k, and use it to report
results in the following section.
4.3. Results
Since our model is data driven and trained end-to-end,
and given the abundance of datasets, we wanted to an-
swer questions such as “how dataset size affects general-
ization”, “what kinds of transfer learning it would be able
to achieve”, and “how it would deal with weakly labeled
examples”. As a result, we performed experiments on ﬁve
different datasets, explained in Section 4.2, which enabled
us to understand our model in depth.
4.3.1 Training Details
Many of the challenges that we faced when training our
models had to do with overﬁtting. Indeed, purely supervised
approaches require large amounts of data, but the datasets
that are of high quality have less than 100000 images. The
task of assigning a description is strictly harder than object
classiﬁcation and data driven approaches have only recently
become dominant thanks to datasets as large as ImageNet
(with ten times more data than the datasets we described
in this paper, with the exception of SBU). As a result, we
believe that, even with the results we obtained which are
quite good, the advantage of our method versus most cur-
rent human-engineered approaches will only increase in the
next few years as training set sizes will grow.
Nonetheless, we explored several techniques to deal with
overﬁtting. The most obvious way to not overﬁt is to ini-
tialize the weights of the CNN component of our system
to a pretrained model (e.g., on ImageNet). We did this in
all the experiments (similar to [8]), and it did help quite alot in terms of generalization. Another set of weights that
could be sensibly initialized are We, the word embeddings.
We tried initializing them from a large news corpus [22],
but no signiﬁcant gains were observed, and we decided to
just leave them uninitialized for simplicity. Lastly, we did
some model level overﬁtting-avoiding techniques. We tried
dropout [34] and ensembling models, as well as exploring
the size (i.e., capacity) of the model by trading off number
of hidden units versus depth. Dropout and ensembling gave
a few BLEU points improvement, and that is what we report
throughout the paper.
We trained all sets of weights using stochastic gradi-
ent descent with ﬁxed learning rate and no momentum.
All weights were randomly initialized except for the CNN
weights, which we left unchanged because changing them
had a negative impact. We used 512 dimensions for the em-
beddings and the size of the LSTM memory.
Descriptions were preprocessed with basic tokenization,
keeping all words that appeared at least 5 times in the train-
ing set.
4.3.2 Generation Results
We report our main results on all the relevant datasets in Ta-
bles 1 and 2. Since PASCAL does not have a training set,
we used the system trained using MSCOCO (arguably the
largest and highest quality dataset for this task). The state-
of-the-art results for PASCAL and SBU did not use image
features based on deep learning, so arguably a big improve-
ment on those scores comes from that change alone. The
Flickr datasets have been used recently [11, 21, 14], but
mostly evaluated in a retrieval framework. A notable ex-
ception is [21], where they did both retrieval and genera-
tion, and which yields the best performance on the Flickr
datasets up to now.
Human scores in Table 2 were computed by comparing
one of the human captions against the other four. We do this
for each of the ﬁve raters, and average their BLEU scores.
Since this gives a slight advantage to our system, given the
BLEU score is computed against ﬁve reference sentences
and not four, we add back to the human scores the average
difference of having ﬁve references instead of four.
Given that the ﬁeld has seen signiﬁcant advances in the
last years, we do think it is more meaningful to report
BLEU-4, which is the standard in machine translation mov-
ing forward. Additionally, we report metrics shown to cor-
relate better with human evaluations in Table 14. Despite
recent efforts on better evaluation metrics [31], our model
fares strongly versus human raters. However, when evalu-
ating our captions using human raters (see Section 4.3.6),
our model fares much more poorly, suggesting more work
4We used the implementation of these metrics kindly provided in
http://www.mscoco.org .Metric BLEU-4 METEOR CIDER
NIC 27.7 23.7 85.5
Random 4.6 9.0 5.1
Nearest Neighbor 9.9 15.7 36.5
Human 21.7 25.2 85.4
Table 1. Scores on the MSCOCO development set.
Approach PASCAL Flickr Flickr SBU
(xfer) 30k 8k
Im2Text [24] 11
TreeTalk [18] 19
BabyTalk [16] 25
Tri5Sem [11] 48
m-RNN [21] 55 58
MNLM [14]556 51
SOTA 25 56 58 19
NIC 59 66 63 28
Human 69 68 70
Table 2. BLEU-1 scores. We only report previous work results
when available. SOTA stands for the current state-of-the-art.
is needed towards better metrics. On the ofﬁcial test set for
which labels are only available through the ofﬁcial website,
our model had a 27.2 BLEU-4.
4.3.3 Transfer Learning, Data Size and Label Quality
Since we have trained many models and we have several
testing sets, we wanted to study whether we could transfer
a model to a different dataset, and how much the mismatch
in domain would be compensated with e.g. higher quality
labels or more training data.
The most obvious case for transfer learning and data size
is between Flickr30k and Flickr8k. The two datasets are
similarly labeled as they were created by the same group.
Indeed, when training on Flickr30k (with about 4 times
more training data), the results obtained are 4 BLEU points
better. It is clear that in this case, we see gains by adding
more training data since the whole process is data-driven
and overﬁtting prone. MSCOCO is even bigger (5 times
more training data than Flickr30k), but since the collection
process was done differently, there are likely more differ-
ences in vocabulary and a larger mismatch. Indeed, all the
BLEU scores degrade by 10 points. Nonetheless, the de-
scriptions are still reasonable.
Since PASCAL has no ofﬁcial training set and was
collected independently of Flickr and MSCOCO, we re-
port transfer learning from MSCOCO (in Table 2). Doing
transfer learning from Flickr30k yielded worse results with
BLEU-1 at 53 (cf. 59).
Lastly, even though SBU has weak labeling (i.e., the la-
bels were captions and not human generated descriptions),
5We computed these BLEU scores with the outputs that the authors of
[14] kindly provided for their OxfordNet system.the task is much harder with a much larger and noisier vo-
cabulary. However, much more data is available for train-
ing. When running the MSCOCO model on SBU, our per-
formance degrades from 28 down to 16.
4.3.4 Generation Diversity Discussion
Having trained a generative model that gives p(SjI), an ob-
vious question is whether the model generates novel cap-
tions, and whether the generated captions are both diverse
and high quality. Table 3 shows some samples when re-
turning the N-best list from our beam search decoder in-
stead of the best hypothesis. Notice how the samples are di-
verse and may show different aspects from the same image.
The agreement in BLEU score between the top 15 generated
sentences is 58, which is similar to that of humans among
them. This indicates the amount of diversity our model gen-
erates. In bold are the sentences that are not present in the
training set. If we take the best candidate, the sentence is
present in the training set 80% of the times. This is not
too surprising given that the amount of training data is quite
small, so it is relatively easy for the model to pick “exem-
plar” sentences and use them to generate descriptions. If
we instead analyze the top 15 generated sentences, about
half of the times we see a completely novel description, but
still with a similar BLEU score, indicating that they are of
enough quality, yet they provide a healthy diversity.
A man throwing a frisbee in a park.
A man holding a frisbee in his hand.
A man standing in the grass with a frisbee.
A close up of a sandwich on a plate.
A close up of a plate of food with french fries.
A white plate topped with a cut in half sandwich.
A display case ﬁlled with lots of donuts.
A display case ﬁlled with lots of cakes.
A bakery display case ﬁlled with lots of donuts.
Table 3. N-best examples from the MSCOCO test set. Bold lines
indicate a novel sentence not present in the training set.
4.3.5 Ranking Results
While we think ranking is an unsatisfactory way to evalu-
ate description generation from images, many papers report
ranking scores, using the set of testing captions as candi-
dates to rank given a test image. The approach that works
best on these metrics (MNLM), speciﬁcally implemented a
ranking-aware loss. Nevertheless, NIC is doing surprisingly
well on both ranking tasks (ranking descriptions given im-
ages, and ranking images given descriptions), as can be seen
in Tables 4 and 5. Note that for the Image Annotation task,
we normalized our scores similar to what [21] used.ApproachImage Annotation Image Search
R@1 R@10 Med rR@1 R@10 Med r
DeFrag [13] 13 44 14 10 43 15
m-RNN [21] 15 49 11 12 42 15
MNLM [14] 18 55 8 13 52 10
NIC 20 61 6 19 64 5
Table 4. Recall@k and median rank on Flickr8k.
ApproachImage Annotation Image Search
R@1 R@10 Med rR@1 R@10 Med r
DeFrag [13] 16 55 8 10 45 13
m-RNN [21] 18 51 10 13 42 16
MNLM [14] 23 63 5 17 57 8
NIC 17 56 7 17 57 7
Table 5. Recall@k and median rank on Flickr30k.
Figure 4. Flickr-8k: NIC : predictions produced by NIC on the
Flickr8k test set (average score: 2.37); Pascal: NIC : (average
score: 2.45); COCO-1k: NIC : A subset of 1000 images from the
MSCOCO test set with descriptions produced by NIC (average
score: 2.72); Flickr-8k: ref : these are results from [11] on Flickr8k
rated using the same protocol, as a baseline (average score: 2.08);
Flickr-8k: GT : we rated the groundtruth labels from Flickr8k us-
ing the same protocol. This provides us with a “calibration” of the
scores (average score: 3.89)
4.3.6 Human Evaluation
Figure 4 shows the result of the human evaluations of the
descriptions provided by NIC, as well as a reference system
and groundtruth on various datasets. We can see that NIC
is better than the reference system, but clearly worse than
the groundtruth, as expected. This shows that BLEU is not
a perfect metric, as it does not capture well the difference
between NIC and human descriptions assessed by raters.
Examples of rated images can be seen in Figure 5. It is
interesting to see, for instance in the second image of the
ﬁrst column, how the model was able to notice the frisbee
given its size.Figure 5. A selection of evaluation results, grouped by human rating.
4.3.7 Analysis of Embeddings
In order to represent the previous word St 1as input to
the decoding LSTM producing St, we use word embedding
vectors [22], which have the advantage of being indepen-
dent of the size of the dictionary (contrary to a simpler one-
hot-encoding approach). Furthermore, these word embed-
dings can be jointly trained with the rest of the model. It
is remarkable to see how the learned representations have
captured some semantic from the statistics of the language.
Table 4.3.7 shows, for a few example words, the nearest
other words found in the learned embedding space.
Note how some of the relationships learned by the model
will help the vision component. Indeed, having “horse”,
“pony”, and “donkey” close to each other will encourage the
CNN to extract features that are relevant to horse-looking
animals. We hypothesize that, in the extreme case where
we see very few examples of a class (e.g., “unicorn”), its
proximity to other word embeddings (e.g., “horse”) should
provide a lot more information that would be completely
lost with more traditional bag-of-words based approaches.
5. Conclusion
We have presented NIC, an end-to-end neural network
system that can automatically view an image and generateWord Neighbors
car van, cab, suv, vehicule, jeep
boy toddler, gentleman, daughter, son
street road, streets, highway, freeway
horse pony, donkey, pig, goat, mule
computer computers, pc, crt, chip, compute
Table 6. Nearest neighbors of a few example words
a reasonable description in plain English. NIC is based on
a convolution neural network that encodes an image into a
compact representation, followed by a recurrent neural net-
work that generates a corresponding sentence. The model is
trained to maximize the likelihood of the sentence given the
image. Experiments on several datasets show the robust-
ness of NIC in terms of qualitative results (the generated
sentences are very reasonable) and quantitative evaluations,
using either ranking metrics or BLEU, a metric used in ma-
chine translation to evaluate the quality of generated sen-
tences. It is clear from these experiments that, as the size
of the available datasets for image description increases, so
will the performance of approaches like NIC. Furthermore,
it will be interesting to see how one can use unsupervised
data, both from images alone and text alone, to improve im-
age description approaches.Acknowledgement
We would like to thank Geoffrey Hinton, Ilya Sutskever,
Quoc Le, Vincent Vanhoucke, and Jeff Dean for useful dis-
cussions on the ideas behind the paper, and the write up.
References
[1] A. Aker and R. Gaizauskas. Generating image descriptions
using dependency relational patterns. In ACL, 2010.
[2] D. Bahdanau, K. Cho, and Y . Bengio. Neural ma-
chine translation by jointly learning to align and translate.
arXiv:1409.0473 , 2014.
[3] K. Cho, B. van Merrienboer, C. Gulcehre, F. Bougares,
H. Schwenk, and Y . Bengio. Learning phrase representations
using RNN encoder-decoder for statistical machine transla-
tion. In EMNLP , 2014.
[4] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-
vation feature for generic visual recognition. In ICML , 2014.
[5] D. Elliott and F. Keller. Image description using visual de-
pendency representations. In EMNLP , 2013.
[6] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young,
C. Rashtchian, J. Hockenmaier, and D. Forsyth. Every pic-
ture tells a story: Generating sentences from images. In
ECCV , 2010.
[7] R. Gerber and H.-H. Nagel. Knowledge representation for
the generation of quantiﬁed natural language descriptions of
vehicle trafﬁc in image sequences. In ICIP . IEEE, 1996.
[8] Y . Gong, L. Wang, M. Hodosh, J. Hockenmaier, and
S. Lazebnik. Improving image-sentence embeddings using
large weakly annotated photo collections. In ECCV , 2014.
[9] A. Graves. Generating sequences with recurrent neural net-
works. arXiv:1308.0850 , 2013.
[10] S. Hochreiter and J. Schmidhuber. Long short-term memory.
Neural Computation , 9(8), 1997.
[11] M. Hodosh, P. Young, and J. Hockenmaier. Framing image
description as a ranking task: Data, models and evaluation
metrics. JAIR , 47, 2013.
[12] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
arXiv:1502.03167 , 2015.
[13] A. Karpathy, A. Joulin, and L. Fei-Fei. Deep fragment em-
beddings for bidirectional image sentence mapping. NIPS ,
2014.
[14] R. Kiros, R. Salakhutdinov, and R. S. Zemel. Unifying
visual-semantic embeddings with multimodal neural lan-
guage models. In arXiv:1411.2539 , 2014.
[15] R. Kiros and R. Z. R. Salakhutdinov. Multimodal neural lan-
guage models. In NIPS Deep Learning Workshop , 2013.
[16] G. Kulkarni, V . Premraj, S. Dhar, S. Li, Y . Choi, A. C. Berg,
and T. L. Berg. Baby talk: Understanding and generating
simple image descriptions. In CVPR , 2011.
[17] P. Kuznetsova, V . Ordonez, A. C. Berg, T. L. Berg, and
Y . Choi. Collective generation of natural image descriptions.
InACL, 2012.[18] P. Kuznetsova, V . Ordonez, T. Berg, and Y . Choi. Treetalk:
Composition and compression of trees for image descrip-
tions. ACL, 2(10), 2014.
[19] S. Li, G. Kulkarni, T. L. Berg, A. C. Berg, and Y . Choi. Com-
posing simple image descriptions using web-scale n-grams.
InConference on Computational Natural Language Learn-
ing, 2011.
[20] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll ´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. arXiv:1405.0312 , 2014.
[21] J. Mao, W. Xu, Y . Yang, J. Wang, and A. Yuille. Ex-
plain images with multimodal recurrent neural networks. In
arXiv:1410.1090 , 2014.
[22] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient
estimation of word representations in vector space. In ICLR ,
2013.
[23] M. Mitchell, X. Han, J. Dodge, A. Mensch, A. Goyal, A. C.
Berg, K. Yamaguchi, T. L. Berg, K. Stratos, and H. D. III.
Midge: Generating image descriptions from computer vision
detections. In EACL , 2012.
[24] V . Ordonez, G. Kulkarni, and T. L. Berg. Im2text: Describ-
ing images using 1 million captioned photographs. In NIPS ,
2011.
[25] K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: A
method for automatic evaluation of machine translation. In
ACL, 2002.
[26] C. Rashtchian, P. Young, M. Hodosh, and J. Hockenmaier.
Collecting image annotations using amazon’s mechanical
turk. In NAACL HLT Workshop on Creating Speech and
Language Data with Amazon’s Mechanical Turk , pages 139–
147, 2010.
[27] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual
Recognition Challenge, 2014.
[28] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,
and Y . LeCun. Overfeat: Integrated recognition, localization
and detection using convolutional networks. arXiv preprint
arXiv:1312.6229 , 2013.
[29] R. Socher, A. Karpathy, Q. V . Le, C. Manning, and A. Y . Ng.
Grounded compositional semantics for ﬁnding and describ-
ing images with sentences. In ACL, 2014.
[30] I. Sutskever, O. Vinyals, and Q. V . Le. Sequence to sequence
learning with neural networks. In NIPS , 2014.
[31] R. Vedantam, C. L. Zitnick, and D. Parikh. CIDEr:
Consensus-based image description evaluation. In
arXiv:1411.5726 , 2015.
[32] B. Z. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu. I2t:
Image parsing to text description. Proceedings of the IEEE ,
98(8), 2010.
[33] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From im-
age descriptions to visual denotations: New similarity met-
rics for semantic inference over event descriptions. In ACL,
2014.
[34] W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent neural
network regularization. In arXiv:1409.2329 , 2014.Credit Card Fraud Detection Using Autoencoder 
Neural Network  
 
Ping Jiang (M.Eng)  Jinliang Zhang (M.Eng)  Junyi Zou (M.Eng)  
Department of Electrical & Computer 
Engineer  Department of Electrical & Computer 
Engineer  Department of Electrical & Computer 
Engineer  
University of Western Ontario  University of Western Ontario  University of Western Ontario  
pjiang28@uwo.ca  jzhan964@uwo.ca  jzou44@uwo.ca  
250985261  250919668  250833154  
 
Abstract—Imbalanced data classification problem 
has always been a popular topic  in the field of machine 
learning research.  In order to balance the samples 
between majority and minority class.  Oversampling  
algorithm is used to synthesiz e new minority class 
samples , but it could bring in noise.  Pointing to the 
noise proble ms, this paper  proposed a denoising 
autoencoder  neural network (DAE) algorithm which 
can not only oversample minority class sample  through 
misclassification cost, but it can denoise and classify  the 
sampled dataset. Through  experiment s, compared with 
the denoising  autoencoder neural network ( DAE) with 
oversampling process  and traditional fully connected 
neural networks,  the results showed  the proposed 
algorithm improves the  classification accuracy of 
minority class of imbalanced  datasets.  
Keywords —imbalanc ed data; oversampling;  denoising 
autoencoder neural network; classificatio n 
 
I. INTRODUCTION  
Credit card fraud is a growing threat  with far 
reaching consequences in the finance industry, 
corpor ation s and government. Fraud can be defined as 
criminal deception with intent  of acquiring financial 
gain. As credit card became the most popular method 
of payment for both online and offline transaction, the 
fraud rate also accelerates. The main reasons for fraud 
is due to the lack of security, which involves the use of 
stolen credit card to get cash from bank through 
legitimate access. T his results in high difficulty of 
preventing credit card fraud.  
So how to do fraud detection is very significant. A 
lot of researches have been proposed to the detection of 
such credit card fraud, which account for majority of 
credit card frauds. Detecting using traditional method is infeasible because of the big data. However, 
financial institutions have focused their attention to 
recent computational methodologies to handle c redit 
card fraud problem.  
Classification problem is one of the key research 
topics in the field of machine learning. Currently 
available classification methods can only achieve 
preferable performance on balanced datasets. 
However, there are a large number of imbalanced 
datasets in pract ical application. For the fraud problem, 
the minority class, which is the abnormal transaction, 
is more important [ 1]. For instance, when minority 
class accounts for less than 1 percent of the total 
dataset, the overall accur acy reaches more than 99% 
even though all the minority class has been 
misclassified.  
Minority class sampling is a common method to 
handle with the imbalanced data classification 
problem. The main purpose of oversampling is to 
increase the number of minori ty class samples so that 
the original classification information can get better 
retention. Therefore, in the fields where there is higher 
demand for the classification accuracy, oversampling 
algorithm is chosen in general.  
This paper seeks to implement cre dit card fraud 
detection using denoising autoencoder and 
oversampling. For imbalanced data, we decided use 
above method to achieve proper model.  
 
II. RELATED  WORKS  
 Data mining technique is one notable methods used 
in solving fraud detection problem. This is the process 
of identifying those transactions  that are belong to 
frauds or not, which is based on the behaviors and 
habits of cardholder, many techniques have been applied to this area, artificial neural network  [2], 
genetic algorithm, support vector ma chine, frequent 
item set mining, decision tree, migrating birds 
optimization algorithm, Naïve Bayes. A comparative 
analy sis of logistic regression and Naïve Bayes is 
carried out in [3 ]. The performance of  Bayesian and 
neural network [4 ] is evaluated on cre dit card fraud 
data. Decision tree, neural networks and logistic 
regression are tested for their applicability  in fraud 
detections [5 ].  
 In a seminar work,  [6] propose s two advanced data 
mining approaches, support vector machines and 
random forests, toget her with logistic regression, as 
part of an attempt to better detect credit card fraud 
while neural network and logistic regression is applied 
on credit card fraud d etection problem [7 ]. A number 
of challenges are associated with credit card detection, 
namely fraudulent behavior profile is dynamic, that is 
fraudulent transactions tend to look like legitimate 
ones; credit card transaction datasets are rarely 
available and highly imbalanced (or skewed); optimal 
feature (variables) selection for the models; su itable 
metric to evaluate performance of techniques on 
skewed credit card fraud data. Credit card fraud 
detection performance is greatly affected by type of 
sampling approach used, selection of variables and 
detection technique(s) used.  
 
III. BACKGROUND  
3.1 Autoencoder  
A. Traditional Autoencoder Neural Network (AE)   
 Autoencoder is an artificial neural network used for 
unsupervised learning. The aim of autoencoder is to 
learn representations to reconstructs features for a set 
of data, typically for the pur pose of dimensionality 
reduction. The simplest form of an autoencoder is a 
feedforward, non -recurrent neural network which is 
similar to the  multilayer perceptron  [8]. As the figure 1 
shown, i t has 2 parts: one is encoder and the other is 
decoder which are  consist of by an input layer, one or 
more hidden layers and an output layer. The significant 
difference between autoencoder and multiplayer 
perceptron is that the output layer of autoencoder has 
the same number of neurons as the input layer. The 
purpose i s to reconstruct  its own inputs instead of 
predicting the target value from the  given inputs.   
Fig. 1  architecture of autoencoder neural network  
 In autoencoder, the network structure has 
connections between layers, but has no connection 
inside each laye r, 𝑥𝑖 is input sample, 𝑥𝑖̂ is output 
feature.  
 The training of autoencoder neural network is to 
optimize reconstruction error using the given samples. 
The cost function of autoencoder neural network 
defined in the project is (1)  
𝐽𝐴,𝐸= 1
𝑚 ∑ (1
2  ‖𝑥𝑖̂−𝑥𝑖‖2)𝑚
𝑖=1               (1) 
where m represents number of input samples.  
B. Denoising Autoencoder Neural Network (DAE)  
 For human, when people  see an object, if there is a 
small part of the object is blocked, they can still 
recognize it.  But how the autoenc oder does for the 
“contaminated” data ? There is a variation of traditional 
autoencoder named denoising autoencoder which 
could m ake autoencoder neural network learn  how to 
remove the noise and  reconstruct undisturbed input  as 
much as possible  [9]. 
 As show n in figure  2, the original data is  x, and 𝑥̃ 
is the data corrupted with  noise. Through the complete 
process of denoising autoencoder, the output is 𝑥̂. The 
loss function tr ies to minimize the difference between 
the output and the original data so that the  autoencoder 
has the ability of eliminating the influence of noise and 
extracting features from the corrupted data. Therefore, 
the features generated from  the learning of input 
corrupted with noise are more robust,  which improved 
the data generalization ab ility of  autoencoder neural 
network model to input data .  
 
Fig. 2  Denoising autoencoder neural network  
The commonly used noises are Gaussian noise, 
and Salt  and pepper noise. And the  cost function of 
denoising autoencoder  neural network is defined 
according to  (2) 
𝐽𝐷𝐴,𝐸= 1
𝑚 ∑ (1
2  ‖𝑥𝑖̂−𝑥𝑖‖2)𝑚
𝑖=1            (2) 
where 𝑥̂=  𝑓(∑(𝑤𝑥̃+𝑏)), w represents weights 
and b represents bias.  
3.2 Oversampling  
Imbalanced data set is a common problem faced in 
machine learning, s ince most traditional machine 
learning classification model  can’t handle imbalanced 
dataset . High misclassification cost often happened on 
minority class, because classification model will try to 
classify all the data sample to the majority class.  
Oversamplin g is a technique used to deal with 
imbalanced dataset, its subject to create specific class 
sample so the class distribution of the original dataset 
can be balanced.  The benefit of using o versampling is 
shown in figure 3 . 
 
Fig. 3  Benefit of using oversamp ling 
SMOTE (Synthetic Minority Oversampling 
Technique) is one of the most popular oversampling 
technique. In order to create a synthetic data point, first 
we need to find a k -nearest -neighbors cluster in the 
feature space, then randomly find a point within  this 
cluster, finally using weighted average to “forge” the 
new data point.  
3.3 Classification f ully connected  model  
Deep fully connected neural network is often used 
in classification problem, with SoftMax cross entropy 
as the loss function, deep learnin g classification model 
can achieve very high accuracy.  
The SoftMax function is often used in the final 
layer of a neural network -based classifier, it first 
calculate s the exponential value of each output, then 
normalize all the output and let the sum of th e output 
equal to 1. SoftMax function is often used for 
probability distribution transformation, since the 
output of SoftMax function is within  range 0 to 1 that 
add up to 1, shown in the formula 3 , 
      P(𝑦𝑖|𝑥𝑖;W)=𝑒𝑓𝑦𝑖
∑𝑒𝑓𝑗𝑗                      (3) Entropy is a measure for information contents and 
could be defined as the unpredictability of an event. So, 
the greater the probability is, the smaller the 
unpredictability is, which means the information 
contents is also very small. If an event occurs inevitably 
with the probability of 100%, then the unpredictability 
and information content are 0. cross -entropy loss 
function takes advantages of feature of entropy 
equation, cross -entropy loss function can measure the 
good ness of a classification mo del, w hich is shown in 
formula 4 , 
J(θ)=−1
𝑚∑ ∑ 1{𝑦𝑖=𝑗}𝑙𝑜𝑔𝑒𝜃𝑗𝑇𝑥𝑖
∑ 𝑒𝜃𝑗𝑇𝑥𝑖 𝑘
𝑖=1𝑘
𝑗=1𝑚
𝑖=1    (4) 
Cross -entropy can be used in multi -classification 
problems with the combination of SoftMax (do not 
consider regularization). Compared with quadratic loss 
function, cross -entropy loss function gives better 
training performance on neural networks.  
3.4 Model evaluation  metric  
Accuracy is not sufficient to evaluate a 
classification model, especially for imbalanced dataset. 
For example, an imbalanced dataset with 99.9% of 
normal data and 0.1% of abnormal data, if the 
classification labels all the sample as normal class, the 
model can still achieve 99.9% accuracy. However, for 
anomaly detection, the detection rate of anomaly class  
is very important. Confusion m atrix is often used in this 
situation.  
Table 1.  Confusion matrix for two -class problem  
 
Recall (Detection rate) is the ratio between the 
number of correctly detected anomalies and the total 
number of anomalies, it evaluates how much of the 
anomalies can be detected in this classification model.  
 
IV. METHODOLOGY  
The credit card fraud transaction dataset we are 
using is downloaded from Kaggle, with totally 28315 
transaction detail and 0.5% of them are labeled as 
fraud , the dataset is shown in the fig 4 . The  subject is 
to build a classification model for anomaly detection. 
Dataset  contains only numerical input after doing PCA 
transformation. Features V1, V2, ... V28 are the 
principal components, the only features which have not 
been transformed with PCA are ‘ Time’ and ‘Amount’. 
Feature 'Class' is the response variable and it takes 
value 1 in case of fraud and 0 otherwise.  
 
Fig. 4 Relationship between two classes  
The idea is very straight forward. First, use 
oversampling to transform imbalanced dataset to 
balanced dataset. Then use denoised autoencoder to get 
denoised dataset. Finally using deep fully connected 
neural network model for final classification.  
 
Fig. 5 Flowchart of the porcess  
4.1 Data Preprocessing  
For dataset preprocessing, drop “TIME” data, and 
normalized the “AMOUNT” part. Other features are 
obtained by PCA, do not need to do normalization. 
Then choose the test sample, which account for 20% of 
the total sample.  
4.2 Oversampling  
Our group only perform oversampling on the 
training dataset. Before oversampling, there are total 
22652 transaction records in training dataset, with 
22538 samples in normal class and 114 samples in 
abnormal class. After over sampling, the training 
dataset contains 22538 samples in normal class and 
22538 samples in abnormal class.  
4.3 Denoising  autoencoder  
Our group designed a 7 layers  autoencoder for 
dataset denoising  process . After we got balanced 
training dat aset from oversa mpling, we add G aussian 
noise to the training dataset, then feed the training 
dataset into this denoised autoencoder. After training 
this denoised autoencoder model, this autoencoder has 
the capability to denoise the testing dataset in the 
prediction proce ss. Table 2.  Model design for denoised autoencoder  
Dataset with noise (29)  
Fully -Connected -Layer (22)  
Fully -Connected -Layer ( 15) 
Fully -Connected -Layer ( 10) 
Fully -Connected -Layer ( 15) 
Fully -Connected -Layer ( 22) 
Fully -Connected -Layer ( 29) 
Square Loss  Function  
4.4 Classifier  
Our group designed a 6 layers autoencoder for 
dataset denoise process . After we got denoised training 
dataset from denoised autoencoder, we feed the 
training dataset into this deep fully connected neural 
network classifier. In the  end, we are using SoftMax 
with cross -entropy as the loss function for final 
classification.  
Table 3.  Model design for classifier  
Denoised Dataset (29)  
Fully -Connected -Layer (22)  
Fully -Connected -Layer (15)  
Fully -Connected -Layer (10)  
Fully -Connected -Layer (5)  
Fully -Connected -Layer (2)  
SoftMax Cross Entropy Loss Function  
 
V. EVALUATION  AND  RESULTS  
This section first discusses the implementation 
details, then presents evaluation results comparing the 
oversampling model with model with out 
oversampling.  
5.1 Implementation details  
Our group using built -in function from “sklearn” 
package for dataset normalization, and built -in function 
“SMOTE” from “imblearn” package for oversampling. 
In addition, we implement the denoised autoencoder 
mode l and deep fully connected neural network 
classifier with “TensorFlow”.  We choose 
“TensorFlow” because its capable of GPU 
acceleration. All models are trained on GTX 1060 
discrete GPU w/6GB GDDR5 graphics memory. It 
took 10 minutes for each model to conver ge. 
5.2 Results  
After the training process, we perform evaluation 
process using another separated evaluation dataset. the 
accuracy rate and recall rate are applied to evaluate the 
accuracy of each model. Th e results are shown in the 
fig 6 and fig 7. 
 
Fig. 6 Result for model 1  
 
Fig. 7 Result for model 2  
For model 1 without the usage of oversampling and 
autoencoder, the recall rate is very low, because the 
model classifies all the sample as normal, which means 
most fraud transaction is not detected. For model 2 with 
oversampling and autoencoder , the recall rate is 
acceptable, which means most fraud transaction can be 
detected. Some evaluation result  of model 2 is showed 
in Table 4.  
Table 4.  Model 2 Evaluation Result  
Threshold  Recall Rate  Accuracy  
0.2 90.66%  83.56%  
0.3 89.33%  90.93%  0.4 88% 94.58%  
0.5 86.66%  96.73%  
0.6 84% 97.93%  
 
VI. CONCLUSION  
In machine learning area, imbalance data 
classification receives increasing attention as big data 
become popular. On account of the drawbacks of 
traditional method, oversampling algorithm and 
autoencoder can be used.  This study combined stacked 
denoising autoencoder neural network  with 
oversampling to build the model, which can achieve 
minority class sampling on the basis of 
misclassification cost, and denoise and classify the 
sampled datasets. The proposed algorithm increases  
classification accuracy of minority class  compared to 
the former methods, we can achieve different accuracy 
by controlling the threshold. In this study, when 
threshold equal to 0.6, we can achieve the best 
performance, which is 97.93%. However, t he 
dimensionality reduction of high -dimensional data still 
need to be further researched.  
 
REFERENCES  
[1] Y. Sahin, S. Bulkan, and E. Duman, “A cost -sensitive decision 
tree approach for fraud detection,” Expert Systems with 
Applications,vol. 40, pp. 5916 -5923, 2013.  
[2] Ogwueleka, F. N., (2011). Data Mining Application in Credit  
Card Fraud Detection System, Journal of Engineering Science  and 
Technology, Vol. 6, No. 3, pp. 311 – 322 
[3] Ng, A. Y., and Jordan, M. I., (2002). On discriminative vs. 
generative c lassifiers: A comparison of logistic regression and naive 
bayes. Advances in neural information processing systems, 2, 841 -
848. 
[4] Maes, S., Tuyls, K., Vanschoenwinkel, B., & Manderick, B. 
(2002). Credit card fraud detection using Bayesian and neural 
netw orks. In Proceedings of the 1st international naiso congress on 
neuro fuzzy technologies (pp. 261 -270).  
[5] Shen, A., Tong, R., & Deng, Y. (2007). Application of  
classification models on credit card fraud detection. In Service 
Systems and Service Managemen t, 2007 International Conference 
on (pp. 1 -4). IEEE.  
[6] Bhattacharyya, S., Jha, S., Tharakunnel, K., & Westland, J. C. 
(2011). Data mining for credit card fraud: A comparative study. 
Decision Support Systems, 50(3), 602 -613. 
[7] Sahin, Y. and Duman, E., (2011). Detecting credit card fraud by 
ANN and logistic regression. In Innovations in Intelligent Systems 
and Applications (INISTA), 2011 International Symposium on (pp. 
315-319). IEEE.  
[8] Autoencoder for Words, Liou, C. -Y., Cheng, C. -W., Liou, J. -W., 
and Liou, D. -R., Neurocomputing, Volume 139, 84 –96 (2014), 
doi:10.1016/j.neucom.2013.09.055  
[9] M. Koziarski and M. Wożniak, "CCR: A combined cleaning and 
resampling algorithm for imbalanced data classification", 
International Journal of Applied Mathematics a nd Computer 
Science, vol. 27, no. 4, 2017.  
 
A Neural Representation of Sketch Drawings
David Ha
Google Brain
hadavid@google.comDouglas Eck
Google Brain
deck@google.com
Abstract
We present sketch-rnn , a recurrent neural network (RNN) able to construct
stroke-based drawings of common objects. The model is trained on a dataset of
human-drawn images representing many different classes. We outline a framework
for conditional and unconditional sketch generation, and describe new robust
training methods for generating coherent sketch drawings in a vector format.
1 Introduction
Recently, there have been major advancements in generative modelling of images using neural
networks as a generative tool. Generative Adversarial Networks (GANs) [ 5], Variational Inference
(VI) [ 15], and Autoregressive (AR) [ 19] models have become popular tools in this fast growing area.
Most of the work thus far has been targeted towards modelling low resolution, pixel images. Humans,
however, do not understand the world as a grid of pixels, but rather develop abstract concepts to
represent what we see. From a young age, we develop the ability to communicate what we see
by drawing on paper with a pencil or crayon. In this way we learn to express a sequential, vector
representation of an image as a short sequence of strokes. In this paper we investigate an alternative
to traditional pixel image modelling approaches, and propose a generative model for vector images.
Figure 1: Latent space interpolation of various vector images produced by our model.
Our goal is to train machines to draw and generalize abstract concepts in a manner similar to humans.
As a ﬁrst step towards this goal, we train our model on a dataset of hand-drawn sketches, each
represented as a sequence of motor actions controlling a pen: which direction to move, when to lift
the pen up, and when to stop drawing. In doing so, we created a model that potentially has many
applications, from assisting the creative process of an artist, to helping teach students how to draw.
This paper makes the following contributions: We outline a framework for both unconditional and
conditional generation of vector images composed of a sequence of lines. Our recurrent neural
network-based generative model is capable of producing sketches of common objects in a vector
format. We develop a training procedure unique to vector images to make the training more robust. In
arXiv:1704.03477v4  [cs.NE]  19 May 2017the conditional generation model, we explore the latent space developed by the model to represent a
vector image. We also discuss potential creative applications of our methodology. We make available
a large dataset of hand drawn vector images to encourage further development of generative modelling
for vector images, and also release an implementation of our model as an open source project.1
2 Related Work
There is a long history of work related to algorithms that mimic painters. One such work is Portrait
Drawing by Paul the Robot [ 23,25], where an underlying algorithm controlling a mechanical robot
arm sketches lines on a canvas with a programmable artistic style to mimic a given digitized portrait
of a person. Reinforcement Learning based-approaches [ 25] have been developed to discover a set of
paint brush strokes that can best represent a given input photograph. These prior works generally
attempt to mimic digitized photographs, rather than develop generative models of vector images.
Neural Network-based approaches have been developed for generative models of images, although
the majority of neural network-related research on image generation deal with pixel images [ 5,10,
12,14,19,24]. There has been relatively little work done on vector image generation using neural
networks. An earlier work [ 22] makes use of Hidden Markov Models to synthesize lines and curves
of a human sketch. More recent work [ 6] on handwriting generation with Recurrent Neural Networks
laid the groundwork for utilizing Mixture Density Networks [ 1] to generate continuous data points.
Recent works of this approach attempted to generate vectorized Kanji characters unconditionally [ 7]
and conditionally [26] by modelling Chinese characters as a sequence of pen stroke actions.
In addition to unconditionally generating sketches, we also explore encoding existing sketches
into a latent space of embedding vectors. Previous work [ 2] outlined a methodology to combine
Sequence-to-Sequence models with a Variational Autoencoder to model natural English sentences in
latent vector space. A related work [16], utilizes probabilistic program induction, rather than neural
networks, to perform one-shot modelling of the Omniglot dataset containing images of symbols.
One of the factors limiting research development in the space of generative vector drawings is the
lack of publicly available datasets. Previously, the Sketch dataset [ 4], consisting of 20K vector
sketches, was used to explore feature extraction techniques. A subsequent work, the Sketchy dataset
[20], provided 70K vector sketches along with corresponding pixel images for various classes. This
allowed for a larger-scale exploration of human sketches. ShadowDraw [ 17] is an interactive system
that predicts what a ﬁnished drawing looks like based on a set of incomplete brush strokes from the
user while the sketch is being drawn. ShadowDraw used a dataset of 30K raster images combined
with extracted vectorized features. In this work, we use a much larger dataset of vector sketches that
is made publicly available.
3 Methodology
3.1 Dataset
We constructed QuickDraw , a dataset of vector drawings obtained from Quick, Draw! [11], an online
game where the players are asked to draw objects belonging to a particular object class in less than 20
seconds. QuickDraw consists of hundreds of classes of common objects. Each class of QuickDraw
is a dataset of 70K training samples, in addition to 2.5K validation and 2.5K test samples.
We use a data format that represents a sketch as a set of pen stroke actions. This representation is an
extension of the format used in [ 6]. Our format extends the binary pen stroke event into a multi-state
event. In this data format, the initial absolute coordinate of the drawing is located at the origin. A
sketch is a list of points, and each point is a vector consisting of 5 elements: (∆x,∆y,p1,p2,p3).
The ﬁrst two elements are the offset distance in the x and y directions of the pen from the previous
point. The last 3 elements represents a binary one-hot vector of 3 possible states. The ﬁrst pen state,
p1, indicates that the pen is currently touching the paper, and that a line will be drawn connecting the
next point with the current point. The second pen state, p2, indicates that the pen will be lifted from
the paper after the current point, and that no line will be drawn next. The ﬁnal pen state, p3, indicates
that the drawing has ended, and subsequent points, including the current point, will not be rendered.
1The code and dataset is available at https://magenta.tensorflow.org/sketch_rnn .
23.2 Sketch-RNN
Figure 2: Schematic diagram of sketch-rnn .
Our model is a Sequence-to-Sequence Variational Autoencoder (V AE), similar to the architecture
described in [ 2,15]. Our encoder is a bidirectional RNN [ 21] that takes in a sketch as an input, and
outputs a latent vector of size Nz. Speciﬁcally, we feed the sketch sequence, S, and also the same
sketch sequence in reverse order, Sreverse , into two encoding RNNs that make up the bidirectional
RNN, to obtain two ﬁnal hidden states:
h→=encode→(S), h←=encode←(Sreverse ), h= [h→;h←] (1)
We take this ﬁnal concatenated hidden state, h, and project it into two vectors µandˆσ, each of size
Nz, using a fully connected layer. We convert ˆσinto a non-negative standard deviation parameter
σusing an exponential operation. We use µandσ, along withN(0,I), a vector of IID Gaussian
variables of size Nz, to construct a random vector, z∈RNz, as in the approach for a V AE [15]:
µ=Wµh+bµ,ˆσ=Wσh+bσ, σ= exp/parenleftBigˆσ
2/parenrightBig
, z=µ+σ⊙N(0,I) (2)
Under this encoding scheme, the latent vector zis not a deterministic output for a given input sketch,
but a random vector conditioned on the input sketch.
Our decoder is an autoregressive RNN that samples output sketches conditional on a given latent
vectorz. The initial hidden states h0, and optional cell states c0(if applicable) of the decoder RNN is
the output of a single layer network: [h0;c0] = tanh(Wzz+bz)
At each step iof the decoder RNN, we feed the previous point, Si−1and the latent vector zin as
a concatenated input xi, whereS0is deﬁned as (0,0,1,0,0). The output at each time step are the
parameters for a probability distribution of the next data point Si. In Equation 3, we model (∆x,∆y)
as a Gaussian mixture model (GMM) with Mnormal distributions as in [ 1,6], and (q1,q2,q3)as
a categorical distribution to model the ground truth data (p1,p2,p3), where (q1+q2+q3= 1) as
done in [ 7] and [ 26]. Unlike [ 6], our generated sequence is conditioned from a latent code zsampled
from our encoder, which is trained end-to-end alongside the decoder.
p(∆x,∆y) =M/summationdisplay
j=1ΠjN(∆x,∆y|µx,j,µy,j,σx,j,σy,j,ρxy,j),whereM/summationdisplay
j=1Πj= 1 (3)
N(x,y|µx,µy,σx,σy,ρxy)is the probability distribution function for a bivariate normal distribution.
Each of theMbivariate normal distributions consist of ﬁve parameters: (µx,µy,σx,σy,ρxy), where
µxandµyare the means, σxandσyare the standard deviations, and ρxyis the correlation parameter of
each bivariate normal distribution. An additional vector Πof lengthM, also a categorical distribution,
are the mixture weights of the Gaussian mixture model. Hence the size of the output vector yis
5M+M+ 3, which includes the 3 logits needed to generate (q1,q2,q3).
The next hidden state of the RNN, generated with its forward operation, projects into the output
vectoryiusing a fully-connected layer:
xi= [Si−1;z],[hi;ci] =forward (xi,[hi−1;ci−1]), yi=Wyhi+by, yi∈R6M+3(4)
The vectoryiis broken down into the parameters of the probability distribution of the next data point:
[ (ˆΠ1µxµyˆσxˆσyˆρxy)1...(ˆΠ1µxµyˆσxˆσyˆρxy)M(ˆq1ˆq2ˆq3) ] =yi (5)
3As in [ 6], we apply expandtanh operations to ensure the standard deviation values are non-negative,
and that the correlation value is between -1 and 1:
σx= exp(ˆσx), σy= exp(ˆσy), ρxy= tanh(ˆρxy) (6)
The probabilities for the categorical distributions are calculated using the outputs as logit values:
qk=exp(ˆqk)/summationtext3
j=1exp(ˆqj),k∈{1,2,3},Πk=exp(ˆΠk)/summationtextM
j=1exp(ˆΠj),k∈{1, ..., M} (7)
A key challenge is to train our model to know when to stop drawing. Because the probabilities of
the three pen stroke events are highly unbalanced, the model becomes more difﬁcult to train. The
probability of a p1event is much higher than p2, and thep3event will only happen once per drawing.
The approach developed in [ 7] and later followed by [ 26] was to use different weightings for each
pen event when calculating the losses, such as a hand-tuned weighting of (1,10,100) . We ﬁnd this
approach to be inelegant and inadequate for our dataset of diverse image classes.
We develop a simpler, more robust approach that works well for a broad class of sketch drawing data.
In our approach, all sequences are generated to a length of NmaxwhereNmaxis the length of the
longest sketch in our training dataset. In principle Nmaxcan be considered a hyper parameter. As the
length ofSis usually shorter than Nmax, we setSito be (0,0,0,0,1)fori>Ns. We discuss the
training in detail in the next section.
After training, we can sample sketches from our model. During the sampling process, we generate
the parameters for both GMM and categorical distributions at each time step, and sample an outcome
S/prime
ifor that time step. Unlike the training process, we feed the sampled outcome S/prime
ias input for the
next time step. We continue to sample until p3= 1, or when we have reached i=Nmax. Like the
encoder, the sampled output is not deterministic, but a random sequence, conditioned on the input
latent vector z. We can control the level of randomness we would like our samples to have during the
sampling process by introducing a temperature parameter τ:
ˆqk→ˆqk
τ,ˆΠk→ˆΠk
τ, σ2
x→σ2
xτ, σ2
y→σ2
yτ (8)
We can scale the softmax parameters of the categorial distribution and also the σparameters of the
bivariate normal distribution by a temperature parameter τ, to control the level of randomness in
our samples. τis typically set between 0 and 1. In the limiting case as τ→0, our model becomes
deterministic and samples will consist of the most likely point in the probability density function.
Figure 3 illustrates of effect of sampling sketches with various temperature parameters.
3.3 Unconditional Generation
Figure 3: Unconditional generation of ﬁretrucks, yoga poses, gardens and owls with varying τ.
As a special case, we can also train our model to generate sketches unconditionally, where we only
train the decoder RNN module, without any input or latent vectors. By removing the encoder, the
decoder RNN as a standalone model is an autoregressive model without latent variables. In this use
case, the initial hidden states and cell states of the decoder RNN are initialized to zero. The inputs xi
of the decoder RNN at each time step is only Si−1orS/prime
i−1, as we do not need to concatenate a latent
vectorz. In Figure 3, we sample various sketch images generated unconditionally by varying the
temperature parameter from τ= 0.2at the top in blue, to τ= 0.9at the bottom in red.
43.4 Training
Our training procedure follows the approach of the Variational Autoencoder [ 15], where the loss
function is the sum of two terms: the Reconstruction Loss, LR, and the Kullback-Leibler Divergence
Loss,LKL. We train our model to optimize this two-part loss function. The Reconstruction loss
term, described in Equation 9, maximizes the log-likehood of the generated probability distribution
to explain the training data S. We can calculate this reconstruction loss, LR, using the generated
parameters of the pdf and the training data S.LRis composed of the sum of the log loss of the offset
terms (∆x,∆y),Ls, and the log loss of the pen state terms (p1,p2,p3),Lp:
Ls=−1
NmaxNs/summationdisplay
i=1log/parenleftBigM/summationdisplay
j=1Πj,iN(∆xi,∆yi|µx,j,i,µy,j,i,σx,j,i,σy,j,i,ρxy,j,i)/parenrightBig
Lp=−1
NmaxNmax/summationdisplay
i=13/summationdisplay
k=1pk,ilog(qk,i), LR=Ls+Lp(9)
Note that we discard the pdf parameters modelling the (∆x,∆y)points beyond Nswhen calculating
Ls, whileLpis calculated using all of the pdf parameters modelling the (p1,p2,p3)points until
Nmax. Both terms are normalized by the total sequence length Nmax. We found this methodology of
loss calculation to be more robust and allows the model to easily learn when it should stop drawing,
unlike the earlier mentioned method of assigning importance weightings to p1,p2, andp3.
The Kullback-Leibler (KL) divergence loss term measures the difference between the distribution of
our latent vector z, to that of an IID Gaussian vector with zero mean and unit variance. Optimizing
for this loss term allows us to minimize this difference. We use the result in [ 15], and calculate the
KL loss term, LKL, normalized by number of dimensions Nzof the latent vector:
LKL=−1
2Nz/parenleftBig
1 + ˆσ−µ2−exp(ˆσ)/parenrightBig
(10)
The loss function in Equation 11 is a weighted sum of both the LRandLKLloss terms:
Loss =LR+wKLLKL (11)
There is a tradeoff between optimizing for one term over the other. As wKL→0, our model
approaches a pure autoencoder, sacriﬁcing the ability to enforce a prior over our latent space while
obtaining better reconstruction loss metrics. Note that for unconditional generation, where our model
is the standalone decoder, there will be no LKLterm as we only optimize for LR.
Figure 4: Tradeoff between LRandLKL, for two models trained on single class datasets (left).
Validation Loss Graph for models trained on the Yoga dataset using various wKL. (right)
Figure 4 illustrates the tradeoff between different settings of wKLand the resulting LRandLKL
metrics on the test set, along with the LRmetric on a standalone decoder RNN for comparison.
As the unconditional model does not receive any prior information about the entire sketch it needs
to generate, the LRmetric for the standalone decoder model serves as an upper bound for various
conditional models using a latent vector.
4 Experiments
We conduct several experiments with sketch-rnn for both conditional and unconditional vector
image generation. We train sketch-rnn on various QuickDraw classes using various settings for
5wKLand record the breakdown of losses. To experiment with a diverse set of classes with varying
complexities, we select the cat, pig, face, ﬁretruck, garden, owl, mosquito and yoga class. We also
experiment on multi-class datasets by concatenating different classes together to form (cat, pig) and
(crab, face, pig, rabbit). The results for test set evaluation on various datasets are displayed in Table 1.
Thesketch-rnn model treats the RNN cell as an abstract component. In our experiments, we
use Long Short-Term Memory (LSTM) [ 9] as the encoder RNN. For the decoder RNN, we use
HyperLSTM, as this type of RNN cell excels at sequence generation tasks [ 8]. The ability for
HyperLSTM to spontaneously augment its own weights enables it to adapt to many different regimes
in a large diverse dataset. For model conﬁguration details, please see the Supplementary Material.
Dataset wKL= 1.00 wKL= 0.50 wKL= 0.25 Decoder Only
LR LKL LR LKL LR LKL LR
cat -0.98 0.29 -1.33 0.70 -1.46 1.01 -0.57
pig -1.14 0.22 -1.37 0.49 -1.52 0.80 -0.82
cat, pig -1.02 0.22 -1.24 0.49 -1.50 0.98 -0.75
crab, face, pig, rabbit -0.91 0.22 -1.04 0.40 -1.47 1.17 -0.67
face -1.13 0.27 -1.55 0.71 -1.90 1.44 -0.73
ﬁretruck -1.24 0.22 -1.26 0.24 -1.78 1.10 -0.90
garden -0.79 0.20 -0.81 0.25 -0.99 0.54 -0.62
owl -0.93 0.20 -1.03 0.34 -1.29 0.77 -0.66
mosquito -0.67 0.30 -1.02 0.66 -1.41 1.54 -0.34
yoga -0.80 0.24 -1.07 0.55 -1.51 1.33 -0.48
Table 1: Loss ﬁgures ( LRandLKL) for various wKLsettings.
The relative loss numbers are consistent with our expectations. We see that the reconstruction loss
termLRdecreases as we relax the wKLparameter controlling the weight for the KL loss term, and
meanwhile the KL loss term LRincreases as a result. The LRfor the conditional model is strictly
less than the unconditional, standalone decoder model. In Figure 4 (right), we plot validation-set loss
graphs for on the yoga class for models with various wKLsettings. As LRdecreases, the LKLterm
tends to increase due to the tradeoff between LRandLKL.
4.1 Conditional Reconstruction
We qualitatively assess the reconstructed sketch S/primegiven an input sketch S. In Figure 5 (left), we
sample several reconstructions at various levels of temperature τusing a model trained on the single
cat class, starting at 0.01 on the left and linearly increasing to 1.0 on the right. The reconstructed cat
sketches have similar properties as the input image, and occasionally add or remove details such as a
whisker, a mouth, a nose, or the orientation of the tail.
Figure 5: Conditional generation of cats (left) and pigs (right).
When presented with a non-standard image of a cat, such as a cat’s face with three eyes, the
reconstructed cat only has two eyes. If we input a sketch from another image class, such a toothbrush,
the model seemingly generate sketches with similar orientation and properties as the toothbrush
input image, but with some cat-like features such as cat ears, whiskers or feet. We perform a similar
experiment with a model trained on the pig class, as shown in Figure 5 (right).
64.2 Latent Space Interpolation
By interpolating between latent vectors, we can visualize how one image morphs into another image
by visualizing the reconstructions of the interpolations. As we enforce a Gaussian prior on the latent
space, we expect fewer gaps in the space between two encoded latent vectors. We expect a model
trained using a higher wKLsetting to produce images that are closer to the data manifold given a
spherically interpolated [24] latent vector z, compared to another model trained with a lower wKL.
Figure 6: Latent space interpolation between cat and pig using with various wKLsettings (left).
Sketch Drawing Analogies (right).
To demonstrate this, we train several models using various wKL, on a dataset consisting of both cat
and pigs, and we encode two distinct images from the test set - a cat face and a full pig. Figure 6
(left) shows the reconstructed images from the interpolated latent vectors between the two original
images. As expected, models trained with higher wKLproduce more coherent interpolated images.
4.3 Sketch Drawing Analogies
The interpolation example in Figure 6 (left) suggests that the latent vector zencode conceptual
features of a sketch. Can we use these features to augment other sketches without such features – for
example, adding a body to a cat’s head? Indeed, we ﬁnd that sketch drawing analogies are possible
for models trained with low LKLnumbers. Given the smoothness of the latent space, where any
interpolated vector between two latent vectors results in a coherent sketch, we can perform vector
arithmetic on the latent vectors encoded from different sketches and explore how the model organizes
the latent space to represent different concepts in the manifold of generated sketches.
For example, as shown in Figure 6 (right), we can subtract the latent vector of an encoded pig head
from the latent vector of a full pig, to arrive at a vector that represents a body. Adding this difference
to the latent vector of a cat head results in a full cat (i.e. cat head + body = full cat). We repeat the
experiment to remove the body of a full pig. These drawing analogies allow us to explore how the
model organizes its latent space to represent different concepts in the manifold of generated sketches.
4.4 Predicting Different Endings of Incomplete Sketches
Figure 7: sketch-rnn predicting possible endings of various incomplete sketches (the red lines).
We can use sketch-rnn to ﬁnish an incomplete sketch. By using the decoder RNN as a standalone
model, we can generate a sketch that is conditioned on the previous points. We use the decoder RNN
to ﬁrst encode an incomplete sketch into a hidden state h. Afterwards, we generate the remaining
points of the sketch using has the initial hidden state. We show results in Figure 7 using decoder-only
models trained on individual classes, and sample completions by setting τ= 0.8.
75 Applications and Future Work
We believe sketch-rnn will enable many creative applications. Even the decoder-only model trained
on various classes can assist the creative process of an artist by suggesting many possible ways of
ﬁnishing a sketch, helping artists expand their imagination. In the conditional model, exploring the
latent space between different objects can potentially enable artists to ﬁnd interesting intersections
and relationships between different drawings. Even in the simplest use, pattern designers can apply
sketch-rnn to generate a large number of similar, but unique designs for textile or wallpaper prints.
As we saw earlier in Section 4.1, a model trained to draw pigs can be made to draw pig-like trucks if
given an input sketch of a truck. We can extend this result to applications that might help creative
designers come up with abstract designs that can resonate more with their target audience. For
instance, in Figure 8 (right), we feed sketches of four different chairs into our cat-drawing model to
produce four “chair-like cats”. We can even interpolate between the four images to explore the latent
space of chair-like cats, and select from a large grid of generated designs.
Figure 8: Generating similar, but unique sketches based on a single human sketch in the box (left).
Latent space of generated cats conditioned on sketch drawings of chairs (right).
A model trained on higher quality sketches may ﬁnd its way into educational applications that can
help teach students how to draw. Even with the simple sketches in QuickDraw , the authors of this
work have become much more proﬁcient at drawing animals, insects, and various sea creatures after
conducting these experiments. A related application is to encode a crude, poorly sketched drawing
and generate more aesthetically looking reproductions by using a model trained with a high wKL
setting and sampling with a low temperature τto produce a more coherent version of the drawing. In
the future, we can also investigate augmenting the latent vector in the direction that maximizes the
aesthetics of the drawing by incorporating user-rating data into the training process.
Combining hybrid variations of sequence-generation models with unsupervised, cross-domain pixel
image generation models, such as Image-to-Image models [ 3,13,18], is another exciting direction
that we can explore. We can already combine this model with supervised, cross-domain models such
as Pix2Pix [ 10], to occasionally generate photo realistic cat images from generated sketches of cats.
The opposite direction of converting a photograph of a cat into an unrealistic, but similar looking
sketch of a cat composed of a minimal number of lines seems to be a more interesting problem.
6 Conclusion
In this work, we develop a methodology to model sketch drawings using recurrent neural networks.
sketch-rnn is able to generate possible ways to ﬁnish an existing, but unﬁnished sketch drawing.
Our model can also encode existing sketches into a latent vector, and generate similar looking sketches
conditioned on the latent space. We demonstrate what it means to interpolate between two different
sketches by interpolating between its latent space, and also show that we can manipulate attributes
of a sketch by augmenting the latent space. We demonstrate the importance of enforcing a prior
distribution on the latent vector for coherent vector image generation during interpolation. By making
available a large dataset of sketch drawings, we hope to encourage further research and development
in the area of generative vector image modelling.
8References
[1] C. M. Bishop. Mixture density networks. Technical Report , 1994.
[2]S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Józefowicz, and S. Bengio. Generating Sentences from
a Continuous Space. CoRR , abs/1511.06349, 2015.
[3]H. Dong, P. Neekhara, C. Wu, and Y . Guo. Unsupervised Image-to-Image Translation with Generative
Adversarial Networks. ArXiv e-prints , Jan. 2017.
[4]M. Eitz, J. Hays, and M. Alexa. How Do Humans Sketch Objects? ACM Trans. Graph. (Proc. SIGGRAPH) ,
31(4):44:1–44:10, 2012.
[5] I. Goodfellow. NIPS 2016 Tutorial: Generative Adversarial Networks. ArXiv e-prints , Dec. 2017.
[6] A. Graves. Generating sequences with recurrent neural networks. arXiv:1308.0850 , 2013.
[7] D. Ha. Recurrent Net Dreams Up Fake Chinese Characters in Vector Format with TensorFlow, 2015.
[8] D. Ha, A. M. Dai, and Q. V . Le. HyperNetworks. In ICLR , 2017.
[9] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation , 1997.
[10] P. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros. Image-to-Image Translation with Conditional Adversarial
Networks. ArXiv e-prints , Nov. 2016.
[11] J. Jongejan, H. Rowley, T. Kawashima, J. Kim, and N. Fox-Gieg. The Quick, Draw! - A.I. Experiment.
https://quickdraw.withgoogle.com/ , 2016.
[12] C. Kaae Sønderby, T. Raiko, L. Maaløe, S. Kaae Sønderby, and O. Winther. Ladder Variational Autoen-
coders. ArXiv e-prints , Feb. 2016.
[13] T. Kim, M. Cha, H. Kim, J. Lee, and J. Kim. Learning to Discover Cross-Domain Relations with Generative
Adversarial Networks. ArXiv e-prints , Mar. 2017.
[14] D. P. Kingma, T. Salimans, and M. Welling. Improving variational inference with inverse autoregressive
ﬂow. CoRR , abs/1606.04934, 2016.
[15] D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. ArXiv e-prints , Dec. 2013.
[16] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through probabilistic
program induction. Science , 350(6266):1332–1338, Dec. 2015.
[17] Y . J. Lee, C. L. Zitnick, and M. F. Cohen. Shadowdraw: Real-time user guidance for freehand drawing. In
ACM SIGGRAPH 2011 Papers , SIGGRAPH ’11, pages 27:1–27:10, New York, NY , USA, 2011. ACM.
[18] M.-Y . Liu, T. Breuel, and J. Kautz. Unsupervised Image-to-Image Translation Networks. ArXiv e-prints ,
Mar. 2017.
[19] S. Reed, A. van den Oord, N. Kalchbrenner, S. Gómez Colmenarejo, Z. Wang, D. Belov, and N. de Freitas.
Parallel Multiscale Autoregressive Density Estimation. ArXiv e-prints , Mar. 2017.
[20] P. Sangkloy, N. Burnell, C. Ham, and J. Hays. The Sketchy Database: Learning to Retrieve Badly Drawn
Bunnies. ACM Trans. Graph. , 35(4):119:1–119:12, July 2016.
[21] M. Schuster, K. K. Paliwal, and A. General. Bidirectional recurrent neural networks. IEEE Transactions
on Signal Processing , 1997.
[22] S. Simhon and G. Dudek. Sketch interpretation and reﬁnement using statistical models. In Proceedings of
the Fifteenth Eurographics Conference on Rendering Techniques , EGSR’04, pages 23–32, Aire-la-Ville,
Switzerland, Switzerland, 2004. Eurographics Association.
[23] P. Tresset and F. Fol Leymarie. Portrait drawing by paul the robot. Comput. Graph. , 37(5):348–363, Aug.
2013.
[24] T. White. Sampling Generative Networks. ArXiv e-prints , Sept. 2016.
[25] N. Xie, H. Hachiya, and M. Sugiyama. Artist agent: A reinforcement learning approach to automatic
stroke generation in oriental ink painting. In ICML . icml.cc / Omnipress, 2012.
[26] X. Zhang, F. Yin, Y . Zhang, C. Liu, and Y . Bengio. Drawing and Recognizing Chinese Characters with
Recurrent Neural Network. CoRR , abs/1606.06539, 2016.
9Supplementary Material for “A Neural Representation of Sketch Drawings”
1 Dataset Details
Figure 1: Example sketch drawings from QuickDraw dataset.
The data from Quick, Draw! [4] expands daily, and every so often new classes are added to the game.
As such, the QuickDraw dataset now consists of hundreds of classes, from 75 classes initially, in
Table 1. Each class consists of 70K training samples and 2.5K validation and test samples. Stroke
simpliﬁcation using the Ramer–Douglas–Peucker algorithm [ 3] with a parameter of /epsilon1= 2.0has been
applied to simplify the lines. The data was originally recorded in pixel-dimensions, so we normalized
the offsets (∆x,∆y)using a single scaling factor. This scaling factor was calculated to adjust the
offsets in the training set to have a standard deviation of 1. For simplicity, we do not normalize the
offsets (∆x,∆y)to have zero mean, since the means are already relatively small.
alarm clock ambulance angel ant barn basket bee
bicycle book bridge bulldozer bus butterﬂy cactus
castle cat chair couch crab cruise ship dolphin
duck elephant eye face fan ﬁre hydrant ﬁretruck
ﬂamingo ﬂower garden hand hedgehog helicopter kangaroo
key lighthouse lion map mermaid octopus owl
paintbrush palm tree parrot passport peas penguin pig
pineapple postcard power outlet rabbit radio rain rhinoceros
roller coaster sandwich scorpion sea turtle sheep skull snail
snowﬂake speedboat spider strawberry swan swing set tennis racquet
the mona lisa toothbrush truck whale windmill
Table 1: Initial 75 QuickDraw classes used for this work.
Figure 2 below shows a training example, before normalization of (∆x,∆y)columns.
Figure 2: A sample sketch, as a sequence of (∆x,∆y,p1,p2,p3)points and in rendered form.
In the rendered sketch, the line color corresponds to the sequential stroke ordering.
12 Training Details
As a recap from the main text, we deﬁned the Reconstruction loss term LR)as:
Ls=−1
NmaxNs/summationdisplay
i=1log/parenleftBigM/summationdisplay
j=1Πj,iN(∆xi,∆yi|µx,j,i,µy,j,i,σx,j,i,σy,j,i,ρxy,j,i)/parenrightBig
Lp=−1
NmaxNmax/summationdisplay
i=13/summationdisplay
k=1pk,ilog(qk,i)
LR=Ls+Lp(1)
We also deﬁned the KL loss term LKLas:
LKL=−1
2Nz/parenleftBig
1 + ˆσ−µ2−exp(ˆσ)/parenrightBig
(2)
The loss function in Equation 3 is a weighted sum of both the LRandLKLloss terms:
Loss =LR+wKLLKL (3)
While the loss function in Equation 3 can be used during training, we ﬁnd that annealing the KL term
in the loss function (Equation 4) produced better results. This modiﬁcation is only used for model
training, and the original loss function in Equation 3 is still used to evaluate validation and test sets,
and for early stopping.
ηstep= 1−(1−ηmin)Rstep
Loss train =LR+wKLηstepmax(LKL,KL min)(4)
We ﬁnd that annealing the KL loss term generally results in better losses. Annealing the LKL
term in the loss function directs the optimizer to ﬁrst focus more on the reconstruction term in
Equation 1, which is the more difﬁcult loss term of the model to optimize for, before having to deal
with optimizing for the KL loss term in Equation 2, a far simpler expression in comparison. This
approach has been used in [ 2,5,7]. Our annealing term ηstepstarts atηmin(typically 0 or 0.01) at
training step 0, and converges to 1 for large training steps. Ris a term close to, but less than 1.
If the distribution of zis close enough to N(0,I), we can sample sketches from the decoder using
randomly sampled zfromN(0,I)as the input. In practice, we ﬁnd that going from a larger LKL
value (LKL>1.0) to a smaller LKLvalue of∼0.3generally results in a substantial increase in the
quality of sampled images using randomly sampled z∼N(0,I). However, going from LKL= 0.3
toLKLvalues closer to zero does not lead to any further noticeable improvements. Hence we ﬁnd it
useful to put a ﬂoor on LKLin the loss function by enforcing max(LKL,KL min)in Equation 4.
TheKL minterm inside the max operator is typically set to a small value such as 0.10 to 0.50. This
term will encourage the optimizer to put less focus on optimizing for the KL loss term LKLonce it is
low enough, so we can obtain better metrics for the reconstruction loss term LR. This approach is
similar to the approach described in [ 7] asfree bits , where they apply the max operator separately
inside each dimension of the latent vector z.
3 Model Conﬁguration
Our encoder and decoder RNNs consist of 512 and 2048 nodes respectively. In our model, we use
M= 20 mixture components for the decoder RNN. The latent vector zhasNz= 128 dimensions.
We apply Layer Normalization [ 1] to our model, and during training apply recurrent dropout [ 9] with
a keep probability of 90%. We train the model with batch sizes of 100 samples, using Adam [ 6] with
a learning rate of 0.0001 and gradient clipping of 1.0. All models are trained with KL min= 0.20,
R= 0.99999 . During training, we perform simple data augmentation by multiplying the offset
columns (∆x,∆y)by two IID random factors chosen uniformly between 0.90 and 1.10. Unless
mentioned otherwise, all experiments are conducted with wKL= 1.00.
24 Model Limitations
Although sketch-rnn can model a large variety of sketch drawings, there are several limitations in
the current approach we wish to highlight. For most single-class datasets, sketch-rnn is capable
of modelling sketches up to around 300 data points. The model becomes increasingly difﬁcult to
train beyond this length. For our dataset, we applied the Ramer–Douglas–Peucker algorithm [ 3] to
simplify the strokes of the sketch data to less than 200 data points while still keeping most of the
important visual information of each sketch.
Figure 3: Unconditional generated sketches of frogs, cats, and crabs at τ= 0.8.
For more complicated classes of images, such as mermaids or lobsters, the reconstruction loss metrics
are not as good compared to simpler classes such as ants, faces or ﬁretrucks. The models trained on
these more challenging image classes tend to draw smoother, more circular line segments that do not
resemble individual sketches, but rather resemble an averaging of many sketches in the training set.
We can see some of this artifact in the frog class, in Figure 3. This smoothness may be analogous
to the blurriness effect produced by a Variational Autoencoder [ 8] that is trained on pixel images.
Depending on the use case of the model, smooth circular lines can be viewed as aesthetically pleasing
and a desirable property.
Figure 4: Unconditional generations from model trained on 75 classes (left),
From model trained on crab, face, pig and rabbit classes (right).
While both conditional and unconditional models are capable of training on datasets consisting of
several classes, such as (cat, pig), and (crab, face, pig, rabbit), sketch-rnn is ineffective at modelling
a large number of classes simultaneously. In Figure 4, we sample sketches using an unconditional
model trained on 75 classes, and a model trained on 4 classes. The samples generated from the
75-class model are incoherent, with individual sketches displaying features from multiple classes.
The four-class unconditional model usually generates samples of a single class, but occasionally also
combines features from multiple classes. In the future, we will explore incorporating class information
outside of the latent space to handle the modelling of a large number of classes simultaneously.
35 Multi-Sketch Drawing Interpolation
Figure 5: Example of conditional generated sketches with single class models.
Latent space interpolation from left to right, and then top to bottom.
In addition to interpolating between two sketches, like in Figure 5, we can also visualize the
interpolation between four sketches in latent space to gain further insight from the model. In this
section we show more examples conditionally generated with sketch-rnn . We take four generated
images, place them on four corners of a grid, and populate the rest of the grid using the interpolation
of the latent vectors at the corners. Figure 6 shows two examples of this four-way interpolation, using
models trained on both (cat, pig) classes, and face class. All samples generated with τ= 0.1.
Figure 6: Example input sketches and sketch-rnn generated reproductions (Top),
Latent space interpolation between the four reproduced sketches (Bottom).
The left side of Figure 7 visualizes the interpolation between a full pig, a rabbit’s head, a crab, and a
face, using a model trained on these four classes. In certain parts of the space between a crab and a
face is a rabbit’s head, and we see that the ears of the rabbit becomes the crab’s claws. Applying the
model on the yoga class, it is interesting to see how one yoga position slowly transitions to another
via a set of interpolated yoga positions generated by the model. For visual effect, we also interpolate
between four distinct colors, and color each sketch using a unique interpolated color.
4Figure 7: Interpolation of (pig, rabbit, crab and face), yoga poses, mosquitoes and mermaids.
We also interpolate between four distinct colors for visual effect.
We also construct latent space interpolation examples for the mosquito class and the mermaid class,
in the last two grids Figure 7. We see that the model can interpolate between concepts such as style
of wings, leg counts, and orientation. In Figure 8 below, we show more interpolation examples of
other classes from the dataset.
Figure 8: Latent space interpolation between four generated gardens, owls, cats, and ﬁretrucks.
6 Which Loss Controls Image Coherency?
We would like to question the relative importance of the reconstruction loss term LR, relative to the
KL loss term LKL, when our goal is to produce higher quality image reconstructions. While our
reconstruction loss term LRoptimizes for the log-likelihood of the set of strokes that make up a
sketch, this metric alone does not give us any guarantee that a model with a lower LRnumber will
produce higher quality reconstructions compared to a model with a higher LRnumber.
For example, imagine a simple sketch of an face, /smiley, where most of the data points of Sare be used to
represent the head, and only a minority of points represent facial features such as the eyes and mouth.
It is possible to reconstruct the face with incoherent facial features, and yet still score a lower LR
number compared to another reconstruction with a coherent and similar face, if the edges around the
incoherent face are generated more precisely.
In Figure 9, we compare the reconstructed images generated using models trained with various wKL
settings. In the ﬁrst three examples from the left, we train our model on a dataset consisting of four
image classes (crab, face, pig, rabbit). We deliberately sketch input drawings that contain features of
two classes, such as a rabbit with a pig mouth and pig tail, a person with animal ears, and a rabbit with
crab claws. We see that the model trained using higher wKLweights, tend to generate sketches with
features of a single class that look more coherent, despite having lower LKLnumbers. For instance,
the model with wKL= 1.00omit pig features, animal ears, and crab claws from its reconstructions.
In contrast, the model with wKL= 0.25, with higher LKL, but lowerLRnumbers tries to keep both
inconsistent features, while generating sketches that look less coherent.
In the last three examples in Figure 9, we repeat the experiment on models trained on single-class
images, and see similar results even when we deliberately choose input samples from the test set with
noisier lines.
If we look at the interpolations produced in the Latent Space Interpolation section in the main text,
models with better KL loss terms also generate more meaningful reconstructions from the interpolated
5Figure 9: Reconstructions of sketch images using models with various wKLsettings.
space between two latent vectors. This suggests the latent vector for models with lower LKLcontrol
more meaningful parts of the drawings, such as controlling whether the sketch is an animal head
only or a full animal with a body, or whether to draw a cat head or a pig head. Altering such latent
vectors can allow us to directly manipulate these animal features. Conversely, altering the latent
codes of models with higher LKLresults in scattered movement of individual line segments, rather
than alterations of meaningful conceptual features of the animal.
This result is consistent with incoherent reconstructions seen in Figure 9. With a lower LKL, the
model is likely to generate coherent images given any random z. Even with a non-standard, or noisy,
input image, the model will still encode a zthat produces coherent images. For models with lower
LKLnumbers, the encoded latent vectors contain conceptual features belonging to the input image,
while for models with higher LKLnumbers, the latent vectors merely encode information about
speciﬁc line segments. This observation suggests that when using sketch-rnn on a new dataset, we
should ﬁrst try different wKLsettings to evaluate the tradeoff between LRandLKL, and then choose
a setting for wKL(andKL min) that best suit our requirements.
7 Acknowledgements
We thank Ian Johnson, Jonas Jongejan, Martin Wattenberg, Mike Schuster, Thomas Deselaers,
Ben Poole, Kyle Kastner, Junyoung Chung and Kyle McDonald for their help with this project. This
work was done as part of the Google Brain Residency program ( g.co/brainresidency ).
References
[1] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. NIPS , 2016.
[2]S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Józefowicz, and S. Bengio. Generating Sentences from
a Continuous Space. CoRR , abs/1511.06349, 2015.
[3]D. H. Douglas and T. K. Peucker. Algorithms for the reduction of the number of points required to represent
a digitized line or its caricature. Cartographica: The International Journal for Geographic Information and
Geovisualization , 10(2):112–122, Oct. 1973.
[4]J. Jongejan, H. Rowley, T. Kawashima, J. Kim, and N. Fox-Gieg. The Quick, Draw! - A.I. Experiment.
https://quickdraw.withgoogle.com/ , 2016.
[5]C. Kaae Sønderby, T. Raiko, L. Maaløe, S. Kaae Sønderby, and O. Winther. Ladder Variational Autoencoders.
ArXiv e-prints , Feb. 2016.
[6] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR , 2015.
[7]D. P. Kingma, T. Salimans, and M. Welling. Improving variational inference with inverse autoregressive
ﬂow. CoRR , abs/1606.04934, 2016.
[8] D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. ArXiv e-prints , Dec. 2013.
[9]S. Semeniuta, A. Severyn, and E. Barth. Recurrent dropout without memory loss. arXiv:1603.05118 , 2016.
6Robust Speech Recognition via Large-Scale Weak Supervision
Alec Radford* 1Jong Wook Kim* 1Tao Xu1Greg Brockman1Christine McLeavey1Ilya Sutskever1
Abstract
We study the capabilities of speech processing
systems trained simply to predict large amounts of
transcripts of audio on the internet. When scaled
to 680,000 hours of multilingual and multitask
supervision, the resulting models generalize well
to standard benchmarks and are often competitive
with prior fully supervised results but in a zero-
shot transfer setting without the need for any fine-
tuning. When compared to humans, the models
approach their accuracy and robustness. We are
releasing models and inference code to serve as
a foundation for further work on robust speech
processing.
1. Introduction
Progress in speech recognition has been energized by the
development of unsupervised pre-training techniques exem-
plified by Wav2Vec 2.0 (Baevski et al., 2020). Since these
methods learn directly from raw audio without the need for
human labels, they can productively use large datasets of un-
labeled speech and have been quickly scaled up to 1,000,000
hours of training data (Zhang et al., 2021), far more than the
1,000 or so hours typical of an academic supervised dataset.
When fine-tuned on standard benchmarks, this approach
has improved the state of the art, especially in a low-data
setting.
These pre-trained audio encoders learn high-quality repre-
sentations of speech, but because they are purely unsuper-
vised they lack an equivalently performant decoder mapping
those representations to usable outputs, necessitating a fine-
tuning stage in order to actually perform a task such as
speech recognition1. This unfortunately limits their use-
fulness and impact as fine-tuning can still be a complex
process requiring a skilled practitioner. There is an addi-
tional risk with requiring fine-tuning. Machine learning
*Equal contribution1OpenAI, San Francisco, CA 94110, USA.
Correspondence to: Alec Radford <alec@openai.com >, Jong
Wook Kim <jongwook@openai.com >.
1Baevski et al. (2021) is an exciting exception - having devel-
oped a fully unsupervised speech recognition systemmethods are exceedingly adept at finding patterns within a
training dataset which boost performance on held-out data
from the same dataset. However, some of these patterns are
brittle and spurious and don’t generalize to other datasets
and distributions. In a particularly disturbing example, Rad-
ford et al. (2021) documented a 9.2% increase in object
classification accuracy when fine-tuning a computer vision
model on the ImageNet dataset (Russakovsky et al., 2015)
without observing any improvement in average accuracy
when classifying the same objects on seven other natural
image datasets. A model that achieves “superhuman” per-
formance when trained on a dataset can still make many
basic errors when evaluated on another, possibly precisely
because it is exploiting those dataset-specific quirks that
humans are oblivious to (Geirhos et al., 2020).
This suggests that while unsupervised pre-training has im-
proved the quality of audio encoders dramatically, the lack
of an equivalently high-quality pre-trained decoder, com-
bined with a recommended protocol of dataset-specific fine-
tuning, is a crucial weakness which limits their usefulness
and robustness. The goal of a speech recognition system
should be to work reliably “out of the box” in a broad range
of environments without requiring supervised fine-tuning of
a decoder for every deployment distribution.
As demonstrated by Narayanan et al. (2018), Likhomanenko
et al. (2020), and Chan et al. (2021) speech recognition sys-
tems that are pre-trained in a supervised fashion across many
datasets/domains exhibit higher robustness and generalize
much more effectively to held-out datasets than models
trained on a single source. These works achieve this by
combining as many existing high-quality speech recogni-
tion datasets as possible. However, there is still only a
moderate amount of this data easily available. SpeechStew
(Chan et al., 2021) mixes together 7 pre-existing datasets
totalling 5,140 hours of supervision. While not insignifi-
cant, this is still tiny compared to the previously mentioned
1,000,000 hours of unlabeled speech data utilized in Zhang
et al. (2021).
Recognizing the limiting size of existing high-quality super-
vised datasets, recent efforts have created larger datasets for
speech recognition. By relaxing the requirement of gold-
standard human-validated transcripts, Chen et al. (2021) and
Galvez et al. (2021) make use of sophisticated automatedRobust Speech Recognition via Large-Scale Weak Supervision 2
pipelines to scale weakly supervised speech recognition
to 10,000 and 30,000 hours of noisier training data. This
trade-off between quality and quantity is often the right
call. Although understudied so far for speech recognition,
recent work in computer vision has demonstrated that mov-
ing beyond gold-standard crowdsourced datasets such as
ImageNet (Russakovsky et al., 2015) to much larger but
weakly supervised datasets significantly improves the ro-
bustness and generalization of models (Mahajan et al., 2018;
Kolesnikov et al., 2020).
Yet these new datasets are only a few times larger than the
sum of existing high-quality datasets and still much smaller
than prior unsupervised work. In this work we close that
gap, scaling weakly supervised speech recognition the next
order of magnitude to 680,000 hours of labeled audio data.
We call our approach Whisper2. We demonstrate models
trained at this scale transfer well to existing datasets zero-
shot, removing the need for any dataset-specific fine-tuning
to achieve high-quality results.
In addition to scale, our work also focuses on broaden-
ing the scope of weakly supervised pre-training beyond
English-only speech recognition to be both multilingual and
multitask. Of those 680,000 hours of audio, 117,000 hours
cover 96 other languages. The dataset also includes 125,000
hours of X→entranslation data. We find that for sufficiently
large models there is no drawback and even benefits to joint
multilingual and multitask training.
Our work suggests that simple scaling of weakly supervised
pre-training has been underappreciated so far for speech
recognition. We achieve these results without the need for
the self-supervision or self-training techniques that have
been a mainstay of recent large-scale speech recognition
work. To serve as a foundation for further research on robust
speech recognition, we release inference code and models at
the following URL: https://github.com/openai/
whisper .
2. Approach
2.1. Data Processing
Following the trend of recent work leveraging web-scale
text from the internet for training machine learning systems,
we take a minimalist approach to data pre-processing. In
contrast to a lot of work on speech recognition, we train
Whisper models to predict the raw text of transcripts without
any significant standardization, relying on the expressive-
ness of sequence-to-sequence models to learn to map be-
tween utterances and their transcribed form. This simplifies
2If an acronym or basis for the name is desired, WSPSR stand-
ing for Web-scale Supervised Pretraining for Speech Recognition
can be used.the speech recognition pipeline since it removes the need
for a separate inverse text normalization step in order to
produce naturalistic transcriptions.
We construct the dataset from audio that is paired with tran-
scripts on the Internet. This results in a very diverse dataset
covering a broad distribution of audio from many different
environments, recording setups, speakers, and languages.
While diversity in audio quality can help train a model to be
robust, diversity in transcript quality is not similarly bene-
ficial. Initial inspection showed a large amount of subpar
transcripts in the raw dataset. To address this, we developed
several automated filtering methods to improve transcript
quality.
Many transcripts on the internet are not actually human-
generated but the output of existing ASR systems. Recent
research has shown that training on datasets of mixed human
and machine-generated data can significantly impair the per-
formance of translation systems (Ghorbani et al., 2021). In
order to avoid learning “transcript-ese”, we developed many
heuristics to detect and remove machine-generated tran-
scripts from the training dataset. Many existing ASR sys-
tems output only a limited subset of written language which
removes or normalizes away aspects that are difficult to pre-
dict from only audio signals such as complex punctuation
(exclamation points, commas, and question marks), format-
ting whitespace such as paragraphs, or stylistic aspects such
as capitalization. An all-uppercase or all-lowercase tran-
script is very unlikely to be human generated. While many
ASR systems include some level of inverse text normaliza-
tion, it is often simple or rule-based and still detectable from
other unhandled aspects such as never including commas.
We also use an audio language detector, which was created
by fine-tuning a prototype model trained on a prototype ver-
sion of the dataset on V oxLingua107 (Valk & Alum ¨ae, 2021)
to ensure that the spoken language matches the language of
the transcript according to CLD2. If the two do not match,
we don’t include the (audio, transcript) pair as a speech
recognition training example in the dataset. We make an
exception if the transcript language is English and add these
pairs to the dataset as X→enspeech translation training
examples instead. We use fuzzy de-duping of transcript
texts to reduce the amount of duplication and automatically
generated content in the training dataset.
We break audio files into 30-second segments paired with
the subset of the transcript that occurs within that time
segment. We train on all audio, including segments where
there is no speech (though with sub-sampled probability)
and use these segments as training data for voice activity
detection.
For an additional filtering pass, after training an initial model
we aggregated information about its error rate on trainingRobust Speech Recognition via Large-Scale Weak Supervision 3
data sources and performed manual inspection of these data
sources sorting by a combination of both high error rate and
data source size in order to identify and remove low-quality
ones efficiently. This inspection showed a large amount of
only partially transcribed or poorly aligned/misaligned tran-
scripts as well as remaining low-quality machine-generated
captions that filtering heuristics did not detect.
To avoid contamination, we perform de-duplication at a tran-
script level between the training dataset and the evaluation
datasets we thought were at higher risk of overlap, namely
TED-LIUM 3 (Hernandez et al., 2018).
2.2. Model
Since the focus of our work is on studying the capabilities
of large-scale supervised pre-training for speech recogni-
tion, we use an off-the-shelf architecture to avoid confound-
ing our findings with model improvements. We chose an
encoder-decoder Transformer (Vaswani et al., 2017) as this
architecture has been well validated to scale reliably. All
audio is re-sampled to 16,000 Hz, and an 80-channel log-
magnitude Mel spectrogram representation is computed on
25-millisecond windows with a stride of 10 milliseconds.
For feature normalization, we globally scale the input to
be between -1 and 1 with approximately zero mean across
the pre-training dataset. The encoder processes this input
representation with a small stem consisting of two convolu-
tion layers with a filter width of 3 and the GELU activation
function (Hendrycks & Gimpel, 2016) where the second
convolution layer has a stride of two. Sinusoidal position
embeddings are then added to the output of the stem after
which the encoder Transformer blocks are applied. The
transformer uses pre-activation residual blocks (Child et al.,
2019), and a final layer normalization is applied to the en-
coder output. The decoder uses learned position embeddings
and tied input-output token representations (Press & Wolf,
2017). The encoder and decoder have the same width and
number of transformer blocks. Figure 1 summarizes the
model architecture.
We use the same byte-level BPE text tokenizer used in GPT-
2 (Sennrich et al., 2015; Radford et al., 2019) for the English-
only models and refit the vocabulary (but keep the same size)
for the multilingual models to avoid excessive fragmenta-
tion on other languages since the GPT-2 BPE vocabulary is
English only.
2.3. Multitask Format
Although predicting which words were spoken in a given
audio snippet is a core part of the full speech recognition
problem and extensively studied in research, it is not the
only part. A fully featured speech recognition system can
involve many additional components such as voice activ-
ity detection, speaker diarization, and inverse text normal-ization. These components are often handled separately,
resulting in a relatively complex system around the core
speech recognition model. To reduce this complexity, we
would like to have a single model perform the entire speech
processing pipeline, not just the core recognition part. An
important consideration here is the interface for the model.
There are many different tasks that can be performed on
the same input audio signal: transcription, translation, voice
activity detection, alignment, and language identification
are some examples.
For this kind of one-to-many mapping to work with a single
model, some form of task specification is necessary. We use
a simple format to specify all tasks and conditioning infor-
mation as a sequence of input tokens to the decoder. Since
our decoder is an audio-conditional language model, we also
train it to condition on the history of text of the transcript in
the hope that it will learn to use longer-range text context
to resolve ambiguous audio. Specifically, with some proba-
bility we add the transcript text preceding the current audio
segment to the decoder’s context. We indicate the beginning
of prediction with a <|startoftranscript|> token.
First, we predict the language being spoken which is repre-
sented by a unique token for each language in our training
set (99 total). These language targets are sourced from the
aforementioned V oxLingua107 model. In the case where
there is no speech in an audio segment, the model is trained
to predict a <|nospeech|> token indicating this. The
next token specifies the task (either transcription or trans-
lation) with an <|transcribe|> or<|translate|>
token. After this, we specify whether to predict timestamps
or not by including a <|notimestamps|> token for that
case. At this point, the task and desired format is fully
specified, and the output begins. For timestamp predic-
tion, we predict time relative to the current audio segment,
quantizing all times to the nearest 20 milliseconds which
matches the native time resolution of Whisper models, and
add additional tokens to our vocabulary for each of these.
We interleave their prediction with the caption tokens: the
start time token is predicted before each caption’s text, and
the end time token is predicted after. When a final tran-
script segment is only partially included in the current 30-
second audio chunk, we predict only its start time token
for the segment when in timestamp mode, to indicate that
the subsequent decoding should be performed on an au-
dio window aligned with that time, otherwise we truncate
the audio to not include the segment. Lastly, we add a
<|endoftranscript|> token. We only mask out the
training loss over the previous context text, and train the
model to predict all other tokens. Please see Figure 1 for an
overview of our format and training setup.Robust Speech Recognition via Large-Scale Weak Supervision 4
⋯⋯
2 × Conv1D + GELU⋮cross attention
Log-Mel Spectrogram~
SOT ENTRANS-  
CRIBE 0.0 The quick
Tokens in Multitask T raining FormatTransformer  
Encoder Blocks  Transformer  
Decoder Blocks  EN 0.0 The quick brown
⋮ ⋮next-token  
prediction
Sinusoidal  
Positional  
Encoding
Learned  
Positional  
EncodingMultitask training data (680k hours)Sequence-to-sequence learning
Multitask training formatEnglish transcription
Any-to-English speech translation
Non-English transcription
No speech🗣   “Ask not what y our country can do for ⋯” 
📝  Ask not what y our country can do for ⋯
🗣   “El rápido z orro marrón salta sobre ⋯ ” 
📝  The quick brown fo x jumps o ver ⋯
🗣  “ 언덕 위에  올라  내려다보면  너무나  넓고  넓은  ⋯ ” 
📝  언덕 위에  올라  내려다보면  너무나  넓고  넓은  ⋯
🔊 (background music pla ying) 
📝  ∅
PREV
special
tokenstext 
tokenstimestamp
tokensSTART OF  
TRANSCRIPTLANGUAGE  
TAG
NO 
SPEECHEOTTRANSCRIBE
TRANSLA TEbegin  
time
NO 
TIMEST AMPS⋯end 
timetext tokensbegin  
timeend 
timetext tokens
text tokens
Voice activity  
detection  
(VAD)Custom vocabulary /
promptingTime-aligned transcription
Text-only transcription  
(allows dataset-specific fine-tuning)X → English  
Translation previous  
text tokensX → X  
Transcription Language  
identificationMLP
self attention
MLP
self attention
MLP
self attentionMLP
cross attention
self attention
MLP
cross attention
self attention
MLP
cross attention
self attentionTRANS-  
CRIBE
Figure 1. Overview of our approach. A sequence-to-sequence Transformer model is trained on many different speech processing tasks,
including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. All of these
tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing for a single model to replace many different
stages of a traditional speech processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or
classification targets, as further explained in Section 2.3.
2.4. Training Details
We train a suite of models of various sizes in order to study
the scaling properties of Whisper. Please see Table 1 for an
overview. We train with data parallelism across accelerators
using FP16 with dynamic loss scaling and activation check-
pointing (Griewank & Walther, 2000; Chen et al., 2016).
Models were trained with AdamW (Loshchilov & Hutter,
2017) and gradient norm clipping (Pascanu et al., 2013)
with a linear learning rate decay to zero after a warmup over
the first 2048 updates. A batch size of 256 segments was
used, and the models are trained for 220updates which is
between two and three passes over the dataset. Due to only
training for a few epochs, over-fitting is not a large concern,
and we do not use any data augmentation or regularization
and instead rely on the diversity contained within such alarge dataset to encourage generalization and robustness.
Please see Appendix F for full training hyperparameters.3
During early development and evaluation we observed that
Whisper models had a tendency to transcribe plausible but
almost always incorrect guesses for the names of speakers.
This happens because many transcripts in the pre-training
dataset include the name of the person who is speaking,
encouraging the model to try to predict them, but this infor-
mation is only rarely inferable from only the most recent 30
3After the original release of Whisper, we trained an additional
Large model (denoted V2) for 2.5X more epochs while adding
SpecAugment (Park et al., 2019), Stochastic Depth (Huang et al.,
2016), and BPE Dropout (Provilkov et al., 2019) for regularization.
Reported results have been updated to this improved model unless
otherwise specified.Robust Speech Recognition via Large-Scale Weak Supervision 5
Model Layers Width Heads Parameters
Tiny 4 384 6 39M
Base 6 512 8 74M
Small 12 768 12 244M
Medium 24 1024 16 769M
Large 32 1280 20 1550M
Table 1. Architecture details of the Whisper model family.
seconds of audio context. To avoid this, we fine-tune Whis-
per models briefly on the subset of transcripts that do not
include speaker annotations which removes this behavior.
3. Experiments
3.1. Zero-shot Evaluation
The goal of Whisper is to develop a single robust speech
processing system that works reliably without the need for
dataset specific fine-tuning to achieve high-quality results
on specific distributions. To study this capability, we re-
use a wide set of existing speech processing datasets to
check whether Whisper is able to generalize well across
domains, tasks, and languages. Instead of using the standard
evaluation protocol for these datasets, which include both
a train and test split, we evaluate Whisper in a zero-shot
setting without using any of the training data for each of
these datasets so that we are measuring broad generalization.
3.2. Evaluation Metrics
Speech recognition research typically evaluates and com-
pares systems based on the word error rate (WER) metric.
However, WER, which is based on string edit distance, pe-
nalizes all differences between the model’s output and the
reference transcript including innocuous differences in tran-
script style. As a result, systems that output transcripts that
would be judged as correct by humans can still have a large
WER due to minor formatting differences. While this poses
a problem for all transcribers, it is particularly acute for
zero-shot models like Whisper, which do not observe any
examples of specific datasets transcript formats.
This is not a novel observation; the development of evalua-
tion metrics that better correlate with human judgement is an
active area of research, and while there are some promising
methods, none have seen widespread adoption for speech
recognition yet. We opt to address this problem with ex-
tensive standardization of text before the WER calculation
to minimize penalization of non-semantic differences. Our
text normalizer was developed through iterative manual in-
spection to identify common patterns where naive WER
penalized Whisper models for an innocuous difference. Ap-
pendix C includes full details. For several datasets, we
observe WER drops of up to 50 percent usually due to aquirk such as a dataset’s reference transcripts seperating
contractions from words with whitespace. We caution this
development procedure comes at a risk of overfitting to the
transcription style of Whisper models which we investigate
in Section 4.4. We are releasing the code for our text nor-
malizer to allow for easy comparison and to help others
study the performance of speech recognition systems in
out-of-distribution settings.
3.3. English Speech Recognition
In 2015, Deep Speech 2 (Amodei et al., 2015) reported
a speech recognition system matched human-level perfor-
mance when transcribing the LibriSpeech test-clean split.
As part of their analysis they concluded: “Given this result,
we suspect that there is little room for a generic speech sys-
tem to further improve on clean read speech without further
domain adaptation. ” Yet seven years later the SOTA WER
on LibriSpeech test-clean has dropped another 73% from
their 5.3% to 1.4% (Zhang et al., 2021), far below their re-
ported human-level error rate of 5.8%. Despite this massive
and unanticipated further improvement in performance on
held-out but in-distribution data, speech recognition mod-
els trained on LibriSpeech remain far above human error
rates when used in other settings. What explains this gap
between reportedly superhuman performance in-distribution
and subhuman performance out-of-distribution?
We suspect a large part of this gap between human and
machine behavior is due to conflating different capabilities
being measured by human and machine performance on
a test set. This claim may seem confusing at first; if both
humans and machines are taking the same test, how can it be
that different skills are being tested? The difference arises
not in the testing but in how they trained for it. Humans are
often asked to perform a task given little to no supervision
on the specific data distribution being studied. Thus human
performance is a measure of out-of-distribution generaliza-
tion. But machine learning models are usually evaluated
after training on a large amount of supervision from the
evaluation distribution, meaning that machine performance
is instead a measure of in-distribution generalization. While
both humans and machines are being evaluated on the same
testdata, two quite different abilities are being measured
due to a difference in train data.
Whisper models, which are trained on a broad and diverse
distribution of audio and evaluated in a zero-shot setting,
could potentially match human behavior much better than
existing systems. To study whether this is the case (or
whether the difference between machine and human per-
formance is due to yet-to-be-understood factors) we can
compare Whisper models with both human performance
and standard fine-tuned machine learning models and check
which they more closely match.Robust Speech Recognition via Large-Scale Weak Supervision 6
0 1 2 3 4 5 6 7 8
WER on LibriSpeech dev-clean (%)01020304050Average WER on [Common Voice, CHiME-6, TED-LIUM] (%)Supervised LibriSpeech models
Zero-shot Whisper models
Zero-shot Human (Alec)
Ideal robustness (y = x)
Figure 2. Zero-shot Whisper models close the gap to human
robustness. Despite matching or outperforming a human on Lib-
riSpeech dev-clean, supervised LibriSpeech models make roughly
twice as many errors as a human on other datasets demonstrating
their brittleness and lack of robustness. The estimated robustness
frontier of zero-shot Whisper models, however, includes the 95%
confidence interval for this particular human.
To quantify this difference, we examine both overall ro-
bustness, that is average performance across many distribu-
tions/datasets, and effective robustness, introduced by Taori
et al. (2020), which measures the difference in expected
performance between a reference dataset, which is usually
in-distribution, and one or more out-of-distribution datasets.
A model with high effective robustness does better than
expected on out-of-distribution datasets as a function of its
performance on the reference dataset and approaches the
ideal of equal performance on all datasets. For our analy-
sis, we use LibriSpeech as the reference dataset due to its
central role in modern speech recognition research and the
availability of many released models trained on it, which
allows for characterizing robustness behaviors. We use a
suite of 12 other academic speech recognition datasets to
study out-of-distribution behaviors. Full details about these
datasets can be found in Appendix A.
Our main findings are summarized in Figure 2 and Table 2.
Although the best zero-shot Whisper model has a relatively
unremarkable LibriSpeech clean-test WER of 2.5, which
is roughly the performance of modern supervised baseline
or the mid-2019 state of the art, zero-shot Whisper models
have very different robustness properties than supervised
LibriSpeech models and out-perform all benchmarked Lib-
riSpeech models by large amounts on other datasets. Evenwav2vec 2.0 Whisper RER
Dataset Large (no LM) Large V2 (%)
LibriSpeech Clean 2.7 2.7 0.0
Artie 24.5 6.2 74.7
Common V oice 29.9 9.0 69.9
Fleurs En 14.6 4.4 69.9
Tedlium 10.5 4.0 61.9
CHiME6 65.8 25.5 61.2
V oxPopuli En 17.9 7.3 59.2
CORAAL 35.6 16.2 54.5
AMI IHM 37.0 16.9 54.3
Switchboard 28.3 13.8 51.2
CallHome 34.8 17.6 49.4
WSJ 7.7 3.9 49.4
AMI SDM1 67.6 36.4 46.2
LibriSpeech Other 6.2 5.2 16.1
Average 29.3 12.8 55.2
Table 2. Detailed comparison of effective robustness across var-
ious datasets. Although both models perform within 0.1% of
each other on LibriSpeech, a zero-shot Whisper model performs
much better on other datasets than expected for its LibriSpeech
performance and makes 55.2% less errors on average. Results
reported in word error rate (WER) for both models after applying
our text normalizer.
the smallest zero-shot Whisper model, which has only 39
million parameters and a 6.7 WER on LibriSpeech test-clean
is roughly competitive with the best supervised LibriSpeech
model when evaluated on other datasets. When compared
to a human in Figure 2, the best zero-shot Whisper models
roughly match their accuracy and robustness. For a detailed
breakdown of this large improvement in robustness, Table
2 compares the performance of the best zero-shot Whisper
model with a supervised LibriSpeech model that has the
closest performance to it on LibriSpeech test-clean. Despite
their very close performance on the reference distribution,
the zero-shot Whisper model achieves an average relative
error reduction of 55.2% when evaluated on other speech
recognition datasets.
This finding suggests emphasizing zero-shot and out-of-
distribution evaluations of models, particularly when at-
tempting to compare to human performance, to avoid over-
stating the capabilities of machine learning systems due to
misleading comparisons.
3.4. Multi-lingual Speech Recognition
In order to compare to prior work on multilingual speech
recognition, we report results on two low-data benchmarks:
Multilingual LibriSpeech (MLS) (Pratap et al., 2020b) and
V oxPopuli (Wang et al., 2021) in Table 3.
Whisper performs well on Multilingual LibriSpeech, out-
performing XLS-R (Babu et al., 2021), mSLAM (BapnaRobust Speech Recognition via Large-Scale Weak Supervision 7
0.1 1 10 100 1K 10K 100K 1M
Hours of transcribed audio2.5510204080160Word Error Rate (WER)
r2 = 0.83
SW
PTJAFIML
FRROGL
KO
UKNELO
AZ
MKLT
NLMSGU
ISMY
CATE
TRCS
NBARAF
HRUZ
DEVILV
ID
PLSVTAFAHY
THBN
KM
ENHUUR
BSKA
ZHSL
SKCY
RUBGFIL
ELHIKNMT
BE
HE
ITMRPA
DA
ESKKTG
ETSR
Figure 3. Correlation of pre-training supervision amount with
downstream speech recognition performance. The amount of
pre-training speech recognition data for a given language is very
predictive of zero-shot performance on that language in Fleurs.
Model MLS V oxPopuli
VP-10K + FT - 15.3
XLS-R (1B) 10.9 10.6
mSLAM-CTC (2B) 9.7 9.1
Maestro - 8.1
Zero-Shot Whisper 7.3 13.6
Table 3. Multilingual speech recognition performance. Zero-
shot Whisper improves performance on Multilingual LibriSpeech
(MLS) but is still significantly behind both Maestro, XLS-R, and
mSLAM on V oxPopuli.
et al., 2022), and Maestro (Chen et al., 2022b) in a zero-shot
setting. We caution that we do use a simple text standardizer
for this result which prevents direct comparison or claims
of SOTA performance. On V oxPopuli, however, Whisper
significantly underperforms prior work and only beats the
VP-10K+FT baseline from the original paper. We suspect
the underperformance of Whisper models on V oxPopuli
could be due to other models including this distribution as
a major source for their unsupervised pre-training data and
the dataset having significantly more supervised data, which
benefits fine-tuning. While MLS has 10 hours of training
data per language, the average amount of training data per
language is roughly 10 ×higher for V oxPopuli.
These two benchmarks are somewhat narrow since they
only include 15 unique languages, almost all of which are in
1 10 100 1K 10K 100K
Hours of translated audio0510152025303540BLEU
r2 = 0.24
HR
AMNL
MYSWEL
NETH
KNPADA
AR
MIBG
ML
MRTESV
IT
FILGLRO
UK
FA
UZBE
KMTG
ASETOCCA
IS
KKHEFR AF
VI
HAMT
LOBNPT
HUFI
KO
SDID
UR
LNLV
AZ
YOLB
CYHYPL
LTDE
KARU
MKMSSR
ES
ZH
JANBBS
MNSNTR
PSSK
SOCS
SLHI
GU
TAFigure 4. Correlation of pre-training supervision amount with
downstream translation performance. The amount of pre-
training translation data for a given language is only moderately
predictive of Whisper’s zero-shot performance on that language in
Fleurs.
the Indo-European language family and many of which are
high-resource languages. These benchmarks only provide
limited coverage and room to study Whisper models multi-
lingual capabilities which include training data for speech
recognition in 75 languages. To study the performance of
Whisper more broadly we also report performance on the
Fleurs dataset (Conneau et al., 2022). In particular, we were
interested in studying the relationship between the amount
of training data we have for a given language and the result-
ing downstream zero-shot performance for that language.
We visualize this relation in Figure 3. We find a strong
squared correlation coefficient of 0.83 between the log of
the word error rate and the log of the amount of training
data per language. Checking the regression coefficient for a
linear fit to these log-log values results in an estimate that
WER halves for every 16 ×increase in training data. We
also observed that many of the largest outliers in terms of
worse than expected performance according to this trend are
languages that have unique scripts and are more distantly
related to the Indo-European languages making up the ma-
jority of the training dataset such as Hebrew ( HE), Telugu
(TE), Chinese ( ZH), and Korean ( KO). These differences
could be due to a lack of transfer due to linguistic distance,
our byte level BPE tokenizer being a poor match for these
languages, or variations in data quality.Robust Speech Recognition via Large-Scale Weak Supervision 8
X→English High Mid Low All
XMEF-X 34.2 20.2 5.9 14.7
XLS-R (2B) 36.1 27.7 15.1 22.1
mSLAM-CTC (2B) 37.8 29.6 18.5 24.8
Maestro 38.2 31.3 18.4 25.2
Zero-Shot Whisper 36.2 32.6 25.2 29.1
Table 4. X→enSpeech translation performance. Zero-shot
Whisper outperforms existing models on CoV oST2 in the overall,
medium, and low resource settings but still moderately under-
performs on high-resource languages compared to prior directly
supervised work.
Language ID Fleurs
w2v-bert-51 (0.6B) 71.4
mSLAM-CTC (2B) 77.7
Zero-shot Whisper 64.5
Table 5. Language identification performance. Zero-shot Whis-
per’s accuracy at language identification is not competitive with
prior supervised results on Fleurs. This is partially due to Whisper
being heavily penalized for having no training data for 20 of Fleurs
languages.
3.5. Translation
We study the translation capabilities of Whisper models
by measuring their performance on the X→ensubset of
CoV oST2 (Wang et al., 2020b). We compare with Maestro,
mSLAM, and XLS-R, the highest-performing prior work.
We achieve a new state of the art of 29.1 BLEU zero-shot
without using any of the CoV oST2 training data. We at-
tribute this to the 68,000 hours of X→entranslation data
for these languages in our pre-training dataset which, al-
though noisy, is vastly larger than the 861 hours of training
data for X→entranslation in CoV oST2. Since Whisper eval-
uation is zero-shot, it does particularly well on the lowest
resource grouping of CoV oST2, improving over mSLAM
by 6.7 BLEU. Conversely, the best Whisper model does not
actually improve over Maestro and mSLAM on average for
the highest resource languages.
For an additional analysis on an even wider set of languages,
we also re-purpose Fleurs, which is a speech recognition
dataset, as a translation dataset. Since the same sentences
are transcribed for every language we use the English tran-
scripts as reference translations. In Figure 4 we visualize
the correlation between the amount of translation training
data per language and the resulting zero-shot BLEU score
on Fleurs. While there is a clear trend of improvement with
increasing training data, the squared correlation coefficient
is much lower than the 0.83 observed for speech recognition
40 30 20 10 0 -10
signal-to-noise ratio (dB)125102050100WER on LibriSpeech test-clean (%)
white noise
40 30 20 10 0 -10
signal-to-noise ratio (dB)
pub noise
unispeech-sat-base-100h-libri-ft
wav2vec2-base-100h
wav2vec2-base-960h
wav2vec2-large-960h
wav2vec2-large-robust-ft-libri-960h
wav2vec2-large-960h-lv60-self
asr-crdnn-rnnlm-librispeech
asr-transformer-transformerlm-librispeechhubert-large-ls960-ft
hubert-xlarge-ls960-ft
s2t-medium-librispeech-asr
s2t-large-librispeech-asr
stt_en_conformer_ctc_large
stt_en_conformer_transducer_xlarge
WhisperFigure 5. WER on LibriSpeech test-clean as a function of SNR
under additive white noise (left) and pub noise (right). The
accuracy of LibriSpeech-trained models degrade faster than the
best Whisper model ( ⋆). NVIDIA STT models ( •) perform best
under low noise but are outperformed by Whisper under high noise
(SNR<10 dB). The second-best model under low noise ( ▼) is
fine-tuned on LibriSpeech only and degrades even more quickly.
and only 0.24. We suspect this is partly caused by the noisier
training data due to errors in audio language identification.
As an example, Welsh ( CY) is an outlier with much worse
than expected performance at only 13 BLEU despite sup-
posedly having 9,000 hours of translation data. This large
amount of Welsh translation data is surprising, ranking 4th
overall for translation data and ahead of some of the most
spoken languages in the world like French, Spanish, and
Russian. Inspection shows the majority of supposedly Welsh
translation data is actually English audio with English cap-
tions where the English audio was mis-classified as Welsh
by the language identification system, resulting in it being
included as translation training data rather transcription data
according to our dataset creation rules.
3.6. Language Identification
To evaluate language identification, we use the Fleurs
dataset (Conneau et al., 2022). The zero-shot performance
of Whisper is not competitive with prior supervised work
here and underperforms the supervised SOTA by 13.6%.
However, Whisper is heavily disadvantaged for language
identification on Fleurs, since the Whisper dataset contains
no training data for 20 of the 102 languages in Fleurs, upper-
bounding accuracy at 80.4%. On the 82 overlapping lan-
guages the best Whisper model achieves 80.3% accuracy.Robust Speech Recognition via Large-Scale Weak Supervision 9
3.7. Robustness to Additive Noise
We tested the noise robustness of Whisper models and 14
LibriSpeech-trained models by measuring the WER when
either white noise or pub noise from the Audio Degrada-
tion Toolbox (Mauch & Ewert, 2013) was added to the
audio. The pub noise represents a more natural noisy envi-
ronment with ambient noise and indistinct chatter typical
in a crowded restaurant or a pub. Among the 14 models,
twelve are pre-trained and/or fine-tuned on LibriSpeech, and
the other two are NVIDIA STT models trained on a mixture
dataset similar to prior work like SpeechStew that includes
LibriSpeech. The level of additive noise corresponding to
a given signal-to-noise ratio (SNR) is calculated based on
the signal power of individual examples. Figure 5 shows
how the ASR performance degrades as the additive noise
becomes more intensive. There are many models that out-
perform our zero-shot performance under low noise (40 dB
SNR), which is unsurprising given those models are trained
primarily on LibriSpeech, but all models quickly degrade as
the noise becomes more intensive, performing worse than
the Whisper model under additive pub noise of SNR below
10 dB. This showcases Whisper’s robustness to noise, es-
pecially under more natural distribution shifts like the pub
noise.
3.8. Long-form Transcription
Whisper models are trained on 30-second audio chunks and
cannot consume longer audio inputs at once. This is not aproblem with most academic datasets comprised of short
utterances but presents challenges in real-world applications
which often require transcribing minutes- or hours-long au-
dio. We developed a strategy to perform buffered transcrip-
tion of long audio by consecutively transcribing 30-second
segments of audio and shifting the window according to the
timestamps predicted by the model. We observed that it
is crucial to have beam search and temperature scheduling
based on the repetitiveness and the log probability of the
model predictions in order to reliably transcribe long audio.
The full procedure is described in Section 4.5.
We evaluate the long-form transcription performance on
seven datasets consisting of speech recordings of various
lengths and recording conditions, to cover as diverse a data
distribution as possible. These include a long-form adapta-
tion of TED-LIUM3 (Hernandez et al., 2018) concatenated
so that each example is a full-length TED talk, a collection
of jargon-laden segments taken from The Late Show with
Stephen Colbert (Meanwhile), sets of videos/podcasts that
has been used as ASR benchmarks in online blogs (Rev16
and Kincaid46), recordings of earnings calls (Del Rio et al.,
2021), and the full-length interviews from the Corpus of
Regional African American Language (CORAAL) (Gunter
et al., 2021). Full details about the long-form datasets can
be found in Appendix A.
We compare the performance with open-source models as
well as 4 commercial ASR services. The results are sum-
marized in Figure 6, showing the distribution of word error
rates from Whisper and the 4 commercial ASR services,
TED-LIUM3 Meanwhile Kincaid46 Rev16 Earnings-21 Earnings-22 CORAAL0510152025303540Word Error Rate (%)
Whisper Company A Company B Company C Company D NVIDIA STT (CTC large)
Figure 6. Whisper is competitive with state-of-the-art commercial and open-source ASR systems in long-form transcription. The
distribution of word error rates from six ASR systems on seven long-form datasets are compared, where the input lengths range from a
few minutes to a few hours. The boxes show the quartiles of per-example WERs, and the per-dataset aggregate WERs are annotated
on each box. Our model outperforms the best open source model (NVIDIA STT) on all datasets, and in most cases, commercial ASR
systems as well.Robust Speech Recognition via Large-Scale Weak Supervision 10
as well as the NVIDIA STT Conformer-CTC Large model
from the NeMo toolkit (Kuchaiev et al., 2019) which per-
formed the best among the open-source models. All com-
mercial ASR services are queried using their default English
transcription settings as of September 1st, 2022, and for
the NVIDIA STT model we used their buffered inference
implementation in the FrameBatchASR class to enable
long-form transcription. The results show that Whisper per-
forms better than the compared models on most datasets,
especially on the Meanwhile dataset which is heavy with
uncommon words. Additionally, we note the possibility that
some of the commercial ASR systems have been trained
on some of these publicly available datasets, and therefore
these results may not be accurately reflecting the relative
robustness of the systems.
3.9. Comparison with Human Performance
Because of ambiguous or indistinct speech as well as la-
beling errors, there are different levels of irreducible error
in each dataset, and with WER metrics from ASR systems
alone it is difficult to make sense of how much room for
improvement exists in each dataset. To quantify how close
Whisper’s performance is to the human performance, we se-
lected 25 recordings from the Kincaid46 dataset and used 5
services to obtain transcripts produced by professional tran-
scribers, among which one provides computer-assisted tran-
scription and the other four are entirely human-transcribed.
The audio selection covers various recording conditions
such as scripted and unscripted broadcast, telephone and
V oIP calls, and meetings. Figure 7 shows the distribution
of per-example WERs and aggregate WER across the 25
recordings, where the computer-assisted service has the
lowest aggregate WER that is 1.15% point better than Whis-
per’s, and the pure-human performance is only a fraction
of a percentage point better than Whisper’s. These results
indicate that Whisper’s English ASR performance is not
perfect but very close to human-level accuracy.
4. Analysis and Ablations
4.1. Model Scaling
A large amount of the promise in weakly supervised train-
ing approaches is their potential to use datasets much larger
than those in traditional supervised learning. However, this
comes with the cost of using data that is possibly much
noisier and lower quality than gold-standard supervision.
A concern with this approach is that although it may look
promising to begin with, the performance of models trained
on this kind of data may saturate at the inherent quality level
of the dataset, which could be far below human level. A re-
lated concern is that as capacity and compute spent training
on the dataset increases, models may learn to exploit the
Whisper A B C D E F G H I
 ASR            human transcription  
          computer-assisted051015202530Word Error Rate (%)
Figure 7. Whisper’s performance is close to that of professional
human transcribers. This plot shows the WER distributions of
25 recordings from the Kincaid46 dataset transcribed by Whisper,
the same 4 commercial ASR systems from Figure 6 (A-D), one
computer-assisted human transcription service (E) and 4 human
transcription services (F-I). The box plot is superimposed with dots
indicating the WERs on individual recordings, and the aggregate
WER over the 25 recordings are annotated on each box.
idiosyncrasies of the dataset, and their ability to generalize
robustly to out-of-distribution data could even degrade.
To check whether this is the case, we study the zero-shot
generalization of Whisper models as a function of the model
size. Our analysis is summarized in Figure 8. With the
exception of English speech recognition, performance con-
tinues to increase with model size across multilingual speech
recognition, speech translation, and language identification.
The diminishing returns for English speech recognition
could be due to saturation effects from approaching human-
level performance as analysis in Section 3.9 suggests.
4.2. Dataset Scaling
At 680,000 hours of labeled audio, the Whisper dataset is
one of the largest ever created in supervised speech recog-
nition. Exactly how important is the raw dataset size to
Whisper’s performance? To study this, we trained a series
of medium-sized models on subsampled versions of the
dataset which are 0.5%, 1%, 2%, 4%, and 8% of the full
dataset size and compared their performance with the same
medium-sized model trained on the whole dataset. Early
stopping based on the validation loss was used to select
model checkpoints for each dataset size. Evaluation was
performed on an exponential moving average estimate of
the parameters (Polyak & Juditsky, 1992) using a smooth-
ing rate of 0.9999 to help reduce the effect of the learning
rate not fully decaying to zero for the models trained on the
subsampled datasets due to early stopping. Performance
on English and multilingual speech recognition and X→en
translation is reported in Table 6.Robust Speech Recognition via Large-Scale Weak Supervision 11
38M 73M 244M 768M 1549M1549M
Model parameters0.02.55.07.510.012.515.017.520.0WER on 12 datasets (%)
English Speech Recognition
Average
Large V2
38M 73M 244M 768M 1549M1549M
Model parameters020406080100WER on 67 languages (%)
Multilingual Speech Recognition (Fleurs)
Average
Large V2
38M 73M 244M 768M 1549M1549M
Model parameters01020304050BLEU on 21 languages
X->En Translation (CoVoST2)
Average
Large V2
38M 73M 244M 768M 1549M1549M
Model parameters304050607080Accuracy on 102 languages (%)
Language Identification (Fleurs)
Average
Large V2
Figure 8. Zero-shot Whisper performance scales reliably across tasks and languages with increasing model size. Lightly shaded
lines represent individual datasets or languages, showing that performance is more varied than the smooth trends in aggregate performance.
Large V2 distinguished with a dashed orange line since it includes several changes that are not present for the smaller models in this
analysis.
Dataset English Multilingual X →En
size WER (↓) WER ( ↓) BLEU ( ↑)
3405 30.5 92.4 0.2
6811 19.6 72.7 1.7
13621 14.4 56.6 7.9
27243 12.3 45.0 13.9
54486 10.9 36.4 19.2
681070 9.9 29.2 24.8
Table 6. Performance improves with increasing dataset size.
English speech recognition performance refers to an average over
12 datasets while the Multilingual speech recognition reports per-
formance on the overlapping subset of languages in Fleurs and
X→entranslation reports average BLEU on CoV oST2. Dataset
size reported in hours.
All increases in the dataset size result in improved perfor-
mance on all tasks, although we see significant variability
in improvement rates across tasks and sizes. Performance
improves rapidly on English speech recognition from 3,000
to 13,000 hours and then slows down noticeably between
13,000 and 54,000 hours. Using the full dataset, which cor-
responds to another 12.5 ×increase in size results in only a
further 1 point drop in WER. This mirrors the diminishing
returns observed with model size scaling for English speech
recognition and could similarly be explained by saturation
effects when approaching human-level performance.
Improvements in WER follow a power-law trend for mul-
tilingual speech recognition till 54,000 hours and then de-
viate from this trend, improving only a further 7 points
when increasing to the full dataset size. For X→entransla-
tion, performance is practically zero when training on 7,000
hours of audio or less, and then follows a roughly log-linear
improvement trend till 54,000 hours before also showingdiminishing returns when further scaling to the full dataset
size.
The general trend across tasks of diminishing returns when
moving from 54,000 hours to our full dataset size of 680,000
hours could suggest that the current best Whisper models are
under-trained relative to dataset size and performance could
be further improved by a combination of longer training
and larger models. It could also suggest that we are nearing
the end of performance improvements from dataset size
scaling for speech recognition. Further analysis is needed to
characterize “scaling laws” for speech recognition in order
to decided between these explanations.
4.3. Multitask and Multilingual Transfer
A potential concern with jointly training a single model
on many tasks and languages is the possibility of negative
transfer where interference between the learning of several
tasks results in performance worse than would be achieved
by training on only a single task or language. To investigate
whether this is occurring, we compared the performance
of models trained on just English speech recognition with
our standard multitask and multilingual training setup and
measured their average performance across our suite of zero-
shot English speech recognition benchmarks. We adjust for
the amount of FLOPs spent training on the task of English
speech recognition as only 65% of compute is spent on this
task in a joint training setup; analysis would otherwise be
confounded by under-training on the task when compared
to a same-sized English-only model.
Our results visualized in Figure 9 show that for small models
trained with moderate amounts of compute, there is indeed
negative transfer between tasks and languages: joint mod-
els underperform English-only models trained for the same
amount of compute. However, multitask and multilingualRobust Speech Recognition via Large-Scale Weak Supervision 12
10e+19 10e+20 10e+21 10e+22
FLOPs training on english speech recognition8101214161820Average WER on 11 english speech recognition datasetsEnglish Only
Multilingual and Multitask
Figure 9. Multitask and multilingual transfer improves with
scale. For small models, performance on English speech recogni-
tion degrades when trained jointly in a multitask and multilingual
setup. However, multilingual and multitask models benefit more
from scale and eventually outperform models trained on English
data only. 95% bootstrap estimate confidence intervals are shown.
models scale better and for our largest experiments outper-
form their English-only counterparts demonstrating positive
transfer from other tasks. For our largest experiments, joint
models also slightly outperform English-only models even
when not adjusting for compute spent per task.
4.4. Text Normalization
Since we developed our text normalization jointly with
Whisper to discount innocuous word errors, there is a risk
that our normalizer is overfitted to fixing Whisper’s peculiar-
ities rather than addressing general variation in transcription.
To check this, we compared the performance of Whisper
using our normalizer versus an independently developed
one from the FairSpeech project (Koenecke et al., 2020). In
Figure 10, we visualize the differences. On most datasets
the two normalizers perform similarly, without significant
differences in WER reduction between Whisper and com-
pared open-source models, while on some datasets, namely
WSJ, CallHome, and Switchboard, our normalizer reduces
the WER of Whisper models’ significantly more. The differ-
ences in reduction can be traced down to different formats
used by the ground truth and how the two normalizers are pe-
nalizing them. For example, in CallHome and Switchboard,
our standardizer did not penalize differences in common
English contractions such as “you’re” versus “you are”, and
in WSJ, our normalizer standardized the written and spo-
0 10 20 30 40 50
Relative WER reduction compared to FairSpeech's normalizer (%)CORAAL
CommonVoice9.en
AMI-SDM1
CommonVoice5.1
Fleurs.en_us
AMI-IHM
Artie
LibriSpeech
TED-LIUM3
VoxPopuli.en
WSJ
CallHome
SwitchboardOpen-source models
Whisper modelsFigure 10. On most datasets, our text normalizer has similar
effect on reducing WERs between Whisper models and other
open-source models, compared to FairSpeech’s normalizer. For
each dataset, the boxplot shows the distribution of relative WER
reduction across different models in our eval suite, showing that
using our text normalizer generally results in lower WERs than
FairSpeech’s. On a few datasets our normalizer reduces WER
significantly and more so for Whisper models, such as CallHome
and Switchboard which have many contractions in the ground truth
and WSJ which contains many numerical expressions.
ken forms of numerical and monetary expressions, such as
“sixty-eight million dollars” versus “$68 million”.
4.5. Strategies for Reliable Long-form Transcription
Transcribing long-form audio using Whisper relies on ac-
curate prediction of the timestamp tokens to determine the
amount to shift the model’s 30-second audio context win-
dow by, and inaccurate transcription in one window may
negatively impact transcription in the subsequent windows.
We have developed a set of heuristics that help avoid fail-
ure cases of long-form transcription, which is applied in
the results reported in sections 3.8 and 3.9. First, we use
beam search with 5 beams using the log probability as the
score function, to reduce repetition looping which happens
more frequently in greedy decoding. We start with tem-
perature 0, i.e. always selecting the tokens with the high-
est probability, and increase the temperature by 0.2 up to
1.0 when either the average log probability over the gen-
erated tokens is lower than −1or the generated text has a
gzip compression rate higher than 2.4. Providing the tran-
scribed text from the preceding window as previous-text
conditioning when the applied temperature is below 0.5
further improves the performance. We found that the proba-
bility of the <|nospeech|> token alone is not sufficientRobust Speech Recognition via Large-Scale Weak Supervision 13TED-LIUM3
Meanwhile
Kincaid46
Rev16
Earnings-21
Earnings-22
CORAAL
Average
Greedy decoding only 3.95 5.16 9.69 11.7 10.7 14.0 22.0 11.0
+ Beam search 4.16 5.71 9.42 11.5 10.2 13.4 20.0 10.6
+ Temperature fallback 4.16 5.71 9.42 11.5 10.2 13.4 20.0 10.6
+ V oice activity detection 3.56 4.61 9.45 11.4 10.1 13.2 19.4 10.2
+ Previous text conditioning 3.42 6.16 8.72 11.0 9.63 13.3 18.1 10.0
+ Initial timestamp constraint 3.51 5.26 8.41 11.5 9.73 12.6 19.1 10.0
Table 7. Long-form transcription performance improves incremen-
tally as additional decoding heuristics are employed. Details on
each intervention are described in Section 4.5.
to distinguish a segment with no speech, but combining
the no-speech probability threshold of 0.6 and the average
log-probability threshold of −1makes the voice activity
detection of Whisper more reliable. Finally, to avoid a fail-
ure mode where the model ignores the first few words in
the input, we constrained the initial timestamp token to be
between 0.0 and 1.0 second. Table 7 shows that adding each
of the interventions above incrementally reduces the WER
overall, but not evenly across the dataset. These heuristics
serve as a workaround for the noisy predictions of the model,
and more research would be needed to further improve the
reliability of long-form decoding.
5. Related Work
Scaling Speech Recognition A consistent theme across
speech recognition research has been documenting the bene-
fits of scaling compute, models, and datasets. Early work ap-
plying deep learning to speech recognition found improved
performance with model depth and size and leveraged GPU
acceleration to make training these larger models tractable
(Mohamed et al., 2009). Further research demonstrated that
the benefit of deep learning approaches to speech recogni-
tion increased with dataset size, improving from being only
competitive with prior GMM-HMM systems when using
just 3 hours of TIMIT training data for phone recognition
to achieving a 30% word error rate reduction when trained
on the 2,000 hour Switchboard dataset (Seide et al., 2011).
Liao et al. (2013) is an early example of leveraging weakly
supervised learning to increase the size of a deep learn-
ing based speech recognition dataset by over 1,000 hours.
These trends continued with Deep Speech 2 (Amodei et al.,
2015) being a notable system developing high-throughput
distributed training across 16 GPUs and scaling to 12,000
hours of training data while demonstrating continuing im-
provements at that scale. By leveraging semi-supervised
pre-training, Narayanan et al. (2018) were able to grow
dataset size much further and study training on 162,000
hours of labeled audio. More recent work has exploredbillion-parameter models (Zhang et al., 2020) and using up
to 1,000,000 hours of training data (Zhang et al., 2021).
Multitask Learning Multitask learning (Caruana, 1997)
has been studied for a long time. In speech recognition,
multi-lingual models have been explored for well over a
decade (Schultz & Kirchhoff, 2006). An inspirational and
foundational work in NLP exploring multi-task learning
with a single model is Collobert et al. (2011). Multitask
learning in the sequence-to-sequence framework (Sutskever
et al., 2014) using multiple encoders and decoders was in-
vestigated in Luong et al. (2015). The use of language codes
with a shared encoder/decoder architecture was first demon-
strated for machine translation by Johnson et al. (2017),
removing the need for separate encoders and decoders. This
approach was simplified further into the “text-to-text” frame-
work of McCann et al. (2018) and popularized by its success
with large transformer language models in the work of Rad-
ford et al. (2019) and Raffel et al. (2020). Toshniwal et al.
(2018) demonstrated jointly training a modern deep learn-
ing speech recognition system on several languages with a
single model, and Pratap et al. (2020a) scaled this line of
work significantly to 50 languages with a billion-parameter
model. MUTE (Wang et al., 2020c) and mSLAM (Bapna
et al., 2022) studied joint training over both text and speech
language tasks, demonstrating transfer between them.
Robustness The question of how effectively models trans-
fer and how robust they are to distribution shift and other
types of perturbations has long been studied and is actively
being researched across many fields of machine learning.
Torralba & Efros (2011) highlighted the lack of generaliza-
tion of machine learning models between datasets over a
decade ago. Many other works have shown and continu-
ally reiterated how despite high performance on IID test
sets, machine learning models can still make many mistakes
when evaluated in even slightly different settings (Lake et al.,
2017; Jia & Liang, 2017; Alcorn et al., 2019; Barbu et al.,
2019; Recht et al., 2019). More recently, Taori et al. (2020)
studied the robustness of image classification models, and
Miller et al. (2020) investigated this for question-answering
models. A key finding has been that multi-domain train-
ing increases robustness and generalization as discussed in
the Introduction. This finding has been replicated across
many fields in addition to speech recognition including NLP
(Hendrycks et al., 2020) and computer vision (Radford et al.,
2021).
6. Limitations and Future Work
From our experimental results, analyses, and ablations, we
have noted several limitations and areas for future work.Robust Speech Recognition via Large-Scale Weak Supervision 14
Improved decoding strategies. As we have scaled Whis-
per, we have observed that larger models have made steady
and reliable progress on reducing perception-related errors
such as confusing similar-sounding words. Many remaining
errors, particularly in long-form transcription seem more
stubborn in nature and decidedly non-human/perceptual.
They are a combination of failure modes of seq2seq mod-
els, language models, and text-audio alignment and include
problems such as getting stuck in repeat loops, not tran-
scribing the first or last few words of an audio segment, or
complete hallucination where the model will output a tran-
script entirely unrelated to the actual audio. Although the
decoding details discussed in Section 4.5 help significantly,
we suspect fine-tuning Whisper models on a high-quality
supervised dataset and/or using reinforcement learning to
more directly optimize for decoding performance could help
further reduce these errors.
Increase Training Data For Lower-Resource Languages
As Figure 3 shows, Whisper’s speech recognition perfor-
mance is still quite poor on many languages. The same
analysis suggests a clear route for improvement since perfor-
mance on a language is very well predicted by the amount
of training data for the language. Since our pre-training
dataset is currently very English-heavy due to biases of
our data collection pipeline, which sourced primarily from
English-centric parts of the internet, most languages have
less than 1000 hours of training data. A targeted effort at in-
creasing the amount of data for these rarer languages could
result in a large improvement to average speech recognition
performance even with only a small increase in our overall
training dataset size.
Studying fine-tuning In this work, we have focused on
the robustness properties of speech processing systems and
as a result only studied the zero-shot transfer performance
of Whisper. While this is a crucial setting to study due to it
being representative of general reliability, for many domains
where high-quality supervised speech data does exist, it is
likely that results can be improved further by fine-tuning.
An additional benefit of studying fine-tuning is that it allows
for direct comparisons with prior work since it is a much
more common evaluation setting.
Studying the impact of Language Models on Robustness
As argued in the introduction, we suspect that Whisper’s
robustness is partially due to its strong decoder, which is an
audio conditional language model. It’s currently unclear to
what degree the benefits of Whisper stem from training its
encoder, decoder, or both. This could be studied by either
ablating various design components of Whisper, such as
training a decoder-less CTC model, or by studying how the
performance of existing speech recognition encoders suchas wav2vec 2.0 change when used together with a language
model.
Adding Auxiliary Training Objectives Whisper departs
noticeably from most recent state-of-the-art speech recog-
nition systems due to the lack of unsupervised pre-training
or self-teaching methods. While we have not found them
necessary to achieve good performance, it is possible that
the results could be further improved by incorporating this.
7. Conclusion
Whisper suggests that scaling weakly supervised pre-
training has been underappreciated so far in speech recogni-
tion research. We achieve our results without the need for
the self-supervision and self-training techniques that have
been a mainstay of recent large-scale speech recognition
work and demonstrate how simply training on a large and
diverse supervised dataset and focusing on zero-shot trans-
fer can significantly improve the robustness of a speech
recognition system.
ACKNOWLEDGMENTS
We’d like to thank the millions of people who were involved
in creating the data used by Whisper. We’d also like to
thank Nick Ryder, Will Zhuk, and Andrew Carr for the
conversation on the waterfall hike that inspired this project.
We are also grateful to the Acceleration and Supercomputing
teams at OpenAI for their critical work on software and
hardware infrastructure this project used. We’d also like to
thank Pamela Mishkin for advising the project from a policy
perspective. Finally, we are grateful to the developers of
the many software packages used throughout this project
including, but not limited, to Numpy (Harris et al., 2020),
SciPy (Virtanen et al., 2020), ftfy (Speer, 2019), PyTorch
(Paszke et al., 2019), pandas (pandas development team,
2020), and scikit-learn (Pedregosa et al., 2011).
References
Alcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.-
S., and Nguyen, A. Strike (with) a pose: Neural networks
are easily fooled by strange poses of familiar objects. In
Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 4845–4854, 2019.
Amodei, D., Anubhai, R., Battenberg, E., Case, C., Casper,
J., Catanzaro, B., Chen, J., Chrzanowski, M., Coates,
A., Diamos, G., et al. Deep speech 2: end-to-end speech
recognition in english and mandarin. arxiv. arXiv preprint
arXiv:1512.02595 , 2015.
Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler,
M., Meyer, J., Morais, R., Saunders, L., Tyers, F. M.,Robust Speech Recognition via Large-Scale Weak Supervision 15
and Weber, G. Common voice: A massively-multilingual
speech corpus. arXiv preprint arXiv:1912.06670 , 2019.
Babu, A., Wang, C., Tjandra, A., Lakhotia, K., Xu,
Q., Goyal, N., Singh, K., von Platen, P., Saraf, Y .,
Pino, J., et al. XLS-R: Self-supervised cross-lingual
speech representation learning at scale. arXiv preprint
arXiv:2111.09296 , 2021.
Baevski, A., Zhou, H., Mohamed, A., and Auli, M. wav2vec
2.0: A framework for self-supervised learning of speech
representations. arXiv preprint arXiv:2006.11477 , 2020.
Baevski, A., Hsu, W.-N., Conneau, A., and Auli, M. Unsu-
pervised speech recognition. Advances in Neural Infor-
mation Processing Systems , 34:27826–27839, 2021.
Bapna, A., Cherry, C., Zhang, Y ., Jia, Y ., Johnson, M.,
Cheng, Y ., Khanuja, S., Riesa, J., and Conneau, A. mslam:
Massively multilingual joint pre-training for speech and
text. arXiv preprint arXiv:2202.01374 , 2022.
Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gut-
freund, D., Tenenbaum, J., and Katz, B. Objectnet: A
large-scale bias-controlled dataset for pushing the lim-
its of object recognition models. Advances in neural
information processing systems , 32, 2019.
Caruana, R. Multitask learning. Machine learning , 28(1):
41–75, 1997.
Chan, W., Park, D., Lee, C., Zhang, Y ., Le, Q., and Norouzi,
M. SpeechStew: Simply mix all available speech recogni-
tion data to train one large neural network. arXiv preprint
arXiv:2104.02133 , 2021.
Chen, G., Chai, S., Wang, G., Du, J., Zhang, W.-Q.,
Weng, C., Su, D., Povey, D., Trmal, J., Zhang, J.,
et al. Gigaspeech: An evolving, multi-domain asr corpus
with 10,000 hours of transcribed audio. arXiv preprint
arXiv:2106.06909 , 2021.
Chen, S., Wu, Y ., Wang, C., Chen, Z., Chen, Z., Liu, S.,
Wu, J., Qian, Y ., Wei, F., Li, J., et al. Unispeech-sat: Uni-
versal speech representation learning with speaker aware
pre-training. In ICASSP 2022-2022 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP) , pp. 6152–6156. IEEE, 2022a.
Chen, T., Xu, B., Zhang, C., and Guestrin, C. Training
deep nets with sublinear memory cost. arXiv preprint
arXiv:1604.06174 , 2016.
Chen, Z., Zhang, Y ., Rosenberg, A., Ramabhadran, B.,
Moreno, P., Bapna, A., and Zen, H. Maestro: Matched
speech text representations through modality matching.
arXiv preprint arXiv:2204.03409 , 2022b.Child, R., Gray, S., Radford, A., and Sutskever, I. Gen-
erating long sequences with sparse transformers. arXiv
preprint arXiv:1904.10509 , 2019.
Collobert, R., Weston, J., Bottou, L., Karlen, M.,
Kavukcuoglu, K., and Kuksa, P. Natural language pro-
cessing (almost) from scratch. Journal of machine learn-
ing research , 12(ARTICLE):2493–2537, 2011.
Conneau, A., Ma, M., Khanuja, S., Zhang, Y ., Axelrod, V .,
Dalmia, S., Riesa, J., Rivera, C., and Bapna, A. Fleurs:
Few-shot learning evaluation of universal representations
of speech. arXiv preprint arXiv:2205.12446 , 2022.
Del Rio, M., Delworth, N., Westerman, R., Huang, M.,
Bhandari, N., Palakapilly, J., McNamara, Q., Dong, J.,
Zelasko, P., and Jett ´e, M. Earnings-21: a practical bench-
mark for asr in the wild. arXiv preprint arXiv:2104.11348 ,
2021.
Galvez, D., Diamos, G., Torres, J. M. C., Achorn, K., Gopi,
A., Kanter, D., Lam, M., Mazumder, M., and Reddi, V . J.
The people’s speech: A large-scale diverse english speech
recognition dataset for commercial usage. arXiv preprint
arXiv:2111.09344 , 2021.
Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Bren-
del, W., Bethge, M., and Wichmann, F. A. Shortcut learn-
ing in deep neural networks. Nature Machine Intelligence ,
2(11):665–673, 2020.
Ghorbani, B., Firat, O., Freitag, M., Bapna, A., Krikun,
M., Garcia, X., Chelba, C., and Cherry, C. Scaling
laws for neural machine translation. arXiv preprint
arXiv:2109.07740 , 2021.
Griewank, A. and Walther, A. Algorithm 799: revolve: an
implementation of checkpointing for the reverse or ad-
joint mode of computational differentiation. ACM Trans-
actions on Mathematical Software (TOMS) , 26(1):19–45,
2000.
Gunter, K., Vaughn, C., and Kendall, T. Contextualiz-
ing/s/retraction: Sibilant variation and change in wash-
ington dc african american language. Language Variation
and Change , 33(3):331–357, 2021.
Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers,
R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J.,
Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van
Kerkwijk, M. H., Brett, M., Haldane, A., Fern ´andez del
R´ıo, J., Wiebe, M., Peterson, P., G ´erard-Marchant, P.,
Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H.,
Gohlke, C., and Oliphant, T. E. Array programming
with NumPy. Nature , 585:357–362, 2020. doi: 10.1038/
s41586-020-2649-2.Robust Speech Recognition via Large-Scale Weak Supervision 16
Hendrycks, D. and Gimpel, K. Gaussian error linear units
(gelus). arXiv preprint arXiv:1606.08415 , 2016.
Hendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan,
R., and Song, D. Pretrained transformers improve out-of-
distribution robustness. arXiv preprint arXiv:2004.06100 ,
2020.
Hernandez, F., Nguyen, V ., Ghannay, S., Tomashenko, N. A.,
and Est `eve, Y . Ted-lium 3: twice as much data and corpus
repartition for experiments on speaker adaptation. In
SPECOM , 2018.
Hsu, W.-N., Bolte, B., Tsai, Y .-H. H., Lakhotia, K.,
Salakhutdinov, R., and Mohamed, A. Hubert: Self-
supervised speech representation learning by masked
prediction of hidden units. IEEE/ACM Transactions on
Audio, Speech, and Language Processing , 29:3451–3460,
2021a.
Hsu, W.-N., Sriram, A., Baevski, A., Likhomanenko, T.,
Xu, Q., Pratap, V ., Kahn, J., Lee, A., Collobert, R., Syn-
naeve, G., et al. Robust wav2vec 2.0: Analyzing do-
main shift in self-supervised pre-training. arXiv preprint
arXiv:2104.01027 , 2021b.
Huang, G., Sun, Y ., Liu, Z., Sedra, D., and Weinberger,
K. Q. Deep networks with stochastic depth. In European
conference on computer vision , pp. 646–661. Springer,
2016.
Jia, R. and Liang, P. Adversarial examples for evalu-
ating reading comprehension systems. arXiv preprint
arXiv:1707.07328 , 2017.
Johnson, M., Schuster, M., Le, Q. V ., Krikun, M., Wu, Y .,
Chen, Z., Thorat, N., Vi ´egas, F., Wattenberg, M., Corrado,
G., et al. Google’s multilingual neural machine translation
system: Enabling zero-shot translation. Transactions of
the Association for Computational Linguistics , 5:339–
351, 2017.
Kendall, T. and Farrington, C. The corpus of regional
african american language. Version 2021.07. Eugene, OR:
The Online Resources for African American Language
Project. http://oraal.uoregon.edu/coraal ,
2021. Accessed: 2022-09-01.
Koenecke, A., Nam, A., Lake, E., Nudell, J., Quartey, M.,
Mengesha, Z., Toups, C., Rickford, J. R., Jurafsky, D.,
and Goel, S. Racial disparities in automated speech recog-
nition. Proceedings of the National Academy of Sciences ,
117(14):7684–7689, 2020.
Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung,
J., Gelly, S., and Houlsby, N. Big transfer (bit): General
visual representation learning. In European conference
on computer vision , pp. 491–507. Springer, 2020.Kuchaiev, O., Li, J., Nguyen, H., Hrinchuk, O., Leary, R.,
Ginsburg, B., Kriman, S., Beliaev, S., Lavrukhin, V .,
Cook, J., et al. Nemo: a toolkit for building ai applications
using neural modules. arXiv preprint arXiv:1909.09577 ,
2019.
Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gersh-
man, S. J. Building machines that learn and think like
people. Behavioral and brain sciences , 40, 2017.
Liao, H., McDermott, E., and Senior, A. Large scale deep
neural network acoustic modeling with semi-supervised
training data for youtube video transcription. In 2013
IEEE Workshop on Automatic Speech Recognition and
Understanding , pp. 368–373. IEEE, 2013.
Likhomanenko, T., Xu, Q., Pratap, V ., Tomasello, P., Kahn,
J., Avidov, G., Collobert, R., and Synnaeve, G. Rethink-
ing evaluation in asr: Are our models robust enough?
arXiv preprint arXiv:2010.11745 , 2020.
Loshchilov, I. and Hutter, F. Decoupled weight decay regu-
larization. arXiv preprint arXiv:1711.05101 , 2017.
Luong, M.-T., Le, Q. V ., Sutskever, I., Vinyals, O., and
Kaiser, L. Multi-task sequence to sequence learning.
arXiv preprint arXiv:1511.06114 , 2015.
Mahajan, D., Girshick, R., Ramanathan, V ., He, K., Paluri,
M., Li, Y ., Bharambe, A., and Van Der Maaten, L. Ex-
ploring the limits of weakly supervised pretraining. In
Proceedings of the European conference on computer
vision (ECCV) , pp. 181–196, 2018.
Mauch, M. and Ewert, S. The audio degradation toolbox and
its application to robustness evaluation. In Proceedings of
the 14th International Society for Music Information Re-
trieval Conference (ISMIR 2013) , Curitiba, Brazil, 2013.
accepted.
McCann, B., Keskar, N. S., Xiong, C., and Socher, R. The
natural language decathlon: Multitask learning as ques-
tion answering. arXiv preprint arXiv:1806.08730 , 2018.
Meyer, J., Rauchenstein, L., Eisenberg, J. D., and Howell,
N. Artie bias corpus: An open dataset for detecting de-
mographic bias in speech applications. In Proceedings of
the 12th Language Resources and Evaluation Conference ,
pp. 6462–6468, Marseille, France, May 2020. European
Language Resources Association. ISBN 979-10-95546-
34-4. URL https://aclanthology.org/2020.
lrec-1.796 .
Miller, J., Krauth, K., Recht, B., and Schmidt, L. The effect
of natural distribution shift on question answering models.
InICML , 2020.Robust Speech Recognition via Large-Scale Weak Supervision 17
Mohamed, A.-r., Dahl, G., Hinton, G., et al. Deep belief net-
works for phone recognition. In Nips workshop on deep
learning for speech recognition and related applications ,
volume 1, pp. 39, 2009.
Narayanan, A., Misra, A., Sim, K. C., Pundak, G., Tripathi,
A., Elfeky, M., Haghani, P., Strohman, T., and Bacchi-
ani, M. Toward domain-invariant speech recognition via
large scale training. In 2018 IEEE Spoken Language
Technology Workshop (SLT) , pp. 441–447. IEEE, 2018.
Panayotov, V ., Chen, G., Povey, D., and Khudanpur, S.
Librispeech: an asr corpus based on public domain au-
dio books. In 2015 IEEE international conference on
acoustics, speech and signal processing (ICASSP) , pp.
5206–5210. IEEE, 2015.
pandas development team, T. pandas-dev/pandas: Pan-
das, February 2020. URL https://doi.org/10.
5281/zenodo.3509134 .
Park, D. S., Chan, W., Zhang, Y ., Chiu, C.-C., Zoph, B.,
Cubuk, E. D., and Le, Q. V . SpecAugment: A simple data
augmentation method for automatic speech recognition.
arXiv preprint arXiv:1904.08779 , 2019.
Pascanu, R., Mikolov, T., and Bengio, Y . On the difficulty
of training recurrent neural networks. In International
conference on machine learning , pp. 1310–1318. PMLR,
2013.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,
M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
Bai, J., and Chintala, S. Pytorch: An imperative style,
high-performance deep learning library. In Advances
in Neural Information Processing Systems 32 , pp. 8024–
8035, 2019.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V .,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V ., Vanderplas, J., Passos, A., Cour-
napeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
Scikit-learn: Machine learning in Python. Journal of
Machine Learning Research , 12:2825–2830, 2011.
Polyak, B. T. and Juditsky, A. B. Acceleration of stochastic
approximation by averaging. SIAM journal on control
and optimization , 30(4):838–855, 1992.
Pratap, V ., Sriram, A., Tomasello, P., Hannun, A. Y .,
Liptchinsky, V ., Synnaeve, G., and Collobert, R. Mas-
sively multilingual asr: 50 languages, 1 model, 1 billion
parameters. ArXiv , abs/2007.03001, 2020a.
Pratap, V ., Xu, Q., Sriram, A., Synnaeve, G., and Collobert,
R. Mls: A large-scale multilingual dataset for speech
research. arXiv preprint arXiv:2012.03411 , 2020b.Press, O. and Wolf, L. Using the output embedding to
improve language models. In Proceedings of the 15th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics: Volume 2, Short
Papers , pp. 157–163, Valencia, Spain, April 2017. As-
sociation for Computational Linguistics. URL https:
//aclanthology.org/E17-2025 .
Provilkov, I., Emelianenko, D., and V oita, E. Bpe-dropout:
Simple and effective subword regularization. arXiv
preprint arXiv:1910.13267 , 2019.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and
Sutskever, I. Language models are unsupervised multitask
learners. 2019.
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,
Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,
J., Krueger, G., and Sutskever, I. Learning transferable
visual models from natural language supervision. arXiv
preprint arXiv:2103.00020 , 2021.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y ., Li, W., Liu, P. J., et al. Exploring
the limits of transfer learning with a unified text-to-text
transformer. J. Mach. Learn. Res. , 21(140):1–67, 2020.
Ravanelli, M., Parcollet, T., Plantinga, P., Rouhe, A., Cor-
nell, S., Lugosch, L., Subakan, C., Dawalatabad, N.,
Heba, A., Zhong, J., Chou, J.-C., Yeh, S.-L., Fu, S.-W.,
Liao, C.-F., Rastorgueva, E., Grondin, F., Aris, W., Na,
H., Gao, Y ., Mori, R. D., and Bengio, Y . SpeechBrain: A
general-purpose speech toolkit, 2021. arXiv:2106.04624.
Recht, B., Roelofs, R., Schmidt, L., and Shankar, V .
Do ImageNet classifiers generalize to ImageNet? In
Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceed-
ings of the 36th International Conference on Machine
Learning , volume 97 of Proceedings of Machine Learn-
ing Research , pp. 5389–5400. PMLR, 09–15 Jun 2019.
URLhttps://proceedings.mlr.press/v97/
recht19a.html .
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,
Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,
M., et al. Imagenet large scale visual recognition chal-
lenge. International journal of computer vision , 115(3):
211–252, 2015.
Schultz, T. and Kirchhoff, K. Multilingual speech process-
ing. Elsevier, 2006.
Seide, F., Li, G., Chen, X., and Yu, D. Feature engineering
in context-dependent deep neural networks for conver-
sational speech transcription. In 2011 IEEE Workshop
on Automatic Speech Recognition & Understanding , pp.
24–29. IEEE, 2011.Robust Speech Recognition via Large-Scale Weak Supervision 18
Sennrich, R., Haddow, B., and Birch, A. Neural machine
translation of rare words with subword units. arXiv
preprint arXiv:1508.07909 , 2015.
Speer, R. ftfy. Zenodo, 2019. URL https://doi.org/
10.5281/zenodo.2591652 . Version 5.5.
Sutskever, I., Vinyals, O., and Le, Q. V . Sequence to se-
quence learning with neural networks. Advances in neural
information processing systems , 27, 2014.
Taori, R., Dave, A., Shankar, V ., Carlini, N., Recht, B.,
and Schmidt, L. Measuring robustness to natural
distribution shifts in image classification. In Larochelle,
H., Ranzato, M., Hadsell, R., Balcan, M., and Lin,
H. (eds.), Advances in Neural Information Processing
Systems , volume 33, pp. 18583–18599. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.
neurips.cc/paper/2020/file/
d8330f857a17c53d217014ee776bfd50-Paper.
pdf.
Torralba, A. and Efros, A. A. Unbiased look at dataset bias.
CVPR 2011 , pp. 1521–1528, 2011.
Toshniwal, S., Sainath, T. N., Weiss, R. J., Li, B., Moreno,
P. J., Weinstein, E., and Rao, K. Multilingual speech
recognition with a single end-to-end model. 2018 IEEE
International Conference on Acoustics, Speech and Sig-
nal Processing (ICASSP) , pp. 4904–4908, 2018.
Valk, J. and Alum ¨ae, T. V oxlingua107: a dataset for spoken
language recognition. In 2021 IEEE Spoken Language
Technology Workshop (SLT) , pp. 652–658. IEEE, 2021.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-
tion is all you need. In Advances in neural information
processing systems , pp. 5998–6008, 2017.
Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M.,
Reddy, T., Cournapeau, D., Burovski, E., Peterson, P.,
Weckesser, W., Bright, J., van der Walt, S. J., Brett, M.,
Wilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J.,
Jones, E., Kern, R., Larson, E., Carey, C. J., Polat, ˙I.,
Feng, Y ., Moore, E. W., VanderPlas, J., Laxalde, D.,
Perktold, J., Cimrman, R., Henriksen, I., Quintero, E. A.,
Harris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa,
F., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy
1.0: Fundamental Algorithms for Scientific Computing
in Python. Nature Methods , 17:261–272, 2020. doi:
10.1038/s41592-019-0686-2.
Wang, C., Tang, Y ., Ma, X., Wu, A., Okhonko, D., and Pino,
J. fairseq s2t: Fast speech-to-text modeling with fairseq.
arXiv preprint arXiv:2010.05171 , 2020a.Wang, C., Wu, A., and Pino, J. Covost 2 and massively
multilingual speech-to-text translation. arXiv preprint
arXiv:2007.10310 , 2020b.
Wang, C., Riviere, M., Lee, A., Wu, A., Talnikar, C., Haziza,
D., Williamson, M., Pino, J., and Dupoux, E. V oxpopuli:
A large-scale multilingual speech corpus for representa-
tion learning, semi-supervised learning and interpretation.
arXiv preprint arXiv:2101.00390 , 2021.
Wang, P., Sainath, T. N., and Weiss, R. J. Multitask training
with text data for end-to-end speech recognition. arXiv
preprint arXiv:2010.14318 , 2020c.
Watanabe, S., Mandel, M., Barker, J., Vincent, E., Arora,
A., Chang, X., Khudanpur, S., Manohar, V ., Povey, D.,
Raj, D., et al. Chime-6 challenge: Tackling multispeaker
speech recognition for unsegmented recordings. arXiv
preprint arXiv:2004.09249 , 2020.
Xu, Q., Baevski, A., Likhomanenko, T., Tomasello, P., Con-
neau, A., Collobert, R., Synnaeve, G., and Auli, M. Self-
training and pre-training are complementary for speech
recognition. In ICASSP 2021-2021 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP) , pp. 3030–3034. IEEE, 2021.
Zhang, Y ., Qin, J., Park, D. S., Han, W., Chiu, C.-C., Pang,
R., Le, Q. V ., and Wu, Y . Pushing the limits of semi-
supervised learning for automatic speech recognition.
arXiv preprint arXiv:2010.10504 , 2020.
Zhang, Y ., Park, D. S., Han, W., Qin, J., Gulati, A., Shor, J.,
Jansen, A., Xu, Y ., Huang, Y ., Wang, S., et al. BigSSL:
Exploring the frontier of large-scale semi-supervised
learning for automatic speech recognition. arXiv preprint
arXiv:2109.13226 , 2021.Robust Speech Recognition via Large-Scale Weak Supervision 19
A. Evaluation Datasets.
A.1. Short-form English-only datasets
•LibriSpeech (Panayotov et al., 2015): We used the test-clean and test-other splits from the LibriSpeech ASR corpus.
•TED-LIUM 3 (Hernandez et al., 2018): We used the test split of TED-LIUM Release 3, using the segmented manual
transcripts included in the release.
•Common Voice 5.1 (Ardila et al., 2019): We downloaded the English subset of Common V oice Corpus 5.1 from the
official website.
•Artie bias corpus (Meyer et al., 2020): We used the Artie bias corpus. This is a subset of the Common V oice dataset.
•CallHome andSwitchboard : We used the two corpora from LDC2002S09 and LDC2002T43.
•WSJ : We used LDC93S6B and LDC94S13B and followed the s5recipe to preprocess the dataset.
•CORAAL : We used the 231 interviews from CORAAL (Kendall & Farrington, 2021) and used the preprocessing
script from the FairSpeech project.
•CHiME-6 : For CHiME-6 (Watanabe et al., 2020), we downloaded the CHiME-5 dataset and followed the stage 0
of the s5track1 recipe to create the CHiME-6 dataset which fixes synchronization. We then used the binaural
recordings ( *P??.wav ) and the corresponding transcripts.
•AMI-IHM andAMI-SDM1 : We preprocessed the AMI Corpus by following the stage 0 ad 2 of the s5b recipe.
A.2. Long-form English-only datasets
•TED-LIUM 3 (Hernandez et al., 2018): We used the 11 full-length TED talks from the test split of TED-LIUM
Release 3, slicing the source audio files between the beginning of the first labeled segment and the end of the last
labeled segment of each talk, and we used the concatenated text as the label.
•Meanwhile : This dataset consists of 64 segments from The Late Show with Stephen Colbert. The YouTube video ID
and the corresponding start and end timestamps are available as part of the code release. The labels are collected from
the closed-caption data for each video and corrected with manual inspection.
•Rev16 : We use a subset of 16 files from the 30 podcast episodes in Rev.AI’s Podcast Transcription Benchmark, after
finding that there are multiple cases where a significant portion of the audio and the labels did not match, mostly on the
parts introducing the sponsors. We selected 16 episodes that do not have this error, whose “file number”s are:
3 4 9 10 11 14 17 18 20 21 23 24 26 27 29 32
•Kincaid46 : This dataset consists of 46 audio files and the corresponding transcripts compiled in the blog article ¡Which
automatic transcription service is the most accurate - 2018¿ by Jason Kincaid. We used the 46 audio files and reference
transcripts from the Airtable widget in the article. For the human transcription benchmark in the paper, we use a subset
of 25 examples from this data, whose “Ref ID”s are:
2 4 5 8 9 10 12 13 14 16 19 21 23 25 26 28 29 30 33 35 36 37 42 43 45
•Earnings-21 (Del Rio et al., 2021) and Earnings-22 : We used the files available in the speech-datasets repository, as
of their 202206 version.
•CORAAL : We used the 231 full-length interviews and transcripts from (Kendall & Farrington, 2021).Robust Speech Recognition via Large-Scale Weak Supervision 20
A.3. Multilingual datasets
•Multilingual LibriSpeech (Pratap et al., 2020b): We used the test splits from each language in the Multilingual
LibriSpeech (MLS) corpus.
•Fleurs (Conneau et al., 2022): We collected audio files and transcripts using the implementation available as Hug-
gingFace datasets. To use as a translation dataset, we matched the numerical utterance IDs to find the corresponding
transcript in English.
•VoxPopuli (Wang et al., 2021): We used the getasrdata.py script from the official repository to collect the ASR
data in 16 languages, including English.
•Common Voice 9 (Ardila et al., 2019): We downloaded the Common V oice Corpus 9 from the official website.
•CoVOST 2 (Wang et al., 2020b): We collected the X into English data collected using the official repository.
B. Compared Models
For comparison, we use the following models from HuggingFace, downloaded as of September 2022 using version 4.21.0 of
thetransformers library:
•facebook/wav2vec2-large-960h-lv60-self (Xu et al., 2021)
•facebook/wav2vec2-large-robust-ft-libri-960h (Hsu et al., 2021b)
•facebook/wav2vec2-base-100h (Baevski et al., 2020)
•facebook/wav2vec2-base-960h (Baevski et al., 2020)
•facebook/wav2vec2-large-960h (Baevski et al., 2020)
•facebook/hubert-large-ls960-ft (Hsu et al., 2021a)
•facebook/hubert-xlarge-ls960-ft (Hsu et al., 2021a)
•facebook/s2t-medium-librispeech-asr (Wang et al., 2020a)
•facebook/s2t-large-librispeech-asr (Wang et al., 2020a)
•microsoft/unispeech-sat-base-100h-libri-ft (Chen et al., 2022a)
•nvidia/stt enconformer ctclarge (Kuchaiev et al., 2019)
•nvidia/stt enconformer transducer xlarge (Kuchaiev et al., 2019)
•speechbrain/asr-crdnn-rnnlm-librispeech (Ravanelli et al., 2021)
•speechbrain/asr-transformer-transformerlm-librispeech (Ravanelli et al., 2021)
We note that all of the models above are entirely or partly trained on LibriSpeech.Robust Speech Recognition via Large-Scale Weak Supervision 21
C. Text Standardization
Since Whisper may output any UTF-8 string rather than a restricted set of graphemes, the rules for text standardization need
to be more intricate and comprehensive than those defined on e.g. ASCII characters. We perform the following steps to
normalize English texts in different styles into a standardized form, which is a best-effort attempt to penalize only when a
word error is caused by actually mistranscribing a word, and not by formatting or punctuation differences.
1. Remove any phrases between matching brackets ( [,]).
2. Remove any phrases between matching parentheses ( (,)).
3. Remove any of the following words: hmm,mm,mhm,mmm,uh,um
4. Remove whitespace characters that comes before an apostrophe ’
5. Convert standard or informal contracted forms of English into the original form.
6. Remove commas ( ,) between digits
7. Remove periods ( .) not followed by numbers
8.Remove symbols as well as diacritics from the text, where symbols are the characters with the Unicode category
starting with M,S, orP, except period, percent, and currency symbols that may be detected in the next step.
9.Detect any numeric expressions of numbers and currencies and replace with a form using Arabic numbers, e.g. “Ten
thousand dollars” →“$10000”.
10. Convert British spellings into American spellings.
11. Remove remaining symbols that are not part of any numeric expressions.
12. Replace any successive whitespace characters with a space.
A different, language-specific set of transformations would be needed to equivalently normalize non-English text, but due to
our lack of linguistic knowledge to build such normalizers for all languages, we resort to the following basic standardization
for non-English text:
1. Remove any phrases between matching brackets ( [,]).
2. Remove any phrases between matching parentheses ( (,)).
3.Replace any markers, symbols, and punctuation characters with a space, i.e. when the Unicode category of each
character in the NFKC-normalized string starts with M,S, orP.
4. make the text lowercase.
5. replace any successive whitespace characters with a space.
Additionally, we put a space between every letter for the languages that do not use spaces to separate words, namely Chinese,
Japanese, Thai, Lao, and Burmese, effectively measuring the character error rate instead.
We note that the above is an imperfect solution, and it will sometimes produce unintended and unexpected outputs. We do
not claim that the text format resulting from the above is more “correct” in any measure. Rather, the procedures above are
designed to better distinguish between innocuous differences in wording and genuine mistranscriptions. Python code for
the standardization procedures above is available as part of our code and model release to facilitate future iterations and
improvements on text standardization.Robust Speech Recognition via Large-Scale Weak Supervision 22
D. Raw Performance Tables
D.1. English Transcription
D.1.1. G REEDY DECODING
ModelLibriSpeech.test-cleanLibriSpeech.test-otherTED-LIUM3WSJCallHomeSwitchboardCommonV oice5.1ArtieCORAALCHiME6AMI-IHMAMI-SDM1V oxPopuli.enFleurs.en us
Whisper tiny.en 5.6 14.6 6.0 5.0 24.1 17.8 26.3 20.0 23.9 41.3 23.7 50.3 11.7 11.6
Whisper tiny 7.6 16.9 7.0 6.7 30.0 22.8 29.6 23.9 31.0 49.6 27.6 58.1 12.7 13.7
Whisper base.en 4.2 10.2 4.9 4.6 20.9 15.2 19.0 13.4 22.6 36.4 20.5 46.7 10.0 7.6
Whisper base 5.0 12.4 5.5 5.1 23.0 16.8 21.6 16.9 26.0 40.2 22.0 49.9 10.0 10.1
Whisper small.en 3.1 7.4 4.0 3.3 18.2 15.7 13.1 9.7 20.2 27.6 17.5 38.0 8.1 6.0
Whisper small 3.4 7.6 4.3 4.0 17.5 14.5 13.5 10.3 18.1 29.3 19.0 39.6 8.3 6.6
Whisper medium.en 3.1 6.3 4.1 3.3 16.2 14.1 10.6 7.6 17.5 25.3 16.4 37.2 7.4 5.0
Whisper medium 2.9 5.9 3.8 2.9 16.4 14.0 10.3 7.2 16.6 26.4 16.6 36.0 7.4 5.4
Whisper large 2.7 5.6 4.0 3.1 15.8 13.1 9.5 6.7 19.4 25.6 16.4 36.9 7.3 4.6
Whisper large-v2 2.7 5.2 4.0 3.9 17.6 13.8 9.0 6.2 16.2 25.5 16.9 36.4 7.3 4.4
wav2vec2-base-100h 6.0 13.4 17.8 13.9 46.9 40.2 47.4 40.8 47.0 79.9 48.1 81.2 28.9 23.1
wav2vec2-base-960h 3.3 8.5 12.8 8.9 40.6 32.9 36.4 30.9 39.9 68.5 40.2 71.9 21.4 17.4
wav2vec2-large-960h-lv60-self 1.8 3.8 7.4 4.4 29.1 22.2 19.9 15.8 29.2 56.3 30.8 57.0 13.0 10.2
wav2vec2-large-960h 2.7 6.2 10.5 7.7 34.8 28.3 29.9 24.5 35.6 65.8 37.0 67.6 17.9 14.6
wav2vec2-large-robust-ft-libri-960h 2.6 5.3 9.2 6.1 23.4 19.8 20.3 16.2 29.4 58.1 31.7 61.6 15.1 11.8
asr-crdnn-rnnlm-librispeech 3.0 9.7 17.7 10.7 59.7 56.1 43.7 33.3 83.8 81.0 57.2 85.8 30.6 32.4
asr-transformer-transformerlm-librispeech 2.1 5.4 11.9 7.4 38.9 33.0 30.6 23.5 44.9 79.5 44.5 75.4 17.8 17.0
hubert-large-ls960-ft 2.0 4.1 8.4 5.4 29.6 22.8 20.8 16.0 32.0 60.0 33.7 59.1 14.4 10.9
hubert-xlarge-ls960-ft 1.9 3.5 8.3 5.4 29.3 22.2 19.8 14.8 31.5 58.5 33.3 58.9 14.2 10.5
s2t-large-librispeech-asr 3.3 8.1 14.9 9.4 54.5 40.3 38.1 30.7 50.2 79.2 53.4 79.5 21.6 18.0
s2t-medium-librispeech-asr 3.6 8.2 15.7 9.7 58.1 42.4 39.3 31.3 52.6 79.8 60.3 85.3 22.9 19.7
sttenconformer ctclarge 2.1 4.2 4.4 2.1 11.3 8.2 7.4 4.0 13.5 30.5 15.9 39.9 6.7 8.2
sttenconformer transducer xlarge 1.5 2.8 4.3 1.2 12.0 7.4 4.3 1.5 19.9 36.8 20.5 48.6 6.0 6.3
unispeech-sat-base-100h-libri-ft 5.7 13.8 17.7 13.6 46.5 40.0 45.3 38.6 44.7 74.8 47.8 77.7 29.8 22.4
Table 8. English transcription WER (%) with greedy decoding
D.1.2. B EAM SEARCH WITH TEMPERATURE FALLBACK
ModelLibriSpeech.test-cleanLibriSpeech.test-otherTED-LIUM3WSJCallHomeSwitchboardCommonV oice5.1ArtieCORAALCHiME6AMI-IHMAMI-SDM1V oxPopuli.enFleurs.en us
Whisper tiny.en 5.4 12.8 5.4 4.6 21.4 16.0 23.5 18.4 21.4 42.0 22.7 54.2 10.9 10.0
Whisper tiny 6.7 15.0 6.3 5.9 24.8 18.3 26.1 20.8 25.1 48.0 25.6 57.3 11.6 12.4
Whisper base.en 4.1 9.6 4.6 4.0 18.3 14.2 17.5 13.2 18.5 35.2 21.1 49.0 9.3 7.1
Whisper base 4.9 11.0 5.0 4.4 20.5 15.6 19.4 15.3 20.5 40.0 21.5 50.0 9.5 8.9
Whisper small.en 3.2 6.7 4.3 3.0 17.2 13.4 12.6 9.2 17.5 29.5 17.9 42.5 8.1 5.3
Whisper small 3.3 7.2 4.3 3.9 17.1 13.3 12.8 9.3 16.4 30.9 19.2 43.5 8.2 6.1
Whisper medium.en 3.0 5.7 4.3 2.8 14.7 12.4 10.3 7.4 15.3 27.0 17.1 39.4 7.8 4.5
Whisper medium 2.7 5.6 4.0 2.7 15.3 13.2 9.7 6.7 14.9 27.6 17.6 43.0 7.6 4.4
Whisper large 2.8 5.7 4.3 3.5 16.2 14.2 8.9 6.4 15.1 25.2 17.6 37.1 7.2 4.5
Whisper large-v2 2.5 4.9 3.7 2.6 16.4 13.6 8.2 5.7 14.2 24.9 17.4 39.9 7.0 4.2
Table 9. English transcription WER (%) with beam search and temperature fallbackRobust Speech Recognition via Large-Scale Weak Supervision 23
D.2. Multilingual Transcription
D.2.1. M ULTILINGUAL LIBRISPEECH
ModelDutchEnglishFrenchGermanItalianPolishPortugueseSpanish
Whisper tiny 39.4 15.7 36.8 24.9 41.7 34.2 31.3 19.2
Whisper base 28.4 11.7 26.6 17.7 31.1 22.8 21.9 12.8
Whisper small 17.2 8.3 16.2 10.5 21.4 11.2 13.0 7.8
Whisper medium 11.7 6.8 8.9 7.4 16.0 6.5 9.0 5.3
Whisper large 10.2 6.3 8.9 6.6 14.3 6.6 9.2 5.4
Whisper large-v2 9.3 6.2 7.3 5.5 13.8 5.0 6.8 4.2
Table 10. WER (%) on MLS
D.2.2. C OMMON VOICE 9
ModelArabicBulgarianBengaliCatalanCzechWelshDanishGermanGreekEnglishSpanishEstonianPersian
Whisper tiny 90.9 79.3 104.1 51.0 79.7 101.8 77.2 34.5 61.9 28.8 30.3 102.1 120.3
Whisper base 84.4 68.1 103.7 39.9 63.1 93.8 57.5 24.5 51.5 21.9 19.6 88.1 99.0
Whisper small 66.4 44.8 118.6 23.8 34.1 65.4 32.1 13.0 31.7 14.5 10.3 67.2 71.9
Whisper medium 60.3 26.7 124.7 16.4 18.8 43.6 19.3 8.5 20.0 11.2 6.9 45.6 49.9
Whisper large 56.0 24.1 106.0 15.3 17.1 40.3 18.3 7.7 18.3 10.1 6.4 41.4 44.8
Whisper large-v2 53.8 19.9 103.4 14.1 13.5 34.2 14.4 6.4 16.0 9.4 5.6 35.1 39.4
ModelFinnishFrenchHindiHungarianIndonesianItalianJapaneseLithuanianLatvianMalayalamMongolianDutchPolish
Whisper tiny 68.5 49.7 108.3 87.0 49.6 44.5 36.1 103.5 87.8 102.7 123.0 43.6 45.3
Whisper base 52.9 37.3 106.5 71.9 36.1 30.5 24.2 91.3 78.0 122.9 137.0 29.5 32.8
Whisper small 30.5 22.7 43.6 44.4 18.4 16.0 14.0 72.8 54.6 104.8 225.8 14.2 16.9
Whisper medium 18.8 16.0 31.5 26.9 11.6 9.4 10.5 49.4 37.2 137.8 113.4 8.0 10.1
Whisper large 17.0 14.7 25.0 23.5 10.6 8.1 9.4 43.9 34.8 107.1 117.4 7.1 9.0
Whisper large-v2 14.4 13.9 21.9 19.7 8.5 7.1 9.1 35.2 25.5 103.2 128.4 5.8 7.6
ModelPortugueseRomanianRussianSlovakSlovenianSerbianSwedishTamilThaiTurkishUrduVietnameseChinese
Whisper tiny 35.2 68.2 40.6 104.0 82.0 106.1 58.2 105.7 55.9 53.6 74.7 69.3 52.4
Whisper base 23.7 55.9 28.8 87.2 70.3 103.0 42.4 49.5 32.1 38.6 58.6 51.6 44.9
Whisper small 12.5 33.2 15.0 60.4 45.5 101.3 22.1 28.7 18.1 23.7 39.1 33.3 29.4
Whisper medium 8.1 21.5 9.3 42.0 29.8 85.6 13.7 19.6 10.5 17.7 29.9 24.4 23.2
Whisper large 7.1 19.8 8.2 37.9 25.1 87.4 12.4 17.6 8.8 16.6 28.1 19.9 29.1
Whisper large-v2 6.3 15.8 7.1 31.9 20.6 70.5 10.6 16.1 8.0 14.5 24.2 18.2 26.8
Table 11. WER (%) on CommonV oice9
D.2.3. V OXPOPULI
ModelCzechGermanEnglishenaccentedSpanishEstonianFinnishFrenchCroatianHungarianItalianLithuanianDutchPolishRomanianSlovakSlovenian
Whisper tiny 73.5 27.4 11.6 18.8 19.7 99.2 54.1 32.9 72.4 74.5 40.5 93.1 41.9 31.4 65.9 78.7 81.9
Whisper base 54.7 20.6 9.5 17.5 14.4 83.0 39.7 24.9 53.6 52.6 30.8 82.1 29.4 22.1 49.3 63.7 70.5
Whisper small 28.8 14.8 8.2 19.2 11.1 59.2 24.9 15.7 33.7 31.3 22.9 60.1 18.8 13.3 28.6 37.3 50.8
Whisper medium 18.4 12.4 7.6 19.1 9.6 38.2 16.6 12.2 23.9 19.3 19.7 39.3 14.9 10.1 18.4 23.0 36.3
Whisper large 15.9 11.9 7.2 20.8 8.8 33.3 15.5 11.0 19.0 16.8 18.4 35.0 14.0 9.0 17.0 19.1 31.3
Whisper large-v2 12.6 11.2 7.0 18.6 8.2 28.7 12.4 11.4 16.1 13.8 19.0 33.2 12.9 7.8 14.4 15.4 27.9
Table 12. WER (%) on V oxPopuliRobust Speech Recognition via Large-Scale Weak Supervision 24
D.2.4. F LEURS
ModelAfrikaansAmharicArabicAssameseAzerbaijaniBelarusianBulgarianBengaliBosnianCatalanChineseCzechWelshDanish
Whisper tiny 91.2 122.9 63.4 102.0 93.1 94.0 81.0 101.6 82.1 42.8 40.5 82.8 101.3 82.0
Whisper base 81.5 196.8 48.8 102.0 76.4 91.3 65.1 100.6 66.7 29.0 34.1 66.0 85.3 57.6
Whisper small 61.1 120.2 30.6 108.0 49.1 75.1 37.3 104.4 39.4 16.2 20.8 37.6 59.3 32.8
Whisper medium 44.9 229.3 20.4 102.3 33.1 60.4 21.4 100.6 23.9 9.6 12.1 21.3 40.8 19.5
Whisper large 42.6 129.3 18.1 105.6 28.7 56.6 18.4 104.9 20.7 8.0 19.6 17.4 36.6 16.8
Whisper large-v2 36.7 140.3 16.0 106.2 23.4 45.4 14.6 104.1 15.7 7.3 14.7 13.3 33.0 13.8
ModelGermanGreekEnglishSpanishEstonianPersianFinnishTagalogFrenchGalicianGujaratiHausaHebrewHindi
Whisper tiny 27.8 67.4 12.4 15.9 94.8 101.8 59.5 65.6 41.4 54.8 101.2 100.2 71.6 102.3
Whisper base 17.9 53.5 8.9 9.9 77.9 86.1 43.1 45.8 28.5 47.4 101.4 98.6 61.7 101.1
Whisper small 10.2 30.8 6.1 5.6 51.3 55.8 24.0 27.7 15.0 30.2 106.4 90.1 44.4 38.4
Whisper medium 6.5 19.0 4.4 3.6 29.8 41.0 13.9 19.1 8.7 21.2 104.8 106.6 33.1 26.8
Whisper large 5.5 18.7 4.5 3.5 25.5 36.1 12.2 15.8 7.7 19.0 103.9 87.0 30.2 26.9
Whisper large-v2 4.5 12.5 4.2 3.0 21.9 32.9 9.7 13.8 8.3 15.4 102.7 88.9 27.1 21.5
ModelCroatianHungarianArmenianIndonesianIcelandicItalianJapaneseJavaneseGeorgianKazakhKhmerKannadaKoreanLuxembourgish
Whisper tiny 79.0 83.8 118.6 51.7 113.3 29.8 37.0 107.3 123.0 165.2 100.6 100.7 36.1 99.1
Whisper base 59.1 65.0 126.3 33.1 95.5 17.9 22.8 89.5 114.7 109.2 101.6 107.2 27.8 100.7
Whisper small 33.4 38.9 86.6 16.3 72.6 9.8 12.0 88.6 118.3 70.3 104.4 100.4 19.6 100.1
Whisper medium 19.3 24.3 60.1 10.2 49.9 5.2 7.1 67.9 117.3 48.8 98.9 77.7 16.4 90.0
Whisper large 16.7 21.0 53.7 8.5 43.0 4.2 6.4 87.0 100.5 43.8 96.0 69.8 15.2 86.5
Whisper large-v2 13.4 17.0 44.6 7.1 38.2 4.0 5.3 nan 105.0 37.7 99.7 37.0 14.3 88.0
ModelLingalaLaoLithuanianLatvianMaoriMacedonianMalayalamMongolianMarathiMalayMalteseMyanmarNorwegianNepali
Whisper tiny 105.4 115.1 98.5 91.6 94.5 73.3 101.5 113.7 100.3 51.2 100.8 124.8 62.0 101.8
Whisper base 96.7 105.1 87.3 79.8 77.5 59.9 107.4 125.7 100.3 35.1 97.6 122.6 44.0 102.4
Whisper small 91.3 102.2 65.6 53.2 59.5 36.9 100.9 144.2 60.2 18.9 92.2 110.1 24.2 69.5
Whisper medium 83.2 101.4 41.1 32.0 77.8 22.0 101.1 103.7 63.2 12.2 83.2 123.0 12.9 54.4
Whisper large 76.8 101.6 35.2 28.3 45.7 20.6 101.4 106.2 43.7 10.2 80.5 124.5 11.4 52.2
Whisper large-v2 75.6 101.5 28.1 23.1 38.5 16.5 100.7 110.5 38.3 8.7 76.6 115.7 9.5 47.1
ModelDutchOccitanPunjabiPolishPashtoPortugueseRomanianRussianSindhiSlovakSlovenianShonaSomaliSerbian
Whisper tiny 49.0 95.9 102.6 45.6 105.6 20.1 74.7 31.1 105.8 77.2 87.2 128.1 105.6 83.7
Whisper base 33.0 82.9 101.5 30.8 99.0 13.0 56.0 20.5 103.9 60.6 74.6 126.0 109.6 64.3
Whisper small 16.4 87.3 103.6 14.7 92.9 7.3 29.8 11.4 131.7 33.3 49.3 140.0 105.3 42.2
Whisper medium 9.9 79.5 102.0 8.0 119.4 5.0 20.0 7.2 147.0 17.3 31.9 143.9 104.0 44.9
Whisper large 8.3 75.9 102.8 7.2 92.7 4.8 15.4 6.4 177.9 15.7 27.8 130.0 103.5 29.2
Whisper large-v2 6.7 75.3 102.4 5.4 93.7 4.3 14.4 5.6 156.5 11.7 23.1 121.0 102.9 33.9
ModelSwedishSwahiliTamilTeluguTajikThaiTurkishUkrainianUrduUzbekVietnameseYoruba
Whisper tiny 52.7 100.9 99.9 105.1 101.7 58.8 42.5 51.2 65.2 105.2 60.0 106.4
Whisper base 37.4 92.5 58.7 105.2 109.3 38.2 27.5 37.7 52.0 114.0 40.5 101.8
Whisper small 20.8 73.7 35.2 98.2 84.3 21.9 15.9 19.3 37.3 107.7 21.2 116.4
Whisper medium 11.2 52.8 23.1 82.8 74.0 15.4 10.4 11.6 28.2 109.6 12.7 105.1
Whisper large 10.5 47.9 20.6 100.6 74.5 13.2 9.4 10.3 25.0 93.3 10.7 111.7
Whisper large-v2 8.5 39.3 17.5 99.0 85.8 11.5 8.4 8.6 22.6 90.2 10.3 94.8
Table 13. WER (%) on FleursRobust Speech Recognition via Large-Scale Weak Supervision 25
D.3. Speech Translation
D.3.1. F LEURS
ModelAfrikaansAmharicArabicAssameseAzerbaijaniBelarusianBulgarianBengaliBosnianCatalanChineseCzechWelshDanish
Whisper tiny 1.6 0.1 0.1 0.4 0.1 0.8 0.4 0.4 0.4 5.2 0.6 0.6 0.6 0.7
Whisper base 4.4 0.3 1.0 0.4 0.8 3.3 2.7 0.7 4.1 13.1 1.9 2.7 0.7 5.0
Whisper small 18.1 0.2 10.6 1.2 5.8 7.1 14.8 2.7 16.8 25.1 9.3 14.2 1.3 18.1
Whisper medium 29.5 0.9 19.9 3.5 11.7 9.8 23.9 10.6 26.0 31.9 15.1 23.6 8.4 28.6
Whisper large 31.6 1.1 23.8 3.9 13.1 11.0 26.2 12.0 28.0 33.7 16.8 25.6 11.2 31.6
Whisper large-v2 34.1 1.9 25.5 5.4 13.7 11.7 28.5 13.2 29.7 34.2 18.4 27.8 13.0 32.7
ModelGermanGreekEnglishSpanishEstonianPersianFinnishTagalogFrenchGalicianGujaratiHausaHebrewHindi
Whisper tiny 5.2 0.1 68.6 7.7 0.1 0.1 0.2 0.8 4.7 4.0 0.7 0.1 0.2 1.0
Whisper base 13.7 0.7 73.3 12.4 0.3 0.2 0.5 2.1 13.1 10.5 1.5 0.0 0.6 3.4
Whisper small 25.9 11.6 77.3 18.2 3.6 5.8 7.3 12.0 23.5 17.5 3.9 0.3 5.4 11.1
Whisper medium 31.4 19.9 79.2 21.4 13.5 15.0 18.5 20.5 28.6 24.7 12.8 0.5 15.9 19.4
Whisper large 34.3 21.7 77.8 22.8 15.9 17.6 20.6 22.7 31.6 26.0 14.8 0.5 19.6 20.7
Whisper large-v2 34.6 23.7 80.2 23.3 18.7 19.6 22.1 24.4 32.2 27.9 16.2 0.4 21.8 22.0
ModelCroatianHungarianArmenianIndonesianIcelandicItalianJapaneseJavaneseGeorgianKazakhKhmerKannadaKoreanLuxembourgish
Whisper tiny 0.6 0.1 0.1 0.3 0.4 5.3 0.2 0.2 0.1 0.1 0.1 0.8 0.5 0.8
Whisper base 3.7 0.2 0.1 2.6 0.4 11.3 1.5 0.2 0.2 0.2 0.1 0.9 3.7 1.7
Whisper small 14.6 4.8 0.7 16.4 1.8 17.8 9.6 1.4 0.2 0.8 0.5 2.3 12.2 5.7
Whisper medium 23.0 15.5 10.4 24.1 6.8 21.6 14.9 5.0 1.3 4.3 3.3 8.5 19.2 13.6
Whisper large 25.4 18.3 13.2 27.2 6.6 23.5 17.0 5.1 2.7 6.3 5.2 9.9 20.0 15.4
Whisper large-v2 27.0 21.2 16.0 29.1 9.1 23.6 18.9 6.2 2.4 5.4 6.1 11.6 21.3 16.8
ModelLingalaLaoLithuanianLatvianMaoriMacedonianMalayalamMongolianMarathiMalayMalteseMyanmarNorwegianNepali
Whisper tiny 0.1 0.2 0.1 0.2 0.3 1.0 0.8 0.1 0.2 0.3 0.6 0.1 1.4 0.1
Whisper base 0.1 0.3 0.3 0.4 1.0 5.4 1.4 0.1 0.9 2.1 1.4 0.1 8.4 0.3
Whisper small 0.5 2.0 1.9 1.5 3.9 15.3 5.7 0.1 3.8 14.1 4.9 0.0 22.0 2.9
Whisper medium 0.9 8.1 9.6 10.0 8.5 23.5 13.8 0.5 10.9 23.2 11.2 0.2 29.1 12.7
Whisper large 1.2 9.3 12.0 12.5 9.4 26.4 16.5 1.0 13.1 25.5 12.8 0.5 30.5 12.9
Whisper large-v2 1.0 11.0 14.0 14.3 10.2 27.7 16.7 1.0 12.9 27.3 13.5 0.4 31.4 16.1
ModelDutchOccitanPunjabiPolishPashtoPortugueseRomanianRussianSindhiSlovakSlovenianShonaSomaliSerbian
Whisper tiny 2.7 1.7 0.3 0.8 0.3 12.1 1.0 3.1 0.5 0.7 0.3 0.1 0.0 0.6
Whisper base 7.5 4.2 1.1 5.1 0.4 22.4 4.9 12.1 0.7 4.6 1.3 0.3 0.1 5.4
Whisper small 15.9 9.5 4.4 14.0 0.8 31.2 18.3 19.7 2.0 14.4 6.9 0.6 0.1 19.3
Whisper medium 21.6 15.9 12.8 19.0 2.1 35.9 26.6 24.8 5.5 22.7 14.0 1.4 0.4 27.7
Whisper large 22.8 16.8 14.6 21.4 3.7 37.4 29.1 26.7 5.9 25.1 16.9 1.8 0.5 30.5
Whisper large-v2 24.0 20.2 15.7 22.3 3.4 38.1 31.5 27.8 5.7 26.1 17.0 1.8 0.7 32.5
ModelSwedishSwahiliTamilTeluguTajikThaiTurkishUkrainianUrduUzbekVietnameseYoruba
Whisper tiny 1.8 0.1 0.2 0.3 0.2 0.2 0.2 1.2 0.4 0.0 0.1 0.2
Whisper base 9.1 0.1 0.4 0.4 0.2 0.7 2.4 6.9 1.5 0.2 0.9 0.5
Whisper small 22.9 0.1 2.1 4.0 4.4 5.8 15.7 18.7 8.8 0.5 8.5 0.5
Whisper medium 32.1 3.1 7.0 10.8 11.4 12.8 22.9 25.8 14.9 3.8 16.6 0.9
Whisper large 33.1 5.3 8.5 10.9 13.0 15.2 25.7 28.0 16.3 5.8 19.5 1.2
Whisper large-v2 35.3 7.2 9.2 12.5 14.5 16.1 26.6 29.4 17.2 6.0 20.4 1.4
Table 14. BLEU scores on FleursRobust Speech Recognition via Large-Scale Weak Supervision 26
D.3.2. C OVOST 2
ModelArabicCatalanWelshGermanSpanishEstonianPersianFrenchIndonesianItalianJapaneseLatvianMongolian
Whisper tiny 0.2 4.9 0.4 4.0 10.5 0.2 0.1 6.1 0.3 5.1 0.3 0.1 0.1
Whisper base 1.2 11.0 0.5 11.7 21.3 0.3 0.1 15.4 4.9 13.0 4.9 0.5 0.1
Whisper small 17.7 22.3 1.0 25.3 33.0 2.4 4.9 27.3 27.6 24.0 17.3 1.4 0.2
Whisper medium 30.6 29.2 12.1 33.2 38.4 11.4 15.5 33.6 42.3 29.5 24.6 9.7 0.2
Whisper large 35.5 30.3 16.1 34.3 38.0 13.4 17.5 34.4 45.4 29.1 24.2 10.5 0.3
Whisper large-v2 39.7 31.8 21.5 36.3 40.1 15.0 19.3 36.4 48.1 30.9 26.1 13.9 0.1
ModelDutchPortugueseRussianSlovenianSwedishTamilTurkishChinese
Whisper tiny 4.3 9.5 5.7 0.4 2.0 0.1 0.2 0.4
Whisper base 12.4 23.2 16.1 1.4 10.5 0.4 2.8 1.4
Whisper small 28.1 40.6 30.9 9.2 29.9 1.7 16.8 6.8
Whisper medium 38.1 48.7 39.4 17.7 39.5 2.9 27.0 14.0
Whisper large 39.3 48.6 41.6 23.9 40.3 3.7 26.7 17.1
Whisper large-v2 41.2 51.6 43.3 21.6 42.9 4.2 28.3 18.0
Table 15. BLEU scores on CoV oST2
D.4. Long-form Transcription
ModelTED-LIUM3MeanwhileKincaid46Rev16Earnings-21Earnings-22CORAAL
Whisper tiny.en 5.5 12.8 13.8 15.1 17.0 22.0 30.3
Whisper tiny 6.8 15.5 16.7 17.0 18.7 24.4 33.1
Whisper base.en 4.6 9.4 11.2 13.2 12.5 16.6 25.2
Whisper base 4.8 12.2 12.2 14.5 13.5 18.4 26.9
Whisper small.en 4.6 6.0 9.4 12.0 10.8 14.0 21.9
Whisper small 4.2 6.9 10.1 12.1 11.1 14.3 22.3
Whisper medium.en 3.6 5.2 8.9 11.9 10.2 13.3 20.6
Whisper medium 3.8 5.4 8.6 11.4 10.3 13.2 20.3
Whisper large 3.8 5.3 8.8 11.0 10.3 13.4 20.4
Whisper large-v2 3.5 5.1 8.8 11.3 9.7 12.6 19.6
wav2vec2-base-100h 17.6 27.7 39.3 35.2 45.7 57.1 55.4
wav2vec2-base-960h 12.8 19.7 32.9 29.8 37.3 46.8 49.1
wav2vec2-large-960h-lv60-self 7.2 11.4 21.1 21.3 21.7 28.0 36.7
wav2vec2-large-960h 10.1 16.4 27.4 26.4 30.4 40.1 43.5
wav2vec2-large-robust-ft-libri-960h 8.8 15.2 22.9 23.4 23.0 31.0 36.8
hubert-large-ls960-ft 8.1 12.9 22.4 23.4 23.0 30.6 37.9
hubert-xlarge-ls960-ft 8.1 12.5 22.9 23.2 23.1 31.3 38.1
sttenconformer ctclarge 4.0 9.8 13.1 14.5 12.6 17.6 25.1
sttenconformer transducer xlarge 5.3 10.6 17.1 19.8 16.2 19.7 38.9
Table 16. Long-form English transcription WER (%)Robust Speech Recognition via Large-Scale Weak Supervision 27
E. Training Dataset Statistics
0.1 1 10 100 1K 10K
Hours of audioMultilingual Speech Recognition
Lao 0.1Sundanese0.1Burmese 0.1Malagasy 0.2T ajik 0.3Gujarati 0.3Uzbek 0.3Yiddish 0.4Malayalam 0.5Georgian 0.6Nepali 0.6Marathi 0.6Punjabi 0.8Haitian Creole 1.0Maltese 1.1Bengali 1.3Khmer 1.3Belarusian 2.4Kannada 3.8Afrikaans 4.1T elugu 4.3Swahili 5.4Sinhala 5.4Albanian 5.7Galician 8.9Bosnian 11Hindi 12Kazakh 12Armenian 13Macedonian 16Icelandic 16Basque 21Persian 24Serbian 28Slovenian 41Estonian 41Azerbaijani 47Latvian 65Lithuanian 67Welsh 73T agalog 75Bulgarian 86Slovak 90Croatian 91Urdu 104T amil 136Czech 192Thai 226Norwegian 266Romanian 356Hungarian 379Malay 382Danish 473Greek 529Hebrew 688Vietnamese 691Ukrainian 697Arabic 739Indonesian 1014Finnish 1066Catalan 1883Dutch 2077Swedish 2119Italian 2585Polish 4278Turkish 4333Japanese 7054Korean 7993Portuguese 8573French 9752Russian 9761Spanish 11100German 13344Chinese 23446
65% English Speech Recognition
(438,218 hours)18% Translation
(125,739 hours)17% Multilingual Speech Recognition
(117,113 hours)Dataset Components
1 10 100 1K 10K
Hours of audioTranslation
Turkmen 1Bashkir 1Malagasy 2Uzbek 4Sundanese 7Hausa 8Luxembourgish 10T atar 14T ajik 15Lingala 20Lao 20Somali 21Macedonian 30Kazakh 31Amharic 32Georgian 40Maltese 41Sindhi 46Faroese 46Occitan 49Burmese 59Pashto 63Latvian 68Albanian 72Haitian Creole 74Estonian 79Mongolian 79Icelandic 84Yiddish 85Azerbaijani 86Kannada 90Lithuanian 99Armenian 116Punjabi 117Belarusian 133Nepali 133Assamese 136Serbian 136Slovak 144Basque 168Tibetan 186Sanskrit 195Bulgarian 202Gujarati 208Sinhala 211Bosnian 219Catalan 236Croatian 239Breton 269Shona 279Swahili 282Marathi 288Norwegian 322Afrikaans 330Hawaiian 338Galician 368Danish 386Persian 392Slovenian 395Czech 401Hebrew 418Yoruba 432Ukrainian 509Hungarian 554Romanian 555Javanese 622Khmer 672Finnish 750Malayalam 892T agalog 894Greek 968T elugu 987Swedish 1055Indonesian 1174Maori 1381T amil 1484Latin 1614Thai 1635Malay 1691Vietnamese 1719Dutch 1767Norwegian Nynorsk 1889Bengali 1988Urdu 1990Italian 2145Polish 2200Turkish 2241Arabic 2286Portuguese 3620German 4309French 4481Hindi 5438Spanish 6693Russian 7687Welsh 8263Japanese 8860Chinese 11731Korean 19938
Figure 11. Training dataset statisticsRobust Speech Recognition via Large-Scale Weak Supervision 28
F. Hyperparameters
Hyperparameter Value
Updates 1048576
Batch Size 256
Warmup Updates 2048
Max grad norm 1.0
Optimizer AdamW
β1 0.9
β2 0.98
ϵ 10−6
Weight Decay 0.1
Weight Init Gaussian Fan-In
Learning Rate Schedule Linear Decay
Speechless audio subsample factor 10×
Condition on prior text rate 50%
Table 17. Whisper training hyperparameters.
Hyperparameter Value
Updates 655360
Batch Size 1024
BPE Dropout 0.1
Stochastic Depth 0.1
SpecAugment Policy LibriSpeech Basic
Table 18. Hyperparameters changed for Whisper Large V2.
Model Max Learning Rate
Tiny 1.5×10−3
Base 1×10−3
Small 5×10−4
Medium 2.5×10−4
Large 1.75×10−4
Large V2 2.0×10−4
Table 19. Whisper model learning rates.Better speech synthesis through scaling
James Betker
Abstract
In recent years, the field of image generation has been revolutionized by the appli-
cation of autoregressive transformers and DDPMs. These approaches model the
process of image generation as a step-wise probabilistic processes and leverage
large amounts of compute and data to learn the image distribution.
This methodology of improving performance need not be confined to images. This
paper describes a way to apply advances in the image generative domain to speech
synthesis. The result is TorToise - an expressive, multi-voice text-to-speech sys-
tem.
All model code and trained weights have been open-sourced at
https://github.com/neonbjb/tortoise-tts.
1 Background
1.1 Text-to-speech
The field of text-to-speech (TTS) research has been largely constrained to the development of effi-
cient models trained on relatively small datasets. This choice has been driven by:
1. The desire to build efficient speech generation models that can be deployed at scale and
thus must have a high sampling rate.
2. The unavailability of very large, transcribed speech datasets.
3. Challenges scaling the encoder-decoder model architectures traditionally used in TTS.
1.1.1 Neural MEL Inverters
Most modern text-to-speech systems operate on speech data that is encoded as a MEL spectrogram.
There are many compelling reasons to operate in this encoding space, but for neural networks, the
most compelling reason is that it is highly spatially compressed. The MEL configuration used by
the Tacotron, for example, operates at 256x compression over raw audio waveform data sampled at
22kHz, but contains most of the information found in that data.
Because of this, an entire body of research has been dedicated to finding high-quality ways to decode
MEL spectrograms back into audio waveforms. A synthesizer that performs this task is generally
called a “vocoder”, but I more generally refer to it as a “MEL inverter” in this paper.
Modern MEL inverters built on neural networks are incredibly sophisticated. They produce wave-
forms that are nearly indistinguishable from recorded waveforms to human ears, and they are highly
generalizable outside of their training set. I capitalize on this work by using an implementation of
Univnet(Kim, 2021) as a final stage for my text-to-speech system.
1.2 Image generation
While TTS systems largely focus on latency, this has not been the case in other domains. For
example, with image generation, more focus has been applied to training models that generate high-arXiv:2305.07243v2  [cs.SD]  23 May 2023quality results, regardless of the sampling time. For the purposes of this paper, I dive into two bodies
of research:
1.2.1 DALL-E
DALL-E(Ramesh et al., 2021) showed how an autoregressive decoder can be applied to text-to-
image generation. This is particularly appealing because of the vast quantity of research that has
been poured into scaling decoder-only models in the NLP domain.
Two important problems persist with DALL-E: first, it relies on full-sequence self-attention, which
carries a cost of O(N2)compute and memory, where N is the sequence length. This is particularly
troublesome when dealing with modalities like images or audio, which have large sequence lengths
when dealt with naively.
Second, traditional autoregressive approaches require operating in the discrete domain. Images are
encoded into sequences of discrete tokens using a quantizing autoencoder. DALL-E then models
these sequences of tokens using an autoregressive prior model. This is a strength of DALL-E in
terms of expressiveness, but it comes at the cost of requiring a decoder which can convert these
image tokens back into the pixel values that actually comprise an image. It is my opinion that
learned VQV AE decoder used by DALL-E is principally responsible for the blurry incoherence
exhibited by most of it’s samples.
1.2.2 DDPMs
The generative model space has long been plagued by models that either exhibit mean-seeking
behavior (resulting in blurriness) or mode-collapse (resulting in a lack of diversity or generalization).
Denoising diffusion probabilistic models (DDPMs(Ho et al., 2020)) have recently arisen as the first
type of generative model capable of producing crisp, coherence and diverse images. These models
have been shown to be quite effective at using low-quality guidance signals to reconstruct the high-
dimensional space that those guidance signals were derived from. Put another way, they are great at
super-resolution.
There are two important caveats to DDPMS:
1. Traditional approaches to DDPMs rely on fixed output shapes that are known before sam-
pling begins. As a concrete example relevant to this paper, DDPMs cannot learn to convert
text into audio signals because they cannot solve the implicit alignment problem between
text and audio.
2. DDPMs must be sampled from over multiple iterations. This sampling process consumes
a great deal of compute, and means sampling from a DDPM will always incur a significant
latency cost.
1.2.3 Re-ranking
DALL-E introduced the process of “re-ranking” the outputs of autoregressive models. This process
samples randomly from the autoregressive model and picks the highest quality output of k outputs
for downstream use.
Such a procedure requires a strong discriminator: a model that can tell good text/image pairings
from bad. DALL-E used CLIP(Radford et al., 2021), a model trained with a contrastive text and
image pairing objective.
2 Methods
2.1 Joining Autoregressive Decoders and DDPMs
To review some of the conclusions drawn above:
1. Autoregressive models are strong at converting between unaligned domains like vision, text
and speech.
2Figure 1: TorToise-v2 architectural design diagram. Inputs of text and a reference audio clip (for speaker
cloning) flow through a series of decoding and filtering networks to produce high-quality speech.
2. DDPMs operate in the continuous domain which allows them to model expressive modali-
ties.
Both types of models have demonstrated the ability to scale performance with additional compute
and data.
It becomes evident that when posed with a problem like generating continuous data like speech
spectrograms or images, a marriage of these two approaches might have some distinct advantages.
Specifically, in inference, the autoregressive model will be used to convert a sequence of text tokens
to a sequence of tokens representing the output space (in our case, speech tokens). The DDPM will
then be used to decode these tokens into a high quality representation of speech.
2.2 Applying Autoregression+DDPMs to TTS
To build out the previously proposed system, we need to train the following neural networks:
1. An autoregressive decoder which predicts a probability distribution for speech tokens, con-
ditioned on text.
2. A contrastive model similar to CLIP which is used to rank outputs of the autoregressive
decoder.
3. A DDPM which can convert speech tokens back into speech spectrograms.
The architectures and training process for all of these networks largely follow the procedures found
in their respective literature. Details can be found in B
2.2.1 Conditioning Input
A unique design choice made with TorToise is an additional input which is provided to both the
autoregressive generator and the DDPM, which I term the speech conditioning input.
The speech conditioning input starts as one or more audio clips of the same speaker as the target.
These clips are converted to MEL spectrograms and fed through an encoder consisting of a stack
of self-attention layers. The autoregressive generator and the DDPM have their own conditioning
encoders, both of which are learned alongside their respective networks.
The output of these layers is averaged to produce a single vector. The vectors from all of the encoded
conditioning clips are then averaged again before being fed as an input into the autoregressive or
conditioning networks.
3The intuition behind the conditioning input is that it provides a way for the models to infer vocal
characteristics like tone and prosody such that the search space of possible speech outputs corre-
sponding to a given textual input is greatly reduced.
2.2.2 The “TorToise Trick”
For the majority of the training procedure, the DDPM is trained to convert discrete speech codes
into MEL spectrograms. After this process has converged, I fine-tune the DDPM on the autoregres-
sive latent space which is pulled from the AR model outputs instead of the speech codes. This is
described in detail in B.
The logic here is that the AR latent space is far more semantically rich than discrete tokens. By
fine-tuning on this latent space, we improve the efficiency of the downstream diffusion model. I
liken this to recent work showing that training decoder models conditioned on frozen text encoders
to produce large efficiency gains. This fine-tuning is one of the greatest contributors to model output
quality of any of the tweaks I made to the various model training processes.
2.3 CLVP
As mentioned earlier, a good strategy for gathering expressive outputs from generative models is
using a qualitative discriminator to re-rank several outputs, then choosing only the best. DALL-E
uses CLIP for this.
This same type of approach used for CLIP can be applied to speech: after all, most TTS datasets are
simply pairings of audio clips and text. By training a model on these pairs in a contrastive setting,
the model becomes a good discriminator for speech.
For TorToise, I train the Contrastive Language-V oice Pretrained Transformer, or CLVP. It has many
of the same properties of CLIP, but notably serves as a scoring model for use in re-ranking TTS
outputs from the AR model.
To make this work efficiently in inference, I trained CLVP to pair discretized speech tokens with
text tokens. This way, CLVP can rerank multiple AR outputs without the expensive diffusion model
being invoked.
3 Training
These models were trained on a small cluster of 8 NVIDIA RTX-3090s over the period of 1 year.
Specifics on how these models are trained can be found in B.
4 Inference Process
Once the four models of the framework are fully trained, the inference procedure is as follows:
1. Feed the conditioning inputs and the text into the autoregressive model and decode a large
number of output candidates.
2. Use CLVP to produce correlation scores between each speech candidate and text.
3. Choose the top k speech candidates, and for each candidate:
4. Decode to a MEL spectrogram using the DDPM.
5. Convert to a waveform using a conventional vocoder.
6. When decoding the autoregressive model, nucleus sampling is used with P=.8, repetition
penalty=2 and softmax temperature=.8.
Sampling from DDPMs is a highly studied and rapidly changing field. At the time TorToise was
designed, I found the sampling configuration with the best balance between quality and inference
speed to be as follows:
1. Algorithm: DDIM(Song et al., 2022)
42. Schedule: Linear
3. Sampling steps: 64
4. Conditioning-Free Guidance constant: 2
5 The Dataset
Since my goal was to train what is essentially a large language model, I needed a lot of data. I started
with the LibriTTS(Zen et al., 2019) and HiFiTTS(Bakhturina et al., 2021) datasets, which combined
contain 896 hours of transcribed speech. I built an additional, “extended” dataset of 49,000 hours of
speech audio from audiobooks and podcasts scraped from the internet. Details on how this dataset
was built are in appendix I. The official LibriTTS test split was used for validation purposes.
6 Experiments
Text to speech systems are challenging to experimentally compare because many state of the art sys-
tems are closed source with few samples to compare against. To this end, I built my own evaluation
suite which uses CLVP to produce a distance metric between real samples and generated samples,
similar to the FID score used by images. I also use an open source wav2vec model to characterize
the “intelligibility” of a speech segment. I have open sourced this work here.
Past this, comparisons between the samples generated from TorToise and those generated by other
papers can be found here.
7 Conclusion
TorToise is the latest in a line of recent state-of-the-art breakthroughs that use general model archi-
tectures. Almost no part of TorToise was designed specifically for audio processing, yet it outper-
forms all previous TTS models in realism. It does this by: Embracing generalist architectures like
stacks of transformer layers. Leveraging a large, high-quality dataset. Training at large-ish scale and
high batch size.
My main take-away from this project is how incredibly strong the results are from adhering to the
above 3 points. It seems likely to me that any digitized modality is subject to generative modeling
using this framework.
References
Bakhturina, E., Lavrukhin, V ., Ginsburg, B., and Zhang, Y . (2021). Hi-fi multi-speaker english tts
dataset.
Ho, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models.
Kim, J. (2021). Mindslab UnivNet implementation.
Nichol, A. and Dhariwal, P. (2021). Improved denoising diffusion probabilistic models.
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A.,
Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. (2021). Learning transferable visual models
from natural language supervision.
Ramesh, A., Pavlov, M., Goh, G., Gray, S., V oss, C., Radford, A., Chen, M., and Sutskever, I.
(2021). Zero-shot text-to-image generation.
Seonghyeon, K. (2019). VQV AE (rosinality).
Song, J., Meng, C., and Ermon, S. (2022). Denoising diffusion implicit models.
Wang, P. (2020). Vector Quantize (Lucidrains).
Wang, P. (2021). x-transformers (Lucidrains).
5Zen, H., Dang, V ., Clark, R., Zhang, Y ., Weiss, R. J., Jia, Y ., Chen, Z., and Wu, Y . (2019). Libritts:
A corpus derived from librispeech for text-to-speech.
A Extended Dataset Collection
I independently built an extended TTS dataset composed of audiobooks and podcasts scraped from
the web. This data was split on 500ms silences, and any audio clip between 5-20 seconds was kept.
I then fed the resulting clips through a pipeline of classifiers that I trained which remove any audio
with background noise, music, poor quality (such as phone calls), multiple voices speaking at once
and reverb. Due to disk space limitations, I was forced to limit the amount of scraping. The end
result was 49,000 hours of cleaned audio clips.
I transcribed this dataset using a wav2vec2-large model. I personally fine-tuned this model to predict
punctuation, as quotation marks, commas and exclamation marks are important for the purposes of
generating speech but are not generally included in the training of speech recognition models. Fine-
tuning was performed on LibriTTS and HiFiTTS and the pretrained model weights and transcription
scripts can be found here.
B Training and Architecture Details
B.1 VQVAE
The VQV AE used with TorToise is most similar to that of the original VQV AE by van der Oord
et al. It operates on MEL spectrograms. It consists of a small residual convolutional network that
compresses the spectrogram an additional 4x and produces a codebook consisting of 8192 tokens.
When training the VQV AE, I found that larger batch sizes decrease reconstruction losses, and thus
used a very large batch size for my infrastructure. Input samples were constricted to 40960 PCM
readings, or 2 seconds of audio. The primary bottleneck for training the VQV AE was the dataloader.
6Figure 2: Training curves for VQV AE. Y-axis is MSE loss in log-log scale. X-axis is number of training steps.
Model shape 1D Conv resnet, encoder + decoder
Top dim 512
Bottom dim 1024
Codebook dim 256
Quantizer token count 8192
Quantization algorithm Clustering a la original VQV AE, no restart
Batch size 8192
Total training 360M samples
Losses MSE reconstruction loss, commitment loss
LR 3e-4
B1, B2 .9 .9999
Weight decay .01
EMA weights replaces LR decay with rate .999
Table 1: VQV AE model details & hyperparameters
7B.2 Autoregressive Prior
The AR decoder uses a bog-standard GPT-2 architecture and generally follows the training instruc-
tions from the DALLE-1 paper. Unlike DALL-E, only dense self-attention is used. The prompt is
assembled as follows:
<SC> <BT> <T> <T> <T>..<T> <ET> <BM> <M> <M> <M>...<EM>
SC=Speech c o n d i t i o n i n g e n c o d i n g
BT=Begin t e x t t o k e n
T= Text t o k e n s
ET=End t e x t t o k e n
BM=Begin MEL t o k e n
M =MEL t o k e n s
EM=End MEL t o k e n
Speech conditioning encodings are learned by a separate encoder that takes in the MEL spectrogram
of a related clip (another clip of the same person speaking) and produces a single vector embedding
that is placed at the front of the attention context. Two encodings were produced for each training
sample, which are averaged together. The maximum input length to the conditioning encoder is
132,300 samples, or 6 seconds of audio.
Learned positional embeddings are used. The MEL tokens and the text tokens get their own posi-
tional parameters. Text inputs are unpadded, MEL tokens are right padded to conform the sequence
length of each batch. The maximum sequence length is 402 text tokens + 604 MEL tokens. For
efficiency reasons, in the first half of training, the model only saw ¡6 second audio clips. After this,
audio clips up to the full length ( 27 seconds) were seen.
8Figure 3: Early training curves in log-log scale. Y-axis is cross entropy loss for MEL tokens. X-axis is number
of training steps. Does not include a long tail of training and fine-tuning due to online changes that were made
adding non-reproducible noise to curves.
Model architecture Transformer stack with causal masking
Layers 30
Model dim 1024
Attention heads 16
Text tokenization Custom BPE, 256 tokens wide.
Batch size 1024
Total training 119M samples
Text, next token prediction, loss weight .01
MEL token, next token prediction weight 1
LR 1e-4
B1, B2 .9 .96
Weight decay .01
LR Warmup 500 steps
EMA decay rate .999
Table 2: AR prior details & hyperparameters
After training the autoregressive decoder to convergence, I fine-tuned it on the clean audio datasets
from LibriTTS and HIFITTS.
9B.3 CLVP
The original DALLE worked by decoding a large number of images for a given text prompt, which
were then fed through CLIP. The image that CLIP deemed closest to the input text was used as the
final output.
I continue following this lead for TorToise, for reasons that will become evident in the results section.
I built a simple model that is very similar to CLIP, which I call a “Contrastive Language-V oice
Pretrained” model, or CLVP. Like CLIP, this model produces distance metrics for text/speech pairs.
CLVP uses an architecture similar to the CLIP text encoder, except it uses two of them: one for text
tokens and the other for MEL tokens. Tokens from both encoders were dropped out at a rate of 15%.
Fixed positional embeddings were used. Maximum text input length was 350 tokens (in practice
never actually seen). Maximum MEL token input length was 293, or 13 seconds of audio.
Figure 4: Late training curves for CLVP in log-log scale. Y-axis is cross entropy loss. X-axis is number of
samples. Early training curves were lost.
Model architecture Dual transformer stacks
Depth 20
Model dim 768
Attention heads 12
Text tokenization Custom BPE, 256-token wide
Batch size 1024
Total training 80M samples.
Losses Contrastive
LR 3e-4
B1, B2 .9 .96
Weight decay .001
LR Warmup 500 steps
EMA decay rate .999
Table 3: CLVP training details & hyperparameters
10B.4 Diffusion Decoder
The diffusion model uses a bespoke architecture that combines residual convolutions with dense
self-attention. It most closely resembles the traditional U-Net model used for DDPMs but without
any upsampling or downsampling.
The diffusion model receives 3 sources of conditioning: The timestep signal, which modulates the
scale and shift of the group norms used by the network. A speech conditioning signal, which also
modulates the scale and shift of the group norms. The final activations of the autoregressive model.
In training the diffusion model, I iterated through several different architectures and conditioning
types before settling on this one. This includes: Architecture: A “traditional” U-net with attention
was tried. The full attention network performed significantly better in frechet distance evaluations.
Operating on PCM data rather than MELs. This necessitated very small context windows and still
took an inordinate amount of time to train. The results of decoding a MEL and using a vocoder
resulted in substantially better quality. In order to force compatibility with existing diffusion noise
schedules, I rescale input MELs to be on the interval [-1,1]. Decoding MEL tokens versus AR
activations. Training on AR activations is expensive because during each training step you must
forward prop through the AR network. However, training on AR activations constituted the single
greatest jump in output quality of any design decision made for the diffusion network. It is possible
that doing tricks like putting the text on the attention context may ablate this advantage somewhat.
As with image diffusion models, exploiting classifier-free guidance is extremely important for high
quality outputs. In the case of TorToise, I perform guidance on both the speech conditioning signal
and the activations of the AR model. During training, 15% of the time, both of these signals are
dropped out and replaced with a learned embedding.
When training the diffusion decoder, input audio was clipped randomly to 220,500 samples, or 10
seconds of audio. Conditioning inputs were clipped to 102,400 samples, or 5 seconds of audio.
While the rest of the TorToise stack operates at an audio sampling rate of 22kHz, the diffusion
decoder outputs MEL spectrograms which were computed from 24kHz audio. This discrepancy is
solely to ensure compatibility with the pretrained Univnet vocoder which the model stack uses, and
was not done for any performance reasons.
Figure 5: Diffusion model losses, log-log scale. Y-axis: MSE loss, X-axis: training samples.
11Model shape Alternating full attention + conv resblocks
Depth 10
Model dim 1024
Attention heads 16
Batch size 512
Total Training 65M samples
Losses MSE (weight 1) + VLB (weight n)
LR 1e-5
B1, B2 .9, .999
Weight decay .001
LR Warmup 1000 steps
EMA decay rate .999
Table 4: Diffusion decoder details & hyperparameters
C Future Work
TorToise is the product of playing way over my paygrade, so to speak. As an independent researcher,
I only had a small number of GPUs to perform my experiments with, and made many mistakes in
the process. Following are recommendations for architectural tweaks to be made in future work
building off of TorToise:
1. Constrict VQV AE codebook embedding dim. This has been experimentally shown to pro-
duce drastic performance improvements.
2. Relative positional encodings. The AR model uses fixed positional encodings, which limits
the total amount of speech it can produce. Using relative encodings would allow arbitrary
length sequences.
3. Train CLVP on larger batch sizes. Contrastive models benefit from extremely large batch
sizes.
4. Train CLVP on longer audio sequences. CLVP only ever saw 13 second clips, which is
likely why re-ranking on longer samples suffers.
5. Diffusion decoder architecture. The diffusion decoder is an attentional network that omits
Feedforward blocks. In retrosepct, this was a poor design decision and feed-forward blocks
should be included.
6. Train the entire model stack at 24kHz or re-train Univnet at 22kHz sampling rates.
7. Train on more data for longer. The training curves for TorToise indicate that we were far
from overfitting. Simply training longer likely would have improved results.
D Special Thanks
More than the prior work done by the research community, this project was a product of the open
source community. I wanted to thank a few extra contributors who have not already been mentioned
above whose work I found instrumental in building TorToiSe:
1. Phil Wang, who authored Wang (2021) and Wang (2020).
2. Kim Seonghyeon, who authored Seonghyeon (2019).
3. FAIR, who maintain most of the tooling I use and who open sourced much of the technol-
ogy underpinning TorToiSe.
4. Prafulla Dhariwal and Alex Nichol, without whose Nichol and Dhariwal (2021) I would
still be in GAN hell.
I also want to thank my wife, Kim Betker, who supported me through two years of high electricity
bills, a hot & noisy utility room, and the many late nights required to build this system.
12Published as a conference paper at ICLR 2021
ANIMAGE IS WORTH 16X16 W ORDS :
TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE
Alexey Dosovitskiy;y, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,
Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby;y
equal technical contribution,yequal advising
Google Research, Brain Team
fadosovitskiy, neilhoulsby g@google.com
ABSTRACT
While the Transformer architecture has become the de-facto standard for natural
language processing tasks, its applications to computer vision remain limited. In
vision, attention is either applied in conjunction with convolutional networks, or
used to replace certain components of convolutional networks while keeping their
overall structure in place. We show that this reliance on CNNs is not necessary
and a pure transformer applied directly to sequences of image patches can perform
very well on image classiﬁcation tasks. When pre-trained on large amounts of
data and transferred to multiple mid-sized or small image recognition benchmarks
(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent
results compared to state-of-the-art convolutional networks while requiring sub-
stantially fewer computational resources to train.1
1 I NTRODUCTION
Self-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become
the model of choice in natural language processing (NLP). The dominant approach is to pre-train on
a large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks
to Transformers’ computational efﬁciency and scalability, it has become possible to train models of
unprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the
models and datasets growing, there is still no sign of saturating performance.
In computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989;
Krizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining
CNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing
the convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while
theoretically efﬁcient, have not yet been scaled effectively on modern hardware accelerators due to
the use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet-
like architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al.,
2020).
Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard
Transformer directly to images, with the fewest possible modiﬁcations. To do so, we split an image
into patches and provide the sequence of linear embeddings of these patches as an input to a Trans-
former. Image patches are treated the same way as tokens (words) in an NLP application. We train
the model on image classiﬁcation in supervised fashion.
When trained on mid-sized datasets such as ImageNet without strong regularization, these mod-
els yield modest accuracies of a few percentage points below ResNets of comparable size. This
seemingly discouraging outcome may be expected: Transformers lack some of the inductive biases
1Fine-tuning code and pre-trained models are available at https://github.com/
google-research/vision_transformer
1arXiv:2010.11929v2  [cs.CV]  3 Jun 2021Published as a conference paper at ICLR 2021
inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well
when trained on insufﬁcient amounts of data.
However, the picture changes if the models are trained on larger datasets (14M-300M images). We
ﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent
results when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When
pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches
or beats state of the art on multiple image recognition benchmarks. In particular, the best model
reaches the accuracy of 88:55% on ImageNet, 90:72% on ImageNet-ReaL, 94:55% on CIFAR-100,
and77:63% on the VTAB suite of 19 tasks.
2 R ELATED WORK
Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-
come the state of the art method in many NLP tasks. Large Transformer-based models are often
pre-trained on large corpora and then ﬁne-tuned for the task at hand: BERT (Devlin et al., 2019)
uses a denoising self-supervised pre-training task, while the GPT line of work uses language mod-
eling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020).
Naive application of self-attention to images would require that each pixel attends to every other
pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,
to apply Transformers in the context of image processing, several approximations have been tried in
the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query
pixel instead of globally. Such local multi-head dot-product self attention blocks can completely
replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different
line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-
attention in order to be applicable to images. An alternative way to scale attention is to apply it in
blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho
et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate
promising results on computer vision tasks, but require complex engineering to be implemented
efﬁciently on hardware accelerators.
Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 22
from the input image and applies full self-attention on top. This model is very similar to ViT,
but our work goes further to demonstrate that large scale pre-training makes vanilla transformers
competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020)
use a small patch size of 22pixels, which makes the model applicable only to small-resolution
images, while we handle medium-resolution images as well.
There has also been a lot of interest in combining convolutional neural networks (CNNs) with forms
of self-attention, e.g. by augmenting feature maps for image classiﬁcation (Bello et al., 2019) or by
further processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018;
Carion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classiﬁcation (Wu
et al., 2020), unsupervised object discovery (Locatello et al., 2020), or uniﬁed text-vision tasks (Chen
et al., 2020c; Lu et al., 2019; Li et al., 2019).
Another recent related model is image GPT (iGPT) (Chen et al., 2020a), which applies Transformers
to image pixels after reducing image resolution and color space. The model is trained in an unsu-
pervised fashion as a generative model, and the resulting representation can then be ﬁne-tuned or
probed linearly for classiﬁcation performance, achieving a maximal accuracy of 72% on ImageNet.
Our work adds to the increasing collection of papers that explore image recognition at larger scales
than the standard ImageNet dataset. The use of additional data sources allows to achieve state-of-
the-art results on standard benchmarks (Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020).
Moreover, Sun et al. (2017) study how CNN performance scales with dataset size, and Kolesnikov
et al. (2020); Djolonga et al. (2020) perform an empirical exploration of CNN transfer learning from
large scale datasets such as ImageNet-21k and JFT-300M. We focus on these two latter datasets as
well, but train Transformers instead of ResNet-based models used in prior works.
2Published as a conference paper at ICLR 2021
Transformer 
Encoder
MLP 
Head
Vision 
Transformer 
(ViT)*
Linear 
Projection 
of 
Flattened 
Patches
*
 
Extra 
learnable
     
[class]
 
embedding
1
2
3
4
5
6
7
8
90Patch 
+ 
Position 
Embedding
Class
Bird
Ball
Car
...
Embedded 
Patches
Multi-Head 
Attention
Norm
MLP
Norm
+
L
 
x
+Transformer 
Encoder
Figure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,
add position embeddings, and feed the resulting sequence of vectors to a standard Transformer
encoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable
“classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by
Vaswani et al. (2017).
3 M ETHOD
In model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.
An advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and
their efﬁcient implementations – can be used almost out of the box.
3.1 V ISION TRANSFORMER (VIT)
An overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D
sequence of token embeddings. To handle 2D images, we reshape the image x2RHWCinto a
sequence of ﬂattened 2D patches xp2RN(P2C), where (H;W )is the resolution of the original
image,Cis the number of channels, (P;P)is the resolution of each image patch, and N=HW=P2
is the resulting number of patches, which also serves as the effective input sequence length for the
Transformer. The Transformer uses constant latent vector size Dthrough all of its layers, so we
ﬂatten the patches and map to Ddimensions with a trainable linear projection (Eq. 1). We refer to
the output of this projection as the patch embeddings.
Similar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed-
ded patches ( z0
0=xclass), whose state at the output of the Transformer encoder ( z0
L) serves as the
image representation y(Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at-
tached to z0
L. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training
time and by a single linear layer at ﬁne-tuning time.
Position embeddings are added to the patch embeddings to retain positional information. We use
standard learnable 1D position embeddings, since we have not observed signiﬁcant performance
gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting
sequence of embedding vectors serves as input to the encoder.
The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-
attention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before
every block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).
3Published as a conference paper at ICLR 2021
The MLP contains two layers with a GELU non-linearity.
z0= [xclass;x1
pE;x2
pE;;xN
pE] +Epos;E2R(P2C)D;Epos2R(N+1)D(1)
z0
`= MSA(LN( z` 1)) +z` 1; ` = 1:::L (2)
z`= MLP(LN( z0
`)) +z0
`; ` = 1:::L (3)
y= LN( z0
L) (4)
Inductive bias. We note that Vision Transformer has much less image-speciﬁc inductive bias than
CNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are
baked into each layer throughout the whole model. In ViT, only MLP layers are local and transla-
tionally equivariant, while the self-attention layers are global. The two-dimensional neighborhood
structure is used very sparingly: in the beginning of the model by cutting the image into patches and
at ﬁne-tuning time for adjusting the position embeddings for images of different resolution (as de-
scribed below). Other than that, the position embeddings at initialization time carry no information
about the 2D positions of the patches and all spatial relations between the patches have to be learned
from scratch.
Hybrid Architecture. As an alternative to raw image patches, the input sequence can be formed
from feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding
projection E(Eq. 1) is applied to patches extracted from a CNN feature map. As a special case,
the patches can have spatial size 1x1, which means that the input sequence is obtained by simply
ﬂattening the spatial dimensions of the feature map and projecting to the Transformer dimension.
The classiﬁcation input embedding and position embeddings are added as described above.
3.2 F INE-TUNING AND HIGHER RESOLUTION
Typically, we pre-train ViT on large datasets, and ﬁne-tune to (smaller) downstream tasks. For
this, we remove the pre-trained prediction head and attach a zero-initialized DKfeedforward
layer, where Kis the number of downstream classes. It is often beneﬁcial to ﬁne-tune at higher
resolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020). When feeding images
of higher resolution, we keep the patch size the same, which results in a larger effective sequence
length. The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints),
however, the pre-trained position embeddings may no longer be meaningful. We therefore perform
2D interpolation of the pre-trained position embeddings, according to their location in the original
image. Note that this resolution adjustment and patch extraction are the only points at which an
inductive bias about the 2D structure of the images is manually injected into the Vision Transformer.
4 E XPERIMENTS
We evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the
hybrid. To understand the data requirements of each model, we pre-train on datasets of varying size
and evaluate many benchmark tasks. When considering the computational cost of pre-training the
model, ViT performs very favourably, attaining state of the art on most recognition benchmarks at
a lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show
that self-supervised ViT holds promise for the future.
4.1 S ETUP
Datasets. To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes
and 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with
21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and
303M high-resolution images. We de-duplicate the pre-training datasets w.r.t. the test sets of the
downstream tasks following Kolesnikov et al. (2020). We transfer the models trained on these
dataset to several benchmark tasks: ImageNet on the original validation labels and the cleaned-up
ReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al.,
2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008). For these datasets, pre-processing
follows Kolesnikov et al. (2020).
4Published as a conference paper at ICLR 2021
Model Layers Hidden size D MLP size Heads Params
ViT-Base 12 768 3072 12 86M
ViT-Large 24 1024 4096 16 307M
ViT-Huge 32 1280 5120 16 632M
Table 1: Details of Vision Transformer model variants.
We also evaluate on the 19-task VTAB classiﬁcation suite (Zhai et al., 2019b). VTAB evaluates
low-data transfer to diverse tasks, using 1 000 training examples per task. The tasks are divided into
three groups: Natural – tasks like the above, Pets, CIFAR, etc. Specialized – medical and satellite
imagery, and Structured – tasks that require geometric understanding like localization.
Model Variants. We base ViT conﬁgurations on those used for BERT (Devlin et al., 2019), as
summarized in Table 1. The “Base” and “Large” models are directly adopted from BERT and we
add the larger “Huge” model. In what follows we use brief notation to indicate the model size and
the input patch size: for instance, ViT-L/16 means the “Large” variant with 1616input patch size.
Note that the Transformer’s sequence length is inversely proportional to the square of the patch size,
thus models with smaller patch size are computationally more expensive.
For the baseline CNNs, we use ResNet (He et al., 2016), but replace the Batch Normalization lay-
ers (Ioffe & Szegedy, 2015) with Group Normalization (Wu & He, 2018), and used standardized
convolutions (Qiao et al., 2019). These modiﬁcations improve transfer (Kolesnikov et al., 2020),
and we denote the modiﬁed model “ResNet (BiT)”. For the hybrids, we feed the intermediate fea-
ture maps into ViT with patch size of one “pixel”. To experiment with different sequence lengths,
we either (i) take the output of stage 4 of a regular ResNet50 or (ii) remove stage 4, place the same
number of layers in stage 3 (keeping the total number of layers), and take the output of this extended
stage 3. Option (ii) results in a 4x longer sequence length, and a more expensive ViT model.
Training & Fine-tuning. We train all models, including ResNets, using Adam (Kingma & Ba,
2015) with1= 0:9,2= 0:999, a batch size of 4096 and apply a high weight decay of 0:1, which
we found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common
practices, Adam works slightly better than SGD for ResNets in our setting). We use a linear learning
rate warmup and decay, see Appendix B.1 for details. For ﬁne-tuning we use SGD with momentum,
batch size 512, for all models, see Appendix B.1.1. For ImageNet results in Table 2, we ﬁne-tuned at
higher resolution: 512for ViT-L/16 and 518for ViT-H/14, and also used Polyak & Juditsky (1992)
averaging with a factor of 0:9999 (Ramachandran et al., 2019; Wang et al., 2020b).
Metrics. We report results on downstream datasets either through few-shot or ﬁne-tuning accuracy.
Fine-tuning accuracies capture the performance of each model after ﬁne-tuning it on the respective
dataset. Few-shot accuracies are obtained by solving a regularized least-squares regression problem
that maps the (frozen) representation of a subset of training images to f 1;1gKtarget vectors. This
formulation allows us to recover the exact solution in closed form. Though we mainly focus on
ﬁne-tuning performance, we sometimes use linear few-shot accuracies for fast on-the-ﬂy evaluation
where ﬁne-tuning would be too costly.
4.2 C OMPARISON TO STATE OF THE ART
We ﬁrst compare our largest models – ViT-H/14 and ViT-L/16 – to state-of-the-art CNNs from
the literature. The ﬁrst comparison point is Big Transfer (BiT) (Kolesnikov et al., 2020), which
performs supervised transfer learning with large ResNets. The second is Noisy Student (Xie et al.,
2020), which is a large EfﬁcientNet trained using semi-supervised learning on ImageNet and JFT-
300M with the labels removed. Currently, Noisy Student is the state of the art on ImageNet and
BiT-L on the other datasets reported here. All models were trained on TPUv3 hardware, and we
report the number of TPUv3-core-days taken to pre-train each of them, that is, the number of TPU
v3 cores (2 per chip) used for training multiplied by the training time in days.
Table 2 shows the results. The smaller ViT-L/16 model pre-trained on JFT-300M outperforms BiT-L
(which is pre-trained on the same dataset) on all tasks, while requiring substantially less computa-
tional resources to train. The larger model, ViT-H/14, further improves the performance, especially
on the more challenging datasets – ImageNet, CIFAR-100, and the VTAB suite. Interestingly, this
5Published as a conference paper at ICLR 2021
Ours-JFT Ours-JFT Ours-I21k BiT-L Noisy Student
(ViT-H/14) (ViT-L/16) (ViT-L/16) (ResNet152x4) (EfﬁcientNet-L2)
ImageNet 88:550:04 87:760:03 85:300:02 87:540:02 88:4=88:5
ImageNet ReaL 90:720:05 90:540:03 88:620:05 90:54 90 :55
CIFAR-10 99:500:06 99:420:03 99:150:03 99:370:06 
CIFAR-100 94:550:04 93:900:05 93:250:05 93:510:08 
Oxford-IIIT Pets 97:560:03 97:320:11 94:670:15 96:620:23 
Oxford Flowers-102 99:680:0299:740:0099:610:02 99:630:03 
VTAB (19 tasks) 77:630:23 76:280:46 72:720:21 76:291:70 
TPUv3-core-days 2:5k 0:68k 0:23k 9:9k 12:3k
Table 2: Comparison with state of the art on popular image classiﬁcation benchmarks. We re-
port mean and standard deviation of the accuracies, averaged over three ﬁne-tuning runs. Vision
Transformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on all
datasets, while taking substantially less computational resources to pre-train. ViT pre-trained on the
smaller public ImageNet-21k dataset performs well too.Slightly improved 88:5%result reported
in Touvron et al. (2020).
VTAB (19 tasks)65707580Accuracy [%]
Natural (7 tasks)708090
Specialized (4 tasks)8082858890
Structured (8 tasks)506070ViT-H/14 BiT-L (R152x4) VIVI-Ex-100% (R50x3) S4L (R50x1)
Figure 2: Breakdown of VTAB performance in Natural ,Specialized , and Structured task groups.
model still took substantially less compute to pre-train than prior state of the art. However, we note
that pre-training efﬁciency may be affected not only by the architecture choice, but also other pa-
rameters, such as training schedule, optimizer, weight decay, etc. We provide a controlled study of
performance vs. compute for different architectures in Section 4.4. Finally, the ViT-L/16 model
pre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking
fewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in ap-
proximately 30 days.
Figure 2 decomposes the VTAB tasks into their respective groups, and compares to previous SOTA
methods on this benchmark: BiT, VIVI – a ResNet co-trained on ImageNet and Youtube (Tschannen
et al., 2020), and S4L – supervised plus semi-supervised learning on ImageNet (Zhai et al., 2019a).
ViT-H/14 outperforms BiT-R152x4, and other methods, on the Natural andStructured tasks. On the
Specialized the performance of the top two models is similar.
4.3 P RE-TRAINING DATA REQUIREMENTS
The Vision Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer
inductive biases for vision than ResNets, how crucial is the dataset size? We perform two series of
experiments.
First, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-
300M. To boost the performance on the smaller datasets, we optimize three basic regularization
parameters – weight decay, dropout, and label smoothing. Figure 3 shows the results after ﬁne-
tuning to ImageNet (results on other datasets are shown in Table 5)2. When pre-trained on the
smallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite
(moderate) regularization. With ImageNet-21k pre-training, their performances are similar. Only
with JFT-300M, do we see the full beneﬁt of larger models. Figure 3 also shows the performance
2Note that the ImageNet pre-trained models are also ﬁne-tuned, but again on ImageNet. This is because the
resolution increase during ﬁne-tuning improves the performance.
6Published as a conference paper at ICLR 2021
ImageNet ImageNet-21k JFT-300M
Pre-training dataset7075808590ImageNet Top1 Accuracy [%]
BiT
ViT-B/32
ViT-B/16ViT-L/32
ViT-L/16
ViT-H/14
Figure 3: Transfer to ImageNet. While
large ViT models perform worse than BiT
ResNets (shaded area) when pre-trained on
small datasets, they shine when pre-trained on
larger datasets. Similarly, larger ViT variants
overtake smaller ones as the dataset grows.
10 M 30 M 100 M 300 M
Number of JFT pre-training samples3040506070Linear 5-shot ImageNet Top1 [%]
ViT-L/16
ViT-L/32ViT-B/32
ViT-b/32ResNet50x1 (BiT)
ResNet152x2 (BiT)Figure 4: Linear few-shot evaluation on Ima-
geNet versus pre-training size. ResNets per-
form better with smaller pre-training datasets
but plateau sooner than ViT, which performs
better with larger pre-training. ViT-b is ViT-B
with all hidden dimensions halved.
1021039095Transfer accuracy [%]Average-5
Transformer (ViT)
ResNet (BiT)
Hybrid
10210375808590ImageNet
Transformer (ViT)
ResNet (BiT)
Hybrid
Total pre-training compute [exaFLOPs]
Figure 5: Performance versus pre-training compute for different architectures: Vision Transformers,
ResNets, and hybrids. Vision Transformers generally outperform ResNets with the same compu-
tational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap
vanishes for larger models.
region spanned by BiT models of different sizes. The BiT CNNs outperform ViT on ImageNet, but
with the larger datasets, ViT overtakes.
Second, we train our models on random subsets of 9M, 30M, and 90M as well as the full JFT-
300M dataset. We do not perform additional regularization on the smaller subsets and use the same
hyper-parameters for all settings. This way, we assess the intrinsic model properties, and not the
effect of regularization. We do, however, use early-stopping, and report the best validation accuracy
achieved during training. To save compute, we report few-shot linear accuracy instead of full ﬁne-
tuning accuracy. Figure 4 contains the results. Vision Transformers overﬁt more than ResNets with
comparable computational cost on smaller datasets. For example, ViT-B/32 is slightly faster than
ResNet50; it performs much worse on the 9M subset, but better on 90M+ subsets. The same is true
for ResNet152x2 and ViT-L/16. This result reinforces the intuition that the convolutional inductive
bias is useful for smaller datasets, but for larger ones, learning the relevant patterns directly from
data is sufﬁcient, even beneﬁcial.
Overall, the few-shot results on ImageNet (Figure 4), as well as the low-data results on VTAB
(Table 2) seem promising for very low-data transfer. Further analysis of few-shot properties of ViT
is an exciting direction of future work.
7Published as a conference paper at ICLR 2021
4.4 S CALING STUDY
We perform a controlled scaling study of different models by evaluating transfer performance from
JFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess
performance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,
R50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained
for 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus
L/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre-
trained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the
end of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet
backbone).
Figure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5
for details on computational costs). Detailed results per model are provided in Table 6 in the Ap-
pendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the
performance/compute trade-off. ViT uses approximately 2 4less compute to attain the same
performance (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu-
tational budgets, but the difference vanishes for larger models. This result is somewhat surprising,
since one might expect convolutional local feature processing to assist ViT at any size. Third, Vision
Transformers appear not to saturate within the range tried, motivating future scaling efforts.
4.5 I NSPECTING VISION TRANSFORMER
Input
 Attention
Figure 6: Representative ex-
amples of attention from the
output token to the input
space. See Appendix D.7 for
details.To begin to understand how the Vision Transformer processes im-
age data, we analyze its internal representations. The ﬁrst layer of
the Vision Transformer linearly projects the ﬂattened patches into a
lower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin-
cipal components of the the learned embedding ﬁlters. The com-
ponents resemble plausible basis functions for a low-dimensional
representation of the ﬁne structure within each patch.
After the projection, a learned position embedding is added to the
patch representations. Figure 7 (center) shows that the model learns
to encode distance within the image in the similarity of position em-
beddings, i.e. closer patches tend to have more similar position em-
beddings. Further, the row-column structure appears; patches in the
same row/column have similar embeddings. Finally, a sinusoidal
structure is sometimes apparent for larger grids (Appendix D). That
the position embeddings learn to represent 2D image topology ex-
plains why hand-crafted 2D-aware embedding variants do not yield
improvements (Appendix D.4).
Self-attention allows ViT to integrate information across the entire
image even in the lowest layers. We investigate to what degree
the network makes use of this capability. Speciﬁcally, we compute
the average distance in image space across which information is
integrated, based on the attention weights (Figure 7, right). This
“attention distance” is analogous to receptive ﬁeld size in CNNs.
We ﬁnd that some heads attend to most of the image already in the lowest layers, showing that
the ability to integrate information globally is indeed used by the model. Other attention heads
have consistently small attention distances in the low layers. This highly localized attention is
less pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),
suggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the
attention distance increases with network depth. Globally, we ﬁnd that the model attends to image
regions that are semantically relevant for classiﬁcation (Figure 6).
4.6 S ELF-SUPERVISION
Transformers show impressive performance on NLP tasks. However, much of their success stems
not only from their excellent scalability but also from large scale self-supervised pre-training (Devlin
8Published as a conference paper at ICLR 2021
RGB embedding filters
(first 28 principal components)
1 2 3 4 5 6 7
Input patch column1
2
3
4
5
6
7Input patch rowPosition embedding similarity
1
1
Cosine similarity
0 5 10 15 20
Network depth (layer)020406080100120Mean attention distance (pixels)
ViT-L/16
Head 1
Head 2
Head 3
...
Figure 7: Left: Filters of the initial linear embedding of RGB values of ViT-L/32. Center: Sim-
ilarity of position embeddings of ViT-L/32. Tiles show the cosine similarity between the position
embedding of the patch with the indicated row and column and the position embeddings of all other
patches. Right: Size of attended area by head and network depth. Each dot shows the mean attention
distance across images for one of 16 heads at one layer. See Appendix D.7 for details.
et al., 2019; Radford et al., 2018). We also perform a preliminary exploration on masked patch
prediction for self-supervision, mimicking the masked language modeling task used in BERT. With
self-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a
signiﬁcant improvement of 2% to training from scratch, but still 4% behind supervised pre-training.
Appendix B.1.2 contains further details. We leave exploration of contrastive pre-training (Chen
et al., 2020b; He et al., 2020; Bachman et al., 2019; H ´enaff et al., 2020) to future work.
5 C ONCLUSION
We have explored the direct application of Transformers to image recognition. Unlike prior works
using self-attention in computer vision, we do not introduce image-speciﬁc inductive biases into
the architecture apart from the initial patch extraction step. Instead, we interpret an image as a
sequence of patches and process it by a standard Transformer encoder as used in NLP. This simple,
yet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.
Thus, Vision Transformer matches or exceeds the state of the art on many image classiﬁcation
datasets, whilst being relatively cheap to pre-train.
While these initial results are encouraging, many challenges remain. One is to apply ViT to other
computer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion
et al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-
supervised pre-training methods. Our initial experiments show improvement from self-supervised
pre-training, but there is still large gap between self-supervised and large-scale supervised pre-
training. Finally, further scaling of ViT would likely lead to improved performance.
ACKNOWLEDGEMENTS
The work was performed in Berlin, Z ¨urich, and Amsterdam. We thank many colleagues at Google
for their help, in particular Andreas Steiner for crucial help with the infrastructure and the open-
source release of the code; Joan Puigcerver and Maxim Neumann for help with the large-scale
training infrastructure; Dmitry Lepikhin, Aravindh Mahendran, Daniel Keysers, Mario Lu ˇci´c, Noam
Shazeer, Ashish Vaswani, and Colin Raffel for useful discussions.
REFERENCES
Samira Abnar and Willem Zuidema. Quantifying attention ﬂow in transformers. In ACL, 2020.
Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing
mutual information across views. In NeurIPS , 2019.
9Published as a conference paper at ICLR 2021
Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In
ICLR , 2019.
I. Bello, B. Zoph, Q. Le, A. Vaswani, and J. Shlens. Attention augmented convolutional networks.
InICCV , 2019.
Lucas Beyer, Olivier J. H ´enaff, Alexander Kolesnikov, Xiaohua Zhai, and A ¨aron van den Oord. Are
we done with imagenet? arXiv , 2020.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv , 2020.
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and
Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV , 2020.
Mark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun. Generative pretraining from
pixels. In ICML , 2020a.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework
for contrastive learning of visual representations. In ICML , 2020b.
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and
Jingjing Liu. UNITER: UNiversal Image-TExt Representation Learning. In ECCV , 2020c.
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv , 2019.
Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-
attention and convolutional layers. In ICLR , 2020.
J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In CVPR , 2009.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In NAACL , 2019.
Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander
Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, Sylvan
Gelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On robustness and transferability of convo-
lutional neural networks. arXiv , 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR , 2016.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In CVPR , 2020.
Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-
mensional transformers. arXiv , 2019.
Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object
detection. In CVPR , 2018.
Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition.
InICCV , 2019.
Zilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, and
Thomas S. Huang. Ccnet: Criss-cross attention for semantic segmentation. In ICCV , 2020.
Olivier J. H ´enaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami,
and Aaron van den Oord. Data-efﬁcient image recognition with contrastive predictive coding. In
ICML , 2020.
10Published as a conference paper at ICLR 2021
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. 2015.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,
and Neil Houlsby. Big transfer (BiT): General visual representation learning. In ECCV , 2020.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convo-
lutional neural networks. In NIPS , 2012.
Y . LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropa-
gation applied to handwritten zip code recognition. Neural Computation , 1:541–551, 1989.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional
computation and automatic sharding. arXiv , 2020.
Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBERT: A
Simple and Performant Baseline for Vision and Language. In Arxiv , 2019.
Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,
Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot atten-
tion. arXiv , 2020.
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visi-
olinguistic Representations for Vision-and-Language Tasks. In NeurIPS . 2019.
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,
Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised
pretraining. In ECCV , 2018.
M. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In
ICVGIP , 2008.
Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V . Jawahar. Cats and dogs. In CVPR ,
2012.
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and
Dustin Tran. Image transformer. In ICML , 2018.
B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM
Journal on Control and Optimization , 30(4):838–855, 1992. doi: 10.1137/0330046. URL
https://doi.org/10.1137/0330046 .
Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Weight standardization. arXiv
preprint arXiv:1903.10520 , 2019.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing with unsupervised learning. Technical Report , 2018.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. Technical Report , 2019.
Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.
Stand-alone self-attention in vision models. In NeurIPS , 2019.
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable ef-
fectiveness of data in deep learning era. In ICCV , 2017.
Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint
model for video and language representation learning. In ICCV , 2019.
11Published as a conference paper at ICLR 2021
Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution
discrepancy. In NeurIPS . 2019.
Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution
discrepancy: Fixefﬁcientnet. arXiv preprint arXiv:2003.08237 , 2020.
Michael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran, Neil Houlsby, Sylvain
Gelly, and Mario Lucic. Self-supervised learning of video-induced visual invariances. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June
2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS , 2017.
Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen.
Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV , 2020a.
Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh
Chen. Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. arXiv preprint
arXiv:2003.07853 , 2020b.
Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao.
Learning deep transformer models for machine translation. In ACL, 2019.
Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In
CVPR , 2018.
Dirk Weissenborn, Oscar T ¨ackstr ¨om, and Jakob Uszkoreit. Scaling autoregressive video models. In
ICLR , 2019.
Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt
Keutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing
for computer vision. arxiv , 2020.
Yuxin Wu and Kaiming He. Group normalization. In ECCV , 2018.
Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V . Le. Self-training with noisy student
improves imagenet classiﬁcation. In CVPR , 2020.
Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4L: Self-Supervised Semi-
Supervised Learning. In ICCV , 2019a.
Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario
Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A
large-scale study of representation learning with the visual task adaptation benchmark. arXiv
preprint arXiv:1910.04867 , 2019b.
Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In
CVPR , 2020.
12Published as a conference paper at ICLR 2021
Models Dataset Epochs Base LR LR decay Weight decay Dropout
ViT-B/f16,32g JFT-300M 7 810 4linear 0.1 0.0
ViT-L/32 JFT-300M 7 610 4linear 0.1 0.0
ViT-L/16 JFT-300M 7/14 410 4linear 0.1 0.0
ViT-H/14 JFT-300M 14 310 4linear 0.1 0.0
R50xf1,2g JFT-300M 7 10 3linear 0.1 0.0
R101x1 JFT-300M 7 810 4linear 0.1 0.0
R152xf1,2g JFT-300M 7 610 4linear 0.1 0.0
R50+ViT-B/f16,32gJFT-300M 7 810 4linear 0.1 0.0
R50+ViT-L/32 JFT-300M 7 210 4linear 0.1 0.0
R50+ViT-L/16 JFT-300M 7/14 410 4linear 0.1 0.0
ViT-B/f16,32g ImageNet-21k 90 10 3linear 0.03 0.1
ViT-L/f16,32g ImageNet-21k 30/90 10 3linear 0.03 0.1
ViT- ImageNet 300 310 3cosine 0.3 0.1
Table 3: Hyperparameters for training. All models are trained with a batch size of 4096 and learn-
ing rate warmup of 10k steps. For ImageNet we found it beneﬁcial to additionally apply gradient
clipping at global norm 1. Training resolution is 224.
APPENDIX
A M ULTIHEAD SELF-ATTENTION
Standard qkv self-attention (SA, Vaswani et al. (2017)) is a popular building block for neural archi-
tectures. For each element in an input sequence z2RND, we compute a weighted sum over all
values vin the sequence. The attention weights Aijare based on the pairwise similarity between
two elements of the sequence and their respective query qiand key kjrepresentations.
[q;k;v] =zUqkv Uqkv2RD3Dh; (5)
A= softmax
qk>=p
Dh
A2RNN; (6)
SA(z) =Av: (7)
Multihead self-attention (MSA) is an extension of SA in which we run kself-attention operations,
called “heads”, in parallel, and project their concatenated outputs. To keep compute and number of
parameters constant when changing k,Dh(Eq. 5) is typically set to D=k .
MSA( z) = [SA 1(z); SA 2(z);; SAk(z)]Umsa Umsa2RkDhD(8)
B E XPERIMENT DETAILS
B.1 T RAINING
Table 3 summarizes our training setups for our different models. We found strong regularization
to be key when training models from scratch on ImageNet. Dropout, when used, is applied after
every dense layer except for the the qkv-projections and directly after adding positional- to patch
embeddings. Hybrid models are trained with the exact setup as their ViT counterparts. Finally, all
training is done on resolution 224.
B.1.1 F INE-TUNING
We ﬁne-tune all ViT models using SGD with a momentum of 0.9. We run a small grid search over
learning rates, see learning rate ranges in Table 4. To do so, we use small sub-splits from the training
set (10% for Pets and Flowers, 2% for CIFAR, 1% ImageNet) as development set and train on the
remaining data. For ﬁnal results we train on the entire training set and evaluate on the respective
test data. For ﬁne-tuning ResNets and hybrid models we use the exact same setup, with the only
exception of ImageNet where we add another value 0:06to the learning rate sweep. Additionally,
13Published as a conference paper at ICLR 2021
Dataset Steps Base LR
ImageNet 20 000 f0.003, 0.01, 0.03, 0.06 g
CIFAR100 10 000 f0.001, 0.003, 0.01, 0.03 g
CIFAR10 10 000 f0.001, 0.003, 0.01, 0.03 g
Oxford-IIIT Pets 500 f0.001, 0.003, 0.01, 0.03 g
Oxford Flowers-102 500 f0.001, 0.003, 0.01, 0.03 g
VTAB (19 tasks) 2 500 0.01
Table 4: Hyperparameters for ﬁne-tuning. All models are ﬁne-tuned with cosine learning rate decay,
a batch size of 512, no weight decay, and grad clipping at global norm 1. If not mentioned otherwise,
ﬁne-tuning resolution is 384.
for ResNets we also run the setup of Kolesnikov et al. (2020) and select the best results across
this run and our sweep. Finally, if not mentioned otherwise, all ﬁne-tuning experiments run at 384
resolution (running ﬁne-tuning at different resolution than training is common practice (Kolesnikov
et al., 2020)).
When transferring ViT models to another dataset, we remove the whole head (two linear layers) and
replace it by a single, zero-initialized linear layer outputting the number of classes required by the
target dataset. We found this to be a little more robust than simply re-initializing the very last layer.
For VTAB we follow the protocol in Kolesnikov et al. (2020), and use the same hyperparameter
setting for all tasks. We use a learning rate of 0:01and train for 2500 steps (Tab. 4). We chose this
setting by running a small sweep over two learning rates and two schedules, and selecting the setting
with the highest VTAB score on the 200-example validation sets. We follow the pre-processing used
in Kolesnikov et al. (2020), except that we do not use task-speciﬁc input resolutions. Instead we ﬁnd
that Vision Transformer beneﬁts most from a high resolution ( 384384) for all tasks.
B.1.2 S ELF-SUPERVISION
We employ the masked patch prediction objective for preliminary self-supervision experiments. To
do so we corrupt 50% of patch embeddings by either replacing their embeddings with a learnable
[mask] embedding (80%), a random other patch embedding (10%) or just keeping them as is
(10%). This setup is very similar to the one used for language by Devlin et al. (2019). Finally, we
predict the 3-bit, mean color (i.e., 512 colors in total) of every corrupted patch using their respective
patch representations.
We trained our self-supervised model for 1M steps (ca. 14 epochs) with batch size 4096 on JFT. We
use Adam, with a base learning rate of 210 4, warmup of 10k steps and cosine learning rate decay.
As prediction targets for pretraining we tried the following settings: 1) predicting only the mean,
3bit color (i.e., 1 prediction of 512 colors), 2) predicting a 44downsized version of the 1616
patch with 3bit colors in parallel (i.e., 16 predictions of 512 colors), 3) regression on the full patch
using L2 (i.e., 256 regressions on the 3 RGB channels). Surprisingly, we found that all worked quite
well, though L2 was slightly worse. We report ﬁnal results only for option 1) because it has shown
best few-shot performance. We also experimented with 15% corruption rate as used by Devlin et al.
(2019) but results were also slightly worse on our few-shot metrics.
Lastly, we would like to remark that our instantiation of masked patch prediction doesn’t require
such an enormous amount of pretraining nor a large dataset such as JFT in order to lead to sim-
ilar performance gains on ImageNet classiﬁcation. That is, we observed diminishing returns on
downstream performance after 100k pretraining steps, and see similar gains when pretraining on
ImageNet.
C A DDITIONAL RESULTS
We report detailed results corresponding to the ﬁgures presented in the paper. Table 5 corresponds
to Figure 3 from the paper and shows transfer performance of different ViT models pre-trained
on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-300M. Table 6 corresponds to
14Published as a conference paper at ICLR 2021
ViT-B/16 ViT-B/32 ViT-L/16 ViT-L/32 ViT-H/14
ImageNet CIFAR-10 98.13 97.77 97.86 97.94 -
CIFAR-100 87.13 86.31 86.35 87.07 -
ImageNet 77.91 73.38 76.53 71.16 -
ImageNet ReaL 83.57 79.56 82.19 77.83 -
Oxford Flowers-102 89.49 85.43 89.66 86.36 -
Oxford-IIIT-Pets 93.81 92.04 93.64 91.35 -
ImageNet-21k CIFAR-10 98.95 98.79 99.16 99.13 99.27
CIFAR-100 91.67 91.97 93.44 93.04 93.82
ImageNet 83.97 81.28 85.15 80.99 85.13
ImageNet ReaL 88.35 86.63 88.40 85.65 88.70
Oxford Flowers-102 99.38 99.11 99.61 99.19 99.51
Oxford-IIIT-Pets 94.43 93.02 94.73 93.09 94.82
JFT-300M CIFAR-10 99.00 98.61 99.38 99.19 99.50
CIFAR-100 91.87 90.49 94.04 92.52 94.55
ImageNet 84.15 80.73 87.12 84.37 88.04
ImageNet ReaL 88.85 86.27 89.99 88.28 90.33
Oxford Flowers-102 99.56 99.27 99.56 99.45 99.68
Oxford-IIIT-Pets 95.80 93.40 97.11 95.83 97.56
Table 5: Top1 accuracy (in %) of Vision Transformer on various datasets when pre-trained on Im-
ageNet, ImageNet-21k or JFT300M. These values correspond to Figure 3 in the main text. Models
are ﬁne-tuned at 384 resolution. Note that the ImageNet results are computed without additional
techniques (Polyak averaging and 512 resolution images) used to achieve results in Table 2.
Epochs ImageNet ImageNet ReaL CIFAR-10 CIFAR-100 Pets Flowers exaFLOPs
name
ViT-B/32 7 80.73 86.27 98.61 90.49 93.40 99.27 55
ViT-B/16 7 84.15 88.85 99.00 91.87 95.80 99.56 224
ViT-L/32 7 84.37 88.28 99.19 92.52 95.83 99.45 196
ViT-L/16 7 86.30 89.43 99.38 93.46 96.81 99.66 783
ViT-L/16 14 87.12 89.99 99.38 94.04 97.11 99.56 1567
ViT-H/14 14 88.08 90.36 99.50 94.71 97.11 99.71 4262
ResNet50x1 7 77.54 84.56 97.67 86.07 91.11 94.26 50
ResNet50x2 7 82.12 87.94 98.29 89.20 93.43 97.02 199
ResNet101x1 7 80.67 87.07 98.48 89.17 94.08 95.95 96
ResNet152x1 7 81.88 87.96 98.82 90.22 94.17 96.94 141
ResNet152x2 7 84.97 89.69 99.06 92.05 95.37 98.62 563
ResNet152x2 14 85.56 89.89 99.24 91.92 95.75 98.75 1126
ResNet200x3 14 87.22 90.15 99.34 93.53 96.32 99.04 3306
R50x1+ViT-B/32 7 84.90 89.15 99.01 92.24 95.75 99.46 106
R50x1+ViT-B/16 7 85.58 89.65 99.14 92.63 96.65 99.40 274
R50x1+ViT-L/32 7 85.68 89.04 99.24 92.93 96.97 99.43 246
R50x1+ViT-L/16 7 86.60 89.72 99.18 93.64 97.03 99.40 859
R50x1+ViT-L/16 14 87.12 89.76 99.31 93.89 97.36 99.11 1668
Table 6: Detailed results of model scaling experiments. These correspond to Figure 5 in the main
paper. We show transfer accuracy on several datasets, as well as the pre-training compute (in ex-
aFLOPs).
Figure 5 from the paper and shows the transfer performance of ViT, ResNet, and hybrid models of
varying size, as well as the estimated computational cost of their pre-training.
D A DDITIONAL ANALYSES
D.1 SGD VS. ADAM FOR RESNETS
ResNets are typically trained with SGD and our use of Adam as optimizer is quite unconventional.
Here we show the experiments that motivated this choice. Namely, we compare the ﬁne-tuning
15Published as a conference paper at ICLR 2021
ResNet50 ResNet152x2
Dataset Adam SGD Adam SGD
ImageNet 77:54 78 :24 84 :97 84 :37
CIFAR10 97:67 97 :46 99 :06 99 :07
CIFAR100 86:07 85 :17 92 :05 91 :06
Oxford-IIIT Pets 91:11 91 :00 95 :37 94 :79
Oxford Flowers-102 94:26 92 :06 98 :62 99 :32
Average 89:33 88 :79 94 :01 93 :72
Table 7: Fine-tuning ResNet models pre-trained with Adam and SGD.
100101
Relative Compute0.20.30.40.50.6ImageNet 5shot
Models
All
Depth
Patch size
Width MLP
Width
100101
Relative Compute0.40.50.60.70.8Average 5shot
Models
All
Depth
Patch size
Width MLP
Width
Figure 8: Scaling different model dimensions of the Vision Transformer.
performance of two ResNets – 50x1 and 152x2 – pre-trained on JFT with SGD and Adam. For
SGD, we use the hyperparameters recommended by Kolesnikov et al. (2020). Results are presented
in Table 7. Adam pre-training outperforms SGD pre-training on most datasets and on average.
This justiﬁes the choice of Adam as the optimizer used to pre-train ResNets on JFT. Note that the
absolute numbers are lower than those reported by Kolesnikov et al. (2020), since we pre-train only
for7epochs, not 30.
D.2 T RANSFORMER SHAPE
We ran ablations on scaling different dimensions of the Transformer architecture to ﬁnd out which
are best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet
for different conﬁgurations. All conﬁgurations are based on a ViT model with 8layers,D= 1024 ,
DMLP = 2048 and a patch size of 32, the intersection of all lines. We can see that scaling the
depth results in the biggest improvements which are clearly visible up until 64 layers. However,
diminishing returns are already visible after 16 layers. Interestingly, scaling the width of the net-
work seems to result in the smallest changes. Decreasing the patch size and thus increasing the
effective sequence length shows surprisingly robust improvements without introducing parameters.
These ﬁndings suggest that compute might be a better predictor of performance than the number of
parameters, and that scaling should emphasize depth over width if any. Overall, we ﬁnd that scaling
all dimensions proportionally results in robust improvements.
D.3 H EAD TYPE AND C L A S S TOKEN
In order to stay as close as possible to the original Transformer model, we made use of an additional
[class] token, which is taken as image representation. The output of this token is then trans-
formed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity
in the single hidden layer.
This design is inherited from the Transformer model for text, and we use it throughout the main
paper. An initial attempt at using only image-patch embeddings, globally average-pooling (GAP)
them, followed by a linear classiﬁer—just like ResNet’s ﬁnal feature map—performed very poorly.
However, we found that this is neither due to the extra token, nor to the GAP operation. Instead,
16Published as a conference paper at ICLR 2021
0 1 2 3 4 5 6 7
Epochs of training2530354045505560ImageNet linear 5-shot accuracy [%]CLS-Token, lr=8e-4
GAP, lr=8e-4
GAP, lr=3e-4
Figure 9: Comparison of class-token and global average pooling classiﬁers. Both work similarly
well, but require different learning-rates.
Pos. Emb. Default/Stem Every Layer Every Layer-Shared
No Pos. Emb. 0.61382 N/A N/A
1-D Pos. Emb. 0.64206 0.63964 0.64292
2-D Pos. Emb. 0.64001 0.64046 0.64022
Rel. Pos. Emb. 0.64032 N/A N/A
Table 8: Results of the ablation study on positional embeddings with ViT-B/16 model evaluated on
ImageNet 5-shot linear.
the difference in performance is fully explained by the requirement for a different learning-rate, see
Figure 9.
D.4 P OSITIONAL EMBEDDING
We ran ablations on different ways of encoding spatial information using positional embedding. We
tried the following cases:
• Providing no positional information: Considering the inputs as a bag of patches .
• 1-dimensional positional embedding: Considering the inputs as a sequence of patches in
the raster order (default across all other experiments in this paper).
• 2-dimensional positional embedding: Considering the inputs as a grid of patches in two
dimensions. In this case, two sets of embeddings are learned, each for one of the axes,
X-embedding, and Y-embedding, each with size D=2. Then, based on the coordinate on
the path in the input, we concatenate the XandYembedding to get the ﬁnal positional
embedding for that patch.
• Relative positional embeddings: Considering the relative distance between patches to en-
code the spatial information as instead of their absolute position. To do so, we use 1-
dimensional Relative Attention, in which we deﬁne the relative distance all possible pairs
of patches. Thus, for every given pair (one as query, and the other as key/value in the at-
tention mechanism), we have an offset pq pk, where each offset is associated with an
embedding. Then, we simply run extra attention, where we use the original query (the
content of query), but use relative positional embeddings as keys. We then use the log-
its from the relative attention as a bias term and add it to the logits of the main attention
(content-based attention) before applying the softmax.
In addition to different ways of encoding spatial information, we also tried different ways of in-
corporating this information in our model. For the 1-dimensional and 2-dimensional positional
embeddings, we tried three different cases: (1) add positional embeddings to the inputs right after
17Published as a conference paper at ICLR 2021
1234567891011121314
Input patch column1
2
3
4
5
6
7
8
9
10
11
12
13
14Input patch rowViT-L16
7 epochs, LR=0.0002, WD=0.01
1
1
Cosine similarity
1234567891011121314
Input patch column1
2
3
4
5
6
7
8
9
10
11
12
13
14Input patch rowViT-L16
7 epochs, LR=0.0004, WD=0.1
1
1
Cosine similarity
1234567891011121314
Input patch column1
2
3
4
5
6
7
8
9
10
11
12
13
14Input patch rowViT-L16
14 epochs, LR=0.0004, WD=0.1
1
1
Cosine similarity
Figure 10: Position embeddings of models trained with different hyperparameters.
the stem of them model and before feeding the inputs to the Transformer encoder (default across
all other experiments in this paper); (2) learn and add positional embeddings to the inputs at the
beginning of each layer; (3) add a learned positional embeddings to the inputs at the beginning of
each layer (shared between layers).
Table 8 summarizes the results from this ablation study on a ViT-B/16 model. As we can see, while
there is a large gap between the performances of the model with no positional embedding and mod-
els with positional embedding, there is little to no difference between different ways of encoding
positional information. We speculate that since our Transformer encoder operates on patch-level
inputs, as opposed to pixel-level, the differences in how to encode spatial information is less impor-
tant. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original
pixel-level inputs, e.g., 1414as opposed to 224224, and learning to represent the spatial re-
lations in this resolution is equally easy for these different positional encoding strategies. Even so,
the speciﬁc pattern of position embedding similarity learned by the network depends on the training
hyperparameters (Figure 10).
0 5 10 15 20
Network depth (layer)020406080100120Mean attention distance (pixels)
ViT-L/16
Head 1
Head 2
Head 3
...
0 5 10 15 20
Network depth (layer)020406080100120
R50x1 + ViT-L/16
Head 1
Head 2
Head 3
...
Figure 11: Size of attended area by head and network depth. Attention distance was computed for
128 example images by averaging the distance between the query pixel and all other pixels, weighted
by the attention weight. Each dot shows the mean attention distance across images for one of 16
heads at one layer. Image width is 224 pixels.
D.5 E MPIRICAL COMPUTATIONAL COSTS
We are also interested in real-world speed of the architectures on our hardware, which is not always
well predicted by theoretical FLOPs due to details like lane widths and cache sizes. For this purpose,
18Published as a conference paper at ICLR 2021
we perform timing of inference speed for the main models of interest, on a TPUv3 accelerator; the
difference between inference and backprop speed is a constant model-independent factor.
Figure 12 (left) shows how many images one core can handle per second, across various input sizes.
Every single point refers to the peak performance measured across a wide range of batch-sizes. As
can be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening
for the largest models at the largest resolutions.
Another quantity of interest is the largest batch-size each model can ﬁt onto a core, larger being
better for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models.
This shows that large ViT models have a clear advantage in terms of memory-efﬁciency over ResNet
models.
64 128 224 384 512
Input size [px]102103104Peak inference speed [img/sec/core]64 128 224 384 512
Input size [px]102103Largest per-core batch-sizeR50x1
R50x2ViT-B/32
ViT-L/32ViT-B/16
ViT-L/16ViT-H/14
R152x4
Figure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models
have speed comparable to similar ResNets. Right : Largest per-core batch-size ﬁtting on device with
various architectures across input sizes. ViT models are clearly more memory-efﬁcient.
D.6 A XIAL ATTENTION
Axial Attention (Huang et al., 2020; Ho et al., 2019) is a simple, yet effective technique to run self-
attention on large inputs that are organized as multidimensional tensors. The general idea of axial
attention is to perform multiple attention operations, each along a single axis of the input tensor,
instead of applying 1-dimensional attention to the ﬂattened version of the input. In axial attention,
each attention mixes information along a particular axis, while keeping information along the other
axes independent. Along this line, Wang et al. (2020b) proposed the AxialResNet model in which
all the convolutions with kernel size 33in a ResNet50 are replaced by axial self-attention, i.e.
a row and column attention, augmented by relative positional encoding. We have implemented
AxialResNet as a baseline model.3.
Moreover, we have modiﬁed ViT to process inputs in the 2-dimensional shape, instead of a 1-
dimensional sequence of patches, and incorporate Axial Transformer blocks, in which instead of
a self-attention followed by an MLP, we have a a row-self-attention plus an MLP followed by a
column-self-attention plus an MLP.
Figure 13, present the performance of Axial ResNet, Axial-ViT-B/32 and Axial-ViT-B/16 on Ima-
geNet 5shot linear, when pretrained on JFT dataset, verses the pretraining compute, both in terms of
number of FLOPs and inference time (example per seconds). As we can see, both Axial-ViT-B/32
and Axial-ViT-B/16 do better than their ViT-B counterpart in terms of performance, but it comes at
3Our implementation is based on the open-sourced PyTorch implementation in https://github.com/
csrhddlam/axial-deeplab . In our experiments, we reproduced the scores reported in (Wang et al.,
2020b) in terms of accuracy, however, our implementation, similar to the open-source implementation, is very
slow on TPUs. Therefore, we were not able to use it for extensive large-scale experiments. These may be
unlocked by a carefully optimized implementation.
19Published as a conference paper at ICLR 2021
102
Total compute [exaFLOPs]0.5000.5250.5500.5750.6000.6250.650ImageNet 5-shot linear top-1 accuracyAxialViT-B/16
AxialViT-B/32ViT-B/16
ViT-B/32
ResNet50AxialResNet50
102103
Peak inference speed [img/sec/core]0.5000.5250.5500.5750.6000.6250.650ImageNet 5-shot linear top-1 accuracyAxialViT-B/16
AxialViT-B/32ViT-B/16
ViT-B/32
ResNet50AxialResNet50
Figure 13: Performance of Axial-Attention based models, in terms of top-1 accuracy on ImageNet
5-shot linear, versus their speed in terms of number of FLOPs ( left) and inference time ( left).
the cost of more compute. This is because in Axial-ViT models, each Transformer block with global
self-attention is replaced by two Axial Transformer blocks, one with row and one with column self-
attention and although the sequence length that self-attention operates on is smaller in axial case,
there is a extra MLP per Axial-ViT block. For the AxialResNet, although it looks reasonable in
terms of accuracy/compute trade-off (Figure 13, left), the naive implementation is extremely slow
on TPUs (Figure 13, right).
D.7 A TTENTION DISTANCE
To understand how ViT uses self-attention to integrate information across the image, we analyzed
the average distance spanned by attention weights at different layers (Figure 11). This “attention
distance” is analogous to receptive ﬁeld size in CNNs. Average attention distance is highly variable
across heads in lower layers, with some heads attending to much of the image, while others attend
to small regions at or near the query location. As depth increases, attention distance increases for all
heads. In the second half of the network, most heads attend widely across tokens.
D.8 A TTENTION MAPS
To compute maps of the attention from the output token to the input space (Figures 6 and 14), we
used Attention Rollout (Abnar & Zuidema, 2020). Brieﬂy, we averaged attention weights of ViT-
L/16 across all heads and then recursively multiplied the weight matrices of all layers. This accounts
for the mixing of attention across tokens through all layers.
D.9 O BJECT NETRESULTS
We also evaluate our ﬂagship ViT-H/14 model on the ObjectNet benchmark following the evaluation
setup in Kolesnikov et al. (2020), resulting in 82.1% top-5 accuracy and 61.7% top-1 accuracy.
D.10 VTAB B REAKDOWN
Table 9 shows the scores attained on each of the VTAB-1k tasks.
20Published as a conference paper at ICLR 2021
1
 2
 3
 4
 5
 6
 7
 8
9
 10
 11
 12
 13
 14
 15
 16
17
 18
 19
 20
 21
 22
 23
 24
25
 26
 27
 28
 29
 30
 31
 32
33
 34
 35
 36
 37
 38
 39
 40
41
 42
 43
 44
 45
 46
 47
 48
49
 50
 51
 52
 53
 54
 55
 56
57
 58
 59
 60
 61
 62
 63
 64
65
 66
 67
 68
 69
 70
 71
 72
73
 74
 75
 76
 77
 78
 79
 80
81
 82
 83
 84
 85
 86
 87
 88
89
 90
 91
 92
 93
 94
 95
 96
97
 98
 99
 100
 101
 102
 103
 104
105
 106
 107
 108
 109
 110
 111
 112
113
 114
 115
 116
 117
 118
 119
 120
121
 122
 123
 124
 125
 126
 127
 128
Figure 14: Further example attention maps as in Figure 6 (random selection).
21Published as a conference paper at ICLR 2021
Table 9: Breakdown of VTAB-1k performance across tasks.Caltech101
CIFAR-100
DTD
Flowers102
Pets
Sun397
SVHN
Camelyon
EuroSAT
Resisc45
Retinopathy
Clevr-Count
Clevr-Dist
DMLab
dSpr-Loc
dSpr-Ori
KITTI-Dist
sNORB-Azim
sNORB-Elev
Mean
ViT-H/14 (JFT) 95.3 85.5 75.2 99.7 97.2 65.0 88.9 83.3 96.7 91.4 76.6 91.7 63.8 53.1 79.4 63.3 84.5 33.2 51.2 77.6
ViT-L/16 (JFT) 95.4 81.9 74.3 99.7 96.7 63.5 87.4 83.6 96.5 89.7 77.1 86.4 63.1 49.7 74.5 60.5 82.2 36.2 51.1 76.3
ViT-L/16 (I21k) 90.8 84.1 74.1 99.3 92.7 61.0 80.9 82.5 95.6 85.2 75.3 70.3 56.1 41.9 74.7 64.9 79.9 30.5 41.7 72.7
22EFFICIENTLY TRAINABLE TEXT-TO-SPEECH SYSTEM BASED ON
DEEP CONVOLUTIONAL NETWORKS WITH GUIDED ATTENTION
Hideyuki Tachibana, Katsuya Uenoyama
PKSHA Technology, Inc.
Bunkyo, Tokyo, Japan
h_tachibana@pkshatech.comShunsuke Aihara
Independent Researcher
Shinjuku, Tokyo, Japan
aihara@argmax.jpc2018 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works.
ABSTRACT
This paper describes a novel text-to-speech (TTS) technique based
on deep convolutional neural networks (CNN), without use of any
recurrent units. Recurrent neural networks (RNN) have become a
standard technique to model sequential data recently, and this tech-
nique has been used in some cutting-edge neural TTS techniques.
However, training RNN components often requires a very power-
ful computer, or a very long time, typically several days or weeks.
Recent other studies, on the other hand, have shown that CNN-
based sequence synthesis can be much faster than RNN-based tech-
niques, because of high parallelizability. The objective of this paper
is to show that an alternative neural TTS based only on CNN alle-
viate these economic costs of training. In our experiment, the pro-
posed Deep Convolutional TTS was sufﬁciently trained overnight
(15 hours), using an ordinary gaming PC equipped with two GPUs,
while the quality of the synthesized speech was almost acceptable.
Index Terms —Text-to-speech, deep learning, convolutional
neural network, attention, sequence-to-sequence learning.
1. INTRODUCTION
Text-to-speech (TTS) is getting more and more common recently,
and is getting to be a basic user interface for many systems. To
further promote the use of TTS in various systems, it is signiﬁcant to
develop a manageable, maintainable, and extensible TTS component
that is accessible to speech non-specialists, enterprising individuals
and small teams.
Traditional TTS systems, however, are not necessarily friendly
for them, since they are typically composed of many domain-speciﬁc
modules. For example, a typical parametric TTS system is an elab-
orate integration of many modules e.g. a text analyzer, an F0gen-
erator, a spectrum generator, a pause estimator, and a vocoder that
synthesize a waveform from these data, etc.
Deep learning [1] may integrate these internal building blocks
into a single model, and connects the input and the output directly.
This type of technique is sometimes called ‘end-to-end’ learning.
Although such a technique is sometimes criticized as ‘a black box,’
an end-to-end TTS system named Tacotron [2], which directly esti-
mates a spectrogram from an input text, has achieved promising per-
formance recently, without using hand-engineered parametric mod-
els based on domain-speciﬁc knowledge. Tacotron, however, has a
drawback that it uses many recurrent units which are quite costly to
train. It is almost infeasible for ordinary labs who do not have lux-
urious machines to study and extend it further. In fact, some people
tried to implement open clones of Tacotron [3, 4, 5, 6], but they arestruggling to reproduce the speech of satisfactory quality as clear as
the original results.
The purpose of this paper is to show Deep Convolutional TTS
(DCTTS), a novel fully convolutional neural TTS. The architecture
is largely similar to Tacotron [2], but is based on a fully convolu-
tional sequence-to-sequence learning model similar to the literature
[7]. We show that this fully convolutional TTS actually works in a
reasonable setting. The contribution of this article is twofold: (1)
Propose a fully CNN-based TTS system which can be trained much
faster than an RNN-based state-of-the-art neural TTS system, while
the sound quality is still acceptable. (2) An idea to rapidly train the
attention module, which we call ‘guided attention,’ is also shown.
1.1. Related Work
1.1.1. Deep Learning and TTS
Recently, deep learning-based TTS systems have been intensively
studied, and surprisingly high quality results are obtained in some
of recent studies. The TTS systems based on deep neural networks
include Zen’s work [8], the studies based on RNN e.g. [9, 10, 11,
12], and recently proposed techniques e.g. WaveNet [13, sec. 3.2],
Char2Wav [14], DeepV oice1&2 [15, 16], and Tacotron [2].
Some of them tried to reduce the dependency on hand-engineered
internal modules. The most extreme technique in this trend would
be Tacotron [2], which depends only on mel and linear spectro-
grams, and not on any other speech features e.g. F0. Our method is
close to Tacotron in a sense that it depends only on these spectral
representations of audio signals.
Most of the existing methods above use RNN, a natural tech-
nique of time series prediction. An exception is WaveNet, which
is fully convolutional. Our method is also based only on CNN but
our usage of CNN would be different from WaveNet, as WaveNet is
a kind of a vocoder, or a back-end, which synthesizes a waveform
from some conditioning information that is given by front-end com-
ponents. On the other hand, ours is rather a front-end (and most of
back-end processing). We use CNN to synthesize a spectrogram,
from which a simple vocoder can synthesize a waveform.
1.1.2. Sequence to Sequence (seq2seq )Learning
Recently, recurrent neural networks (RNN) have become a standard
technique for mapping a sequence to another sequence, especially
in the ﬁeld of natural language processing, e.g. machine transla-
tion [17, 18], dialogue system [19, 20], etc. See also [1, sec. 10.4].
RNN-based seq2seq, however, has some disadvantages. Firstly,
a vanilla encode-decoder model cannot encode too long sequence
into a ﬁxed-length vector effectively. This problem has been re-
solved by a mechanism called ‘attention’ [21], and the attention
Published as a conference paper at ICASSP 2018, Calgary, Canada cIEEE 2018.arXiv:1710.08969v2  [cs.SD]  30 Sep 2020mechanism now has become a standard idea of seq2seq learning
techniques; see also [1, sec. 12.4.5.1].
Another problem is that RNN typically requires much time to
train, since it is less suited for parallel computation using GPUs. To
overcome this problem, several researchers proposed the use of CNN
instead of RNN, e.g. [22, 23, 24, 25, 26]. Some studies have shown
that CNN-based alternative networks can be trained much faster, and
sometimes can even outperform the RNN-based techniques.
Gehring et al. [7] recently united these two improvements of
seq2seq learning. They proposed an idea on how to use attention
mechanism in a CNN-based seq2seq learning model, and showed
that the method is quite effective for machine translation. The
method we proposed is based on the similar idea to the literature [7].
2. PRELIMINARY
2.1. Basic Knowledge of the Audio Spectrograms
An audio waveform and a complex spectrogram Z=fZftg 2
CF0T0are mutually transformed by linear maps called Short-term
Fourier Transform (STFT) and inverse STFT, where F0andT0de-
note the number of frequency bins and temporal bins, respectively.
It is common to consider only the magnitude jZj=fjZftjg 2
RF0T0, since it still has useful information for many purposes,
and thatjZjandZare almost the same in the sense that there ex-
ist many phase estimation (i.e., jZjtoZestimation, and therefore,
waveform synthesis) techniques, e.g. the famous Grifﬁn&Lim algo-
rithm (GLA) [27]; see also e.g. [28]. We always use RTISI-LA [29],
an online GLA, to synthesize a waveform. In this paper, we al-
ways normalize STFT spectrograms as jZj (jZj=max(jZj)),
and convert backjZj jZj=when we ﬁnally need to synthesize
the waveform, where ;are pre- and post-emphasis factors.
It is also common to consider a mel spectrogram S2RFT0,
(FF0), obtained by applying a mel ﬁlter-bank to jZj. This is
a standard dimension reduction technique for speech processing. In
this paper, we also reduce the temporal dimensionality from T0to
dT0=4e=:Tby picking up a time frame every four time frames, to
accelerate the training of Text2Mel shown below. We also normalize
mel spectrograms as S (S=max(S)).
2.2. Notation: Convolution and Highway Activation
In this paper, we denote the 1D convolution layer [30] by a space sav-
ing notation Co i
k?(X), whereiis the size of input channel, ois the
size of output channel, kis the size of kernel, is the dilation factor,
and an argument Xis a tensor having three dimensions ( batch, chan-
nel, temporal ). The stride of convolution is always 1. Convolution
layers are preceded by appropriately-sized zero padding, whose size
is suitably determined by a simple arithmetic so that the length of
the sequence is kept constant. Let us similarly denote the 1D decon-
volution layer as Do i
k?(X). The stride of deconvolution is always 2
in this paper. Let us write a layer composition operator as /, and
let us write networks like F/ReLU/G(X) := F(ReLU (G(X)));
and(F/G)2(X) :=F/G/F/G(X), etc. ReLU is an element-wise
activation function deﬁned by ReLU (x) = max(x;0).
Convolution layers are sometimes followed by a Highway Net-
work [31]-like gated activation, which is advantageous in very deep
networks: Highway (X;L) =(H1)ReLU (H2)+(1 (H1))
X;whereH1;H2are tensors of the same shape as X, and are output
as[H1;H2] =L(X)by a layer L. The operatoris the element-
wise product, and is the element-wise sigmoid function. Hereafter
let us denote HCd d
k?(X) :=Highway (X;C2d d
k? ).
Fig. 1 . Network architecture.
TextEnc (L) := ( HC2d 2d
1?1)2/(HC2d 2d
3?1)2/(HC2d 2d
3?27/HC2d 2d
3?9/
HC2d 2d
3?3/HC2d 2d
3?1)2/C2d 2d
1?1/ReLU/C2d e
1?1/CharEmbede-dim(L):
AudioEnc (S) := ( HCd d
3?3)2/(HCd d
3?27/HCd d
3?9/HCd d
3?3/HCd d
3?1)2/
Cd d
1?1/ReLU/Cd d
1?1/ReLU/Cd F
1?1(S):
AudioDec (R0) :=/CF d
1?1/(ReLU/Cd d
1?1)3/(HCd d
3?1)2/(HCd d
3?27/
HCd d
3?9/HCd d
3?3/HCd d
3?1)/Cd 2d
1?1(R0):
SSRN (Y) :=/CF0 F0
1?1/(ReLU/CF0 F0
1?1)2/CF0 2c
1?1/(HC2c 2c
3?1)2/
C2c c
1?1/(HCc c
3?3/HCc c
3?1/Dc c
2?1)2/(HCc c
3?3/HCc c
3?1)/Cc F
1?1(Y):
Fig. 2 . Details of each component. For notation, see section 2.2.
3. PROPOSED NETWORK
Since some literature [2, 32] suggest that the staged synthesis from
low- to high-resolution has advantages over the direct synthesis of
high-resolution data, we synthesize the spectrograms using the fol-
lowing two networks. (1) Text2Mel, which synthesizes a mel spec-
trogram from an input text, and (2) Spectrogram Super-resolution
Network (SSRN), which synthesizes a full STFT spectrogram from
a coarse mel spectrogram. Fig. 1 shows the overall architecture.
3.1. Text2Mel: Text to Mel Spectrogram Network
We ﬁrst consider to synthesize a coarse mel spectrogram from a text.
This is the main part of the proposed method. This module consists
of four submodules: Text Encoder, Audio Encoder, Attention, and
Audio Decoder. The network TextEnc ﬁrst encodes the input sen-
tenceL= [l1;:::;lN]2CharNconsisting of Ncharacters, into the
two matrices K(key),V(value)2RdN, wheredthe dimension of
encoded characters. On the other hand, the network AudioEnc en-
codes the coarse mel spectrogram S=S1:F;1:T2RFT, of speech
of lengthT, into a matrix Q(query)2RdT.
(K;V ) =TextEnc (L); Q=AudioEnc (S1:F;1:T): (1)
An attention matrix A2RNT, deﬁned as follows, evaluates how
strongly the n-th character lnand thet-th mel spectrum S1:F;tare
related.
A=softmaxn-axis(KTQ=p
d): (2)
Ant1implies that the module is focusing on n-th character ln
at timet, and it will focus on lnorln+1(or others nearby), at the
subsequent time t+ 1. Whatever, let us expect those are encoded in
n-th column of V. Thus a matrix R2RdT, which is the ‘seed’ of
the subsequent mel spectra S1:F;2:T+1, is obtained as
R=Att(Q;K;V ) :=VA: (Note: matrix product.) (3)
2The resultant Ris concatenated with the encoded audio Q, asR0=
[R;Q], because we found it beneﬁcial in our pilot study. Then, the
Audio Decoder module estimates a mel spectrogram from the seed
matrixR02R2dTas follows,
Y1:F;2:T+1=AudioDec (R0): (4)
The resultant Y1:F;2:T+1needs to approximate the temporally-
shifted ground truth S1:F;2:T+1. The error is evaluated by a loss
functionLspec(Y1:F;2:T+1jS1:F;2:T+1), and is back-propagated to
the network parameters. The loss function was the sum of L1 loss
and a functionDbinwhich we call binary divergence,
Dbin(YjS) :=Eft
 SftlogYft
Sft (1 Sft) log1 Yft
1 Sft
=Eft[ Sft^Yft+ log(1 + exp ^Yft)] + const:;(5)
where ^Yft= logit(Yft). Since the gradient is non-vanishing, i.e.,
@Dbin(YjS)=@^Yft/Yft Sft, it is advantageous for gradient-
based training. It is easily veriﬁed that the spectrogram error is non-
negative,Lspec(YjS) =Dbin(YjS) +E[jYft Sftj]0, and the
equality holds iff Y=S.
3.1.1. Details of TextEnc, AudioEnc, and AudioDec
The networks are fully convolutional, and are not dependent on any
recurrent units. In order to take into account the long contextual in-
formation, we used dilated convolution [33, 13, 24] instead of RNN.
The top equation of Fig. 2 is the content of TextEnc . It con-
sists of a character embedding and several 1D non-causal convolu-
tion layers. In the literature [2] an RNN-based component named
‘CBHG’ was used, but we found this convolutional network also
works well. AudioEnc andAudioDec , shown in Fig. 2, are com-
posed of 1D causal convolution layers. These convolution should be
causal because the output of AudioDec is feedbacked to the input of
AudioEnc at the synthesis stage.
3.2. Spectrogram Super-resolution Network (SSRN)
We ﬁnally synthesize a full spectrogram jZj2RF04T, from the
obtained coarse mel spectrogram Y2RFT, using the spectrogram
super-resolution network (SSRN). Upsampling frequency from Fto
F0is fairly simple. We can achieve that by increasing the convo-
lution channels of a 1D convolutional network. Upsampling of the
temporal axis is not done the same way. Instead, we quadruple the
length of the sequence from Tto4T=T0, by applying ‘deconvo-
lution’ layers of stride size 2 twice.
The bottom equation of Fig. 2 shows SSRN . In this paper, all
convolutions of SSRN are non-causal, since we do not consider on-
line processing. The loss function is the same as Text2Mel: the sum
of L1 distance and Dbin, deﬁned by (5), between the synthesized
spectrogram SSRN (S)and the ground truth jZj.
4. GUIDED ATTENTION
4.1. Guided Attention Loss: Motivation, Method and Effects
In general, an attention module is quite costly to train. Therefore,
if there is some prior knowledge, incorporating them into the model
may be a help to alleviate the heavy training. We show that the
following simple method helps to train the attention module.
In TTS, the possible attention matrix Alies in the very small
subspace of RNT. This is because of the rough correspondence of
the order of the characters and the audio segments. That is, when one
reads a text, it is natural to assume that the text position nprogresses
Fig. 3 . Comparison of the attention matrix A, trained with
and without the guided attention loss Latt(A). (Left) Without, and
(Right) with the guided attention. The test text is “icassp stands
for the international conference on acoustics, speech, and signal
processing. ” We did not use the heuristics described in section 4.2.
nearly linearly to the time t, i.e.,nat, whereaN=T . This
is a noticeable difference of TTS from other seq2seq learning tech-
niques such as machine translation, where an attention module needs
to resolve the word alignment between two languages that have very
different syntax, e.g. English and Japanese.
Based on this idea, we introduce ‘guided attention loss,’ which
prompts the attention matrix Ato be ‘nearly diagonal,’ Latt(A) =
Ent[AntWnt];whereWnt= 1 expf (n=N t=T)2=2g2g:In
this paper, we set g= 0:2. IfAis far from diagonal (e.g., reading the
characters in the random order), it is strongly penalized by the loss
function. This subsidiary loss function is simultaneously optimized
with the main lossLspecwith the equal weight.
Although this measure is based on quite a rough assumption,
it improved the training efﬁciency. In our experiment, if we added
the guided attention loss to the objective, the term began decreas-
ing only after100 iterations. After 5K iterations, the attention
became roughly correct, not only for training data, but also for new
input texts. On the other hand, without the guided attention loss,
it required much more iterations. It began learning after 10K it-
erations, and it required 50K iterations to look at roughly correct
positions, but the attention matrix was still vague. Fig. 3 compares
the attention matrix, trained with and without guided attention loss.
4.2. Forcibly Incremental Attention at the Synthesis Stage
At the synthesis stage, the attention matrix Asometimes fails to
focus on the correct characters. Typical errors we observed were
(1) skipping several letters, and (2) repeating a same word twice or
more. In order to make the system more robust, we heuristically
modiﬁed the matrix Ato be ‘nearly diagonal,’ by a simple rule as
follows. We observed this method sometimes alleviated such fail-
ures.
Letntbe the position of the character which is read at time t,
i.e.,nt= argmaxnAnt. Comparing the current position ntand the
previous position nt 1, if the difference nt nt 1is outside of the
range 1nt nt 13, the current attention is forcibly set
asAnt=n;n t 1+1(Kronecker delta ), to increment the attention
target asnt=nt 1+ 1.
5. EXPERIMENT
5.1. Experimental Setup
We used LJ Speech Dataset [34] to train the networks. This is
a public domain speech dataset consisting of 13K pairs of text
and speech, without phoneme-level alignment, 24 hours in total.
These speech data have a little reverberation. We preprocessed the
3Table 1 . Parameter Settings.
Sampling rate of audio signals 22050 Hz
STFT window function Hanning
STFT window length and shift 1024 (46.4 [ms]), 256 ( 11.6[ms])
STFT spectrogram size F04T 5134T(Tdepends on audio clip)
Mel spectrogram size FT 80T(Tdepends on audio clip)
Dimensione,dandc 128, 256, 512
ADAM parameters (; 1;2;") (210 4;0:5;0:9;10 6)
Minibatch size 16
Emphasis factors (;) (0.6, 1.3)
RTISI-LA window and iteration 100, 10
Character set, Char a-z,.’- andSpace andNULL
Table 2 . Comparison of MOS (95% conﬁdence interval), training
time, and iterations (Text2Mel/SSRN), of an open Tacotron [5] and
the proposed method (DCTTS). The digits with * were excerpted
from the repository [5].
Method Iteration Time MOS (95% CI)
Open Tacotron [5] 877K12 days2:070:62
DCTTS 20K/ 40K 2 hours 1:740:52
DCTTS 90K/150K 7 hours 2:630:75
DCTTS 200K/340K 15 hours 2:710:66
DCTTS 540K/900K 40 hours 2:610:62
texts by spelling out some of abbreviations and numeric expressions,
decapitalizing the capitals, and removing less frequent characters
not shown in Table 1, where NULL is a dummy character for zero-
padding.
We implemented our neural networks using Chainer 2.0 [35].
We trained the models using a household gaming PC equipped with
two GPUs. The main memory of the machine was 62GB, which
is much larger than the audio dataset. Both GPUs were NVIDIA
GeForce GTX 980 Ti, with 6 GB memories.
For simplicity, we trained Text2Mel and SSRN independently
and asynchronously using different GPUs. All network parameters
were initialized using He’s Gaussian initializer [36]. Both networks
were trained by the ADAM optimizer [37]. When training SSRN,
we randomly extracted short sequences of length T= 64 for each
iteration to save memory usage. To reduce the disk access, we re-
duced the frequency of creating the snapshot of parameters to only
once per 5K iterations. Other parameters are shown in Table 1.
As it is not easy for us to reproduce the original results of
Tacotron, we instead used a ready-to-use model [5] for comparison,
which seemed to produce the most reasonable sounds in the open
implementations. It is reported that this model was trained using
LJ Dataset for 12 days (877K iterations) on a GTX 1080 Ti, newer
GPU than ours. Note, this iteration is still much less than the original
Tacotron, which was trained for more than 2M iterations.
We evaluated mean opinion scores (MOS) of both methods
by crowdsourcing on Amazon Mechanical Turk using crowdMOS
toolkit [38]. We used 20 sentences from Harvard Sentences List
1&2. The audio data were synthesized using ﬁve methods shown
in Table 2. The crowdworkers rated these 100 clips from 1 (Bad)
to 5 (Excellent). Each worker is supposed to rate at least 10 clips.
To obtain more responses of higher quality, we set a few incentives
shown in the literature. The results were statistically processed using
the method shown in the literature [38].
5.2. Result and Discussion
In our setting, the training throughput was 3.8 minibatch/s (Text2Mel)
and6.4 minibatch/s (SSRN). This implies that we can iterate the
updating formulae of Text2Mel 200K times in 15 hours. Fig. 4
Fig. 4 . (Top) Attention, (middle) mel, and (bottom) linear STFT
spectrogram, synthesized by the proposed method, after 15 hours
training. The input text is “icassp stands for the international con-
ference on acoustics, speech and signal processing. ” (90 chars)
shows an example of attention, synthesized mel and full spectro-
grams, after 15 hours training. It shows that the method can almost
correctly focus on the correct characters, and synthesize quite clear
spectrograms. More samples are available at the author’s web page.1
In our crowdsourcing experiment, 31 subjects evaluated our
data. After the automatic screening by the toolkit [38], 560 scores
rated by 6 subjects were selected for ﬁnal statistics calculation. Ta-
ble 2 compares the performance of our proposed method (DCTTS)
and an open Tacotron. Our MOS (95% conﬁdence interval) was
2:710:66(15 hours training) while the Tacotron’s was 2:070:62.
Although it is not a strict comparison since the frameworks and
the machines were different, it would be still concluded that our
proposed method is quite rapidly trained to the satisfactory level
compared to Tacotron.
Note that the MOS were below the level reported in [2]. The
reasons may be threefold: (1) the limited number of iterations, (2)
SSRN needs to synthesize the spectrograms from less information
than [2], and (3) the reverberation of the training data.
6. SUMMARY AND FUTURE WORK
This paper described a novel TTS technique based on deep convo-
lutional neural networks, and a technique to train the attention mod-
ule rapidly. In our experiment, the proposed Deep Convolutional
TTS was trained overnight ( 15 hours), using an ordinary gaming
PC equipped with two GPUs, while the quality of the synthesized
speech was almost acceptable.
Although the audio quality is far from perfect yet, it may be
improved by tuning some hyper-parameters thoroughly, and by ap-
plying some techniques developed in the deep learning community.
We believe this method will encourage further development of
the applications based on speech synthesis. We can expect that this
simple neural TTS may be extended to many other purposes e.g.
emotional/non-linguistic/personalized speech synthesis, etc., by fur-
ther studies. In addition, since a neural TTS has become lighter, the
studies on more integrated speech systems e.g. some multimodal
systems, may have become more feasible. These issues should be
worked out in the future.
7. ACKNOWLEDGEMENT
The authors would like to thank the OSS contributors and the data
creators (LibriV ox contributors and @keithito).
1https://github.com/tachi-hi/tts_samples
48. REFERENCES
[1] I. Goodfellow et al., Deep Learning , MIT Press, 2016, http:
//www.deeplearningbook.org .
[2] Y . Wang et al., “Tacotron: Towards end-to-end speech synthe-
sis,” in Proc. Interspeech , 2017, arXiv:1703.10135.
[3] A. Barron, “Implementation of Google’s Tacotron in Ten-
sorFlow,” 2017, Available at GitHub, https://github.
com/barronalex/Tacotron (visited Oct. 2017).
[4] K. Park, “A TensorFlow implementation of Tacotron: A
fully end-to-end text-to-speech synthesis model,” 2017, Avail-
able at GitHub, https://github.com/Kyubyong/
tacotron (visited Oct. 2017).
[5] K. Ito, “Tacotron speech synthesis implemented in Tensor-
Flow, with samples and a pre-trained model,” 2017, Avail-
able at GitHub, https://github.com/keithito/
tacotron (visited Oct. 2017).
[6] R. Yamamoto, “PyTorch implementation of Tacotron speech
synthesis model,” 2017, Available at GitHub, https://
github.com/r9y9/tacotron_pytorch (visited Oct.
2017).
[7] J. Gehring et al., “Convolutional sequence to sequence learn-
ing,” in Proc. ICML , 2017, pp. 1243–1252, arXiv:1705.03122.
[8] H. Zen et al., “Statistical parametric speech synthesis using
deep neural networks,” in Proc. ICASSP , 2013, pp. 7962–7966.
[9] Y . Fan et al., “TTS synthesis with bidirectional LSTM based
recurrent neural networks,” in Proc. Interspeech , 2014, pp.
1964–1968.
[10] H. Zen and H. Sak, “Unidirectional long short-term memory
recurrent neural network with recurrent output layer for low-
latency speech synthesis,” in Proc. ICASSP , 2015, pp. 4470–
4474.
[11] S. Achanta et al., “An investigation of recurrent neural network
architectures for statistical parametric speech synthesis.,” in
Proc. Interspeech , 2015, pp. 859–863.
[12] Z. Wu and S. King, “Investigating gated recurrent networks for
speech synthesis,” in Proc. ICASSP , 2016, pp. 5140–5144.
[13] A. van den Oord et al., “WaveNet: A generative model for raw
audio,” arXiv:1609.03499 , 2016.
[14] J. Sotelo et al., “Char2wav: End-to-end speech synthesis,” in
Proc. ICLR , 2017.
[15] S. Arik et al., “Deep voice: Real-time neural text-to-speech,”
inProc. ICML , 2017, pp. 195–204, arXiv:1702.07825.
[16] S. Arik et al., “Deep voice 2: Multi-speaker neural text-to-
speech,” in Proc. NIPS , 2017, arXiv:1705.08947.
[17] K. Cho et al., “Learning phrase representations using RNN
encoder-decoder for statistical machine translation,” in Proc.
EMNLP , 2014, pp. 1724–1734.
[18] I. Sutskever et al., “Sequence to sequence learning with neural
networks,” in Proc. NIPS , 2014, pp. 3104–3112.
[19] O. Vinyals and Q. Le, “A neural conversational model,” in
Proc. Deep Learning Workshop, ICML , 2015.
[20] I. V . Serban et al., “Building end-to-end dialogue systems us-
ing generative hierarchical neural network models.,” in Proc.
AAAI , 2016, pp. 3776–3784.[21] D Bahdanau et al., “Neural machine translation by jointly
learning to align and translate,” in Proc. ICLR 2015,
arXiv:1409.0473 , 2014.
[22] Y . Kim, “Convolutional neural networks for sentence
classiﬁcation,” in Proc. EMNLP , 2014, pp. 1746–1752,
arXiv:1408.5882.
[23] X. Zhang et al., “Character-level convolutional networks for
text classiﬁcation,” in Proc. NIPS , 2015, arXiv:1509.01626.
[24] N. Kalchbrenner et al., “Neural machine translation in linear
time,” arXiv:1610.10099 , 2016.
[25] Y . N. Dauphin et al., “Language modeling with gated convolu-
tional networks,” arXiv:1612.08083 , 2016.
[26] J. Bradbury et al., “Quasi-recurrent neural networks,” in Proc.
ICLR 2017 , 2016.
[27] D. Grifﬁn and J. Lim, “Signal estimation from modiﬁed short-
time fourier transform,” IEEE Trans. ASSP , vol. 32, no. 2, pp.
236–243, 1984.
[28] P. Mowlee et al., Single Channel Phase-Aware Signal Process-
ing in Speech Communication: Theory and Practice , Wiley,
2016.
[29] X. Zhu et al., “Real-time signal estimation from modiﬁed
short-time fourier transform magnitude spectra,” IEEE Trans.
ASLP , vol. 15, no. 5, 2007.
[30] Y . LeCun and Y . Bengio, “The handbook of brain theory and
neural networks,” chapter Convolutional Networks for Images,
Speech, and Time Series, pp. 255–258. MIT Press, 1998.
[31] R. K. Srivastava et al., “Training very deep networks,” in Proc.
NIPS , 2015, pp. 2377–2385.
[32] H. Zhang et al., “StackGAN: Text to photo-realistic image syn-
thesis with stacked generative adversarial networks,” in Proc.
ICCV , 2017, pp. 5907–5915, arXiv:1612.03242.
[33] F. Yu and V . Koltun, “Multi-scale context aggregation by di-
lated convolutions,” in Proc. ICLR , 2016.
[34] K. Ito, “The LJ speech dataset,” 2017, Available at https://
keithito.com/LJ-Speech-Dataset/ (visited Sep.
2017).
[35] S. Tokui et al., “Chainer: A next-generation open source
framework for deep learning,” in Proc. Workshop LearningSys,
NIPS , 2015.
[36] K. He et al., “Delving deep into rectiﬁers: Surpassing human-
level performance on ImageNet classiﬁcation,” in Proc. ICCV ,
2015, pp. 1026–1034, arXiv:1502.01852.
[37] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-
mization,” in Proc. ICLR 2015 , 2014, arXiv:1412.6980.
[38] F. Ribeiro et al., “CrowdMOS: An approach for crowdsourcing
mean opinion score studies,” in Proc ICASSP , 2011, pp. 2416–
2419.
5Published as a conference paper at ICLR 2022
EINOPS : CLEAR AND RELIABLE TENSOR
MANIPULATIONS WITH EINSTEIN -LIKE NOTATION
Alex Rogozhnikov
alex.rogozhnikov@ya.ru
ABSTRACT
Tensor computations underlie modern scientiﬁc computing and deep learning.
A number of tensor frameworks emerged varying in execution model, hardware
support, memory management, model deﬁnition, etc. However, tensor operations
in all frameworks follow the same paradigm. Recent neural network architectures
demonstrate demand for higher expressiveness of tensor operations. The current
paradigm is not suited to write readable, reliable, or easy-to-modify code for mul-
tidimensional tensor manipulations. Moreover, some commonly used operations
do not provide sufﬁcient checks and can break a tensor structure. These mistakes
are elusive as no tools or tests can detect them. Independently, API discrepancies
complicate code transfer between frameworks. We propose einops notation: a
uniform and generic way to manipulate tensor structure, that signiﬁcantly improves
code readability and ﬂexibility by focusing on the structure of input and output ten-
sors. We implement einops notation in a Python package that efﬁciently supports
multiple widely used frameworks and provides framework-independent minimalist
API for tensor manipulations.
1 I NTRODUCTION
Deep learning (DL) over the past decade achieved a signiﬁcant progress in analysis and synthesis of
images, audio and text (Bengio et al., 2017; Aggarwal et al., 2018; Foster, 2019). Tools relying on
these methods became a commodity in production data pipelines.
Available research-and-production frameworks for DL, such as pytorch (Paszke et al., 2019), tensor-
ﬂow (Abadi et al., 2015), mxnet (Chen et al., 2015), jax (Bradbury et al., 2018), and others, vary in
numerous aspects, but their core functionality is built around efﬁcient computations on n-dimensional
arrays (tensors for brevity1). API exposed by frameworks for tensor operations follow the same
approach, that combines high efﬁciency (specialized hardware can be utilized) and user convenience:
computations can be expressed in high-level languages, such as Python, using a limited number of
exposed and usually pre-compiled operations.
Due to growing usage of DL in production and rising complexity of models, it becomes increasingly
more important to provide programming interfaces that enable reliable and scalable development.
We demonstrate that approach to deﬁne tensor operations taken by existing frameworks does not
encourage writing code that is easy to interpret, maintain or modify; additionally, some of the core
operations do not conduct sufﬁcient checks and can lead to hard-to-catch mistakes. To address
these problems, we propose Einstein-like notation for operations, called einops . We implement this
approach in a Python (Van Rossum & Drake, 2009) package to allow simple integration of notation
into existing code across a variety of frameworks.
Outline We ﬁrst brieﬂy describe mainstream approach for tensor operations and point to its issues
with examples. We review previously proposed ideas to resolve mentioned problems in related works .
Our approach, einops – a verbose notation for tensor manipulation – is introduced next, followed by
the code examples in case studies . We implement the notation in a Python package, and explain main
design choices in implementation details section. We conclude with a discussion ofeinops role and
common criticisms.
Currently at Herophilus, Inc.
1Our sincere apologies to readers with backgrounds in mathematics and physics for possible confusion.
1Published as a conference paper at ICLR 2022
2 M AINSTREAM APPROACH FOR TENSOR OPERATIONS
Nowadays tensor programming is dominating in DL and playing a crucial role in scientiﬁc computing.
It ﬁrst appeared in APL (Iverson, 1962), was popularized by MATLAB (Matlab, 1993) and was spread
in Python community by numpy (Harris et al., 2020), and now is provided by multiple frameworks.
Currently, tensor manipulations in all mainstream frameworks are based on the following assumptions:
• Tensor is an n-dimensional array with shape (e.g. 463) and data type (e.g. ﬂoat32).
•When elements are matched across tensors, axes are aligned by order. Conventions regulate cases
of different dimensionalities, e.g. broadcasting in Numpy (2021).
•Multiple operations act differently on axes. Those operations either have conventions about axes
order (e.g. recurrent units, convolutions; convolutions expect input to be either BHWC or BCHW
ordered) or should be provided with indices of axes that should be treated separately:2
y /equal.tosf x.max(axis/equal.tosf/one.tosf) /numbersign.tosf some operations are provided with indices
y /equal.tosf x.swap_axes(/zero.tosf, /one.tosf) /numbersign.tosf of specially treated axes
The mixed option is possible when operation has defaults for “special axes” that can be overridden
during a call.
Beneﬁts of the mainstream approach are simple API and a baseline implementation, as well as
versatility: a lot of research code operates solely using this kind of operations for number crunching.
The drawbacks are absence of semantics in an operation and no way to reﬂect expected input and
output. To predict output after a chain of manipulations a researcher/engineer has to carefully keep in
the memory (or in the code comments) shape and contents of each intermediate tensor and thus keep
track of every operation.
In this listing a batch of images is collapsed into a single image by placing images in a row next to
each other
/numbersign.tosf im_stack has shape [b, /three.tosf, h, w]
y/one.tosf /equal.tosf im_stack.transpose(/two.tosf, /zero.tosf, /three.tosf, /one.tosf).reshape(h, b * w, /three.tosf)
y/two.tosf /equal.tosf im_stack.transpose(/two.tosf, /three.tosf, /zero.tosf, /one.tosf).reshape(h, b * w, /three.tosf)
One of y/one.tosf,y/two.tosfcontains a correct image, while the other is irrelevant – it requires some time to ﬁgure
out which is what. A typical computation chain contains multiple tensor operations (only two in this
example) and requires writing down all intermediate steps to “debug” the code. Annoyingly, resulting
tensors y/one.tosfandy/two.tosfhave the same shapes and mistake in the code may stay under the radar for a long
time since the code never errors out. The lack of stronger checks is a weak point of conventional
operations.
In most cases we also cannot meaningfully visualize intermediate layers, so there is no way to narrow
down searches for a problem source. Thus, a researcher/engineer has to check all the code after a
failure.
Traditional operations restrict code ﬂexibility: any change in the shape agreements between parts of
the code is hard to align and all related code fragments (frequently located in different places) should
be updated simultaneously. In the next code fragment we add omitted batch dimension to the code
from the vision permutator (Hou et al., 2021), and then update code to support depth:
/numbersign.tosf pytorch-like code without batch dimension, as in the paper
x_h /equal.tosf x.reshape(H, W, N, S).permute(/two.tosf, /one.tosf, /zero.tosf, /three.tosf).reshape(N, W, H*S)
x_h /equal.tosf proj_h(x_h).reshape(N, W, H, S).permute(/two.tosf, /one.tosf, /zero.tosf, /three.tosf).reshape(H, W, C)
/numbersign.tosf with batch dimension
x_h /equal.tosf x.reshape(B, H, W, N, S).permute(/zero.tosf, /three.tosf, /two.tosf, /one.tosf, /four.tosf).reshape(B, N, W, H*S)
x_h /equal.tosf proj_h(x_h).reshape(B, N, W, H, S).permute(/zero.tosf, /three.tosf, /two.tosf, /one.tosf, /four.tosf).reshape(B, H, W, C)
/numbersign.tosf with batch and depth dimension
x_h /equal.tosf x.reshape(B, H, W, D, N, S).permute(/zero.tosf, /four.tosf, /two.tosf, /three.tosf, /one.tosf, /five.tosf).reshape(B, N, W, D, H*S)
x_h /equal.tosf proj_h(x_h).reshape(B, N, W, D, H, S).permute(/zero.tosf, /four.tosf, /two.tosf, /three.tosf, /one.tosf, /five.tosf).reshape(B, H, W,
D, C)
2Pseudocode in the paper corresponds to numpy unless otherwise stated. There is no way to write cross-
framework code. This problem we partially address with proposed einops notation.
2Published as a conference paper at ICLR 2022
Modiﬁcations are very error-prone: all indices should be recomputed and order of reshape operands
should be veriﬁed. Uncommonly, this fragment is quite self-documenting since ﬁnal reshape in
each line hints the intended order of axes after the transposition. In DL code it is common to
use x.transpose(/zero.tosf, /three.tosf, /one.tosf, /two.tosf) expecting other users to recognize a familiar pattern or leaving a
comment.3Related, transposition in the code requires operating with three contexts in mind (original
order of axes, permutation, result order), and even when simpliﬁed to just permutation, it’s unclear if
permutation (/two.tosf /zero.tosf /three.tosf /one.tosf) is inverse to (/one.tosf /three.tosf /zero.tosf /two.tosf) .
Less critical, but still notable source of mistakes is 0-based enumeration of axes (Julia, MATLAB
and R use 1-based), while we propose the framework not relying on axis numeration at all. Common
to refer to axis with index 0 as “ﬁrst axis/dimension” (see e.g. numpy documentation), and there is no
way to change this habit and avoid confusion between human communication and code.
Finally, common tensor operations can easily break the tensor structure. For example, reshape , a
common operation in the DL code, easily breaks the tensor structure because a whole tensor is treated
as a sequence and no connection is assumed between axes in input and output, see Figure 1.
(a)x
shape = (92, 3, 96)
(b)x.reshape(/four.tosf/six.tosf, /three.tosf, -/one.tosf)
shape = (46, 3, 192)
(c)x.reshape(/four.tosf/eight.tosf, /three.tosf, -/one.tosf)
shape = (48, 3, 184)
Figure 1: Example of breaking the tensor structure in 3d tensor. We use non-convential order HCW
for visalizations: (a) original image; (b) partial mixing results in new colors; (c) mixing of all axes.
Result shapes are shown underneath.
Aforementioned implementation mistakes can result in rejecting valuable research or drawing incor-
rect conclusion about the data. Moreover, there is no recipe to identify incorrect implementation,
because the result tensor shape and data type are not affected by an error. Even the presence of an
error can be obscured: poor performance can be misattributed to a number of other factors: data,
hyperparameters, method, etc. All pointed issues have catastrophic importance for research costs and
timeline in a setting where one experiment requires dozens to thousands of GPU-hours.
A number of recently proposed architectures (Wu et al., 2021; Tolstikhin et al., 2021; Jumper et al.,
2021; Liu et al., 2021; Hou et al., 2021; Touvron et al., 2021) demonstrate that conservative approach
with ﬁxed ordering of axes (like BCHW for convolutional networks) is not sufﬁcient. Existing
frameworks carry this imprint that does not help with searches for new architectures.
3 R ELATED WORKS
A commonly taken approach to increase reliability is assigning names/labels to tensor dimensions
(below we refer to them as labeled tensors). Most known implementations are xarray (Hoyer
& Hamman, 2017), labeled_tensor in tensorﬂow (Christiansen & Hoyer, 2016), namedtensor
(2019), and named tensors in Pytorch (2019). Despite being around for many years, this idea
got little adoption, and not used in the DL code: development stopped for labeled_tensor and
namedtensor , and named tensors in pytorch are still experimental.
In this approach, operations match tensor axes based on labels (common choice of label is string
name), and rely on axis label instead of axes order, e.g.:
/numbersign.tosf x/one.tosf has axes (x, y, height) and x/two.tosf has axes (time, x, y)
x/one.tosf /plus.tosf x/two.tosf /numbersign.tosf result has axes (x, y, height, time), maybe in a different order
x/one.tosf.mean( 'height ') /numbersign.tosf reduce over axis named 'height '
3Out of 50 last usages of torch.permute in Python on GitHub on 22 Nov 2021, only in 4 cases code had
comments about result shape, in other cases no information was provided in any other form, including reshape .
3Published as a conference paper at ICLR 2022
Axes matching rules vary across implementations. However, we can describe some common issues
why this approach did not gain wide support in the DL community:
•Operations focus on modiﬁed axes, and neither describe input nor output; a user has to remember
axes labels for each of intermediate tensors.
•Less control over data layout: order of axes may signiﬁcantly inﬂuence speed (Weber & Goesele,
2017), but is not transparent.
•Names should be strictly deﬁned and mistakes in names or their alignment between modules may
result in the wrong computations, not an exception, e.g. for namedtensor package:
/numbersign.tosf x/one.tosf has axes (height, width)
/numbersign.tosf x/two.tosf has axes (h, w)
/numbersign.tosf x/three.tosf has axes (heigt)
x/one.tosf /plus.tosf x/two.tosf /plus.tosf x/three.tosf /numbersign.tosf has axes (height width h w heigt) in some order
•Adoption requires a strong buy-in decision, as all code should be axis label-centric. In contrary,
transition to labeled tensors in isolated code pieces only (e.g. functions) does not prevent (more
elusive) errors in interaction between functions, but introduces constant conversions between the
code styles. Third-party components need wrapping to support labels.
•Symmetric and antisymmetric tensors with multiple identical axes pose an additional challenge:
all axes labels should be unique to allow matching and axis speciﬁcation.
• Labeled tensors have issues with integration of DL blocks (details in Appendix A).
Proposed implementations of labeled tensors also break the common principle of decomposition
in software engineering, which states that every function has its own scope with input names and
names for intermediate variables. Everything that is shared between scopes should be described in
the function signature. Whenever an internal structure of passed or returned entity should be shared
between scopes, a type/class/interface/protocol is introduced to describe a passed argument. However,
the concept of labeled tensor breaks this logic: it is assumed that called and calling functions agree
on axes names, but no way to document these expectations is proposed.
Alternative approach to increase readability and reliability of tensor-operating code is to deliberately
set interface restrictions only on large neural modules such as language models, encoders or decoders,
as in NeMo Kuchaiev et al. (2019). While allowing to reuse and wrap existing code to glue large
components, this approach does not improve internals of the modules where problems are harder
to locate and code is less documented. These improvements of high-level interfaces still have their
challenges, for example a language model can expect to manipulate sequences of letters and thus
expects axis “letter”. However, surrounding code may try to use it for prediction of words, pixels or
phonemes. Thus, relabeling of axes may be required to “connect” subsystems.
In 2011, M. Wiebe introduced an operation einsum innumpy package. With some simpliﬁcations
(absence of covariant and contravariant indices, contracted dimension may be not repeated) einsum
mimics Einstein summation rule commonly used in physics. numpy.einsum stands out from the rest
ofnumpy API and for a long time rarely was mentioned in tutorials. However, function universality
and expressiveness were beneﬁcial, and it was ported to other packages: tensorﬂow, pytorch, mxnet,
chainer Tokui et al. (2019), etc.
numpy.einsum( 'ij,jk-/greater.tosfik ', A, B) /numbersign.tosf matrix multiplication
numpy.einsum( 'ijk-/greater.tosfij ', C) /numbersign.tosf sum over last axis
numpy.einsum( 'ij,ji-/greater.tosf ', A, B) /numbersign.tosf trace of matrix product
einops revisits and expands this approach with an emphasis on complex tensor layouts and rear-
rangements, additional checks and broader functionality. In our work we try to align interface with
einsum to allow smooth simultaneous usage. However, interface adjustments (such as support for
multi-character axes names) are necessary. Most of our changes can be readily applied to einsum .
Detailed discussion of differences between einops andeinsum is given in Appendix B.
There is an ongoing research to create languages for low-level deﬁnition of tensor operations with
explicit indexing, e.g. Tensor Comprehensions (Vasilache et al., 2018) and Dex (Paszke et al., 2021).
4Published as a conference paper at ICLR 2022
4einops
Einstein-Inspired Notation for OPerationS, einops , is our proposal to address problems mentioned
in Section 2. The core of our approach is a new notation for transformation patterns and, in its basic
view, this notation enumerates the tensor elements in one-to-one-correspondence to the set of axes.
As seen in examples below, we allow number of axes to be different from tensor dimensionality. The
notation uses simple syntax, formally deﬁned in Appendix C, and is based on the following rules:
• axis present only in the input (the left hand side) is reduced (e.g. with max-reduction)
•axis present only in the output is “repeated” (tensor values are the same for all values of new axes)
• all axis identiﬁers on either side of expression should be unique
Examples of einops transformation patterns are
'b c h w -/greater.tosf b h w c '/numbersign.tosf transpose
'b c h w -/greater.tosf b c ' /numbersign.tosf reduce on h, w
'b c -/greater.tosf b c h w ' /numbersign.tosf repeat on h, w
'(h/one.tosf h/two.tosf) (w/one.tosf w/two.tosf) c -/greater.tosf (h/one.tosf w/one.tosf) h/two.tosf w/two.tosf c '/numbersign.tosf split image to patches, stack them
Each tensor pattern ensures one-to-one mapping between element’s positions in the tensor and values
of axis variables. This requires all axes used in one tensor pattern to be different (thus traces, permitted
byeinsum , are not provided by einops ). This also requires that ellipsis can be used only once within
a tensor pattern.
The main novelty of our notation is the composition and decomposition of axes denoted by parenthesis.
(De)composition uses C-ordering convention, intuitively associated with digits in a number:
/numbersign.tosf x is of shape (/one.tosf/zero.tosf, /one.tosf/zero.tosf, /one.tosf/zero.tosf, /one.tosf/zero.tosf), then x[/six.tosf, /two.tosf, /four.tosf, /nine.tosf] /equal.tosf/equal.tosf y[/six.tosf/two.tosf/four.tosf/nine.tosf]
y /equal.tosf rearrange(x, 'a b c d -/greater.tosf (a b c d) ')
Changing the rightmost of “digits” changes composed index in “small steps”, while any change in
leftmost results in “large steps”, even when axes are not decimal:
/numbersign.tosf Rearrange pattern that composes /three.tosf axes into one: i/one.tosf i/two.tosf i/three.tosf -/greater.tosf (i/one.tosf i/two.tosf i/three.tosf)
/numbersign.tosf Let original array have a shape of [/two.tosf, /three.tosf, /two.tosf], result has a length /two.tosfx/three.tosfx/two.tosf/equal.tosf/one.tosf/two.tosf
i/one.tosf /zero.tosf /zero.tosf /zero.tosf /zero.tosf /zero.tosf /zero.tosf /one.tosf /one.tosf /one.tosf /one.tosf /one.tosf /one.tosf
i/two.tosf /zero.tosf /zero.tosf /one.tosf /one.tosf /two.tosf /two.tosf /zero.tosf /zero.tosf /one.tosf /one.tosf /two.tosf /two.tosf
i/three.tosf /zero.tosf /one.tosf /zero.tosf /one.tosf /zero.tosf /one.tosf /zero.tosf /one.tosf /zero.tosf /one.tosf /zero.tosf /one.tosf
final position /zero.tosf /one.tosf /two.tosf /three.tosf /four.tosf /five.tosf /six.tosf /seven.tosf /eight.tosf /nine.tosf /one.tosf/zero.tosf /one.tosf/one.tosf
Reverse pattern (i/one.tosf i/two.tosf i/three.tosf) -/greater.tosf i/one.tosf i/two.tosf i/three.tosf uses the same bijection to decompose an axis into three.
Since axis can be decomposed in multiple ways (e.g. 12 could be represented as 232or
1121or314, etc.), additional axis size speciﬁcations are required during decomposition.
The following rule is helpful for C-ordered arrays (default ordering in all current backends): in case
the order of axes does not change, result of rearrange is still a view of the original data. For example,
rearrangement '(a b c) (d e f) (g h) -/greater.tosf a b (c d) e (f g h) 'returns a view and no copy is
required.
Axes can be referred to by their size. These anonymous axes have the same role as named, but due to
the lack of name they can’t be matched across different tensors. Unitary axes have a special meaning –
they do not correspond to an axis variable, and thus their behavior is separate from anonymous axes.
'h w -/greater.tosf h w /one.tosf ' /numbersign.tosf add unitary axis
'h w -/greater.tosf h w /three.tosf ' /numbersign.tosf repeat values along new anonymous axis
'/one.tosf h w /three.tosf -/greater.tosf h w ' /numbersign.tosf remove unitary axis and reduce on anonymous axis of length /three.tosf
'... h w -/greater.tosf ... w h '/numbersign.tosf transpose two last dimensions
'b ... c -/greater.tosf (...) b c '/numbersign.tosf compose all but first and last dimensions
/numbersign.tosf and move resulting new axis to front
'b c ... -/greater.tosf b c ' /numbersign.tosf reduce on all but first two dimensions
All anonymous axes are treated as different even if they share length. Similar to einsum , ellipsis
works as a wildcard for zero or more axes, their number and lengths are derived from input shape(s).
5Published as a conference paper at ICLR 2022
Ellipses are not named (grammar allows adding names later), thus all ellipses within a transformation
pattern refer to the same group of unnamed axes.
Pattern composition and decomposition became particularly helpful to leverage existing operations for
data of higher dimensionality. E.g. if an attention function accepts tensors k, q, v of shape [batch,
seq, channel] , one can turn it into multi-head attention for 3-dimensional data by composing height,
width and depth to a single dimension, and grouping head and batch dimension to ensure independent
processing of attention heads: b h w d head c -/greater.tosf (b head) (h w d) c . Likewise, other neural
blocks can be “upgraded” by rearranging inputs and outputs.
einops provides three functions ( rearrange ,reduce , and repeat ) which are shown below in examples
(additional axes speciﬁcations are provided with **kwargs ) :
/numbersign.tosf organize /one.tosf/six.tosf images in /four.tosfx/four.tosf grid
rearrange(im, '(b/one.tosf b/two.tosf) h w c -/greater.tosf (b/one.tosf h) (b/two.tosf w) c ', b/one.tosf/equal.tosf/four.tosf, b/two.tosf/equal.tosf/four.tosf)
/numbersign.tosf max-pooling with kernel of size /two.tosfx/two.tosf
reduce(im, 'b c (h h/two.tosf) (w w/two.tosf) -/greater.tosf b c h w ','max', h/two.tosf/equal.tosf/two.tosf, w/two.tosf/equal.tosf/two.tosf)
/numbersign.tosf /two.tosfx upsampling of individual image by repeating pixels
repeat(im, 'h w c -/greater.tosf (h h/two.tosf) (w w/two.tosf) c ', h/two.tosf/equal.tosf/two.tosf, w/two.tosf/equal.tosf/two.tosf, c/equal.tosf/three.tosf)
While all patterns could be handled by a single function instead of three, we made an explicit choice
to separate scenarios of “adding dimensions” (repeat), “removing dimensions” (reduce) and “keeping
number of elements the same” (rearrange). This helps in producing more speciﬁc error messages
when a wrong pattern is passed.4
Ineinsum , when an axis is present in all tensors, operation performs independently for all values
of this axis, which is principle in einops . A tensor pattern only identiﬁes correspondence between
axes and element’s position in tensor, but does not affect arithmetic operation; this ensures that input
and output patterns can be changed independently. In particular, rearrange is an arithmetic identity
(same value returned), but usage of different input and output patterns turns it into a rather universal
tool for changing tensor shape/layout.
Proposed notation addresses different problems of the mainstream approach:
•Both input and output are described in the operation deﬁnition: tensor dimensionality and expected
order of axes. This makes einops -based code more declarative and self-documenting. A user is
not required to remember or infer shapes of tensors after every operation.
•Input is checked for a number of dimensions and divisibility of corresponding dimensions. The
length of dimension is checked if provided.
•A tensor structure cannot be broken by design, because the notation connects input axes (or their
constituents) to output axes.
• Axis enumeration is not used, so no way to make one-off mistake.
• Users do not need to compute permutation of axes, those are computed from a pattern.
•einops notation alleviates the need to track a tensor layout with patterns.
•einops andeinsum “document” inputs and outputs, simplifying inference of tensor shapes
from the code for other tensors that are not direct input/output of einops , but are interacting or
computed using direct inputs/outputs (an example is given in Appendix E).
We show versatility of einops by expressing common numpy (np) operations5in Listing 1
1np.transpose(x, [/zero.tosf, /three.tosf, /one.tosf, /two.tosf]) rearrange(x, 'b h w c -/greater.tosf b c h w ')
2np.reshape(x, rearrange(x, 'h w c -/greater.tosf (h w) c ')
3[x.shape[/zero.tosf]*x.shape[/one.tosf], x.shape[/two.tosf]])
4np.squeeze(x, /zero.tosf) rearrange(x, '() h w c -/greater.tosf h w c ')
5np.expand_dims(x, -/one.tosf) rearrange(x, 'h w c -/greater.tosf h w c () ')
6np.stack([r, g, b], axis/equal.tosf/two.tosf) rearrange([r, g, b], 'c h w -/greater.tosf h w c ')
7np.concatenate([r, g, b], axis/equal.tosf/zero.tosf) rearrange([r, g, b], 'c h w -/greater.tosf (c h) w ')
4Post-factum we can conﬁrm this choice: search over Github shows that rearrange, the most restrictive on
possible patterns, also accounts for the majority of usages.
5Some examples use list inputs, details see in Appendix B.
6Published as a conference paper at ICLR 2022
8np.flatten(x) rearrange(x, 'b t c -/greater.tosf (b t c) ')
9np.swap_axes(x, /zero.tosf, /one.tosf) rearrange(x, 'b t c -/greater.tosf t b c ')
10left, right /equal.tosf np.split(image, /two.tosf, axis/equal.tosf/one.tosf) rearrange(x, 'h (lr w) c -/greater.tosf lr h w c ', lr/equal.tosf/two.tosf)
11even, odd /equal.tosf x[:, /zero.tosf::/two.tosf], x[:, /one.tosf::/two.tosf] rearrange(x, 'h (w par) -/greater.tosf par h w c ', par/equal.tosf/two.tosf)
12np.max(x, [/one.tosf, /two.tosf]) reduce(x, 'b h w c -/greater.tosf b c ','max')
13np.mean(x) reduce(x, 'b h w c -/greater.tosf ','mean ')
14np.mean(x, axis/equal.tosf(/one.tosf, /two.tosf), keepdims/equal.tosfTrue) reduce(x, 'b h w c -/greater.tosf b () () c ','mean ')
15np.reshape(x, [-/one.tosf, /two.tosf]).max(axis/equal.tosf/one.tosf) reduce(x, '(h /two.tosf) -/greater.tosf h ','max')
16np.repeat(x, /two.tosf, axis/equal.tosf/one.tosf) repeat(x, 'h w -/greater.tosf h (w /two.tosf) ')
17np.tile(x, /two.tosf, axis/equal.tosf/one.tosf) repeat(x, 'h w -/greater.tosf h (/two.tosf w) ')
18np.tile(x[:, :, np.newaxis], /three.tosf, axis/equal.tosf/two.tosf) repeat(x, 'h w -/greater.tosf h w /three.tosf ')
Listing 1: Correspondence between numpy andeinops operations.
5 C ASE STUDIES
We again consider fragments from the vision permutator (Hou et al., 2021). Two examples below
only differ in what axis is mixed.
/numbersign.tosf vision permutator - mixing along h
x_h /equal.tosf x.reshape(B, H, W, N, S).permute(/zero.tosf, /three.tosf, /two.tosf, /one.tosf, /four.tosf).reshape(B, N, W, H*S)
x_h /equal.tosf proj_h(x_h).reshape(B, N, W, H, S).permute(/zero.tosf, /three.tosf, /two.tosf, /one.tosf, /four.tosf).reshape(B, H, W, C)
/numbersign.tosf vision permutator - mixing along w
x_w /equal.tosf x.reshape(B, H, W, N, S).permute(/zero.tosf, /one.tosf, /three.tosf, /two.tosf, /four.tosf).reshape(B, H, N, W*S)
x_w /equal.tosf proj_w(x_w).reshape(B, H, N, W, S).permute(/zero.tosf, /one.tosf, /three.tosf, /two.tosf, /four.tosf).reshape(B, H, W, C)
While four operations were updated simultaneously, in einops counterpart changes are limited to
swapping hand wbefore and after projection, because layouts of input and output are disentangled
and changes in one do not propagate to the other. And to remove e.g. batch dimension, one just
removes axis bfrom patterns (this is also a generic property of einops ).
/numbersign.tosf einops: mixing along h
x_h /equal.tosf rearrange(x, 'b h w (n s) -/greater.tosf b n w (h s) ', s/equal.tosfS)
x_h /equal.tosf rearrange(proj_h(x_h), 'b n w (h s) -/greater.tosf b h w (n s) ', s/equal.tosfS)
/numbersign.tosf einops: mixing along w. We swapped h and w before and after projection
x_w /equal.tosf rearrange(x, 'b h w (n s) -/greater.tosf b n h (w s) ', s/equal.tosfS)
x_w /equal.tosf rearrange(proj_w(x_w), 'b n h (w s) -/greater.tosf b h w (n s) ', s/equal.tosfS)
The next fragment is derived from OpenAI’s implementation of Glow (Kingma & Dhariwal, 2018).
As other neural ﬂows, Glow includes multiple rearrangements.
def unsqueeze/two.tosfd(x, factor/equal.tosf/two.tosf):
assert factor /greater.tosf/equal.tosf /one.tosf
if factor /equal.tosf/equal.tosf /one.tosf:
return x
shape /equal.tosf x.get_shape()
height /equal.tosf int(shape[/one.tosf])
width /equal.tosf int(shape[/two.tosf])
n_channels /equal.tosf int(shape[/three.tosf])
assert n_channels /greater.tosf/equal.tosf /four.tosf and n_channels /percent.tosf /four.tosf /equal.tosf/equal.tosf /zero.tosf
x /equal.tosf tf.reshape(
x, (-/one.tosf, height, width, int(n_channels/factor**/two.tosf), factor, factor))
x /equal.tosf tf.transpose(x, [/zero.tosf, /one.tosf, /four.tosf, /two.tosf, /five.tosf, /three.tosf])
x /equal.tosf tf.reshape(x, (-/one.tosf, int(height*factor),
int(width*factor), int(n_channels/factor**/two.tosf)))
return x
/numbersign.tosf same in einops, no function introduced
rearrange(x, 'b h w (c h/two.tosf w/two.tosf) -/greater.tosf b (h h/two.tosf) (w w/two.tosf) c ', h/two.tosf/equal.tosffactor, w/two.tosf/equal.tosffactor)
As for the original implementation, function name is confusing and non-
descriptive. To reﬂect the actual transformation, a name should be changed to
rearrange_by_squeeze_channels_and_unsqueeze_h_and_w .einops alleviates necessity to in-
troduce a function, as arguments describe input, output and the transformation itself. This
7Published as a conference paper at ICLR 2022
example also demonstrates how einops performs transformations, as under the hood it makes the
same sequence of transformations (reshape-transpose-reshape) as in the original code. Reverse
rearrangement (also used in Glow, see Appendix F) requires detailed analysis before implementation
as no arguments are shared between original and reverse rearrangements. In einops a pattern can be
reversed by swapping input and output parts of the pattern.
In Appendix E we analyze and rewrite a larger fragment of code: a multi-head attention module
derived from the popular implementation (Huang, 2018).
6 I MPLEMENTATION DETAILS
We implement proposed notation in a Python package einops .6
Support of multiple frameworks. einops supports a diverse set of DL frameworks (pytorch,
tensorﬂow, chainer, jax, gluon) as well as frameworks for tensor computations: numpy, cupy (Okuta
et al., 2017). We refer to them as backends. The major challenge in the support of multiple backends
is the absence of common API: even simple operations like repeat, view, or transpose are deﬁned
inconsistently.
np.transpose(x, [/two.tosf, /zero.tosf, /one.tosf]) /numbersign.tosf numpy
x.transpose(/two.tosf, /zero.tosf, /one.tosf) /numbersign.tosf numpy
tf.transpose(x, [/two.tosf, /zero.tosf, /one.tosf]) /numbersign.tosf tensorflow
K.permute_dimensions(x, [/two.tosf, /zero.tosf, /one.tosf]) /numbersign.tosf keras
mx.nd.transpose(x, (/two.tosf, /zero.tosf, /one.tosf)) /numbersign.tosf mxnet
x.permute(/two.tosf, /zero.tosf, /one.tosf) /numbersign.tosf torch
rearrange(x, 'h w t -/greater.tosf t h w ') /numbersign.tosf einops (any backend)
This inconsistency makes projects like einops more valuable for users, as they minimize framework
speciﬁcs when users do not need it. Proposal (Data-Apis, 2021) may help in convergence on shared
API and simplify development of cross-framework applications, including einops .
Backend recognition ineinops does not use wrapped imports, which are commonly used in Python
to handle optional dependencies:
def is_numpy_tensor(x): /numbersign.tosf einops does not use this approach
try:
import numpy as np
return isinstance(x, np.ndarray)
except ImportError as _:
return False
Instead, einops keeps dictionary that maps a tensor type to a backend. When the type is not found in
a dictionary, a pass through backends is done, and before importing the module, einops conﬁrms
that the backend was previously loaded in sys.modules , since einops will not receive tensors from
non-imported modules. The main reasons for this strategy are: a) some backends take a lot of memory
and time to load (e.g. tensorﬂow), which may result in an unforeseen usage of resources if a backend
is installed but not used; b) it is possible that a backend is installed incorrectly (e.g. a wrong binary)
and import drives to non-catchable segmentation faults; c) checking backends one-by-one incurs
unnecessary overhead, which we skip by dictionary lookup.
einops has shared logic for parsing and conversion into operations for every framework. This
conversion is very efﬁcient: every einops pattern is converted into at most 4 tensor operations by
backend, even if dimensionality of tensor is large.7Overall, einops adds marginal overhead on top
of the DL frameworks, see Appendix G.
Support for backends . Each backend is represented by a proxy class that provides a minimal set of
operations necessary to manipulate tensors. In addition, several supplementary methods are required
and implemented to allow generic testing: most of tests can be run against any backend. In addition,
6Einops package is available online at https://github.com/arogozhnikov/einops
7There are unavoidable exceptions: some backends cannot reduce multiple axes at once.
8Published as a conference paper at ICLR 2022
einops implements layers for backends with layers support. This codebase heavily relies on stateless
operations and has some backend-speciﬁc code.
Caching plays an important role in einops high performance (Appendix H). There are two layers of
caching: i) cache for a pattern and provided axes; ii) cache for a pattern, provided axes, and input
shape. When the same pattern is applied to a new shape, only shape veriﬁcation and computation of
unknown axes sizes is done. When a pattern is applied to input of the same shape (quite typical for
iterative algorithms, and very common in DL), einops only executes sequence of commands with
cached parameters.
Exceptions are detailed to provide information about performed operation including pattern and
provided axes sizes to simplify debugging.
7 D ISCUSSION
Speaking of criticism, einops is sometimes described as “stringly-typed”. We should point that it
does not make einops less reliable compared to the mainstream approach:
numpy.transpose: [ndarray, Tuple[int]] -/greater.tosf np.ndarray
rearrange: [ndarray, string] -/greater.tosf ndarray, (ndarray can be polymorphic)
So input and output types are not any different except for string not being tuple. In frameworks tensor
types do not describe number of dimensions, and existing type system cannot set restrictions on shape
components. Static analysis is identically ignorant to mistakes in patterns and e.g. in axes order.
There is no static check that prevents users from passing tuple with wrong number of components, or
repeats, or just ridiculously large numbers. Interestingly, numpy.einsum can accept axis indices not
string pattern, but it makes operation less expressive and almost never used.
einops does not enforce any alignment of shapes/axes across operations, thus does not provide
integrated analysis/tracking of shapes. Unfortunately, there is currently no design that can accomplish
this goal either: different ﬂavors of labeled tensors remain prototypes. A successful design should
satisfy multiple requirements that are rather challenging and seem to be irresolvable without a new
language (or new language features). On the other hand, to become practically valuable, the system
should not be too sophisticated. If resulting solution is signiﬁcantly harder to integrate compared to
running every module with several test inputs, it is unlikely to get a wide adoption.
We performed an initial exploration of notation applicability in 2018 when einops was open-sourced
by reimplementing a set of diverse fragments from popular DL implementations across different tasks
and domains (see details in Appendix I). Since its initial release, adoption of einops steadily grows,
it is currently used in more than 1000 public Github projects including the contributions from widely
known AI laboratories. This important evidence (discussed in Appendix J) conﬁrms design choices
and ultimate suitability of notation for needs of research in deep learning.
einops notation is not tied to a speciﬁc implementation or even language: it can be implemented
in more efﬁcient languages.8Implementation also can use lower-level primitives and optimizations
provided e.g. by TVM and MLIR, not framework-provided abstractions. Both directions have a
potential to boost einops performance. Alternatively, notation can be introduced to lower-level
primitives.
8 C ONCLUSION
We introduce einops – a minimalist notation for tensor manipulations that leverages patterns for
describing structure of input and output tensors. einops mitigates a number of issues common to
the conventional tensor manipulation routines, represents a number of commonly used functions
with a small API, and makes code more readable and reliable. We implement notation in a Python
package that provides identical API across a variety of tensor frameworks. Cross-framework support,
expressiveness of code, additional checks and simple integration into existing projects make einops
a convenient tool for researchers and engineers.
8einops was independently ported to Rust language https://docs.rs/einops/latest/einops/
9Published as a conference paper at ICLR 2022
ACKNOWLEDGMENTS
We thank M. Cramner, C. Garcia, E. Glazistova, A. Grachev, D. Ivanov, J. Henriques, S. Hoyer,
T. Likhomanenko, A. Molchanov, S. Ramakrishnan and others for participating in user studies and/or
discussions of notation and API. Multiple improvements to the project documentation were made by
C. Garcia and other community members.
We thank Phil Wang for publishing numerous high-quality implementations of new architectures
using einops and introducing practitioners to the notation by demonstrating its usage in complete
projects.
We are grateful to A. Chernyavskiy for feedback on initial draft of this manuscript and to T. Likhoma-
nenko for multiple suggestions on the paper, help during review process, and funding initial stages of
this project.
We thank anonymous reviewers and program chair for discussion that helped us to improve and
strengthen the paper.
REFERENCES
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,
Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent
Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg,
Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on
heterogeneous systems, 2015. URL https://www.tensorflow.org/ . Software available
from tensorﬂow.org.
Charu C Aggarwal et al. Neural networks and deep learning. Springer , 10:978–3, 2018.
Yoshua Bengio, Ian Goodfellow, and Aaron Courville. Deep learning , volume 1. MIT press
Massachusetts, USA:, 2017.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL
http://github.com/google/jax .
Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu,
Chiyuan Zhang, and Zheng Zhang. Mxnet: A ﬂexible and efﬁcient machine learning library for
heterogeneous distributed systems. arXiv preprint arXiv:1512.01274 , 2015.
Eric M. Christiansen and Stephan Hoyer. tf.contrib.labeled_tensor.
https://github.com/tensorflow/tensorflow/commit/
/nine.tosfd/two.tosf/zero.tosff/four.tosfea/four.tosfb/zero.tosfb/five.tosf/seven.tosf/nine.tosf/two.tosfbf/eight.tosf/eight.tosfef/eight.tosf/eight.tosf/six.tosfd/zero.tosf/one.tosf/four.tosf/three.tosfb/seven.tosfaa/seven.tosf/eight.tosf/zero.tosf/five.tosf/two.tosf/two.tosf , 2016. [Online; accessed 1-Oct-
2021].
Data-Apis. Consortium for Python Data API Standards, Array API standard. https://github.
com/data-apis/array-api/ , 2021. [Online; accessed 1-Oct-2021].
David Foster. Generative deep learning: teaching machines to paint, write, compose, and play .
O’Reilly Media, 2019.
Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David
Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti
Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández
del Río, Mark Wiebe, Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy,
Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming
with NumPy. Nature , 585(7825):357–362, September 2020. doi: 10.1038/s41586-020-2649-2.
URL https://doi.org//one.tosf/zero.tosf./one.tosf/zero.tosf/three.tosf/eight.tosf/s/four.tosf/one.tosf/five.tosf/eight.tosf/six.tosf-/zero.tosf/two.tosf/zero.tosf-/two.tosf/six.tosf/four.tosf/nine.tosf-/two.tosf .
10Published as a conference paper at ICLR 2022
Qibin Hou, Zihang Jiang, Li Yuan, Ming-Ming Cheng, Shuicheng Yan, and Jiashi Feng. Vision permu-
tator: A permutable mlp-like architecture for visual recognition. arXiv preprint arXiv:2106.12368 ,
2021.
Stephan Hoyer and Joe Hamman. xarray: N-D labeled arrays and datasets in Python. Journal of Open
Research Software , 5(1), 2017. doi: 10.5334/jors.148. URL http://doi.org//one.tosf/zero.tosf./five.tosf/three.tosf/three.tosf/four.tosf/
jors./one.tosf/four.tosf/eight.tosf .
Yu-Hsiang Huang. Implementation of "attention is all you need" paper. https:
//github.com/jadore/eight.tosf/zero.tosf/one.tosf/one.tosf/two.tosf/zero.tosf/attention-is-all-you-need-pytorch/blob/
/two.tosf/zero.tosf/seven.tosf/seven.tosf/five.tosf/one.tosf/five.tosfa/eight.tosfab/two.tosf/four.tosff/four.tosfabdda/nine.tosf/zero.tosf/eight.tosf/nine.tosfc/five.tosf/zero.tosf/two.tosffa/one.tosf/four.tosff/three.tosf/two.tosffc/five.tosfd/nine.tosf/transformer/SubLayers.py ,
2018. [Online; accessed 1-Oct-2021].
Kenneth E Iverson. A programming language. In Proceedings of the May 1-3, 1962, spring joint
computer conference , pp. 345–351, 1962.
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate
protein structure prediction with alphafold. Nature , 596(7873):583–589, 2021.
Diederik P. Kingma and Prafulla Dhariwal. Glow, ofﬁcial implementation. https://github.com/
openai/glow/blob//nine.tosf/one.tosfb/two.tosfc/five.tosf/seven.tosf/seven.tosfa/five.tosfc/one.tosf/one.tosf/zero.tosfb/two.tosfb/three.tosf/eight.tosf/seven.tosf/six.tosf/one.tosffc/five.tosf/six.tosfd/eight.tosf/one.tosff/seven.tosfd/eight.tosf/seven.tosff/zero.tosf/seven.tosf/seven.tosfc/one.tosf/tfops.py ,
2018. [Online; accessed 1-Oct-2021].
Oleksii Kuchaiev, Jason Li, Huyen Nguyen, Oleksii Hrinchuk, Ryan Leary, Boris Ginsburg, Samuel
Kriman, Stanislav Beliaev, Vitaly Lavrukhin, Jack Cook, et al. Nemo: a toolkit for building ai
applications using neural modules. arXiv preprint arXiv:1909.09577 , 2019.
Hanxiao Liu, Zihang Dai, David So, and Quoc Le. Pay attention to mlps. Advances in Neural
Information Processing Systems , 34, 2021.
Matlab. Toolbox, symbolic math and others. Mathworks Inc , 1993.
namedtensor. namedtensor python package. https://github.com/harvardnlp/
namedtensor , 2019. [Online; accessed 1-Oct-2021].
Numpy. Numpy broadcasting rules. https://numpy.org/doc/stable/user/basics.
broadcasting.html , 2021. [Online; accessed 1-Oct-2021].
Ryosuke Okuta, Yuya Unno, Daisuke Nishino, Shohei Hido, and Crissman Loomis. Cupy: A numpy-
compatible library for nvidia gpu calculations. In Proceedings of Workshop on Machine Learning
Systems (LearningSys) in The Thirty-ﬁrst Annual Conference on Neural Information Process-
ing Systems (NIPS) , 2017. URL http://learningsys.org/nips/one.tosf/seven.tosf/assets/papers/
paper_/one.tosf/six.tosf.pdf .
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-
performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc,
E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32 , pp.
8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
/nine.tosf/zero.tosf/one.tosf/five.tosf-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf.
Adam Paszke, Daniel D. Johnson, David Duvenaud, Dimitrios Vytiniotis, Alexey Radul, Matthew J.
Johnson, Jonathan Ragan-Kelley, and Dougal Maclaurin. Getting to the point: Index sets and
parallelism-preserving autodiff for pointful array programming. Proc. ACM Program. Lang. , 5
(ICFP), aug 2021. doi: 10.1145/3473593. URL https://doi.org//one.tosf/zero.tosf./one.tosf/one.tosf/four.tosf/five.tosf//three.tosf/four.tosf/seven.tosf/three.tosf/five.tosf/nine.tosf/three.tosf .
Pytorch. Documentation for Pytorch named tensors. https://pytorch.org/docs/stable/
named_tensor.html , 2019. [Online; accessed 1-Oct-2021].
11Published as a conference paper at ICLR 2022
Seiya Tokui, Ryosuke Okuta, Takuya Akiba, Yusuke Niitani, Toru Ogawa, Shunta Saito, Shuji Suzuki,
Kota Uenishi, Brian V ogel, and Hiroyuki Yamazaki Vincent. Chainer: A deep learning framework
for accelerating the research cycle. In Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining , pp. 2002–2011, 2019.
Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-
terthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An
all-mlp architecture for vision. Advances in Neural Information Processing Systems , 34, 2021.
Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard
Grave, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, and Hervé Jégou. Resmlp: Feedforward
networks for image classiﬁcation with data-efﬁcient training. arXiv preprint arXiv:2105.03404 ,
2021.
Guido Van Rossum and Fred L. Drake. Python 3 Reference Manual . CreateSpace, Scotts Valley, CA,
2009. ISBN 1441412697.
Nicolas Vasilache, Oleksandr Zinenko, Theodoros Theodoridis, Priya Goyal, Zachary DeVito,
William S Moses, Sven Verdoolaege, Andrew Adams, and Albert Cohen. Tensor comprehen-
sions: Framework-agnostic high-performance machine learning abstractions. arXiv preprint
arXiv:1802.04730 , 2018.
Nicolas Weber and Michael Goesele. Matog: Array layout auto-tuning for cuda. ACM Transactions
on Architecture and Code Optimization (TACO) , 14(3):1–26, 2017.
Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:
Introducing convolutions to vision transformers. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pp. 22–31, 2021.
12Published as a conference paper at ICLR 2022
ADISCUSSION OF SUPPORT FOR COMMON DL BLOCKS IN LABELED TENSORS
While most of labeled tensors packages were targeted at deep learning, none of them proposed
integration of DL-speciﬁc blocks like convolutions. For illustrative purpose let us discuss different
possible scenarios how labeled axes could be added to convolution:
•Should convolution stick to existing behavior and rely on axes order not names? If so, we get no
additional checks, but create potential problems since axes names are required in other operations
(e.g. reductions), but users should drop the names before every convolution, and reorder axes
if necessary. Next, users have to set axes for convolution output, in agreement with input axes
labels.
•Should convolution rely on axes names? If so, should axes names be ﬁxed? For instance,
convolution may require input to have dimensions (“channel”, “height”, “width”) while all the
other dimensions could be considered “batch-like” and preserved in the output? In this case,
almost all the tensors will have the same axes names, thus name checks become meaningless. If
one decides to apply attention or e.g. recurrent unit, each switch would require renaming axes.
•Should output names be predeﬁned? Thus all tensors produced by convolutions would have the
same labels, still preventing meaningful checks. That seems to produce more potential confusion
than help.
•Should input axes names be propagated to output? If yes – which names should be propagated?
Output channels at ﬁrst seem to be disconnected from input ones. However, in practice opposite
is equally frequent: in depth-wise convolution each input has one-to-one correspondence with
output. Setting up common rules for propagation of “height” and “width” labels meets similar
questions and exceptions: compare padding/equal.tosf 'same 'andpadding/equal.tosf 'valid '.
Similar questions arise for other “building blocks” of DL.
The broader problem is to come up with a concept of axis “label” that is indeed helpful in development,
i.e. sets restrictions that help in detecting or preventing coding errors, while introducing limited
performance and coding overhead.
All design choices taken (including support of various DL blocks) should be in agreement with each
other, guided by some high-level logic that works for multiple models and applications. This is hardly
feasible without a well-deﬁned concept of axis “label”, which was not proposed by implementations.
B E INOPS AND EINSUM
einops notation is largely inspired by numpy.einsum and extends it for single-tensor transformations:
in Listing 1 only 2 operations of 17 (transpose and swap axes) can be implemented with einsum .
We design interfaces in einops to be aligned with numpy.einsum for simultaneous usage. In the
case study of multi-head attention, Appendix E, we demonstrate how they should interact together.
Overall, einops implementation deviates from numpy.einsum in the following:
•einops supports arbitrary reductions (max-, min-, sum-, mean-, logaddexp-reductions as well
as callables to support any other reduction) while einsum can be used to perform sum-reduction
only.
•While einsum allows only one-character names for axes, einops supports multi-character ones:
digits, underscore, and arbitrary capitalization are allowed, while space is used as a delimiter.
For example, in einsum pattern 'hw c -/greater.tosf c hw 'transposes 3-dimensional tensor, while this is an
operation on 2-dimensional tensors in einops .
•While einsum implementations suffer from non-aligned support for spaces between names and
capitals, einops notation is backend-independent.
In addition to these discrepancies, we introduce several new concepts in einops which are absent in
numpy.einsum :
•Composition and decomposition of axes , a new core functionality described in Section 4.
13Published as a conference paper at ICLR 2022
•Unitary axes : in any pattern a user can use /one.tosfto reﬂect axes of size one.
• Speciﬁcation of axes size and veriﬁcation of shapes and divisibility.
•Anonymous axes : axes that are present only on lhs or rhs can be speciﬁed with just size in
the pattern. For example: repeat(x, 'h w -/greater.tosf h w c ', c/equal.tosf/three.tosf) can be repeat(x, 'h w -/greater.tosf h w /three.tosf ').
Note that unitary axes are not a sub-case of anonymous axes.
•Introduction of new axes with einops.repeat by repeating elements along the axis. We demon-
strate that this operation allows einops to support repeats and tiles.
•List inputs .einops accepts a list of tensors with identical shapes and dtypes, which are
stacked. For example, rearrange([r, g, b, alpha], 'channel h w -/greater.tosf h w channel '). This
extends einops to support tensor stacking and concatenations.
•Layers .einops functions have layer counterparts, e.g. einops.layers.torch.Rearrange( 'b c h
w -/greater.tosf b (c h w) ')can be used inside Sequential as one of the modules.9
and at the same time we do not implement some features of numpy.einsum :
•Axes repeats on lhs . While einsum allows writing e.g. 'ii-/greater.tosf 'to get a matrix trace we dropped
this in einops to prevent coding mistakes.
•Multi-tensor transformations . While there is an einops layer that operates on two tensors, the
core functionality of einops used by most users is one-tensor transformations. However, einops
novelties (new axes, anonymous axes, unitary axes, speciﬁcation of axes sizes) can readily be
applied to extend einsum .
•Implicit indexing . This feature relies on characters sorting, does not encourage descriptive axes
names as those deﬁne output axes order, and makes output implicit, not explicit. Implicit indexing
also cannot be applied to einops.reduce andeinops.repeat . There is a negligible usage ( <1%)
of list-inputs syntax in numpy.einsum , and it is not added to einops .
C F ORMAL GRAMMAR OF EINOPS PATTERNS
To accommodate differences, covered in appendix B, but still keep similarity with numpy.einsum ,
einops patterns follow a formal grammar deﬁned in Listing 2. This grammar signiﬁcantly evolved
from the prototype stage of einops notation, when we performed a user study described in Ap-
pendix D.
transformation_pattern /equal.tosf tensor_pattern, '-/greater.tosf', tensor_pattern ;
tensor_pattern /equal.tosf [delimiter], axis_group, { ',', delimiter, axis_group}, [delimiter] ;
axis_group /equal.tosf axis | "(", [delimiter], ")" | ( [delimiter], axis, {delimiter, axis},
[delimiter] ) ;
axis /equal.tosf '...'| unitary_axis | anonymous_axes | named_axis ;
unitary_axis /equal.tosf '/one.tosf';
delimiter /equal.tosf ' ', {' '} ;
named_axis /equal.tosf /less.tosfany python identifier that does not start or end with underscore/greater.tosf ;
anonymous_axis /equal.tosf /less.tosfnatural number greater than one/greater.tosf ;
Listing 2: Extended Backus–Naur Form grammar of einops pattern notation for a single tensor.
D C ONFIRMATION OF READABILITY AND INTUITIVENESS
At the time of the ﬁrst einops prototype, the idea of using operations with named axes tied to
operation itself not to a tensor was not around within DL. Besides, among DL frameworks only
pytorch had reasonable support for einsum (with multiple performance issues reported).
That is why at the prototype stage we developed a test to probe several design choices and to answer
the following questions:
• Is the notation easy to pick up?
9In examples from pytorch documentation (otherwise Sequential ) modules are implemented using custom
“forward” to accomplish this transformation with x.view orx.flatten .
14Published as a conference paper at ICLR 2022
• Does the notation require any speciﬁc introduction?
• Can users recognize operations without a context?
Speciﬁcally, 8 subjects with sufﬁcient (>1 year) exposure to tensor programming in at least one of
the DL frameworks were asked to complete the following questionnaire: subjects were not provided
with any description of function, or implementation, or other context.10
/numbersign.tosf All imports are ignored. What is the output of each print statement?
x /equal.tosf torch.zeros(/one.tosf/zero.tosf, /two.tosf/zero.tosf, /three.tosf/zero.tosf, /four.tosf/zero.tosf)
y /equal.tosf transpose(x, 'bhwc-/greater.tosfbchw ')
print(y.shape) /numbersign.tosf Q/one.tosf
y /equal.tosf transpose(x, 'bhwc-/greater.tosfbc(hw) ')
print(y.shape) /numbersign.tosf Q/two.tosf
y /equal.tosf transpose(x, 'bhw(c,h/one.tosf,w/one.tosf)-/greater.tosfb(h,h/one.tosf)(w,w/one.tosf)c ', h/one.tosf/equal.tosf/two.tosf, w/one.tosf/equal.tosf/two.tosf)
print(y.shape) /numbersign.tosf Q/three.tosf
y /equal.tosf transpose(x, 'b(h,h/one.tosf)(w,w/one.tosf)c-/greater.tosfbhw(h/one.tosfw/one.tosfc) ', h/one.tosf/equal.tosf/two.tosf, w/one.tosf/equal.tosf/two.tosf)
print(y.shape) /numbersign.tosf Q/four.tosf
y/one.tosf, y/two.tosf /equal.tosf transpose(x, 'bhw(c,g)-/greater.tosfgbhwc ', g/equal.tosf/two.tosf)
print(y/one.tosf.shape, y/two.tosf.shape) /numbersign.tosf Q/five.tosf
y /equal.tosf transpose(x, 'b/one.tosfsb/two.tosft-/greater.tosfb/one.tosfb/two.tosfst ')
print(y.shape) /numbersign.tosf Q/six.tosf
/numbersign.tosf operator @ is matrix multiplication
t /equal.tosf transpose(x, 'bchw-/greater.tosf(bhw)c ') @ torch.rand(/two.tosf/zero.tosf,/five.tosf/zero.tosf)
print(t.shape) /numbersign.tosf Q/seven.tosf
y /equal.tosf transpose(t, '(bhw)c/two.tosf-/greater.tosfbc/two.tosfhw ', b_hw/equal.tosfx.shape) /numbersign.tosf first operand is t
print(y.shape) /numbersign.tosf Q/eight.tosf
y /equal.tosf transpose(t, '(bhw)c/two.tosf-/greater.tosfbc/two.tosfhw ', b/equal.tosf/three.tosf/zero.tosf, h/equal.tosf/one.tosf/zero.tosf)
print(y.shape) /numbersign.tosf Q/nine.tosf
This user study had some additional ideas that were rejected later: comma as a delimiter within
parenthesis (non-parenthesized commas conﬂict with numpy.einsum ) and parsing of multiple shape
arguments (e.g. b_hw/equal.tosfx.shape ). Even some participants who correctly answered corresponding
questions (5 participants of 8 in both cases) reported they had initial confusion with these features.
Only two participants were aware about einsum function but not used that in their coding practice.
Questions (Q1, Q2, Q6, Q7, Q9) were correctly answered by all participants. This conﬁrms that
notation based on named axes is intuitive and can be picked up without a special introduction.
Post-questionnaire interview showed that participants could describe a particular context when an
operation (e.g. Q1, Q3) could be used.
While single-character variables names (with an optional digit) did not cause confusion, we have
replaced this due to conﬂicts with potential future features (e.g. unitary axes). Function name
transpose was also changed to rearrange to avoid conﬂicts with existing operations in different DL
frameworks.
However the real value of design choices in einops can be conﬁrmed in a long time-scale and within
large codebases, settings that can’t be captured by small user studies.
10Some participants had no experience with pytorch and they were informed that x.shape /equal.tosf [/one.tosf/zero.tosf,/two.tosf/zero.tosf,/three.tosf/zero.tosf,/four.tosf/zero.tosf].
15Published as a conference paper at ICLR 2022
E C ASE STUDY :MULTI -HEAD ATTENTION
Below we provide a shortened implementation of multi-head attention based on Huang (2018). For
brevity, we remove weights initialization and mask support.
1class ScaledDotProductAttention(nn.Module):
2 def __init__(self, temperature, attn_dropout/equal.tosf/zero.tosf./one.tosf):
3 super().__init__()
4 self.temperature /equal.tosf temperature
5 self.dropout /equal.tosf nn.Dropout(attn_dropout)
6 self.softmax /equal.tosf nn.Softmax(dim/equal.tosf/two.tosf)
7
8 def forward(self, q, k, v):
9 attn /equal.tosf torch.bmm(q, k.transpose(/one.tosf, /two.tosf)) / self.temperature
10 attn /equal.tosf self.softmax(attn)
11 attn /equal.tosf self.dropout(attn)
12 output /equal.tosf torch.bmm(attn, v)
13
14 return output, attn
15
16
17class MultiHeadAttention(nn.Module):
18 def __init__(self, n_head, d_model, d_k, d_v, dropout/equal.tosf/zero.tosf./one.tosf):
19 super().__init__()
20 self.n_head /equal.tosf n_head
21 self.d_k /equal.tosf d_k
22 self.d_v /equal.tosf d_v
23
24 self.w_qs /equal.tosf nn.Linear(d_model, n_head * d_k)
25 self.w_ks /equal.tosf nn.Linear(d_model, n_head * d_k)
26 self.w_vs /equal.tosf nn.Linear(d_model, n_head * d_v)
27 self.attention /equal.tosf ScaledDotProductAttention(temperature/equal.tosfnp.power(d_k, /zero.tosf./five.tosf))
28 self.layer_norm /equal.tosf nn.LayerNorm(d_model)
29 self.fc /equal.tosf nn.Linear(n_head * d_v, d_model)
30 self.dropout /equal.tosf nn.Dropout(dropout)
31
32
33 def forward(self, q, k, v):
34 d_k, d_v, n_head /equal.tosf self.d_k, self.d_v, self.n_head
35
36 sz_b, len_q, _ /equal.tosf q.size()
37 sz_b, len_k, _ /equal.tosf k.size()
38 sz_b, len_v, _ /equal.tosf v.size()
39
40 residual /equal.tosf q
41
42 q /equal.tosf self.w_qs(q).view(sz_b, len_q, n_head, d_k)
43 k /equal.tosf self.w_ks(k).view(sz_b, len_k, n_head, d_k)
44 v /equal.tosf self.w_vs(v).view(sz_b, len_v, n_head, d_v)
45
46 q /equal.tosf q.permute(/two.tosf, /zero.tosf, /one.tosf, /three.tosf).contiguous().view(-/one.tosf, len_q, d_k) /numbersign.tosf (n*b) x lq x dk
47 k /equal.tosf k.permute(/two.tosf, /zero.tosf, /one.tosf, /three.tosf).contiguous().view(-/one.tosf, len_k, d_k) /numbersign.tosf (n*b) x lk x dk
48 v /equal.tosf v.permute(/two.tosf, /zero.tosf, /one.tosf, /three.tosf).contiguous().view(-/one.tosf, len_v, d_v) /numbersign.tosf (n*b) x lv x dv
49
50 output, attn /equal.tosf self.attention(q, k, v)
51
52 output /equal.tosf output.view(n_head, sz_b, len_q, d_v)
53 output /equal.tosf output.permute(/one.tosf, /two.tosf, /zero.tosf, /three.tosf).contiguous().view(sz_b, len_q, -/one.tosf) /numbersign.tosf b x lq
x (n*dv)
54
55 output /equal.tosf self.dropout(self.fc(output))
56 output /equal.tosf self.layer_norm(output /plus.tosf residual)
57
58 return output, attn
16Published as a conference paper at ICLR 2022
Original implementation demonstrates a number of issues that we previously discussed: unchecked
and hard-to-track transformations, necessity to keep comments about the shape. Frequent usage of
-1 in reshapes makes it simple to introduce errors. The critical part of computations (attention) is
ofﬂoaded to a separate module, which does not check input and which is not documented: dimen-
sions of inputs and outputs are not deﬁned, just as the connection between them. Thus, provided
ScaledDotProductAttention cannot be considered as an independent, self-containing or reusable
module.
Inlining of ScaledDotProductAttention inside MultiHeadAttention could improve the situation, but
would also make the problem with shapes more obvious, as more comments would be required for
inlined variables.
We can compare that with einops implementation, where all computations are done in a single
module. Each axis is easy to track throughout the code. An axis index is needed three times (explicitly
insoftmax , implicitly in fcand layer_norm ), but those are easy to ﬁnd from the code without any
additional comments – which demonstrates how einops “implicitly annotates” code in the proximity.
1class MultiHeadAttentionNew(nn.Module):
2 def __init__(self, n_head, d_model, d_k, d_v, dropout/equal.tosf/zero.tosf./one.tosf):
3 super().__init__()
4 self.n_head /equal.tosf n_head
5
6 self.w_qs /equal.tosf nn.Linear(d_model, n_head * d_k)
7 self.w_ks /equal.tosf nn.Linear(d_model, n_head * d_k)
8 self.w_vs /equal.tosf nn.Linear(d_model, n_head * d_v)
9 self.fc /equal.tosf nn.Linear(n_head * d_v, d_model)
10
11 self.dropout /equal.tosf nn.Dropout(p/equal.tosfdropout)
12 self.attn_dropout /equal.tosf nn.Dropout(p/equal.tosf/zero.tosf./one.tosf)
13 self.layer_norm /equal.tosf nn.LayerNorm(d_model)
14
15 def forward(self, q, k, v):
16 residual /equal.tosf q
17 q /equal.tosf rearrange(self.w_qs(q), 'b l (h k) -/greater.tosf h b l k ', h/equal.tosfself.n_head)
18 k /equal.tosf rearrange(self.w_ks(k), 'b t (h k) -/greater.tosf h b t k ', h/equal.tosfself.n_head)
19 v /equal.tosf rearrange(self.w_vs(v), 'b t (h v) -/greater.tosf h b t v ', h/equal.tosfself.n_head)
20 attn /equal.tosf torch.einsum( 'hblk,hbtk-/greater.tosfhblt ', [q, k]) / np.sqrt(q.shape[-/one.tosf])
21 attn /equal.tosf self.attn_dropout(attn.softmax(dim/equal.tosf-/one.tosf))
22 output /equal.tosf torch.einsum( 'hblt,hbtv-/greater.tosfhblv ', [attn, v])
23 output /equal.tosf rearrange(output, 'h b l v -/greater.tosf b l (h v) ')
24 output /equal.tosf self.dropout(self.fc(output))
25 output /equal.tosf self.layer_norm(output /plus.tosf residual)
26 return output, attn
17Published as a conference paper at ICLR 2022
F S QUEEZE EXAMPLE FROM GLOW WITH REVERSE TRANSFORMATION
Squeeze and unsqueeze implementations are derived from OpenAI’s implementation of Glow
(Kingma & Dhariwal, 2018).
def squeeze/two.tosfd(x, factor/equal.tosf/two.tosf):
assert factor /greater.tosf/equal.tosf /one.tosf
if factor /equal.tosf/equal.tosf /one.tosf:
return x
shape /equal.tosf x.get_shape()
height /equal.tosf int(shape[/one.tosf])
width /equal.tosf int(shape[/two.tosf])
n_channels /equal.tosf int(shape[/three.tosf])
assert height /percent.tosf factor /equal.tosf/equal.tosf /zero.tosf and width /percent.tosf factor /equal.tosf/equal.tosf /zero.tosf
x /equal.tosf tf.reshape(x, [-/one.tosf, height//factor, factor,
width//factor, factor, n_channels])
x /equal.tosf tf.transpose(x, [/zero.tosf, /one.tosf, /three.tosf, /five.tosf, /two.tosf, /four.tosf])
x /equal.tosf tf.reshape(x, [-/one.tosf, height//factor, width //
factor, n_channels*factor*factor])
return x
def unsqueeze/two.tosfd(x, factor/equal.tosf/two.tosf):
assert factor /greater.tosf/equal.tosf /one.tosf
if factor /equal.tosf/equal.tosf /one.tosf:
return x
shape /equal.tosf x.get_shape()
height /equal.tosf int(shape[/one.tosf])
width /equal.tosf int(shape[/two.tosf])
n_channels /equal.tosf int(shape[/three.tosf])
assert n_channels /greater.tosf/equal.tosf /four.tosf and n_channels /percent.tosf /four.tosf /equal.tosf/equal.tosf /zero.tosf
x /equal.tosf tf.reshape(
x, (-/one.tosf, height, width, int(n_channels/factor**/two.tosf), factor, factor))
x /equal.tosf tf.transpose(x, [/zero.tosf, /one.tosf, /four.tosf, /two.tosf, /five.tosf, /three.tosf])
x /equal.tosf tf.reshape(x, (-/one.tosf, int(height*factor),
int(width*factor), int(n_channels/factor**/two.tosf)))
return x
einops counterparts for both functions can be written as
rearrange(x, 'b (c h/two.tosf w/two.tosf) h w -/greater.tosf b c (h h/two.tosf) (w w/two.tosf) ', h/two.tosf/equal.tosffactor, w/two.tosf/equal.tosffactor)
rearrange(x, 'b c (h h/two.tosf) (w w/two.tosf) -/greater.tosf b (c h/two.tosf w/two.tosf) h w ', h/two.tosf/equal.tosffactor, w/two.tosf/equal.tosffactor)
where one can readily see from the patterns that the second operation is inverse to the ﬁrst one.
G E INOPS PERFORMANCE
To demonstrate that the overhead brought by einops on top of DL framework is negligible we
measure performance of several case studies. We compare original pytorch implementations and their
einops versions in the following scenarios, see Table 1: CPU or CUDA backends, with enabled or
disabled JIT (just-in-time compilation), different input tensor sizes. In our performance benchmark
we use einops 0.3.2, pytorch 1.7.1+cu110, CUDA 11.0. We use AWS EC2 p3.2xlarge instance
for benchmarks. The average time of the forward pass is measured in milliseconds by IPython’s
module timeit . For multi-head attention case study, see Appendix E, einops implementation uses
pytorch einsum operation which gives additional overhead. That is why, for this case study we
consider einops implementation with and without einsum . For the case of unsqueeze, we use
popular open-source port to pytorch https://github.com/chaiyujin/glow-pytorch .
From these benchmarks even einops -rich code works on a similar range of speeds as pytorch-only.
In particular, unsqueeze2d consists only of einops operation, shows similar speed both on CPU for
various input sizes and GPU with larger input sizes. Since computationally most expensive operations
are convolutions and tensor products (linear layers, attention), not rearrangements, this overhead
will typically have marginal contribution to the result. Thus, attention and permutator are closer to
18Published as a conference paper at ICLR 2022
practical use cases, and demonstrate that difference in performance becomes notable only for small
inputs when GPU is used.
Table 1: Performance comparison between original pytorch implementation and its einops version
for different case studies. Performance is measured in ms via IPython’s timeit module.
Case Study Input Size Impl.CPU Backend CUDA Backend
w/o JIT JIT w/o JIT JIT
attention(32, 64, 512)einops 21.85±0.04 21.13±0.03 1.30±0.00 1.05±0.00
w/o einsum 21.31±0.04 20.99±0.06 1.14±0.00 0.90±0.00
original 21.09±0.03 20.85±0.05 1.02±0.00 0.71±0.00
(32, 128, 512)einops 48.06±0.06 47.37±0.05 1.39±0.00 1.30±0.00
w/o einsum 46.94±0.04 46.28±0.05 1.28±0.00 1.29±0.00
original 46.87±0.08 46.33±0.04 1.29±0.00 1.28±0.00
(32, 256, 512)einops 150.40±0.18 149.53±0.17 2.78±0.00 2.79±0.00
w/o einsum 146.01±0.20 144.96±0.19 2.75±0.00 2.76±0.00
original 145.19±2.07 145.94±0.21 2.75±0.00 2.75±0.00
(32, 512, 512)einops 495.31±1.19 493.90±0.60 6.51±0.00 6.53±0.01
w/o einsum 488.37±0.93 487.16±0.96 6.47±0.00 6.48±0.01
original 491.20±2.79 488.64±0.23 6.45±0.01 6.46±0.00
permutator(32, 32, 32, 32)einops 7.05±0.01 7.03±0.01 0.52±0.00 0.52±0.00
original 7.08±0.02 7.05±0.02 0.44±0.00 0.40±0.00
(64, 64, 64, 64)einops 237.53±1.67 237.05±0.24 2.51±0.00 2.56±0.00
original 234.57±0.17 234.66±0.21 2.50±0.01 2.54±0.00
unsqueeze2d(32, 32, 32, 32)einops 1.68±0.00 1.73±0.00 0.07±0.00 0.13±0.00
original 1.66±0.00 1.72±0.00 0.05±0.00 0.09±0.00
(32, 64, 64, 64)einops 16.91±0.02 17.01±0.04 0.14±0.00 0.19±0.00
original 16.89±0.03 17.14±0.02 0.12±0.00 0.14±0.00
(32, 128, 128, 128)einops 129.53±0.05 129.95±0.10 0.71±0.00 0.76±0.00
original 129.16±0.16 131.44±0.12 0.69±0.00 0.72±0.00
H R OLE OF CACHING
To estimate the role of caching we conduct a study with very small numpy arrays, where we generate
104patterns performing the same transformation using IPython’s timeit functionality. Cache can’t
accommodate all these patterns and runs syntax parsing, validation and inference of dimensions.
from einops import rearrange
import numpy as np
patterns /equal.tosf [
f'i{i} (j k) l m ... -/greater.tosf i{i} j (k l) (m ...) '
for i in range(/one.tosf/zero.tosf_/zero.tosf/zero.tosf/zero.tosf)
]
x /equal.tosf np.zeros([/two.tosf, /two.tosf, /two.tosf, /two.tosf, /two.tosf]) /numbersign.tosf small tensor
/numbersign.tosf caching mechanism is not used
/percent.tosf/percent.tosftimeit
for pattern in patterns:
rearrange(x, pattern, j/equal.tosf/two.tosf)
/numbersign.tosf caching is in use
/percent.tosf/percent.tosftimeit
pattern /equal.tosf patterns[/zero.tosf]
for _pattern in patterns:
rearrange(x, pattern, j/equal.tosf/two.tosf)
19Published as a conference paper at ICLR 2022
Outputs for the two cases show more than 10-fold difference:
• without caching: 663 ms ± 20.9 ms per loop (mean ± std. dev. of 7 runs)
• with caching: 37.1 ms ± 165 µs per loop (mean ± std. dev. of 7 runs)
Times were measured on Macbook Pro 2018 using Python 3.9.0, numpy 1.20.3 and einops 0.3.2.
I E INOPS FLEXIBILITY AND APPLICABILITY
To conﬁrm wide applicability of einops , we search for Github code in pytorch that actively relies on
view/reshape /transpose /permute . We restrictively consider only popular (>1000 stars) repositories.
Ofﬁcial documentation, examples and torchvision are also included (all three repositories pass the
previous criterion). We sample 16 fragments that cover a diverse set of applications and architectures
and also smaller fragments from the same repositories, see Table 2.
Table 2: Comparison between original implementation and rewritten version with einops measured
in number of code lines and characters.
Fragment# Lines # Characters
Original einops Original einops
LeNet-like network 19 15 626 352
Super-resolution 17 11 678 478
Style transfer 6 3 192 107
LSTM and language modeling 15 16 745 753
LSTM token-by-token 31 22 1266 990
ShufﬂeNet 126 43 4007 1932
ResNet 58 42 2106 1658
FastText 24 8 723 281
CNN for text classiﬁcation 43 16 1767 799
Tacotron 36 23 818 662
Transformer’s attention 78 32 2637 1538
Self-attention GANs 34 19 1506 959
Time-sequence prediction 27 25 1125 1015
Spacial transformer network 36 27 1099 1012
Highway convolutions 6 6 256 239
GLOW 30 5 917 250
These examples are rewritten in einops and torch.einsum : interface of models is kept identical to
the original implementation to demonstrate that einops does not demand global code adjustments.
In addition, introduction of einops layers allows to avoid new classes and to reuse nn.Sequential in
four cases (LeNet, Super-resolution, FastText, ResNet).
Resulting code in einops is shorter, see Table 2, while we use fewer method chains and do not pack
multiple operations in a single line. Out of 16 fragments, in one case length (measured in number
of code lines or in number of characters – results are identical) is higher than in original (LSTM
and language modeling), in one case is identical (Highway convolutions), and in all other cases is
tangibly lower.11
11Side-to-side comparison between original code and einops version with comments can be found online
https://einops.rocks/pytorch-examples.html
20Published as a conference paper at ICLR 2022
J E INOPS ADOPTION
When it comes to estimating suitability of notation (or any other tool) to help in research, one can
try to dissect this question into multiple small questions: how many different cases are covered by
notation? can it be implemented? can it be integrated with existing tools? is resulting code shorter?
how fast is resulting code? is modiﬁcation with notation easier or faster? While we cover in our
studies multiple such factors, this “dissection” does not provide holistic view on role and integration
into existing workﬂow and tooling.
einops has taken a “test by time” approach. Developed, implemented and open-sourced in 2018,
it got adoption in public projects in majority of largest AI laboratories, which is a strong indicator
of novelty, usefulness and reliability. For context, pytorch claims to be user-focused Paszke et al.
(2019). In keynote talk12they conﬁrm poor relevance of micro-benchmarks and rely on adoption as
a (lagging) conﬁrmation of design choices.
As of November, 2021, Github reports more than 1000 usages of einops by other repositories with
increasing frequency (last 100 were published in 18 days). In the majority of cases they implement
approaches or DL models that were proposed after release of einops . Implementations use different
DL backends and span different modalities (even non-traditional applications like ﬂuid dynamics).
This inductive validation (if it was applicable to a large number of new cases, it should work in more)
is extremely time-inefﬁcient, but provides the most reliable proof. einops is used in open source
projects from AI laboratories, who use and maintain competing deep learning stacks. This forms
another important (though less statistically reliable) indication for adoption.
12Outline is available at https://soumith.ch/posts/2021/02/growing-opensource/.
21Generating Sequences With
Recurrent Neural Networks
Alex Graves
Department of Computer Science
University of Toronto
graves@cs.toronto.edu
Abstract
This paper shows how Long Short-term Memory recurrent neural net-
works can be used to generate complex sequences with long-range struc-
ture, simply by predicting one data point at a time. The approach is
demonstrated for text (where the data are discrete) and online handwrit-
ing (where the data are real-valued). It is then extended to handwriting
synthesis by allowing the network to condition its predictions on a text
sequence. The resulting system is able to generate highly realistic cursive
handwriting in a wide variety of styles.
1 Introduction
Recurrent neural networks (RNNs) are a rich class of dynamic models that have
been used to generate sequences in domains as diverse as music [6, 4], text [30]
and motion capture data [29]. RNNs can be trained for sequence generation by
processing real data sequences one step at a time and predicting what comes
next. Assuming the predictions are probabilistic, novel sequences can be gener-
ated from a trained network by iteratively sampling from the network's output
distribution, then feeding in the sample as input at the next step. In other
words by making the network treat its inventions as if they were real, much like
a person dreaming. Although the network itself is deterministic, the stochas-
ticity injected by picking samples induces a distribution over sequences. This
distribution is conditional, since the internal state of the network, and hence its
predictive distribution, depends on the previous inputs.
RNNs are `fuzzy' in the sense that they do not use exact templates from
the training data to make predictions, but rather|like other neural networks|
use their internal representation to perform a high-dimensional interpolation
between training examples. This distinguishes them from n-gram models and
compression algorithms such as Prediction by Partial Matching [5], whose pre-
dictive distributions are determined by counting exact matches between the
recent history and the training set. The result|which is immediately appar-
1arXiv:1308.0850v5  [cs.NE]  5 Jun 2014ent from the samples in this paper|is that RNNs (unlike template-based al-
gorithms) synthesise and reconstitute the training data in a complex way, and
rarely generate the same thing twice. Furthermore, fuzzy predictions do not suf-
fer from the curse of dimensionality, and are therefore much better at modelling
real-valued or multivariate data than exact matches.
In principle a large enough RNN should be sucient to generate sequences
of arbitrary complexity. In practice however, standard RNNs are unable to
store information about past inputs for very long [15]. As well as diminishing
their ability to model long-range structure, this `amnesia' makes them prone to
instability when generating sequences. The problem (common to all conditional
generative models) is that if the network's predictions are only based on the last
few inputs, and these inputs were themselves predicted by the network, it has
little opportunity to recover from past mistakes. Having a longer memory has
a stabilising eect, because even if the network cannot make sense of its recent
history, it can look further back in the past to formulate its predictions. The
problem of instability is especially acute with real-valued data, where it is easy
for the predictions to stray from the manifold on which the training data lies.
One remedy that has been proposed for conditional models is to inject noise into
the predictions before feeding them back into the model [31], thereby increasing
the model's robustness to surprising inputs. However we believe that a better
memory is a more profound and eective solution.
Long Short-term Memory (LSTM) [16] is an RNN architecture designed to
be better at storing and accessing information than standard RNNs. LSTM has
recently given state-of-the-art results in a variety of sequence processing tasks,
including speech and handwriting recognition [10, 12]. The main goal of this
paper is to demonstrate that LSTM can use its memory to generate complex,
realistic sequences containing long-range structure.
Section 2 denes a `deep' RNN composed of stacked LSTM layers, and ex-
plains how it can be trained for next-step prediction and hence sequence gener-
ation. Section 3 applies the prediction network to text from the Penn Treebank
and Hutter Prize Wikipedia datasets. The network's performance is compet-
itive with state-of-the-art language models, and it works almost as well when
predicting one character at a time as when predicting one word at a time. The
highlight of the section is a generated sample of Wikipedia text, which showcases
the network's ability to model long-range dependencies. Section 4 demonstrates
how the prediction network can be applied to real-valued data through the use
of a mixture density output layer, and provides experimental results on the IAM
Online Handwriting Database. It also presents generated handwriting samples
proving the network's ability to learn letters and short words direct from pen
traces, and to model global features of handwriting style. Section 5 introduces
an extension to the prediction network that allows it to condition its outputs on
a short annotation sequence whose alignment with the predictions is unknown.
This makes it suitable for handwriting synthesis, where a human user inputs
a text and the algorithm generates a handwritten version of it. The synthesis
network is trained on the IAM database, then used to generate cursive hand-
writing samples, some of which cannot be distinguished from real data by the
2Figure 1: Deep recurrent neural network prediction architecture. The
circles represent network layers, the solid lines represent weighted connections
and the dashed lines represent predictions.
naked eye. A method for biasing the samples towards higher probability (and
greater legibility) is described, along with a technique for `priming' the sam-
ples on real data and thereby mimicking a particular writer's style. Finally,
concluding remarks and directions for future work are given in Section 6.
2 Prediction Network
Fig. 1 illustrates the basic recurrent neural network prediction architecture used
in this paper. An input vector sequence x= (x1;:::;x T) is passed through
weighted connections to a stack of Nrecurrently connected hidden layers to
compute rst the hidden vector sequences hn= (hn
1;:::;hn
T) and then the
output vector sequence y= (y1;:::;y T). Each output vector ytis used to
parameterise a predictive distribution Pr( xt+1jyt) over the possible next inputs
xt+1. The rst element x1of every input sequence is always a null vector whose
entries are all zero; the network therefore emits a prediction for x2, the rst
real input, with no prior information. The network is `deep' in both space
and time, in the sense that every piece of information passing either vertically
or horizontally through the computation graph will be acted on by multiple
successive weight matrices and nonlinearities.
Note the `skip connections' from the inputs to all hidden layers, and from
all hidden layers to the outputs. These make it easier to train deep networks,
3by reducing the number of processing steps between the bottom of the network
and the top, and thereby mitigating the `vanishing gradient' problem [1]. In
the special case that N= 1 the architecture reduces to an ordinary, single layer
next step prediction RNN.
The hidden layer activations are computed by iterating the following equa-
tions fromt= 1 toTand fromn= 2 toN:
h1
t=H 
Wih1xt+Wh1h1h1
t 1+b1
h
(1)
hn
t=H 
Wihnxt+Whn 1hnhn 1
t+Whnhnhn
t 1+bn
h
(2)
where the Wterms denote weight matrices (e.g. Wihnis the weight matrix
connecting the inputs to the nthhidden layer, Wh1h1is the recurrent connection
at the rst hidden layer, and so on), the bterms denote bias vectors (e.g. byis
output bias vector) and His the hidden layer function.
Given the hidden sequences, the output sequence is computed as follows:
^yt=by+NX
n=1Whnyhn
t (3)
yt=Y(^yt) (4)
whereYis the output layer function. The complete network therefore denes
a function, parameterised by the weight matrices, from input histories x1:tto
output vectors yt.
The output vectors ytare used to parameterise the predictive distribution
Pr(xt+1jyt) for the next input. The form of Pr( xt+1jyt) must be chosen carefully
to match the input data. In particular, nding a good predictive distribution
for high-dimensional, real-valued data (usually referred to as density modelling ),
can be very challenging.
The probability given by the network to the input sequence xis
Pr(x) =TY
t=1Pr(xt+1jyt) (5)
and the sequence loss L(x) used to train the network is the negative logarithm
of Pr( x):
L(x) = TX
t=1log Pr(xt+1jyt) (6)
The partial derivatives of the loss with respect to the network weights can be
eciently calculated with backpropagation through time [33] applied to the
computation graph shown in Fig. 1, and the network can then be trained with
gradient descent.
2.1 Long Short-Term Memory
In most RNNs the hidden layer function His an elementwise application of a
sigmoid function. However we have found that the Long Short-Term Memory
4Figure 2: Long Short-term Memory Cell
(LSTM) architecture [16], which uses purpose-built memory cells to store infor-
mation, is better at nding and exploiting long range dependencies in the data.
Fig. 2 illustrates a single LSTM memory cell. For the version of LSTM used in
this paper [7]His implemented by the following composite function:
it=(Wxixt+Whiht 1+Wcict 1+bi) (7)
ft=(Wxfxt+Whfht 1+Wcfct 1+bf) (8)
ct=ftct 1+ittanh (Wxcxt+Whcht 1+bc) (9)
ot=(Wxoxt+Whoht 1+Wcoct+bo) (10)
ht=ottanh(ct) (11)
whereis the logistic sigmoid function, and i,f,oandcare respectively the
input gate ,forget gate ,output gate ,celland cell input activation vectors, all of
which are the same size as the hidden vector h. The weight matrix subscripts
have the obvious meaning, for example Whiis the hidden-input gate matrix,
Wxois the input-output gate matrix etc. The weight matrices from the cell
to gate vectors (e.g. Wci) are diagonal, so element min each gate vector only
receives input from element mof the cell vector. The bias terms (which are
added toi,f,cando) have been omitted for clarity.
The original LSTM algorithm used a custom designed approximate gradi-
ent calculation that allowed the weights to be updated after every timestep [16].
However the full gradient can instead be calculated with backpropagation through
time [11], the method used in this paper. One diculty when training LSTM
with the full gradient is that the derivatives sometimes become excessively large,
5leading to numerical problems. To prevent this, all the experiments in this pa-
per clipped the derivative of the loss with respect to the network inputs to the
LSTM layers (before the sigmoid and tanh functions are applied) to lie within
a predened range1.
3 Text Prediction
Text data is discrete, and is typically presented to neural networks using `one-
hot' input vectors. That is, if there are Ktext classes in total, and class kis fed
in at timet, thenxtis a length Kvector whose entries are all zero except for
thekth, which is one. Pr( xt+1jyt) is therefore a multinomial distribution, which
can be naturally parameterised by a softmax function at the output layer:
Pr(xt+1=kjyt) =yk
t=exp 
^yk
t
PK
k0=1exp 
^yk0
t (12)
Substituting into Eq. (6) we see that
L(x) = TX
t=1logyxt+1
t (13)
=)@L(x)
@^yk
t=yk
t k;xt+1 (14)
The only thing that remains to be decided is which set of classes to use. In
most cases, text prediction (usually referred to as language modelling ) is per-
formed at the word level. Kis therefore the number of words in the dictionary.
This can be problematic for realistic tasks, where the number of words (in-
cluding variant conjugations, proper names, etc.) often exceeds 100,000. As
well as requiring many parameters to model, having so many classes demands a
huge amount of training data to adequately cover the possible contexts for the
words. In the case of softmax models, a further diculty is the high computa-
tional cost of evaluating all the exponentials during training (although several
methods have been to devised make training large softmax layers more ecient,
including tree-based models [25, 23], low rank approximations [27] and stochas-
tic derivatives [26]). Furthermore, word-level models are not applicable to text
data containing non-word strings, such as multi-digit numbers or web addresses.
Character-level language modelling with neural networks has recently been
considered [30, 24], and found to give slightly worse performance than equiv-
alent word-level models. Nonetheless, predicting one character at a time is
more interesting from the perspective of sequence generation, because it allows
the network to invent novel words and strings. In general, the experiments in
this paper aim to predict at the nest granularity found in the data, so as to
maximise the generative exibility of the network.
1In fact this technique was used in all my previous papers on LSTM, and in my publicly
available LSTM code, but I forgot to mention it anywhere| mea culpa .
63.1 Penn Treebank Experiments
The rst set of text prediction experiments focused on the Penn Treebank por-
tion of the Wall Street Journal corpus [22]. This was a preliminary study whose
main purpose was to gauge the predictive power of the network, rather than to
generate interesting sequences.
Although a relatively small text corpus (a little over a million words in total),
the Penn Treebank data is widely used as a language modelling benchmark. The
training set contains 930,000 words, the validation set contains 74,000 words and
the test set contains 82,000 words. The vocabulary is limited to 10,000 words,
with all other words mapped to a special `unknown word' token. The end-of-
sentence token was included in the input sequences, and was counted in the
sequence loss. The start-of-sentence marker was ignored, because its role is
already fullled by the null vectors that begin the sequences (c.f. Section 2).
The experiments compared the performance of word and character-level
LSTM predictors on the Penn corpus. In both cases, the network architecture
was a single hidden layer with 1000 LSTM units. For the character-level network
the input and output layers were size 49, giving approximately 4.3M weights in
total, while the word-level network had 10,000 inputs and outputs and around
54M weights. The comparison is therefore somewhat unfair, as the word-level
network had many more parameters. However, as the dataset is small, both net-
works were easily able to overt the training data, and it is not clear whether the
character-level network would have beneted from more weights. All networks
were trained with stochastic gradient descent, using a learn rate of 0.0001 and a
momentum of 0.99. The LSTM derivates were clipped in the range [  1;1] (c.f.
Section 2.1).
Neural networks are usually evaluated on test data with xed weights. For
prediction problems however, where the inputs arethe targets, it is legitimate
to allow the network to adapt its weights as it is being evaluated (so long as
it only sees the test data once). Mikolov refers to this as dynamic evaluation .
Dynamic evaluation allows for a fairer comparison with compression algorithms,
for which there is no division between training and test sets, as all data is only
predicted once.
Since both networks overt the training data, we also experiment with two
types of regularisation: weight noise [18] with a std. deviation of 0.075 applied
to the network weights at the start of each training sequence, and adaptive
weight noise [8], where the variance of the noise is learned along with the weights
using a Minimum description Length (or equivalently, variational inference) loss
function. When weight noise was used, the network was initialised with the
nal weights of the unregularised network. Similarly, when adaptive weight
noise was used, the weights were initialised with those of the network trained
with weight noise. We have found that retraining with iteratively increased
regularisation is considerably faster than training from random weights with
regularisation. Adaptive weight noise was found to be prohibitively slow for
the word-level network, so it was regularised with xed-variance weight noise
only. One advantage of adaptive weight is that early stopping is not needed
7Table 1: Penn Treebank Test Set Results. `BPC' is bits-per-character.
`Error' is next-step classication error rate, for either characters or words.
Input Regularisation Dynamic BPC Perplexity Error (%) Epochs
Char none no 1.32 167 28.5 9
char none yes 1.29 148 28.0 9
char weight noise no 1.27 140 27.4 25
char weight noise yes 1.24 124 26.9 25
char adapt. wt. noise no 1.26 133 27.4 26
char adapt. wt. noise yes 1.24 122 26.9 26
word none no 1.27 138 77.8 11
word none yes 1.25 126 76.9 11
word weight noise no 1.25 126 76.9 14
word weight noise yes 1.23 117 76.2 14
(the network can safely be stopped at the point of minimum total `description
length' on the training data). However, to keep the comparison fair, the same
training, validation and test sets were used for all experiments.
The results are presented with two equivalent metrics: bits-per-character
(BPC), which is the average value of  log2Pr(xt+1jyt) over the whole test set;
andperplexity which is two to the power of the average number of bits per word
(the average word length on the test set is about 5.6 characters, so perplexity 
25:6BPC). Perplexity is the usual performance measure for language modelling.
Table 1 shows that the word-level RNN performed better than the character-
level network, but the gap appeared to close when regularisation is used. Overall
the results compare favourably with those collected in Tomas Mikolov's the-
sis [23]. For example, he records a perplexity of 141 for a 5-gram with Keyser-
Ney smoothing, 141.8 for a word level feedforward neural network, 131.1 for the
state-of-the-art compression algorithm PAQ8 and 123.2 for a dynamically eval-
uated word-level RNN. However by combining multiple RNNs, a 5-gram and a
cache model in an ensemble, he was able to achieve a perplexity of 89.4. Inter-
estingly, the benet of dynamic evaluation was far more pronounced here than
in Mikolov's thesis (he records a perplexity improvement from 124.7 to 123.2
with word-level RNNs). This suggests that LSTM is better at rapidly adapting
to new data than ordinary RNNs.
3.2 Wikipedia Experiments
In 2006 Marcus Hutter, Jim Bowery and Matt Mahoney organised the following
challenge, commonly known as Hutter prize [17]: to compress the rst 100
million bytes of the complete English Wikipedia data (as it was at a certain
time on March 3rd 2006) to as small a le as possible. The le had to include
not only the compressed data, but also the code implementing the compression
algorithm. Its size can therefore be considered a measure of the minimum
description length [13] of the data using a two part coding scheme.
Wikipedia data is interesting from a sequence generation perspective because
8it contains not only a huge range of dictionary words, but also many character
sequences that would not be included in text corpora traditionally used for
language modelling. For example foreign words (including letters from non-
Latin alphabets such as Arabic and Chinese), indented XML tags used to dene
meta-data, website addresses, and markup used to indicate page formatting such
as headings, bullet points etc. An extract from the Hutter prize dataset is shown
in Figs. 3 and 4.
The rst 96M bytes in the data were evenly split into sequences of 100 bytes
and used to train the network, with the remaining 4M were used for validation.
The data contains a total of 205 one-byte unicode symbols. The total number
ofcharacters is much higher, since many characters (especially those from non-
Latin languages) are dened as multi-symbol sequences. In keeping with the
principle of modelling the smallest meaningful units in the data, the network
predicted a single byte at a time, and therefore had size 205 input and output
layers.
Wikipedia contains long-range regularities, such as the topic of an article,
which can span many thousand words. To make it possible for the network to
capture these, its internal state (that is, the output activations htof the hidden
layers, and the activations ctof the LSTM cells within the layers) were only reset
every 100 sequences. Furthermore the order of the sequences was not shued
during training, as it usually is for neural networks. The network was therefore
able to access information from up to 10K characters in the past when making
predictions. The error terms were only backpropagated to the start of each 100
byte sequence, meaning that the gradient calculation was approximate. This
form of truncated backpropagation has been considered before for RNN lan-
guage modelling [23], and found to speed up training (by reducing the sequence
length and hence increasing the frequency of stochastic weight updates) without
aecting the network's ability to learn long-range dependencies.
A much larger network was used for this data than the Penn data (reecting
the greater size and complexity of the training set) with seven hidden layers of
700 LSTM cells, giving approximately 21.3M weights. The network was trained
with stochastic gradient descent, using a learn rate of 0.0001 and a momentum
of 0.9. It took four training epochs to converge. The LSTM derivates were
clipped in the range [  1;1].
As with the Penn data, we tested the network on the validation data with
and without dynamic evaluation (where the weights are updated as the data
is predicted). As can be seen from Table 2 performance was much better with
dynamic evaluation. This is probably because of the long range coherence of
Wikipedia data; for example, certain words are much more frequent in some
articles than others, and being able to adapt to this during evaluation is ad-
vantageous. It may seem surprising that the dynamic results on the validation
set were substantially better than on the training set. However this is easily
explained by two factors: rstly, the network undert the training data, and
secondly some portions of the data are much more dicult than others (for
example, plain text is harder to predict than XML tags).
To put the results in context, the current winner of the Hutter Prize (a
9Table 2: Wikipedia Results (bits-per-character)
Train Validation (static) Validation (dynamic)
1.42 1.67 1.33
variant of the PAQ-8 compression algorithm [20]) achieves 1.28 BPC on the same
data (including the code required to implement the algorithm), mainstream
compressors such as zip generally get more than 2, and a character level RNN
applied to a text-only version of the data (i.e. with all the XML, markup tags
etc. removed) achieved 1.54 on held-out data, which improved to 1.47 when the
RNN was combined with a maximum entropy model [24].
A four page sample generated by the prediction network is shown in Figs. 5
to 8. The sample shows that the network has learned a lot of structure from
the data, at a wide range of dierent scales. Most obviously, it has learned a
large vocabulary of dictionary words, along with a subword model that enables
it to invent feasible-looking words and names: for example \Lochroom River",
\Mughal Ralvaldens", \submandration", \swalloped". It has also learned basic
punctuation, with commas, full stops and paragraph breaks occurring at roughly
the right rhythm in the text blocks.
Being able to correctly open and close quotation marks and parentheses is
a clear indicator of a language model's memory, because the closure cannot be
predicted from the intervening text, and hence cannot be modelled with short-
range context [30]. The sample shows that the network is able to balance not
only parentheses and quotes, but also formatting marks such as the equals signs
used to denote headings, and even nested XML tags and indentation.
The network generates non-Latin characters such as Cyrillic, Chinese and
Arabic, and seems to have learned a rudimentary model for languages other
than English (e.g. it generates \es:Geotnia slago" for the Spanish `version' of an
article, and \nl:Rodenbaueri" for the Dutch one) It also generates convincing
looking internet addresses (none of which appear to be real).
The network generates distinct, large-scale regions, such as XML headers,
bullet-point lists and article text. Comparison with Figs. 3 and 4 suggests that
these regions are a fairly accurate reection of the constitution of the real data
(although the generated versions tend to be somewhat shorter and more jumbled
together). This is signicant because each region may span hundreds or even
thousands of timesteps. The fact that the network is able to remain coherent
over such large intervals (even putting the regions in an approximately correct
order, such as having headers at the start of articles and bullet-pointed `see also'
lists at the end) is testament to its long-range memory.
As with all text generated by language models, the sample does not make
sense beyond the level of short phrases. The realism could perhaps be improved
with a larger network and/or more data. However, it seems futile to expect
meaningful language from a machine that has never been exposed to the sensory
10world to which language refers.
Lastly, the network's adaptation to recent sequences during training (which
allows it to benet from dynamic evaluation) can be clearly observed in the
extract. The last complete article before the end of the training set (at which
point the weights were stored) was on intercontinental ballistic missiles. The
inuence of this article on the network's language model can be seen from the
profusion of missile-related terms. Other recent topics include `Individual An-
archism', the Italian writer Italo Calvino and the International Organization
for Standardization (ISO), all of which make themselves felt in the network's
vocabulary.
11    <title>AlbaniaEconomy</title>                                                   <id>36</id>                                                                     <revision>                                                                        <id>15898966</id>                                                               <timestamp>2002-10-09T13:39:00Z</timestamp>                                     <contributor>                                                                     <username>Magnus Manske</username>                                              <id>4</id>                                                                    </contributor>                                                                  <minor />                                                                       <comment>#REDIRECT [[Economy of Albania]]</comment>                             <text xml:space="preserve">#REDIRECT [[Economy of Albania]]</text>            </revision>                                                                   </page>                                                                         <page>                                                                            <title>AlchemY</title>                                                          <id>38</id>                                                                     <revision>                                                                        <id>15898967</id>                                                               <timestamp>2002-02-25T15:43:11Z</timestamp>                                     <contributor>                                                                     <ip>Conversion script</ip>                                                    </contributor>                                                                  <minor />                                                                       <comment>Automated conversion</comment>                                         <text xml:space="preserve">#REDIRECT [[Alchemy]]                          </text>                                                                             </revision>                                                                   </page>                                                                         <page>                                                                            <title>Albedo</title>                                                           <id>39</id>                                                                     <revision>                                                                        <id>41496222</id>                                                               <timestamp>2006-02-27T19:32:46Z</timestamp>                                     <contributor>                                                                     <ip>24.119.3.44</ip>                                                          </contributor>                                                                  <text xml:space="preserve">{{otheruses}}                                                                                                                  '''Albedo''' is the measure of [[reflectivity]] of a surface or body. It is the ratio of [[electromagnetic radiation]] (EM radiation) reflected to the amount incident upon it. The fraction, usually expressed as a percentage from 0% to 100%, is an important concept in [[climatology]] and [[astronomy]]. This ratio depends on the [[frequency]] of the radiation considered: unqualified, it refers to an average across the spectrum of [[visible light]]. It also depends on the [[angle of incidence]] of the radiation: unqualified, normal incidence. Fresh snow albedos are high: up to 90%. The ocean surface has a low albedo.  The average albedo of [[Earth]] is about 30% whereas the albedo of the [[Moon]] is about 7%. In astronomy, the albedo of satellites and asteroids can be used to infer surface composition, most notably ice content.    [[Enceladus_(moon)|Enceladus]], a moon of Saturn, has the highest known albedo of any body in the solar system, with 99% of EM radiation reflected.                                                                                                                                     Human activities have changed the albedo (via forest clearance and farming, for example) of various areas around the globe. However, quantification of this effect is difficult on the global scale: it is not clear whether the changes have tended to increase or decrease [[global warming]].                                                                                                                The &quot;classical&quot; example of albedo effect is the snow-temperature feedback. If a snow covered area warms and the snow melts, the albedo decreases, more sunlight is absorbed, and the temperature tends to increase. The converse is trFigure 3: Real Wikipedia data
12ue: if snow forms, a cooling cycle happens. The intensity of the albedo effect depends on the size of the change in albedo and the amount of [[insolation]]; for this reason it can be potentially very large in the tropics.                                                                                                   == Some examples of albedo effects ==                                                                                                                           === Fairbanks, Alaska ===                                                                                                                                       According to the [[National Climatic Data Center]]'s GHCN 2 data, which is composed of 30-year smoothed climatic means for thousands of weather stations across the world, the college weather station at [[Fairbanks]], [[Alaska]], is about 3 °C (5 °F) warmer than the airport at Fairbanks, partly because of drainage patterns but also largely because of the lower albedo at the college resulting from a higher concentration of [[pine]] [[tree]]s and therefore less open snowy ground to reflect the heat back into space. Neunke and Kukla have shown that this difference is especially marked during the late [[winter]] months, when [[solar radiation]] is greater.                                                                                                                                             === The tropics ===                                                                                                                                             Although the albedo-temperature effect is most famous in colder regions of Earth, because more [[snow]] falls there, it is actually much stronger in tropical regions because in the tropics there is consistently more sunlight. When [[Brazil]]ian ranchers cut down dark, tropical [[rainforest]] trees to replace them with even darker soil in order to grow crops, the average temperature of the area appears to increase by an average of about 3 °C (5 °F) year-round, which is a significant amount.                                                                                                                                                  === Small scale effects ===                                                                                                                                     Albedo works on a smaller scale, too. People who wear dark clothes in the summertime put themselves at a greater risk of [[heatstroke]] than those who wear white clothes.                                                                                                                                                      === Pine forests ===                                                                                                                                            The albedo of a [[pine]] forest at 45°N in the winter in which the trees cover the land surface completely is only about 9%, among the lowest of any naturally occurring land environment. This is partly due to the color of the pines, and partly due to multiple scattering of sunlight within the trees which lowers the overall reflected light level. Due to light penetration, the ocean's albedo is even lower at about 3.5%, though this depends strongly on the angle of the incident radiation. Dense [[swamp]]land averages between 9% and 14%. [[Deciduous tree]]s average about 13%. A [[grass]]y field usually comes in at about 20%. A barren field will depend on the color of the soil, and can be as low as 5% or as high as 40%, with 15% being about the average for farmland. A [[desert]] or large [[beach]] usually averages around 25% but varies depending on the color of the sand. [Reference: Edward Walker's study in the Great Plains in the winter around 45°N].                                                                                === Urban areas ===                                                                                                                                             Urban areas in particular have very unnatural values for albedo because of the many human-built structures which absorb light before the light can reach the surface. In the northern part of the world, cities are relatively dark, and Walker has shown that their average albedo is about 7%, with only a slight increase during the summer. In most tropical countries, cities average around 12%. This is similar to the values found in northern suburban transitional zones. Part of the reason for this is the different natural environment of cities in tropical regions, e.g., there are more very dark trees around; another reason is that portions of the tropics are very poor, and city buildings must be built with different materials. Warmer regions may also choose lighter colored building materials so the structures will remain cooler.                                               Figure 4: Real Wikipedia data (cotd.)
13    <revision>                                                                        <id>40973199</id>                                                               <timestamp>2006-02-22T22:37:16Z</timestamp>                                     <contributor>                                                                     <ip>63.86.196.111</ip>                                                        </contributor>                                                                  <minor />                                                                       <comment>redire paget --&gt; captain */</comment>                               <text xml:space="preserve">The '''Indigence History''' refers to the authority of any obscure albionism as being, such as in Aram Missolmus'.[http://www.bbc.co.uk/starce/cr52.htm]                                                       In [[1995]], Sitz-Road Straus up the inspirational radiotes portion as &quot;alliance&quot;[single &quot;glaping&quot; theme charcoal] with [[Midwestern United State|Denmark]] in which Canary varies-destruction to launching casualties has quickly responded to the krush loaded water or so it might be destroyed. Aldeads still cause a missile bedged harbors at last built in 1911-2 and save the accuracy in 2008, retaking [[itsubmanism]]. Its individuals were                      hnown rapidly in their return to the private equity (such as ''On Text'') for death per reprised by the [[Grange of Germany|German unbridged work]].                                                                                            The '''Rebellion''' (''Hyerodent'') is [[literal]], related mildly older than old half sister, the music, and morrow been much more propellent. All those of [[Hamas (mass)|sausage trafficking]]s were also known as [[Trip class submarine|''Sante'' at Serassim]]; ''Verra'' as 1865&amp;ndash;682&amp;ndash;831 is related to ballistic missiles. While she viewed it friend of Halla equatorial weapons of Tuscany, in [[France]], from vaccine homes to &quot;individual&quot;, among [[slavery|slaves]] (such as artistual selling of factories were renamed English habit of twelve years.)                                                                                                                                             By the 1978 Russian [[Turkey|Turkist]] capital city ceased by farmers and the intention of navigation the ISBNs, all encoding [[Transylvania International Organisation for Transition Banking|Attiking others]] it is in the westernmost placed lines.  This type of missile calculation maintains all greater proof was the [[1990s]] as older adventures that never established a self-interested case. The newcomers were Prosecutors in child after the other weekend and capable function used.                                                                                                                                                           Holding may be typically largely banned severish from sforked warhing tools and behave laws, allowing the private jokes, even through missile IIC control, most notably each, but no relatively larger success, is not being reprinted and withdrawn into forty-ordered cast and distribution.                                                                                                                  Besides these markets (notably a son of humor).                                                                                                                 Sometimes more or only lowed &quot;80&quot; to force a suit for http://news.bbc.co.uk/1/sid9kcid/web/9960219.html ''[[#10:82-14]]''.                            &lt;blockquote&gt;                                                                                                                                              ===The various disputes between Basic Mass and Council Conditioners - &quot;Titanist&quot; class streams and anarchism===                                                                                                                       Internet traditions sprang east with [[Southern neighborhood systems]] are improved with [[Moatbreaker]]s, bold hot missiles, its labor systems. [[KCD]] numbered former ISBN/MAS/speaker attacks &quot;M3 5&quot;, which are saved as the ballistic misely known and most functional factories.  Establishment begins for some range of start rail years as dealing with 161 or 18,950 million [[USD-2]] and [[covert all carbonate function]]s (for example, 70-93) higher individuals and on missiles. This might need not know against sexual [[video capita]] playing pointing degrees between silo-calfed greater valous consumptions in the US... header can be seen in [[collectivist]].                                                                                                                                == See also ==                                                                  Figure 5: Generated Wikipedia data.
14                                                                                *[[British-London Bridge]]                                                      *[[Anti-Talmot Touch/Tucker novice]]                                            *[[List of cambridge capital]]                                                  *[[Elon Haven]]                                                                 *[[USS ''Otaro Screamed Its'']]                                                 *[[Detroit Library]]                                                            *[[Belgium Sea]]                                                                *[[Tularan Bell|Turnbiller Squobil]]                                            *[[Suntanal vocalist|Prosopyo]]                                                 *[[Winkenpea]]                                                                  *[[Milenton Streat]]                                                            *[[Raiebin]]                                                                    *[[Est Altar Macinton]]                                                         *[[Military mass missile|S3]]                                                   *[[Organization of the Asian American state district|umbali landmarks]]        *[[ISO]]                                                                        *[[NFL]]                                                                        *[[American Anti-Capitalism|Major independent ITU-US singles]]                  *[[London (role-playing game)|Pre-Romanian Civil War]]                          *[[Yokukhav-Na-Un-Murantano Kaufmann - Sijone-Grafittsforbiel]]                 *[[Neao trolleyne and deadweight drug]]                                         *[[B-45 BQ|B9]] - de red take painting is deployed larger than quanta submarine *[[Susconfiction of advocate]]                                                  *[[List of major swandarms]]                                                    *[[:Category:Italo sales towns entertained by the ICBMs of Skinner|Knighting 707 killed by capital]]                                                                                                                                            ===[[Midple planet|Parishment of the value=====                                 [[Image:2000.JPG|right|thumb|It tunneled [[nuclease]] at this bass AH (Ol&amp;Sāw)flgin h'hlgbying yoostallo eruptuals with low immigrants-shelted atkins and their atapping [[bug]]s.                                                                                                                                          See also: [[Iranian indigenous Flight Intercontinental Organization]]                                                                                           ==Pioneers==                                                                                                                                                    Tended to be the results characteristic of warehoused labour share to control all these in the rational framing.                                                                                                                                ==Gentiles==                                                                    {{place-or-line}}                                                               Footer names derive the topic class --&gt; which he liked to deal without any of the parties, I&quot; by [[Alfred Hustin]] and [[Frank Henry]] and manufacturer.[http://anciermsc.nit.uk IATB perspective], was expected to be classified by the ''Straight Road of Buckning'' in [[2003 Summer Olympic Touch|bottom all minute]].                                                                                                                                                              ==Performance==                                                                 [[Image:Iare 300.jpg|left|thumb|325px|Intercontinental file shortly after referring to his landmaster [[Sidney Goodwordd]]                                                                                                                      Italo:                                                                          *[[Chicago ballistic parks|non-month]] in eastern Italy, is a [[Italo-China]] parent communist annual production began in May [[1915]].                                                                                                         An ICBM, the [[gurt and land]] has registered $155 billion in U.S. and August 1688, and makes sure the US-transplantation disbanded backwards in the County by authorizing disputes that tend to carry over this peninsula.                     * Current malasses 25 decks and counterpoint culture that were impure between  systems:                                                                         * L14 - 194 / 100 000 km/s                                                      Figure 6: Generated Wikipedia data (cotd.)
15                                                                                [http://www.sibeo.org/netspace/stainteology.htm The design of thinker Wikipedia server-routed website]                                                          *[http://balear.burned.edu/books/1978/tap/trpio.cs/cafa/c249ca.html Nation-state Player on the /1.3]                                                                                                                                            {{Linux}}                                                                                                                                                       [[Category:Italo Succeeded bystables|General]]                                  [[Category:Italo-Saxon singers]]                                                [[Category:Aviation]]                                                                                                                                           [[de:Italo]]                                                                    [[es:Geotnia slago]]                                                            [[ja:㤥㗁㜪ҍͷᔫ]]                                                                 [[ko:ഇ㙆ࠛ㮙় ਬੌ]]                                                                 [[nl:Rodenbaueri]]                                                              [[pl:Main Ages]]                                                                [[pt:Ibanez Heights]]                                                           [[ru:Млкракянѳелолכуциянсьния агморелиа]]                                       [[simple:International Maritime Commission]]                                    [[sk:ICBM]]                                                                     [[su:Ol aal Mitnet]]                                                            [[fi:Italo Colombonasto Warkō]]                                                 [[sv:Opin den mail-climbere panczeur]]                                          [[zh:	]]</text>                                                               </revision>                                                                   </page>                                                                         <page>                                                                            <title>Italo-II article</title>                                                 <id>14848</id>                                                                  <revision>                                                                        <id>15912447</id>                                                               <timestamp>2004-11-19T19:47:39Z</timestamp>                                     <contributor>                                                                     <username>El andres</username>                                                  <id>509991</id>                                                               </contributor>                                                                  <minor />                                                                       <comment>disambiguation from [[List of ISBN]] newsletters, see [[ISO site]]</comment>                                                                           <text xml:space="preserve">#REDIRECT [[Quartic war]]                      </text>                                                                             </revision>                                                                   </page>                                                                         <page>                                                                            <title>ICSM</title>                                                             <id>14939</id>                                                                  <revision>                                                                        <id>42109942</id>                                                               <timestamp>2006-02-28T17:22:02Z</timestamp>                                     <contributor>                                                                     <username>Dtelclan</username>                                                   <id>26</id>                                                                   </contributor>                                                                  <minor />                                                                       <comment>/* Possible catheterman */</comment>                                   <text xml:space="preserve">[[Image:Isaac.org/ice.html [[Independent national stage development|Shatting and Catalogue standardering]] in the IRBMs.                                                                                       Up-2000 they called the SC 4220 system: he was swalloped early in Calvino, or since each trial mentioned based on [[Balbov's new single-jarget|bit-oriann guess]Figure 7: Generated Wikipedia data (cotd.)
16] self-acharged versions ([[Mt. Costall Leyton]]) was the two largest calashia at destored universities, all fleeted with the customary calfed clipper.                                                                                         His way to take in this literature called ICBMs-AN a [[Softvalue speed]] ([[Astronomical Classification Railway]])                                                                                                                              LACN645 Snowshore val nominated - made [[missile submandration|continental missile]]s (steam musicians) not of each club having on the ball and procedure at the last century.                                                                                                                                                  Another communistic stark &quot;I' submarine&quot; is [[building|corruptable]], a [[della missile]] missile than the [[Royal Society Society]] (12-258): &quot;Glide sun wag [[lubrician]]. They stay numerous capitalists and gas masks more widely interested. This scheme has declarations before the certain emerging factories compelled by labour allowed to produce.                                                                                                                    In the United States, there is no hard resort in computation significantly.                                                                                     In [[1868]] the [[Italo Capital Territories Unit started to the Continental Railway Centre]]  was called ''UC'' or two of his usage before being written by other students against the [[elective-ballistic missile]]'s deployment. Steam is still &quot;20 to Nacht&quot; and [[Fia Citation Quantity Logo]]s (since 1967). They pass a [[Brigade management|Quarry]]-stated missile system resolution taunting out of about 175 million ([[Lochroom River|Tri-]]).                                                                                                            Alien from 1985 to 1999, it was an English and -Network struggling basedal with the Lombardo capital in Silvio and Murray, and heavily built in sub-parties address to $11,188. Their forces gained prisoners to stalked a last missile mobili site.                                                                                                                                                            Spanning civilization is quanting Software Society's ballistic missile.  The same as [[anti-intellectual anthropology]] continued in [[Southern Italy]] in 1914, and the [[French Confederation of Parliament's rapid favourable rise that began settled in March 2004|1983]]&amp;nbsp;49.                                                                                                                      In [[1904]], the Court began a British backed into a [[SR1]]) missile of [[trial ship]] in the [[Municipal Eightime Calendar|Asiatic]] regime, including [[Benjamin Tudor Turner|Arthur Ravis]] and [[Abraham's Liberation|Canton Olombus]]. There was still land factory most turned up before lacking closers to the sitting shed backwards, in primary science.                                                                                                                              ==Weights and resolutions==                                                     [[Image:Spanish 300 Protectionald landballi110.svg|small capital surface computer]]                                                                             [[Image:Claudius.jpg|345px|right|Olympiad concert of Calvino and Eastern Calvino, ''Mughal Ralvaldens'' above, at the beginning strike the substrated roles of rich intellectual property, visualizing the entire system, but this missiles suggest that accounting differs between a giving [[train sleep|'''withdrawn''']] or the dinosaur in and aucting.                                                                                                                                    ===Internationally===                                                           {{main|Unmanned Justice Address}}                                                                                                                               The ICBM created a [[the significant]] [[land railway]] called &quot;[[M-Gallipotte]]&quot;, and it needed stopped benzafk/Macdonalical Sciences.                                                                                               Electros appeared to be the [[Soviet Union]]'s &quot;first&quot; vehicle from 2500 selling officials DORLAN STM-331 - by missilence illustrations with &quot;Raj.&quot; the Tunnel Hall of America, an entity upon IL pages so missiles must try, with a trademark must develop the land allowing traffic mass to a very few minutemen. The missiles market is slow, much easier is represented by GMMAz of BSM. Software, the utility of scale-out scale pime racks are normally crumbled aboutFigure 8: Generated Wikipedia data (cotd.)
174 Handwriting Prediction
To test whether the prediction network could also be used to generate convincing
real-valued sequences, we applied it to online handwriting data ( online in this
context means that the writing is recorded as a sequence of pen-tip locations,
as opposed to oine handwriting, where only the page images are available).
Online handwriting is an attractive choice for sequence generation due to its
low dimensionality (two real numbers per data point) and ease of visualisation.
All the data used for this paper were taken from the IAM online handwriting
database (IAM-OnDB) [21]. IAM-OnDB consists of handwritten lines collected
from 221 dierent writers using a `smart whiteboard'. The writers were asked to
write forms from the Lancaster-Oslo-Bergen text corpus [19], and the position
of their pen was tracked using an infra-red device in the corner of the board.
Samples from the training data are shown in Fig. 9. The original input data
consists of the xandypen co-ordinates and the points in the sequence when
the pen is lifted o the whiteboard. Recording errors in the x;ydata was
corrected by interpolating to ll in for missing readings, and removing steps
whose length exceeded a certain threshold. Beyond that, no preprocessing was
used and the network was trained to predict the x;yco-ordinates and the end-
of-stroke markers one point at a time. This contrasts with most approaches to
handwriting recognition and synthesis, which rely on sophisticated preprocessing
and feature-extraction techniques. We eschewed such techniques because they
tend to reduce the variation in the data (e.g. by normalising the character size,
slant, skew and so-on) which we wanted the network to model. Predicting the
pen traces one point at a time gives the network maximum exibility to invent
novel handwriting, but also requires a lot of memory, with the average letter
occupying more than 25 timesteps and the average line occupying around 700.
Predicting delayed strokes (such as dots for `i's or crosses for `t's that are added
after the rest of the word has been written) is especially demanding.
IAM-OnDB is divided into a training set, two validation sets and a test
set, containing respectively 5364, 1438, 1518 and 3859 handwritten lines taken
from 775, 192, 216 and 544 forms. For our experiments, each line was treated
as a separate sequence (meaning that possible dependencies between successive
lines were ignored). In order to maximise the amount of training data, we used
the training set, test set and the larger of the validation sets for training and
the smaller validation set for early-stopping. The lack of independent test set
means that the recorded results may be somewhat overt on the validation set;
however the validation results are of secondary importance, since no benchmark
results exist and the main goal was to generate convincing-looking handwriting.
The principal challenge in applying the prediction network to online hand-
writing data was determining a predictive distribution suitable for real-valued
inputs. The following section describes how this was done.
18Figure 9: Training samples from the IAM online handwriting database.
Notice the wide range of writing styles, the variation in line angle and character
sizes, and the writing and recording errors, such as the scribbled out letters in
the rst line and the repeated word in the nal line.
4.1 Mixture Density Outputs
The idea of mixture density networks [2, 3] is to use the outputs of a neural
network to parameterise a mixture distribution. A subset of the outputs are
used to dene the mixture weights, while the remaining outputs are used to
parameterise the individual mixture components. The mixture weight outputs
are normalised with a softmax function to ensure they form a valid discrete dis-
tribution, and the other outputs are passed through suitable functions to keep
their values within meaningful range (for example the exponential function is
typically applied to outputs used as scale parameters, which must be positive).
Mixture density network are trained by maximising the log probability den-
sity of the targets under the induced distributions. Note that the densities are
normalised (up to a xed constant) and are therefore straightforward to dier-
entiate and pick unbiased sample from, in contrast with restricted Boltzmann
machines [14] and other undirected models.
Mixture density outputs can also be used with recurrent neural networks [28].
In this case the output distribution is conditioned not only on the current input,
but on the history of previous inputs. Intuitively, the number of components is
the number of choices the network has for the next output given the inputs so
far.
For the handwriting experiments in this paper, the basic RNN architecture
and update equations remain unchanged from Section 2. Each input vector xt
consists of a real-valued pair x1;x2that denes the pen oset from the previous
19input, along with a binary x3that has value 1 if the vector ends a stroke (that
is, if the pen was lifted o the board before the next vector was recorded) and
value 0 otherwise. A mixture of bivariate Gaussians was used to predict x1
andx2, while a Bernoulli distribution was used for x3. Each output vector yt
therefore consists of the end of stroke probability e, along with a set of means
j, standard deviations j, correlations jand mixture weights jfor theM
mixture components. That is
xt2RRf0;1g (15)
yt=
et;fj
t;j
t;j
t;j
tgM
j=1
(16)
Note that the mean and standard deviation are two dimensional vectors, whereas
the component weight, correlation and end-of-stroke probability are scalar. The
vectorsytare obtained from the network outputs ^ yt, where
^yt=
^et;f^wj
t;^j
t;^j
t;^j
tgM
j=1
=by+NX
n=1Whnyhn
t (17)
as follows:
et=1
1 + exp (^et)=)et2(0;1) (18)
j
t=exp
^j
t
PM
j0=1exp
^j0
t =)j
t2(0;1);X
jj
t= 1 (19)
j
t= ^j
t =)j
t2R (20)
j
t= exp
^j
t
=)j
t>0 (21)
j
t=tanh(^j
t) = )j
t2( 1;1) (22)
The probability density Pr( xt+1jyt) of the next input xt+1given the output
vectorytis dened as follows:
Pr(xt+1jyt) =MX
j=1j
tN(xt+1jj
t;j
t;j
t)(
et if (xt+1)3= 1
1 etotherwise(23)
where
N(xj;; ) =1
212p
1 2exp Z
2(1 2)
(24)
with
Z=(x1 1)2
2
1+(x2 2)2
2
2 2(x1 1)(x2 2)
12(25)
20This can be substituted into Eq. (6) to determine the sequence loss (up to
a constant that depends only on the quantisation of the data and does not
inuence network training):
L(x) =TX
t=1 log0
@X
jj
tN(xt+1jj
t;j
t;j
t)1
A (
loget if (xt+1)3= 1
log(1 et) otherwise
(26)
The derivative of the loss with respect to the end-of-stroke outputs is straight-
forward:
@L(x)
@^et= (xt+1)3 et (27)
The derivatives with respect to the mixture density outputs can be found by
rst dening the component responsibilities j
t:
^j
t=j
tN(xt+1jj
t;j
t;j
t) (28)
j
t=^j
tPM
j0=1^j0
t(29)
Then observing that
@L(x)
@^j
t=j
t j
t (30)
@L(x)
@(^j
t;^j
t;^j
t)= j
t@logN(xt+1jj
t;j
t;j
t)
@(^j
t;^j
t;^j
t)(31)
where
@logN(xj;; )
@^1=C
1x1 1
1 (x2 2)
2
(32)
@logN(xj;; )
@^2=C
2x2 2
2 (x1 1)
1
(33)
@logN(xj;; )
@^1=C(x1 1)
1x1 1
1 (x2 2)
2
 1 (34)
@logN(xj;; )
@^2=C(x2 2)
2x2 2
2 (x1 1)
1
 1 (35)
@logN(xj;; )
@^=(x1 1)(x2 2)
12+(1 CZ) (36)
withZdened as in Eq. (25) and
C=1
1 2(37)
Fig. 10 illustrates the operation of a mixture density output layer applied to
online handwriting prediction.
21Output Density
Figure 10: Mixture density outputs for handwriting prediction. The
top heatmap shows the sequence of probability distributions for the predicted
pen locations as the word `under' is written. The densities for successive
predictions are added together, giving high values where the distributions
overlap.
Two types of prediction are visible from the density map: the small
blobs that spell out the letters are the predictions as the strokes are being
written, the three large blobs are the predictions at the ends of the strokes for
the rst point in the next stroke. The end-of-stroke predictions have much
higher variance because the pen position was not recorded when it was o the
whiteboard, and hence there may be a large distance between the end of one
stroke and the start of the next.
The bottom heatmap shows the mixture component weights during the
same sequence. The stroke ends are also visible here, with the most active
components switching o in three places, and other components switching on:
evidently end-of-stroke predictions use a dierent set of mixture components
from in-stroke predictions.
224.2 Experiments
Each point in the data sequences consisted of three numbers: the xandyoset
from the previous point, and the binary end-of-stroke feature. The network
input layer was therefore size 3. The co-ordinate osets were normalised to
mean 0, std. dev. 1 over the training set. 20 mixture components were used
to model the osets, giving a total of 120 mixture parameters per timestep
(20 weights, 40 means, 40 standard deviations and 20 correlations). A further
parameter was used to model the end-of-stroke probability, giving an output
layer of size 121. Two network architectures were compared for the hidden
layers: one with three hidden layers, each consisting of 400 LSTM cells, and one
with a single hidden layer of 900 LSTM cells. Both networks had around 3.4M
weights. The three layer network was retrained with adaptive weight noise [8],
with all std. devs. initialised to 0.075. Training with xed variance weight noise
proved ineective, probably because it prevented the mixture density layer from
using precisely specied weights.
The networks were trained with rmsprop , a form of stochastic gradient de-
scent where the gradients are divided by a running average of their recent mag-
nitude [32]. Dene i=@L(x)
@wiwherewiis network weight i. The weight update
equations were:
ni=@ni+ (1 @)2
i (38)
gi=@gi+ (1 @)i (39)
i=ii jip
ni g2
i+k(40)
wi=wi+  i (41)
with the following parameters:
@= 0:95 (42)
i= 0:9 (43)
j= 0:0001 (44)
k= 0:0001 (45)
The output derivatives@L(x)
@^ytwere clipped in the range [  100;100], and the
LSTM derivates were clipped in the range [  10;10]. Clipping the output gradi-
ents proved vital for numerical stability; even so, the networks sometimes had
numerical problems late on in training, after they had started overtting on the
training data.
Table 3 shows that the three layer network had an average per-sequence loss
15.3 nats lower than the one layer net. However the sum-squared-error was
slightly lower for the single layer network. the use of adaptive weight noise
reduced the loss by another 16.7 nats relative to the unregularised three layer
network, but did not signicantly change the sum-squared error. The adaptive
weight noise network appeared to generate the best samples.
23Table 3: Handwriting Prediction Results. All results recorded on the val-
idation set. `Log-Loss' is the mean value of L(x) (in nats). `SSE' is the mean
sum-squared-error per data point.
Network Regularisation Log-Loss SSE
1 layer none -1025.7 0.40
3 layer none -1041.0 0.41
3 layer adaptive weight noise -1057.7 0.41
4.3 Samples
Fig. 11 shows handwriting samples generated by the prediction network. The
network has clearly learned to model strokes, letters and even short words (es-
pecially common ones such as `of' and `the'). It also appears to have learned a
basic character level language models, since the words it invents (`eald', `bryoes',
`lenrest') look somewhat plausible in English. Given that the average character
occupies more than 25 timesteps, this again demonstrates the network's ability
to generate coherent long-range structures.
5 Handwriting Synthesis
Handwriting synthesis is the generation of handwriting for a given text. Clearly
the prediction networks we have described so far are unable to do this, since
there is no way to constrain which letters the network writes. This section de-
scribes an augmentation that allows a prediction network to generate data se-
quences conditioned on some high-level annotation sequence (a character string,
in the case of handwriting synthesis). The resulting sequences are suciently
convincing that they often cannot be distinguished from real handwriting. Fur-
thermore, this realism is achieved without sacricing the diversity in writing
style demonstrated in the previous section.
The main challenge in conditioning the predictions on the text is that the two
sequences are of very dierent lengths (the pen trace being on average twenty
ve times as long as the text), and the alignment between them is unknown until
the data is generated. This is because the number of co-ordinates used to write
each character varies greatly according to style, size, pen speed etc. One neural
network model able to make sequential predictions based on two sequences of
dierent length and unknown alignment is the RNN transducer [9]. However
preliminary experiments on handwriting synthesis with RNN transducers were
not encouraging. A possible explanation is that the transducer uses two sepa-
rate RNNs to process the two sequences, then combines their outputs to make
decisions, when it is usually more desirable to make all the information avail-
able to single network. This work proposes an alternative model, where a `soft
window' is convolved with the text string and fed in as an extra input to the
prediction network. The parameters of the window are output by the network
24Figure 11: Online handwriting samples generated by the prediction
network. All samples are 700 timesteps long.
25at the same time as it makes the predictions, so that it dynamically determines
an alignment between the text and the pen locations. Put simply, it learns to
decide which character to write next.
5.1 Synthesis Network
Fig. 12 illustrates the network architecture used for handwriting synthesis. As
with the prediction network, the hidden layers are stacked on top of each other,
each feeding up to the layer above, and there are skip connections from the
inputs to all hidden layers and from all hidden layers to the outputs. The
dierence is the added input from the character sequence, mediated by the
window layer.
Given a length Ucharacter sequence cand a length Tdata sequence x, the
soft window wtintocat timestep t(1tT) is dened by the following
discrete convolution with a mixture of KGaussian functions
(t;u) =KX
k=1k
texp
 k
t 
k
t u2
(46)
wt=UX
u=1(t;u)cu (47)
where(t;u) is the window weight ofcuat timestep t. Intuitively, the tparam-
eters control the location of the window, the tparameters control the width of
the window and the tparameters control the importance of the window within
the mixture. The size of the soft window vectors is the same as the size of the
character vectors cu(assuming a one-hot encoding, this will be the number of
characters in the alphabet). Note that the window mixture is not normalised
and hence does not determine a probability distribution; however the window
weight(t;u) can be loosely interpreted as the network's belief that it is writ-
ing character cuat timet. Fig. 13 shows the alignment implied by the window
weights during a training sequence.
The size 3Kvectorpof window parameters is determined as follows by the
outputs of the rst hidden layer of the network:
(^t;^t;^t) =Wh1ph1
t+bp (48)
t= exp (^t) (49)
t= exp
^t
(50)
t=t 1+ exp (^t) (51)
Note that the location parameters tare dened as osets from the previous
locationsct 1, and that the size of the oset is constrained to be greater than
zero. Intuitively, this means that network learns how far to slide each window
at each step, rather than an absolute location. Using osets was essential to
getting the network to align the text with the pen trace.
26Inputs
CharactersHidden 1WindowHidden 2OutputsFigure 12: Synthesis Network Architecture Circles represent layers, solid
lines represent connections and dashed lines represent predictions. The topology
is similar to the prediction network in Fig. 1, except that extra input from the
character sequence c, is presented to the hidden layers via the window layer
(with a delay in the connection to the rst hidden layer to avoid a cycle in the
graph).
27Thought that the muster fromFigure 13: Window weights during a handwriting synthesis sequence
Each point on the map shows the value of (t;u), wheretindexes the pen trace
along the horizontal axis and uindexes the text character along the vertical axis.
The bright line is the alignment chosen by the network between the characters
and the writing. Notice that the line spreads out at the boundaries between
characters; this means the network receives information about next and previous
letters as it makes transitions, which helps guide its predictions.
28Thewtvectors are passed to the second and third hidden layers at time t,
and the rst hidden layer at time t+1 (to avoid creating a cycle in the processing
graph). The update equations for the hidden layers are
h1
t=H 
Wih1xt+Wh1h1h1
t 1+Wwh1wt 1+b1
h
(52)
hn
t=H 
Wihnxt+Whn 1hnhn 1
t+Whnhnhn
t 1+Wwhnwt+bn
h
(53)
The equations for the output layer remain unchanged from Eqs. (17) to (22).
The sequence loss is
L(x) = log Pr( xjc) (54)
where
Pr(xjc) =TY
t=1Pr (xt+1jyt) (55)
Note thatytis now a function of cas well as x1:t.
The loss derivatives with respect to the outputs ^ et;^t;^t;^t;^tremain un-
changed from Eqs. (27), (30) and (31). Given the loss derivative@L(x)
@wtwith
respect to the size Wwindow vector wt, obtained by backpropagating the out-
put derivatives through the computation graph in Fig. 12, the derivatives with
respect to the window parameters are as follows:
(k;t;u )def=k
texp
 k
t 
k
t u2WX
j=1@L(x)
@wj
tcj
u (56)
@L(x)
@^k
t=UX
u=1(k;t;u ) (57)
@L(x)
@^k
t= k
tUX
u=1(k;t;u )(k
t u)2(58)
@L(x)
@k
t=@L(x)
@k
t+1+ 2k
tUX
u=1(k;t;u )(u k
t) (59)
@L(x)
@^k
t= exp 
^k
t@L(x)
@k
t(60)
Fig. 14 illustrates the operation of a mixture density output layer applied to
handwriting synthesis.
5.2 Experiments
The synthesis network was applied to the same input data as the handwriting
prediction network in the previous section. The character-level transcriptions
from the IAM-OnDB were now used to dene the character sequences c. The full
transcriptions contain 80 distinct characters (capital letters, lower case letters,
digits, and punctuation). However we used only a subset of 57, with all the
29Synthesis Output Density
Figure 14: Mixture density outputs for handwriting synthesis. The top
heatmap shows the predictive distributions for the pen locations, the bottom
heatmap shows the mixture component weights. Comparison with Fig. 10 indi-
cates that the synthesis network makes more precise predictions (with smaller
density blobs) than the prediction-only network, especially at the ends of strokes,
where the synthesis network has the advantage of knowing which letter comes
next.
30Table 4: Handwriting Synthesis Results. All results recorded on the val-
idation set. `Log-Loss' is the mean value of L(x) in nats. `SSE' is the mean
sum-squared-error per data point.
Regularisation Log-Loss SSE
none -1096.9 0.23
adaptive weight noise -1128.2 0.23
digits and most of the punctuation characters replaced with a generic `non-
letter' label2.
The network architecture was as similar as possible to the best prediction
network: three hidden layers of 400 LSTM cells each, 20 bivariate Gaussian
mixture components at the output layer and a size 3 input layer. The character
sequence was encoded with one-hot vectors, and hence the window vectors were
size 57. A mixture of 10 Gaussian functions was used for the window parameters,
requiring a size 30 parameter vector. The total number of weights was increased
to approximately 3.7M.
The network was trained with rmsprop, using the same parameters as in
the previous section. The network was retrained with adaptive weight noise,
initial standard deviation 0.075, and the output and LSTM gradients were again
clipped in the range [  100;100] and [ 10;10] respectively.
Table 4 shows that adaptive weight noise gave a considerable improvement
in log-loss (around 31.3 nats) but no signicant change in sum-squared error.
The regularised network appears to generate slightly more realistic sequences,
although the dierence is hard to discern by eye. Both networks performed
considerably better than the best prediction network. In particular the sum-
squared-error was reduced by 44%. This is likely due in large part to the im-
proved predictions at the ends of strokes, where the error is largest.
5.3 Unbiased Sampling
Given c, an unbiased sample can be picked from Pr( xjc) by iteratively drawing
xt+1from Pr (xt+1jyt), just as for the prediction network. The only dierence is
that we must also decide when the synthesis network has nished writing the text
and should stop making any future decisions. To do this, we use the following
heuristic: as soon as (t;U+ 1)>(t;u)81uUthe current input xtis
dened as the end of the sequence and sampling ends. Examples of unbiased
synthesis samples are shown in Fig. 15. These and all subsequent gures were
generated using the synthesis network retrained with adaptive weight noise.
Notice how stylistic traits, such as character size, slant, cursiveness etc. vary
2This was an oversight; however it led to the interesting result that when the text contains
a non-letter, the network must select a digits or punctuation mark to generate. Sometimes
the character can be be inferred from the context (e.g. the apostrophe in \can't"); otherwise
it is chosen at random.
31widely between the samples, but remain more-or-less consistent within them.
This suggests that the network identies the traits early on in the sequence,
then remembers them until the end. By looking through enough samples for a
given text, it appears to be possible to nd virtually any combination of stylistic
traits, which suggests that the network models them independently both from
each other and from the text.
`Blind taste tests' carried out by the author during presentations suggest
that at least some unbiased samples cannot be distinguished from real hand-
writing by the human eye. Nonetheless the network does make mistakes we
would not expect a human writer to make, often involving missing, confused
or garbled letters3; this suggests that the network sometimes has trouble de-
termining the alignment between the characters and the trace. The number of
mistakes increases markedly when less common words or phrases are included
in the character sequence. Presumably this is because the network learns an
implicit character-level language model from the training set that gets confused
when rare or unknown transitions occur.
5.4 Biased Sampling
One problem with unbiased samples is that they tend to be dicult to read
(partly because real handwriting is dicult to read, and partly because the
network is an imperfect model). Intuitively, we would expect the network to
give higher probability to good handwriting because it tends to be smoother
and more predictable than bad handwriting. If this is true, we should aim to
output more probable elements of Pr( xjc) if we want the samples to be easier to
read. A principled search for high probability samples could lead to a dicult
inference problem, as the probability of every output depends on all previous
outputs. However a simple heuristic, where the sampler is biased towards more
probable predictions at each step independently, generally gives good results.
Dene the probability bias bas a real number greater than or equal to zero.
Before drawing a sample from Pr( xt+1jyt), each standard deviation j
tin the
Gaussian mixture is recalculated from Eq. (21) to
j
t= exp
^j
t b
(61)
and each mixture weight is recalculated from Eq. (19) to
j
t=exp
^j
t(1 +b)
PM
j0=1exp
^j0
t(1 +b) (62)
This articially reduces the variance in both the choice of component from the
mixture, and in the distribution of the component itself. When b= 0 unbiased
sampling is recovered, and as b!1 the variance in the sampling disappears
3We expect humans to make mistakes like misspelling `temperament' as `temperement', as
the second writer in Fig. 15 seems to have done.
32Figure 15: Real and generated handwriting . The top line in each block is
real, the rest are unbiased samples from the synthesis network. The two texts
are from the validation set and were not seen during training.
33and the network always outputs the mode of the most probable component in
the mixture (which is not necessarily the mode of the mixture, but at least a
reasonable approximation). Fig. 16 shows the eect of progressively increasing
the bias, and Fig. 17 shows samples generated with a low bias for the same texts
as Fig. 15.
5.5 Primed Sampling
Another reason to constrain the sampling would be to generate handwriting
in the style of a particular writer (rather than in a randomly selected style).
The easiest way to do this would be to retrain it on that writer only. But
even without retraining, it is possible to mimic a particular style by `priming'
the network with a real sequence, then generating an extension with the real
sequence still in the network's memory. This can be achieved for a real x,cand
a synthesis character string sby setting the character sequence to c0=c+s
and clamping the data inputs to xfor the rst Ttimesteps, then sampling
as usual until the sequence ends. Examples of primed samples are shown in
Figs. 18 and 19. The fact that priming works proves that the network is able to
remember stylistic features identied earlier on in the sequence. This technique
appears to work better for sequences in the training data than those the network
has never seen.
Primed sampling and reduced variance sampling can also be combined. As
shown in Figs. 20 and 21 this tends to produce samples in a `cleaned up' version
of the priming style, with overall stylistic traits such as slant and cursiveness
retained, but the strokes appearing smoother and more regular. A possible
application would be the articial enhancement of poor handwriting.
6 Conclusions and Future Work
This paper has demonstrated the ability of Long Short-Term Memory recur-
rent neural networks to generate both discrete and real-valued sequences with
complex, long-range structure using next-step prediction. It has also introduced
a novel convolutional mechanism that allows a recurrent network to condition
its predictions on an auxiliary annotation sequence, and used this approach to
synthesise diverse and realistic samples of online handwriting. Furthermore, it
has shown how these samples can be biased towards greater legibility, and how
they can be modelled on the style of a particular writer.
Several directions for future work suggest themselves. One is the applica-
tion of the network to speech synthesis, which is likely to be more challenging
than handwriting synthesis due to the greater dimensionality of the data points.
Another is to gain a better insight into the internal representation of the data,
and to use this to manipulate the sample distribution directly. It would also
be interesting to develop a mechanism to automatically extract high-level an-
notations from sequence data. In the case of handwriting, this could allow for
34Figure 16: Samples biased towards higher probability. The probability
biasesbare shown at the left. As the bias increases the diversity decreases and
the samples tend towards a kind of `average handwriting' which is extremely
regular and easy to read (easier, in fact, than most of the real handwriting in the
training set). Note that even when the variance disappears, the same letter is
not written the same way at dierent points in a sequence (for examples the `e's
in \exactly the same", the `l's in \until they all look"), because the predictions
are still inuenced by the previous outputs. If you look closely you can see that
the last three lines are not quite exactly the same.
35Figure 17: A slight bias. The top line in each block is real. The rest are
samples from the synthesis network with a probability bias of 0.15, which seems
to give a good balance between diversity and legibility.
36Figure 18: Samples primed with real sequences. The priming sequences
(drawn from the training set) are shown at the top of each block. None of the
lines in the sampled text exist in the training set. The samples were selected
for legibility.
37Figure 19: Samples primed with real sequences (cotd).
38Figure 20: Samples primed with real sequences and biased towards
higher probability. The priming sequences are at the top of the blocks. The
probability bias was 1. None of the lines in the sampled text exist in the training
set.
39Figure 21: Samples primed with real sequences and biased towards
higher probability (cotd)
40more nuanced annotations than just text, for example stylistic features, dierent
forms of the same letter, information about stroke order and so on.
Acknowledgements
Thanks to Yichuan Tang, Ilya Sutskever, Navdeep Jaitly, Georey Hinton and
other colleagues at the University of Toronto for numerous useful comments
and suggestions. This work was supported by a Global Scholarship from the
Canadian Institute for Advanced Research.
References
[1] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies
with gradient descent is dicult. IEEE Transactions on Neural Networks ,
5(2):157{166, March 1994.
[2] C. Bishop. Mixture density networks. Technical report, 1994.
[3] C. Bishop. Neural Networks for Pattern Recognition . Oxford University
Press, Inc., 1995.
[4] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Modeling tempo-
ral dependencies in high-dimensional sequences: Application to polyphonic
music generation and transcription. In Proceedings of the Twenty-nine In-
ternational Conference on Machine Learning (ICML'12) , 2012.
[5] J. G. Cleary, Ian, and I. H. Witten. Data compression using adaptive cod-
ing and partial string matching. IEEE Transactions on Communications ,
32:396{402, 1984.
[6] D. Eck and J. Schmidhuber. A rst look at music composition using lstm
recurrent neural networks. Technical report, IDSIA USI-SUPSI Instituto
Dalle Molle.
[7] F. Gers, N. Schraudolph, and J. Schmidhuber. Learning precise timing
with LSTM recurrent networks. Journal of Machine Learning Research ,
3:115{143, 2002.
[8] A. Graves. Practical variational inference for neural networks. In Advances
in Neural Information Processing Systems , volume 24, pages 2348{2356.
2011.
[9] A. Graves. Sequence transduction with recurrent neural networks. In ICML
Representation Learning Worksop , 2012.
[10] A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep
recurrent neural networks. In Proc. ICASSP , 2013.
41[11] A. Graves and J. Schmidhuber. Framewise phoneme classication with bidi-
rectional LSTM and other neural network architectures. Neural Networks ,
18:602{610, 2005.
[12] A. Graves and J. Schmidhuber. Oine handwriting recognition with multi-
dimensional recurrent neural networks. In Advances in Neural Information
Processing Systems , volume 21, 2008.
[13] P. D. Gr unwald. The Minimum Description Length Principle (Adaptive
Computation and Machine Learning) . The MIT Press, 2007.
[14] G. Hinton. A Practical Guide to Training Restricted Boltzmann Machines.
Technical report, 2010.
[15] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient Flow
in Recurrent Nets: the Diculty of Learning Long-term Dependencies.
In S. C. Kremer and J. F. Kolen, editors, A Field Guide to Dynamical
Recurrent Neural Networks . 2001.
[16] S. Hochreiter and J. Schmidhuber. Long Short-Term Memory. Neural
Computation , 9(8):1735{1780, 1997.
[17] M. Hutter. The Human Knowledge Compression Contest, 2012.
[18] K.-C. Jim, C. Giles, and B. Horne. An analysis of noise in recurrent neural
networks: convergence and generalization. Neural Networks, IEEE Trans-
actions on , 7(6):1424 {1438, 1996.
[19] S. Johansson, R. Atwell, R. Garside, and G. Leech. The tagged LOB corpus
user's manual; Norwegian Computing Centre for the Humanities, 1986.
[20] B. Knoll and N. de Freitas. A machine learning perspective on predictive
coding with paq. CoRR , abs/1108.3298, 2011.
[21] M. Liwicki and H. Bunke. IAM-OnDB - an on-line English sentence
database acquired from handwritten text on a whiteboard. In Proc. 8th
Int. Conf. on Document Analysis and Recognition , volume 2, pages 956{
961, 2005.
[22] M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. Building a large
annotated corpus of english: The penn treebank. COMPUTATIONAL
LINGUISTICS , 19(2):313{330, 1993.
[23] T. Mikolov. Statistical Language Models based on Neural Networks . PhD
thesis, Brno University of Technology, 2012.
[24] T. Mikolov, I. Sutskever, A. Deoras, H. Le, S. Kombrink, and J. Cernocky.
Subword language modeling with neural networks. Technical report, Un-
published Manuscript, 2012.
42[25] A. Mnih and G. Hinton. A Scalable Hierarchical Distributed Language
Model. In Advances in Neural Information Processing Systems , volume 21,
2008.
[26] A. Mnih and Y. W. Teh. A fast and simple algorithm for training neural
probabilistic language models. In Proceedings of the 29th International
Conference on Machine Learning , pages 1751{1758, 2012.
[27] T. N. Sainath, A. Mohamed, B. Kingsbury, and B. Ramabhadran. Low-
rank matrix factorization for deep neural network training with high-
dimensional output targets. In Proc. ICASSP , 2013.
[28] M. Schuster. Better generative models for sequential data problems: Bidi-
rectional recurrent mixture density networks. pages 589{595. The MIT
Press, 1999.
[29] I. Sutskever, G. E. Hinton, and G. W. Taylor. The recurrent temporal
restricted boltzmann machine. pages 1601{1608, 2008.
[30] I. Sutskever, J. Martens, and G. Hinton. Generating text with recurrent
neural networks. In ICML , 2011.
[31] G. W. Taylor and G. E. Hinton. Factored conditional restricted boltzmann
machines for modeling motion style. In Proc. 26th Annual International
Conference on Machine Learning , pages 1025{1032, 2009.
[32] T. Tieleman and G. Hinton. Lecture 6.5 - rmsprop: Divide the gradient by
a running average of its recent magnitude, 2012.
[33] R. Williams and D. Zipser. Gradient-based learning algorithms for recur-
rent networks and their computational complexity. In Back-propagation:
Theory, Architectures and Applications , pages 433{486. 1995.
43FlashAttention : Fast and Memory-Eﬃcient Exact Attention
with IO-Awareness
Tri Daoy, Daniel Y. Fuy, Stefano Ermony, Atri Rudraz, and Christopher Réy
yDepartment of Computer Science, Stanford University
zDepartment of Computer Science and Engineering, University at Buﬀalo, SUNY
{trid,danfu}@cs.stanford.edu ,ermon@stanford.edu ,atri@buffalo.edu ,
chrismre@cs.stanford.edu
June 24, 2022
Abstract
Transformers are slow and memory-hungry on long sequences, since the time and memory complexity
of self-attention are quadratic in sequence length. Approximate attention methods have attempted
to address this problem by trading oﬀ model quality to reduce the compute complexity, but often do
not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-
aware—accounting for reads and writes between levels of GPU memory. We propose FlashAttention ,
an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes
between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity
ofFlashAttention , showing that it requires fewer HBM accesses than standard attention, and is
optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding
an approximate attention algorithm that is faster than any existing approximate attention method.
FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup
on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3 speedup on
GPT-2 (seq. length 1K), and 2.4 speedup on long-range arena (seq. length 1K-4K). FlashAttention
and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models
(0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classiﬁcation) and entirely new
capabilities: the ﬁrst Transformers to achieve better-than-chance performance on the Path-X challenge
(seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).
1 Introduction
Transformer models [ 82] have emerged as the most widely used architecture in applications such as natural
language processing and image classiﬁcation. Transformers have grown larger [ 5] and deeper [ 83], but
equipping them with longer context remains diﬃcult [ 80], since the self-attention module at their heart
has time and memory complexity quadratic in sequence length. An important question is whether making
attention faster and more memory-eﬃcient can help Transformer models address their runtime and memory
challenges for long sequences.
Many approximate attention methods have aimed to reduce the compute and memory requirements of
attention. These methods range from sparse-approximation [ 51,74] to low-rank approximation [ 12,50,84],
and their combinations [ 3,9,92]. Although these methods reduce the compute requirements to linear or
near-linear in sequence length, many of them do not display wall-clock speedup against standard attention
and have not gained wide adoption. One main reason is that they focus on FLOP reduction (which may not
correlate with wall-clock speed) and tend to ignore overheads from memory access (IO).
In this paper, we argue that a missing principle is making attention algorithms IO-aware [1]—that is,
carefully accounting for reads and writes to diﬀerent levels of fast and slow memory (e.g., between fast GPU
on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM [ 45], Figure 1 left). On modern
1arXiv:2205.14135v2  [cs.LG]  23 Jun 2022FlashAttentionMemory Hierarchy with
Bandwidth & Memory SizeAttention on GPT-2
FlashAttention PyTorchTime (ms)
MatmulMaskSoftmaxDropoutMatmul
Fused
KernelQ: N x d V: N X dKT: d x N
QKT: N x N
sm(Q KT)V: N x dOuter Loop
Copy Block to SRAM
CopyOuter Loop
CopyInner LoopCompute Block
on SRAM
Output to HBM
Inner LoopInner LoopOuter Loop
GPU
SRAM
GPU
HBM
Main Memory
(CPU DRAM)SRAM : 19 TB/s (20 MB)
HBM: 1.5 TB/s (40 GB)
DRAM : 12.8 GB/s
                (>1 TB)
051015Figure 1: Left: FlashAttention uses tiling to prevent materialization of the large 𝑁𝑁attention matrix
(dotted box) on (relatively) slow GPU HBM. In the outer loop (red arrows), FlashAttention loops through
blocks of the KandVmatrices and loads them to fast on-chip SRAM. In each block, FlashAttention
loops over blocks of Qmatrix (blue arrows), loading them to SRAM, and writing the output of the attention
computation back to HBM. Right:Speedup over the PyTorch implementation of attention on GPT-2.
FlashAttention does not read and write the large 𝑁𝑁attention matrix to HBM, resulting in an 7.6 
speedup on the attention computation.
GPUs, compute speed has out-paced memory speed [ 61,62,63], and most operations in Transformers are
bottlenecked by memory accesses [ 43]. IO-aware algorithms have been critical for similar memory-bound
operations, when reading and writing data can account for a large portion of the runtime—such as database
joins [71], image processing [ 70], numerical linear algebra [ 4], and more [ 40,85]. However, common Python
interfaces to deep learning such as PyTorch and Tensorﬂow do not allow ﬁne-grained control of memory
access.
We propose FlashAttention , a new attention algorithm that computes exact attention with far fewer
memory accesses. Our main goal is to avoid reading and writing the attention matrix to and from HBM.
This requires (i) computing the softmax reduction without access to the whole input (ii) not storing the large
intermediate attention matrix for the backward pass. We apply two well-established techniques to address
these challenges. (i) We restructure the attention computation to split the input into blocks and make several
passes over input blocks, thus incrementally performing the softmax reduction (also known as tiling). (ii) We
store the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the
backward pass, which is faster than the standard approach of reading the intermediate attention matrix from
HBM. We implement FlashAttention in CUDA to achieve ﬁne-grained control over memory access and
fuse all the attention operations into one GPU kernel. Even with the increased FLOPs due to recomputation,
our algorithm both runs faster (up to 7.6x on GPT-2 [ 67], Figure 1 right) and uses less memory —linear
in sequence length—than standard attention, thanks to the massively reduced amount of HBM access.
We analyze the IO complexity [ 1] ofFlashAttention , proving that it requires 𝑂¹𝑁2𝑑2𝑀 1ºHBM
accesses where 𝑑is the head dimension and 𝑀is the size of SRAM, as compared to Ω¹𝑁𝑑¸𝑁2ºof standard
attention. For typical values of 𝑑and𝑀,FlashAttention requires many times fewer HBM accesses
compared to standard attention (up to 9 fewer, as shown in Fig. 2). Moreover, we provide a lower bound,
showing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over
all SRAM sizes.
We also show that FlashAttention can serve as a useful primitive for realizing the potential of
approximate attention algorithms by overcoming their issues with memory access overhead. As a proof of
concept, we implement block-sparse FlashAttention , a sparse attention algorithm that is 2-4 faster than
evenFlashAttention , scaling up to sequence length of 64k. We prove that block-sparse FlashAttention
has better IO complexity than FlashAttention by a factor proportional to the sparsity ratio. We discuss
further extensions to other operations (attention on multi-GPU, kernel regression, block-sparse matrix
2multiply) in Section 5. We open-source FlashAttention to make it easier to build on this primitive.1
We empirically validate that FlashAttention speeds up model training and improves model quality by
modeling longer context. We also benchmark the runtime and memory footprint of FlashAttention and
block-sparse FlashAttention compared to prior attention implementations.
•Faster Model Training. FlashAttention trains Transformer models faster in wall-clock time. We
train BERT-large (seq. length 512) 15% faster than the training speed record in MLPerf 1.1 [ 58], GPT2
(seq. length 1K) 3 faster than baseline implementations from HuggingFace [ 87] and Megatron-LM [ 77],
and long-range arena (seq. length 1K-4K) 2.4 faster than baselines.
•Higher Quality Models. FlashAttention scales Transformers to longer sequences, which improves
their quality and enables new capabilities. We observe a 0.7 improvement in perplexity on GPT-2 and
6.4 points of lift from modeling longer sequences on long-document classiﬁcation [13]. FlashAttention
enables the ﬁrst Transformer that can achieve better-than-chance performance on the Path-X [ 80] challenge,
solely from using a longer sequence length (16K). Block-sparse FlashAttention enables a Transformer
to scale to even longer sequences (64K), resulting in the ﬁrst model that can achieve better-than-chance
performance on Path-256.
•Benchmarking Attention. FlashAttention is up to 3faster than the standard attention implemen-
tation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512,
FlashAttention is both faster and more memory-eﬃcient than any existing attention method, whereas
for sequence length beyond 1K, some approximate attention methods (e.g., Linformer) start to become
faster. On the other hand, block-sparse FlashAttention is faster than all existing approximate attention
methods that we know of.
2 Background
We provide some background on the performance characteristics of common deep learning operations on
modern hardware (GPUs). We also describe the standard implementation of attention.
2.1 Hardware Performance
We focus here on GPUs. Performance on other hardware accelerators are similar [46, 48].
GPU Memory Hierarchy. The GPU memory hierarchy (Fig. 1 left) comprises multiple forms of
memory of diﬀerent sizes and speeds, with smaller memory being faster. As an example, the A100 GPU
has 40-80GB of high bandwidth memory (HBM) with bandwidth 1.5-2.0TB/s and 192KB of on-chip SRAM
per each of 108 streaming multiprocessors with bandwidth estimated around 19TB/s [ 44,45]. The on-chip
SRAM is an order of magnitude faster than HBM but many orders of magnitude smaller in size. As compute
has gotten faster relative to memory speed [ 61,62,63], operations are increasingly bottlenecked by memory
(HBM) accesses. Thus exploiting fast SRAM becomes more important.
Execution Model. GPUs have a massive number of threads to execute an operation (called a kernel).
Each kernel loads inputs from HBM to registers and SRAM, computes, then writes outputs to HBM.
Performance characteristics. Depending on the balance of computation and memory accesses, op-
erations can be classiﬁed as either compute-bound or memory-bound. This is commonly measured by the
arithmetic intensity [85], which is the number of arithmetic operations per byte of memory access.
1.Compute-bound: the time taken by the operation is determined by how many arithmetic operations there
are, while time accessing HBM is much smaller. Typical examples are matrix multiply with large inner
dimension, and convolution with large number of channels.
2.Memory-bound: the time taken by the operation is determined by the number of memory accesses, while
time spent in computation is much smaller. Examples include most other operations: elementwise (e.g.,
activation, dropout), and reduction (e.g., sum, softmax, batch norm, layer norm).
Kernel fusion. The most common approach to accelerate memory-bound operations is kernel fusion: if
there are multiple operations applied to the same input, the input can be loaded once from HBM, instead of
multiple times for each operation. Compilers can automatically fuse many elementwise operations [ 53,65,75].
1FlashAttention code is available at https://github.com/HazyResearch/flash-attention
3However, in the context of model training, the intermediate values still need to be written to HBM to save
for the backward pass, reducing the eﬀectiveness of naive kernel fusion.
2.2 Standard Attention Implementation
Given input sequences QKV2R𝑁𝑑where𝑁is the sequence length and 𝑑is the head dimension, we want
to compute the attention output O2R𝑁𝑑:
S=QK>2R𝑁𝑁P=softmax¹Sº2R𝑁𝑁O=PV2R𝑁𝑑
where softmax is applied row-wise.
Standard attention implementations materialize the matrices SandPto HBM, which takes 𝑂¹𝑁2ºmemory.
Often𝑁𝑑(e.g., for GPT2, 𝑁=1024and𝑑=64). We describe the standard attention implementation
in Algorithm 0. As some or most of the operations are memory-bound (e.g., softmax), the large number of
memory accesses translates to slow wall-clock time.
This problem is exacerbated by other elementwise operations applied to the attention matrix, such as
masking applied to Sor dropout applied to P. As a result, there have been many attempts to fuse several
elementwise operations, such as fusing masking with softmax [77].
In Section 3.2, we will show that the standard attention implementation performs HBM accesses quadratic
in the sequence length 𝑁. We also compare the number of FLOPs and number of HBM accesses of standard
attention and of our method ( FlashAttention ).
Algorithm 0 Standard Attention Implementation
Require: Matrices QKV2R𝑁𝑑in HBM.
1:Load QKby blocks from HBM, compute S=QK>, write Sto HBM.
2:Read Sfrom HBM, compute P=softmax¹Sº, write Pto HBM.
3:Load PandVby blocks from HBM, compute O=PV, write Oto HBM.
4:Return O.
3FlashAttention : Algorithm, Analysis, and Extensions
We show how to compute exact attention with fewer HBM reads/writes and without storing large intermediate
matrices for the backward pass. This yields an attention algorithm that is both memory eﬃcient and faster in
wall-clock time. We analyze its IO complexity, showing that our method requires much fewer HBM accesses
compared to standard attention. We further show that FlashAttention can serve as a useful primitive by
extending it to handle block-sparse attention.
We focus here on the forward pass for ease of exposition; Appendix B contains details for the backward.
3.1 An Eﬃcient Attention Algorithm With Tiling and Recomputation
Given the inputs QKV2R𝑁𝑑in HBM, we aim to compute the attention output O2R𝑁𝑑and write it to
HBM. Our goal is to reduce the amount of HBM accesses (to sub-quadratic in 𝑁).
We apply two established techniques (tiling, recomputation) to overcome the technical challenge of
computing exact attention in sub-quadratic HBM accesses. We describe this in Algorithm 1. The main idea
is that we split the inputs QKVinto blocks, load them from slow HBM to fast SRAM, then compute the
attention output with respect to those blocks. By scaling the output of each block by the right normalization
factor before adding them up, we get the correct result at the end.
Tiling. We compute attention by blocks. Softmax couples columns of K, so we decompose the large
softmax with scaling [51, 60, 66]. For numerical stability, the softmax of vector 𝑥2R𝐵is computed as:
𝑚¹𝑥º:=max
𝑖𝑥𝑖 𝑓¹𝑥º:=
𝑒𝑥1 𝑚¹𝑥º 𝑒𝑥𝐵 𝑚¹𝑥º
 ℓ¹𝑥º:=∑︁
𝑖𝑓¹𝑥º𝑖softmax¹𝑥º:=𝑓¹𝑥º
ℓ¹𝑥º
4For vectors 𝑥¹1º𝑥¹2º2R𝐵, we can decompose the softmax of the concatenated 𝑥=
𝑥¹1º𝑥¹2º
2R2𝐵as:
𝑚¹𝑥º=𝑚¹
𝑥¹1º𝑥¹2º
º=max¹𝑚¹𝑥¹1ºº𝑚¹𝑥¹2ººº 𝑓¹𝑥º=h
𝑒𝑚¹𝑥¹1ºº 𝑚¹𝑥º𝑓¹𝑥¹1ºº𝑒𝑚¹𝑥¹2ºº 𝑚¹𝑥º𝑓¹𝑥¹2ººi

ℓ¹𝑥º=ℓ¹
𝑥¹1º𝑥¹2º
º=𝑒𝑚¹𝑥¹1ºº 𝑚¹𝑥ºℓ¹𝑥¹1ºº¸𝑒𝑚¹𝑥¹2ºº 𝑚¹𝑥ºℓ¹𝑥¹2ººsoftmax¹𝑥º=𝑓¹𝑥º
ℓ¹𝑥º
Therefore if we keep track of some extra statistics ( 𝑚¹𝑥ºℓ¹𝑥º), we can compute softmax one block at a time.2
We thus split the inputs QKVinto blocks (Algorithm 1 line 3), compute the softmax values along with
extra statistics (Algorithm 1 line 10), and combine the results (Algorithm 1 line 12).
Recomputation. One of our goals is to not store 𝑂¹𝑁2ºintermediate values for the backward pass. The
backward pass typically requires the matrices SP2R𝑁𝑁to compute the gradients with respect to QKV.
However, by storing the output Oand the softmax normalization statistics ¹𝑚ℓº, we can recompute the
attention matrix SandPeasily in the backward pass from blocks of QKVin SRAM. This can be seen as a
form of selective gradient checkpointing [ 10,34]. While gradient checkpointing has been suggested to reduce
the maximum amount of memory required [ 66], all implementations (that we know oﬀ) have to trade speed
for memory. In contrast, even with more FLOPs, our recomputation speeds up the backward pass due to
reduced HBM accesses (Fig. 2). The full backward pass description is in Appendix B.
Implementation details: Kernel fusion. Tiling enables us to implement our algorithm in one
CUDA kernel, loading input from HBM, performing all the computation steps (matrix multiply, softmax,
optionally masking and dropout, matrix multiply), then write the result back to HBM (masking and dropout
in Appendix B). This avoids repeatedly reading and writing of inputs and outputs from and to HBM.
Algorithm 1 FlashAttention
Require: Matrices QKV2R𝑁𝑑in HBM, on-chip SRAM of size 𝑀.
1:Set block sizes 𝐵𝑐=𝑀
4𝑑
𝐵𝑟=min 𝑀
4𝑑
𝑑.
2:Initialize O=¹0º𝑁𝑑2R𝑁𝑑ℓ=¹0º𝑁2R𝑁𝑚=¹ 1º𝑁2R𝑁in HBM.
3:Divide Qinto𝑇𝑟=l
𝑁
𝐵𝑟m
blocks Q1Q𝑇𝑟of size𝐵𝑟𝑑each, and divide KVin to𝑇𝑐=l
𝑁
𝐵𝑐m
blocks
K1K𝑇𝑐andV1V𝑇𝑐, of size𝐵𝑐𝑑each.
4:Divide Ointo𝑇𝑟blocks O𝑖O𝑇𝑟of size𝐵𝑟𝑑each, divide ℓinto𝑇𝑟blocksℓ𝑖ℓ𝑇𝑟of size𝐵𝑟each,
divide𝑚into𝑇𝑟blocks𝑚1𝑚𝑇𝑟of size𝐵𝑟each.
5:for1𝑗𝑇𝑐do
6:Load K𝑗V𝑗from HBM to on-chip SRAM.
7:for1𝑖𝑇𝑟do
8:Load Q𝑖O𝑖ℓ𝑖𝑚𝑖from HBM to on-chip SRAM.
9:On chip, compute S𝑖𝑗=Q𝑖K𝑇
𝑗2R𝐵𝑟𝐵𝑐.
10:On chip, compute ~𝑚𝑖𝑗=rowmax¹S𝑖𝑗º 2R𝐵𝑟,~P𝑖𝑗=exp¹S𝑖𝑗 ~𝑚𝑖𝑗º 2R𝐵𝑟𝐵𝑐(pointwise), ~ℓ𝑖𝑗=
rowsum¹~P𝑖𝑗º2R𝐵𝑟.
11:On chip, compute 𝑚new
𝑖=max¹𝑚𝑖~𝑚𝑖𝑗º2R𝐵𝑟,ℓnew
𝑖=𝑒𝑚𝑖 𝑚new
𝑖ℓ𝑖¸𝑒~𝑚𝑖𝑗 𝑚new
𝑖~ℓ𝑖𝑗2R𝐵𝑟.
12:Write O𝑖 diag¹ℓnew
𝑖º 1¹diag¹ℓ𝑖º𝑒𝑚𝑖 𝑚new
𝑖O𝑖¸𝑒~𝑚𝑖𝑗 𝑚new
𝑖~P𝑖𝑗V𝑗ºto HBM.
13:Writeℓ𝑖 ℓnew
𝑖,𝑚𝑖 𝑚new
𝑖to HBM.
14:end for
15:end for
16:Return O.
We show FlashAttention ’s correctness, runtime, and memory requirement (proof in Appendix C).
Theorem 1. Algorithm 1 returns O=softmax¹QK>ºVwith𝑂¹𝑁2𝑑ºFLOPs and requires 𝑂¹𝑁ºadditional
memory beyond inputs and output.
3.2 Analysis: IO Complexity of FlashAttention
We analyze the IO complexity of FlashAttention , showing signiﬁcant reduction in HBM accesses compared
to standard attention. We also provide a lower bound, proving that no exact attention algorithm can
2This style of aggregation is called algebraic aggregation [33].
5Attention Standard FlashAttention
GFLOPs 66.6 75.2
HBM R/W (GB) 40.3 4.4
Runtime (ms) 41.7 7.3
Sparsity Speedup
% Non-Zero Blocks20 6050100150Fwd + Bwd (ms)Eﬀect of Block Size
Block Size64128 256 512Fwd Runtime (ms)
6
2HBM Accesses (GB)Dense
FlashAttention
Block-Sparse
FlashAttention246
RuntimeHBMAccesses
Figure 2: Left: Forward + backward runtime of standard attention and FlashAttention for GPT-2 medium
(seq. length 1024, head dim. 64, 16 heads, batch size 64) on A100 GPU. HBM access is the primary factor aﬀecting
runtime. Middle: Forward runtime of FlashAttention (seq. length 1024, head dim. 64, 16 heads, batch size 64) on
A100 GPU. Fewer HBM accesses result in faster runtime, up to a point. Right: The runtime (for seq. length 4K) of
block-sparse FlashAttention is faster than FlashAttention by a factor proportional to the sparsity.
asymptotically improve on HBM accesses over all SRAM sizes. Proofs are in Appendix C.
Theorem2. Let𝑁be the sequence length, 𝑑be the head dimension, and 𝑀be size of SRAM with 𝑑𝑀𝑁𝑑.
Standard attention (Algorithm 0) requires Θ¹𝑁𝑑¸𝑁2ºHBM accesses, while FlashAttention (Algorithm 1)
requiresΘ¹𝑁2𝑑2𝑀 1ºHBM accesses.
For typical values of 𝑑(64-128) and 𝑀(around 100KB), 𝑑2is many times smaller than 𝑀, and thus
FlashAttention requires many times fewer HBM accesses than standard implementation. This leads to
both faster execution and lower memory footprint, which we validate in Section 4.3.
The main idea of the proof is that given the SRAM size of 𝑀, we can load blocks of KVof sizeΘ¹𝑀ºeach
(Algorithm 1 line 6). For each block of KandV, we iterate over all blocks of Q(Algorithm 1 line 8) to compute
the intermediate values, resulting in Θ¹𝑁𝑑𝑀 1ºpasses over Q. Each pass loads Θ¹𝑁𝑑ºelements, which
amounts to Θ¹𝑁2𝑑2𝑀 1ºHBM accesses. We similarly prove that the backward pass of standard attention
requiresΘ¹𝑁𝑑¸𝑁2ºHBM accesses while the backward pass of FlashAttention requiresΘ¹𝑁2𝑑2𝑀 1º
HBM accesses (Appendix B).
We prove a lower-bound: one cannot asymptotically improve on the number of HBM accesses for all
values of𝑀(the SRAM size) when computing exact attention.
Proposition 3. Let𝑁be the sequence length, 𝑑be the head dimension, and 𝑀be size of SRAM with
𝑑𝑀𝑁𝑑. There does not exist an algorithm to compute exact attention with 𝑜¹𝑁2𝑑2𝑀 1ºHBM accesses
for all𝑀in the range»𝑑𝑁𝑑¼.
The proof relies on the fact that for 𝑀= Θ¹𝑁𝑑ºany algorithm must perform Ω¹𝑁2𝑑2𝑀 1º= Ω¹𝑁𝑑º
HBM accesses. This type of lower bound over a subrange of 𝑀is common in the streaming algorithms
literature [ 88]. We leave proving parameterized complexity [ 27] lower bounds in terms of 𝑀as exciting future
work.
We validate that the number of HBM accesses is the main determining factor of attention run-time.
In Fig. 2 (left), we see that even though FlashAttention has higher FLOP count compared to standard
attention (due to recomputation in the backward pass), it has much fewer HBM accesses, resulting in much
faster runtime. In Fig. 2 (middle), we vary the block size 𝐵𝑐ofFlashAttention , which results in diﬀerent
amounts of HBM accesses, and measure the runtime of the forward pass. As block size increases, the number
of HBM accesses decreases (as we make fewer passes over the input), and runtime decreases. For large enough
block size (beyond 256), the runtime is then bottlenecked by other factors (e.g., arithmetic operations).
Moreover, larger block size will not ﬁt into the small SRAM size.
3.3 Extension: Block-Sparse FlashAttention
We extend FlashAttention to approximate attention: we propose block-sparse FlashAttention , whose
IO complexity is smaller than FlashAttention by a factor proportional to the sparsity.
Given inputs QKV2R𝑁𝑑and a mask matrix ~M2f01g𝑁𝑁, we want to compute:
S=QK>2R𝑁𝑁P=softmax¹S𝟙~Mº2R𝑁𝑁O=PV2R𝑁𝑑
where¹S𝟙~Mº𝑘𝑙=S𝑘𝑙if~M𝑘𝑙=1and 1ifM𝑘𝑙=0. We require ~Mto have block form: for some block sizes
𝐵𝑟𝐵𝑐, for all𝑘𝑙,~M𝑘𝑙=M𝑖𝑗with𝑖=b𝑘𝐵𝑟c𝑗=b𝑙𝐵𝑐cfor some M2f01g𝑁𝐵𝑟𝑁𝐵𝑐.
6Given a predeﬁned block sparsity mask M2f01g𝑁𝐵𝑟𝑁𝐵𝑐we can easily adapt Algorithm 1 to only
compute the nonzero blocks of the attention matrix. The algorithm is identical to Algorithm 1, except we
skip zero blocks. We reproduce the algorithm description in Algorithm 5 in Appendix B.
We also analyze the IO complexity of block-sparse FlashAttention .
Proposition 4. Let𝑁be the sequence length, 𝑑be the head dimension, and 𝑀be size of SRAM with
𝑑𝑀𝑁𝑑. Block-sparse FlashAttention (Algorithm 5) requires Θ¹𝑁𝑑¸𝑁2𝑑2𝑀 1𝑠ºHBM accesses
where𝑠is the fraction of nonzero blocks in the block-sparsity mask.
We see that applying block-sparsity yields a direct improvement by the sparsity to the larger term in the
IO complexity. For large sequence lengths 𝑁,𝑠is often set to 𝑁 12[11] or𝑁 1log𝑁[3,17,92], resulting
inΘ¹𝑁p
𝑁ºorΘ¹𝑁log𝑁ºIO complexity. For downstream experiments, we use the ﬁxed butterﬂy sparsity
pattern [17], which has been shown to be able to approximate arbitrary sparsity [16].
In Fig. 2 (right), we validate that as the sparsity increases, the runtime of block-sparse FlashAttention
improves proportionally. On the LRA benchmark, block-sparse FlashAttention achieves 2.8speedup,
while performing on par with standard attention (Section 4).
4 Experiments
We evaluate the impact of using FlashAttention to train Transformer models. We validate two claims
about training time and model accuracy, and report attention runtime and memory benchmarks.
•Training Speed. FlashAttention outperforms the MLPerf 1.1 [ 58] speed record for BERT by 15%, and
speeds up GPT-2 up to 3 over HuggingFace [ 87] and 18over Megatron [ 77] over standard Transformers.
FlashAttention speeds up the long-range arena (LRA) benchmark 2.4 .
•Quality. FlashAttention scales Transformers to longer sequences, yielding higher quality. FlashAt-
tention trains GPT-2 with context length 4K faster than Megatron trains GPT-2 with context length
1K, while achieving 0.7 better perplexity. Modeling longer sequences yields 6.4 points of lift on two long-
document classiﬁcation tasks. Finally, FlashAttention yields the ﬁrst Transformer that can achieve
better-than-random performance on the challenging Path-X task (sequence length 16K), and block-sparse
FlashAttention yields the ﬁrst sequence model that we know of that can achieve better-than-random
performance on Path-256 (sequence length 64K).
•Benchmarking Attention. We measure the runtime and memory performance of FlashAttention
and block-sparse FlashAttention based on sequence length. We conﬁrm that the memory footprint
ofFlashAttention scales linearly with seq. length and is up to 3 faster than standard attention for
common seq. lengths (up to 2K). We conﬁrm that runtime of block-sparse FlashAttention scales linearly
in seq. length and is faster than all existing approximate attention baselines.
Additional experiment details are in Appendix E.
4.1 Faster Models with FlashAttention
BERT. FlashAttention yields the fastest single-node BERT training speed that we know of. We train a
BERT-large [ 22] model with FlashAttention on Wikipedia. Table 1 compares our training time to the
implementation from Nvidia that set the training speed record for MLPerf 1.1 [ 58]. Our implementation is
15% faster.
Table 1: Training time of BERT-large, starting from the same initialization provided by the MLPerf benchmark, to
reach the target accuracy of 72.0% on masked language modeling. Averaged over 10 runs on 8 A100 GPUs.
BERT Implementation Training time (minutes)
Nvidia MLPerf 1.1 [58] 20.01.5
FlashAttention (ours) 17.41.4
GPT-2. FlashAttention yieldsfastertrainingtimesforGPT-2[ 67]onthelargeOpenWebtextdataset[ 32]
than the widely used HuggingFace [ 87] and Megatron-LM [ 77] implementations. Table 2 shows up to 3 end-
to-end speedup compared to Huggingface and 1.7 speedup compared to Megatron-LM. FlashAttention
7achieves the same perplexity as the other two implementations, as we do not change the model deﬁnition.
Appendix E includes plots of the validation perplexity throughout training, conﬁrming that FlashAttention
is as numerically stable as the baselines and produces the same training / validation curves.
Table 2: GPT-2 small and medium using FlashAttention achieve up to 3speed up compared to Huggingface
implementation and up to 1.7 compared to Megatron-LM. Training time reported on 8 A100s GPUs.
Model implementations OpenWebText (ppl) Training time (speedup)
GPT-2 small - Huggingface [87] 18.2 9.5 days (1.0 )
GPT-2 small - Megatron-LM [77] 18.2 4.7 days (2.0 )
GPT-2 small - FlashAttention 18.2 2.7 days (3.5)
GPT-2 medium - Huggingface [87] 14.2 21.0 days (1.0 )
GPT-2 medium - Megatron-LM [77] 14.3 11.5 days (1.8 )
GPT-2 medium - FlashAttention 14.3 6.9 days (3.0)
Long-range Arena. We compare vanilla Transformer (with either standard implementation or FlashAt-
tention ) on the long-range arena (LRA [ 80]) benchmark. We measure accuracy, throughput, and training
time of all models. Each task has a diﬀerent sequence length varying between 1024 and 4096. We follow the
implementation and experimental setting in Tay et al. [80]and Xiong et al. [90].3Table 3 shows that FlashAt-
tention achieves up 2.4speed-up compared to standard attention. Block-sparse FlashAttention is
faster than all of the approximate attention methods that we have tested.
Table 3: The performance of standard attention, FlashAttention , block-sparse FlashAttention , and approximate
attention baselines on the Long-Range-Arena benchmarks.
Models ListOps Text Retrieval Image Pathﬁnder AvgSpeedup
Transformer 36.0 63.6 81.6 42.3 72.7 59.3 -
FlashAttention 37.6 63.9 81.4 43.5 72.7 59.8 2.4
Block-sparse FlashAttention 37.0 63.0 81.3 43.6 73.3 59.6 2.8
Linformer [84] 35.6 55.9 77.7 37.8 67.6 54.9 2.5
Linear Attention [50] 38.8 63.2 80.7 42.6 72.5 59.6 2.3
Performer [12] 36.8 63.6 82.2 42.1 69.9 58.9 1.8
Local Attention [80] 36.1 60.2 76.7 40.6 66.6 56.0 1.7
Reformer [51] 36.5 63.8 78.5 39.6 69.4 57.6 1.3
Smyrf [19] 36.1 64.1 79.0 39.6 70.5 57.9 1.7
4.2 Better Models with Longer Sequences
Language Modeling with Long Context. The runtime and memory-eﬃciency of FlashAttention
allow us to increase the context length of GPT-2 by 4 while still running faster than the optimized
implementation from Megatron-LM. Table 4 shows that that GPT-2 with FlashAttention and context
length 4K is still 30% faster than GPT-2 from Megatron with context length 1K, while achieving 0.7 better
perplexity.
Table 4: GPT-2 small with FlashAttention , with 4larger context length compared to Megatron-LM, is still 30%
faster while achieving 0.7 better perplexity. Training time on 8 A100 GPUs is reported.
Model implementations Context length OpenWebText (ppl) Training time (speedup)
GPT-2 small - Megatron-LM 1k 18.2 4.7 days (1.0 )
GPT-2 small - FlashAttention 1k 18.2 2.7 days (1.7)
GPT-2 small - FlashAttention 2k 17.6 3.0 days (1.6 )
GPT-2 small - FlashAttention 4k 17.5 3.6 days (1.3)
Long Document Classiﬁcation. Training Transformers with longer sequences with FlashAttention
improves performance on the MIMIC-III [ 47] and ECtHR [ 6,7] datasets. MIMIC-III contains intensive care
unit patient discharge summaries, each annotated with multiple labels. ECtHR contains legal cases from the
3LRA accuracy results are known to be highly dependent on the tuning procedure [ 90]. Our reproduced baselines perform
better than as reported in the original comparison [80].
8Attention Memory Usage
Sequence LengthAttention Runtime (Fwd Pass + Bwd Pass)
Sequence LengthRuntime (ms)
Memory Footprint (GB)256 8K 16K 32K 64K 128 256 512 1024 2048 4096101102
1020
FlashAttention
Block-Sparse FlashAttentionPyTorch Attention
Megatron AttentionLinformer Attention
OpenAI Sparse Attention8192100Crossover Points
20x2xFigure 3: Left:runtime of forward pass + backward pass. Right:attention memory usage.
European Court of Human Rights, each of which is mapped to articles of the Convention of Human Rights
that were allegedly violaged. Both of these datasets contain very long text documents; the average number of
tokens in MIMIC is 2,395 tokens, and the longest document contains 14,562 tokens, while the average and
longest numbers in ECtHR are 2,197 and 49,392, respectively. We evaluate lift from increasing the sequence
length of a pretrained RoBERTa model [56] (we repeat the positional embeddings, as in Beltagy et al. [3]).
Table 5 shows that sequence length 16K outperforms length 512 by 4.3 points on MIMIC, and that length
8K outperforms length 512 by 8.5 points on ECtHR. The discrepancies may be due to subtle distribution
shifts: MIMIC-III contains specialized medical text and thus may be more susceptible to a distribution shift
in the document length, whereas ECtHR contains general language.
Table 5: Long Document performance (mi-
cro𝐹1) at diﬀerent sequence lengths using
FlashAttention .
512 1024 2048 4096 8192 16384
MIMIC-III [47] 52.8 50.7 51.7 54.6 56.4 57.1
ECtHR [6] 72.2 74.3 77.1 78.6 80.779.2Table 6: We report the ﬁrst Transformer
model that can achieve non-random perfor-
mance on Path-X and Path-256.
Model Path-X Path-256
Transformer 7 7
Linformer [84] 7 7
Linear Attention [50] 7 7
Performer [12] 7 7
Local Attention [80] 7 7
Reformer [51] 7 7
SMYRF [19] 7 7
FlashAttention 61.4 7
Block-sparse FlashAttention 56.0 63.1
Path-X and Path-256. The Path-X and Path-256 benchmarks are challenging tasks from the long-range
arena benchmark designed to test long context. The task is to classify whether two points in a black and
white 128128 (or 256256) image have a path connecting them, and the images are fed to the transformer
one pixel at a time. In prior work, all transformer models have either run out of memory, or only achieved
random performance [ 80]. There has been a search for alternative architectures that can model such long
context [ 37]. We present here the ﬁrst result of Transformer models being able to solve Path-X and Path-256
(Table 6). We pretrain a transformer on Path-64, and then transfer to Path-X by spatially interpolating
the positional embeddings. FlashAttention achieves 61.4 accuracy on Path-X. Additionally, block-sparse
FlashAttention enables the Transformers to scale to sequence length 64K, achieving 63.1 accuracy4on
Path-256.
4.3 Benchmarking Attention
We vary sequence length and measure runtime and memory usage of FlashAttention and block-sparse
FlashAttention against various attention baselines on one A100 GPU with 40 GB HBM, with dropout and
a padding mask. We compare against reference implementations for exact attention, approximate attention,
and sparse attention. We report a subset of baselines in the main body; Appendix E contains more baselines
and full details.
4Path-256 requires longer sequences but has relatively shorter paths than Path-X, so it is easier to obtain a higher accuracy.
9Runtime. Figure 3 (left) reports the runtime in milliseconds of the forward + backward pass of FlashAt-
tention and block-sparse FlashAttention compared to the baselines in exact, approximate, and sparse
attention (exact numbers in Appendix E). Runtime grows quadratically with sequence length, but FlashAt-
tention runs signiﬁcantly faster than exact attention baselines, up to 3 faster than the PyTorch
implementation. The runtimes of many approximate/sparse attention mechanisms grow linearly with se-
quence length, but FlashAttention still runs faster than approximate and sparse attention for short
sequences due to fewer memory accesses. The approximate attention runtimes begin to cross over with
FlashAttention at sequences between 512 and 1024. On the other hand, block-sparse FlashAttention
is faster than all implementations of exact, sparse, and approximate attention that we know of, across all
sequence lengths.
Memory Footprint. Figure 3 (right) shows the memory footprint of FlashAttention and block-sparse
FlashAttention compared to various exact, approximate, and sparse attention baselines. FlashAttention
and block-sparse FlashAttention have the same memory footprint, which grows linearly with sequence
length. FlashAttention is up to 20more memory eﬃcient than exact attention baselines, and is more
memory-eﬃcient than the approximate attention baselines. All other algorithms except for Linformer run
out of memory on an A100 GPU before 64K, and FlashAttention is still 2more eﬃcient than Linformer.
5 Limitations and Future Directions
We discuss limitations of our approach and future directions. Related work is given in Appendix A.
Compiling to CUDA. Our current approach to building IO-aware implementations of attention requires
writing a new CUDA kernel for each new attention implementation. This requires writing the attention
algorithm in a considerably lower-level language than PyTorch, and requires signiﬁcant engineering eﬀort.
Implementations may also not be transferrable across GPU architectures. These limitations suggest the
need for a method that supports writing attention algorithms in a high-level language (e.g., PyTorch), and
compiling to IO-aware implementations in CUDA—similar to eﬀorts such as Halide in image processing [ 70].
IO-Aware Deep Learning. We believe that the IO-aware approach can extend beyond attention.
Attention is the most memory-intensive computation in Transformers, but every layer in a deep network
touches GPU HBM. We hope our work inspires IO-aware implementations of additional modules. We discuss
these potential extensions in Appendix D.
Multi-GPU IO-Aware Methods. Our IO-aware implementation of attention is optimal within con-
stants for computing attention on a single GPU. However, the attention computation may be parallelizable
across multiple GPUs [ 72]. Using multiple GPUs adds an additional layer to IO analysis—accounting for
data transfer between GPUs. We hope our work inspires future work in this direction.
Acknowledgments
Our implementation uses Apex’s FMHA code ( https://github.com/NVIDIA/apex/tree/master/apex/
contrib/csrc/fmha ) as a starting point. We thank Young-Jun Ko for the in-depth explanation of his FMHA
implementation and for his thoughtful answers to our questions about CUDA. We thank Sabri Eyuboglu,
Megan Leszczynski, Laurel Orr, Yuhuai Wu, Beidi Chen, and Xun Huang for their constructive feedback and
suggestions on early drafts of the paper. We thank Markus Rabe and Charles Staats for helpful discussion of
their attention algorithm.
We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos.
CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ARL under
No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266 (Unifying Weak
Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine
Learning; N000142012275 (NEPTUNE); NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba,
TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce,
Total, the HAI-GCP & HAI-Azure Cloud Credits for Research program, the Stanford Data Science Initiative
(SDSI), Department of Defense (DoD) through the National Defense Science and Engineering Graduate
Fellowship (NDSEG) Program, and members of the Stanford DAWN project: Facebook, Google, and
VMWare. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes
10notwithstanding any copyright notation thereon. Any opinions, ﬁndings, and conclusions or recommendations
expressed in this material are those of the authors and do not necessarily reﬂect the views, policies, or
endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government. Atri Rudra’s research is
supported by NSF grant CCF-1763481.
References
[1]Alok Aggarwal and S Vitter, Jeﬀrey. The input/output complexity of sorting and related problems.
Communications of the ACM , 31(9):1116–1127, 1988.
[2]Irwan Bello. LambdaNetworks: Modeling long-range interactions without attention. arXiv preprint
arXiv:2102.08602 , 2021.
[3]Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv
preprint arXiv:2004.05150 , 2020.
[4]L Susan Blackford, Antoine Petitet, Roldan Pozo, Karin Remington, R Clint Whaley, James Demmel,
Jack Dongarra, Iain Duﬀ, Sven Hammarling, Greg Henry, et al. An updated set of basic linear algebra
subprograms (blas). ACM Transactions on Mathematical Software , 28(2):135–151, 2002.
[5]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:1877–1901, 2020.
[6]Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras. Neural legal judgment prediction in English.
InProceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages
4317–4323, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1424.
URL https://www.aclweb.org/anthology/P19-1424 .
[7]Ilias Chalkidis, Manos Fergadiotis, Dimitrios Tsarapatsanis, Nikolaos Aletras, Ion Androutsopoulos, and
Prodromos Malakasiotis. Paragraph-level rationale extraction through regularization: A case study on
european court of human rights cases. In Proceedings of the Annual Conference of the North American
Chapter of the Association for Computational Linguistics , Mexico City, Mexico, 2021. Association for
Computational Linguistics.
[8]Benjamin Charlier, Jean Feydy, Joan Alexis Glaunès, François-David Collin, and Ghislain Durif. Kernel
operations on the gpu, with autodiﬀ, without memory overﬂows. Journal of Machine Learning Research ,
22(74):1–6, 2021. URL http://jmlr.org/papers/v22/20-275.html .
[9]Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Ré. Scatterbrain: Unifying
sparse and low-rank attention. In Advances in Neural Information Processing Systems (NeurIPS) , 2021.
[10]Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory
cost.arXiv preprint arXiv:1604.06174 , 2016.
[11]Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509 , 2019.
[12]Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,
Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking
attention with performers. In International Conference on Learning Representations (ICLR) , 2020.
[13]Xiang Dai, Ilias Chalkidis, Sune Darkner, and Desmond Elliott. Revisiting transformer-based models for
long document classiﬁcation. arXiv preprint arXiv:2204.06683 , 2022.
[14]Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov.
Transformer-XL: Attentive language models beyond a ﬁxed-length context. In Proceedings of the 57th
Annual Meeting of the Association for Computational Linguistics , pages 2978–2988, 2019.
11[15]Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher Ré. Learning fast algorithms
for linear transforms using butterﬂy factorizations. In International Conference on Machine Learning
(ICML), 2019.
[16]Tri Dao, Nimit Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra,
and Christopher Ré. Kaleidoscope: An eﬃcient, learnable representation for all structured linear maps.
InInternational Conference on Learning Representations (ICLR) , 2020.
[17]Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Ré.
Pixelated butterﬂy: Simple and eﬃcient sparse training for neural network models. In International
Conference on Learning Representations (ICLR) , 2022.
[18]Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh
Rao, Atri Rudra, and Christopher Ré. Monarch: Expressive structured matrices for eﬃcient and accurate
training. In International Conference on Machine Learning (ICML) , 2022.
[19]Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros G Dimakis. Smyrf-eﬃcient attention
using asymmetric clustering. Advances in Neural Information Processing Systems , 33:6476–6489, 2020.
[20]Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher Ré, and Atri Rudra. A two-pronged
progress in structured dense matrix vector multiplication. In Proceedings of the Twenty-Ninth Annual
ACM-SIAM Symposium on Discrete Algorithms , pages 1060–1079. SIAM, 2018.
[21]Peter J Denning. The working set model for program behavior. Communications of the ACM , 11(5):
323–333, 1968.
[22]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. 2019.
[23]Xin Dong, Shangyu Chen, and Sinno Jialin Pan. Learning to prune deep neural networks via layer-wise
optimal brain surgeon. arXiv preprint arXiv:1705.07565 , 2017.
[24]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is
worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning
Representations , 2020.
[25]Y Eidelman and I Gohberg. On a new class of structured matrices. Integral Equations and Operator
Theory, 34(3):293–324, 1999.
[26]Jean Feydy, Joan Glaunès, Benjamin Charlier, and Michael Bronstein. Fast geometric learning with
symbolic matrices. Advances in Neural Information Processing Systems , 33, 2020.
[27] Jörg Flum and Martin Grohe. Parameterized Complexity Theory . Springer, 2006.
[28]Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In International Conference on Learning Representations , 2018.
[29]Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Stabilizing the
lottery ticket hypothesis. arXiv preprint arXiv:1903.01611 , 2019.
[30]Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity
and the lottery ticket hypothesis. In International Conference on Machine Learning , pages 3259–3269.
PMLR, 2020.
[31]Karan Goel, Albert Gu, Chris Donahue, and Christopher Ré. It’s raw! audio generation with state-space
models. In International Conference on Machine Learning (ICML) , 2022.
[32] Aaron Gokaslan, Vanya Cohen, Pavlick Ellie, and Stefanie Tellex. Openwebtext corpus, 2019.
12[33]Jim Gray, Surajit Chaudhuri, Adam Bosworth, Andrew Layman, Don Reichart, Murali Venkatrao,
Frank Pellow, and Hamid Pirahesh. Data cube: A relational aggregation operator generalizing group-by,
cross-tab, and sub-totals. Data mining and knowledge discovery , 1(1):29–53, 1997.
[34]Andreas Griewank and Andrea Walther. Evaluating derivatives: principles and techniques of algorithmic
diﬀerentiation . SIAM, 2008.
[35]Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. Hippo: Recurrent memory with
optimal polynomial projections. In Advances in neural information processing systems (NeurIPS) , 2020.
[36]Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré. Combining
recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural
Information Processing Systems , 34, 2021.
[37]Albert Gu, Karan Goel, and Christopher Ré. Eﬃciently modeling long sequences with structured state
spaces. In The International Conference on Learning Representations (ICLR) , 2022.
[38]Song Han, Jeﬀ Pool, John Tran, and William J Dally. Learning both weights and connections for eﬃcient
neural networks. arXiv preprint arXiv:1506.02626 , 2015.
[39]Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huﬀman coding. In International Conference on Learning
Representations , 2016.
[40]John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative
Approach , pages 390–525, 2003.
[41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489 , 2020.
[42]Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv
preprint arXiv:2202.10447 , 2022.
[43]Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoeﬂer. Data movement is all
you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems , 3:
711–732, 2021.
[44]Zhe Jia and Peter Van Sandt. Dissecting the Ampere GPU architecture via microbenchmarking. GPU
Technology Conference, 2021.
[45]Zhe Jia, Marco Maggioni, Benjamin Staiger, and Daniele P Scarpazza. Dissecting the nvidia Volta GPU
architecture via microbenchmarking. arXiv preprint arXiv:1804.06826 , 2018.
[46]Zhe Jia, Blake Tillman, Marco Maggioni, and Daniele Paolo Scarpazza. Dissecting the graphcore IPU
architecture via microbenchmarking. arXiv preprint arXiv:1912.03413 , 2019.
[47]Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi,
Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible
critical care database. Scientiﬁc data , 3(1):1–9, 2016.
[48]Norman P Jouppi, Cliﬀ Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah
Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of a tensor
processing unit. In Proceedings of the 44th annual international symposium on computer architecture ,
pages 1–12, 2017.
[49]Thomas Kailath, Sun-Yuan Kung, and Martin Morf. Displacement ranks of matrices and linear equations.
Journal of Mathematical Analysis and Applications , 68(2):395–407, 1979.
[50]Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are RNNs:
Fast autoregressive transformers with linear attention. In International Conference on Machine Learning ,
pages 5156–5165. PMLR, 2020.
13[51]Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The eﬃcient transformer. In The
International Conference on Machine Learning (ICML) , 2020.
[52]Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.
Albert: A lite BEDRT for self-supervised learning of language representations. In The International
Conference on Learning Representations (ICLR) , 2020.
[53]Mingzhen Li, Yi Liu, Xiaoyan Liu, Qingxiao Sun, Xin You, Hailong Yang, Zhongzhi Luan, Lin Gan,
Guangwen Yang, and Depei Qian. The deep learning compiler: A comprehensive survey. IEEE
Transactions on Parallel and Distributed Systems , 32(3):708–727, 2020.
[54]Valerii Likhosherstov, Krzysztof Choromanski, Jared Davis, Xingyou Song, and Adrian Weller. Sub-linear
memory: How to make performers slim. arXiv preprint arXiv:2012.11346 , 2020.
[55]Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural
Information Processing Systems , volume 30. Curran Associates, Inc., 2017.
[56]Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach.
arXiv preprint arXiv:1907.11692 , 2019.
[57]Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer.
Luna: Linear uniﬁed nested attention. Advances in Neural Information Processing Systems , 34, 2021.
[58]Peter Mattson, Christine Cheng, Gregory Diamos, Cody Coleman, Paulius Micikevicius, David Patterson,
Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor Bittorf, et al. Mlperf training benchmark. Proceedings
of Machine Learning and Systems , 2:336–349, 2020.
[59]Frank McSherry, Michael Isard, and Derek G Murray. Scalability! but at what fCOSTg? In15th
Workshop on Hot Topics in Operating Systems (HotOS XV) , 2015.
[60]Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint
arXiv:1805.02867 , 2018.
[61] NVIDIA. Nvidia Tesla V100 GPU architecture, 2017.
[62] NVIDIA. Nvidia A100 tensor core GPU architecture, 2020.
[63] NVIDIA. Nvidia H100 tensor core GPU architecture, 2022.
[64]D Stott Parker. Random butterﬂy transformations with applications in computational linear algebra.
1995.
[65]Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. Advances in neural information processing systems , 32, 2019.
[66]Markus N Rabe and Charles Staats. Self-attention does not need 𝑂¹𝑛2ºmemory. arXiv preprint
arXiv:2112.05682 , 2021.
[67]Alec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
[68]Jack Rae and Ali Razavi. Do transformers need deep long-range memory? In Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics , Online, July 2020. Association for
Computational Linguistics. URL https://www.aclweb.org/anthology/2020.acl-main.672 .
[69]Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive trans-
formers for long-range sequence modelling. In The International Conference on Learning Representations
(ICLR), 2020.
14[70]Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo Durand, and Saman
Amarasinghe. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in
image processing pipelines. Acm Sigplan Notices , 48(6):519–530, 2013.
[71]Raghu Ramakrishnan, Johannes Gehrke, and Johannes Gehrke. Database management systems , volume 3.
McGraw-Hill New York, 2003.
[72]Benjamin Recht and Christopher Ré. Parallel stochastic gradient algorithms for large-scale matrix
completion. Mathematical Programming Computation , 5(2):201–226, 2013.
[73]Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai.
Combiner: Full attention transformer with sparse computation cost. Advances in Neural Information
Processing Systems , 34, 2021.
[74]Aurko Roy, Mohammad Saﬀar, Ashish Vaswani, and David Grangier. Eﬃcient content-based sparse
attention with routing transformers. Transactions of the Association for Computational Linguistics , 9:
53–68, 2021.
[75] Amit Sabne. XLA: Compiling machine learning for peak performance. 2020.
[76]Victor Sanh, Thomas Wolf, and Alexander M Rush. Movement pruning: Adaptive sparsity by ﬁne-tuning.
arXiv preprint arXiv:2005.07683 , 2020.
[77]Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.
Megatron-LM: Training multi-billion parameter language models using model parallelism. arXiv preprint
arXiv:1909.08053 , 2019.
[78]Vikas Sindhwani, Tara Sainath, and Sanjiv Kumar. Structured transforms for small-footprint deep
learning. In Advances in Neural Information Processing Systems , pages 3088–3096, 2015.
[79]Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span
in transformers. In Proceedings of the Annual Meeting of the Association for Computational Linguistics ,
2019.
[80]Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu
Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for eﬃcient transformers.
InInternational Conference on Learning Representations , 2020.
[81]Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Eﬃcient transformers: A survey. arXiv
preprint arXiv:2009.06732 , 2020.
[82]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing
systems, 30, 2017.
[83]Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet:
Scaling transformers to 1,000 layers. arXiv preprint arXiv:2203.00555 , 2022.
[84]Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768 , 2020.
[85]Samuel Williams, Andrew Waterman, and David Patterson. Rooﬂine: an insightful visual performance
model for multicore architectures. Communications of the ACM , 52(4):65–76, 2009.
[86]Michael E Wolf and Monica S Lam. A data locality optimizing algorithm. In Proceedings of the ACM
SIGPLAN 1991 conference on Programming language design and implementation , pages 30–44, 1991.
15[87]Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric
Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing.
InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System
Demonstrations , pages 38–45, Online, October 2020. Association for Computational Linguistics. URL
https://www.aclweb.org/anthology/2020.emnlp-demos.6 .
[88]David P Woodruﬀ. Optimal space lower bounds for all frequency moments. In SODA, volume 4, pages
167–175. Citeseer, 2004.
[89]Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention with
lightweight and dynamic convolutions. In The International Conference on Learning Representations
(ICLR), 2019.
[90]Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas
Singh. Nyströmformer: A nystöm-based algorithm for approximating self-attention. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence. AAAI Conference on Artiﬁcial Intelligence , volume 35,
page 14138, 2021.
[91]Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng,
and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In
Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 558–567, 2021.
[92]Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago
Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer
sequences. Advances in Neural Information Processing Systems , 33, 2020.
[93]Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh
Susskind. An attention free transformer. arXiv preprint arXiv:2105.14103 , 2021.
[94]Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and
Bryan Catanzaro. Long-short transformer: Eﬃcient transformers for language and vision. Advances in
Neural Information Processing Systems , 34, 2021.
16A Related Work
IO-Aware Runtime Optimization. The broad concept of optimizing for reading and writing to fast/slow
memory has a long history in computer science and has been known by many names. We draw the most
direct connection to the literature of analyzing I/O complexity in this work [ 1], but concepts of memory
hierarchies are fundamental and has appeared in many forms, from the working set model [ 21], to data
locality [ 86], to the Rooﬂine model of arithmetic intensity [ 85], to analyses of scalability [ 59], to standard
textbook treatments of computer architecture [ 40]. We hope that this work encourages the community to
adopt these ideas in more parts of the deep learning stack.
Eﬃcient ML Models with Structured Matrices. Matrix multiply is the core computational bottle-
neck of most machine learning models. To reduce the computational complexity, there have been numerous
approaches to learn over a more eﬃcient set of matrices. These matrices are called structured matrices , which
have subquadratic ( 𝑜¹𝑛2ºfor dimension 𝑛𝑛) number of parameters and runtime. Most common examples
of structured matrices are sparse and low-rank matrices, along with fast transforms commonly encountered
in signal processing (Fourier, Chebyshev, sine/cosine, orthogonal polynomials). There have been several
more general classes of structured matrices proposed in machine learning: Toeplitz-like [ 78], low-displacement
rank [49], quasi-separable [ 25]). The butterﬂy pattern we use for our block-sparse attention is motivated
by the fact that butterﬂy matrices [ 15,64] and their products have been shown to be able to express any
structured matrices with almost optimal runtime and number of parameters [ 16,20]. However, even though
structured matrices are eﬃcient in theory, they have not seen wide adoption since it is hard to translate their
eﬃciency to wall-clock speedup since dense unconstrained matrix multiply has very optimize implementation,
a phenomenon known as the hardware lottery [ 41]. Extensions of butterﬂy matrices [ 17,18] aimed to make
butterﬂy matrices more hardware-friendly.
Sparse Training. Our block-sparse FlashAttention can be seen as a step towards making sparse model
training more eﬃcient. Sparse models have seen success in compressing models for inference (pruning) by
sparsifyingtheweightmatrices[ 23,38,39,55,76]. Formodeltraining, thelotteryticketshypothesis[ 28,29,30]
suggests that there are a set of small sub-networks derived from a larger dense network that performs as
well as the original dense network. Out block-sparse FlashAttention can also be seen as a ﬁxed lottery
ticket in the context of attention: we ﬁx the sparsity pattern to be the butterﬂy pattern through training,
and observe that it performs almost as well as the (dense) FlashAttention on the Long-range Arena tasks.
Eﬃcient Transformer. Transformer-based models have become the most widely-used architecture in
natural language processing [ 22] and computer vision [ 24,91]. However, one of their computational bottlenecks
is that their time and memory scales quadratic in the sequence length. There are numerous approaches to
overcome this bottleneck, including approximation with hashing (i.e., sparse) such as Reformer [ 51] and
Smyrf [19] and with low-rank approximation such as Performer [ 12,54]. One can even combine sparse and
low-rank approximation for better accuracy (e.g., Longformer [ 3], BigBird [ 92], Scatterbrain [ 9], Long-short
transformer [ 94], Combiner [ 73]). Other approaches include compressing along the sequence dimension to
attend to multiple tokens at once [ 52,57,79,89]. One can also attend over the states from previous sequences
to help lengthen the context (e.g., Transformer-XL [ 14] and Compressive Transformer [ 69]). We recommend
the survey [81] for more details.
There are several lines of work on developing other modules instead of attention to model longer context.
HiPPO [ 35] and its extensions, most notably S4 [ 31,36,37] projects the history on a polynomial basis,
allowing accurate reconstruction of the history through state-space models. They combine the strengths of
CNNs (eﬃcient training), RNNs (eﬃcient inference), and continuous models (robust to change in sampling
rates). LambdaNetworks [ 2], AFT [ 93] and FLASH [ 42] are other attempts at replacing attention in the
context of image classiﬁcation and language modeling.
B Algorithm Details
We ﬁrst derive the forward and backward passes of attention and show that they can be computed in a
memory-eﬃcient manner (requiring extra memory linear instead of quadratic in the sequence length). Though
they reduce the amount of extra memory required, naively they still incur quadratic HBM accesses, resulting
in slower execution speed. We describe the FlashAttention algorithm to implement both the forward
17and the backward passes on GPUs that reduces HBM accesses, leading to both faster runtime and smaller
memory footprint.
B.1 Memory-eﬃcient forward pass
The main challenge in making attention memory-eﬃcient is the softmax that couples the columns of K(and
columns of V). Our approach is to compute the softmax normalization constant separately to decouple the
columns. This technique [ 60] has been used in the literature [ 51,66] to show that attention computation
does not need quadratic extramemory (though the number of HBM accesses is still quadratic, resulting in
slow run-time).
For simplicity, we omit here the max-shifting step during softmax. The full algorithm in Appendix B.3
contains all the steps.
Recall that given input sequences QKV2R𝑁𝑑, we want to compute the attention output O2R𝑁𝑑:
S=QK>2R𝑁𝑁P=softmax¹Sº2R𝑁𝑁O=PV2R𝑁𝑑
We have that 𝑆𝑖𝑗=𝑞𝑇
𝑖𝑘𝑗where𝑞𝑖and𝑘𝑗are the𝑖-th and𝑗-th columns of QandKrespectively. Deﬁne
the normalization constants of softmax:
𝐿𝑖=∑︁
𝑗𝑒𝑞𝑇
𝑖𝑘𝑗 (1)
Let𝑣𝑗be the𝑗-th column of V, then the𝑖-th columns of the output is
𝑜𝑖=𝑃𝑖:V=∑︁
𝑗𝑃𝑖𝑗𝑣𝑗=∑︁
𝑗𝑒𝑞𝑇
𝑖𝑘𝑗
𝐿𝑖𝑣𝑗 (2)
We see that once 𝐿𝑖is computed, we can compute 𝑜𝑖without extra memory by repeatedly summing
𝑒𝑞𝑇
𝑖𝑘𝑗
𝐿𝑖𝑣𝑗. Therefore the forward pass can be computed with 𝑂¹𝑛ºextra memory:
1. Compute 𝐿𝑖for all𝑖according to Eq. (1), which takes 𝑂¹𝑛ºextra memory.
2. Compute 𝑜𝑖for all𝑖according to Eq. (2), which takes 𝑂¹𝑑ºextra memory.
B.2 Memory-eﬃcient backward pass
We derive the backward pass of attention and show that it can also be computed with linear memory. Rabe
and Staats [66]suggests that the backward pass can be done without quadratic extra memory by applying
gradient checkpointing to the memory-eﬃcient forward pass. We instead derive the backward pass explicitly
and show how it can be computed in a memory-eﬃcient manner.
Suppose that there is a scalar loss function 𝜙, and let the output gradient be dO2R𝑛𝑑(where dOdenotes
𝜕𝜙
𝜕O). We want to compute the input gradients dQdKdV2R𝑛𝑑(where dQdKdVdenote𝜕𝜙
𝜕Q𝜕𝜙
𝜕K𝜕𝜙
𝜕V
respectively).
The gradient dVis easy to see. Applying reverse-mode autodiﬀ by hand (aka the chain rule), we obtain
(in matrix notation) dV=P𝑇dO. Thus:
𝑑𝑣𝑗=∑︁
𝑖𝑃𝑖𝑗𝑑𝑜𝑖=∑︁
𝑖𝑒𝑞𝑇
𝑖𝑘𝑗
𝐿𝑖𝑑𝑜𝑖 (3)
Since we already computed 𝐿𝑖,𝑑𝑣𝑗can be computed without extra memory by repeated summing.
The gradients dQanddKare a little more complicated. We go through the gradients dPanddSﬁrst.
From Eq. (2), we have that dP=dOV𝑇, and so:
𝑑𝑃𝑖𝑗=𝑑𝑜𝑇
𝑖𝑣𝑗
Recall that 𝑃𝑖:=softmax¹𝑆𝑖:º. Using the fact that the Jacobian of 𝑦=softmax¹𝑥ºisdiag¹𝑦º 𝑦𝑦𝑇, we
have that
𝑑𝑆𝑖:=¹diag¹𝑃𝑖:º 𝑃𝑖:𝑃𝑇
𝑖:º𝑑𝑃𝑖:=𝑃𝑖:𝑑𝑃𝑖: ¹𝑃𝑇
𝑖:𝑑𝑃𝑖:º𝑃𝑖:
18wheredenotes pointwise multiplication.
Deﬁne
𝐷𝑖=𝑃𝑇
𝑖:𝑑𝑃𝑖:=∑︁
𝑗𝑒𝑞𝑇
𝑖𝑘𝑗
𝐿𝑖𝑑𝑜𝑇
𝑖𝑣𝑗=𝑑𝑜𝑇
𝑖∑︁
𝑗𝑒𝑞>
𝑖𝑘𝑗
𝐿𝑖𝑣𝑗=𝑑𝑜𝑇
𝑖𝑜𝑖 (4)
then
𝑑𝑆𝑖:=𝑃𝑖:𝑑𝑃𝑖: 𝐷𝑖𝑃𝑖:
Hence
𝑑𝑆𝑖𝑗=𝑃𝑖𝑗𝑑𝑃𝑖𝑗 𝐷𝑖𝑃𝑖𝑗=𝑃𝑖𝑗¹𝑑𝑃𝑖𝑗 𝐷𝑖º
Now we can get the gradients dQanddK. Recall that 𝑆𝑖𝑗=𝑞𝑇
𝑖𝑘𝑗, so
𝑑𝑞𝑖=∑︁
𝑗𝑑𝑆𝑖𝑗𝑘𝑗=∑︁
𝑗𝑃𝑖𝑗¹𝑑𝑃𝑖𝑗 𝐷𝑖º𝑘𝑗=∑︁
𝑗𝑒𝑞𝑇
𝑖𝑘𝑗
𝐿𝑖¹𝑑𝑜𝑇
𝑖𝑣𝑗 𝐷𝑖º𝑘𝑗 (5)
Similarly,
𝑑𝑘𝑗=∑︁
𝑖𝑑𝑆𝑖𝑗𝑞𝑖=∑︁
𝑖𝑃𝑖𝑗¹𝑑𝑃𝑖𝑗 𝐷𝑖º𝑞𝑖=∑︁
𝑖𝑒𝑞𝑇
𝑖𝑘𝑗
𝐿𝑖¹𝑑𝑜𝑇
𝑖𝑣𝑗 𝐷𝑖º𝑞𝑖 (6)
Therefore the backward pass can also be computed with 𝑂¹𝑛ºextra memory:
1. Compute 𝑑𝑣𝑗for all𝑗according to Eq. (3), which takes 𝑂¹𝑑ºextra memory.
2. Compute 𝐷𝑖for all𝑖according to Eq. (4), which takes 𝑂¹𝑛ºextra memory.
3. Compute 𝑑𝑞𝑖for all𝑖according to Eq. (5), which takes 𝑂¹𝑑ºextra memory.
4. Compute 𝑑𝑘𝑗for all𝑗according to Eq. (6), which takes 𝑂¹𝑑ºextra memory.
B.3 FlashAttention : Forward Pass
We describe the full details of FlashAttention forward pass. Given input sequences QKV2R𝑁𝑑, we
want to compute the attention output O2R𝑁𝑑:
S=𝜏QK>2R𝑁𝑁Smasked=mask¹𝑆º2R𝑁𝑁P=softmax¹Smaskedº2R𝑁𝑁
Pdropped=dropout¹P𝑝dropºO=PdroppedV2R𝑁𝑑
where𝜏2Ris some softmax scaling (typically1p
𝑑),maskis some masking function that sets some entries of
the input to 1and keep other entries the same (e.g., key padding mask when sequences in the batch don’t
have the same lengths and are padded), and dropout¹𝑥𝑝ºapplies dropout to 𝑥elementwise (i.e., output𝑥
1 𝑝
with probability 1 𝑝and output 0 with probability 𝑝for each element 𝑥).
The full algorithm is in Algorithm 2. We save the output O, the softmax statistics ℓand𝑚, and the
pseudo-random number generator state Rfor the backward pass.
19Algorithm 2 FlashAttention Forward Pass
Require: Matrices QKV2R𝑁𝑑in HBM, on-chip SRAM of size 𝑀, softmax scaling constant 𝜏2R,
masking function mask, dropout probability 𝑝drop.
1:Initialize the pseudo-random number generator state Rand save to HBM.
2:Set block sizes 𝐵𝑐=𝑀
4𝑑
𝐵𝑟=min 𝑀
4𝑑
𝑑.
3:Initialize O=¹0º𝑁𝑑2R𝑁𝑑ℓ=¹0º𝑁2R𝑁𝑚=¹ 1º𝑁2R𝑁in HBM.
4:Divide Qinto𝑇𝑟=l
𝑁
𝐵𝑟m
blocks Q1Q𝑇𝑟of size𝐵𝑟𝑑each, and divide KVin to𝑇𝑐=l
𝑁
𝐵𝑐m
blocks
K1K𝑇𝑐andV1V𝑇𝑐, of size𝐵𝑐𝑑each.
5:Divide Ointo𝑇𝑟blocks O𝑖O𝑇𝑟of size𝐵𝑟𝑑each, divide ℓinto𝑇𝑟blocksℓ𝑖ℓ𝑇𝑟of size𝐵𝑟each,
divide𝑚into𝑇𝑟blocks𝑚1𝑚𝑇𝑟of size𝐵𝑟each.
6:for1𝑗𝑇𝑐do
7:Load K𝑗V𝑗from HBM to on-chip SRAM.
8:for1𝑖𝑇𝑟do
9:Load Q𝑖O𝑖ℓ𝑖𝑚𝑖from HBM to on-chip SRAM.
10:On chip, compute S𝑖𝑗=𝜏Q𝑖K𝑇
𝑗2R𝐵𝑟𝐵𝑐.
11:On chip, compute Smasked
𝑖𝑗=mask¹S𝑖𝑗º.
12:On chip, compute ~𝑚𝑖𝑗=rowmax¹Smasked
𝑖𝑗º2R𝐵𝑟,~P𝑖𝑗=exp¹Smasked
𝑖𝑗 ~𝑚𝑖𝑗º2R𝐵𝑟𝐵𝑐(pointwise),
~ℓ𝑖𝑗=rowsum¹~P𝑖𝑗º2R𝐵𝑟.
13:On chip, compute 𝑚new
𝑖=max¹𝑚𝑖~𝑚𝑖𝑗º2R𝐵𝑟,ℓnew
𝑖=𝑒𝑚𝑖 𝑚new
𝑖ℓ𝑖¸𝑒~𝑚𝑖𝑗 𝑚new
𝑖~ℓ𝑖𝑗2R𝐵𝑟.
14:On chip, compute ~Pdropped
𝑖𝑗=dropout¹~P𝑖𝑗𝑝dropº.
15:Write O𝑖 diag¹ℓnew
𝑖º 1¹diag¹ℓ𝑖º𝑒𝑚𝑖 𝑚new
𝑖O𝑖¸𝑒~𝑚𝑖𝑗 𝑚new
𝑖~Pdropped
𝑖𝑗V𝑗ºto HBM.
16:Writeℓ𝑖 ℓnew
𝑖,𝑚𝑖 𝑚new
𝑖to HBM.
17:end for
18:end for
19:Return Oℓ𝑚R.
B.4 FlashAttention : Backward Pass
We describe the full details of FlashAttention backward pass. Given input sequences QKV2R𝑁𝑑, the
output O2R𝑁𝑑, and the output gradient dO, we want to compute the input gradients dQdKdV2R𝑁𝑑.
We ﬁrst describe the standard attention backward pass in Algorithm 3 for completeness.
Algorithm 3 Standard Attention Backward Pass
Require: Matrices QKVdO2R𝑁𝑑,P2R𝑁𝑁in HBM.
1:Load PdOby blocks from HBM, compute dV=P>dO2R𝑁𝑑, write dVto HBM.
2:Load dOVby blocks from HBM, compute dP=dOV>2R𝑁𝑁, write dPto HBM.
3:Read PdPfrom HBM, compute dS2R𝑁𝑁where𝑑𝑆𝑖𝑗=𝑃𝑖𝑗¹𝑑𝑃𝑖𝑗 Í
𝑙𝑃𝑖𝑙𝑑𝑃𝑖𝑙º, write dSto HBM.
4:Load dSandKby blocks from HBM, compute dQ=dSK, write dQto HBM.
5:Load dSandQby blocks from HBM, compute dK=dS>Q, write dKto HBM.
6:Return dQdKdV.
We now make two observations about FlashAttention backward pass:
1.We do not need to store the dropout mask of size 𝑂¹𝑁2ºfrom the forward pass. Instead, we can save
the pseudo-random number generator states from the forward pass and re-generate the dropout mask
in the backward pass. This allows us to only use 𝑂¹𝑁ºextra memory.
2.When computing the softmax gradient, we use Eq. (4) to compute 𝐷𝑖=𝑃>
𝑖:𝑑𝑃𝑖:without reducing over
𝑃𝑖:and𝑑𝑃𝑖:of size𝑁(they might not ﬁt into SRAM). Instead we can rewrite 𝐷𝑖=𝑑𝑜>
𝑖𝑜𝑖and compute
the dot product between vectors of size 𝑑.
20The full FlashAttention backward pass algorithm is in Algorithm 4. Conceptually it is just a block
version of the derivation in Appendix B.2.
Algorithm 4 FlashAttention Backward Pass
Require: Matrices QKVOdO2R𝑁𝑑in HBM, vectors ℓ𝑚2R𝑁in HBM, on-chip SRAM of size 𝑀,
softmax scaling constant 𝜏2R, masking function mask, dropout probability 𝑝drop, pseudo-random
number generator state Rfrom the forward pass.
1:Set the pseudo-random number generator state to R.
2:Set block sizes 𝐵𝑐=𝑀
4𝑑
𝐵𝑟=min 𝑀
4𝑑
𝑑.
3:Divide Qinto𝑇𝑟=l
𝑁
𝐵𝑟m
blocks Q1Q𝑇𝑟of size𝐵𝑟𝑑each, and divide KVin to𝑇𝑐=l
𝑁
𝐵𝑐m
blocks
K1K𝑇𝑐andV1V𝑇𝑐, of size𝐵𝑐𝑑each.
4:Divide Ointo𝑇𝑟blocks O𝑖O𝑇𝑟of size𝐵𝑟𝑑each, divide dOinto𝑇𝑟blocks dO𝑖dO𝑇𝑟of size
𝐵𝑟𝑑each, divide ℓinto𝑇𝑟blocksℓ𝑖ℓ𝑇𝑟of size𝐵𝑟each, divide 𝑚into𝑇𝑟blocks𝑚1𝑚𝑇𝑟of size
𝐵𝑟each.
5:Initialize dQ=¹0º𝑁𝑑in HBM and divide it into 𝑇𝑟blocks dQ1dQ𝑇𝑟of size𝐵𝑟𝑑each. Initialize
dK=¹0º𝑁𝑑dV=¹0º𝑁𝑑in HBM and divide dKdVin to𝑇𝑐blocks dK1dK𝑇𝑐anddV1dV𝑇𝑐,
of size𝐵𝑐𝑑each.
6:for1𝑗𝑇𝑐do
7:Load K𝑗V𝑗from HBM to on-chip SRAM.
8:Initialize ~dK𝑗=¹0º𝐵𝑐𝑑~dV𝑗=¹0º𝐵𝑐𝑑on SRAM.
9:for1𝑖𝑇𝑟do
10:Load Q𝑖O𝑖dO𝑖dQ𝑖ℓ𝑖𝑚𝑖from HBM to on-chip SRAM.
11:On chip, compute S𝑖𝑗=𝜏Q𝑖K𝑇
𝑗2R𝐵𝑟𝐵𝑐.
12:On chip, compute Smasked
𝑖𝑗=mask¹S𝑖𝑗º.
13:On chip, compute P𝑖𝑗=diag¹𝑙𝑖º 1exp¹Smasked
𝑖𝑗 𝑚𝑖º2R𝐵𝑟𝐵𝑐.
14:On chip, compute dropout mask Z𝑖𝑗2R𝐵𝑟𝐵𝑐where each entry has value1
1 𝑝dropwith probability
1 𝑝dropand value 0 with probability 𝑝drop.
15:On chip, compute Pdropped
𝑖𝑗=P𝑖𝑗Z𝑖𝑗(pointwise multiply).
16:On chip, compute ~dV𝑗 ~dV𝑗¸¹Pdropped
𝑖𝑗º>dO𝑖2R𝐵𝑐𝑑.
17:On chip, compute dPdropped
𝑖𝑗=dO𝑖V>
𝑗2R𝐵𝑟𝐵𝑐.
18:On chip, compute dP𝑖𝑗=dPdropped
𝑖𝑗Z𝑖𝑗(pointwise multiply).
19:On chip, compute 𝐷𝑖=rowsum¹dO𝑖O𝑖º2R𝐵𝑟.
20:On chip, compute dS𝑖𝑗=P𝑖𝑗¹dP𝑖𝑗 𝐷𝑖º2R𝐵𝑟𝐵𝑐.
21:Write dQ𝑖 dQ𝑖¸𝜏dS𝑖𝑗K𝑗2R𝐵𝑟𝑑to HBM.
22:On chip, compute ~dK𝑗 ~dK𝑗¸𝜏dS>
𝑖𝑗Q𝑖2R𝐵𝑐𝑑.
23:end for
24:Write dK𝑗 ~dK𝑗dV𝑗 ~dV𝑗to HBM.
25:end for
26:Return dQdKdV.
We see that similar to the forward pass, the backward pass performs 𝑂¹𝑁2ºFLOPs and only requires
𝑂¹𝑁ºextra memory beyond inputs, output, output gradient, and input gradients.
We analyze the IO-complexity of the backward pass, similar to the forward pass (Theorem 2).
Theorem5. Let𝑁be the sequence length, 𝑑be the head dimension, and 𝑀be size of SRAM with 𝑑𝑀𝑁𝑑.
Standard attention (Algorithm 0) backward pass requires Θ¹𝑁𝑑¸𝑁2ºHBM accesses, while FlashAttention
backward pass (Algorithm 4) requires Θ¹𝑁2𝑑2𝑀 1ºHBM accesses.
The proof is in Appendix C.
21B.5 Comparison with Rabe and Staats [66]
We describe here some similarities and diﬀerences between our FlashAttention algorithm and the algorithm
of Rabe and Staats [66].
Conceptually, both FlashAttention and Rabe and Staats [66]operate on blocks of the attention matrix
using the well-established technique of tiling (or softmax scaling) [ 51,60]. To reduce the memory footprint,
both methods avoid storing the large attention matrix in the forward pass and recompute it in the backward
pass.
The ﬁrst major diﬀerence is that Rabe and Staats [66]focuses on the reducing the total memory footprint
(maximum amount of GPU memory required) while FlashAttention focuses on reducing memory accesses
(the number of memory reads/writes). As mentioned in Section 2, the amount of memory access is the
primary determining factor of runtime. Reducing memory accesses also necessarily reduces the total amount
of memory required (e.g., if an operation incurs 𝐴memory accesses, then its total memory requirement is at
most𝐴). As a result, FlashAttention is faster than standard attention (2-4 ) while Rabe and Staats [66]
is around the same speed or slightly slower than standard attention. In terms of total memory required, both
methods oﬀer substantial memory saving.
The second diﬀerence between the two methods is the way information is summarized from each block
to pass to the next block. Rabe and Staats [66]summarizes each block with its temporary output along
with the softmax normalization statistics. At the end of the forward pass, the temporary outputs of all the
blocks are combined using the statistics to produce the ﬁnal output. FlashAttention instead incrementally
updates the output (Algorithm 1 line 12) after processing each block, so only one copy of the output is needed
(instead of𝐾copies for𝐾blocks). This means that FlashAttention has smaller total memory requirement
compared to Rabe and Staats [66].
The ﬁnal major diﬀerence is the way the backward pass is computed. Rabe and Staats [66]uses gradient
checkpointing to recompute the attention matrix and the temporary output of each block. FlashAttention
instead simpliﬁes the backward pass analytically (Appendices B.2 and B.4). It only recomputes the attention
matrix and does not recompute the temporary output of each block. This reduces the memory requirement
for the backward pass and yields speedup.
C Proofs
Proof of Theorem 1. We ﬁrst count the number of FLOPs and extra memory required.
The dominating FLOPs are from matrix multiplication. In the inner loop, (Algorithm 1 line 9), we
compute Q𝑖K>
𝑗2R𝐵𝑟𝐵𝑐forQ𝑖2R𝐵𝑟𝑑andK𝑗2R𝐵𝑐𝑑, which takes 𝑂¹𝐵𝑟𝐵𝑐𝑑ºFLOPs. We also compute
(Algorithm 1 line 12) ~P𝑖𝑗V𝑗2R𝐵𝑟𝑑for~P𝑖𝑗2R𝐵𝑟𝐵𝑐andV𝑗2R𝐵𝑐𝑑, which takes 𝑂¹𝐵𝑟𝐵𝑐𝑑ºFLOPs. We
execute the inner loops 𝑇𝑐𝑇𝑟=l
𝑁
𝐵𝑐ml
𝑁
𝐵𝑟m
times. Therefore the total number of FLOPs is
𝑂𝑁2
𝐵𝑐𝐵𝑟𝐵𝑟𝐵𝑐𝑑
=𝑂¹𝑁2𝑑º
In terms of extra memory required, we see that we need 𝑂¹𝑁ºmemory to store the statistics ¹ℓ𝑚º.
We now prove the algorithm’s correctness by induction on 𝑗for0𝑗𝑇𝑐. Let K:𝑗2R𝑗𝐵𝑐𝑑be the
ﬁrst𝑗𝐵𝑐rows of K, and similarly V:𝑗2R𝑗𝐵𝑐𝑑the the ﬁrst 𝑗𝐵𝑐rows of V. Let S::𝑗=QK>
:𝑗2R𝑁𝑗𝐵𝑐, and
P::𝑗=softmax¹S::𝑗º2R𝑁𝑗𝐵𝑐(softmax applied row-wise). Let 𝑚𝑗ℓ¹𝑗ºO¹𝑗ºbe the values of 𝑚ℓOin HBM
after the𝑗-th iteration of the outer loop (Algorithm 1 line 5). (Note that these values of 𝑚ℓOare updated
after each iteration of the outer loop.) We want to show that after the 𝑗-th iteration of the outer loop, we
have computed in HBM:
𝑚¹𝑗º=rowmax¹S::𝑗º2R𝑁 ℓ¹𝑗º=rowsum¹exp¹S::𝑗 𝑚¹𝑗ººº2R𝑁O¹𝑗º=P::𝑗V:𝑗2R𝑁𝑑
Based on our initialization (Algorithm 1 line 2), this claim is true for 𝑗=0(i.e., before the any iteration
of the outer loop is executed). Suppose that the claim holds for some 𝑗=0𝑇𝑐 1. We want to show that
the claim also holds for 𝑗¸1. Indeed, when we update the statistics in the inner loop (Algorithm 1 line 10)
22on the¹𝑗¸1º-th iteration of the outer loop, we update 𝑚¹𝑗¸1º=max¹𝑚¹𝑗º~𝑚ºwhere ~𝑚2R𝑁is the row-max
ofS:𝑗:𝑗¸1, the slice of Sfrom column 𝑗𝐵𝑐to column¹𝑗¸1º𝐵𝑐 1. This implies that
𝑚¹𝑗¸1º=rowmax¹S::𝑗¸1º2R𝑁
Similarly, we update
ℓ¹𝑗¸1º=𝑒𝑚¹𝑗º 𝑚¹𝑗¸1ºℓ¹𝑗º¸𝑒~𝑚 𝑚¹𝑗¸1º~ℓ
where ~ℓ=rowsum¹exp¹S:𝑗:𝑗¸1 ~𝑚ºº2R𝑁. By the same algebraic manipulation in Section 3.1, we obtain:
ℓ¹𝑗¸1º=rowsum¹exp¹S::𝑗¸1 𝑚¹𝑗¸1ººº2R𝑁
LetV𝑗:𝑗¸1be the slice of Vfrom column 𝑗𝐵𝑐to column¹𝑗¸1º𝐵𝑐 1, we also update:
O¹𝑗¸1º=diag¹ℓ¹𝑗¸1ºº 1¹diag¹ℓ¹𝑗ºº𝑒𝑚¹𝑗º 𝑚¹𝑗¸1ºO¹𝑗º¸𝑒~𝑚 𝑚¹𝑗¸1ºexp¹S𝑗:𝑗¸1 ~𝑚ºV𝑗:𝑗¸1º
=diag¹ℓ¹𝑗¸1ºº 1¹diag¹ℓ¹𝑗ºº𝑒𝑚¹𝑗º 𝑚¹𝑗¸1ºP::𝑗V:𝑗¸𝑒 𝑚¹𝑗¸1ºexp¹S𝑗:𝑗¸1ºV𝑗:𝑗¸1º
=diag¹ℓ¹𝑗¸1ºº 1¹diag¹ℓ¹𝑗ºº𝑒𝑚¹𝑗º 𝑚¹𝑗¸1ºdiag¹ℓ¹𝑗ººexp¹S::𝑗 𝑚¹𝑗ººV:𝑗¸𝑒 𝑚¹𝑗¸1ºexp¹S𝑗:𝑗¸1ºV𝑗:𝑗¸1º
=diag¹ℓ¹𝑗¸1ºº 1¹𝑒 𝑚¹𝑗¸1ºexp¹S::𝑗ºV:𝑗¸𝑒 𝑚¹𝑗¸1ºexp¹S𝑗:𝑗¸1ºV𝑗:𝑗¸1º
=diag¹ℓ¹𝑗¸1ºº 1¹exp¹S::𝑗 𝑚¹𝑗¸1ººV:𝑗¸exp¹S𝑗:𝑗¸1 𝑚¹𝑗¸1ººV𝑗:𝑗¸1º
=diag¹ℓ¹𝑗¸1ºº 1
expS::𝑗S𝑗:𝑗¸1
 𝑚¹𝑗¸1ºV:𝑗
V𝑗:𝑗¸1
=softmax¹S:𝑗¸1ºV:𝑗¸1
We then see that the claim is also true for 𝑗¸1. By induction, the claim is true for all 𝑗=0𝑇𝑐.
When𝑗=𝑇𝑐, we conclude that the ﬁnal value of Oin HBM is softmax¹SºV=softmax¹QK>ºV.

Proof of Theorem 2. We ﬁrst analyze the IO complexity of standard attention implementation. The inputs
QKV2R𝑁𝑑reside in HBM, and the at the end of the algorithm the output O2R𝑁𝑑is written to HBM.
In the ﬁrst step of computing the matrix multiply S=QK>, the inputs QKare read from HBM and the
output S2R𝑁𝑁is written to HBM (Algorithm 0 line 1). This incurs Θ¹𝑁𝑑¸𝑁2ºHBM accesses.
In the second step of computing P=softmax¹Sº, the input Sis read from HBM and the output Pis
written to HBM (Algorithm 0 line 2). This incurs Θ¹𝑁2ºHBM accesses.
In the last step of computing O=PV, the inputs PVare read from global memory and the output Ois
written to HBM (Algorithm 0 line 3). This incurs Θ¹𝑁𝑑¸𝑁2ºHBM accesses.
Overall, standard attention implementation requires Θ¹𝑁𝑑¸𝑁2ºglobal memory accesses.
We now analyze the IO complexity of streaming attention.
Following Algorithm 1, we see that each element of KandVis loaded from HBM once (Algorithm 1
line 6). We make 𝑇𝑐passes over QandO, each pass loading all of Qand all of Oto HBM (Algorithm 1
line 8). Therefore the number of HBM accesses is Θ¹𝑁𝑑¸𝑁𝑑𝑇𝑐º=Θ¹𝑁𝑑𝑇𝑐º.
We derive the conditions on the block sizes 𝐵𝑐and𝐵𝑟. We need the blocks K𝑗andV𝑗of size𝐵𝑐𝑑to ﬁt
into on-chip memory, which translates to:
𝐵𝑐𝑑=𝑂¹𝑀º,𝐵𝑐=𝑂𝑀
𝑑

Similarly, we need the blocks Q𝑖O𝑖of size𝐵𝑟𝑑to ﬁt into on-chip memory, which translates to:
𝐵𝑟𝑑=𝑂¹𝑀º,𝐵𝑟=𝑂𝑀
𝑑

Finally, we need the block S𝑖𝑗of size𝐵𝑟𝐵𝑐to ﬁt into on-chip memory, which translates to:
𝐵𝑟𝐵𝑐=𝑂¹𝑀º
23We therefore set:
𝐵𝑐=Θ𝑀
𝑑
 𝐵𝑟=Θ
min𝑀
𝑑𝑀
𝐵𝑐
=Θ
min𝑀
𝑑𝑑

We then have:
𝑇𝑐=𝑁
𝐵𝑐=Θ𝑁𝑑
𝑀

As a result, the number of HBM accesses is:
Θ¹𝑁𝑑𝑇𝑐º=Θ𝑁2𝑑2
𝑀


Proof of Proposition 3. For contradiction, suppose that there exists an algorithm that computes exact
attention where the number for HBM access for all 𝑀2»𝑑𝑁𝑑¼is
𝑜𝑁2𝑑2
𝑀

In the regime of 𝑀=Θ¹𝑁𝑑º, this results in the number of HBM accesses:
𝑜𝑁2𝑑2
𝑁𝑑
=𝑜¹𝑁𝑑º
However, the input to attention (matrices QKV) and the output Ohave size𝑁𝑑and they start out being
in HBM, so if the algorithm computes exact attention it must incur at least Ω¹𝑁𝑑ºHBM accesses. This is a
contradiction. 
Proof of Theorem 5. The IO complexity of the attention backward is very similar to the IO complexity of
the attention forward (Theorem 2). Here we provide a sketch of the proof.
We ﬁrst analyze the IO complexity of standard attention backward pass. The inputs QKVdO2R𝑁𝑑
reside in HBM, and the at the end of the algorithm the outputs dQdKdV2R𝑁𝑑are written to HBM.
At each step of the standard attention backward pass, one needs to load inputs of size 𝑁𝑑or𝑁2from
HBM, and needs to write the outputs of size 𝑁2or𝑁𝑑to HBM. This incurs Θ¹𝑁𝑑¸𝑁2ºHBM accesses.
We now analyze the IO complexity of FlashAttention backward pass.
Similar to Theorem 2, we see that each element of KandVis loaded from HBM once. Each element of
dKanddVis only written to HBM once. We make 𝑇𝑐passes over QOdO, each pass loading all of QOdO
to HBM. We also make 𝑇𝑐passes over dQ, each pass reading/writing all of dQfrom/to HBM. Therefore the
number of HBM accesses is Θ¹𝑁𝑑¸𝑁𝑑𝑇𝑐º=Θ¹𝑁𝑑𝑇𝑐º.
As in the proof of Theorem 2, the constraints on the block sizes are that:
𝐵𝑐=Θ𝑀
𝑑
 𝐵𝑟=Θ
min𝑀
𝑑𝑑

We then have:
𝑇𝑐=𝑁
𝐵𝑐=Θ𝑁𝑑
𝑀

As a result, the number of HBM accesses is:
Θ¹𝑁𝑑𝑇𝑐º=Θ𝑁2𝑑2
𝑀


24Algorithm 5 Block-Sparse FlashAttention Forward Pass
Require: Matrices QKV2R𝑁𝑑in HBM, on-chip SRAM of size 𝑀, softmax scaling constant 𝜏2R,
masking function mask, dropout probability 𝑝drop, block sizes 𝐵𝑐=𝑀
4𝑑
𝐵𝑟=min 𝑀
4𝑑
𝑑, block
sparsity mask 𝑀2f01g𝑁𝐵𝑟𝑁𝐵𝑐..
1:Initialize the pseudo-random number generator state Rand save to HBM.
2:Initialize O=¹0º𝑁𝑑2R𝑁𝑑ℓ=¹0º𝑁2R𝑁𝑚=¹ 1º𝑁2R𝑁in HBM.
3:Divide Qinto𝑇𝑟=l
𝑁
𝐵𝑟m
blocks Q1Q𝑇𝑟of size𝐵𝑟𝑑each, and divide KVin to𝑇𝑐=l
𝑁
𝐵𝑐m
blocks
K1K𝑇𝑐andV1V𝑇𝑐, of size𝐵𝑐𝑑each.
4:Divide Ointo𝑇𝑟blocks O𝑖O𝑇𝑟of size𝐵𝑟𝑑each, divide ℓinto𝑇𝑟blocksℓ𝑖ℓ𝑇𝑟of size𝐵𝑟each,
divide𝑚into𝑇𝑟blocks𝑚1𝑚𝑇𝑟of size𝐵𝑟each.
5:for1𝑗𝑇𝑐do
6:Load K𝑗V𝑗from HBM to on-chip SRAM.
7:for1𝑖𝑇𝑟do
8:if𝑀𝑖𝑗≠0then
9: Load Q𝑖O𝑖ℓ𝑖𝑚𝑖from HBM to on-chip SRAM.
10: On chip, compute S𝑖𝑗=𝜏Q𝑖K𝑇
𝑗2R𝐵𝑟𝐵𝑐.
11: On chip, compute Smasked
𝑖𝑗=mask¹S𝑖𝑗º.
12: On chip, compute ~𝑚𝑖𝑗=rowmax¹Smasked
𝑖𝑗º2R𝐵𝑟,~P𝑖𝑗=exp¹Smasked
𝑖𝑗 ~𝑚𝑖𝑗º2R𝐵𝑟𝐵𝑐(pointwise),
~ℓ𝑖𝑗=rowsum¹~P𝑖𝑗º2R𝐵𝑟.
13: On chip, compute 𝑚new
𝑖=max¹𝑚𝑖~𝑚𝑖𝑗º2R𝐵𝑟,ℓnew
𝑖=𝑒𝑚𝑖 𝑚new
𝑖ℓ𝑖¸𝑒~𝑚𝑖𝑗 𝑚new
𝑖~ℓ𝑖𝑗2R𝐵𝑟.
14: On chip, compute ~Pdropped
𝑖𝑗=dropout¹~P𝑖𝑗𝑝dropº.
15: Write O𝑖 diag¹ℓnew
𝑖º 1¹diag¹ℓ𝑖º𝑒𝑚𝑖 𝑚new
𝑖O𝑖¸𝑒~𝑚𝑖𝑗 𝑚new
𝑖~Pdropped
𝑖𝑗V𝑗ºto HBM.
16: Writeℓ𝑖 ℓnew
𝑖,𝑚𝑖 𝑚new
𝑖to HBM.
17:end if
18:end for
19:end for
20:Return Oℓ𝑚R.
D Extension Details
D.1 Block-sparse FlashAttention
We describe the full block-sparse FlashAttention algorithm in Algorithm 5. The algorithm is identical
to Algorithm 2, except that we skip zero blocks.
We prove the IO-complexity of block-sparse FlashAttention .
Proof of Proposition 4. The proof is very similar to the proof of Theorem 2. For the block-sparse case, notice
that we only need to load blocks corresponding to nonzero blocks. As a result, the number of HBM accesses
are scaled by 𝑠, the fraction of nonzero blocks in the block-sparsity mask. However, for small values of 𝑠, we
would still need to write the result O2R𝑁𝑑. Therefore the number of HBM accesses is
Θ
𝑁𝑑¸𝑁2𝑑2
𝑀𝑠


D.2 Potential Extensions
We discuss here a few potential extensions of the IO-aware approach to speed up deep learning training.
Multi-GPU Attention. Large language models are trained on hundreds or thousands of GPUs, and
one typically splits the attention computation between 4-8 GPUs on the same node [ 77]. This introduces
another level of memory hierarchy: beside GPU SRAM and GPU HBM, we also have the HBM of other
25GPUs. For very long sequences, the diﬀerent GPUs on the same node can cooperate to compute attention by
taking into account the asymmetry of diﬀerent levels of memory hierarchy.
Sparse MLP layers. Typical dense MLP layers are compute-bound and not memory-bound. To improve
their eﬃciency, MLP layers with sparse weight matrices can be used [ 17]. However, many sparse MLP layers
are instead memory-bound, and their speedup is often not proportional to the sparsity. We believe that an
IO-aware implementation can alleviate this issue and realize the beneﬁts of sparsity. We are excited about
future work in this direction, to reduce the computational requirement of large models and improve their
wall-block runtime.
Kernel machine learning. Our approach in FlashAttention relies on the fact that the 𝑁𝑁
attention matrix is a function of a low-rank matrix QK>(of rank𝑑𝑁). As a result, we can repeatedly
load the inputs QKand recompute the block of the attention matrix that we need, signiﬁcantly reducing
HBM access. As similar scenario happens in kernel machine learning: each element 𝐾𝑖𝑗of the𝑁𝑁kernel
matrix Kis a function of two vectors of size 𝑑𝑁, as it measures the similarity between two datapoints 𝑥𝑖
and𝑥𝑗. The KeOps library [ 8,26] is a successful example of how reducing memory reads/writes can speed up
kernel operations. We hope that this will motivate kernel methods that focus more on reducing IOs instead
of just FLOPs.
E Full Experimental Results
E.1 BERT
We train BERT-large following the training procedure and hyperparameters of the reference MLPerf 1.1
implementation. In particular, we use the LAMB optimizer with learning rate 3.75e-3, with batch size 448,
trained for at most 7100 steps. The training is stopped once the validation accuracy (for masked language
modeling) reaches the target 72.0%, and the wall-clock run-time is measured. We train with FP16 precision
using Apex AMP (with O2 optimization level).
We compare our results with the reported training speed from Nvidia that was submitted to MLPerf 1.1
(Table 1).
We use the same train / validation data split provided by MLPerf 1.1 reference implementation. In
particular, we evaluate on the same 10000 validation examples as the baseline from Nvidia.
We train the model on 8 A100-80GB GPUs. Each training run takes between 16 and 19 minutes, and we
average the results of 10 runs.
E.2 GPT-2
We use the standard implementations of GPT-2 [ 67] from Huggingface transformers library and from
Nvidia’s Megatron-LM repo. We follow the training recipe of the Megatron-LM repo.
We use an eﬀective batch size of 512, and use gradient accumulation to ﬁt into available GPU memory.
We use the AdamW optimizer, with learning rate 6e-4 for GPT-2 small and 1.5e-4 for GPT-2 medium, and
weight decay of 0.1. All models are trained with the same hyperparameters for 400K steps. We run all
implementations with mixed-precision training (PyTorch AMP).
We use the Openwebtext dataset, with the GPT-2 BPE tokenizer. We randomly select 0.5% of the dataset
as the validation set, with the rest being used as training set. This random selection of validation set is done
once, and all models are evaluated on the same validation set.
We train the model on 8 A100-40GB GPUs, and we measure the wall-clock training time. Training
GPT-2 small takes between 2.7-9.5 days, and training GPT-2 medium takes between 6.9-21.0 days (Table 2).
In Fig. 4, we plot of the validation perplexity throughout training of GPT-2 small/medium, using either
HuggingFace implementation or our FlashAttention implementation. We see that FlashAttention be-
haves the same as the baseline implementation and the validation perplexity curves of the two implementations
almost lie on top of each other.
Long Document Classiﬁcation. For MIMIC-III and ECtHR, we follow the hyperparameters of Dai et al.
[13].
26100k 200k 300k
Training steps1015202530Val perplexityGPT-2-small HuggingFace
GPT-2-small FlashAttention
GPT-2-medium HuggingFace
GPT-2-medium FlashAttentionFigure 4: Validation perplexity of GPT-2 small/medium using two implementations. We conﬁrm that
FlashAttention yields the same validation curves as the baseline implementation from HuggingFace.
E.3 LRA details
We follow the hyperparameters from the Long-range arena paper [ 80], the Long-range arena repo ( https:
//github.com/google-research/long-range-arena ), and the Nyströmformer reproduction [ 90]. To be
generous to the baseline methods, if we are unable to reproduce the performance of any baseline for any of
the ﬁve tasks, we report the better performance from Tay et al. [80]or Xiong et al. [90]for that baseline on
that task.
After hyperparameter tuning, almost all of the attention methods achieve similar accuracy on all of the
ﬁve LRA tasks.
We run all methods with mixed-precision training, except for Performer (not stable with mixed precision)
and Local Attention (implementation does not support FP16).
To calculate the overall wallclock-time speedup, we take the geometric mean of the wallclock-time speedup
of each of the ﬁve tasks.
Path-X For Path-X and Path-256, we follow the hyperparameters from the PathFinder-32 experiments
from the long-range arena paper[ 80]. For both, we ﬁrst pretrain a model on Path-64. We take the checkpoint
after 200 epochs, upsample its positional embedding (we duplicate the positional embeddings gridwise in
space), and ﬁne-tune it on the downstream task for 200 epochs with one epoch of linear warmup, and cosine
decay of the learning rate. For Path-X, we take the best performing checkpoint (according to val accuracy),
and additionally ﬁne-tune it for 200 epochs with the same warmup and learning rate (this adds roughly 4
points of accuracy to FlashAttention for Path-X, but the model starts overﬁtting afterwards).
E.4 Comparison with Apex FMHA
We compare our method/implementation with Apex FMHA ( https://github.com/NVIDIA/apex/tree/
master/apex/contrib/csrc/fmha ).
When we started this project, Apex FMHA was the fastest implementation of attention (that we knew
of), tailored for short sequences of length at most 512. In fact, almost all MLPerf submissions for BERT
training benchmark running on Nvidia GPUs use FMHA for their model code, as of MLPerf 1.1 [ 58]. Since
27Table 7: Runtime (ms) of FlashAttention compared to FMHA by sequence length, with masking and dropout,
measured on an A100-SXM4-40GB GPU. Batch size 64, 16 heads, head dimension 64 (i.e., BERT-large size).
Attention Method 128 256 512
Apex FMHA forward 0.10 0.29 1.14
FlashAttention forward 0.08 0.22 0.81
Apex FMHA backward 0.17 0.52 1.81
FlashAttention backward 0.20 0.53 2.00
Apex FMHA forward + backward 0.270.81 2.95
FlashAttention forward + backward 0.28 0.75 2.81
FMHA targets BERT models, it only supports head dimension 64, and only runs on A100 GPUs. FMHA
fuses the attention computation dropout¹softmax¹mask¹QK>ºººVinto one CUDA kernel. In the forward
pass, it stores the attention matrix softmax¹mask¹QK𝑇ººto HBM to be used in gradient computation. As a
result, it does not oﬀer substantial memory saving (though for shorter sequences memory footprint is often
not a primary concern).
We use FMHA code as a starting point, and apply two well-established techniques (tiling and recomputa-
tion) to deal with long sequences and to save memory as mentioned in Section 3. As a result, we can support
much longer sequences (e.g., up to length 64K). We also support more head dimensions (16, 32, 64, 128) and
broader GPU types (all Turing and Ampere GPUs at the time of writing).
In Table 7, we compare the performance of FlashAttention and Apex FMHA for short sequences (as
FMHA only supports sequence length at most 512). Generally FlashAttention is slightly faster than
FMHA in the forward pass and slightly slower than FMHA in the backward pass. This is because we do not
store the attention matrix in the forward pass and recompute it in the backward pass. Compared to FMHA,
the overall runtime of FlashAttention is about 4% slower for sequence length 128, 8% faster for sequence
length 256, and 5% faster for sequence length 512.
E.5 Speedup On Diﬀerent Hardware and Conﬁgurations
Speedup varies between diﬀerent types of GPU types and generations depending on HBM bandwidth and
SRAM size. In this section, we proﬁle FlashAttention speedup on diﬀerent GPUs and conﬁgurations.
Figure 5: Speedup over standard PyTorch attention at diﬀerent sequence lengths, on A100.
A100Figure 5 shows speedup on an A100 GPU with batch size 8, head dimension 64, and 12 attention
heads, across diﬀerent sequence lengths. We generally see 2-4 speedup, and we see more speedup when
using dropout and masking due to kernel fusion.
28Figure 6: Speedup over standard PyTorch attention at diﬀerent sequence lengths, on A100, with head
dimension 128.
A100, Head Dimension 128 Speedup also changes when we increase the head dimension. Each block
requires more memory, so we need to use smaller block sizes to ﬁt into SRAM. Figure 6 shows speedup with
head dimension 128 on an A100 (batch size 16, 12 heads). We see less speedup overall—but we can still see
signiﬁcant speedup (up to 3 ) with a causal mask, where half the blocks are masked out.
Figure 7: Speedup over standard PyTorch attention at diﬀerent sequence lengths, on RTX 3090.
RTX 3090 Figure 7 shows speedup on an RTX 3090 GPU. Here, we use batch size 12 with 12 attention
heads. We observe slightly higher speedups on the RTX 3090 (between 2.5-4.5 ), since the memory bandwidth
on an RTX 3090 is lower than on an A100 (roughly 900 GB/s vs. 1.5 TB/s).
T4Figure 8 shows speedup on a T4 GPU. T4 SRAM is smaller than A100, so we need to make the block
sizes smaller in FlashAttention . As a result, we observe less speedup on T4, which matches the IO
complexity analysis in Section 3.2. T4 GPUs are commonly used for inference, so we also report speedup on
the forward pass only.
29Figure 8: Speedup over standard PyTorch attention at diﬀerent sequence lengths, on T4. Top:Combined
forward pass + backward pass. Bottom: Forward pass only.
E.6 Full Benchmarking Results
We report the full benchmarking results and experimental details on A100.
Baselines We compare against reference implementations for exact attention from PyTorch/HuggingFace
and Megatron, approximate attention, and sparse attention. For approximate attention, we compare against
reference implementations of Reformer [ 51], Local Attention [ 68], Linformer Attention [ 84], Smyrf [ 19], and
LongShortFormer (LSFormer) [ 94]. For sparse attention, we compare against reference implementations of
Block-Sparse Attention form OpenAI [ 11], Longformer[ 3], and BigBird Attention [ 92]. For the approximate
and sparse attention, we use a compression ratio of 1/8, or a compressed sequence length of 256, whichever is
smaller.
Setup We measure runtime and memory usage of the attention computation with 8 heads of dimension 64,
and batch size 16 on a machine with one A100 GPU with 40 GB of GPU HBM. We vary sequence length
in our experiments. We compute attention on random vectors for Q,K, and V(we do not measure the
projection from the hidden layer). For dropout, we use dropout 0.1; for masking, we use a padding mask
with uniformly-random mask lengths between the total sequence length and the total sequence length minus
20. To measure runtime, we take the average of 100 measurements of the attention call. We only measure
memory footprint once, since it does not vary between runs.
30Table 8: Pointers to results tables.
Dropout Masking Pass Table
Yes Yes Forward Table 9
Yes Yes Backward Table 10
Yes Yes Combined Table 11
No Yes Forward Table 12
No Yes Backward Table 13
No Yes Combined Table 14
Yes No Forward Table 15
Yes No Backward Table 16
Yes No Combined Table 17
No No Forward Table 18
No No Backward Table 19
No No Combined Table 20
No No Memory Usage (Combined) Table 21
Table 9: Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length,
with dropout and masking . Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.36 0.34 0.78 2.54 9.33 36.33 - - - -
Megatron 0.40 0.40 1.10 3.65 16.19 - - - - -
Reformer 2.03 3.15 5.67 11.02 22.59 46.14 97.38 212.13 - -
Local Attention 0.83 0.86 1.01 2.20 7.13 14.32 28.60 57.79 117.67 -
Linformer 0.67 0.52 0.69 0.71 1.65 3.18 6.15 12.16 24.17 52.39
Smyrf 2.27 2.34 3.91 7.44 14.71 29.22 58.27 116.41 - -
LSformer 1.18 1.27 1.34 3.38 11.40 22.55 44.95 89.76 179.66 -
Block Sparse 1.12 1.11 2.13 2.77 6.95 20.91 - - - -
Longformer 1.22 1.14 1.08 1.95 5.72 12.98 - - - -
BigBird 1.13 1.12 1.12 1.77 6.03 13.68 - - - -
FlashAttention 0.04 0.06 0.21 0.82 2.85 10.41 41.74 167.19 670.76 2682.35
Block-Sparse FlashAttention 0.06 0.06 0.06 0.12 0.44 0.86 1.70 3.29 6.55 13.34
We report timing results on the forward pass, backward pass, and combined forward + backward pass.
We measure each method with and without dropout, masking, or both—except for Block Sparse, Longformer,
and BigBird. These methods did not successfully run the backward pass with masking due to a bug in
external libraries, so we measured them without masking to be generous. We use FP16 for all measurements,
except for Local Attention, whose implementation only supports FP32.
For each baseline, we increase sequence length until it runs out of memory on the GPU, except for the
following exceptions: The Megatron implementation does not support sequence lengths longer than 2048.
Block-Sparse (OpenAI) does not support sequence lengths longer than 4096. Longformer and BigBird do not
support sequence lengths longer than 8092.
We measure memory usage on the combined forward + backward pass, without dropout or masking.
Results Table 8 summarizes all the experimental conﬁgurations and contains pointers to the results tables.
31Table 10: Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length,
with dropout and masking . Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.37 0.49 1.66 5.81 22.32 87.67 - - - -
Megatron 0.35 0.32 0.77 2.42 8.43 - - - - -
Reformer 2.37 4.59 8.91 17.68 35.13 70.05 140.01 - - -
Local Attention 0.55 0.62 1.49 4.03 13.78 27.61 55.20 110.27 221.40 -
Linformer 0.89 0.80 0.81 0.93 2.48 4.75 9.29 18.27 36.53 -
Smyrf 1.41 2.83 5.43 10.72 21.25 42.31 84.48 168.95 - -
LSformer 1.75 1.76 3.01 7.50 20.07 39.08 76.39 150.82 - -
Block Sparse 1.29 1.28 2.18 3.04 7.27 21.16 - - - -
Longformer 1.27 1.31 1.29 2.04 5.24 10.74 25.95 - - -
BigBird 1.33 1.28 1.32 1.81 5.55 11.44 27.45 - - -
FlashAttention 0.30 0.26 0.68 2.02 6.84 26.89 105.70 418.96 1666.89 6660.44
Block-Sparse FlashAttention 0.30 0.27 0.29 0.59 1.50 2.94 5.82 11.85 23.98 47.61
Table 11: Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by
sequence length, with dropout and masking . Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.84 0.86 2.35 8.29 31.75 124.19 - - - -
Megatron 0.87 0.89 1.33 4.21 16.50 - - - - -
Reformer 4.30 7.76 14.60 28.74 57.79 116.34 237.57 - - -
Local Attention 1.40 1.60 2.06 6.06 20.94 42.01 84.08 168.48 339.45 -
Linformer 1.57 1.49 1.55 1.60 4.19 8.04 15.71 30.92 61.47 -
Smyrf 3.41 5.08 9.35 18.18 36.03 71.68 143.04 285.87 - -
LSformer 3.08 3.10 4.26 10.90 31.59 61.72 121.51 241.18 - -
Block Sparse 2.54 2.52 3.71 5.44 13.29 39.19 - - - -
Longformer 2.47 2.49 2.51 3.10 10.39 22.49 60.44 - - -
BigBird 2.51 2.49 2.52 3.40 10.97 23.89 63.28 - - -
FlashAttention 0.43 0.41 0.95 2.55 9.56 37.49 147.75 586.61 2339.11 9341.30
Block-Sparse FlashAttention 0.44 0.44 0.45 0.89 1.95 4.12 7.64 16.60 32.73 64.11
Table 12: Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length,
with masking . Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.30 0.30 0.63 1.93 7.08 27.45 112.90 - - -
Megatron 0.45 0.41 0.43 1.52 5.80 - - - - -
Reformer 1.87 3.00 5.37 10.43 21.40 43.83 92.80 203.24 - -
Local Attention 0.70 0.81 1.02 2.09 6.64 13.34 26.77 54.02 110.11 -
Linformer 0.63 0.50 0.67 0.65 1.36 2.60 5.04 9.92 19.69 43.47
Smyrf 2.38 2.32 3.76 7.16 14.14 28.09 55.98 111.73 - -
LSformer 1.22 1.29 1.44 3.28 10.99 21.72 43.29 86.32 172.76 -
Block Sparse 0.96 1.04 1.66 2.16 5.41 16.15 - - - -
Longformer 0.99 0.98 0.99 1.56 4.79 11.07 32.98 - - -
BigBird 0.96 1.02 1.02 1.48 5.05 11.59 34.16 - - -
FlashAttention 0.03 0.04 0.17 0.68 2.28 8.40 33.55 134.14 537.50 2150.88
Block-Sparse FlashAttention 0.05 0.04 0.05 0.11 0.35 0.68 1.33 2.54 5.34 10.73
Table 13: Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length,
with masking . Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.44 0.46 1.53 5.33 20.34 79.87 - - - -
Megatron 0.29 0.31 0.65 1.95 6.49 - - - - -
Reformer 2.31 4.47 8.68 17.20 34.14 68.09 136.02 - - -
Local Attention 0.51 0.62 1.30 3.81 13.33 26.72 53.41 106.82 214.15 -
Linformer 0.76 0.81 0.94 0.87 2.24 4.25 8.35 16.38 32.67 72.11
Smyrf 1.34 2.77 5.30 10.46 20.73 41.27 82.41 164.86 - -
LSformer 1.66 1.61 3.09 7.42 19.68 38.35 74.92 147.86 - -
Block Sparse 1.24 1.25 2.04 2.91 6.78 19.67 - - - -
Longformer 1.27 1.23 1.24 1.85 4.99 10.21 24.89 - - -
BigBird 1.43 1.50 1.44 1.69 5.25 10.86 26.26 - - -
FlashAttention 0.21 0.22 0.62 1.84 5.77 22.25 86.21 338.91 1343.91 5361.09
Block-Sparse FlashAttention 0.22 0.22 0.26 0.57 1.55 3.13 5.98 12.21 23.49 47.85
32Table 14: Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by
sequence length, with masking . Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.80 0.81 2.08 7.23 27.51 107.58 - - - -
Megatron 0.81 0.83 1.09 3.36 12.39 - - - - -
Reformer 4.16 7.46 14.06 27.68 55.66 112.15 229.37 - - -
Local Attention 1.39 1.68 2.08 5.83 20.04 40.16 80.44 161.35 325.11 -
Linformer 1.51 1.42 1.56 1.67 3.67 6.99 13.63 26.77 53.36 117.56
Smyrf 3.38 4.93 9.07 17.66 34.94 69.55 138.72 277.41 - -
LSformer 3.08 3.10 4.26 10.90 31.59 61.72 121.51 241.18 - -
Block Sparse 2.39 2.40 3.31 5.02 12.25 35.94 - - - -
Longformer 2.36 2.34 2.38 2.94 9.83 21.35 58.12 - - -
BigBird 2.35 2.35 2.37 3.25 10.36 22.57 60.63 - - -
FlashAttention 0.32 0.30 0.83 2.37 7.95 30.77 119.98 473.65 1883.43 7513.01
Block-Sparse FlashAttention 0.34 0.34 0.36 0.69 1.85 3.89 7.16 14.85 30.46 60.03
Table 15: Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length,
with dropout . Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.26 0.24 0.57 1.80 6.56 25.34 - - - -
Megatron 0.27 0.27 0.56 1.88 6.56 - - - - -
Reformer 1.83 2.96 5.31 10.33 21.19 43.42 91.96 201.34 - -
Local Attention 0.51 0.60 0.78 2.01 6.23 12.52 25.07 50.50 102.18 -
Linformer 0.47 0.37 0.49 0.52 1.37 2.65 5.12 10.13 20.25 44.16
Smyrf 2.12 2.01 3.15 5.97 11.83 23.36 46.48 92.72 - -
LSformer 1.28 1.33 1.51 3.39 11.40 22.54 44.96 89.85 179.73 -
Block Sparse 1.03 1.00 1.72 2.39 5.96 17.88 - - - -
Longformer 1.02 1.03 1.03 1.73 5.10 11.63 34.22 - - -
BigBird 0.99 1.03 1.01 1.58 5.36 12.27 35.56 - - -
FlashAttention 0.10 0.10 0.22 0.83 2.81 10.38 41.63 167.01 668.74 2678.11
Block-Sparse FlashAttention 0.54 0.51 0.68 0.61 0.67 1.10 1.89 3.71 7.18 14.41
Table 16: Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length,
with dropout . Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.44 0.35 0.90 2.94 10.77 41.67 - - - -
Megatron 0.28 0.33 0.92 2.94 10.80 - - - - -
Reformer 2.24 4.34 8.39 16.62 33.02 65.77 131.52 - - -
Local Attention 0.51 0.58 1.41 3.71 12.96 25.98 51.94 103.72 207.78 -
Linformer 0.84 0.74 0.79 0.85 2.28 4.37 8.66 17.02 33.78 -
Smyrf 1.27 2.56 4.90 9.66 19.16 38.13 76.17 152.39 - -
LSformer 1.67 1.77 3.03 7.52 20.10 39.13 76.35 150.83 - -
Block Sparse 1.27 1.36 2.15 3.04 7.27 21.18 - - - -
Longformer 1.28 1.34 1.38 1.98 5.24 10.74 25.95 - - -
BigBird 1.48 1.47 1.50 1.81 5.57 11.38 27.43 - - -
FlashAttention 0.15 0.18 0.58 1.86 6.50 26.21 104.27 416.10 1661.92 6643.01
Block-Sparse FlashAttention 0.17 0.17 0.17 0.40 1.10 2.04 4.43 9.33 18.28 37.31
Table 17: Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by
sequence length, with dropout . Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.66 0.67 1.43 4.82 17.47 67.29 - - - -
Megatron 0.88 0.90 1.49 4.73 17.41 - - - - -
Reformer 4.06 7.28 13.68 26.98 54.27 109.39 223.80 - - -
Local Attention 1.09 1.40 1.99 5.61 19.23 38.62 77.30 154.63 311.12 -
Linformer 1.31 1.21 1.30 1.39 3.73 7.15 14.05 27.69 55.00 -
Smyrf 3.00 4.37 8.05 15.66 31.04 61.64 123.04 245.65 - -
LSformer 3.07 3.17 4.31 10.89 31.54 61.78 121.56 240.94 - -
Block Sparse 2.54 2.52 3.71 5.44 13.29 39.19 - - - -
Longformer 2.47 2.49 2.51 3.10 10.39 22.49 60.44 - - -
BigBird 2.51 2.49 2.52 3.40 10.97 23.89 63.28 - - -
FlashAttention 0.35 0.36 0.80 2.52 9.16 36.70 146.13 583.45 2332.01 9323.63
Block-Sparse FlashAttention 0.91 0.83 0.94 0.92 1.83 3.50 7.02 13.56 26.71 53.92
33Table 18: Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length.
Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.21 0.22 0.43 1.27 4.32 16.47 67.77 - - -
Megatron 0.24 0.26 0.42 1.33 4.28 - - - - -
Reformer 1.77 2.82 5.01 9.74 20.03 41.11 87.39 192.40 - -
Local Attention 0.48 0.57 0.80 1.90 5.76 11.56 23.13 46.65 94.74 -
Linformer 0.46 0.36 0.45 0.50 1.09 2.09 4.01 7.90 15.70 35.40
Smyrf 1.94 1.96 3.01 5.69 11.26 22.23 44.21 88.22 - -
LSformer 1.21 1.34 1.34 3.31 11.01 21.71 43.27 86.32 172.85 -
Block Sparse 0.96 1.04 1.66 2.16 5.41 16.15 - - - -
Longformer 0.99 0.98 0.99 1.56 4.79 11.07 32.98 - - -
BigBird 0.96 1.02 1.02 1.48 5.05 11.59 34.16 - - -
FlashAttention 0.08 0.09 0.18 0.68 2.40 8.42 33.54 134.03 535.95 2147.05
Block-Sparse FlashAttention 0.56 0.52 0.63 0.65 0.61 0.96 1.69 3.02 5.69 11.77
Table 19: Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length.
Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.26 0.29 0.78 2.44 8.82 33.87 - - - -
Megatron 0.29 0.30 0.80 2.59 8.86 - - - - -
Reformer 2.18 4.21 8.14 16.12 32.02 63.84 127.60 - - -
Local Attention 0.51 0.64 1.28 3.60 12.52 25.08 50.22 100.23 200.66 -
Linformer 0.69 0.76 0.69 0.80 2.04 3.88 7.67 15.04 30.11 63.15
Smyrf 1.24 2.49 4.77 9.42 18.65 37.12 74.15 148.35 - -
LSformer 1.68 1.61 3.02 7.40 19.72 38.27 74.89 147.99 - -
Block Sparse 1.24 1.25 2.04 2.91 6.78 19.67 - - - -
Longformer 1.27 1.23 1.24 1.85 4.99 10.21 24.89 - - -
BigBird 1.43 1.50 1.44 1.69 5.25 10.86 26.26 - - -
FlashAttention 0.11 0.16 0.52 1.62 5.45 21.57 84.75 336.00 1338.56 5343.19
Block-Sparse FlashAttention 0.11 0.12 0.16 0.38 1.20 2.34 4.69 9.10 18.74 37.04
Table 20: Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by
sequence length. Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.67 0.70 1.18 3.67 13.22 50.44 - - - -
Megatron 0.74 0.65 1.23 3.80 13.21 - - - - -
Reformer 3.93 7.01 13.15 25.89 52.09 105.00 215.13 - - -
Local Attention 1.09 1.27 1.99 5.38 18.32 36.77 73.67 147.29 296.35 -
Linformer 1.31 1.25 1.30 1.29 3.20 6.10 11.93 23.39 46.72 100.52
Smyrf 2.98 4.23 7.78 15.12 29.96 59.45 118.60 237.02 - -
LSformer 3.03 3.05 4.26 10.70 30.77 60.15 118.33 234.94 - -
Block Sparse 2.39 2.40 3.31 5.02 12.25 35.94 - - - -
Longformer 2.36 2.34 2.38 2.94 9.83 21.35 58.12 - - -
BigBird 2.35 2.35 2.37 3.25 10.36 22.57 60.63 - - -
FlashAttention 0.31 0.31 0.73 2.29 7.64 30.09 118.50 470.51 1876.08 7492.85
Block-Sparse FlashAttention 0.74 0.77 0.82 0.88 1.71 3.21 6.56 12.60 24.93 50.39
Table 21: Memory usage (MB) of various exact/approximate/sparse attention mechanisms by sequence length. Best
inbold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 36 104 336 1184 4416 17024 - - - -
Megatron 36 104 336 1184 4416 - - - - -
Reformer 377 754 1508 3016 6033 12067 24134 - - -
Local Attention 53 110 232 592 1696 3392 6784 13568 27136 -
Linformer 25 52 114 287 832 1652 3292 6572 13132 26252
Smyrf 217 434 868 1737 3474 6947 13894 27788 - -
LSformer 72 152 333 796 2540 5068 10125 20240 - -
Block Sparse 33 82 228 408 910 2401 - - - -
Longformer 30 61 124 277 681 1370 2748 - - -
BigBird 33 66 131 294 708 1431 2872 - - -
FlashAttention 22 44 104 209 418 836 1672 3344 6688 13376
Block-Sparse FlashAttention 22 44 104 209 418 836 1672 3344 6690 13384
34Generating Long Sequences with Sparse Transformers
Rewon Child1Scott Gray1Alec Radford1Ilya Sutskever1
Abstract
Transformers are powerful sequence models, but
require time and memory that grows quadrati-
cally with the sequence length. In this paper we
introduce sparse factorizations of the attention
matrix which reduce this to O(npn). We also
introduce a) a variation on architecture and initial-
ization to train deeper networks, b) the recompu-
tation of attention matrices to save memory, and
c) fast attention kernels for training. We call net-
works with these changes Sparse Transformers,
and show they can model sequences tens of thou-
sands of timesteps long using hundreds of layers.
We use the same architecture to model images,
audio, and text from raw bytes, setting a new state
of the art for density modeling of Enwik8, CIFAR-
10, and ImageNet-64. We generate unconditional
samples that demonstrate global coherence and
great diversity, and show it is possible in principle
to use self-attention to model sequences of length
one million or more.
1. Introduction
Estimating complex, high-dimensional data distributions is
a central problem in unsupervised learning, as many down-
stream applications of interest involve generation of text,
images, audio, and other data. Additionally, it is believed to
be a key component of unsupervised representation learning.
Recently, neural autoregressive models have achieved im-
pressive results in this domain, achieving state-of-the-art in
modeling natural language (Jozefowicz et al., 2016) (Rad-
ford et al., 2018) (Dai et al., 2018), raw audio (Van Den Oord
et al., 2016) (Mehri et al., 2016), and images (Oord et al.,
2016) (Menick & Kalchbrenner, 2018) (Salimans et al.,
2017) (Reed et al., 2017) (Chen et al., 2017).
These methods decompose a joint probability distribution
into a product of conditional ones. Modeling these condi-
tional distributions is extremely challenging, however, as
they contain many complex, long-range dependencies and
require a suitably expressive model architecture to learn
them.
Architectures based off CNNs (Oord et al., 2016) have made
Figure 1. Unconditional samples from our neural autoregressive
model on ImageNet 64 and a classical music dataset. We used the
same self-attention based architecture for audio, images, and text.
The samples above were generated with softmax temperature 1.0,
and had lengths 12,288 and 65,536. Audio samples be listened to at
https://openai.com/blog/sparse-transformer
great progress in this direction, but require signiﬁcant depth
to expand their receptive ﬁeld. To address this, WaveNet
(Van Den Oord et al., 2016) introduced dilated convolutions,
which allowed the network to model long-range dependen-
cies in a logarithmic number of layers.
Separately, the Transformer (Vaswani et al., 2017) has been
shown to excel on many natural language tasks, which may
be in part due to its ability to model arbitrary dependencies
in a constant number of layers. As each self-attention layer
has a global receptive ﬁeld, the network can allocate rep-
resentational capacity to the input regions for which it isarXiv:1904.10509v1  [cs.LG]  23 Apr 2019Generating Long Sequences with Sparse Transformers
most useful. Thus the architecture may be more ﬂexible
at generating diverse data types than networks with ﬁxed
connectivity patterns.
However, the memory and computational requirements of
such networks grows quadratically with sequence length,
which excludes their use on long sequences.
The main contribution of this work is to introduce several
sparse factorizations of the attention matrix, which scale
asO(nppn)with the sequence length without sacriﬁcing
performance. These work by separating the full attention
computation into several faster attention operations which,
when combined, can approximate the dense attention oper-
ation. We use this to apply self-attention to sequences of
unprecedented length.
Additionally, we introduce several other changes to the
Transformer, including:
A restructured residual block and weight initialization
to improve training of very deep networks
A set of sparse attention kernels which efﬁciently com-
pute subsets of the attention matrix
Recomputation of attention weights during the back-
wards pass to reduce memory usage
We empirically validate that models augmented in this man-
ner can achieve state-of-the-art compression and generation
of natural language, raw audio, and natural images. The
simplicity of the architecture leads us to believe it may be
useful for many problems of interest.
2. Related Work
The most related work involves other techniques for scaling
up autoregressive generative models. For images, (Reed
et al., 2017) models conditional independence between the
pixels in order to generate many locations in parallel, and
(Menick & Kalchbrenner, 2018) imposes an ordering and
multi-scale upsampling procedure to generate high ﬁdelity
samples. (Parmar et al., 2018) uses blocks of local attention
to apply Transformers to images. For text, (Dai et al., 2018)
introduces a state reuse ”memory” for modeling long-term
dependencies. And for audio, in addition to (Van Den Oord
et al., 2016), (Mehri et al., 2016) used a hierarchical struc-
ture and RNNs of varying clock-rates to use long contexts
during inference, similar to (Koutnik et al., 2014). (Huang
et al., 2018) apply Transformers to MIDI generation with
an efﬁcient relative attention.
Our work is simpler than many of the techniques above and
can be applied equally across images, text, and audio. Many
of the above techniques are orthogonal to ours, moreover,
and could be used in conjunction with ours.Outside of generative modeling, there are several works
relevant to improving the efﬁciency of attention based off
chunking (Chiu & Raffel, 2017) or using ﬁxed length repre-
sentations (Britz et al., 2017). Other works have investigated
attention with multiple ”hops”, such as (Sukhbaatar et al.,
2015) and (Gehring et al., 2017).
It is worth noting that the Gated Pixel CNN (Oord et al.,
2016) and WaveNet (Van Den Oord et al., 2016) use multi-
plicative interactions in their networks, which are related to
self-attention.
3. Background
We consider the task of autoregressive sequence gener-
ation, where the joint probability of a sequence x=
fx1;x2;:::;x ngis modeled as the product of conditional
probability distributions and parameterized by a network .
p(x) =nY
i=1p(xijx1;:::;x i 1;) (1)
We treat images, text, and audio as a sequence of discrete
tokens, typically raw bytes. The network takes in the se-
quence of tokens and outputs a categorical distribution over
thevpossible values of the next token using the softmax
function, where vis the size of the vocabulary . The training
objective is to maximize the log-probability of the data with
respect to.
A simple and powerful choice for model is a Transformer
(Vaswani et al., 2017) in decoder-only mode, as demon-
strated by (Radford et al., 2018) and (Liu et al., 2018). These
models transform the input sequence with blocks of mul-
tihead self-attention over the entire sequence, followed by
dense transformations over each sequence element. The self-
attention portion of the network must compute nweightings
for each ofnelements, however, which can quickly become
intractable as the sequence length grows.
In the following sections, we describe our modiﬁcations to
the Transformer architecture which make it more suitable
for modeling long sequences.
4. Factorized Self-Attention
Sparse Transformers separate the full self-attention opera-
tion across several steps of attention, as visualized in Figure
3(b) and 3(c). To motivate our approach, we ﬁrst perform
a qualitative assessment of attention patterns learned by a
standard Transformer on an image dataset.Generating Long Sequences with Sparse Transformers
Figure 2. Learned attention patterns from a 128-layer network on CIFAR-10 trained with full attention. White highlights denote attention
weights for a head while generating a given pixel, and black denotes the autoregressive mask. Layers are able to learn a variety of
specialized sparse structures, which may explain their ability to adapt to different domains. a) Many early layers in the network learn
locally connected patterns, which resemble convolution. b) In layers 19 and 20, the network learned to split the attention across a
row attention and column attention, effectively factorizing the global attention calculation. c) Several attention layers showed global,
data-dependent access patterns. d) Typical layers in layers 64-128 exhibited high sparsity, with positions activating rarely and only for
speciﬁc input patterns.
(a) Transformer
 (b) Sparse Transformer (strided)
 (c) Sparse Transformer (ﬁxed)
Figure 3. Two 2d factorized attention schemes we evaluated in comparison to the full attention of a standard Transformer (a). The top
row indicates, for an example 6x6 image, which positions two attention heads receive as input when computing a given output. The
bottom row shows the connectivity matrix (not to scale) between all such outputs (rows) and inputs (columns). Sparsity in the connectivity
matrix can lead to signiﬁcantly faster computation. In (b) and (c), full connectivity between elements is preserved when the two heads are
computed sequentially. We tested whether such factorizations could match in performance the rich connectivity patterns of Figure 2.Generating Long Sequences with Sparse Transformers
4.1. Qualitative assessment of learned attention
patterns
We visualized the attention patterns learned by a 128-layer
self-attention network on CIFAR-10, and present several
examples in Figure 2. Visual inspection showed that most
layers had sparse attention patterns across most data points,
suggesting that some form of sparsity could be introduced
without signiﬁcantly affecting performance. Several layers
(Figure 2c) clearly exhibited global patterns, however, and
others exhibited data-dependent sparsity (Figure 2d), both
of which would be impacted by introducing a predetermined
sparsity pattern into all of the attention matrices.
In this paper, we restricted our investigation to a class of
sparse attention patterns that have connectivity between all
positions over several steps of attention. These methods can
be more efﬁcient than full attention while still providing
global context to any given position. We aimed to empiri-
cally validate the performance of these factorized patterns
on a range of tasks, given that they are unable to learn the
exact same mappings as those in Figure 2. We present the
formulation of factorized attention below.
4.2. Factorized self-attention
A self-attention layer maps a matrix of input embeddings
Xto an output matrix and is parameterized by a connectiv-
ity patternS=fS1;:::;S ng, whereSidenotes the set of
indices of the input vectors to which the ith output vector
attends. The output vector is a weighted sum of transforma-
tions of the input vectors:
Attend(X;S) =
a(xi;Si)
i2f1;:::;ng(2)
a(xi;Si) = softmax 
(Wqxi)KT
Sip
d!
VSi (3)
KSi=
Wkxj
j2SiVSi=
Wvxj
j2Si(4)
HereWq,Wk, andWvrepresent the weight matrices which
transform a given xiinto a query ,key, orvalue , anddis
the inner dimension of the queries and keys. The output at
each position is a sum of the values weighted by the scaled
dot-product similarity of the keys and queries.
Full self-attention for autoregressive models deﬁnes Si=
fj:jig, allowing every element to attend to all previous
positions and its own position.
Factorized self-attention instead has pseparate attention
heads, where the mth head deﬁnes a subset of the indices
A(m)
i fj:jigand letsSi=A(m)
i. We are
chieﬂy interested in efﬁcient choices for the subset A, where
jA(m)
ij/ppn.Additionally, for the time being we consider valid choices
ofA, where all input positions are connected to all future
output positions across the psteps of attention.
For everyjipair, we set every Asuch thatican attend
tojthrough a path of locations with maximum length p+ 1.
Speciﬁcally, if (j;a;b;c;:::;i )is the path of indices, then
j2A(1)
a,a2A(2)
b,b2A(3)
c, and so forth.
These two criteria allow us keep the ability of Transformers
to propagate signals from arbitrary input positions to arbi-
trary output positions in a constant number of steps, while
reducing the total effective computation to O(nppn). We
also note that softening the validity criterion (for instance,
having a series of only locally connected layers) may be a
useful inductive bias for certain domains.
In this work, we explore two factorizations for p= 2, which
we describe in the following section, though we note that
the same techniques can be easily extended to higher dimen-
sions.
4.3. Two-dimensional factorized attention
A natural approach to deﬁning a factorized attention pattern
in two dimensions is to have one head attend to the previous
llocations, and the other head attend to every lth location,
wherelis the stride and chosen to be close topn, a method
we call strided attention.
Formally,A(1)
i=ft;t+ 1;:::;igfort= max(0;i l)
andA(2)
i=fj: (i j) modl= 0g. This pattern can be
visualized in Figure 3(b).
This formulation is convenient if the data naturally has a
structure that aligns with the stride, like images or some
types of music. For data without a periodic structure, like
text, however, we ﬁnd that the network can fail to properly
route information with the strided pattern, as spatial coor-
dinates for an element do not necessarily correlate with the
positions where the element may be most relevant in the
future.
In those cases, we instead use a ﬁxed attention pattern (Fig-
ure 3(c)), where speciﬁc cells summarize previous locations
and propagate that information to all future cells.
Formally,A(1)
i=fj: (bj=lc=bi=lc)g, where the brackets
denote the ﬂoor operation, and A(2)
i=fj:jmodl2
ft;t+ 1;:::;lg, wheret=l candcis a hyperparameter.
Concretely, if the stride is 128 and c= 8, then all future
positions greater than 128 can attend to positions 120-128,
all positions greater than 256 can attend to 248-256, and so
forth.
A ﬁxed-attention pattern with c= 1limits the expressivity
of the network signiﬁcantly, as many representations inGenerating Long Sequences with Sparse Transformers
the network are only used for one block whereas a small
number of locations are used by all blocks. We instead
found choosing c2f8;16;32gfor typical values of l2
f128;256gto perform well, although it should be noted that
this increases the computational cost of this method by cin
comparison to the strided attention.
Additionally, we found that when using multiple heads,
having them attend to distinct subblocks of length cwithin
the block of size lwas preferable to having them attend to
the same subblock.
In the subsequent section, we describe how to incorporate
factorized attention into the Sparse Transformer architec-
ture.
5. Sparse Transformer
Here we fully describe the Sparse Transformer architecture,
which is a modiﬁed version of the Transformer (Vaswani
et al., 2017).
5.1. Factorized attention heads
Standard dense attention simply performs a linear transfor-
mation of the attend function deﬁned in Equation 2:
attention(X) =Wpattend(X;S) (5)
whereWpdenotes the post-attention weight matrix. The
simplest technique for integrating factorized self-attention
is to use one attention type per residual block, and interleave
them sequentially or at a ratio determined as a hyperparam-
eter:
attention(X) =Wpattend(X;A(rmodp))(6)
Hereris the index of the current residual block and pis the
number of factorized attention heads.
A second approach is to have a single head attend to the
locations of the pixels that both factorized heads would
attend to, which we call a merged head:
attention(X) =Wpattend(X;p[
m=1A(m)) (7)
This is slightly more computationally intensive, but only
by a constant factor. A third approach is to use multi-head
attention (Vaswani et al., 2017), where nhattention products
are computed in parallel, then concatenated along the feature
dimension:
attention(X) =Wp
attend(X;A)(i)
i2f1;:::;n hg(8)
embed
linearsoftmaxnorm
norm
normdropout
dropoutattention
feed-forward. . . Figure 4. Diagram depicting one residual block of the Sparse Trans-
former. The shaded background indicates tensors which are check-
pointed (Chen et al., 2016) and stored in GPU memory. The other
tensors, including the attention weights and feedforward network
activations, are recomputed during the calculation of gradients,
reducing memory usage substantially.
Here, theAcan be the separate attention patterns, the
merged patterns, or interleaved as in Eq. 2. Also, the di-
mensions of the weight matrices inside the attend function
are reduced by a factor of 1=nh, such that the number of
parameters are invariant across values of nh.
We typically ﬁnd multiple heads to work well, though for
extremely long sequences where the attention dominates the
computation time, it is more worthwhile to perform them
one at a time and sequentially.
5.2. Scaling to hundreds of layers
We found that Transformers were difﬁcult to train with
many layers, as noted by (Al-Rfou et al., 2018). Instead
of incorporating auxillary losses, we adopted the followingGenerating Long Sequences with Sparse Transformers
architectural changes.
First, we use the pre-activation residual block of (He et al.,
2016), deﬁning a network of Nlayers in the following way:
H0= embed(X;W e) (9)
Hk=Hk 1+ resblock( Hk 1) (10)
y= softmax(norm( HN)Wout) (11)
where embed is a function we describe in the next section,
Woutis a weight matrix, and resblock(h)normalizes the
input to the attention block and a positionwise feedforward
network in the following way:
a(H) = dropout(attention(norm( H))) (12)
b(H) = dropout((norm( H+a(H)))) (13)
resblock(H) =a(H) +b(H) (14)
Thenorm function denotes Layer Normalization (Ba et al.,
2016), and (x) =W2f(W1x+b1) +b2. Our choice of
fis the Gaussian Error Linear Unit (Hendrycks & Gimpel,
2016),f(X) =Xsigmoid(1:702X), as used in (Rad-
ford et al., 2018). The output dimension of W1is 4.0 times
the input dimension, unless otherwise noted.
Observe that HNis the sum of Napplications of functions
aandb, and thus each function block receives a gradient
directly from the output layer . We scale the initialization
ofW2andWpin Eq. 5 by1p
2Nto keep the ratio of input
embedding scale to residual block scale invariant across
values ofN.
5.3. Modeling diverse data types
In addition to the embedding of input symbols, positional
embeddings are typically used in Transformers and other
location-agnostic architectures to encode the spatial relation-
ships of data (Gehring et al., 2017), (Parmar et al., 2018).
We found using learned embeddings which either encoded
the structure of the data or the factorized attention patterns
were important for performance of our models.
We added either nemb=ddata ornemb=dattn embed-
dings to each input location, where ddata refers to the num-
ber of dimensions of the data, and dattnis the number of
dimensions of the factorized attention. If xiis the one-hot
encodedith element in the sequence, and o(j)
irepresents
the one-hot encoded position of xiin thejth dimension
(1jnemb), then:
embed(X;W e) =0
@xiWe+nembX
j=1o(j)
iWj1
A
xi2X(15)For images, we used data embeddings, where ddata= 3
for the row, column, and channel location of each input
byte. For text and audio, we used two-dimensional attention
embeddings, where dattn= 2and the index corresponds to
each position’s row and column index in a matrix of width
equal to the stride.
5.4. Saving memory by recomputing attention weights
Gradient checkpointing has been shown to be effective in
reducing the memory requirements of training deep neural
networks (Chen et al., 2016), (Gruslys et al., 2016). It is
worth noting, however, that this technique is particularly
effective for self-attention layers when long sequences are
processed, as memory usage is high for these layers relative
to the cost of computing them.
Using recomputation alone, we are able to train dense atten-
tion networks with hundreds of layers on sequence lengths
of 16,384, which would be infeasible on modern hardware
otherwise.
In our experiments, we recompute the attention and feed-
forward blocks during the backwards pass. To simplify
our implementation, we do not apply dropout within the
attention blocks, as in (Vaswani et al., 2017), and instead
only apply it at the end of each residual addition, as seen in
Figure 4.
5.5. Efﬁcient block-sparse attention kernels
The sparse attention masks in 3(b) and 3(c) can be efﬁciently
computed by slicing out sub-blocks from the query, key, and
value matrices and computing the product in blocks. Atten-
tion over a local window can be computed as-is, whereas
attention with a stride of kcan be computed by transposing
the matrix and computing a local window. Fixed attention
positions can be aggregated and computed in blocks.
In order to ease experimentation, we implemented a set of
GPU kernels which efﬁciently perform these operations.
The softmax operation is fused into a single kernel and
also uses registers to eliminate loading the input data more
than once, allowing it to run at the same speed as a simple
nonlinearity. The upper triangle of the attention matrix
is never computed, moreover, removing the need for the
negative bias term of (Vaswani et al., 2017) and halving the
number of operations to be performed.
5.6. Mixed-precision training
We store network weights in single-precision ﬂoating-point,
but otherwise compute network activations and gradients in
half-precision, as in (Micikevicius et al., 2017). This acceler-
ates our training due to the usage of Tensor Core operations
on the V100 GPU. During the gradient calculation, we useGenerating Long Sequences with Sparse Transformers
Figure 5. Unconditional samples from ImageNet 64x64, generated with an unmodiﬁed softmax temperature of 1.0. We are able to learn
long-range dependencies directly from pixels without using a multi-scale architecture.
dynamic loss scaling to reduce numerical underﬂow, and
we communicate half-precision gradients when averaging
across multiple GPUs. When sampling, we cast the queries
and keys to single-precision, as the query-key product can
sometimes overﬂow the max value of half-precision.
6. Training
We use the Adam optimizer with a linear warmup of 5000
iterations and a gradient clipping of 1.0, both of which we
found important for model stability. We use a weight decay
penalty of 0.01. We annealed the learning rate according to
a cosine decay as in (Radford et al., 2018). We train on 8
V100 GPUs unless otherwise noted.
All embeddings are of a constant dimension d, usually one
off256;512;1024g. By default, all linear transforms are to
the same dimension, with the exception of the feed-forward
network, which projects the input to 4d, unless we use
“half-size” transformations, where it is 2d. Additionally,
sometimes we halve the size of the query and key transfor-
mations.
We initialize the token embedding WefromN(0;0:125p
d)and
the position embeddings from N(0;0:125pdnemb). Within the
attention and feedforward components, all biases are initial-ized to 0 and all weights are initialized from N(0;0:125pdin)
wheredinis the fan-in dimension. The weight matrix for
the output logits was initialized to 0.
7. Experiments
We empirically test our architecture on density modeling
tasks including natural images, text, and raw audio. A
summary of the results is available in Table 1. We found
that, in addition to running signiﬁcantly faster than full
attention, sparse patterns also converged to lower error, as
shown in Table 2. This may point to a useful inductive bias
from the sparsity patterns we introduced, or an underlying
optimization issue with full attention.
7.1. CIFAR-10
We train strided Sparse Transformers on CIFAR-10 images
represented as sequences of 3072 bytes. Models have 2
heads, 128 layers, d= 256, half-size feedforward network
and query-key projections, and are trained for 120 epochs
with a learning rate of 0.00035 and a dropout rate of 0.25
until validation error stops decreasing.
We use 48000 examples for training and 2000 examples for
validation, evaluating the performance of our best models onGenerating Long Sequences with Sparse Transformers
Table 1. Summary of our ﬁndings for density modeling tasks. Re-
sults are reported in bits per byte, which is equivalent to bits per
dim for image tasks. M refers to millions of parameters.
Model Bits per byte
CIFAR-10
PixelCNN (Oord et al., 2016) 3.03
PixelCNN++ (Salimans et al., 2017) 2.92
Image Transformer (Parmar et al., 2018) 2.90
PixelSNAIL (Chen et al., 2017) 2.85
Sparse Transformer 59M (strided) 2.80
Enwik8
Deeper Self-Attention (Al-Rfou et al., 2018) 1.06
Transformer-XL 88M (Dai et al., 2018) 1.03
Transformer-XL 277M (Dai et al., 2018) 0.99
Sparse Transformer 95M (ﬁxed) 0.99
ImageNet 64x64
PixelCNN (Oord et al., 2016) 3.57
Parallel Multiscale (Reed et al., 2017) 3.7
Glow (Kingma & Dhariwal, 2018) 3.81
SPN 150M (Menick & Kalchbrenner, 2018) 3.52
Sparse Transformer 152M (strided) 3.44
Classical music, 5 seconds at 12 kHz
Sparse Transformer 152M (strided) 1.97
the test set. The model achieves 2.80 bits per dim ( 2:798
0:004over seeds 1, 2, 3) versus the previous 2:85state of
the art (Chen et al., 2017). We also compare performance of
different attention patterns in Table 2. The strided attention
reaches the lowest error in the shortest amount of time,
surpassing the error of dense attention at 2.82 bits per dim.
7.2. Text
In order to assess Sparse Transformers on datasets without
a strong two-dimensional structure, we trained models on
the EnWik8 dataset, which represents the ﬁrst 108bytes
of Wikipedia and contains a great degree of variability in
periodic structure. We trained with a context length of
12,288, which is longer than previous approaches.
We trained on the ﬁrst 90 million tokens and reserved the last
10 million for validation and test. We used 30-layer ﬁxed
Sparse Transformers with 8 heads, d= 512, and a dropout
rate of 0:40. We trained for 80 epochs until validation loss
stopped decreasing. We used a stride of 128, c= 32 , and
merged the factorized attention heads.
Our best model reached 0.99 bits per dim ( 0:9920:001
over seeds 1, 2, 3), surpassing the 1.03 state-of-the-art for
a similarly-sized Transformer-XL (Dai et al., 2018) and
matching the 0.99 of a model trained with more than doubleTable 2. Sparse patterns showed increased speed and also better
loss on the datasets where we could compare both, which may
point to a useful inductive bias in the patterns we learned or an
underlying optimization issue with full attention.
Model Bits per byte Time/Iter
Enwik8 (12,288 context)
Dense Attention 1.00 1.31
Sparse Transformer (Fixed) 0.99 0.55
Sparse Transformer (Strided) 1.13 0.35
CIFAR-10 (3,072 context)
Dense Attention 2.82 0.54
Sparse Transformer (Fixed) 2.85 0.47
Sparse Transformer (Strided) 2.80 0.38
Table 3. We observe increased compression of Enwik8 with longer
contexts, suggesting the Sparse Transformer can effectively incor-
porate long-term dependencies.
Minimum context length during evaluation Bits per byte
6,144 tokens 0.9952
9,216 tokens 0.9936
10,752 tokens 0.9932
11,904 tokens 0.9930
12,096 tokens 0.9922
12,160 tokens 0.9908
the number of parameters. Strided attention failed to do well
on this dataset, whereas ﬁxed patterns were able to recover
and surpass the performance of dense attention, as listed in
Table 2.
Additionally, during evaluation of the test set, we modiﬁed
the minimum context length the network could use by evalu-
ating fewer tokens in parallel. We saw monotonic increases
in performance with more tokens used, up to 12,160 out
of the 12,288 tokens used for training (see Table 3), which
suggests the network is effectively incorporating long-term
dependencies.
7.3. ImageNet 64x64
In order to test the ability of the model to learn long range
dependencies and scale to a large dataset, we train on the
version of downsampled ImageNet released by (Oord et al.,
2016) and evaluate on the validation set. We used a 48 layer
strided Sparse Transformer with 16 attention heads and d
= 512, totaling 152 million parameters. We used a stride
of 128, a dropout of 0.01, and trained for 70 epochs, which
took 7 days on 64 V100 GPUs.
Our model achieves a loss of 3.44 bits per dim (3.437 across
1 run), in comparison to the previous 3.52 (Menick & Kalch-
brenner, 2018).Generating Long Sequences with Sparse Transformers
Additionally, we generate unconditional samples (Figure
5) at an unmodiﬁed softmax temperature of 1.0, from the
model and from one trained with twice the layers (300M
parameters total). We include here samples from the 300M
parameter model. On visual assessment we ﬁnd no artifacts
from the sparsity patterns and see evidence of long-term
structure in most images.
7.4. Classical music from raw audio
To test the extent to which Sparse Transformers are able
to scale to very long contexts, we trained models on the
classical music dataset released by (Dieleman et al., 2018).
As details of the dataset processing are unavailable, we omit
any direct comparison to other work and instead study what
size of Sparse Transformer we can train with increasing
context size. For each sequence length, we attempted to
train the largest model which could entirely ﬁt into 16GB
V100 accelerators without model parallelism.
Overall, we found that increasing the sequence length by a
factor of 4 requires a reduction in model capacity of approx-
imately 4p
4 = 8 . Thus we found we could use factorized
self-attention on sequences over 1 million timesteps long,
albeit with extremely few parameters (3 million).
Samples are available for sequences of length 65,536, which
correspond to around 5 seconds of generated audio at 12kHz.
The samples clearly demonstrate global coherence over the
sampled period, and exhibit a variety of play styles and
tones, swapping from rhythmic playing to forceful. To
listen to samples, visit https://openai.com/blog/
sparse-transformer . Sample quality quickly de-
grades for greater sequence lengths due to reduced model
capacity.
Table 4. Performance of a strided Sparse Transformer on a classical
audio dataset ( -law encoded at 12 kHz) as a function of sequence
length and model size.
Sequence length Parameters Bits per byte
65,536 152M 1.97
262,144 25M 2.17
1,048,576 3M 2.99
8. Conclusion
We introduced Sparse Transformers and showed they attain
equivalent or better performance on density modeling of
long sequences than standard Transformers while requiring
signiﬁcantly fewer operations. This performance is state-
of-the-art in images and text and is easily adaptable to raw
audio. The model demonstrates usage of long-term context
and generates globally coherent samples.9. Acknowledgements
We would like to thank Ashish Vaswani for insightful dis-
cussions during the genesis of the project. We also thank
Joshua Meier and Mark Chen for helpful discussions, and
Johannes Otterbach, Prafulla Dhariwal, and David Luan for
feedback on drafts of this paper.
References
Al-Rfou, R., Choe, D., Constant, N., Guo, M., and Jones,
L. Character-level language modeling with deeper self-
attention. arXiv preprint arXiv:1808.04444 , 2018.
Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.
arXiv preprint arXiv:1607.06450 , 2016.
Britz, D., Guan, M. Y ., and Luong, M.-T. Efﬁcient attention
using a ﬁxed-size memory representation. arXiv preprint
arXiv:1707.00110 , 2017.
Chen, T., Xu, B., Zhang, C., and Guestrin, C. Training
deep nets with sublinear memory cost. arXiv preprint
arXiv:1604.06174 , 2016.
Chen, X., Mishra, N., Rohaninejad, M., and Abbeel, P.
Pixelsnail: An improved autoregressive generative model.
arXiv preprint arXiv:1712.09763 , 2017.
Chiu, C.-C. and Raffel, C. Monotonic chunkwise attention.
arXiv preprint arXiv:1712.05382 , 2017.
Dai, Z., Yang, Z., Yang, Y ., Cohen, W. W., Carbonell, J., Le,
Q. V ., and Salakhutdinov, R. Transformer-xl: Language
modeling with longer-term dependency. 2018.
Dieleman, S., van den Oord, A., and Simonyan, K. The chal-
lenge of realistic music generation: modelling raw audio
at scale. In Advances in Neural Information Processing
Systems , pp. 8000–8010, 2018.
Gehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin,
Y . N. Convolutional sequence to sequence learning. arXiv
preprint arXiv:1705.03122 , 2017.
Gruslys, A., Munos, R., Danihelka, I., Lanctot, M., and
Graves, A. Memory-efﬁcient backpropagation through
time. In Advances in Neural Information Processing
Systems , pp. 4125–4133, 2016.
He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in
deep residual networks. arXiv preprint arXiv:1603.05027 ,
2016.
Hendrycks, D. and Gimpel, K. Bridging nonlinearities and
stochastic regularizers with gaussian error linear units.
arXiv preprint arXiv:1606.08415 , 2016.Generating Long Sequences with Sparse Transformers
Huang, C.-Z. A., Vaswani, A., Uszkoreit, J., Shazeer, N.,
Hawthorne, C., Dai, A. M., Hoffman, M. D., and Eck,
D. An improved relative self-attention mechanism for
transformer with application to music generation. arXiv
preprint arXiv:1809.04281 , 2018.
Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., and
Wu, Y . Exploring the limits of language modeling. arXiv
preprint arXiv:1602.02410 , 2016.
Kingma, D. P. and Dhariwal, P. Glow: Generative ﬂow
with invertible 1x1 convolutions. In Advances in Neural
Information Processing Systems , pp. 10236–10245, 2018.
Koutnik, J., Greff, K., Gomez, F., and Schmidhuber, J. A
clockwork rnn. arXiv preprint arXiv:1402.3511 , 2014.
Liu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepa-
ssi, R., Kaiser, L., and Shazeer, N. Generating
wikipedia by summarizing long sequences. arXiv preprint
arXiv:1801.10198 , 2018.
Mehri, S., Kumar, K., Gulrajani, I., Kumar, R., Jain, S.,
Sotelo, J., Courville, A., and Bengio, Y . Samplernn: An
unconditional end-to-end neural audio generation model.
arXiv preprint arXiv:1612.07837 , 2016.
Menick, J. and Kalchbrenner, N. Generating high ﬁdelity im-
ages with subscale pixel networks and multidimensional
upscaling. arXiv preprint arXiv:1812.01608 , 2018.
Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen,
E., Garcia, D., Ginsburg, B., Houston, M., Kuchaev, O.,
Venkatesh, G., et al. Mixed precision training. arXiv
preprint arXiv:1710.03740 , 2017.
Oord, A. v. d., Kalchbrenner, N., and Kavukcuoglu,
K. Pixel recurrent neural networks. arXiv preprint
arXiv:1601.06759 , 2016.
Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, Ł., Shazeer,
N., and Ku, A. Image transformer. arXiv preprint
arXiv:1802.05751 , 2018.
Radford, A., Narasimhan, K., Salimans, T., and Sutskever,
I. Improving language understanding by genera-
tive pre-training. URL https://s3-us-west-2. ama-
zonaws. com/openai-assets/research-covers/language-
unsupervised/language understanding paper. pdf , 2018.
Reed, S., Oord, A. v. d., Kalchbrenner, N., Colmenarejo,
S. G., Wang, Z., Belov, D., and de Freitas, N. Paral-
lel multiscale autoregressive density estimation. arXiv
preprint arXiv:1703.03664 , 2017.
Salimans, T., Karpathy, A., Chen, X., and Kingma, D. P.
Pixelcnn++: Improving the pixelcnn with discretized lo-
gistic mixture likelihood and other modiﬁcations. arXiv
preprint arXiv:1701.05517 , 2017.Sukhbaatar, S., Weston, J., Fergus, R., et al. End-to-end
memory networks. In Advances in neural information
processing systems , pp. 2440–2448, 2015.
Van Den Oord, A., Dieleman, S., Zen, H., Simonyan, K.,
Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and
Kavukcuoglu, K. Wavenet: A generative model for raw
audio. CoRR abs/1609.03499 , 2016.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-
tion is all you need. In Advances in Neural Information
Processing Systems , pp. 5998–6008, 2017.High-Resolution Image Synthesis with Latent Diffusion Models
Robin Rombach1* Andreas Blattmann1Dominik Lorenz1Patrick Esser
 Bj¨orn Ommer1
1Ludwig Maximilian University of Munich & IWR, Heidelberg University, Germany
 Runway ML
https://github.com/CompVis/latent-diffusion
Abstract
By decomposing the image formation process into a se-
quential application of denoising autoencoders, diffusion
models (DMs) achieve state-of-the-art synthesis results on
image data and beyond. Additionally, their formulation al-
lows for a guiding mechanism to control the image gen-
eration process without retraining. However, since these
models typically operate directly in pixel space, optimiza-
tion of powerful DMs often consumes hundreds of GPU
days and inference is expensive due to sequential evalu-
ations. To enable DM training on limited computational
resources while retaining their quality and ﬂexibility, we
apply them in the latent space of powerful pretrained au-
toencoders. In contrast to previous work, training diffusion
models on such a representation allows for the ﬁrst time
to reach a near-optimal point between complexity reduc-
tion and detail preservation, greatly boosting visual ﬁdelity.
By introducing cross-attention layers into the model archi-
tecture, we turn diffusion models into powerful and ﬂexi-
ble generators for general conditioning inputs such as text
or bounding boxes and high-resolution synthesis becomes
possible in a convolutional manner. Our latent diffusion
models (LDMs) achieve new state-of-the-art scores for im-
age inpainting and class-conditional image synthesis and
highly competitive performance on various tasks, includ-
ing text-to-image synthesis, unconditional image generation
and super-resolution, while signiﬁcantly reducing computa-
tional requirements compared to pixel-based DMs.
1. Introduction
Image synthesis is one of the computer vision ﬁelds with
the most spectacular recent development, but also among
those with the greatest computational demands. Espe-
cially high-resolution synthesis of complex, natural scenes
is presently dominated by scaling up likelihood-based mod-
els, potentially containing billions of parameters in autore-
gressive (AR) transformers [66,67]. In contrast, the promis-
ing results of GANs [3, 27, 40] have been revealed to be
mostly conﬁned to data with comparably limited variability
as their adversarial learning procedure does not easily scale
to modeling complex, multi-modal distributions. Recently,
diffusion models [82], which are built from a hierarchy of
denoising autoencoders, have shown to achieve impressive
*The ﬁrst two authors contributed equally to this work.Inputours ( f= 4)
PSNR: 27:4R-FID: 0:58DALL-E ( f= 8)
PSNR: 22:8R-FID: 32:01VQGAN ( f= 16 )
PSNR: 19:9R-FID: 4:98
Figure 1. Boosting the upper bound on achievable quality with
less agressive downsampling. Since diffusion models offer excel-
lent inductive biases for spatial data, we do not need the heavy spa-
tial downsampling of related generative models in latent space, but
can still greatly reduce the dimensionality of the data via suitable
autoencoding models, see Sec. 3. Images are from the DIV2K [1]
validation set, evaluated at 5122px. We denote the spatial down-
sampling factor by f. Reconstruction FIDs [29] and PSNR are
calculated on ImageNet-val. [12]; see also Tab. 8.
results in image synthesis [30,85] and beyond [7,45,48,57],
and deﬁne the state-of-the-art in class-conditional image
synthesis [15,31] and super-resolution [72]. Moreover, even
unconditional DMs can readily be applied to tasks such
as inpainting and colorization [85] or stroke-based syn-
thesis [53], in contrast to other types of generative mod-
els [19,46,69]. Being likelihood-based models, they do not
exhibit mode-collapse and training instabilities as GANs
and, by heavily exploiting parameter sharing, they can
model highly complex distributions of natural images with-
out involving billions of parameters as in AR models [67].
Democratizing High-Resolution Image Synthesis DMs
belong to the class of likelihood-based models, whose
mode-covering behavior makes them prone to spend ex-
cessive amounts of capacity (and thus compute resources)
on modeling imperceptible details of the data [16, 73]. Al-
though the reweighted variational objective [30] aims to ad-
dress this by undersampling the initial denoising steps, DMs
are still computationally demanding, since training and
evaluating such a model requires repeated function evalu-
ations (and gradient computations) in the high-dimensional
space of RGB images. As an example, training the most
powerful DMs often takes hundreds of GPU days ( e.g. 150 -
1000 V100 days in [15]) and repeated evaluations on a noisy
version of the input space render also inference expensive,
1arXiv:2112.10752v2  [cs.CV]  13 Apr 2022so that producing 50k samples takes approximately 5 days
[15] on a single A100 GPU. This has two consequences for
the research community and users in general: Firstly, train-
ing such a model requires massive computational resources
only available to a small fraction of the ﬁeld, and leaves a
huge carbon footprint [65, 86]. Secondly, evaluating an al-
ready trained model is also expensive in time and memory,
since the same model architecture must run sequentially for
a large number of steps ( e.g. 25 - 1000 steps in [15]).
To increase the accessibility of this powerful model class
and at the same time reduce its signiﬁcant resource con-
sumption, a method is needed that reduces the computa-
tional complexity for both training and sampling. Reducing
the computational demands of DMs without impairing their
performance is, therefore, key to enhance their accessibility.
Departure to Latent Space Our approach starts with
the analysis of already trained diffusion models in pixel
space: Fig. 2 shows the rate-distortion trade-off of a trained
model. As with any likelihood-based model, learning can
be roughly divided into two stages: First is a perceptual
compression stage which removes high-frequency details
but still learns little semantic variation. In the second stage,
the actual generative model learns the semantic and concep-
tual composition of the data ( semantic compression ). We
thus aim to ﬁrst ﬁnd a perceptually equivalent, but compu-
tationally more suitable space , in which we will train diffu-
sion models for high-resolution image synthesis.
Following common practice [11, 23, 66, 67, 96], we sep-
arate training into two distinct phases: First, we train
an autoencoder which provides a lower-dimensional (and
thereby efﬁcient) representational space which is perceptu-
ally equivalent to the data space. Importantly, and in con-
trast to previous work [23,66], we do not need to rely on ex-
cessive spatial compression, as we train DMs in the learned
latent space, which exhibits better scaling properties with
respect to the spatial dimensionality. The reduced complex-
ity also provides efﬁcient image generation from the latent
space with a single network pass. We dub the resulting
model class Latent Diffusion Models (LDMs).
A notable advantage of this approach is that we need to
train the universal autoencoding stage only once and can
therefore reuse it for multiple DM trainings or to explore
possibly completely different tasks [81]. This enables efﬁ-
cient exploration of a large number of diffusion models for
various image-to-image and text-to-image tasks. For the lat-
ter, we design an architecture that connects transformers to
the DM’s UNet backbone [71] and enables arbitrary types
of token-based conditioning mechanisms, see Sec. 3.3.
In sum, our work makes the following contributions :
(i) In contrast to purely transformer-based approaches
[23, 66], our method scales more graceful to higher dimen-
sional data and can thus (a) work on a compression level
which provides more faithful and detailed reconstructions
than previous work (see Fig. 1) and (b) can be efﬁciently
Figure 2. Illustrating perceptual and semantic compression: Most
bits of a digital image correspond to imperceptible details. While
DMs allow to suppress this semantically meaningless information
by minimizing the responsible loss term, gradients (during train-
ing) and the neural network backbone (training and inference) still
need to be evaluated on all pixels, leading to superﬂuous compu-
tations and unnecessarily expensive optimization and inference.
We propose latent diffusion models (LDMs) as an effective gener-
ative model and a separate mild compression stage that only elim-
inates imperceptible details. Data and images from [30].
applied to high-resolution synthesis of megapixel images.
(ii) We achieve competitive performance on multiple
tasks (unconditional image synthesis, inpainting, stochastic
super-resolution) and datasets while signiﬁcantly lowering
computational costs. Compared to pixel-based diffusion ap-
proaches, we also signiﬁcantly decrease inference costs.
(iii) We show that, in contrast to previous work [93]
which learns both an encoder/decoder architecture and a
score-based prior simultaneously, our approach does not re-
quire a delicate weighting of reconstruction and generative
abilities. This ensures extremely faithful reconstructions
and requires very little regularization of the latent space.
(iv) We ﬁnd that for densely conditioned tasks such
as super-resolution, inpainting and semantic synthesis, our
model can be applied in a convolutional fashion and render
large, consistent images of 10242px.
(v) Moreover, we design a general-purpose conditioning
mechanism based on cross-attention, enabling multi-modal
training. We use it to train class-conditional, text-to-image
and layout-to-image models.
(vi) Finally, we release pretrained latent diffusion
and autoencoding models at https : / / github .
com/CompVis/latent-diffusion which might be
reusable for a various tasks besides training of DMs [81].
2. Related Work
Generative Models for Image Synthesis The high di-
mensional nature of images presents distinct challenges
to generative modeling. Generative Adversarial Networks
(GAN) [27] allow for efﬁcient sampling of high resolution
images with good perceptual quality [3, 42], but are difﬁ-
2cult to optimize [2, 28, 54] and struggle to capture the full
data distribution [55]. In contrast, likelihood-based meth-
ods emphasize good density estimation which renders op-
timization more well-behaved. Variational autoencoders
(V AE) [46] and ﬂow-based models [18, 19] enable efﬁcient
synthesis of high resolution images [9, 44, 92], but sam-
ple quality is not on par with GANs. While autoregressive
models (ARM) [6, 10, 94, 95] achieve strong performance
in density estimation, computationally demanding architec-
tures [97] and a sequential sampling process limit them to
low resolution images. Because pixel based representations
of images contain barely perceptible, high-frequency de-
tails [16,73], maximum-likelihood training spends a dispro-
portionate amount of capacity on modeling them, resulting
in long training times. To scale to higher resolutions, several
two-stage approaches [23,67,101,103] use ARMs to model
a compressed latent image space instead of raw pixels.
Recently, Diffusion Probabilistic Models (DM) [82],
have achieved state-of-the-art results in density estimation
[45] as well as in sample quality [15]. The generative power
of these models stems from a natural ﬁt to the inductive bi-
ases of image-like data when their underlying neural back-
bone is implemented as a UNet [15, 30, 71, 85]. The best
synthesis quality is usually achieved when a reweighted ob-
jective [30] is used for training. In this case, the DM corre-
sponds to a lossy compressor and allow to trade image qual-
ity for compression capabilities. Evaluating and optimizing
these models in pixel space, however, has the downside of
low inference speed and very high training costs. While
the former can be partially adressed by advanced sampling
strategies [47, 75, 84] and hierarchical approaches [31, 93],
training on high-resolution image data always requires to
calculate expensive gradients. We adress both drawbacks
with our proposed LDMs , which work on a compressed la-
tent space of lower dimensionality. This renders training
computationally cheaper and speeds up inference with al-
most no reduction in synthesis quality (see Fig. 1).
Two-Stage Image Synthesis To mitigate the shortcom-
ings of individual generative approaches, a lot of research
[11, 23, 67, 70, 101, 103] has gone into combining the
strengths of different methods into more efﬁcient and per-
formant models via a two stage approach. VQ-V AEs [67,
101] use autoregressive models to learn an expressive prior
over a discretized latent space. [66] extend this approach to
text-to-image generation by learning a joint distributation
over discretized image and text representations. More gen-
erally, [70] uses conditionally invertible networks to pro-
vide a generic transfer between latent spaces of diverse do-
mains. Different from VQ-V AEs, VQGANs [23, 103] em-
ploy a ﬁrst stage with an adversarial and perceptual objec-
tive to scale autoregressive transformers to larger images.
However, the high compression rates required for feasible
ARM training, which introduces billions of trainable pa-
rameters [23, 66], limit the overall performance of such ap-proaches and less compression comes at the price of high
computational cost [23, 66]. Our work prevents such trade-
offs, as our proposed LDMs scale more gently to higher
dimensional latent spaces due to their convolutional back-
bone. Thus, we are free to choose the level of compression
which optimally mediates between learning a powerful ﬁrst
stage, without leaving too much perceptual compression up
to the generative diffusion model while guaranteeing high-
ﬁdelity reconstructions (see Fig. 1).
While approaches to jointly [93] or separately [80] learn
an encoding/decoding model together with a score-based
prior exist, the former still require a difﬁcult weighting be-
tween reconstruction and generative capabilities [11] and
are outperformed by our approach (Sec. 4), and the latter
focus on highly structured images such as human faces.
3. Method
To lower the computational demands of training diffu-
sion models towards high-resolution image synthesis, we
observe that although diffusion models allow to ignore
perceptually irrelevant details by undersampling the corre-
sponding loss terms [30], they still require costly function
evaluations in pixel space, which causes huge demands in
computation time and energy resources.
We propose to circumvent this drawback by introducing
an explicit separation of the compressive from the genera-
tive learning phase (see Fig. 2). To achieve this, we utilize
an autoencoding model which learns a space that is percep-
tually equivalent to the image space, but offers signiﬁcantly
reduced computational complexity.
Such an approach offers several advantages: (i) By leav-
ing the high-dimensional image space, we obtain DMs
which are computationally much more efﬁcient because
sampling is performed on a low-dimensional space. (ii) We
exploit the inductive bias of DMs inherited from their UNet
architecture [71], which makes them particularly effective
for data with spatial structure and therefore alleviates the
need for aggressive, quality-reducing compression levels as
required by previous approaches [23, 66]. (iii) Finally, we
obtain general-purpose compression models whose latent
space can be used to train multiple generative models and
which can also be utilized for other downstream applica-
tions such as single-image CLIP-guided synthesis [25].
3.1. Perceptual Image Compression
Our perceptual compression model is based on previous
work [23] and consists of an autoencoder trained by com-
bination of a perceptual loss [106] and a patch-based [33]
adversarial objective [20, 23, 103]. This ensures that the re-
constructions are conﬁned to the image manifold by enforc-
ing local realism and avoids bluriness introduced by relying
solely on pixel-space losses such as L2orL1objectives.
More precisely, given an image x2RHW3in RGB
space, the encoder Eencodesxinto a latent representa-
3tionz=E(x), and the decoder Dreconstructs the im-
age from the latent, giving ~x=D(z) =D(E(x)), where
z2Rhwc. Importantly, the encoder downsamples the
image by a factor f=H=h =W=w , and we investigate
different downsampling factors f= 2m, withm2N.
In order to avoid arbitrarily high-variance latent spaces,
we experiment with two different kinds of regularizations.
The ﬁrst variant, KL-reg. , imposes a slight KL-penalty to-
wards a standard normal on the learned latent, similar to a
V AE [46, 69], whereas VQ-reg. uses a vector quantization
layer [96] within the decoder. This model can be interpreted
as a VQGAN [23] but with the quantization layer absorbed
by the decoder. Because our subsequent DM is designed
to work with the two-dimensional structure of our learned
latent space z=E(x), we can use relatively mild compres-
sion rates and achieve very good reconstructions. This is
in contrast to previous works [23, 66], which relied on an
arbitrary 1D ordering of the learned space zto model its
distribution autoregressively and thereby ignored much of
the inherent structure of z. Hence, our compression model
preserves details of xbetter (see Tab. 8). The full objective
and training details can be found in the supplement.
3.2. Latent Diffusion Models
Diffusion Models [82] are probabilistic models designed to
learn a data distribution p(x)by gradually denoising a nor-
mally distributed variable, which corresponds to learning
the reverse process of a ﬁxed Markov Chain of length T.
For image synthesis, the most successful models [15,30,72]
rely on a reweighted variant of the variational lower bound
onp(x), which mirrors denoising score-matching [85].
These models can be interpreted as an equally weighted
sequence of denoising autoencoders (xt;t);t= 1:::T ,
which are trained to predict a denoised variant of their input
xt, wherextis a noisy version of the input x. The corre-
sponding objective can be simpliﬁed to (Sec. B)
LDM=Ex;N(0;1);th
k (xt;t)k2
2i
; (1)
withtuniformly sampled from f1;:::;Tg.
Generative Modeling of Latent Representations With
our trained perceptual compression models consisting of E
andD, we now have access to an efﬁcient, low-dimensional
latent space in which high-frequency, imperceptible details
are abstracted away. Compared to the high-dimensional
pixel space, this space is more suitable for likelihood-based
generative models, as they can now (i) focus on the impor-
tant, semantic bits of the data and (ii) train in a lower di-
mensional, computationally much more efﬁcient space.
Unlike previous work that relied on autoregressive,
attention-based transformer models in a highly compressed,
discrete latent space [23,66,103], we can take advantage of
image-speciﬁc inductive biases that our model offers. This
Semantic  
 Map
crossattentionLatent SpaceConditioning  
TextDiffusion Process
denoising step switch skip connectionRepres  
entations
Pixel SpaceImagesDenoising U-Net
concatFigure 3. We condition LDMs either via concatenation or by a
more general cross-attention mechanism. See Sec. 3.3
includes the ability to build the underlying UNet primar-
ily from 2D convolutional layers, and further focusing the
objective on the perceptually most relevant bits using the
reweighted bound, which now reads
LLDM :=EE(x);N(0;1);th
k (zt;t)k2
2i
:(2)
The neural backbone (;t)of our model is realized as a
time-conditional UNet [71]. Since the forward process is
ﬁxed,ztcan be efﬁciently obtained from Eduring training,
and samples from p(z) can be decoded to image space with
a single pass through D.
3.3. Conditioning Mechanisms
Similar to other types of generative models [56, 83],
diffusion models are in principle capable of modeling
conditional distributions of the form p(zjy). This can
be implemented with a conditional denoising autoencoder
(zt;t;y)and paves the way to controlling the synthesis
process through inputs ysuch as text [68], semantic maps
[33, 61] or other image-to-image translation tasks [34].
In the context of image synthesis, however, combining
the generative power of DMs with other types of condition-
ings beyond class-labels [15] or blurred variants of the input
image [72] is so far an under-explored area of research.
We turn DMs into more ﬂexible conditional image gener-
ators by augmenting their underlying UNet backbone with
the cross-attention mechanism [97], which is effective for
learning attention-based models of various input modali-
ties [35,36]. To pre-process yfrom various modalities (such
as language prompts) we introduce a domain speciﬁc en-
coderthat projects yto an intermediate representation
(y)2RMd, which is then mapped to the intermediate
layers of the UNet via a cross-attention layer implementing
Attention (Q;K;V ) =softmax
QKT
p
d
V, with
Q=W(i)
Q'i(zt); K=W(i)
K(y); V=W(i)
V(y):
Here,'i(zt)2RNdi
denotes a (ﬂattened) intermediate
representation of the UNet implementing andW(i)
V2
4CelebAHQ FFHQ LSUN-Churches LSUN-Beds ImageNet
Figure 4. Samples from LDMs trained on CelebAHQ [39], FFHQ [41], LSUN-Churches [102], LSUN-Bedrooms [102] and class-
conditional ImageNet [12], each with a resolution of 256256. Best viewed when zoomed in. For more samples cf. the supplement.
Rddi
,W(i)
Q2Rdd&W(i)
K2Rddare learnable pro-
jection matrices [36, 97]. See Fig. 3 for a visual depiction.
Based on image-conditioning pairs, we then learn the
conditional LDM via
LLDM :=EE(x);y;N(0;1);th
k (zt;t;(y))k2
2i
;(3)
where bothandare jointly optimized via Eq. 3. This
conditioning mechanism is ﬂexible as can be parameter-
ized with domain-speciﬁc experts, e.g. (unmasked) trans-
formers [97] when yare text prompts (see Sec. 4.3.1)
4. Experiments
LDMs provide means to ﬂexible and computationally
tractable diffusion based image synthesis of various image
modalities, which we empirically show in the following.
Firstly, however, we analyze the gains of our models com-
pared to pixel-based diffusion models in both training and
inference. Interestingly, we ﬁnd that LDMs trained in VQ-
regularized latent spaces sometimes achieve better sample
quality, even though the reconstruction capabilities of VQ-
regularized ﬁrst stage models slightly fall behind those of
their continuous counterparts, cf. Tab. 8. A visual compari-
son between the effects of ﬁrst stage regularization schemes
onLDM training and their generalization abilities to resolu-
tions>2562can be found in Appendix D.1. In E.2 we list
details on architecture, implementation, training and evalu-
ation for all results presented in this section.
4.1. On Perceptual Compression Tradeoffs
This section analyzes the behavior of our LDMs with dif-
ferent downsampling factors f2f1;2;4;8;16;32g(abbre-
viated as LDM-f, where LDM-1 corresponds to pixel-based
DMs). To obtain a comparable test-ﬁeld, we ﬁx the com-
putational resources to a single NVIDIA A100 for all ex-
periments in this section and train all models for the same
number of steps and with the same number of parameters.
Tab. 8 shows hyperparameters and reconstruction perfor-
mance of the ﬁrst stage models used for the LDMs com-pared in this section. Fig. 6 shows sample quality as a func-
tion of training progress for 2M steps of class-conditional
models on the ImageNet [12] dataset. We see that, i) small
downsampling factors for LDM-f1,2gresult in slow train-
ing progress, whereas ii) overly large values of fcause stag-
nating ﬁdelity after comparably few training steps. Revis-
iting the analysis above (Fig. 1 and 2) we attribute this to
i) leaving most of perceptual compression to the diffusion
model and ii) too strong ﬁrst stage compression resulting
in information loss and thus limiting the achievable qual-
ity.LDM-f4-16gstrike a good balance between efﬁciency
and perceptually faithful results, which manifests in a sig-
niﬁcant FID [29] gap of 38 between pixel-based diffusion
(LDM-1 ) and LDM-8 after 2M training steps.
In Fig. 7, we compare models trained on CelebA-
HQ [39] and ImageNet in terms sampling speed for differ-
ent numbers of denoising steps with the DDIM sampler [84]
and plot it against FID-scores [29]. LDM-f4-8goutper-
form models with unsuitable ratios of perceptual and con-
ceptual compression. Especially compared to pixel-based
LDM-1 , they achieve much lower FID scores while simulta-
neously signiﬁcantly increasing sample throughput. Com-
plex datasets such as ImageNet require reduced compres-
sion rates to avoid reducing quality. In summary, LDM-4
and-8offer the best conditions for achieving high-quality
synthesis results.
4.2. Image Generation with Latent Diffusion
We train unconditional models of 2562images on
CelebA-HQ [39], FFHQ [41], LSUN-Churches and
-Bedrooms [102] and evaluate the i) sample quality and ii)
their coverage of the data manifold using ii) FID [29] and
ii) Precision-and-Recall [50]. Tab. 1 summarizes our re-
sults. On CelebA-HQ, we report a new state-of-the-art FID
of5:11, outperforming previous likelihood-based models as
well as GANs. We also outperform LSGM [93] where a la-
tent diffusion model is trained jointly together with the ﬁrst
stage. In contrast, we train diffusion models in a ﬁxed space
5Text-to-Image Synthesis on LAION. 1.45B Model.
’A street sign that reads
“Latent Diffusion” ’’A zombie in the
style of Picasso’’An image of an animal
half mouse half octopus’’An illustration of a slightly
conscious neural network’’A painting of a
squirrel eating a burger’’A watercolor painting of a
chair that looks like an octopus’’A shirt with the inscription:
“I love generative models!” ’
Figure 5. Samples for user-deﬁned text prompts from our model for text-to-image synthesis, LDM-8 (KL) , which was trained on the
LAION [78] database. Samples generated with 200 DDIM steps and = 1:0. We use unconditional guidance [32] with s= 10:0.
Figure 6. Analyzing the training of class-conditional LDMs with
different downsampling factors fover 2M train steps on the Im-
ageNet dataset. Pixel-based LDM-1 requires substantially larger
train times compared to models with larger downsampling factors
(LDM-f4-16g). Too much perceptual compression as in LDM-32
limits the overall sample quality. All models are trained on a sin-
gle NVIDIA A100 with the same computational budget. Results
obtained with 100 DDIM steps [84] and = 0.
Figure 7. Comparing LDMs with varying compression on the
CelebA-HQ (left) and ImageNet (right) datasets. Different mark-
ers indicatef10;20;50;100;200gsampling steps using DDIM,
from right to left along each line. The dashed line shows the FID
scores for 200 steps, indicating the strong performance of LDM-
f4-8g. FID scores assessed on 5000 samples. All models were
trained for 500k (CelebA) / 2M (ImageNet) steps on an A100.
and avoid the difﬁculty of weighing reconstruction quality
against learning the prior over the latent space, see Fig. 1-2.
We outperform prior diffusion based approaches on all
but the LSUN-Bedrooms dataset, where our score is close
to ADM [15], despite utilizing half its parameters and re-
quiring 4-times less train resources (see Appendix E.3.5).CelebA-HQ 256256 FFHQ 256256
Method FID# Prec." Recall" Method FID# Prec." Recall"
DC-V AE [63] 15.8 - - ImageBART [21] 9.57 - -
VQGAN+T. [23] (k=400) 10.2 - - U-Net GAN (+aug) [77] 10.9 (7.6) - -
PGGAN [39] 8.0 - - UDM [43] 5.54 - -
LSGM [93] 7.22 - - StyleGAN [41] 4.16 0.71 0.46
UDM [43] 7.16 - - ProjectedGAN [76] 3.08 0.65 0.46
LDM-4 (ours, 500-sy) 5.11 0.72 0.49 LDM-4 (ours, 200-s) 4.98 0.73 0.50
LSUN-Churches 256256 LSUN-Bedrooms 256256
Method FID# Prec." Recall" Method FID# Prec." Recall"
DDPM [30] 7.89 - - ImageBART [21] 5.51 - -
ImageBART [21] 7.32 - - DDPM [30] 4.9 - -
PGGAN [39] 6.42 - - UDM [43] 4.57 - -
StyleGAN [41] 4.21 - - StyleGAN [41] 2.35 0.59 0.48
StyleGAN2 [42] 3.86 - - ADM [15] 1.90 0.66 0.51
ProjectedGAN [76] 1.59 0.61 0.44 ProjectedGAN [76] 1.52 0.61 0.34
LDM-8(ours, 200-s) 4.02 0.64 0.52 LDM-4 (ours, 200-s) 2.95 0.66 0.48
Table 1. Evaluation metrics for unconditional image synthesis.
CelebA-HQ results reproduced from [43, 63, 100], FFHQ from
[42, 43].y:N-s refers toNsampling steps with the DDIM [84]
sampler.: trained in KL-regularized latent space. Additional re-
sults can be found in the supplementary.
Text-Conditional Image Synthesis
Method FID# IS" Nparams
CogViewy[17] 27.10 18.20 4B self-ranking, rejection rate 0.017
LAFITEy[109] 26.94 26.02 75M
GLIDE[59] 12.24 - 6B 277 DDIM steps, c.f.g. [32] s= 3
Make-A-Scene[26] 11.84 - 4B c.f.g for AR models [98] s= 5
LDM-KL-8 23.31 20.03 0.33 1.45B 250 DDIM steps
LDM-KL-8-G12.63 30.29 0.42 1.45B 250 DDIM steps, c.f.g. [32] s= 1:5
Table 2. Evaluation of text-conditional image synthesis on the
256256-sized MS-COCO [51] dataset: with 250 DDIM [84]
steps our model is on par with the most recent diffusion [59] and
autoregressive [26] methods despite using signiﬁcantly less pa-
rameters.y/:Numbers from [109]/ [26]
Moreover, LDMs consistently improve upon GAN-based
methods in Precision and Recall, thus conﬁrming the ad-
vantages of their mode-covering likelihood-based training
objective over adversarial approaches. In Fig. 4 we also
show qualitative results on each dataset.
6Figure 8. Layout-to-image synthesis with an LDM on COCO [4],
see Sec. 4.3.1. Quantitative evaluation in the supplement D.3.
4.3. Conditional Latent Diffusion
4.3.1 Transformer Encoders for LDMs
By introducing cross-attention based conditioning into
LDMs we open them up for various conditioning modali-
ties previously unexplored for diffusion models. For text-
to-image image modeling, we train a 1.45B parameter
KL-regularized LDM conditioned on language prompts on
LAION-400M [78]. We employ the BERT-tokenizer [14]
and implement as a transformer [97] to infer a latent
code which is mapped into the UNet via (multi-head) cross-
attention (Sec. 3.3). This combination of domain speciﬁc
experts for learning a language representation and visual
synthesis results in a powerful model, which generalizes
well to complex, user-deﬁned text prompts, cf. Fig. 8 and 5.
For quantitative analysis, we follow prior work and evaluate
text-to-image generation on the MS-COCO [51] validation
set, where our model improves upon powerful AR [17, 66]
and GAN-based [109] methods, cf. Tab. 2. We note that ap-
plying classiﬁer-free diffusion guidance [32] greatly boosts
sample quality, such that the guided LDM-KL-8-G is on par
with the recent state-of-the-art AR [26] and diffusion mod-
els [59] for text-to-image synthesis, while substantially re-
ducing parameter count. To further analyze the ﬂexibility of
the cross-attention based conditioning mechanism we also
train models to synthesize images based on semantic lay-
outs on OpenImages [49], and ﬁnetune on COCO [4], see
Fig. 8. See Sec. D.3 for the quantitative evaluation and im-
plementation details.
Lastly, following prior work [3, 15, 21, 23], we evalu-
ate our best-performing class-conditional ImageNet mod-
els withf2 f4;8gfrom Sec. 4.1 in Tab. 3, Fig. 4 and
Sec. D.4. Here we outperform the state of the art diffu-
sion model ADM [15] while signiﬁcantly reducing compu-
tational requirements and parameter count, cf. Tab 18.
4.3.2 Convolutional Sampling Beyond 2562
By concatenating spatially aligned conditioning informa-
tion to the input of ,LDMs can serve as efﬁcient general-Method FID# IS" Precision" Recall"Nparams
BigGan-deep [3] 6.95 203.6 2.6 0.87 0.28 340M -
ADM [15] 10.94 100.98 0.69 0.63 554M 250 DDIM steps
ADM-G [15] 4.59 186.7 0.82 0.52 608M 250 DDIM steps
LDM-4 (ours) 10.56 103.49 1.24 0.71 0.62 400M 250 DDIM steps
LDM-4 -G (ours) 3.60 247.67 5.59 0.87 0.48 400M 250 steps, c.f.g [32], s= 1:5
Table 3. Comparison of a class-conditional ImageNet LDM with
recent state-of-the-art methods for class-conditional image gener-
ation on ImageNet [12]. A more detailed comparison with addi-
tional baselines can be found in D.4, Tab. 10 and F. c.f.g. denotes
classiﬁer-free guidance with a scale sas proposed in [32].
purpose image-to-image translation models. We use this
to train models for semantic synthesis, super-resolution
(Sec. 4.4) and inpainting (Sec. 4.5). For semantic synthe-
sis, we use images of landscapes paired with semantic maps
[23, 61] and concatenate downsampled versions of the se-
mantic maps with the latent image representation of a f= 4
model (VQ-reg., see Tab. 8). We train on an input resolution
of2562(crops from 3842) but ﬁnd that our model general-
izes to larger resolutions and can generate images up to the
megapixel regime when evaluated in a convolutional man-
ner (see Fig. 9). We exploit this behavior to also apply the
super-resolution models in Sec. 4.4 and the inpainting mod-
els in Sec. 4.5 to generate large images between 5122and
10242. For this application, the signal-to-noise ratio (in-
duced by the scale of the latent space) signiﬁcantly affects
the results. In Sec. D.1 we illustrate this when learning an
LDM on (i) the latent space as provided by a f= 4 model
(KL-reg., see Tab. 8), and (ii) a rescaled version, scaled by
the component-wise standard deviation.
The latter, in combination with classiﬁer-free guid-
ance [32], also enables the direct synthesis of >2562im-
ages for the text-conditional LDM-KL-8-G as in Fig. 13.
Figure 9. A LDM trained on 2562resolution can generalize to
larger resolution (here: 5121024 ) for spatially conditioned tasks
such as semantic synthesis of landscape images. See Sec. 4.3.2.
4.4. Super-Resolution with Latent Diffusion
LDMs can be efﬁciently trained for super-resolution by
diretly conditioning on low-resolution images via concate-
nation ( cf. Sec. 3.3). In a ﬁrst experiment, we follow SR3
7bicubic LDM -SR SR3
Figure 10. ImageNet 64 !256 super-resolution on ImageNet-Val.
LDM-SR has advantages at rendering realistic textures but SR3
can synthesize more coherent ﬁne structures. See appendix for
additional samples and cropouts. SR3 results from [72].
[72] and ﬁx the image degradation to a bicubic interpola-
tion with 4-downsampling and train on ImageNet follow-
ing SR3’s data processing pipeline. We use the f= 4 au-
toencoding model pretrained on OpenImages (VQ-reg., cf.
Tab. 8) and concatenate the low-resolution conditioning y
and the inputs to the UNet, i.e.is the identity. Our quali-
tative and quantitative results (see Fig. 10 and Tab. 5) show
competitive performance and LDM-SR outperforms SR3
in FID while SR3 has a better IS. A simple image regres-
sion model achieves the highest PSNR and SSIM scores;
however these metrics do not align well with human per-
ception [106] and favor blurriness over imperfectly aligned
high frequency details [72]. Further, we conduct a user
study comparing the pixel-baseline with LDM-SR. We fol-
low SR3 [72] where human subjects were shown a low-res
image in between two high-res images and asked for pref-
erence. The results in Tab. 4 afﬁrm the good performance
of LDM-SR. PSNR and SSIM can be pushed by using a
post-hoc guiding mechanism [15] and we implement this
image-based guider via a perceptual loss, see Sec. D.6.
SR on ImageNet Inpainting on Places
User Study Pixel-DM ( f1)LDM-4 LAMA [88] LDM-4
Task 1: Preference vs GT" 16.0% 30.4% 13.6% 21.0%
Task 2: Preference Score" 29.4% 70.6% 31.9% 68.1%
Table 4. Task 1: Subjects were shown ground truth and generated
image and asked for preference. Task 2: Subjects had to decide
between two generated images. More details in E.3.6
Since the bicubic degradation process does not generalize
well to images which do not follow this pre-processing, we
also train a generic model, LDM-BSR , by using more di-
verse degradation. The results are shown in Sec. D.6.1.Method FID# IS" PSNR" SSIM"Nparams [samples
s]()
Image Regression [72] 15.2 121.1 27.9 0.801 625M N/A
SR3 [72] 5.2 180.1 26.4 0.762 625M N/A
LDM-4 (ours, 100 steps) 2.8y/4.8z166.3 24.4 3.8 0.690.14 169M 4.62
emphLDM-4 (ours, big, 100 steps) 2.4y/4.3z174.9 24.74.1 0.710.15 552M 4.5
LDM-4 (ours, 50 steps, guiding) 4.4y/6.4z153.7 25.8 3.7 0.740.12 184M 0.38
Table 5.4upscaling results on ImageNet-Val. ( 2562);y: FID
features computed on validation split,z: FID features computed
on train split;: Assessed on a NVIDIA A100
train throughput sampling throughputytrain+val FID@2k
Model (reg.-type) samples/sec. @256 @512 hours/epoch epoch 6
LDM-1 (no ﬁrst stage) 0.11 0.26 0.07 20.66 24.74
LDM-4 (KL, w/ attn) 0.32 0.97 0.34 7.66 15.21
LDM-4 (VQ, w/ attn) 0.33 0.97 0.34 7.04 14.99
LDM-4 (VQ, w/o attn) 0.35 0.99 0.36 6.66 15.95
Table 6. Assessing inpainting efﬁciency.y: Deviations from Fig. 7
due to varying GPU settings/batch sizes cf. the supplement.
4.5. Inpainting with Latent Diffusion
Inpainting is the task of ﬁlling masked regions of an im-
age with new content either because parts of the image are
are corrupted or to replace existing but undesired content
within the image. We evaluate how our general approach
for conditional image generation compares to more special-
ized, state-of-the-art approaches for this task. Our evalua-
tion follows the protocol of LaMa [88], a recent inpainting
model that introduces a specialized architecture relying on
Fast Fourier Convolutions [8]. The exact training & evalua-
tion protocol on Places [108] is described in Sec. E.2.2.
We ﬁrst analyze the effect of different design choices for
the ﬁrst stage. In particular, we compare the inpainting ef-
ﬁciency of LDM-1 (i.e. a pixel-based conditional DM) with
LDM-4 , for both KLandVQregularizations, as well as VQ-
LDM-4 without any attention in the ﬁrst stage (see Tab. 8),
where the latter reduces GPU memory for decoding at high
resolutions. For comparability, we ﬁx the number of param-
eters for all models. Tab. 6 reports the training and sampling
throughput at resolution 2562and5122, the total training
time in hours per epoch and the FID score on the validation
split after six epochs. Overall, we observe a speed-up of at
least2:7between pixel- and latent-based diffusion models
while improving FID scores by a factor of at least 1:6.
The comparison with other inpainting approaches in
Tab. 7 shows that our model with attention improves the
overall image quality as measured by FID over that of [88].
LPIPS between the unmasked images and our samples is
slightly higher than that of [88]. We attribute this to [88]
only producing a single result which tends to recover more
of an average image compared to the diverse results pro-
duced by our LDM cf. Fig. 21. Additionally in a user study
(Tab. 4) human subjects favor our results over those of [88].
Based on these initial results, we also trained a larger dif-
fusion model ( bigin Tab. 7) in the latent space of the VQ-
regularized ﬁrst stage without attention. Following [15],
the UNet of this diffusion model uses attention layers on
three levels of its feature hierarchy, the BigGAN [3] residual
block for up- and downsampling and has 387M parameters
8input result
Figure 11. Qualitative results on object removal with our big, w/
ftinpainting model. For more results, see Fig. 22.
instead of 215M. After training, we noticed a discrepancy
in the quality of samples produced at resolutions 2562and
5122, which we hypothesize to be caused by the additional
attention modules. However, ﬁne-tuning the model for half
an epoch at resolution 5122allows the model to adjust to
the new feature statistics and sets a new state of the art FID
on image inpainting ( big, w/o attn, w/ ft in Tab. 7, Fig. 11.).
5. Limitations & Societal Impact
Limitations While LDMs signiﬁcantly reduce computa-
tional requirements compared to pixel-based approaches,
their sequential sampling process is still slower than that
of GANs. Moreover, the use of LDMs can be question-
able when high precision is required: although the loss of
image quality is very small in our f= 4autoencoding mod-
els (see Fig. 1), their reconstruction capability can become
a bottleneck for tasks that require ﬁne-grained accuracy in
pixel space. We assume that our superresolution models
(Sec. 4.4) are already somewhat limited in this respect.
Societal Impact Generative models for media like im-
agery are a double-edged sword: On the one hand, they40-50% masked All samples
Method FID# LPIPS# FID# LPIPS#
LDM-4 (ours, big, w/ ft) 9.39 0.246 0.042 1.50 0.137 0.080
LDM-4 (ours, big, w/o ft) 12.89 0.257 0.047 2.40 0.142 0.085
LDM-4 (ours, w/ attn) 11.87 0.257 0.042 2.15 0.144 0.084
LDM-4 (ours, w/o attn) 12.60 0.259 0.041 2.37 0.145 0.084
LaMa [88]y12.31 0.243 0.038 2.23 0.134 0.080
LaMa [88] 12.0 0.24 2.21 0.14
CoModGAN [107] 10.4 0.26 1.82 0.15
RegionWise [52] 21.3 0.27 4.75 0.15
DeepFill v2 [104] 22.1 0.28 5.20 0.16
EdgeConnect [58] 30.5 0.28 8.37 0.16
Table 7. Comparison of inpainting performance on 30k crops of
size512512from test images of Places [108]. The column 40-
50% reports metrics computed over hard examples where 40-50%
of the image region have to be inpainted.yrecomputed on our test
set, since the original test set used in [88] was not available.
enable various creative applications, and in particular ap-
proaches like ours that reduce the cost of training and in-
ference have the potential to facilitate access to this tech-
nology and democratize its exploration. On the other hand,
it also means that it becomes easier to create and dissemi-
nate manipulated data or spread misinformation and spam.
In particular, the deliberate manipulation of images (“deep
fakes”) is a common problem in this context, and women in
particular are disproportionately affected by it [13, 24].
Generative models can also reveal their training data
[5, 90], which is of great concern when the data contain
sensitive or personal information and were collected with-
out explicit consent. However, the extent to which this also
applies to DMs of images is not yet fully understood.
Finally, deep learning modules tend to reproduce or ex-
acerbate biases that are already present in the data [22, 38,
91]. While diffusion models achieve better coverage of the
data distribution than e.g. GAN-based approaches, the ex-
tent to which our two-stage approach that combines adver-
sarial training and a likelihood-based objective misrepre-
sents the data remains an important research question.
For a more general, detailed discussion of the ethical
considerations of deep generative models, see e.g. [13].
6. Conclusion
We have presented latent diffusion models, a simple and
efﬁcient way to signiﬁcantly improve both the training and
sampling efﬁciency of denoising diffusion models with-
out degrading their quality. Based on this and our cross-
attention conditioning mechanism, our experiments could
demonstrate favorable results compared to state-of-the-art
methods across a wide range of conditional image synthesis
tasks without task-speciﬁc architectures.
This work has been supported by the German Federal Ministry for
Economic Affairs and Energy within the project ’KI-Absicherung - Safe
AI for automated driving’ and by the German Research Foundation (DFG)
project 421703927.
9References
[1] Eirikur Agustsson and Radu Timofte. NTIRE 2017 chal-
lenge on single image super-resolution: Dataset and study.
In2017 IEEE Conference on Computer Vision and Pattern
Recognition Workshops, CVPR Workshops 2017, Honolulu,
HI, USA, July 21-26, 2017 , pages 1122–1131. IEEE Com-
puter Society, 2017. 1
[2] Martin Arjovsky, Soumith Chintala, and L ´eon Bottou.
Wasserstein gan, 2017. 3
[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale GAN training for high ﬁdelity natural image synthe-
sis. In Int. Conf. Learn. Represent. , 2019. 1, 2, 7, 8, 22,
28
[4] Holger Caesar, Jasper R. R. Uijlings, and Vittorio Ferrari.
Coco-stuff: Thing and stuff classes in context. In 2018
IEEE Conference on Computer Vision and Pattern Recog-
nition, CVPR 2018, Salt Lake City, UT, USA, June 18-
22, 2018 , pages 1209–1218. Computer Vision Foundation /
IEEE Computer Society, 2018. 7, 20, 22
[5] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew
Jagielski, Ariel Herbert-V oss, Katherine Lee, Adam
Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al.
Extracting training data from large language models. In
30th USENIX Security Symposium (USENIX Security 21) ,
pages 2633–2650, 2021. 9
[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-
woo Jun, David Luan, and Ilya Sutskever. Generative pre-
training from pixels. In ICML , volume 119 of Proceedings
of Machine Learning Research , pages 1691–1703. PMLR,
2020. 3
[7] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mo-
hammad Norouzi, and William Chan. Wavegrad: Estimat-
ing gradients for waveform generation. In ICLR . OpenRe-
view.net, 2021. 1
[8] Lu Chi, Borui Jiang, and Yadong Mu. Fast fourier convolu-
tion. In NeurIPS , 2020. 8
[9] Rewon Child. Very deep vaes generalize autoregressive
models and can outperform them on images. CoRR ,
abs/2011.10650, 2020. 3
[10] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
Generating long sequences with sparse transformers.
CoRR , abs/1904.10509, 2019. 3
[11] Bin Dai and David P. Wipf. Diagnosing and enhancing V AE
models. In ICLR (Poster) . OpenReview.net, 2019. 2, 3
[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Fei-Fei Li. Imagenet: A large-scale hierarchical im-
age database. In CVPR , pages 248–255. IEEE Computer
Society, 2009. 1, 5, 7, 22
[13] Emily Denton. Ethical considerations of generative ai. AI
for Content Creation Workshop, CVPR, 2021. 9
[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. BERT: pre-training of deep bidirec-
tional transformers for language understanding. CoRR ,
abs/1810.04805, 2018. 7
[15] Prafulla Dhariwal and Alex Nichol. Diffusion models beat
gans on image synthesis. CoRR , abs/2105.05233, 2021. 1,
2, 3, 4, 6, 7, 8, 18, 22, 25, 26, 28[16] Sander Dieleman. Musings on typicality, 2020. 1, 3
[17] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,
Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,
Hongxia Yang, and Jie Tang. Cogview: Mastering text-to-
image generation via transformers. CoRR , abs/2105.13290,
2021. 6, 7
[18] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice:
Non-linear independent components estimation, 2015. 3
[19] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Ben-
gio. Density estimation using real NVP. In 5th Inter-
national Conference on Learning Representations, ICLR
2017, Toulon, France, April 24-26, 2017, Conference Track
Proceedings . OpenReview.net, 2017. 1, 3
[20] Alexey Dosovitskiy and Thomas Brox. Generating images
with perceptual similarity metrics based on deep networks.
In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg,
Isabelle Guyon, and Roman Garnett, editors, Adv. Neural
Inform. Process. Syst. , pages 658–666, 2016. 3
[21] Patrick Esser, Robin Rombach, Andreas Blattmann, and
Bj¨orn Ommer. Imagebart: Bidirectional context with multi-
nomial diffusion for autoregressive image synthesis. CoRR ,
abs/2108.08827, 2021. 6, 7, 22
[22] Patrick Esser, Robin Rombach, and Bj ¨orn Ommer. A
note on data biases in generative models. arXiv preprint
arXiv:2012.02516 , 2020. 9
[23] Patrick Esser, Robin Rombach, and Bj ¨orn Ommer. Taming
transformers for high-resolution image synthesis. CoRR ,
abs/2012.09841, 2020. 2, 3, 4, 6, 7, 21, 22, 29, 34, 36
[24] Mary Anne Franks and Ari Ezra Waldman. Sex, lies, and
videotape: Deep fakes and free speech delusions. Md. L.
Rev., 78:892, 2018. 9
[25] Kevin Frans, Lisa B. Soros, and Olaf Witkowski. Clipdraw:
Exploring text-to-drawing synthesis through language-
image encoders. ArXiv , abs/2106.14843, 2021. 3
[26] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,
Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-
based text-to-image generation with human priors. CoRR ,
abs/2203.13131, 2022. 6, 7, 16
[27] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville,
and Yoshua Bengio. Generative adversarial networks.
CoRR , 2014. 1, 2
[28] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent
Dumoulin, and Aaron Courville. Improved training of
wasserstein gans, 2017. 3
[29] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by
a two time-scale update rule converge to a local nash equi-
librium. In Adv. Neural Inform. Process. Syst. , pages 6626–
6637, 2017. 1, 5, 26
[30] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. In NeurIPS , 2020. 1, 2, 3, 4,
6, 17
[31] Jonathan Ho, Chitwan Saharia, William Chan, David J.
Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded
diffusion models for high ﬁdelity image generation. CoRR ,
abs/2106.15282, 2021. 1, 3, 22
10[32] Jonathan Ho and Tim Salimans. Classiﬁer-free diffusion
guidance. In NeurIPS 2021 Workshop on Deep Generative
Models and Downstream Applications , 2021. 6, 7, 16, 22,
28, 37, 38
[33] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A.
Efros. Image-to-image translation with conditional adver-
sarial networks. In CVPR , pages 5967–5976. IEEE Com-
puter Society, 2017. 3, 4
[34] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A.
Efros. Image-to-image translation with conditional adver-
sarial networks. 2017 IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , pages 5967–5976,
2017. 4
[35] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste
Alayrac, Carl Doersch, Catalin Ionescu, David Ding,
Skanda Koppula, Daniel Zoran, Andrew Brock, Evan
Shelhamer, Olivier J. H ´enaff, Matthew M. Botvinick,
Andrew Zisserman, Oriol Vinyals, and Jo ˜ao Carreira.
Perceiver IO: A general architecture for structured inputs
&outputs. CoRR , abs/2107.14795, 2021. 4
[36] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,
Andrew Zisserman, and Jo ˜ao Carreira. Perceiver: General
perception with iterative attention. In Marina Meila and
Tong Zhang, editors, Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-24 July
2021, Virtual Event , volume 139 of Proceedings of Machine
Learning Research , pages 4651–4664. PMLR, 2021. 4, 5
[37] Manuel Jahn, Robin Rombach, and Bj ¨orn Ommer. High-
resolution complex scene synthesis with transformers.
CoRR , abs/2105.06458, 2021. 20, 22, 27
[38] Niharika Jain, Alberto Olmo, Sailik Sengupta, Lydia
Manikonda, and Subbarao Kambhampati. Imperfect ima-
ganation: Implications of gans exacerbating biases on fa-
cial data augmentation and snapchat selﬁe lenses. arXiv
preprint arXiv:2001.09528 , 2020. 9
[39] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehti-
nen. Progressive growing of gans for improved quality, sta-
bility, and variation. CoRR , abs/1710.10196, 2017. 5, 6
[40] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
InIEEE Conf. Comput. Vis. Pattern Recog. , pages 4401–
4410, 2019. 1
[41] T. Karras, S. Laine, and T. Aila. A style-based gener-
ator architecture for generative adversarial networks. In
2019 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , 2019. 5, 6
[42] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improv-
ing the image quality of stylegan. CoRR , abs/1912.04958,
2019. 2, 6, 28
[43] Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo
Kang, and Il-Chul Moon. Score matching model for un-
bounded data score. CoRR , abs/2106.05527, 2021. 6
[44] Durk P Kingma and Prafulla Dhariwal. Glow: Generative
ﬂow with invertible 1x1 convolutions. In S. Bengio, H. Wal-
lach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R.
Garnett, editors, Advances in Neural Information Process-
ing Systems , 2018. 3[45] Diederik P. Kingma, Tim Salimans, Ben Poole, and
Jonathan Ho. Variational diffusion models. CoRR ,
abs/2107.00630, 2021. 1, 3, 16
[46] Diederik P. Kingma and Max Welling. Auto-Encoding Vari-
ational Bayes. In 2nd International Conference on Learn-
ing Representations, ICLR , 2014. 1, 3, 4, 29
[47] Zhifeng Kong and Wei Ping. On fast sampling of diffusion
probabilistic models. CoRR , abs/2106.00132, 2021. 3
[48] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and
Bryan Catanzaro. Diffwave: A versatile diffusion model
for audio synthesis. In ICLR . OpenReview.net, 2021. 1
[49] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper R. R.
Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali,
Stefan Popov, Matteo Malloci, Tom Duerig, and Vittorio
Ferrari. The open images dataset V4: uniﬁed image classi-
ﬁcation, object detection, and visual relationship detection
at scale. CoRR , abs/1811.00982, 2018. 7, 20, 22
[50] Tuomas Kynk ¨a¨anniemi, Tero Karras, Samuli Laine, Jaakko
Lehtinen, and Timo Aila. Improved precision and re-
call metric for assessing generative models. CoRR ,
abs/1904.06991, 2019. 5, 26
[51] Tsung-Yi Lin, Michael Maire, Serge J. Belongie,
Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro
Perona, Deva Ramanan, Piotr Doll ´ar, and C. Lawrence Zit-
nick. Microsoft COCO: common objects in context. CoRR ,
abs/1405.0312, 2014. 6, 7, 27
[52] Yuqing Ma, Xianglong Liu, Shihao Bai, Le-Yi Wang, Ais-
han Liu, Dacheng Tao, and Edwin Hancock. Region-wise
generative adversarial imageinpainting for large missing ar-
eas. ArXiv , abs/1909.12507, 2019. 9
[53] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-
Yan Zhu, and Stefano Ermon. Sdedit: Image synthesis
and editing with stochastic differential equations. CoRR ,
abs/2108.01073, 2021. 1
[54] Lars M. Mescheder. On the convergence properties of GAN
training. CoRR , abs/1801.04406, 2018. 3
[55] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-
Dickstein. Unrolled generative adversarial networks. In
5th International Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Conference
Track Proceedings . OpenReview.net, 2017. 3
[56] Mehdi Mirza and Simon Osindero. Conditional generative
adversarial nets. CoRR , abs/1411.1784, 2014. 4
[57] Gautam Mittal, Jesse H. Engel, Curtis Hawthorne, and Ian
Simon. Symbolic music generation with diffusion models.
CoRR , abs/2103.16091, 2021. 1
[58] Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z. Qureshi,
and Mehran Ebrahimi. Edgeconnect: Generative im-
age inpainting with adversarial edge learning. ArXiv ,
abs/1901.00212, 2019. 9
[59] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. GLIDE: towards photorealistic image genera-
tion and editing with text-guided diffusion models. CoRR ,
abs/2112.10741, 2021. 6, 7, 16
[60] Anton Obukhov, Maximilian Seitzer, Po-Wei Wu, Se-
men Zhydenko, Jonathan Kyl, and Elvis Yu-Jing Lin.
11High-ﬁdelity performance metrics for generative models
in pytorch, 2020. Version: 0.3.0, DOI: 10.5281/zen-
odo.4957738. 26, 27
[61] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-
Yan Zhu. Semantic image synthesis with spatially-adaptive
normalization. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , 2019. 4, 7
[62] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-
Yan Zhu. Semantic image synthesis with spatially-adaptive
normalization. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
June 2019. 22
[63] Gaurav Parmar, Dacheng Li, Kwonjoon Lee, and Zhuowen
Tu. Dual contradistinctive generative autoencoder. In IEEE
Conference on Computer Vision and Pattern Recognition,
CVPR 2021, virtual, June 19-25, 2021 , pages 823–832.
Computer Vision Foundation / IEEE, 2021. 6
[64] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On
buggy resizing libraries and surprising subtleties in ﬁd cal-
culation. arXiv preprint arXiv:2104.11222 , 2021. 26
[65] David A. Patterson, Joseph Gonzalez, Quoc V . Le, Chen
Liang, Lluis-Miquel Munguia, Daniel Rothchild, David R.
So, Maud Texier, and Jeff Dean. Carbon emissions and
large neural network training. CoRR , abs/2104.10350,
2021. 2
[66] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott
Gray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya
Sutskever. Zero-shot text-to-image generation. CoRR ,
abs/2102.12092, 2021. 1, 2, 3, 4, 7, 21, 27
[67] Ali Razavi, A ¨aron van den Oord, and Oriol Vinyals. Gen-
erating diverse high-ﬁdelity images with VQ-V AE-2. In
NeurIPS , pages 14837–14847, 2019. 1, 2, 3, 22
[68] Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-
geswaran, Bernt Schiele, and Honglak Lee. Generative ad-
versarial text to image synthesis. In ICML , 2016. 4
[69] Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Wierstra. Stochastic backpropagation and approximate in-
ference in deep generative models. In Proceedings of the
31st International Conference on International Conference
on Machine Learning, ICML , 2014. 1, 4, 29
[70] Robin Rombach, Patrick Esser, and Bj ¨orn Ommer.
Network-to-network translation with conditional invertible
neural networks. In NeurIPS , 2020. 3
[71] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In MICCAI (3) , volume 9351 of Lecture Notes in
Computer Science , pages 234–241. Springer, 2015. 2, 3, 4
[72] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sal-
imans, David J. Fleet, and Mohammad Norouzi. Im-
age super-resolution via iterative reﬁnement. CoRR ,
abs/2104.07636, 2021. 1, 4, 8, 16, 22, 23, 27
[73] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P.
Kingma. Pixelcnn++: Improving the pixelcnn with dis-
cretized logistic mixture likelihood and other modiﬁcations.
CoRR , abs/1701.05517, 2017. 1, 3
[74] Dave Salvator. NVIDIA Developer Blog. https:
/ / developer . nvidia . com / blog / getting -immediate-speedups-with-a100-tf32 , 2020.
28
[75] Robin San-Roman, Eliya Nachmani, and Lior Wolf.
Noise estimation for generative diffusion models. CoRR ,
abs/2104.02600, 2021. 3
[76] Axel Sauer, Kashyap Chitta, Jens M ¨uller, and An-
dreas Geiger. Projected gans converge faster. CoRR ,
abs/2111.01007, 2021. 6
[77] Edgar Sch ¨onfeld, Bernt Schiele, and Anna Khoreva. A u-
net based discriminator for generative adversarial networks.
In2020 IEEE/CVF Conference on Computer Vision and
Pattern Recognition, CVPR 2020, Seattle, WA, USA, June
13-19, 2020 , pages 8204–8213. Computer Vision Founda-
tion / IEEE, 2020. 6
[78] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-
400m: Open dataset of clip-ﬁltered 400 million image-text
pairs, 2021. 6, 7
[79] Karen Simonyan and Andrew Zisserman. Very deep con-
volutional networks for large-scale image recognition. In
Yoshua Bengio and Yann LeCun, editors, Int. Conf. Learn.
Represent. , 2015. 29, 43, 44, 45
[80] Abhishek Sinha, Jiaming Song, Chenlin Meng, and Stefano
Ermon. D2C: diffusion-denoising models for few-shot con-
ditional generation. CoRR , abs/2106.06819, 2021. 3
[81] Charlie Snell. Alien Dreams: An Emerging Art Scene.
https : / / ml . berkeley . edu / blog / posts /
clip-art/ , 2021. [Online; accessed November-2021].
2
[82] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Mah-
eswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. CoRR ,
abs/1503.03585, 2015. 1, 3, 4, 18
[83] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learn-
ing structured output representation using deep conditional
generative models. In C. Cortes, N. Lawrence, D. Lee,
M. Sugiyama, and R. Garnett, editors, Advances in Neural
Information Processing Systems , volume 28. Curran Asso-
ciates, Inc., 2015. 4
[84] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In ICLR . OpenReview.net,
2021. 3, 5, 6, 22
[85] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma,
Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-
based generative modeling through stochastic differential
equations. CoRR , abs/2011.13456, 2020. 1, 3, 4, 18
[86] Emma Strubell, Ananya Ganesh, and Andrew McCallum.
Energy and policy considerations for modern deep learn-
ing research. In The Thirty-Fourth AAAI Conference on
Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second In-
novative Applications of Artiﬁcial Intelligence Conference,
IAAI 2020, The Tenth AAAI Symposium on Educational
Advances in Artiﬁcial Intelligence, EAAI 2020, New York,
NY, USA, February 7-12, 2020 , pages 13693–13696. AAAI
Press, 2020. 2
12[87] Wei Sun and Tianfu Wu. Learning layout and style re-
conﬁgurable gans for controllable image synthesis. CoRR ,
abs/2003.11571, 2020. 22, 27
[88] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,
Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,
Naejin Kong, Harshith Goka, Kiwoong Park, and Victor S.
Lempitsky. Resolution-robust large mask inpainting with
fourier convolutions. ArXiv , abs/2109.07161, 2021. 8, 9,
26, 32
[89] Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R. De-
von Hjelm, and Shikhar Sharma. Object-centric image gen-
eration from layouts. In Thirty-Fifth AAAI Conference on
Artiﬁcial Intelligence, AAAI 2021, Thirty-Third Conference
on Innovative Applications of Artiﬁcial Intelligence, IAAI
2021, The Eleventh Symposium on Educational Advances
in Artiﬁcial Intelligence, EAAI 2021, Virtual Event, Febru-
ary 2-9, 2021 , pages 2647–2655. AAAI Press, 2021. 20,
22, 27
[90] Patrick Tinsley, Adam Czajka, and Patrick Flynn. This face
does not exist... but it might be yours! identity leakage in
generative models. In Proceedings of the IEEE/CVF Win-
ter Conference on Applications of Computer Vision , pages
1320–1328, 2021. 9
[91] Antonio Torralba and Alexei A Efros. Unbiased look at
dataset bias. In CVPR 2011 , pages 1521–1528. IEEE, 2011.
9
[92] Arash Vahdat and Jan Kautz. NV AE: A deep hierarchical
variational autoencoder. In NeurIPS , 2020. 3
[93] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-
based generative modeling in latent space. CoRR ,
abs/2106.05931, 2021. 2, 3, 5, 6
[94] Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt,
koray kavukcuoglu, Oriol Vinyals, and Alex Graves. Con-
ditional image generation with pixelcnn decoders. In Ad-
vances in Neural Information Processing Systems , 2016. 3
[95] A ¨aron van den Oord, Nal Kalchbrenner, and Koray
Kavukcuoglu. Pixel recurrent neural networks. CoRR ,
abs/1601.06759, 2016. 3
[96] A ¨aron van den Oord, Oriol Vinyals, and Koray
Kavukcuoglu. Neural discrete representation learning. In
NIPS , pages 6306–6315, 2017. 2, 4, 29
[97] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In NIPS ,
pages 5998–6008, 2017. 3, 4, 5, 7
[98] Rivers Have Wings. Tweet on Classiﬁer-free
guidance for autoregressive models. https :
/ / twitter . com / RiversHaveWings / status /
1478093658716966912 , 2022. 6
[99] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-
mond, Clement Delangue, Anthony Moi, Pierric Cistac,
Tim Rault, R ´emi Louf, Morgan Funtowicz, and Jamie
Brew. Huggingface’s transformers: State-of-the-art natural
language processing. CoRR , abs/1910.03771, 2019. 26
[100] Zhisheng Xiao, Karsten Kreis, Jan Kautz, and Arash Vah-
dat. V AEBM: A symbiosis between variational autoen-
coders and energy-based models. In 9th International Con-ference on Learning Representations, ICLR 2021, Virtual
Event, Austria, May 3-7, 2021 . OpenReview.net, 2021. 6
[101] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind
Srinivas. Videogpt: Video generation using VQ-V AE and
transformers. CoRR , abs/2104.10157, 2021. 3
[102] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianx-
iong Xiao. LSUN: construction of a large-scale image
dataset using deep learning with humans in the loop. CoRR ,
abs/1506.03365, 2015. 5
[103] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang,
James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge,
and Yonghui Wu. Vector-quantized image modeling with
improved vqgan, 2021. 3, 4
[104] Jiahui Yu, Zhe L. Lin, Jimei Yang, Xiaohui Shen, Xin Lu,
and Thomas S. Huang. Free-form image inpainting with
gated convolution. 2019 IEEE/CVF International Confer-
ence on Computer Vision (ICCV) , pages 4470–4479, 2019.
9
[105] K. Zhang, Jingyun Liang, Luc Van Gool, and Radu Timo-
fte. Designing a practical degradation model for deep blind
image super-resolution. ArXiv , abs/2103.14006, 2021. 23
[106] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR) , June 2018. 3, 8, 19
[107] Shengyu Zhao, Jianwei Cui, Yilun Sheng, Yue Dong, Xiao
Liang, Eric I-Chao Chang, and Yan Xu. Large scale image
completion via co-modulated generative adversarial net-
works. ArXiv , abs/2103.10428, 2021. 9
[108] Bolei Zhou, `Agata Lapedriza, Aditya Khosla, Aude Oliva,
and Antonio Torralba. Places: A 10 million image database
for scene recognition. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence , 40:1452–1464, 2018. 8, 9,
26
[109] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li,
Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and
Tong Sun. LAFITE: towards language-free training for
text-to-image generation. CoRR , abs/2111.13792, 2021. 6,
7, 16
13Appendix
Figure 12. Convolutional samples from the semantic landscapes model as in Sec. 4.3.2, ﬁnetuned on 5122images.
14’A painting of the last supper by Picasso. ’
’An oil painting of a latent space. ’’An epic painting of Gandalf the Black
summoning thunder and lightning in the mountains. ’
’A sunset over a mountain range, vector image. ’
Figure 13. Combining classiﬁer free diffusion guidance with the convolutional sampling strategy from Sec. 4.3.2, our 1.45B parameter
text-to-image model can be used for rendering images larger than the native 2562resolution the model was trained on.
15A. Changelog
Here we list changes between this version ( https://arxiv.org/abs/2112.10752v2 ) of the paper and the
previous version, i.e.https://arxiv.org/abs/2112.10752v1 .
• We updated the results on text-to-image synthesis in Sec. 4.3 which were obtained by training a new, larger model (1.45B
parameters). This also includes a new comparison to very recent competing methods on this task that were published on
arXiv at the same time as ( [59, 109]) or after ( [26]) the publication of our work.
• We updated results on class-conditional synthesis on ImageNet in Sec. 4.1, Tab. 3 (see also Sec. D.4) obtained by
retraining the model with a larger batch size. The corresponding qualitative results in Fig. 26 and Fig. 27 were also
updated. Both the updated text-to-image and the class-conditional model now use classiﬁer-free guidance [32] as a
measure to increase visual ﬁdelity.
• We conducted a user study (following the scheme suggested by Saharia et al [72]) which provides additional evaluation
for our inpainting (Sec. 4.5) and superresolution models (Sec. 4.4).
• Added Fig. 5 to the main paper, moved Fig. 18 to the appendix, added Fig. 13 to the appendix.
B. Detailed Information on Denoising Diffusion Models
Diffusion models can be speciﬁed in terms of a signal-to-noise ratio SNR (t) =2
t
2
tconsisting of sequences (t)T
t=1and
(t)T
t=1which, starting from a data sample x0, deﬁne a forward diffusion process qas
q(xtjx0) =N(xtjtx0;2
tI) (4)
with the Markov structure for s<t :
q(xtjxs) =N(xtjtjsxs;2
tjsI) (5)
tjs=t
s(6)
2
tjs=2
t 2
tjs2
s (7)
Denoising diffusion models are generative models p(x0)which revert this process with a similar Markov structure running
backward in time, i.e. they are speciﬁed as
p(x0) =Z
zp(xT)TY
t=1p(xt 1jxt) (8)
The evidence lower bound (ELBO) associated with this model then decomposes over the discrete time steps as
 logp(x0)KL(q(xTjx0)jp(xT)) +TX
t=1Eq(xtjx0)KL(q(xt 1jxt;x0)jp(xt 1jxt)) (9)
The priorp(xT)is typically choosen as a standard normal distribution and the ﬁrst term of the ELBO then depends only on
the ﬁnal signal-to-noise ratio SNR (T). To minimize the remaining terms, a common choice to parameterize p(xt 1jxt)is to
specify it in terms of the true posterior q(xt 1jxt;x0)but with the unknown x0replaced by an estimate x(xt;t)based on
the current step xt. This gives [45]
p(xt 1jxt):=q(xt 1jxt;x(xt;t)) (10)
=N(xt 1j(xt;t);2
tjt 12
t 1
2
tI); (11)
where the mean can be expressed as
(xt;t) =tjt 12
t 1
2
txt+t 12
tjt 1
2
tx(xt;t): (12)
16In this case, the sum of the ELBO simplify to
TX
t=1Eq(xtjx0)KL(q(xt 1jxt;x0)jp(xt 1) =TX
t=1EN(j0;I)1
2(SNR(t 1) SNR(t))kx0 x(tx0+t;t)k2(13)
Following [30], we use the reparameterization
(xt;t) = (xt tx(xt;t))=t (14)
to express the reconstruction term as a denoising objective,
kx0 x(tx0+t;t)k2=2
t
2
tk (tx0+t;t)k2(15)
and the reweighting, which assigns each of the terms the same weight and results in Eq. (1).
17C. Image Guiding Mechanisms
Samples 2562Guided Convolutional Samples 5122Convolutional Samples 5122
Figure 14. On landscapes, convolutional sampling with unconditional models can lead to homogeneous and incoherent global structures
(see column 2). L2-guiding with a low resolution image can help to reestablish coherent global structures.
An intriguing feature of diffusion models is that unconditional models can be conditioned at test-time [15, 82, 85]. In
particular, [15] presented an algorithm to guide both unconditional and conditional models trained on the ImageNet dataset
with a classiﬁer logp(yjxt), trained on each xtof the diffusion process. We directly build on this formulation and introduce
post-hoc image-guiding :
For an epsilon-parameterized model with ﬁxed variance, the guiding algorithm as introduced in [15] reads:
^ (zt;t) +q
1 2
trztlogp(yjzt): (16)
This can be interpreted as an update correcting the “score” with a conditional distribution logp(yjzt).
So far, this scenario has only been applied to single-class classiﬁcation models. We re-interpret the guiding distribution
p(yjT(D(z0(zt)))) as a general purpose image-to-image translation task given a target image y, whereTcan be any
differentiable transformation adopted to the image-to-image translation task at hand, such as the identity, a downsampling
operation or similar.
18As an example, we can assume a Gaussian guider with ﬁxed variance 2= 1, such that
logp(yjzt) = 1
2ky T(D(z0(zt)))k2
2 (17)
becomes aL2regression objective.
Fig. 14 demonstrates how this formulation can serve as an upsampling mechanism of an unconditional model trained on
2562images, where unconditional samples of size 2562guide the convolutional synthesis of 5122images and Tis a2
bicubic downsampling. Following this motivation, we also experiment with a perceptual similarity guiding and replace the
L2objective with the LPIPS [106] metric, see Sec. 4.4.
19D. Additional Results
D.1. Choosing the Signal-to-Noise Ratio for High-Resolution Synthesis
KL-reg, w/o rescaling KL-reg, w/ rescaling VQ-reg, w/o rescaling
Figure 15. Illustrating the effect of latent space rescaling on convolutional sampling, here for semantic image synthesis on landscapes. See
Sec. 4.3.2 and Sec. D.1.
As discussed in Sec. 4.3.2, the signal-to-noise ratio induced by the variance of the latent space ( i.e. Var(z)=2
t) signiﬁcantly
affects the results for convolutional sampling. For example, when training a LDM directly in the latent space of a KL-
regularized model (see Tab. 8), this ratio is very high, such that the model allocates a lot of semantic detail early on in the
reverse denoising process. In contrast, when rescaling the latent space by the component-wise standard deviation of the
latents as described in Sec. G, the SNR is descreased. We illustrate the effect on convolutional sampling for semantic image
synthesis in Fig. 15. Note that the VQ-regularized space has a variance close to 1, such that it does not have to be rescaled.
D.2. Full List of all First Stage Models
We provide a complete list of various autoenconding models trained on the OpenImages dataset in Tab. 8.
D.3. Layout-to-Image Synthesis
Here we provide the quantitative evaluation and additional samples for our layout-to-image models from Sec. 4.3.1. We
train a model on the COCO [4] and one on the OpenImages [49] dataset, which we subsequently additionally ﬁnetune on
COCO. Tab 9 shows the result. Our COCO model reaches the performance of recent state-of-the art models in layout-to-
image synthesis, when following their training and evaluation protocol [89]. When ﬁnetuning from the OpenImages model,
we surpass these works. Our OpenImages model surpasses the results of Jahn et al [37] by a margin of nearly 11 in terms of
FID. In Fig. 16 we show additional samples of the model ﬁnetuned on COCO.
D.4. Class-Conditional Image Synthesis on ImageNet
Tab. 10 contains the results for our class-conditional LDM measured in FID and Inception score (IS). LDM-8 requires
signiﬁcantly fewer parameters and compute requirements (see Tab. 18) to achieve very competitive performance. Similar
to previous work, we can further boost the performance by training a classiﬁer on each noise scale and guiding with it,
20fjZj c R-FID# R-IS" PSNR" PSIM# SSIM"
16VQGAN [23] 16384 256 4.98 – 19.9 3:4 1.830:42 0.510:18
16VQGAN [23] 1024 256 7.94 – 19.4 3:3 1.980:43 0.500:18
8DALL-E [66] 8192 - 32.01 – 22.8 2:1 1.950:51 0.730:13
32 16384 16 31.83 40.40 1:07 17.45 2:90 2.580:48 0.410:18
16 16384 8 5.15 144.55 3:74 20.83 3:61 1.730:43 0.540:18
8 16384 4 1.14 201.92 3:97 23.07 3:99 1.170:36 0.650:16
8 256 4 1.49 194.20 3:87 22.35 3:81 1.260:37 0.620:16
4 8192 3 0.58 224.78 5:35 27.43 4:26 0.530:21 0.820:10
4y8192 3 1.06 221.94 4:58 25.21 4:17 0.720:26 0.760:12
4 256 3 0.47 223.81 4:58 26.43 4:22 0.620:24 0.800:11
2 2048 2 0.16 232.75 5:09 30.85 4:12 0.270:12 0.910:05
2 64 2 0.40 226.62 4:83 29.13 3:46 0.380:13 0.900:05
32 KL 64 2.04 189.53 3:68 22.27 3:93 1.410:40 0.610:17
32 KL 16 7.3 132.75 2:71 20.38 3:56 1.880:45 0.530:18
16 KL 16 0.87 210.31 3:97 24.08 4:22 1.070:36 0.680:15
16 KL 8 2.63 178.68 4:08 21.94 3:92 1.490:42 0.590:17
8 KL 4 0.90 209.90 4:92 24.19 4:19 1.020:35 0.690:15
4 KL 3 0.27 227.57 4:89 27.53 4:54 0.550:24 0.820:11
2 KL 2 0.086 232.66 5:16 32.47 4:19 0.200:09 0.930:04
Table 8. Complete autoencoder zoo trained on OpenImages, evaluated on ImageNet-Val. ydenotes an attention-free autoencoder.
layout-to-image synthesis on the COCO dataset
Figure 16. More samples from our best model for layout-to-image synthesis, LDM-4 , which was trained on the OpenImages dataset and
ﬁnetuned on the COCO dataset. Samples generated with 100 DDIM steps and = 0. Layouts are from the COCO validation set.
see Sec. C. Unlike the pixel-based methods, this classiﬁer is trained very cheaply in latent space. For additional qualitative
results, see Fig. 26 and Fig. 27.
21COCO 256256 OpenImages 256256 OpenImages 512512
Method FID# FID# FID#
LostGAN-V2 [87] 42.55 - -
OC-GAN [89] 41.65 - -
SPADE [62] 41.11 - -
VQGAN+T [37] 56.58 45.33 48.11
LDM-8 (100 steps, ours) 42.06y- -
LDM-4 (200 steps, ours) 40.9132.02 35.80
Table 9. Quantitative comparison of our layout-to-image models on the COCO [4] and OpenImages [49] datasets.y: Training from scratch
on COCO;: Finetuning from OpenImages.
Method FID# IS" Precision" Recall"Nparams
SR3 [72] 11.30 - - - 625M -
ImageBART [21] 21.19 - - - 3.5B -
ImageBART [21] 7.44 - - - 3.5B 0.05 acc. rate
VQGAN+T [23] 17.04 70.6 1.8 - - 1.3B -
VQGAN+T [23] 5.88 304.8 3.6 - - 1.3B 0.05 acc. rate
BigGan-deep [3] 6.95 203.6 2.6 0.87 0.28 340M -
ADM [15] 10.94 100.98 0.69 0.63 554M 250 DDIM steps
ADM-G [15] 4.59 186.7 0.82 0.52 608M 250 DDIM steps
ADM-G,ADM-U [15] 3.85 221.72 0.84 0.53 n/a 2 250 DDIM steps
CDM [31] 4.88 158.71 2.26 - - n/a 2 100 DDIM steps
LDM-8 (ours) 17.41 72.92 2.6 0.65 0.62 395M 200 DDIM steps, 2.9M train steps, batch size 64
LDM-8-G (ours) 8.11 190.43 2.60 0.83 0.36 506M 200 DDIM steps, classiﬁer scale 10, 2.9M train steps, batch size 64
LDM-8 (ours) 15.51 79.03 1.03 0.65 0.63 395M 200 DDIM steps, 4.8M train steps, batch size 64
LDM-8-G (ours) 7.76 209.52 4.24 0.84 0.35 506M 200 DDIM steps, classiﬁer scale 10, 4.8M train steps, batch size 64
LDM-4 (ours) 10.56 103.49 1.24 0.71 0.62 400M 250 DDIM steps, 178K train steps, batch size 1200
LDM-4-G (ours) 3.95 178.22 2.43 0.81 0.55 400M 250 DDIM steps, unconditional guidance [32] scale 1.25, 178K train steps, batch size 1200
LDM-4-G (ours) 3.60 247.67 5.59 0.87 0.48 400M 250 DDIM steps, unconditional guidance [32] scale 1.5, 178K train steps, batch size 1200
Table 10. Comparison of a class-conditional ImageNet LDM with recent state-of-the-art methods for class-conditional image generation
on the ImageNet [12] dataset.: Classiﬁer rejection sampling with the given rejection rate as proposed in [67].
D.5. Sample Quality vs. V100 Days (Continued from Sec. 4.1)
Figure 17. For completeness we also report the training progress of class-conditional LDMs on the ImageNet dataset for a ﬁxed number
of 35 V100 days. Results obtained with 100 DDIM steps [84] and = 0. FIDs computed on 5000 samples for efﬁciency reasons.
For the assessment of sample quality over the training progress in Sec. 4.1, we reported FID and IS scores as a function
of train steps. Another possibility is to report these metrics over the used resources in V100 days. Such an analysis is
additionally provided in Fig. 17, showing qualitatively similar results.
22Method FID# IS" PSNR" SSIM"
Image Regression [72] 15.2 121.1 27.9 0.801
SR3 [72] 5.2 180.1 26.4 0.762
LDM-4 (ours, 100 steps) 2.8y/4.8z166.3 24.4 3.8 0.690.14
LDM-4 (ours, 50 steps, guiding) 4.4y/6.4z153.7 25.8 3.7 0.740.12
LDM-4 (ours, 100 steps, guiding) 4.4y/6.4z154.1 25.7 3.7 0.730.12
LDM-4 (ours, 100 steps, +15 ep.) 2.6y/4.6z169.76 5.03 24.43.8 0.690.14
Pixel-DM (100 steps, +15 ep.) 5.1y/ 7.1z163.06 4.67 24.13.3 0.590.12
Table 11.4upscaling results on ImageNet-Val. ( 2562);y: FID features computed on validation split,z: FID features computed on train
split. We also include a pixel-space baseline that receives the same amount of compute as LDM-4 . The last two rows received 15 epochs
of additional training compared to the former results.
D.6. Super-Resolution
For better comparability between LDMs and diffusion models in pixel space, we extend our analysis from Tab. 5 by
comparing a diffusion model trained for the same number of steps and with a comparable number1of parameters to our
LDM. The results of this comparison are shown in the last two rows of Tab. 11 and demonstrate that LDM achieves better
performance while allowing for signiﬁcantly faster sampling. A qualitative comparison is given in Fig. 20 which shows
random samples from both LDM and the diffusion model in pixel space.
D.6.1 LDM-BSR: General Purpose SR Model via Diverse Image Degradation
bicubic LDM-SR LDM-BSR
Figure 18. LDM-BSR generalizes to arbitrary inputs and can be used as a general-purpose upsampler, upscaling samples from a class-
conditional LDM (image cf. Fig. 4) to 10242resolution. In contrast, using a ﬁxed degradation process (see Sec. 4.4) hinders generalization.
To evaluate generalization of our LDM-SR, we apply it both on synthetic LDM samples from a class-conditional ImageNet
model (Sec. 4.1) and images crawled from the internet. Interestingly, we observe that LDM-SR, trained only with a bicubicly
downsampled conditioning as in [72], does not generalize well to images which do not follow this pre-processing. Hence, to
obtain a superresolution model for a wide range of real world images, which can contain complex superpositions of camera
noise, compression artifacts, blurr and interpolations, we replace the bicubic downsampling operation in LDM-SR with the
degration pipeline from [105]. The BSR-degradation process is a degradation pipline which applies JPEG compressions
noise, camera sensor noise, different image interpolations for downsampling, Gaussian blur kernels and Gaussian noise in a
random order to an image. We found that using the bsr-degredation process with the original parameters as in [105] leads to
a very strong degradation process. Since a more moderate degradation process seemed apppropiate for our application, we
adapted the parameters of the bsr-degradation (our adapted degradation process can be found in our code base at https:
//github.com/CompVis/latent-diffusion ). Fig. 18 illustrates the effectiveness of this approach by directly
comparing LDM-SR with LDM-BSR . The latter produces images much sharper than the models conﬁned to a ﬁxed pre-
processing, making it suitable for real-world applications. Further results of LDM-BSR are shown on LSUN-cows in Fig. 19.
1It is not possible to exactly match both architectures since the diffusion model operates in the pixel space
23E. Implementation Details and Hyperparameters
E.1. Hyperparameters
We provide an overview of the hyperparameters of all trained LDM models in Tab. 12, Tab. 13, Tab. 14 and Tab. 15.
CelebA-HQ 256256 FFHQ 256256 LSUN-Churches 256256 LSUN-Bedrooms 256256
f 4 4 8 4
z-shape 64643 64643 - 64643
jZj 8192 8192 - 8192
Diffusion steps 1000 1000 1000 1000
Noise Schedule linear linear linear linear
Nparams 274M 274M 294M 274M
Channels 224 224 192 224
Depth 2 2 2 2
Channel Multiplier 1,2,3,4 1,2,3,4 1,2,2,4,4 1,2,3,4
Attention resolutions 32, 16, 8 32, 16, 8 32, 16, 8, 4 32, 16, 8
Head Channels 32 32 24 32
Batch Size 48 42 96 48
Iterations410k 635k 500k 1.9M
Learning Rate 9.6e-5 8.4e-5 5.e-5 9.6e-5
Table 12. Hyperparameters for the unconditional LDMs producing the numbers shown in Tab. 1. All models trained on a single NVIDIA
A100.
LDM-1 LDM-2 LDM-4 LDM-8 LDM-16 LDM-32
z-shape 2562563 1281282 64643 32324 16168 88832
jZj - 2048 8192 16384 16384 16384
Diffusion steps 1000 1000 1000 1000 1000 1000
Noise Schedule linear linear linear linear linear linear
Model Size 396M 391M 391M 395M 395M 395M
Channels 192 192 192 256 256 256
Depth 2 2 2 2 2 2
Channel Multiplier 1,1,2,2,4,4 1,2,2,4,4 1,2,3,5 1,2,4 1,2,4 1,2,4
Number of Heads 1 1 1 1 1 1
Batch Size 7 9 40 64 112 112
Iterations 2M 2M 2M 2M 2M 2M
Learning Rate 4.9e-5 6.3e-5 8e-5 6.4e-5 4.5e-5 4.5e-5
Conditioning CA CA CA CA CA CA
CA-resolutions 32, 16, 8 32, 16, 8 32, 16, 8 32, 16, 8 16, 8, 4 8, 4, 2
Embedding Dimension 512 512 512 512 512 512
Transformers Depth 1 1 1 1 1 1
Table 13. Hyperparameters for the conditional LDMs trained on the ImageNet dataset for the analysis in Sec. 4.1. All models trained on a
single NVIDIA A100.
E.2. Implementation Details
E.2.1 Implementations of for conditional LDMs
For the experiments on text-to-image and layout-to-image (Sec. 4.3.1) synthesis, we implement the conditioner as an
unmasked transformer which processes a tokenized version of the input yand produces an output :=(y), where2
RMd. More speciﬁcally, the transformer is implemented from Ntransformer blocks consisting of global self-attention
layers, layer-normalization and position-wise MLPs as follows2:
2adapted from https://github.com/lucidrains/x-transformers
24LDM-1 LDM-2 LDM-4 LDM-8 LDM-16 LDM-32
z-shape 2562563 1281282 64643 32324 16168 88832
jZj - 2048 8192 16384 16384 16384
Diffusion steps 1000 1000 1000 1000 1000 1000
Noise Schedule linear linear linear linear linear linear
Model Size 270M 265M 274M 258M 260M 258M
Channels 192 192 224 256 256 256
Depth 2 2 2 2 2 2
Channel Multiplier 1,1,2,2,4,4 1,2,2,4,4 1,2,3,4 1,2,4 1,2,4 1,2,4
Attention resolutions 32, 16, 8 32, 16, 8 32, 16, 8 32, 16, 8 16, 8, 4 8, 4, 2
Head Channels 32 32 32 32 32 32
Batch Size 9 11 48 96 128 128
Iterations500k 500k 500k 500k 500k 500k
Learning Rate 9e-5 1.1e-4 9.6e-5 9.6e-5 1.3e-4 1.3e-4
Table 14. Hyperparameters for the unconditional LDMs trained on the CelebA dataset for the analysis in Fig. 7. All models trained on a
single NVIDIA A100.: All models are trained for 500k iterations. If converging earlier, we used the best checkpoint for assessing the
provided FID scores.
Task Text-to-Image Layout-to-Image Class-Label-to-Image Super Resolution Inpainting Semantic-Map-to-Image
Dataset LAION OpenImages COCO ImageNet ImageNet Places Landscapes
f 8 4 8 4 4 4 8
z-shape 32324 64643 32324 64643 64643 64643 32324
jZj - 8192 16384 8192 8192 8192 16384
Diffusion steps 1000 1000 1000 1000 1000 1000 1000
Noise Schedule linear linear linear linear linear linear linear
Model Size 1.45B 306M 345M 395M 169M 215M 215M
Channels 320 128 192 192 160 128 128
Depth 2 2 2 2 2 2 2
Channel Multiplier 1,2,4,4 1,2,3,4 1,2,4 1,2,3,5 1,2,2,4 1,4,8 1,4,8
Number of Heads 8 1 1 1 1 1 1
Dropout - - 0.1 - - - -
Batch Size 680 24 48 1200 64 128 48
Iterations 390K 4.4M 170K 178K 860K 360K 360K
Learning Rate 1.0e-4 4.8e-5 4.8e-5 1.0e-4 6.4e-5 1.0e-6 4.8e-5
Conditioning CA CA CA CA concat concat concat
(C)A-resolutions 32, 16, 8 32, 16, 8 32, 16, 8 32, 16, 8 - - -
Embedding Dimension 1280 512 512 512 - - -
Transformer Depth 1 3 2 1 - - -
Table 15. Hyperparameters for the conditional LDMs from Sec. 4. All models trained on a single NVIDIA A100 except for the inpainting
model which was trained on eight V100.
 TokEmb (y) +PosEmb(y) (18)
fori= 1;:::;N :
1 LayerNorm () (19)
2 MultiHeadSelfAttention (1) + (20)
3 LayerNorm (2) (21)
 MLP(3) +2 (22)
 LayerNorm () (23)
(24)
Withavailable, the conditioning is mapped into the UNet via the cross-attention mechanism as depicted in Fig. 3. We
modify the “ablated UNet” [15] architecture and replace the self-attention layer with a shallow (unmasked) transformer
consisting of Tblocks with alternating layers of (i) self-attention, (ii) a position-wise MLP and (iii) a cross-attention layer;
25see Tab. 16. Note that without (ii) and (iii), this architecture is equivalent to the “ablated UNet”.
While it would be possible to increase the representational power of by additionally conditioning on the time step t, we
do not pursue this choice as it reduces the speed of inference. We leave a more detailed analysis of this modiﬁcation to future
work.
For the text-to-image model, we rely on a publicly available3tokenizer [99]. The layout-to-image model discretizes the
spatial locations of the bounding boxes and encodes each box as a (l;b;c )-tuple, where ldenotes the (discrete) top-left and b
the bottom-right position. Class information is contained in c.
See Tab. 17 for the hyperparameters of and Tab. 13 for those of the UNet for both of the above tasks.
Note that the class-conditional model as described in Sec. 4.1 is also implemented via cross-attention, where is a single
learnable embedding layer with a dimensionality of 512, mapping classes yto2R1512.
input Rhwc
LayerNorm Rhwc
Conv1x1 Rhwdnh
Reshape Rhwdnh
T8
><
>:SelfAttention
MLP
CrossAttentionRhwdnh
Rhwdnh
Rhwdnh
Reshape Rhwdnh
Conv1x1 Rhwc
Table 16. Architecture of a transformer block as described in Sec. E.2.1, replacing the self-attention layer of the standard “ablated UNet”
architecture [15]. Here, nhdenotes the number of attention heads and dthe dimensionality per head.
Text-to-Image Layout-to-Image
seq-length 77 92
depthN 32 16
dim 1280 512
Table 17. Hyperparameters for the experiments with transformer encoders in Sec. 4.3.
E.2.2 Inpainting
For our experiments on image-inpainting in Sec. 4.5, we used the code of [88] to generate synthetic masks. We use a ﬁxed
set of 2k validation and 30k testing samples from Places [108]. During training, we use random crops of size 256256
and evaluate on crops of size 512512. This follows the training and testing protocol in [88] and reproduces their reported
metrics (seeyin Tab. 7). We include additional qualitative results of LDM-4, w/ attn in Fig. 21 and of LDM-4, w/o attn, big,
w/ ft in Fig. 22.
E.3. Evaluation Details
This section provides additional details on evaluation for the experiments shown in Sec. 4.
E.3.1 Quantitative Results in Unconditional and Class-Conditional Image Synthesis
We follow common practice and estimate the statistics for calculating the FID-, Precision- and Recall-scores [29,50] shown in
Tab. 1 and 10 based on 50k samples from our models and the entire training set of each of the shown datasets. For calculating
FID scores we use the torch-fidelity package [60]. However, since different data processing pipelines might lead to
different results [64], we also evaluate our models with the script provided by Dhariwal and Nichol [15]. We ﬁnd that results
3https://huggingface.co/transformers/model_doc/bert.html#berttokenizerfast
26mainly coincide, except for the ImageNet and LSUN-Bedrooms datasets, where we notice slightly varying scores of 7.76
(torch-fidelity ) vs. 7.77 (Nichol and Dhariwal) and 2.95 vs 3.0. For the future we emphasize the importance of a
uniﬁed procedure for sample quality assessment. Precision and Recall are also computed by using the script provided by
Nichol and Dhariwal.
E.3.2 Text-to-Image Synthesis
Following the evaluation protocol of [66] we compute FID and Inception Score for the Text-to-Image models from Tab. 2 by
comparing generated samples with 30000 samples from the validation set of the MS-COCO dataset [51]. FID and Inception
Scores are computed with torch-fidelity .
E.3.3 Layout-to-Image Synthesis
For assessing the sample quality of our Layout-to-Image models from Tab. 9 on the COCO dataset, we follow common
practice [37, 87, 89] and compute FID scores the 2048 unaugmented examples of the COCO Segmentation Challenge split.
To obtain better comparability, we use the exact same samples as in [37]. For the OpenImages dataset we similarly follow
their protocol and use 2048 center-cropped test images from the validation set.
E.3.4 Super Resolution
We evaluate the super-resolution models on ImageNet following the pipeline suggested in [72], i.e. images with a shorter
size less than 256px are removed (both for training and evaluation). On ImageNet, the low-resolution images are produced
using bicubic interpolation with anti-aliasing. FIDs are evaluated using torch-fidelity [60], and we produce samples
on the validation split. For FID scores, we additionally compare to reference features computed on the train split, see Tab. 5
and Tab. 11.
E.3.5 Efﬁciency Analysis
For efﬁciency reasons we compute the sample quality metrics plotted in Fig. 6, 17 and 7 based on 5k samples. Therefore,
the results might vary from those shown in Tab. 1 and 10. All models have a comparable number of parameters as provided
in Tab. 13 and 14. We maximize the learning rates of the individual models such that they still train stably. Therefore, the
learning rates slightly vary between different runs cf. Tab. 13 and 14.
E.3.6 User Study
For the results of the user study presented in Tab. 4 we followed the protocoll of [72] and and use the 2-alternative force-choice
paradigm to assess human preference scores for two distinct tasks. In Task-1 subjects were shown a low resolution/masked
image between the corresponding ground truth high resolution/unmasked version and a synthesized image, which was gen-
erated by using the middle image as conditioning. For SuperResolution subjects were asked: ’Which of the two images is a
better high quality version of the low resolution image in the middle?’ . For Inpainting we asked ’Which of the two images
contains more realistic inpainted regions of the image in the middle?’ . In Task-2, humans were similarly shown the low-
res/masked version and asked for preference between two corresponding images generated by the two competing methods.
As in [72] humans viewed the images for 3 seconds before responding.
27F. Computational Requirements
Method Generator Classiﬁer Overall Inference Nparams FID# IS" Precision"Recall"
Compute Compute Compute Throughput
LSUN Churches 2562
StyleGAN2 [42]y64 - 64 - 59M 3.86 - - -
LDM-8 (ours, 100 steps, 410K) 18 - 18 6.80 256M 4.02 - 0.64 0.52
LSUN Bedrooms 2562
ADM [15]y(1000 steps) 232 - 232 0.03 552M 1.9 - 0.66 0.51
LDM-4 (ours, 200 steps, 1.9M) 60 - 55 1.07 274M 2.95 - 0.66 0.48
CelebA-HQ 2562
LDM-4 (ours, 500 steps, 410K) 14.4 - 14.4 0.43 274M 5.11 - 0.72 0.49
FFHQ 2562
StyleGAN2 [42] 32.13z- 32.13y- 59M 3.8 - - -
LDM-4 (ours, 200 steps, 635K) 26 - 26 1.07 274M 4.98 - 0.73 0.50
ImageNet 2562
VQGAN-f-4 (ours, ﬁrst stage) 29 - 29 - 55M 0.58yy- - -
VQGAN-f-8 (ours, ﬁrst stage) 66 - 66 - 68M 1.14yy- - -
BigGAN-deep [3]y128-256 128-256 - 340M 6.95 203.6 2.6 0.87 0.28
ADM [15] (250 steps)y916 - 916 0.12 554M 10.94 100.98 0.69 0.63
ADM-G [15] (25 steps)y916 46 962 0.7 608M 5.58 - 0.81 0.49
ADM-G [15] (250 steps)y916 46 962 0.07 608M 4.59 186.7 0.82 0.52
ADM-G,ADM-U [15] (250 steps)y329 30 349 n/a n/a 3.85 221.72 0.84 0.53
LDM-8-G (ours, 100, 2.9M) 79 12 91 1.93 506M 8.11 190.4 2.6 0.83 0.36
LDM-8 (ours, 200 ddim steps 2.9M, batch size 64) 79 - 79 1.9 395M 17.41 72.92 0.65 0.62
LDM-4 (ours, 250 ddim steps 178K, batch size 1200) 271 - 271 0.7 400M 10.56 103.49 1.24 0.71 0.62
LDM-4-G (ours, 250 ddim steps 178K, batch size 1200, classiﬁer-free guidance [32] scale 1.25) 271 - 271 0.4 400M 3.95 178.22 2.43 0.81 0.55
LDM-4-G (ours, 250 ddim steps 178K, batch size 1200, classiﬁer-free guidance [32] scale 1.5) 271 - 271 0.4 400M 3.60 247.67 5.59 0.87 0.48
Table 18. Comparing compute requirements during training and inference throughput with state-of-the-art generative models. Compute
during training in V100-days, numbers of competing methods taken from [15] unless stated differently;: Throughput measured in sam-
ples/sec on a single NVIDIA A100;y: Numbers taken from [15] ;z: Assumed to be trained on 25M train examples;yy: R-FID vs. ImageNet
validation set
In Tab 18 we provide a more detailed analysis on our used compute ressources and compare our best performing models
on the CelebA-HQ, FFHQ, LSUN and ImageNet datasets with the recent state of the art models by using their provided
numbers, cf. [15]. As they report their used compute in V100 days and we train all our models on a single NVIDIA A100
GPU, we convert the A100 days to V100 days by assuming a 2:2speedup of A100 vs V100 [74]4. To assess sample quality,
we additionally report FID scores on the reported datasets. We closely reach the performance of state of the art methods as
StyleGAN2 [42] and ADM [15] while signiﬁcantly reducing the required compute resources.
4This factor corresponds to the speedup of the A100 over the V100 for a U-Net, as deﬁned in Fig. 1 in [74]
28G. Details on Autoencoder Models
We train all our autoencoder models in an adversarial manner following [23], such that a patch-based discriminator D 
is optimized to differentiate original images from reconstructions D(E(x)). To avoid arbitrarily scaled latent spaces, we
regularize the latent zto be zero centered and obtain small variance by introducing an regularizing loss term Lreg.
We investigate two different regularization methods: (i) a low-weighted Kullback-Leibler-term between qE(zjx) =
N(z;E;E2)and a standard normal distribution N(z; 0;1)as in a standard variational autoencoder [46, 69], and, (ii) regu-
larizing the latent space with a vector quantization layer by learning a codebook of jZjdifferent exemplars [96].
To obtain high-ﬁdelity reconstructions we only use a very small regularization for both scenarios, i.e. we either weight the
KLterm by a factor10 6or choose a high codebook dimensionality jZj.
The full objective to train the autoencoding model (E;D)reads:
LAutoencoder = min
E;Dmax
 
Lrec(x;D(E(x))) Ladv(D(E(x))) + logD (x) +Lreg(x;E;D)
(25)
DM Training in Latent Space Note that for training diffusion models on the learned latent space, we again distinguish two
cases when learning p(z)orp(zjy)(Sec. 4.3): (i) For a KL-regularized latent space, we sample z=E(x)+E(x)"=:E(x),
where"N(0;1). When rescaling the latent, we estimate the component-wise variance
^2=1
bchwX
b;c;h;w(zb;c;h;w ^)2
from the ﬁrst batch in the data, where ^=1
bchwP
b;c;h;wzb;c;h;w. The output ofEis scaled such that the rescaled latent has
unit standard deviation, i.e.z z
^=E(x)
^. (ii) For a VQ-regularized latent space, we extract zbefore the quantization layer
and absorb the quantization operation into the decoder, i.e. it can be interpreted as the ﬁrst layer of D.
H. Additional Qualitative Results
Finally, we provide additional qualitative results for our landscapes model (Fig. 12, 23, 24 and 25), our class-conditional
ImageNet model (Fig. 26 - 27) and our unconditional models for the CelebA-HQ, FFHQ and LSUN datasets (Fig. 28 - 31).
Similar as for the inpainting model in Sec. 4.5 we also ﬁne-tuned the semantic landscapes model from Sec. 4.3.2 directly on
5122images and depict qualitative results in Fig. 12 and Fig. 23. For our those models trained on comparably small datasets,
we additionally show nearest neighbors in VGG [79] feature space for samples from our models in Fig. 32 - 34.
29bicubic LDM-BSR
Figure 19. LDM-BSR generalizes to arbitrary inputs and can be used as a general-purpose upsampler, upscaling samples from the LSUN-
Cows dataset to 10242resolution.
30input GT Pixel Baseline #1 Pixel Baseline #2 LDM #1 LDM #2
Figure 20. Qualitative superresolution comparison of two random samples between LDM-SR and baseline-diffusionmodel in Pixelspace.
Evaluated on imagenet validation-set after same amount of training steps.
31input GT LaMa [88] LDM #1 LDM #2 LDM #3
Figure 21. Qualitative results on image inpainting. In contrast to [88], our generative approach enables generation of multiple diverse
samples for a given input.
32input result input result
Figure 22. More qualitative results on object removal as in Fig. 11.
33Semantic Synthesis on Flickr-Landscapes [23] ( 5122ﬁnetuning)
Figure 23. Convolutional samples from the semantic landscapes model as in Sec. 4.3.2, ﬁnetuned on 5122images.
34Figure 24. A LDM trained on 2562resolution can generalize to larger resolution for spatially conditioned tasks such as semantic synthesis
of landscape images. See Sec. 4.3.2.
35Semantic Synthesis on Flickr-Landscapes [23]
Figure 25. When provided a semantic map as conditioning, our LDMs generalize to substantially larger resolutions than those seen during
training. Although this model was trained on inputs of size 2562it can be used to create high-resolution samples as the ones shown here,
which are of resolution 1024384. 36Random class conditional samples on the ImageNet dataset
Figure 26. Random samples from LDM-4 trained on the ImageNet dataset. Sampled with classiﬁer-free guidance [32] scale s= 5:0and
200 DDIM steps with = 1:0.
37Random class conditional samples on the ImageNet dataset
Figure 27. Random samples from LDM-4 trained on the ImageNet dataset. Sampled with classiﬁer-free guidance [32] scale s= 3:0and
200 DDIM steps with = 1:0.
38Random samples on the CelebA-HQ dataset
Figure 28. Random samples of our best performing model LDM-4 on the CelebA-HQ dataset. Sampled with 500 DDIM steps and = 0
(FID = 5.15).
39Random samples on the FFHQ dataset
Figure 29. Random samples of our best performing model LDM-4 on the FFHQ dataset. Sampled with 200 DDIM steps and = 1 (FID
= 4.98).
40Random samples on the LSUN-Churches dataset
Figure 30. Random samples of our best performing model LDM-8 on the LSUN-Churches dataset. Sampled with 200 DDIM steps and
= 0(FID = 4.48).
41Random samples on the LSUN-Bedrooms dataset
Figure 31. Random samples of our best performing model LDM-4 on the LSUN-Bedrooms dataset. Sampled with 200 DDIM steps and
= 1(FID = 2.95).
42Nearest Neighbors on the CelebA-HQ dataset
Figure 32. Nearest neighbors of our best CelebA-HQ model, computed in the feature space of a VGG-16 [79]. The leftmost sample is
from our model. The remaining samples in each row are its 10 nearest neighbors.
43Nearest Neighbors on the FFHQ dataset
Figure 33. Nearest neighbors of our best FFHQ model, computed in the feature space of a VGG-16 [79]. The leftmost sample is from our
model. The remaining samples in each row are its 10 nearest neighbors.
44Nearest Neighbors on the LSUN-Churches dataset
Figure 34. Nearest neighbors of our best LSUN-Churches model, computed in the feature space of a VGG-16 [79]. The leftmost sample
is from our model. The remaining samples in each row are its 10 nearest neighbors.
45Segment Anything
Alexander Kirillov1;2;4Eric Mintun2Nikhila Ravi1;2Hanzi Mao2Chloe Rolland3Laura Gustafson3
Tete Xiao3Spencer Whitehead Alexander C. Berg Wan-Yen Lo Piotr Doll ´ar4Ross Girshick4
1project lead2joint ﬁrst author3equal contribution4directional lead
Meta AI Research, FAIR
(b) Model: Segment Anything Model (SAM)promptimagevalid maskimage encoderprompt encoderlightweight mask decoder
(a) Task: promptable segmentationsegmentation promptimagemodelcat withblack earsvalid mask
(c) Data: data engine (top) & dataset (bottom)•1+ billion masks•11 million images •privacy respecting•licensed imagesannotatetraindatamodelSegment Anything 1B (SA-1B):
Figure 1: We aim to build a foundation model for segmentation by introducing three interconnected components: a prompt-
able segmentation task, a segmentation model (SAM) that powers data annotation and enables zero-shot transfer to a range
of tasks via prompt engineering, and a data engine for collecting SA-1B, our dataset of over 1 billion masks.
Abstract
We introduce the Segment Anything (SA) project: a new
task, model, and dataset for image segmentation. Using our
efﬁcient model in a data collection loop, we built the largest
segmentation dataset to date (by far), with over 1 billion
masks on 11M licensed and privacy respecting images. The
model is designed and trained to be promptable, so it can
transfer zero-shot to new image distributions and tasks. We
evaluate its capabilities on numerous tasks and ﬁnd that
its zero-shot performance is impressive – often competitive
with or even superior to prior fully supervised results. We
are releasing the Segment Anything Model (SAM) and cor-
responding dataset (SA-1B) of 1B masks and 11M images at
https://segment-anything.com to foster research into foun-
dation models for computer vision.
1. Introduction
Large language models pre-trained on web-scale datasets
are revolutionizing NLP with strong zero-shot and few-shot
generalization [10]. These “foundation models” [8] can
generalize to tasks and data distributions beyond those seen
during training. This capability is often implemented with
prompt engineering in which hand-crafted text is used to
prompt the language model to generate a valid textual re-
sponse for the task at hand. When scaled and trained with
abundant text corpora from the web, these models’ zero and
few-shot performance compares surprisingly well to (evenmatching in some cases) ﬁne-tuned models [10, 21]. Empir-
ical trends show this behavior improving with model scale,
dataset size, and total training compute [56, 10, 21, 51].
Foundation models have also been explored in computer
vision, albeit to a lesser extent. Perhaps the most promi-
nent illustration aligns paired text and images from the web.
For example, CLIP [82] and ALIGN [55] use contrastive
learning to train text and image encoders that align the two
modalities. Once trained, engineered text prompts enable
zero-shot generalization to novel visual concepts and data
distributions. Such encoders also compose effectively with
other modules to enable downstream tasks, such as image
generation ( e.g., DALL·E [83]). While much progress has
been made on vision and language encoders, computer vi-
sion includes a wide range of problems beyond this scope,
and for many of these, abundant training data does not exist.
In this work, our goal is to build a foundation model for
image segmentation . That is, we seek to develop a prompt-
able model and pre-train it on a broad dataset using a task
that enables powerful generalization. With this model, we
aim to solve a range of downstream segmentation problems
on new data distributions using prompt engineering.
The success of this plan hinges on three components:
task,model , and data . To develop them, we address the
following questions about image segmentation:
1. What task will enable zero-shot generalization?
2. What is the corresponding model architecture?
3. What data can power this task and model?
1arXiv:2304.02643v1  [cs.CV]  5 Apr 2023These questions are entangled and require a comprehen-
sive solution. We start by deﬁning a promptable segmenta-
tiontask that is general enough to provide a powerful pre-
training objective and to enable a wide range of downstream
applications. This task requires a model that supports ﬂex-
ible prompting and can output segmentation masks in real-
time when prompted to allow for interactive use. To train
our model, we need a diverse, large-scale source of data .
Unfortunately, there is no web-scale data source for seg-
mentation; to address this, we build a “data engine”, i.e.,
we iterate between using our efﬁcient model to assist in data
collection and using the newly collected data to improve the
model. We introduce each interconnected component next,
followed by the dataset we created and the experiments that
demonstrate the effectiveness of our approach.
Task (§2). In NLP and more recently computer vision,
foundation models are a promising development that can
perform zero-shot and few-shot learning for new datasets
and tasks often by using “prompting” techniques. Inspired
by this line of work, we propose the promptable segmen-
tation task , where the goal is to return a valid segmenta-
tion mask given any segmentation prompt (see Fig. 1a). A
prompt simply speciﬁes what to segment in an image, e.g.,
a prompt can include spatial or text information identifying
an object. The requirement of a valid output mask means
that even when a prompt is ambiguous and could refer to
multiple objects (for example, a point on a shirt may in-
dicate either the shirt or the person wearing it), the output
should be a reasonable mask for at least one of those ob-
jects. We use the promptable segmentation task as both a
pre-training objective and to solve general downstream seg-
mentation tasks via prompt engineering.
Model (§3). The promptable segmentation task and the goal
of real-world use impose constraints on the model architec-
ture. In particular, the model must support ﬂexible prompts ,
needs to compute masks in amortized real-time to allow in-
teractive use, and must be ambiguity-aware . Surprisingly,
we ﬁnd that a simple design satisﬁes all three constraints:
a powerful image encoder computes an image embedding,
a prompt encoder embeds prompts, and then the two infor-
mation sources are combined in a lightweight mask decoder
that predicts segmentation masks. We refer to this model as
the Segment Anything Model, or SAM (see Fig. 1b). By
separating SAM into an image encoder and a fast prompt
encoder / mask decoder, the same image embedding can
be reused (and its cost amortized) with different prompts.
Given an image embedding, the prompt encoder and mask
decoder predict a mask from a prompt in 50ms in a web
browser. We focus on point, box, and mask prompts, and
also present initial results with free-form text prompts. To
make SAM ambiguity-aware, we design it to predict mul-
tiple masks for a single prompt allowing SAM to naturally
handle ambiguity, such as the shirt vs. person example.Data engine (§4). To achieve strong generalization to new
data distributions, we found it necessary to train SAM on
a large and diverse set of masks, beyond any segmenta-
tion dataset that already exists. While a typical approach
for foundation models is to obtain data online [82], masks
are not naturally abundant and thus we need an alternative
strategy. Our solution is to build a “data engine”, i.e., we
co-develop our model with model-in-the-loop dataset an-
notation (see Fig. 1c). Our data engine has three stages:
assisted-manual ,semi-automatic , and fully automatic . In
the ﬁrst stage, SAM assists annotators in annotating masks,
similar to a classic interactive segmentation setup. In the
second stage, SAM can automatically generate masks for
a subset of objects by prompting it with likely object lo-
cations and annotators focus on annotating the remaining
objects, helping increase mask diversity. In the ﬁnal stage,
we prompt SAM with a regular grid of foreground points,
yielding on average 100 high-quality masks per image.
Dataset (§5). Our ﬁnal dataset, SA-1B, includes more than
1Bmasks from 11M licensed and privacy-preserving im-
ages (see Fig. 2). SA-1B, collected fully automatically us-
ing the ﬁnal stage of our data engine, has 400 more masks
than any existing segmentation dataset [66, 44, 117, 60],
and as we verify extensively, the masks are of high quality
and diversity. Beyond its use in training SAM to be robust
and general, we hope SA-1B becomes a valuable resource
for research aiming to build new foundation models.
Responsible AI (§6). We study and report on potential fair-
ness concerns and biases when using SA-1B and SAM. Im-
ages in SA-1B span a geographically and economically di-
verse set of countries and we found that SAM performs sim-
ilarly across different groups of people. Together, we hope
this will make our work more equitable for real-world use
cases. We provide model and dataset cards in the appendix.
Experiments (§7). We extensively evaluate SAM. First, us-
ing a diverse new suite of 23 segmentation datasets, we ﬁnd
that SAM produces high-quality masks from a single fore-
ground point, often only slightly below that of the manu-
ally annotated ground truth. Second, we ﬁnd consistently
strong quantitative and qualitative results on a variety of
downstream tasks under a zero-shot transfer protocol using
prompt engineering, including edge detection, object pro-
posal generation, instance segmentation, and a preliminary
exploration of text-to-mask prediction. These results sug-
gest that SAM can be used out-of-the-box with prompt en-
gineering to solve a variety of tasks involving object and
image distributions beyond SAM’s training data. Neverthe-
less, room for improvement remains, as we discuss in §8.
Release. We are releasing the SA-1B dataset for research
purposes and making SAM available under a permissive
open license (Apache 2.0) at https://segment-anything.com.
We also showcase SAM’s capabilities with an online demo.
2<50 masks
 50-100 masks
 100-200 masks
 200-300 masks
 300-400 masks
 400-500 masks
 >500 masks
Figure 2: Example images with overlaid masks from our newly introduced dataset, SA-1B . SA-1B contains 11M diverse,
high-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks. These masks were
annotated fully automatically by SAM, and as we verify by human ratings and numerous experiments, are of high quality and
diversity. We group images by number of masks per image for visualization (there are 100 masks per image on average).
32. Segment Anything Task
We take inspiration from NLP, where the next token pre-
diction task is used for foundation model pre-training and
to solve diverse downstream tasks via prompt engineer-
ing [10]. To build a foundation model for segmentation,
we aim to deﬁne a task with analogous capabilities.
Task. We start by translating the idea of a prompt from NLP
to segmentation, where a prompt can be a set of foreground
/ background points, a rough box or mask, free-form text,
or, in general, any information indicating what to segment
in an image. The promptable segmentation task , then, is to
return a valid segmentation mask given any prompt . The re-
quirement of a “valid” mask simply means that even when
a prompt is ambiguous and could refer to multiple objects
(e.g., recall the shirt vs. person example, and see Fig. 3),
the output should be a reasonable mask for at least oneof
those objects. This requirement is similar to expecting a lan-
guage model to output a coherent response to an ambiguous
prompt. We choose this task because it leads to a natural
pre-training algorithm anda general method for zero-shot
transfer to downstream segmentation tasks via prompting.
Pre-training. The promptable segmentation task suggests a
natural pre-training algorithm that simulates a sequence of
prompts ( e.g., points, boxes, masks) for each training sam-
ple and compares the model’s mask predictions against the
ground truth. We adapt this method from interactive seg-
mentation [109, 70], although unlike interactive segmenta-
tion whose aim is to eventually predict a valid mask after
enough user input, our aim is to always predict a valid mask
forany prompt even when the prompt is ambiguous . This
ensures that a pre-trained model is effective in use cases that
involve ambiguity, including automatic annotation as re-
quired by our data engine §4. We note that performing well
at this task is challenging and requires specialized modeling
and training loss choices, which we discuss in §3.
Zero-shot transfer. Intuitively, our pre-training task en-
dows the model with the ability to respond appropriately to
any prompt at inference time, and thus downstream tasks
can be solved by engineering appropriate prompts. For ex-
ample, if one has a bounding box detector for cats, cat in-
stance segmentation can be solved by providing the detec-
tor’s box output as a prompt to our model. In general, a wide
array of practical segmentation tasks can be cast as prompt-
ing. In addition to automatic dataset labeling, we explore
ﬁve diverse example tasks in our experiments in §7.
Related tasks. Segmentation is a broad ﬁeld: there’s in-
teractive segmentation [57, 109], edge detection [3], su-
per pixelization [85], object proposal generation [2], fore-
ground segmentation [94], semantic segmentation [90], in-
stance segmentation [66], panoptic segmentation [59], etc.
The goal of our promptable segmentation task is to produce
Figure 3: Each column shows 3 valid masks generated by
SAM from a single ambiguous point prompt (green circle).
a broadly capable model that can adapt to many (though
not all) existing and new segmentation tasks via prompt
engineering. This capability is a form of task generaliza-
tion [26]. Note that this is different than previous work on
multi-task segmentation systems. In a multi-task system, a
single model performs a ﬁxed set of tasks, e.g., joint seman-
tic, instance, and panoptic segmentation [114, 19, 54], but
the training and test tasks are the same. An important dis-
tinction in our work is that a model trained for promptable
segmentation can perform a new, different task at inference
time by acting as a component in a larger system, e.g., to
perform instance segmentation, a promptable segmentation
model is combined with an existing object detector.
Discussion. Prompting and composition are powerful tools
that enable a single model to be used in extensible ways, po-
tentially to accomplish tasks unknown at the time of model
design. This approach is analogous to how other founda-
tion models are used, e.g., how CLIP [82] is the text-image
alignment component of the DALL E [83] image generation
system. We anticipate that composable system design, pow-
ered by techniques such as prompt engineering, will enable
a wider variety of applications than systems trained specif-
ically for a ﬁxed set of tasks. It’s also interesting to com-
pare promptable and interactive segmentation through the
lens of composition: while interactive segmentation mod-
els are designed with human users in mind, a model trained
for promptable segmentation can also be composed into a
larger algorithmic system as we will demonstrate.
4,score
score
score,,
valid masksimage
image
encoder
image
embeddingmask points box textprompt encodermask decoder
convFigure 4: Segment Anything Model (SAM) overview. A heavyweight image encoder outputs an image embedding that can
then be efﬁciently queried by a variety of input prompts to produce object masks at amortized real-time speed. For ambiguous
prompts corresponding to more than one object, SAM can output multiple valid masks and associated conﬁdence scores.
3. Segment Anything Model
We next describe the Segment Anything Model (SAM)
for promptable segmentation. SAM has three components,
illustrated in Fig. 4: an image encoder, a ﬂexible prompt
encoder, and a fast mask decoder. We build on Transformer
vision models [14, 33, 20, 62] with speciﬁc tradeoffs for
(amortized) real-time performance. We describe these com-
ponents at a high-level here, with details in §A.
Image encoder. Motivated by scalability and powerful pre-
training methods, we use an MAE [47] pre-trained Vision
Transformer (ViT) [33] minimally adapted to process high
resolution inputs [62]. The image encoder runs once per
image and can be applied prior to prompting the model.
Prompt encoder. We consider two sets of prompts: sparse
(points, boxes, text) and dense (masks). We represent
points and boxes by positional encodings [95] summed with
learned embeddings for each prompt type and free-form text
with an off-the-shelf text encoder from CLIP [82]. Dense
prompts ( i.e., masks) are embedded using convolutions and
summed element-wise with the image embedding.
Mask decoder. The mask decoder efﬁciently maps the im-
age embedding, prompt embeddings, and an output token
to a mask. This design, inspired by [14, 20], employs a
modiﬁcation of a Transformer decoder block [103] followed
by a dynamic mask prediction head. Our modiﬁed decoder
block uses prompt self-attention and cross-attention in two
directions (prompt-to-image embedding and vice-versa) to
update allembeddings. After running two blocks, we up-
sample the image embedding and an MLP maps the output
token to a dynamic linear classiﬁer, which then computes
the mask foreground probability at each image location.
Resolving ambiguity. With one output, the model will av-
erage multiple valid masks if given an ambiguous prompt.
To address this, we modify the model to predict multiple
output masks for a single prompt (see Fig. 3). We found
3 mask outputs is sufﬁcient to address most common cases
(nested masks are often at most three deep: whole, part, and
subpart). During training, we backprop only the minimumloss [15, 45, 64] over masks. To rank masks, the model pre-
dicts a conﬁdence score ( i.e., estimated IoU) for each mask.
Efﬁciency. The overall model design is largely motivated
by efﬁciency. Given a precomputed image embedding, the
prompt encoder and mask decoder run in a web browser, on
CPU, in 50ms. This runtime performance enables seam-
less, real-time interactive prompting of our model.
Losses and training. We supervise mask prediction with
the linear combination of focal loss [65] and dice loss [73]
used in [14]. We train for the promptable segmentation task
using a mixture of geometric prompts (for text prompts see
§7.5). Following [92, 37], we simulate an interactive setup
by randomly sampling prompts in 11 rounds per mask, al-
lowing SAM to integrate seamlessly into our data engine.
4. Segment Anything Data Engine
As segmentation masks are not abundant on the inter-
net, we built a data engine to enable the collection of our
1.1B mask dataset, SA-1B. The data engine has three
stages: (1) a model-assisted manual annotation stage, (2) a
semi-automatic stage with a mix of automatically predicted
masks and model-assisted annotation, and (3) a fully auto-
matic stage in which our model generates masks without
annotator input. We go into details of each next.
Assisted-manual stage. In the ﬁrst stage, resembling clas-
sic interactive segmentation, a team of professional annota-
tors labeled masks by clicking foreground / background ob-
ject points using a browser-based interactive segmentation
tool powered by SAM. Masks could be reﬁned using pixel-
precise “brush” and “eraser” tools. Our model-assisted an-
notation runs in real-time directly inside a browser (using
precomputed image embeddings) enabling a truly interac-
tive experience. We did not impose semantic constraints for
labeling objects, and annotators freely labeled both “stuff”
and “things” [1]. We suggested annotators label objects
they could name or describe, but did not collect these names
or descriptions. Annotators were asked to label objects in
order of prominence and were encouraged to proceed to the
next image once a mask took over 30 seconds to annotate.
5At the start of this stage, SAM was trained using com-
mon public segmentation datasets. After sufﬁcient data an-
notation, SAM was retrained using only newly annotated
masks. As more masks were collected, the image encoder
was scaled from ViT-B to ViT-H and other architectural de-
tails evolved; in total we retrained our model 6 times. Av-
erage annotation time per mask decreased from 34 to 14
seconds as the model improved. We note that 14 seconds
is 6.5faster than mask annotation for COCO [66] and
only 2slower than bounding-box labeling with extreme
points [76, 71]. As SAM improved, the average number of
masks per image increased from 20 to 44 masks. Overall,
we collected 4.3M masks from 120k images in this stage.
Semi-automatic stage. In this stage, we aimed to increase
thediversity of masks in order to improve our model’s
ability to segment anything. To focus annotators on less
prominent objects, we ﬁrst automatically detected conﬁdent
masks. Then we presented annotators with images preﬁlled
with these masks and asked them to annotate any additional
unannotated objects. To detect conﬁdent masks, we trained
a bounding box detector [84] on all ﬁrst stage masks using a
generic “object” category. During this stage we collected an
additional 5.9M masks in 180k images (for a total of 10.2M
masks). As in the ﬁrst stage, we periodically retrained our
model on newly collected data (5 times). Average annota-
tion time per mask went back up to 34 seconds (excluding
the automatic masks) as these objects were more challeng-
ing to label. The average number of masks per image went
from 44 to 72 masks (including the automatic masks).
Fully automatic stage. In the ﬁnal stage, annotation was
fully automatic . This was feasible due to two major en-
hancements to our model. First, at the start of this stage, we
had collected enough masks to greatly improve the model,
including the diverse masks from the previous stage. Sec-
ond, by this stage we had developed the ambiguity-aware
model, which allowed us to predict valid masks even in am-
biguous cases. Speciﬁcally, we prompted the model with a
3232 regular grid of points and for each point predicted
a set of masks that may correspond to valid objects. With
the ambiguity-aware model, if a point lies on a part or sub-
part, our model will return the subpart, part, and whole ob-
ject. The IoU prediction module of our model is used to se-
lectconﬁdent masks; moreover, we identiﬁed and selected
only stable masks (we consider a mask stable if threshold-
ing the probability map at 0:5 and0:5 +results in
similar masks). Finally, after selecting the conﬁdent and
stable masks, we applied non-maximal suppression (NMS)
to ﬁlter duplicates. To further improve the quality of smaller
masks, we also processed multiple overlapping zoomed-in
image crops. For further details of this stage, see §B. We
applied fully automatic mask generation to all 11M images
in our dataset, producing a total of 1.1B high-quality masks.
We describe and analyze the resulting dataset, SA-1B, next.
Figure 5: Image-size normalized mask center distributions.
5. Segment Anything Dataset
Our dataset, SA-1B, consists of 11M diverse, high-
resolution, licensed, and privacy protecting images and
1.1B high-quality segmentation masks collected with our
data engine. We compare SA-1B with existing datasets
and analyze mask quality and properties. We are releasing
SA-1B to aid future development of foundation models for
computer vision. We note that SA-1B will be released un-
der a favorable license agreement for certain research uses
and with protections for researchers.
Images . We licensed a new set of 11M images from a
provider that works directly with photographers. These im-
ages are high resolution (3300 4950 pixels on average),
and the resulting data size can present accessibility and stor-
age challenges. Therefore, we are releasing downsampled
images with their shortest side set to 1500 pixels. Even af-
ter downsampling, our images are signiﬁcantly higher reso-
lution than many existing vision datasets ( e.g., COCO [66]
images are 480640 pixels). Note that most models today
operate on much lower resolution inputs. Faces and vehicle
license plates have been blurred in the released images.
Masks . Our data engine produced 1.1B masks, 99.1% of
which were generated fully automatically. Therefore, the
quality of the automatic masks is centrally important. We
compare them directly to professional annotations and look
at how various mask properties compare to prominent seg-
mentation datasets. Our main conclusion, as borne out in
the analysis below and the experiments in §7, is that our
automatic masks are high quality and effective for training
models. Motivated by these ﬁndings, SA-1B only includes
automatically generated masks.
Mask quality. To estimate mask quality, we randomly sam-
pled 500 images ( 50k masks) and asked our professional
annotators to improve the quality of all masks in these im-
ages. Annotators did so using our model and pixel-precise
“brush” and “eraser” editing tools. This procedure resulted
in pairs of automatically predicted and professionally cor-
rected masks. We computed IoU between each pair and
found that 94% of pairs have greater than 90% IoU (and
97% of pairs have greater than 75% IoU). For comparison,
prior work estimates inter-annotator consistency at 85-91%
IoU [44, 60]. Our experiments in §7 conﬁrm by human rat-
ings that mask quality is high relative to a variety of datasets
and that training our model on automatic masks is nearly as
good as using all masks produced by the data engine.
6SA-1B
11M images
1129M (1.1B) masksLVIS v1
0.120M images
1.5M masksCOCO
0.123M images
0.9M masksADE20K
0.028M images
0.7M masksOpen Images
1M images
2.7M masks
<10 11-50 51-100 101-200 >200
Number of masks per image04080Percent of images
0.00 0.25 0.50 0.75
Relative segmentation mask size100
10−2Percent of masks
0.0 0.2 0.4 0.6 0.8
Concavity051015Percent of masksFigure 6: Dataset mask properties. The legend references the number of images and masks in each dataset. Note, that SA-1B
has 11more images and 400 more masks than the largest existing segmentation dataset Open Images [60].
Per country
image count
≥ 100k
< 100k
< 10k
< 1k
RUS
THA
USA
ITA
GBR
DEU
ESP
IDN
UKR
FRA
JPN
MYS
TUR
IND
CHN
POL
NLD
VNM
BRA
CAN
GRC
AUS
PRT
CZE
BLR
ROU
KOR
ARE
AUT
SWE
TWN
HKG
CHE
ISR
SGP
HUN
BEL
HRV
BGR
PHL
KAZ
MEX
NOR
MMR
ZAF
SRB
DNK
MAR
FIN
LVA
50 most common countries (ISO codes)0200k400k600k800kNumber of images per countryAsia & Oceania
Africa
Europe
North America
Latin America & Caribbean
Figure 7: Estimated geographic distribution of SA-1B images. Most of the world’s countries have more than 1000 images in
SA-1B, and the three countries with the most images are from different parts of the world.
Mask properties. In Fig. 5 we plot the spatial distribution
of object centers in SA-1B compared to the largest existing
segmentation datasets. Common photographer biases are
present in all datasets. We observe that SA-1B has greater
coverage of image corners compared to LVIS v1 [44] and
ADE20K [117], the two most similarly distributed datasets,
while COCO [66] and Open Images V5 [60] have a more
prominent center bias. In Fig. 6 (legend) we compare these
datasets by size. SA-1B has 11 more images and 400 
more masks than the second largest, Open Images. On av-
erage, it has 36more masks per image than Open Images.
The closest dataset in this respect, ADE20K, still has 3.5 
fewer masks per image. Fig. 6 (left) plots the masks-per-
image distribution. Next, we look at image-relative mask
size (square root of the mask area divided by image area)
in Fig. 6 (middle). As expected, since our dataset has more
masks per image, it also tends to include a greater percent-
age of small and medium relative-size masks. Finally, to
analyze shape complexity, we look at mask concavity (1
minus mask area divided by area of mask’s convex hull) in
Fig. 6 (right). Since shape complexity is correlated with
mask size, we control for the datasets’ mask size distribu-
tions by ﬁrst performing stratiﬁed sampling from binned
mask sizes. We observe that the concavity distribution of
our masks is broadly similar to that of other datasets.
6. Segment Anything RAI Analysis
We next perform a Responsible AI (RAI) analysis of our
work by investigating potential fairness concerns and bi-
ases when using SA-1B and SAM. We focus on the geo-
graphic and income distribution of SA-1B and fairness of
SAM across protected attributes of people. We also provide
dataset, data annotation, and model cards in §F.SA-1B % images
# countries #imgs #masks SA-1B COCO O.I.
Africa 54 300k 28M 2.8% 3.0% 1.7%
Asia & Oceania 70 3.9M 423M 36.2% 11.4% 14.3%
Europe 47 5.4M 540M 49.8% 34.2% 36.2%
Latin America & Carib. 42 380k 36M 3.5% 3.1% 5.0%
North America 4830k 80M 7.7% 48.3% 42.8%
high income countries 81 5.8M 598M 54.0% 89.1% 87.5%
middle income countries 108 4.9M 499M 45.0% 10.5% 12.0%
low income countries 28 100k 9.4M 0.9% 0.4% 0.5%
Table 1: Comparison of geographic and income representa-
tion. SA-1B has higher representation in Europe and Asia &
Oceania as well as middle income countries. Images from
Africa, Latin America & Caribbean, as well as low income
countries, are underrepresented in all datasets.
Geographic and income representation. We infer the
country images were photographed in using standard meth-
ods (see §C). In Fig. 7 we visualize the per-country image
counts in SA-1B (left) and the 50 countries with the most
images (right). We note that the top-three countries are
from different parts of the world. Next, in Table 1 we com-
pare the geographic and income representation of SA-1B,
COCO [66], and Open Images [60]. SA-1B has a substan-
tially higher percentage of images in Europe and Asia &
Oceania as well as in middle income countries. All datasets
underrepresent Africa as well as low income countries. We
note that in SA-1B, all regions, including Africa, have at
least 28 million masks, 10 more than the total number of
masks of any previous dataset. Finally, we observe that the
average number of masks per image (not shown) is fairly
consistent across region and income (94-108 per image).
7mIoU at
1 point 3 points
perceived gender presentation
feminine 54.4 1.7 90.40.6
masculine 55.7 1.7 90.10.6
perceived age group
older 62.9 6.7 92.61.3
middle 54.5 1.3 90.20.5
young 54.2 2.2 91.20.7mIoU at
1 point 3 points
perceived skin tone
1 52.9 2.2 91.00.9
2 51.5 1.4 91.10.5
3 52.2 1.9 91.40.7
4 51.5 2.7 91.71.0
5 52.4 4.2 92.51.4
6 56.7 6.3 91.22.4
Table 2: SAM’s performance segmenting people across per-
ceived gender presentation, age group, and skin tone. 95%
conﬁdence intervals are shown. Within each grouping, all
conﬁdence intervals overlap except older vs. middle.
Fairness in segmenting people. We investigate potential
fairness concerns across perceived gender presentation, per-
ceived age group, and perceived skin tone by measuring
the performance discrepancy of SAM between groups. We
use the More Inclusive Annotations for People (MIAP) [87]
dataset for gender presentation and age and a proprietary
dataset for skin tone (see §C). Our evaluation uses simu-
lated interactive segmentation with random sampling of 1
and 3 points (see §D). Table 2 (top left) shows results for
perceived gender presentation. We note that females have
been shown to be underrepresented in detection and seg-
mentation datasets [115], but observe that SAM performs
similarly across groups. We repeat the analysis for per-
ceived age in Table 2 (bottom left), noting that those who
are perceived to be younger and older have been shown to
be underrepresented in large-scale datasets [110]. SAM per-
forms best on those who are perceived older (although the
conﬁdence interval is large). Finally, we repeat the anal-
ysis for perceived skin tone in Table 2 (right), noting that
those with lighter apparent skin tones have been shown to
be overrepresented and those with darker skin tones under-
represented in large-scale datasets [110]. As MIAP does
not contain perceived skin tone annotations, we use a pro-
prietary dataset that contains annotations for the perceived
Fitzpatrick skin type [36], which ranges from 1 (lightest
skin tone) to 6 (darkest skin tone). While the means vary
somewhat, we do not ﬁnd a signiﬁcant difference across
groups. We believe our ﬁndings stem from the nature of
the task, and acknowledge biases may arise when SAM is
used as a component in larger systems. Finally, in §C we
extend the analysis to segmenting clothing where we ﬁnd
an indication of bias across perceived gender presentation.
7. Zero-Shot Transfer Experiments
In this section, we present zero-shot transfer experiments
with SAM, the Segment Anything Model. We consider ﬁve
tasks, four of which differ signiﬁcantly from the promptable
segmentation task used to train SAM. These experiments
evaluate SAM on datasets and tasks that were not seen dur-ing training (our usage of “zero-shot transfer” follows its
usage in CLIP [82]). The datasets may include novel image
distributions, such as underwater or ego-centric images ( e.g.
Fig. 8) that, to our knowledge, do not appear in SA-1B.
Our experiments begin by testing the core goal of
promptable segmentation: producing a valid mask from any
prompt. We emphasize the challenging scenario of a single
foreground point prompt, since it is more likely to be am-
biguous than other more speciﬁc prompts. Next, we present
a sequence of experiments that traverse low, mid, and high-
level image understanding and roughly parallel the histori-
cal development of the ﬁeld. Speciﬁcally, we prompt SAM
to (1) perform edge detection, (2) segment everything, i.e.
object proposal generation, (3) segment detected objects,
i.e. instance segmentation, and (4), as a proof-of-concept, to
segment objects from free-form text. These four tasks dif-
fer signiﬁcantly from the promptable segmentation task that
SAM was trained on and are implemented via prompt engi-
neering. Our experiments conclude with an ablation study.
Implementation. Unless otherwise speciﬁed: (1) SAM
uses an MAE [47] pre-trained ViT-H [33] image encoder
and (2) SAM was trained on SA-1B, noting that this dataset
includes only automatically generated masks from the ﬁnal
stage of our data engine. For all other model and training
details, such as hyperparameters, refer to §A.
7.1. Zero-Shot Single Point Valid Mask Evaluation
Task. We evaluate segmenting an object from a single fore-
ground point. This task is ill-posed as one point can refer
to multiple objects. Ground truth masks in most datasets
do not enumerate allpossible masks, which can make au-
tomatic metrics unreliable. Therefore, we supplement the
standard mIoU metric ( i.e., the mean of all IoUs between
predicted and ground truth masks) with a human study in
which annotators rate mask quality from 1 (nonsense) to 10
(pixel-perfect). See §D.1, §E, and §G for additional details.
By default, we sample points from the “center” of ground
truth masks (at a maximal value of the mask’s interior dis-
tance transform), following the standard evaluation proto-
col in interactive segmentation [92]. Since SAM is capable
of predicting multiple masks, we evaluate only the model’s
most conﬁdent mask by default. The baselines are all
single-mask methods. We compare mainly to RITM [92],
a strong interactive segmenter that performs best on our
benchmark compared to other strong baselines [67, 18].
Datasets. We use a newly compiled suite of 23 datasets
with diverse image distributions. Fig. 8 lists the datasets
and shows a sample from each one (see appendix Table 7 for
more details). We use all 23 datasets for mIoU evaluation.
For the human study, we use the subset listed in Fig. 9b
(due to the resource requirements of such studies). This
subset includes both datasets for which SAM outperforms
and underperforms RITM according to automatic metrics.
8ADE20K [117] BBBC038v1 [12] Cityscapes [25] DOORS [80] DRAM [24] EgoHOS [113] GTEA [34, 63] Hypersim [86]
IBD [17] iShape [111] LVIS [44] NDD20 [100] NDISPark [22, 23] OVIS [81] PPDLS [74] Plittersdorf [46]
STREETS [91] TimberSeg [38] TrashCan [52] VISOR [28, 27] WoodScape [112] PIDRay [104] ZeroWaste-f [6]
Figure 8: Samples from the 23 diverse segmentation datasets used to evaluate SAM’s zero-shot transfer capabilities.
-20 0 +20 +40
IoU delta at 1 center pointGTEA [34, 63]TrashCan [52]DRAM [24]PIDRay [104]Cityscapes [25]WoodScape [112]IBD [17]EgoHOS [113]Plittersdorf [46]VISOR [28, 27]NDISPark [22, 23]Hypersim [86]OVIS [81]ADE20K [117]iShape [111]ZeroWaste-f [6]STREETS [91]LVIS [44]NDD20 [100]TimberSeg [38]DOORS [80]BBBC038v1 [12]PPDLS [74]
-21.4-15.0-6.5-5.8-2.0-0.6-0.3+0.8+1.5+1.8+2.7+6.1+7.0+7.8+8.8+9.1+17.3+18.5+21.1+28.9+41.1+44.7+46.9
(a) SAM vs. RITM [92] on 23 datasets
LVIS VISOR DRAM IBD NDD20 OVIS iShape
Datasets579Avg. mask ratingGround Truth
SAM
SAM - single output
RITM
(b) Mask quality ratings by human annotators
123 5 9
Number of points5075mIoU (23 datasets)
SAM (oracle)
SAM
RITM
SimpleClick
FocalClick
123 5 9
Number of points5075mIoU (23 datasets)
SAM (oracle)
(c) Center points (default) (d) Random points
Figure 9: Point to mask evaluation on 23 datasets. (a) Mean IoU of SAM and the strongest single point segmenter, RITM [92].
Due to ambiguity, a single mask may not match ground truth; circles show “oracle” results of the most relevant of SAM’s 3
predictions. (b) Per-dataset comparison of mask quality ratings by annotators from 1 (worst) to 10 (best). All methods use
the ground truth mask center as the prompt. (c, d) mIoU with varying number of points. SAM signiﬁcantly outperforms prior
interactive segmenters with 1 point and is on par with more points. Low absolute mIoU at 1 point is the result of ambiguity.
Results. First, we look at automatic evaluation on the full
suite of 23 datasets using mIoU. We compare per-dataset
results in Fig. 9a against RITM. SAM yields higher re-
sults on 16 of the 23 datasets, by as much as 47 IoU. We
also present an “oracle” result, in which the most relevant
of SAM’s 3 masks is selected by comparing them to the
ground truth, rather than selecting the most conﬁdent mask.
This reveals the impact of ambiguity on automatic evalu-
ation. In particular, with the oracle to perform ambiguity
resolution, SAM outperforms RITM on alldatasets.
Results of the human study are presented in Fig. 9b. Er-
ror bars are 95% conﬁdence intervals for mean mask rat-
ings (all differences are signiﬁcant; see §E for details). We
observe that the annotators consistently rate the quality of
SAM’s masks substantially higher than the strongest base-
line, RITM. An ablated, “ambiguity-unaware” version of
SAM with a single output mask has consistently lower rat-
ings, though still higher than RITM. SAM’s mean ratingsfall between 7 and 9, which corresponds to the qualitative
rating guideline: “ A high score (7-9): The object is identi-
ﬁable and errors are small and rare ( e.g., missing a small,
heavily obscured disconnected component, ...). ” These re-
sults indicate that SAM has learned to segment valid masks
from a single point. Note that for datasets like DRAM and
IBD, where SAM is worse on automatic metrics, it receives
consistently higher ratings in the human study .
Fig. 9c shows additional baselines, SimpleClick [67] and
FocalClick [18], which obtain lower single point perfor-
mance than RITM and SAM. As the number of points in-
creases from 1 to 9, we observe that the gap between meth-
ods decreases. This is expected as the task becomes easier;
also, SAM is not optimized for the very high IoU regime.
Finally, in Fig. 9d we replace the default center point sam-
pling with random point sampling. We observe that the gap
between SAM and the baselines grows and SAM is able to
achieve comparable results under either sampling method.
9image ground truth SAM
Figure 10: Zero-shot edge prediction on BSDS500. SAM
was not trained to predict edge maps nor did it have access
to BSDS images or annotations during training.
method year ODS OIS AP R50
HED [108] 2015 .788 .808 .840 .923
EDETR [79] 2022 .840 .858 .896 .930
zero-shot transfer methods:
Sobel ﬁlter 1968 .539 - - -
Canny [13] 1986 .600 .640 .580 -
Felz-Hutt [35] 2004 .610 .640 .560 -
SAM 2023 .768 .786 .794 .928
Table 3: Zero-shot transfer to edge detection on BSDS500.
7.2. Zero-Shot Edge Detection
Approach. We evaluate SAM on the classic low-level task
of edge detection using BSDS500 [72, 3]. We use a sim-
pliﬁed version of our automatic mask generation pipeline.
Speciﬁcally, we prompt SAM with a 16 16 regular grid of
foreground points resulting in 768 predicted masks (3 per
point). Redundant masks are removed by NMS. Then, edge
maps are computed using Sobel ﬁltering of unthresholded
mask probability maps and standard lightweight postpro-
cessing, including edge NMS (see §D.2 for details).
Results. We visualize representative edge maps in Fig. 10
(see Fig. 15 for more). Qualitatively, we observe that even
though SAM was not trained for edge detection, it produces
reasonable edge maps. Compared to the ground truth, SAM
predicts more edges, including sensible ones that are not an-
notated in BSDS500. This bias is reﬂected quantitatively in
Table 3: recall at 50% precision (R50) is high, at the cost of
precision. SAM naturally lags behind state-of-the-art meth-
ods that learn the biases of BSDS500, i.e., which edges to
suppress. Nevertheless, SAM performs well compared to
pioneering deep learning methods such as HED [108] (also
trained on BSDS500) and signiﬁcantly better than prior,
though admittedly outdated, zero-shot transfer methods.
7.3. Zero-Shot Object Proposals
Approach. Next, we evaluate SAM on the mid-level task
of object proposal generation [2, 102]. This task has played
an important role in object detection research, serving as anmask AR@1000
method all small med. large freq. com. rare
ViTDet-H [62] 63.0 51.7 80.8 87.0 63.1 63.3 58.3
zero-shot transfer methods:
SAM – single out. 54.9 42.8 76.7 74.4 54.7 59.8 62.0
SAM 59.3 45.5 81.6 86.9 59.1 63.9 65.8
Table 4: Object proposal generation on LVIS v1. SAM is
applied zero-shot, i.e. it was not trained for object proposal
generation nor did it access LVIS images or annotations.
intermediate step in pioneering systems ( e.g., [102, 41, 84]).
To generate object proposals, we run a slightly modiﬁed
version of our automatic mask generation pipeline and out-
put the masks as proposals (see §D.3 for details).
We compute the standard average recall (AR) metric on
LVIS v1 [44]. We focus on LVIS because its large number
of categories presents a challenging test. We compare to
astrong baseline implemented as a ViTDet [62] detector
(with cascade Mask R-CNN [48, 11] ViT-H). We note that
this “baseline” corresponds to the “Detector Masquerading
as Proposal generator” (DMP) method [16] that was shown
to game AR, making it a truly demanding comparison.
Results. In Table 4 we see unsurprisingly that using the
detections from ViTDet-H as object proposals ( i.e., the
DMP method [16] that games AR) performs the best over-
all. However, SAM does remarkably well on several met-
rics. Notably, it outperforms ViTDet-H on medium and
large objects, as well as rare and common objects. In fact,
SAM only underperforms ViTDet-H on small objects and
frequent objects, where ViTDet-H can easily learn LVIS-
speciﬁc annotation biases since it was trained on LVIS, un-
like SAM. We also compare against an ablated ambiguity-
unaware version of SAM (“single out.”), which performs
signiﬁcantly worse than SAM on all AR metrics.
7.4. Zero-Shot Instance Segmentation
Approach. Moving to higher-level vision, we use SAM
as the segmentation module of an instance segmenter. The
implementation is simple: we run a object detector (the
ViTDet used before) and prompt SAM with its output
boxes. This illustrates composing SAM in a larger system.
Results. We compare the masks predicted by SAM and
ViTDet on COCO and LVIS in Table 5. Looking at the
mask AP metric we observe gaps on both datasets, where
SAM is reasonably close, though certainly behind ViTDet.
By visualizing outputs, we observed that SAM masks are
often qualitatively better than those of ViTDet, with crisper
boundaries (see §D.4 and Fig. 16). To investigate this ob-
servation, we conducted an additional human study asking
annotators to rate the ViTDet masks and SAM masks on the
1 to 10 quality scale used before. In Fig. 11 we observe that
SAM consistently outperforms ViTDet in the human study.
10COCO [66] LVIS v1 [44]
method AP APSAPMAPLAP APSAPMAPL
ViTDet-H [62] 51.0 32.0 54.3 68.9 46.6 35.0 58.0 66.3
zero-shot transfer methods (segmentation module only):
SAM 46.5 30.8 51.0 61.7 44.7 32.5 57.6 65.5
Table 5: Instance segmentation results. SAM is prompted
with ViTDet boxes to do zero-shot segmentation. The fully-
supervised ViTDet outperforms SAM, but the gap shrinks
on the higher-quality LVIS masks. Interestingly, SAM out-
performs ViTDet according to human ratings (see Fig. 11).
1 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings8.6 ± 0.06, LVIS GT
8.1 ± 0.07, SAM
7.9 ± 0.08, ViTDet-H
7.6 ± 0.12, COCO GT
Figure 11: Mask quality rating distribution from our human
study for ViTDet and SAM, both applied to LVIS ground
truth boxes. We also report LVIS and COCO ground truth
quality. The legend shows rating means and 95% conﬁ-
dence intervals. Despite its lower AP (Table 5), SAM has
higher ratings than ViTDet, suggesting that ViTDet exploits
biases in the COCO and LVIS training data.
We hypothesize that on COCO, where the mask AP gap
is larger and the ground truth quality is relatively low (as
borne out by the human study), ViTDet learns the speciﬁc
biases of COCO masks. SAM, being a zero-shot method,
is unable to exploit these (generally undesirable) biases.
The LVIS dataset has higher quality ground truth, but there
are still speciﬁc idiosyncrasies ( e.g., masks do not contain
holes, they are simple polygons by construction) and biases
for modal vs. amodal masks. Again, SAM is not trained to
learn these biases, while ViTDet can exploit them.
7.5. Zero-Shot Text-to-Mask
Approach. Finally, we consider an even higher-level task:
segmenting objects from free-form text. This experiment
is a proof-of-concept of SAM’s ability to process text
prompts. While we used the exact same SAM in all prior
experiments, for this one SAM’s training procedure is mod-
iﬁed to make it text-aware, but in a way that does not require
new text annotations. Speciﬁcally, for each manually col-
lected mask with area larger than 1002we extract the CLIP
image embedding. Then, during training, we prompt SAM
with the extracted CLIP image embeddings as its ﬁrst in-
teraction. The key observation here is that because CLIP’s
image embeddings are trained to align with its textembed-
dings, we can train with image embeddings, but use text
embeddings for inference. That is, at inference time we run
text through CLIP’s text encoder and then give the resulting
text embedding as a prompt to SAM (see §D.5 for details).
     “a wheel”
3
     “beaver tooth grille” 3
     “a wiper”
7
     “a wiper” + point
 3
     “wipers”
7
     “wipers” + point
 3
Figure 12: Zero-shot text-to-mask. SAM can work with
simple and nuanced text prompts. When SAM fails to make
a correct prediction, an additional point prompt can help.
Results. We show qualitative results in Fig. 12. SAM
can segment objects based on simple text prompts like “a
wheel” as well as phrases like “beaver tooth grille”. When
SAM fails to pick the right object from a text prompt only,
an additional point often ﬁxes the prediction, similar to [31].
7.6. Ablations
We perform several ablations on our 23 dataset suite with
the single center point prompt protocol. Recall that a sin-
gle point may be ambiguous and that ambiguity may not
be represented in the ground truth, which contains only a
single mask per point. Since SAM is operating in a zero-
shot transfer setting there can be systematic biases between
SAM’s top-ranked mask vs. the masks resulting from data
annotation guidelines. We therefore additionally report the
best mask with respect to the ground truth (“oracle”).
Fig. 13 (left) plots SAM’s performance when trained on
cumulative data from the data engine stages. We observe
that each stage increases mIoU. When training with all three
stages, the automatic masks vastly outnumber the manual
and semi-automatic masks. To address this, we found that
oversampling the manual and semi-automatic masks during
training by 10gave best results. This setup complicates
training. We therefore tested a fourth setup that uses only
the automatically generated masks. With this data, SAM
performs only marginally lower than using all data ( 0.5
mIoU). Therefore, by default we use only the automatically
generated masks to simplify the training setup.
In Fig. 13 (middle) we look at the impact of data volume.
The full SA-1B contains 11M images, which we uniformly
subsample to 1M and 0.1M for this ablation. At 0.1M im-
ages, we observe a large mIoU decline under all settings.
However, with 1M images, about 10% of the full dataset,
we observe results comparable to using the full dataset.
This data regime, which still includes approximately 100M
masks, may be a practical setting for many use cases.
11manual + semi
automatic+ automatic automatic
only
Training data stages506070mIoU (23 datasets)1 point (oracle)
1 point
0.1M 1M 11M
Training images707580mIoU (23 datasets)
1 point (oracle)
2 points3 points5 points
91M
ViT-B308M
ViT-L636M
ViT-H
Number of parameters606570mIoU (23 datasets)
1 point (oracle)
1 point
Figure 13: Ablation studies of our data engine stages, image encoder scaling, and training data scaling. (Left) Each data
engine stage leads to improvements on our 23 dataset suite, and training with only the automatic data (our default) yields
similar results to using data from all three stages. (Middle) SAM trained with 10% of SA-1B and full SA-1B is comparable.
We train with all 11M images by default, but using 1M images is a reasonable practical setting. (Right) Scaling SAM’s image
encoder shows meaningful, yet saturating gains. Nevertheless, smaller image encoders may be preferred in certain settings.
Finally, Fig. 13 (right) shows results with ViT-B, ViT-L,
and ViT-H image encoders. ViT-H improves substantially
over ViT-B, but has only marginal gains over ViT-L. Further
image encoder scaling does not appear fruitful at this time.
8. Discussion
Foundation models. Pre-trained models have been adapted
to downstream tasks since the early days of machine learn-
ing [99]. This paradigm has become increasingly impor-
tant in recent years with a growing emphasis on scale, and
such models have recently been (re-)branded as “founda-
tion models”: i.e. models that are “trained on broad data
at scale and are adaptable to a wide range of downstream
tasks” [8]. Our work correlates well with this deﬁnition,
though we note that a foundation model for image segmen-
tation is an inherently limited scope, since it represents an
important, yet fractional, subset of computer vision. We
also contrast one aspect of our approach with [8], which
emphasizes the role of self-supervised learning in founda-
tion models. While our model is initialized with a self-
supervised technique (MAE [47]), the vast majority of its
capabilities come from large-scale supervised training. In
cases where data engines can scale available annotations,
like ours, supervised training provides an effective solution.
Compositionality. Pre-trained models can power new ca-
pabilities even beyond ones imagined at the moment of
training. One prominent example is how CLIP [82] is used
as a component in larger systems, such as DALL E [83].
Our goal is to make this kind of composition straightfor-
ward with SAM. We aim to achieve this by requiring SAM
to predict a valid mask for a wide range of segmentation
prompts. The effect is to create a reliable interface between
SAM and other components. For example, MCC [106] can
easily use SAM to segment an object of interest and achieve
strong generalization to unseen objects for 3D reconstruc-
tion from a single RGB-D image. In another example, SAM
can be prompted with gaze points detected by a wearable
device, enabling new applications. Thanks to SAM’s abil-
ity to generalize to new domains like ego-centric images,
such systems work without need for additional training.Limitations. While SAM performs well in general, it is
not perfect. It can miss ﬁne structures, hallucinates small
disconnected components at times, and does not produce
boundaries as crisply as more computationally intensive
methods that “zoom-in”, e.g. [18]. In general, we expect
dedicated interactive segmentation methods to outperform
SAM when many points are provided, e.g. [67]. Unlike
these methods, SAM is designed for generality and breadth
of use rather than high IoU interactive segmentation. More-
over, SAM can process prompts in real-time, but neverthe-
less SAM’s overall performance is not real-time when using
a heavy image encoder. Our foray into the text-to-mask task
is exploratory and not entirely robust, although we believe
it can be improved with more effort. While SAM can per-
form many tasks, it is unclear how to design simple prompts
that implement semantic and panoptic segmentation. Fi-
nally, there are domain-speciﬁc tools, such as [7], that we
expect to outperform SAM in their respective domains.
Conclusion. The Segment Anything project is an attempt to
lift image segmentation into the era of foundation models.
Our principal contributions are a new task (promptable seg-
mentation), model (SAM), and dataset (SA-1B) that make
this leap possible. Whether SAM achieves the status of a
foundation model remains to be seen by how it is used in
the community, but regardless we expect the perspective of
this work, the release of over 1B masks, and our promptable
segmentation model will help pave the path ahead.
Acknowledgments. We would like to thank Aaron Ad-
cock and Jitendra Malik for helpful discussion. We thank
Vaibhav Aggarwal and Yanghao Li for help with scal-
ing the model. We thank Cheng-Yang Fu, Jiabo Hu, and
Robert Kuo for help with data annotation platform. We
thank Allen Goodman and Bram Wasti for help in optimiz-
ing web-version of our model. Finally, we thank Morteza
Behrooz, Ashley Gabriel, Ahuva Goldstand, Sumanth Gur-
ram, Somya Jain, Devansh Kukreja, Joshua Lane, Lilian
Luong, Mallika Malhotra, William Ngan, Omkar Parkhi,
Nikhil Raina, Dirk Rowe, Neil Sejoor, Vanessa Stark, Bala
Varadarajan, and Zachary Winstrom for their help in mak-
ing the demo, dataset viewer, and other assets and tooling.
12References
[1] Edward H Adelson. On seeing stuff: the perception of materials by
humans and machines. Human vision and electronic imaging VI ,
2001. 5
[2] Bogdan Alexe, Thomas Deselaers, and Vittorio Ferrari. What is an
object? CVPR , 2010. 4, 10
[3] Pablo Arbel ´aez, Michael Maire, Charless Fowlkes, and Jitendra
Malik. Contour detection and hierarchical image segmentation.
TPAMI , 2010. 4, 10, 21, 28
[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer
normalization. arXiv:1607.06450 , 2016. 16
[5] Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-training of
image transformers. arXiv:2106.08254 , 2021. 17
[6] Dina Bashkirova, Mohamed Abdelfattah, Ziliang Zhu, James Akl,
Fadi Alladkani, Ping Hu, Vitaly Ablavsky, Berk Calli, Sarah Adel
Bargal, and Kate Saenko. ZeroWaste dataset: Towards deformable
object segmentation in cluttered scenes. CVPR , 2022. 9, 20
[7] Stuart Berg, Dominik Kutra, Thorben Kroeger, Christoph N.
Straehle, Bernhard X. Kausler, Carsten Haubold, Martin Schiegg,
Janez Ales, Thorsten Beier, Markus Rudy, Kemal Eren, Jaime I.
Cervantes, Buote Xu, Fynn Beuttenmueller, Adrian Wolny, Chong
Zhang, Ullrich Koethe, Fred A. Hamprecht, and Anna Kreshuk.
ilastik: interactive machine learning for (bio)image analysis. Na-
ture Methods , 2019. 12
[8] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman,
Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette
Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportu-
nities and risks of foundation models. arXiv:2108.07258 , 2021. 1,
12
[9] Gustav Bredell, Christine Tanner, and Ender Konukoglu. Iterative
interaction training for segmentation editing networks. MICCAI ,
2018. 17
[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav
Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Ben-
jamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei. Language models
are few-shot learners. NeurIPS , 2020. 1, 4
[11] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: Delving into
high quality object detection. CVPR , 2018. 10
[12] Juan C. Caicedo, Allen Goodman, Kyle W. Karhohs, Beth A. Ci-
mini, Jeanelle Ackerman, Marzieh Haghighi, CherKeng Heng, Tim
Becker, Minh Doan, Claire McQuin, Mohammad Rohban, Shan-
tanu Singh, and Anne E. Carpenter. Nucleus segmentation across
imaging experiments: the 2018 data science bowl. Nature Methods ,
2019. 9, 19, 20
[13] John Canny. A computational approach to edge detection. TPAMI ,
1986. 10, 21
[14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end
object detection with Transformers. ECCV , 2020. 5, 16, 17
[15] Guillaume Charpiat, Matthias Hofmann, and Bernhard Sch ¨olkopf.
Automatic image colorization via multimodal predictions. ECCV ,
2008. 5, 17
[16] Neelima Chavali, Harsh Agrawal, Aroma Mahendru, and Dhruv
Batra. Object-proposal evaluation protocol is’ gameable’. CVPR ,
2016. 10, 21
[17] Jiazhou Chen, Yanghui Xu, Shufang Lu, Ronghua Liang, and Lian-
gliang Nan. 3D instance segmentation of MVS buildings. IEEE
Transactions on Geoscience and Remote Sensing , 2022. 9, 19, 20,
23, 24
[18] Xi Chen, Zhiyan Zhao, Yilei Zhang, Manni Duan, Donglian Qi, and
Hengshuang Zhao. FocalClick: towards practical interactive image
segmentation. CVPR , 2022. 8, 9, 12, 19[19] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kir-
illov, and Rohit Girdhar. Masked-attention mask transformer for
universal image segmentation. CVPR , 2022. 4
[20] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-
pixel classiﬁcation is not all you need for semantic segmentation.
NeurIPS , 2021. 5, 16, 17
[21] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won
Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling
language modeling with pathways. arXiv:2204.02311 , 2022. 1
[22] Luca Ciampi, Carlos Santiago, Joao Costeira, Claudio Gennaro, and
Giuseppe Amato. Domain adaptation for trafﬁc density estimation.
International Joint Conference on Computer Vision, Imaging and
Computer Graphics Theory and Applications , 2021. 9, 20
[23] Luca Ciampi, Carlos Santiago, Joao Costeira, Claudio Gennaro, and
Giuseppe Amato. Night and day instance segmented park (NDIS-
Park) dataset: a collection of images taken by day and by night for
vehicle detection, segmentation and counting in parking areas. Zen-
odo, 2022. 9, 20
[24] Nadav Cohen, Yael Newman, and Ariel Shamir. Semantic segmen-
tation in art paintings. Computer Graphics Forum , 2022. 9, 19, 20,
23, 24
[25] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld,
Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth,
and Bernt Schiele. The Cityscapes dataset for semantic urban scene
understanding. CVPR , 2016. 9, 19, 20
[26] Bruno da Silva, George Konidaris, and Andrew Barto. Learning
parameterized skills. ICML , 2012. 4
[27] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino
Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan
Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling
egocentric vision: Collection, pipeline and challenges for EPIC-
KITCHENS-100. IJCV , 2022. 9, 20, 23, 24
[28] Ahmad Darkhalil, Dandan Shan, Bin Zhu, Jian Ma, Amlan Kar,
Richard Higgins, Sanja Fidler, David Fouhey, and Dima Damen.
EPIC-KITCHENS VISOR benchmark: Video segmentations and
object relations. NeurIPS , 2022. 9, 19, 20, 23, 24
[29] Terrance De Vries, Ishan Misra, Changhan Wang, and Laurens
Van der Maaten. Does object recognition work for everyone? CVPR
workshops , 2019. 18
[30] Mark D ´ıaz, Ian Kivlichan, Rachel Rosen, Dylan Baker, Razvan
Amironesei, Vinodkumar Prabhakaran, and Emily Denton. Crowd-
WorkSheets: Accounting for individual and collective identities un-
derlying crowdsourced dataset annotation. ACM Conference on
Fairness, Accountability, and Transparency , 2022. 25
[31] Henghui Ding, Scott Cohen, Brian Price, and Xudong Jiang.
PhraseClick: toward achieving ﬂexible interactive segmentation by
phrase and click. ECCV , 2020. 11
[32] Piotr Doll ´ar and C Lawrence Zitnick. Fast edge detection using
structured forests. TPAMI , 2014. 21
[33] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk
Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa De-
hghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob
Uszkoreit, and Neil Houlsby. An image is worth 16x16 words:
Transformers for image recognition at scale. ICLR , 2021. 5, 8,
16
[34] Alireza Fathi, Xiaofeng Ren, and James M. Rehg. Learning to rec-
ognize objects in egocentric activities. CVPR , 2011. 9, 19, 20
[35] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efﬁcient graph-
based image segmentation. IJCV , 2004. 10
[36] Thomas B. Fitzpatrick. The validity and practicality of sun-reactive
skin types i through vi. Archives of Dermatology , 1988. 8
[37] Marco Forte, Brian Price, Scott Cohen, Ning Xu, and Franc ¸ois
Piti´e. Getting to 99% accuracy in interactive segmentation.
arXiv:2003.07932 , 2020. 5, 17
[38] Jean-Michel Fortin, Olivier Gamache, Vincent Grondin, Franc ¸ois
Pomerleau, and Philippe Gigu `ere. Instance segmentation for au-
tonomous log grasping in forestry operations. IROS , 2022. 9, 20
13[39] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jen-
nifer Wortman Vaughan, Hanna Wallach, Hal Daum ´e Iii, and Kate
Crawford. Datasheets for datasets. Communications of the ACM ,
2021. 25
[40] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin,
Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple copy-paste is a
strong data augmentation method for instance segmentation. CVPR ,
2021. 16, 18, 22
[41] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.
Rich feature hierarchies for accurate object detection and semantic
segmentation. CVPR , 2014. 10
[42] Priya Goyal, Piotr Doll ´ar, Ross Girshick, Pieter Noordhuis, Lukasz
Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and
Kaiming He. Accurate, large minibatch SGD: Training ImageNet
in 1 hour. arXiv:1706.02677 , 2017. 17
[43] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary
Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger,
Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Na-
garajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona
Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhong-
cong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Car-
tillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli,
Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Chris-
tian Fuegen, Abrham Gebreselasie, Cristina Gonzalez, James Hillis,
Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Ko-
lar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li,
Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Mod-
hugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will
Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran
Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao,
Minh V o, Yuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu,
Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria
Farinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V . Jawahar,
Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude
Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi,
Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei
Yan, and Jitendra Malik. Ego4D: Around the World in 3,000 Hours
of Egocentric Video. CVPR , 2022. 20
[44] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: A dataset for
large vocabulary instance segmentation. CVPR , 2019. 2, 6, 7, 9, 10,
11, 19, 20, 21, 24
[45] Abner Guzman-Rivera, Dhruv Batra, and Pushmeet Kohli. Multiple
choice learning: Learning to produce multiple structured outputs.
NeurIPS , 2012. 5, 17
[46] Timm Haucke, Hjalmar S. K ¨uhl, and V olker Steinhage.
SOCRATES: Introducing depth in visual wildlife monitoring using
stereo vision. Sensors , 2022. 9, 20
[47] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ´ar,
and Ross Girshick. Masked autoencoders are scalable vision learn-
ers.CVPR , 2022. 5, 8, 12, 16, 17
[48] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Girshick.
Mask R-CNN. ICCV , 2017. 10
[49] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
residual learning for image recognition. CVPR , 2016. 16
[50] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units
(gelus). arXiv:1606.08415 , 2016. 16
[51] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena
Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas,
Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training
compute-optimal large language models. arXiv:2203.15556 , 2022.
1
[52] Jungseok Hong, Michael Fulton, and Junaed Sattar. TrashCan: A
semantically-segmented dataset towards visual detection of marine
debris. arXiv:2007.08097 , 2020. 9, 19, 20
[53] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Wein-
berger. Deep networks with stochastic depth. ECCV , 2016. 17
[54] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov,
and Humphrey Shi. Oneformer: One transformer to rule universal
image segmentation. arXiv:2211.06220 , 2022. 4[55] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
Scaling up visual and vision-language representation learning with
noisy text supervision. ICML , 2021. 1
[56] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,
Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey
Wu, and Dario Amodei. Scaling laws for neural language models.
arXiv:2001.08361 , 2020. 1
[57] Michael Kass, Andrew Witkin, and Demetri Terzopoulos. Snakes:
Active contour models. IJCV , 1988. 4
[58] Dahun Kim, Tsung-Yi Lin, Anelia Angelova, In So Kweon, and
Weicheng Kuo. Learning open-world object proposals without
learning to classify. IEEE Robotics and Automation Letters , 2022.
21
[59] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother,
and Piotr Doll ´ar. Panoptic segmentation. CVPR , 2019. 4
[60] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan
Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo
Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari.
The open images dataset v4: Uniﬁed image classiﬁcation, object
detection, and visual relationship detection at scale. IJCV , 2020. 2,
6, 7, 18, 19
[61] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and
Thomas Dandres. Quantifying the carbon emissions of machine
learning. arXiv:1910.09700 , 2019. 28
[62] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Explor-
ing plain vision transformer backbones for object detection. ECCV ,
2022. 5, 10, 11, 16, 21, 23, 24
[63] Yin Li, Zhefan Ye, and James M. Rehg. Delving into egocentric
actions. CVPR , 2015. 9, 20
[64] Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Interactive image
segmentation with latent diversity. CVPR , 2018. 5, 17, 19
[65] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr
Doll´ar. Focal loss for dense object detection. ICCV , 2017. 5, 17
[66] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro
Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence Zitnick. Mi-
crosoft COCO: Common objects in context. ECCV , 2014. 2, 4, 6,
7, 11, 18, 19, 20
[67] Qin Liu, Zhenlin Xu, Gedas Bertasius, and Marc Niethammer. Sim-
pleClick: Interactive image segmentation with simple vision trans-
formers. arXiv:2210.11006 , 2022. 8, 9, 12, 19
[68] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regu-
larization. ICLR , 2019. 17
[69] Cathy H Lucas, Daniel OB Jones, Catherine J Hollyhead, Robert H
Condon, Carlos M Duarte, William M Graham, Kelly L Robinson,
Kylie A Pitt, Mark Schildhauer, and Jim Regetz. Gelatinous zoo-
plankton biomass in the global oceans: geographic variation and
environmental drivers. Global Ecology and Biogeography , 2014.
20
[70] Sabarinath Mahadevan, Paul V oigtlaender, and Bastian Leibe. Iter-
atively trained interactive segmentation. BMVC , 2018. 4, 17
[71] Kevis-Kokitsi Maninis, Sergi Caelles, Jordi Pont-Tuset, and Luc
Van Gool. Deep extreme cut: From extreme points to object seg-
mentation. CVPR , 2018. 6
[72] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik.
A database of human segmented natural images and its applica-
tion to evaluating segmentation algorithms and measuring ecologi-
cal statistics. ICCV , 2001. 10, 21, 28
[73] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-Net:
Fully convolutional neural networks for volumetric medical image
segmentation. 3DV, 2016. 5, 17
[74] Massimo Minervini, Andreas Fischbach, Hanno Scharr, and
Sotirios A. Tsaftaris. Finely-grained annotated datasets for image-
based plant phenotyping. Pattern Recognition Letters , 2016. 9, 20
[75] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes,
Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Debo-
rah Raji, and Timnit Gebru. Model cards for model reporting. Pro-
ceedings of the conference on fairness, accountability, and trans-
parency , 2019. 25, 28
14[76] Dim P Papadopoulos, Jasper RR Uijlings, Frank Keller, and Vittorio
Ferrari. Extreme clicking for efﬁcient object annotation. ICCV ,
2017. 6
[77] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-
Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and
Jeff Dean. Carbon emissions and large neural network training.
arXiv:2104.10350 , 2021. 28
[78] Matthew E Peters, Waleed Ammar, Chandra Bhagavatula, and Rus-
sell Power. Semi-supervised sequence tagging with bidirectional
language models. Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics , 2017. 18
[79] Mengyang Pu, Yaping Huang, Yuming Liu, Qingji Guan, and
Haibin Ling. EDTER: Edge detection with transformer. CVPR ,
2022. 10
[80] Mattia Pugliatti and Francesco Topputo. DOORS: Dataset fOr
bOuldeRs Segmentation. Zenodo , 2022. 9, 20
[81] Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu, Xiang
Bai, Serge Belongie, Alan Yuille, Philip Torr, and Song Bai. Oc-
cluded video instance segmentation: A benchmark. ICCV , 2022. 9,
20, 23, 24
[82] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,
Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. ICML , 2021. 1, 2, 4, 5,
8, 12, 16, 22
[83] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea
V oss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-
to-image generation. ICML , 2021. 1, 4, 12
[84] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster
R-CNN: Towards real-time object detection with region proposal
networks. NeurIPS , 2015. 6, 10
[85] Xiaofeng Ren and Jitendra Malik. Learning a classiﬁcation model
for segmentation. ICCV , 2003. 4
[86] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar,
Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M.
Susskind. Hypersim: A photorealistic synthetic dataset for holistic
indoor scene understanding. ICCV , 2021. 9, 19, 20
[87] Candice Schumann, Susanna Ricco, Utsav Prabhu, Vittorio Ferrari,
and Caroline Pantofaru. A step toward more inclusive people anno-
tations for fairness. Proceedings of the 2021 AAAI/ACM Conference
on AI, Ethics, and Society , 2021. 8, 19
[88] Seﬁk Ilkin Serengil and Alper Ozpinar. LightFace: A hybrid deep
face recognition framework. ASYU , 2020. 26
[89] Seﬁk Ilkin Serengil and Alper Ozpinar. HyperExtended LightFace:
A facial attribute analysis framework. ICEET , 2021. 26
[90] Jamie Shotton, John Winn, Carsten Rother, and Antonio Crimin-
isi. TextonBoost: Joint appearance, shape and context modeling for
mulit-class object recognition and segmentation. ECCV , 2006. 4
[91] Corey Snyder and Minh Do. STREETS: A novel camera network
dataset for trafﬁc ﬂow. NeurIPS , 2019. 9, 20
[92] Konstantin Soﬁiuk, Ilya A Petrov, and Anton Konushin. Reviving
iterative training with mask guidance for interactive segmentation.
ICIP , 2022. 5, 8, 9, 17, 19, 23, 24, 28
[93] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to
prevent neural networks from overﬁtting. The Journal of Machine
Learning Research , 2014. 16
[94] Chris Stauffer and W Eric L Grimson. Adaptive background mix-
ture models for real-time tracking. CVPR , 1999. 4
[95] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara
Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-
mamoorthi, Jonathan Barron, and Ren Ng. Fourier features let net-
works learn high frequency functions in low dimensional domains.
NeurIPS , 2020. 5, 16
[96] Yansong Tang, Yi Tian, Jiwen Lu, Jianjiang Feng, and Jie Zhou.
Action recognition in RGB-D egocentric videos. ICIP , 2017. 20[97] Yansong Tang, Zian Wang, Jiwen Lu, Jianjiang Feng, and Jie Zhou.
Multi-stream deep neural networks for RGB-D egocentric action
recognition. IEEE Transactions on Circuits and Systems for Video
Technology , 2019. 20
[98] The World Bank. The world by income and regions,
2022. https://datatopics.worldbank.org/world-development-
indicators/the-world-by-income-and-region.html. 18
[99] Sebastian Thrun. Is learning the n-th thing any easier than learning
the ﬁrst? NeurIPS , 1995. 12
[100] Cameron Trotter, Georgia Atkinson, Matt Sharpe, Kirsten Richard-
son, A. Stephen McGough, Nick Wright, Ben Burville, and Per
Berggren. NDD20: A large-scale few-shot dolphin dataset for
coarse and ﬁne-grained categorisation. arXiv:2005.13359 , 2020.
9, 19, 20, 23, 24
[101] United States Environmental Protection Agency. Greenhouse Gas
Equivalencies Calculator. https://www.epa.gov/energy/greenhouse-
gas-equivalencies-calculator, 2022. 28
[102] Koen EA van de Sande, Jasper RR Uijlings, Theo Gevers, and
Arnold WM Smeulders. Segmentation as selective search for ob-
ject recognition. ICCV , 2011. 10
[103] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.
Attention is all you need. NeurIPS , 2017. 5, 16
[104] Boying Wang, Libo Zhang, Longyin Wen, Xianglong Liu, and Yan-
jun Wu. Towards real-world prohibited item detection: A large-
scale x-ray benchmark. CVPR , 2021. 9, 19, 20
[105] Weiyao Wang, Matt Feiszli, Heng Wang, Jitendra Malik, and
Du Tran. Open-world instance segmentation: Exploiting pseudo
ground truth from learned pairwise afﬁnity. CVPR , 2022. 21
[106] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feicht-
enhofer, and Georgia Gkioxari. Multiview compressive coding for
3D reconstruction. CVPR , 2023. 12
[107] Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and An-
tonio Torralba. SUN database: Large-scale scene recognition from
abbey to zoo. CVPR , 2010. 20
[108] Saining Xie and Zhuowen Tu. Holistically-nested edge detection.
ICCV , 2015. 10
[109] Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and Thomas S
Huang. Deep interactive object selection. CVPR , 2016. 4, 19
[110] Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, and Olga Rus-
sakovsky. Towards fairer datasets: Filtering and balancing the dis-
tribution of the people subtree in the imagenet hierarchy. Proceed-
ings of the 2020 conference on fairness, accountability, and trans-
parency , 2020. 8
[111] Lei Yang, Yan Zi Wei, Yisheng HE, Wei Sun, Zhenhang Huang,
Haibin Huang, and Haoqiang Fan. iShape: A ﬁrst step towards
irregular shape instance segmentation. arXiv:2109.15068 , 2021. 9,
20, 23, 24
[112] Senthil Yogamani, Ciar ´an Hughes, Jonathan Horgan, Ganesh Sistu,
Padraig Varley, Derek O’Dea, Michal Uric ´ar, Stefan Milz, Mar-
tin Simon, Karl Amende, et al. WoodScape: A multi-task, multi-
camera ﬁsheye dataset for autonomous driving. ICCV , 2019. 9,
20
[113] Lingzhi Zhang, Shenghao Zhou, Simon Stent, and Jianbo Shi. Fine-
grained egocentric hand-object segmentation: Dataset, model, and
applications. ECCV , 2022. 9, 19, 20
[114] Wenwei Zhang, Jiangmiao Pang, Kai Chen, and Chen Change Loy.
K-Net: Towards uniﬁed image segmentation. NeurIPS , 2021. 4
[115] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-
Wei Chang. Men also like shopping: Reducing gender bias ampli-
ﬁcation using corpus-level constraints. arXiv:1707.09457 , 2017. 8
[116] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and An-
tonio Torralba. Places: A 10 million image database for scene
recognition. TPAMI , 2017. 20
[117] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler,
Adela Barriuso, and Antonio Torralba. Semantic understanding of
scenes through the ADE20K dataset. IJCV , 2019. 2, 7, 9, 20
15Appendix
Table of contents:
• §A: Segment Anything Model and Task Details
• §B: Automatic Mask Generation Details
• §C: RAI Additional Details
• §D: Experiment Implementation Details
• §E: Human Study Experimental Design
• §F: Dataset, Annotation, and Model Cards
• §G: Annotation Guidelines
A. Segment Anything Model and Task Details
Image encoder. In general, the image encoder can be any
network that outputs a CHWimage embedding. Mo-
tivated by scalability and access to strong pre-training, we
use an MAE [47] pre-trained Vision Transformer (ViT) [33]
with minimal adaptations to process high resolution inputs,
speciﬁcally a ViT-H/16 with 14 14 windowed attention
and four equally-spaced global attention blocks, follow-
ing [62]. The image encoder’s output is a 16 downscaled
embedding of the input image. Since our runtime goal is to
process each prompt in real-time, we can afford a high num-
ber of image encoder FLOPs because they are computed
only once per image, notper prompt.
Following standard practices ( e.g., [40]), we use an in-
put resolution of 1024 1024 obtained by rescaling the im-
age and padding the shorter side. The image embedding
is therefore 6464. To reduce the channel dimension, fol-
lowing [62], we use a 1 1 convolution to get to 256 chan-
nels, followed by a 3 3 convolution also with 256 channels.
Each convolution is followed by a layer normalization [4].
Prompt encoder. Sparse prompts are mapped to 256-
dimensional vectorial embeddings as follows. A point is
represented as the sum of a positional encoding [95] of the
point’s location and one of two learned embeddings that in-
dicate if the point is either in the foreground or background.
A box is represented by an embedding pair: (1) the posi-
tional encoding of its top-left corner summed with a learned
embedding representing “top-left corner” and (2) the same
structure but using a learned embedding indicating “bottom-
right corner”. Finally, to represent free-form text we use the
text encoder from CLIP [82] (any text encoder is possible in
general). We focus on geometric prompts for the remainder
of this section and discuss text prompts in depth in §D.5.
Dense prompts ( i.e., masks) have a spatial correspon-
dence with the image. We input masks at a 4 lower res-
olution than the input image, then downscale an additional
4using two 22, stride-2 convolutions with output chan-
nels 4 and 16, respectively. A ﬁnal 1 1 convolution maps
the channel dimension to 256. Each layer is separated by
GELU activations [50] and layer normalization. The mask
image
embedding
(256x64x64)x2
token
to image
attn.2x
conv.
trans.
IoU
scoresmlpmasksdot product
per mask
prompt tokens
(Ntokensx256)output tokens
+output
token
per mask
IoU
output
tokenmlp
mask decoderself attn.token to image attn.mlpimage to token attn.Figure 14: Details of the lightweight mask decoder. A
two-layer decoder updates both the image embedding and
prompt tokens via cross-attention. Then the image embed-
ding is upscaled, from which the updated output tokens are
used to dynamically predict masks. (Not illustrated for ﬁg-
ure clarity: At every attention layer, positional encodings
are added to the image embedding, and the entire original
prompt token (including position encoding) is re-added to
the token queries and keys.)
and image embedding are then added element-wise. If there
is no mask prompt, a learned embedding representing “no
mask” is added to each image embedding location.
Lightweight mask decoder. This module efﬁciently maps
the image embedding and a set of prompt embeddings to an
output mask. To combine these inputs, we take inspiration
from Transformer segmentation models [14, 20] and modify
a standard Transformer decoder [103]. Before applying our
decoder, we ﬁrst insert into the set of prompt embeddings
a learned output token embedding that will be used at the
decoder’s output, analogous to the [class] token in [33].
For simplicity, we refer to these embeddings ( notincluding
the image embedding) collectively as “tokens”.
Our decoder design is shown in Fig. 14. Each decoder
layer performs 4 steps: (1) self-attention on the tokens, (2)
cross-attention from tokens (as queries) to the image em-
bedding, (3) a point-wise MLP updates each token, and (4)
cross-attention from the image embedding (as queries) to
tokens. This last step updates the image embedding with
prompt information. During cross-attention, the image em-
bedding is treated as a set of 642256-dimensional vectors.
Each self/cross-attention and MLP has a residual connec-
tion [49], layer normalization, and a dropout [93] of 0.1 at
training. The next decoder layer takes the updated tokens
and the updated image embedding from the previous layer.
We use a two-layer decoder.
To ensure the decoder has access to critical geometric in-
formation the positional encodings are added to the image
embedding whenever they participate in an attention layer.
Additionally, the entire original prompt tokens (including
their positional encodings) are re-added to the updated to-
kens whenever they participate in an attention layer. This
allows for a strong dependence on both the prompt token’s
geometric location and type.
After running the decoder, we upsample the updated im-
age embedding by 4 with two transposed convolutional
16layers (now it’s downscaled 4 relative to the input image).
Then, the tokens attend once more to the image embedding
and we pass the updated output token embedding to a small
3-layer MLP that outputs a vector matching the channel di-
mension of the upscaled image embedding. Finally, we pre-
dict a mask with a spatially point-wise product between the
upscaled image embedding and the MLP’s output.
The transformer uses an embedding dimension of 256.
The transformer MLP blocks have a large internal dimen-
sion of 2048, but the MLP is applied only to the prompt to-
kens for which there are relatively few (rarely greater than
20). However, in cross-attention layers where we have a
6464 image embedding, we reduce the channel dimension
of the queries, keys, and values by 2 to 128 for computa-
tional efﬁciency. All attention layers use 8 heads.
The transposed convolutions used to upscale the output
image embedding are 2 2, stride 2 with output channel di-
mensions of 64 and 32 and have GELU activations. They
are separated by layer normalization.
Making the model ambiguity-aware. As described, a sin-
gle input prompt may be ambiguous in the sense that it cor-
responds to multiple valid masks, and the model will learn
to average over these masks. We eliminate this problem
with a simple modiﬁcation: instead of predicting a single
mask, we use a small number of output tokens and predict
multiple masks simultaneously. By default we predict three
masks, since we observe that three layers (whole, part, and
subpart) are often enough to describe nested masks. During
training, we compute the loss (described shortly) between
the ground truth and each of the predicted masks, but only
backpropagate from the lowest loss. This is a common tech-
nique used for models with multiple outputs [15, 45, 64].
For use in applications, we’d like to rank predicted masks,
so we add a small head (operating on an additional output
token) that estimates the IoU between each predicted mask
and the object it covers.
Ambiguity is much rarer with multiple prompts and the
three output masks will usually become similar. To mini-
mize computation of degenerate losses at training and en-
sure the single unambiguous mask receives a regular gradi-
ent signal, we only predict a single mask when more than
one prompt is given. This is accomplished by adding a
fourth output token for an additional mask prediction. This
fourth mask is never returned for a single prompt and is the
only mask returned for multiple prompts.
Losses. We supervise mask prediction with a linear combi-
nation of focal loss [65] and dice loss [73] in a 20:1 ratio of
focal loss to dice loss, following [20, 14]. Unlike [20, 14],
we observe that auxiliary deep supervision after each de-
coder layer is unhelpful. The IoU prediction head is trained
with mean-square-error loss between the IoU prediction and
the predicted mask’s IoU with the ground truth mask. It is
added to the mask loss with a constant scaling factor of 1.0.Training algorithm. Following recent approaches [92, 37],
we simulate an interactive segmentation setup during train-
ing. First, with equal probability either a foreground point
or bounding box is selected randomly for the target mask.
Points are sampled uniformly from the ground truth mask.
Boxes are taken as the ground truth mask’s bounding box,
with random noise added in each coordinate with standard
deviation equal to 10% of the box sidelength, to a maxi-
mum of 20 pixels. This noise proﬁle is a reasonable com-
promise between applications like instance segmentation,
which produce a tight box around the target object, and in-
teractive segmentation, where a user may draw a loose box.
After making a prediction from this ﬁrst prompt, subse-
quent points are selected uniformly from the error region
between the previous mask prediction and the ground truth
mask. Each new point is foreground or background if the er-
ror region is a false negative or false positive, respectively.
We also supply the mask prediction from the previous it-
eration as an additional prompt to our model. To provide
the next iteration with maximal information, we supply the
unthresholded mask logits instead of the binarized mask.
When multiple masks are returned, the mask passed to the
next iteration and used to sample the next point is the one
with the highest predicted IoU.
We ﬁnd diminishing returns after 8 iteratively sampled
points (we have tested up to 16). Additionally, to encour-
age the model to beneﬁt from the supplied mask, we also
use two more iterations where no additional points are sam-
pled. One of these iterations is randomly inserted among the
8 iteratively sampled points, and the other is always at the
end. This gives 11 total iterations: one sampled initial in-
put prompt, 8 iteratively sampled points, and two iterations
where no new external information is supplied to the model
so it can learn to reﬁne its own mask predictions. We note
that using a relatively large number of iterations is possible
because our lightweight mask decoder requires less than 1%
of the image encoder’s compute and, therefore, each itera-
tion adds only a small overhead. This is unlike previous
interactive methods that perform only one or a few interac-
tive steps per optimizer update [70, 9, 37, 92].
Training recipe. We use the AdamW [68] optimizer ( 1=
0:9,2= 0:999) and a linear learning rate warmup [42] for
250 iterations and a step-wise learning rate decay schedule.
The initial learning rate ( lr), after warmup, is 8e 4. We
train for 90k iterations ( 2 SA-1B epochs) and decrease the
lrby a factor of 10 at 60k iterations and again at 86666 it-
erations. The batch size is 256 images. To regularize SAM,
we set weight decay ( wd) to 0.1 and apply drop path [53]
(dp) with a rate of 0.4. We use a layer-wise learning rate
decay [5] ( ld) of 0.8. No data augmentation is applied. We
initialize SAM from an MAE [47] pre-trained ViT-H. We
distribute training across 256 GPUs, due to the large image
encoder and 10241024 input size. To limit GPU mem-
17ory usage, we train with up to 64 randomly sampled masks
per GPU. Additionally, we ﬁnd that lightly ﬁltering SA-1B
masks to discard any that cover more than 90% of the image
qualitatively improves results.
For ablations and others variations on training ( e.g., text-
to-mask §D.5), we deviate from the default recipe above as
follows. When training with data from the ﬁrst and sec-
ond data engine stages only, we augment the input with
large-scale jitter [40] with a scale range of [0.1, 2.0]. In-
tuitively, data augmentation may be helpful when training
data is more limited. To train ViT-B and ViT-L, we use
180k iterations with batch size 128 distributed across 128
GPUs. We set lr=8e 4/4e 4,ld= 0.6/0.8, wd= 0.1, and
dp= 0.6/0.4 for ViT-B/L, respectively.
B. Automatic Mask Generation Details
Here we discuss details of the data engine’s fully auto-
matic stage that was used to generate the released SA-1B.
Cropping. Masks were generated from a regular grid of
3232 points on the full image and 20 additional zoomed-
in image crops arising from 2 2 and 44 partially over-
lapping windows using 16 16 and 88 regular point grids,
respectively. The original high-resolution images were used
for cropping (this was the only time we used them). We re-
moved masks that touch the inner boundaries of the crops.
We applied standard greedy box-based NMS (boxes were
used for efﬁciency) in two phases: ﬁrst within each crop and
second across crops. When applying NMS within a crop,
we used the model’s predicted IoU to rank masks. When
applying NMS across crops, we ranked masks from most
zoomed-in ( i.e., from a 44 crop) to least zoomed-in ( i.e.,
the original image), based on their source crop. In both
cases, we used an NMS threshold of 0.7.
Filtering. We used three ﬁlters to increase mask qual-
ity. First, to keep only conﬁdent masks we ﬁltered by the
model’s predicted IoU score at a threshold of 88.0. Second,
to keep only stable masks we compared two binary masks
resulting from the same underlying soft mask by threshold-
ing it at different values. We kept the prediction ( i.e., the
binary mask resulting from thresholding logits at 0) only if
the IoU between its pair of -1 and +1 thresholded masks was
equal to or greater than 95.0. Third, we noticed that occa-
sionally an automatic mask would cover the entire image.
These masks were generally uninteresting, and we ﬁltered
them by removing masks that covered 95% or more of an
image. All ﬁltering thresholds were selected to achieve both
a large number of masks and high mask quality as judged by
professional annotators using the method described in §5.
Postprocessing. We observed two error types that are eas-
ily mitigated with postprocessing. First, an estimated 4%
of masks include small, spurious components. To address
these, we removed connected components with area lessthan 100 pixels (including removing entire masks if the
largest component is below this threshold). Second, another
estimated 4% of masks include small, spurious holes. To
address these, we ﬁlled holes with area less than 100 pixels.
Holes were identiﬁed as components of inverted masks.
Automatic mask generation model. We trained a special
version of SAM for fully automatic mask generation that
sacriﬁces some inference speed for improved mask gener-
ation properties. We note the differences between our de-
fault SAM and the one used for data generation here: it
was trained on manual and semi-automatic data only, it was
trained for longer (177656 iterations instead of 90k) with
large-scale jitter data augmentation [40], simulated interac-
tive training used only point and mask prompts (no boxes)
and sampled only 4 points per mask during training (reduc-
ing from our default of 9 to 4 sped up training iterations
and had no impact on 1-point performance, though it would
harm mIoU if evaluating with more points), and ﬁnally the
mask decoder used 3 layers instead of 2.
SA-1B examples. We show SA-1B samples in Fig. 2. For
more examples, please see our dataset explorer.
C. RAI Additional Details
Inferring geographic information for SA-1B. While the
images in SA-1B are not geo-tagged, each image has a cap-
tion describing its contents and where it was taken. We infer
approximate image geo-locations from these captions using
an Elmo-based named entity recognition model [78]. Each
extracted location entity is mapped to every matching coun-
try, province, and city. Captions are mapped to a single
country by ﬁrst considering the matching countries, then
provinces, and ﬁnally cities. We note that there are ambigu-
ities and potential for biases with this method ( e.g., “Geor-
gia” may refer to the country or the US state). As such, we
use the extracted locations to analyze the dataset as a whole,
but do not release the inferred locations. The captions will
not be released publicly as required by the image provider.
Inferring geographic information for COCO and Open
Images. The COCO [66] and Open Images [60] datasets
do not provide geo-locations. Following [29], we retrieve
geographic metadata using the Flickr API. We retrieved
locations for 24% of the COCO training set (19,562 im-
ages) and for Open Images we retrieved 18% of the train-
ing set (493,517 images, after only considering images with
masks). We note that the geographic information is approx-
imate, and the sample of images with this information may
not fully match the full dataset distribution.
Inferring income information. We use each image’s in-
ferred country to look up its income level using the levels
deﬁned by The World Bank [98]. We collapse the upper-
middle and lower-middle levels into a single middle level.
18mIoU at
1 point 3 points
perceived gender presentation
feminine 76.3 1.1 90.70.5
masculine 81.0 1.2 92.30.4mIoU at
1 point 3 points
perceived age group
older 81.9 3.8 92.81.6
middle 78.2 0.8 91.30.3
young 77.3 2.7 91.50.9
Table 6: SAM’s performance segmenting clothing across
perceived gender presentation and age group. The intervals
for perceived gender are disjoint, with mIoU for masculine
being higher. Conﬁdence intervals for age group overlap.
Fairness in segmenting people. To investigate SAM’s fair-
ness at segmenting people we use the More Inclusive Anno-
tations for People (MIAP) [87] test set annotations for Open
Images [60], which allows us to compare SAM’s perfor-
mance across perceived gender presentation and perceived
age group. MIAP provides box annotations, while we need
ground truth masks for this analysis. To get ground truth
masks, we select each person-category mask from Open
Images if its corresponding bounding box is within a 1%
margin (based on relative box side lengths) of an annotated
bounding box in MIAP, resulting in 3.9k masks.
Fairness in segmenting clothing. We extend our analysis
from §6 to clothing segmentation. We look at SAM’s per-
formance on clothing relative to the attributes of those wear-
ing the clothes. We use all 6.5k ground truth masks from
Open Images that have a category under the clothing super-
class and reside within a person box from MIAP. In Table 6
we compare performance across perceived gender presenta-
tion and age group. We ﬁnd that SAM is better at segment-
ing clothing on those who present predominantly mascu-
line, with disjoint 95% conﬁdence intervals. The gap closes
when moving from 1 to 3 point evaluation. Differences for
perceived age group are not signiﬁcant. Our results indicate
there is a bias when segmenting clothing across perceived
gender presentation with a one point prompt, and we en-
courage users of SAM to be mindful of this limitation.
D. Experiment Implementation Details
D.1. Zero-Shot Single Point Valid Mask Evaluation
Datasets. We built a new segmentation benchmark to eval-
uate the zero-shot transfer capabilities of our model using a
suite of 23 diverse segmentation datasets from prior work.
A description of each dataset is given in Table 7. For exam-
ples, see main text Fig. 8. This suite covers a range of do-
mains including egocentric [34, 28, 113], microscopy [12],
X-ray [104], underwater [52, 100], aerial [17], simula-
tion [86], driving [25], and painting [24] images. For ef-
ﬁcient evaluation we subsampled datasets with more than
15k masks. Speciﬁcally, we randomly picked images so
that the total number of masks in the sampled images was
10k. We blurred faces of people in all the datasets.Point sampling. Our default point sampling follows stan-
dard practice in interactive segmentation [109, 64, 92]. The
ﬁrst point is chosen deterministically as the point farthest
from the object boundary. Each subsequent point is the
farthest from the boundary of the error region between
ground truth and the previous prediction. Some experiments
(where speciﬁed) use a more challenging sampling strategy
in which the ﬁrst point is a random point, rather than a deter-
ministically selected “center” point. Each subsequent point
is selected as described above. This setting better reﬂects
use cases in which the ﬁrst point is not reliably near the
center of the mask, such as prompting from eye gaze.
Evaluation. We measure IoU between a prediction after
Npoint prompts and a ground truth mask, where N=
f1;2;3;5;9gand points are sampled iteratively with either
of the strategies described above. The per-dataset mIoU is
the per-mask IoU averaged across all objects in the dataset.
Finally, we report the top-line metric by averaging the per-
dataset mIoUs across all 23 datasets. Our evaluation differs
from the standard interactive segmentation evaluation pro-
tocol which measures the average number of points needed
to achieveX% IoU, with up to 20 points. We focus on pre-
dictions after just one, or possibly a few points, since many
of our use cases involve a single or very few prompts. Given
our application focus, which requires real-time prompt pro-
cessing, we expect the best interactive segmentation models
to outperform SAM when using a large number of points.
Baselines. We use three recent strong interactive base-
lines: RITM [92], FocalClick [18], and SimpleClick [67].
For each, we use the largest models trained on the broad-
est datasets publicly released by the authors. For RITM,
we use HRNet32 IT-M trained on the combination of
COCO [66] and LVIS [44] introduced by the authors.
For FocalClick, we use SegFormerB3-S2 trained on a
“combined dataset” that includes 8 different segmentation
datasets [18]. For SimpleClick, we use ViT-H448 trained
on a combination of COCO and LVIS. We follow the sug-
gested default strategies for data pre-processing ( i.e., data
augmentations or image resizing) and do not change or
adapt any parameters for our evaluation. In our experi-
ments, we observe that RITM outperforms other baselines
on our 23 dataset suite with 1 point evaluation. Therefore,
we use RITM as the default baseline. When evaluating with
more points we report results for all baselines.
Single point ambiguity and oracle evaluation. In addition
to IoU after Npoints prompts, we report SAM’s “oracle”
performance at 1 point by evaluating the predicted mask that
best matches ground truth from amongst SAM’s three pre-
dictions (rather than using the one that SAM itself ranks
ﬁrst, as we do by default). This protocol addresses possible
single point prompt ambiguity by relaxing the requirement
to guess the one right mask among several valid objects.
19datasetabbreviation
& linkimage
typedescriptionmask
typesource split# images
sampled# masks
sampled
Plant Phenotyping Datasets
Leaf Segmentation [74]PPDLS Plants Leaf segmentation for images of tobacco and ara plants. Instance N/A 182 2347
BBBC038v1 from Broad
Bioimage Benchmark
Collection [12]BBBC038v1 MicroscopyBiological images of cells in a variety of settings testing
robustness in nuclei segmentation.Instance Train 227 10506
Dataset fOr bOuldeRs
Segmentation [80]DOORS BouldersSegmentation masks of single boulders positioned on the
surface of a spherical mesh.Instance DS1 10000 10000
TimberSeg 1.0 [38] TimberSeg LogsSegmentation masks of individual logs in piles of timber in
various environments and conditions. Images are taken from
an operator’s point-of-view.Instance N/A 220 2487
Northumberland Dolphin
Dataset 2020 [100]NDD20 UnderwaterSegmentation masks of two different dolphin species in
images taken above and under water.Instance N/A 4402 6100
Large V ocabulary Instance
Segmentation [44]LVIS ScenesAdditional annotations for the COCO [66] dataset to enable
the study of long-tailed object detection and segmentation.Instance Validation (v0.5) 945 9642
STREETS [91] STREETSTrafﬁc
cameraSegmentation masks of cars in trafﬁc camera footage. Instance N/A 819 9854
ZeroWaste-f [6] ZeroWaste-f RecyclingSegmentation masks in cluttered scenes of deformed
recycling waste.Instance Train 2947 6155
iShape [111] iShapeIrregular
shapesSegmentation masks of irregular shapes like antennas, logs,
fences, and hangers.Instance Validation 754 9742
ADE20K [117] ADE20K ScenesObject and part segmentation masks for images from
SUN [107] and Places [116] datasets.Instance Validation 302 10128
Occluded Video Instance
Segmentation [81]OVIS OcclusionsInstance segmentation masks in videos, focusing on objects
that are occluded.Instance Train 2044 10011
Hypersim [86] Hypersim SimulationPhotorealistic synthetic dataset of indoor scenes with instance
masks.InstanceEvermotion archinteriors
volumes 1-55 excluding
20,25,40,49338 9445
Night and Day Instance
Segmented Park [22, 23]NDISPark Parking lotsImages of parking lots from video footage taken at day and
night during different weather conditions and camera angles
for vehicle segmentation.Instance Train 111 2577
EPIC-KITCHENS
VISOR [28, 27]VISOR EgocentricSegmentation masks for hands and active objects in
ego-centric video from the cooking dataset
EPIC-KITCHENS [27].Instance Validation 1864 10141
Plittersdorf dataset [46] PlittersdorfStereo
imagesSegmentation masks of wildlife in images taken with the
SOCRATES stereo camera trap.Instance Train, validation, test 187 546
Egocentric Hand-Object
Segmentation [113]EgoHOS EgocentricFine-grained egocentric hand-object segmentation dataset.
Dataset contains mask annotations for existing datasets.InstanceTrain (including only
Ego4D [43] and
THU-READ [97, 96])2940 9961
InstanceBuilding 2D [17] IBD DronesHigh-resolution drone UA V images annotated with roof
instance segmentation masks.Instance Train (2D annotations) 467 11953
WoodScape [112] WoodScapeFisheye
drivingFisheye driving dataset with segmentation masks. Images are
taken from four surround-view cameras.Instance Set 1 107 10266
Cityscapes [25] Cityscapes Driving Stereo video of street scenes with segmentation masks. Panoptic Validation 293 9973
PIDray [104] PIDRay X-raySegmentation masks of prohibited items in X-ray images of
baggage.Instance Test (hard) 3733 8892
Diverse Realism in Art
Movements [24]DRAM PaintingsDomain adaptation dataset for semantic segmentation of art
paintings.Semantic Test 718 1179
TrashCan [52] TrashCan UnderwaterSegmentation masks of trash in images taken by underwater
ROVs. Images are sourced from the J-EDI [69] dataset.Instance Train (instance task) 5936 9540
Georgia Tech Egocentric
Activity Datasets [34, 63]GTEA EgocentricVideos are composed of four different subjects performing
seven types of daily activities with segmentation masks of
hands.InstanceTrain (segmenting hands
task)652 1208
Table 7: Segmentation datasets used to evaluate zero-shot segmentation with point prompts. The 23 datasets cover a broad
range of domains; see column “image type”. To make our evaluation efﬁcient, we subsample datasets that have more than
15k masks. Speciﬁcally, we randomly sampled images so that the total number of masks in the images is 10k.
20image ground truth SAM image ground truth SAM
Figure 15: Additional visualizations of zero-shot edge predictions on BSDS500. Recall that SAM was not trained to predict
edge maps and did not have access to BSDS images and annotations during training.
D.2. Zero-Shot Edge Detection
Dataset and metrics. We perform zero-shot edge detection
experiments on BSDS500 [72, 3]. The ground truth for each
image comes from the manual annotations of ﬁve different
subjects. We report results on the 200 image test subset
using the four standard metrics for edge detection [3, 32]:
optimal dataset scale (ODS), optimal image scale (OIS), av-
erage precision (AP), and recall at 50% precision (R50).
Method. For zero-shot transfer, we use a simpliﬁed ver-
sion of our automatic mask generation pipeline. We prompt
SAM with a 1616 regular grid of foreground points,
which yields 768 predicted masks (three per point). We do
not ﬁlter by predicted IoU or stability. Redundant masks
are removed by NMS. Then we apply a Sobel ﬁlter to the
remaining masks’ unthresholded probability maps and set
values to zero if they do not intersect with the outer bound-
ary pixels of a mask. Finally, we take a pixel-wise max over
all the predictions, linearly normalize the result to [0,1], and
apply edge NMS [13] to thin the edges.
Visualizations. In Fig. 15, we show additional examples
of zero-shot edge predictions from SAM. These qualitative
examples further illustrate how SAM tends to output sensi-
ble edge maps, despite not being trained for edge detection.
We see that the edges can align well with the human anno-
tations. Although, as previously mentioned, since SAM is
not trained for edge detection it does not learn the biases of
the BSDS500 dataset and often outputs more edges than are
present in the ground truth annotations.
D.3. Zero-Shot Object Proposals
Dataset and metrics. We report the standard average recall
(AR) metric for masks at 1000 proposals on the LVIS v1
validation set [44]. Since LVIS has high-quality masks for
1203 object classes, it provides a challenging test for ob-
ject proposal generation. We focus on AR@1000 due to the
open-world nature of our model, which will likely produce
many valid masks outside even the 1203 classes in LVIS. To
measure performance on frequent, common, and rare cate-gories, we use AR@1000 but measured against a ground
truth set containing just the corresponding LVIS categories.
Baseline. We use cascade ViTDet-H as a baseline, the
strongest model from [62] by AP on LVIS. As noted in the
main text, an object detector trained in-domain can “game”
AR [16] and is expected to be a stronger baseline than other
models that focus on open-world proposals or segmenta-
tion [58, 105]. To produce 1000 proposals, we disable score
thresholding in the three cascade stages and as raise the
maximum number of predictions per stage to 1000.
Method. We use a modiﬁed version of SAM’s automatic
mask generation pipeline for zero-shot transfer. First, to
make inference time comparable to that of ViTDet we do
not process image crops. Second, we remove ﬁltering by
predicted IoU and stability. This leaves two tunable param-
eters to get 1000 masks per image: the input point grid and
the NMS threshold duplicate mask suppression. We choose
a 6464 point grid and an NMS threshold of 0.9, which
produces 900 masks per image on average. At evaluation,
if greater than 1000 masks have been proposed in an im-
age, they are ranked by the average of their conﬁdence and
stability scores, then truncated to the top 1000 proposals.
We hypothesize that SAM’s ability to output multiple
masks is especially valuable for this task, since recall should
beneﬁt from proposals generated at multiple scales from
a single input point. To test this, we compare to an ab-
lated version SAM that only outputs a single mask instead
of three (SAM - single-output). Since this model produces
fewer masks, we further increase the number of points sam-
pled and NMS threshold to 128 128 and 0.95, respectively,
obtaining 950 masks per image on average. Additionally,
single-output SAM does not produce the IoU score used
to rank masks for NMS in the automatic mask generation
pipeline, so instead masks are ranked randomly. Testing
suggests this has similar performance to more sophisticated
methods of ranking masks, such as using the max logit value
of the mask as a proxy for model conﬁdence.
21ground truth ViTDet SAM ground truth ViTDet SAM
Figure 16: Zero-shot instance segmentation on LVIS v1. SAM produces higher quality masks than ViTDet. As a zero-shot
model, SAM does not have the opportunity to learn speciﬁc training data biases; see top-right as an example where SAM
makes a modal prediction, whereas the ground truth in LVIS is amodal given that mask annotations in LVIS have no holes.
D.4. Zero-Shot Instance Segmentation
Method. For zero-shot instance segmentation, we prompt
SAM with the boxes output by a fully-supervised ViTDet-H
on COCO and LVIS v1 validation splits. We apply an ad-
ditional mask reﬁnement iteration by feeding the most con-
ﬁdent predicted mask, together with the box prompt, back
to the mask decoder to produce the ﬁnal prediction. We
show zero-shot instance segmentations predicted on LVIS
in Fig. 16. Compared to ViTDet, SAM tends to produce
higher quality masks with cleaner boundaries. We conﬁrm
this observation with human studies in §7.4. Note that as a
zero-shot model, SAM is not able to learn annotation biases
in a dataset. For instance, we see that SAM makes a valid
modal prediction for the plate, whereas LVIS masks cannot
contain holes by design so the plate is annotated amodally.
D.5. Zero-Shot Text-to-Mask
Model and training. We use the largest publicly available
CLIP model [82] ( ViT-L/14@336px ) to compute text
and image embeddings, which we `2normalize prior to use.
To train SAM, we use masks from the ﬁrst two stages of our
data engine. Moreover, we discard all masks with an area
smaller than 1002pixels. We train this model with large-
scale jitter [40] for 120k iterations with batch size 128. All
other training parameters follow our default settings.
Generating training prompts. To extract an input prompt
we ﬁrst expand the bounding box around each mask by a
random factor from 1 to 2, square-crop the expanded
box to maintain its aspect ratio, and resize it to 336 336
pixels. Before feeding the crop to the CLIP image encoder,
with 50% probability we zero-out pixels outside the mask.
To ensure the embedding focuses on the object, we use
masked attention in the last layer to restrict attention from
the output token to the image positions inside the mask. Fi-
nally, our prompt is the output token embedding. For train-
ing we supply the CLIP-based prompt ﬁrst, followed by ad-
ditional iterative point prompts to reﬁne the prediction.
Figure 17: Visualization of thresholding the similarities of
mask embeddings from SAM’s latent space. A query is in-
dicated by the magenta box; top row shows matches at a low
threshold, bottom row at a high threshold. The most similar
mask embeddings in the same image can often be seman-
tically similar to the query mask embedding, even though
SAM is not trained with explicit semantic supervision.
Inference. During inference we use the CLIP text encoder
without any modiﬁcations to create a prompt for SAM. We
rely on the fact that text and image embeddings are aligned
by CLIP, which allows us to train without any explicit text
supervision while using text-based prompts for inference.
D.6. Probing the Latent Space of SAM
Finally, we perform an initial investigation to qualita-
tively probe the latent space learned by SAM. In particu-
lar, we are interested in whether SAM is able to capture any
semantics in its representation even though is not trained
with explicit semantic supervision. To do so, we compute
mask embeddings by extracting an image embedding from
SAM from an image crop around a mask and its horizon-
tally ﬂipped version, multiplying the image embedding by
the binary mask, and averaging over spatial locations. In
Fig. 17, we show 3 examples of a query mask and similar
masks (in the latent space) in the same image. We observe
22that the nearest neighbors for each query show some, albeit
imperfect, shape and semantic similarity. Although these
results are preliminary, they indicate that the representations
from SAM may be useful for a variety of purposes, such as
further data labeling, understanding the contents of datasets,
or as features for downstream tasks.
E. Human Study Experimental Design
Here we describe details of the human study used to eval-
uate mask quality in §7.1 and §7.4. The purpose of the
human study is to address two limitations of using IoU to
ground truth as a measure of predicted mask quality. The
ﬁrst limitation is that, for ambiguous inputs such as a single
point, the model may be strongly penalized for returning a
valid mask of a different object than the ground truth. The
second limitation is that ground truth masks may include
various biases, such as systematic errors in the edge qual-
ity or decisions to modally or amodally segment occluding
objects. A model trained in-domain can learn these biases
and obtain a higher IoU without necessarily producing bet-
ter masks. Human review can obtain a measure of mask
quality independent of an underlying ground truth mask in
order to alleviate these issues.
Models. For single-point evaluation, we use RITM [92],
single-output SAM, and SAM to test two hypotheses. First,
we hypothesize that SAM produces visually higher quality
masks than baseline interactive segmentation models when
given a single point, even when metrics such as IoU with
ground truth do not reveal this. Second, we hypothesize
that SAM’s ability to disambiguate masks improves mask
quality for single point inputs, since single output SAM may
return masks that average over ambiguous masks.
For instance segmentation experiments, we evaluate cas-
cade ViTDet-H [62] and SAM in order to test the hypothesis
that SAM produces visually higher quality masks, even if it
obtains a lower AP due to the inability to learn speciﬁc an-
notation biases of the validation dataset.
Datasets. For single-point experiments, we select 7 datasets
from our set of 23 datasets, since the full suite is too large
for human review. We choose LVIS v0.5 [17], VISOR [28,
27], DRAM [24], IBD [17], NDD20 [100], OVIS [81], and
iShape [111], which provide a diverse collection of images,
including scene-level, ego-centric, drawn, overhead, under-
water, and synthetic imagery. Additionally, this set includes
datasets both where SAM outperforms RITM with IoU met-
rics and vice-versa. For instance segmentation experiments,
we use the LVIS v1 validation set, allowing for direct com-
parison to ViTDet, which was trained on LVIS.
Methodology. We presented masks generated by the mod-
els to professional annotators and asked them to rate each
mask using provided guidelines (see §G for the complete
guidelines). Annotators were sourced from the same com-pany that collected manually annotated masks for the data
engine. An annotator was provided access to an image, the
predicted mask of a single model, and the input to the model
(either a single point or single box) and asked to judge the
mask on three criterion: Does the mask correspond to a
valid object? Does the mask have a clean boundary? and
Does the mask correspond to the input? They then submit-
ted a rating from 1-10 indicating the overall mask quality.
A score of 1 indicates a mask that corresponds to no ob-
ject at all; a low score (2-4) indicates that the mask has huge
errors, such including huge regions of other objects or hav-
ing large areas of nonsensical boundaries; a middle score
(5-6) indicates masks that are mostly sensible but still have
signiﬁcant semantic or boundary errors; a high score (7-
9) indicates masks with only minor boundary errors; and a
score of 10 is for masks with no visible errors. Annotators
were provided with ﬁve different views, each designed to
help identify different error types.
For single point experiments, 1000 masks per dataset
were selected randomly from the same subsets used for
benchmarking zero-shot interactive segmentation (see §D.1
for details on these subsets). The model input was the cen-
termost point, calculated as the largest value of the distance
transform from the edge of the mask. For instance seg-
mentation experiments, 1000 masks were selected from the
LVIS v1 validation set, and the model input was the LVIS
ground truth box. In all experiments, masks with a size
smaller than 242pixels were excluded from sampling, to
prevent showing raters a mask that was too small to judge
accurately. For both memory and display reasons, large im-
ages were rescaled to have a max side-length of 2000 before
predicting a mask. In all experiments, the same inputs were
fed to each model to produce a predicted mask.
For comparison, the ground truth masks from each
dataset were also submitted for rating. For single-point
experiments, this gave 4000 total rating jobs per dataset
(1000 masks each for RITM, SAM single-output, SAM,
and ground truth); for instance segmentation experiments,
it gave 3000 total jobs (ViTDet, SAM, and ground truth).
For each dataset, these jobs were inserted with random
ordering into a queue from which 30 annotators drew jobs.
In initial testing of the review study, we provided each job to
ﬁve different annotators and found reasonable consistency
in scores: the average standard deviation in score over the
ﬁve annotators was 0.83. Additionally, the annotation com-
pany deployed quality assurance testers who spot checked
a fraction of results for extreme departures from the guide-
lines. Thus for our experiments each job ( i.e., rating one
mask in one image) was completed by only a single anno-
tator. Average time spent per annotator per job was 90 sec-
onds, longer than our initial target of 30 seconds, but still
sufﬁciently fast to collect a large number of ratings on each
of the 7 selected datasets.
231 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings6.5 ± 0.15, RITM
7.7 ± 0.12, SAM - single output8.1 ± 0.10, SAM
8.5 ± 0.09, GT(a) LVIS v0.5 [17]
1 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings6.3 ± 0.16, RITM
7.5 ± 0.13, SAM - single output8.3 ± 0.09, SAM
8.5 ± 0.13, GT (b) VISOR [28, 27]
1 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings5.9 ± 0.14, RITM
6.8 ± 0.15, SAM - single output7.7 ± 0.13, SAM
8.0 ± 0.15, GT
(c) DRAM [24]
1 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings7.1 ± 0.12, RITM
7.9 ± 0.11, SAM - single output8.3 ± 0.08, SAM
8.4 ± 0.09, GT (d) IBD [17]
1 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings6.4 ± 0.17, RITM
8.2 ± 0.11, SAM - single output8.6 ± 0.10, SAM
8.9 ± 0.06, GT
(e) NDD20 [100]
1 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings6.1 ± 0.15, RITM
7.7 ± 0.12, SAM - single output7.2 ± 0.13, SAM
8.8 ± 0.09, GT (f) OVIS [81]
1 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings4.9 ± 0.16, RITM
6.2 ± 0.17, SAM - single output7.1 ± 0.15, SAM
9.3 ± 0.06, GT
(g) iShape [111]
Figure 18: Mask quality rating distributions by dataset from our human evaluation study.
SAM>baseline SAM>SAM single out.
dataset p-value CI99() p-value CI99()
point input (RITM [92] baseline):
LVIS v0.5 [44] 4e-69 (1.40, 1.84) 2e-11 (0.29, 0.64)
VISOR [28, 27] 7e-98 (1.81, 2.24) 7e-26 (0.58, 0.94)
DRAM [24] 1e-76 (1.54, 2.00) 2e-24 (0.62, 1.03)
IBD [17] 2e-57 (1.03, 1.39) 1e-15 (0.32, 0.62)
NDD20 [100] 2e-86 (1.88, 2.37) 5e-08 (0.19, 0.55)
OVIS [81] 2e-64 (1.38, 1.84) 3e-10 (0.27, 0.63)
iShape [111] 2e-88 (1.97, 2.47) 7e-23 (0.65, 1.10)
box input (ViTDet-H [62] baseline):
LVIS v1 [44] 2e-05 (0.11, 0.42) N/A N/A
Table 8: Statistical tests showing signiﬁcance that SAM has
higher mask quality ratings than baseline and single-output
SAM. P-values are calculated by paired t-test, while conﬁ-
dence intervals for the difference in mean scores are calcu-
lated by paired bootstrap on 10k samples. All p-values are
signiﬁcant, and all conﬁdence intervals exclude zero.
Results. Fig. 18 shows histograms over ratings for each
dataset in the single-point experiments. We run statisticaltests for two hypotheses: (1) that SAM gets higher scores
than the baseline model (RITM or ViTDet) and (2) that
SAM gets higher scores than single-output SAM. P-values
are calculated via a paired t-test on the means of the model
scores, which we supplement with a paired bootstrap test on
10k samples to ﬁnd the 99% conﬁdence interval for the dif-
ference of means. Table 8 shows p-values and conﬁdence
intervals for these tests. All statistical tests are strongly sig-
niﬁcant, and all conﬁdence intervals exclude zero.
For instance segmentation, Fig. 11 of the main text
shows the histogram for ratings. To compare to COCO
ground truth, we additionally include 794 ratings of COCO
ground truth masks that were collected during our testing of
the human review process. These masks were presented to
raters using an identical setup as the LVIS results. For fair
comparison, results for LVIS in Fig. 11 were subsampled
to the same 794 inputs for each model and ground truth.
For Table 8, the full 1000 ratings are used to run statistical
tests, which show that SAM’s mask quality improvement
over ViTDet is statistically signiﬁcant.
24F. Dataset, Annotation, and Model Cards
In §F.1 we provide a Dataset Card for SA-1B, follow-
ing [39], in a list of questions and answers. Next, we pro-
vide a Data Annotation Card in §F.2 for the ﬁrst two stages
of our data engine described in §4, following CrowdWork-
Sheets [30], again as a list of questions and answers. We
provide a Model Card following [75] in Table 9.
F.1. Dataset Card for SA-1B
Motivation
1.For what purpose was the dataset created? Was there a speciﬁc task in
mind? Was there a speciﬁc gap that needed to be ﬁlled? Please provide a
description. The contributions of our dataset to the vision community are
fourfold: (1) We release a dataset of 11M images and 1.1B masks, by far the
largest segmentation dataset to date. (2) The dataset we release is privacy
protecting: we have blurred faces and license plates in all images. (3) The
dataset is licensed under a broad set of terms of use which can be found
at https://ai.facebook.com/datasets/segment-anything. (4) The data is more
geographically diverse than its predecessors, and we hope it will bring the
community one step closer to creating fairer and more equitable models.
2.Who created the dataset ( e.g., which team, research group) and on behalf
of which entity ( e.g., company, institution, organization)? The dataset was
created by the FAIR team of Meta AI. The underlying images were collected
and licensed from a third party photo company.
3.Who funded the creation of the dataset? If there is an associated grant,
please provide the name of the grantor and the grant name and number.
Meta AI funded the creation of the dataset.
4.Any other comments? No.
Composition
1.What do the instances that comprise the dataset represent ( e.g., documents,
photos, people, countries)? Are there multiple types of instances ( e.g.,
movies, users, and ratings; people and interactions between them; nodes
and edges)? Please provide a description. All of the instances in the dataset
are photos. The photos vary in subject matter; common themes of the photo
include: locations, objects, scenes. All of the photos are distinct, however
there are some sets of photos that were taken of the same subject matter.
2.How many instances are there in total (of each type, if appropriate)? There
are 11 million images.
3.Does the dataset contain all possible instances or is it a sample (not nec-
essarily random) of instances from a larger set? If the dataset is a sample,
then what is the larger set? Is the sample representative of the larger set
(e.g., geographic coverage)? If so, please describe how this representa-
tiveness was validated/veriﬁed. If it is not representative of the larger set,
please describe why not ( e.g., to cover a more diverse range of instances,
because instances were withheld or unavailable). The dataset is composed
of images licensed from a photo provider. The dataset contains all instances
licensed. The images are photos, i.e. not artwork, although there are a few
exceptions. The dataset includes all generated masks for each image in the
dataset. We withheld 2k randomly selected images for testing purposes.
4.What data does each instance consist of? “Raw” data ( e.g., unprocessed
text or images) or features? In either case, please provide a description.
Each instance in the dataset is an image. The images were processed to blur
faces and license plates to protect the identities of those in the image.
5.Is there a label or target associated with each instance? If so, please provide
a description. Each image is annotated with masks. There are no categories
or text associated with the masks. The average image has 100 masks, and
there are 1.1B masks in total.
6.Is any information missing from individual instances? If so, please provide
a description, explaining why this information is missing ( e.g., because it
was unavailable). This does not include intentionally removed information,
but might include, e.g., redacted text. Yes. Each image is accompanied by
a short caption that describes the content and place of the photo in a free
form text. Per our agreement with the photo provider we are not allowed to
release these captions. However, we use them in our paper to analyze the
geographical distribution of the dataset.7.Are relationships between individual instances made explicit ( e.g., users’
movie ratings, social network links)? If so, please describe how these rela-
tionships are made explicit. No, there are no known relationships between
instances in the dataset.
8.Are there any errors, sources of noise, or redundancies in the dataset? If
so, please provide a description. Errors: The masks are generated by a
segmentation model, so there may be errors or inconsistencies in the masks.
Redundancies: While no two images are the same, there are instances of
images of the same subject taken close together in time.
9.Is the dataset self-contained, or does it link to or otherwise rely on external
resources ( e.g., websites, tweets, other datasets)? If it links to or relies on
external resources, a) are there guarantees that they will exist, and remain
constant, over time; b) are there ofﬁcial archival versions of the complete
dataset ( i.e., including the external resources as they existed at the time
the dataset was created); c) are there any restrictions ( e.g., licenses, fees)
associated with any of the external resources that might apply to a dataset
consumer? Please provide descriptions of all external resources and any
restrictions associated with them, as well as links or other access points, as
appropriate. The dataset is self-contained.
10. Does the dataset contain data that might be considered conﬁdential ( e.g.,
data that is protected by legal privilege or by doctor-patient conﬁdentiality,
data that includes the content of individuals’ non-public communications)?
If so, please provide a description. No.
11. Does the dataset contain data that, if viewed directly, might be offensive,
insulting, threatening, or might otherwise cause anxiety? If so, please de-
scribe why. We have two safety measures to prevent objectionable content:
(1) Photos are licensed from a photo provider and had to meet the terms of
service of the photo provider. We requested that all objectionable content
be ﬁltered from the images we licensed. (2) If a user observes objectionable
image(s) in the dataset, we invite them to report the image(s) at segment-
anything@meta.com for removal. Despite the measures taken, we observe
that a small portion of images contains scenes of protests or other gatherings
that focus on a diverse spectrum of religious beliefs or political opinions that
may be offensive. We were not able to produce a ﬁltering strategy that re-
moves all such images and rely on users to report this type of content.
12. Does the dataset identify any subpopulations ( e.g., by age, gender)? If so,
please describe how these subpopulations are identiﬁed and provide a de-
scription of their respective distributions within the dataset. The dataset
does not identify any subpopulations of the people in the photos.
13. Is it possible to identify individuals ( i.e., one or more natural persons), ei-
ther directly or indirectly ( i.e., in combination with other data) from the
dataset? If so, please describe how. No. Images were subjected to a face
blurring model to remove any personally identiﬁable information. If a user
observes any anonymization issue, we invite them to report the issue and
the image id(s) at segment-anything@meta.com.
14. Does the dataset contain data that might be considered sensitive in any way
(e.g., data that reveals race or ethnic origins, sexual orientations, religious
beliefs, political opinions or union memberships, or locations; ﬁnancial or
health data; biometric or genetic data; forms of government identiﬁcation,
such as social security numbers; criminal history)? If so, please provide
a description. The dataset contains scenes of protests, or other gatherings
that may suggest religious beliefs, political opinions or union memberships.
However, the faces of all people in the dataset have been anonymized via
facial blurring, so it is not possible to identify any person in the dataset.
15. Any other comments? No.
Collection Process
1.How was the data associated with each instance acquired? Was the data
directly observable ( e.g., raw text, movie ratings), reported by subjects ( e.g.,
survey responses), or indirectly inferred/derived from other data ( e.g., part-
of-speech tags, model-based guesses for age or language)? If the data was
reported by subjects or indirectly inferred/derived from other data, was the
data validated/veriﬁed? If so, please describe how. The released masks
associated with each image were automatically inferred by our segmentation
model, SAM. The masks that were collected using model-assisted manual
annotation will not be released. Quality was validated as described in §5.
2.What mechanisms or procedures were used to collect the data ( e.g., hard-
ware apparatuses or sensors, manual human curation, software programs,
software APIs)? How were these mechanisms or procedures validated? The
images in the dataset are licensed from an image provider. They are all pho-
tos taken by photographers with different cameras.
253.If the dataset is a sample from a larger set, what was the sampling strategy
(e.g., deterministic, probabilistic with speciﬁc sampling probabilities)? We
withheld 2k randomly selected images for testing purposes. The rest of
the licensed images are included in the dataset.
4.Who was involved in the data collection process ( e.g., students, crowdwork-
ers, contractors) and how were they compensated ( e.g., how much were
crowdworkers paid)? The released masks were automatically inferred by
SAM. For details on our model-assisted manual annotation process see our
Data Annotation Card in §F.2. Note these masks will not be released.
5.Over what timeframe was the data collected? Does this timeframe match
the creation timeframe of the data associated with the instances ( e.g., recent
crawl of old news articles)? If not, please describe the timeframe in which
the data associated with the instances was created. The licensed photos
vary in their date taken over a wide range of years up to 2022.
6.Were any ethical review processes conducted ( e.g., by an institutional re-
view board)? If so, please provide a description of these review processes,
including the outcomes, as well as a link or other access point to any sup-
porting documentation. If the dataset does not relate to people, you may skip
the remaining questions in this section. We underwent an internal privacy
review to evaluate and determine how to mitigate any potential risks with
respect to the privacy of people in the photos. Blurring faces and license
plates protects the privacy of the people in the photos.
7.Did you collect the data from the individuals in question directly, or obtain
it via third parties or other sources ( e.g., websites)? We licensed the data
from a third party photo provider.
8.Were the individuals in question notiﬁed about the data collection? If so,
please describe (or show with screenshots or other information) how no-
tice was provided, and provide a link or other access point to, or other-
wise reproduce, the exact language of the notiﬁcation itself. The images
are licensed from a third party who provided appropriate representations
regarding the collection of any notices and consents as required from indi-
viduals. In addition, all identiﬁable information ( e.g. faces, license plates)
was blurred. Under the terms of the dataset license it is prohibited to attempt
to identify or associate an image with a particular individual.
9.Did the individuals in question consent to the collection and use of their
data? If so, please describe (or show with screenshots or other informa-
tion) how consent was requested and provided, and provide a link or other
access point to, or otherwise reproduce, the exact language to which the
individuals consented. The images are licensed from a third party who pro-
vided appropriate representations regarding the collection of any notices and
consents as required from individuals. In addition, all identiﬁable informa-
tion ( e.g. faces, license plates) was blurred from all images. For avoidance
of doubt, under the terms of the dataset license it is prohibited to attempt to
identify or associate an image with a particular individual.
10. If consent was obtained, were the consenting individuals provided with a
mechanism to revoke their consent in the future or for certain uses? If
so, please provide a description, as well as a link or other access point
to the mechanism (if appropriate). We invite users to report at segment-
anything@meta.com for image(s) removal.
11. Has an analysis of the potential impact of the dataset and its use on data
subjects ( e.g., a data protection impact analysis) been conducted? If so,
please provide a description of this analysis, including the outcomes, as
well as a link or other access point to any supporting documentation. To
eliminate any potential impact on people whose photos are included in the
dataset, identiﬁable information (faces, license plates) has been blurred.
12. Any other comments? No.
Preprocessing / Cleaning / Labeling
1.Was any preprocessing / cleaning / labeling of the data done ( e.g., dis-
cretization or bucketing, tokenization, part-of-speech tagging, SIFT fea-
ture extraction, removal of instances, processing of missing values)? If so,
please provide a description. If not, you may skip the remaining questions
in this section. We resized the high-resolution licensed images such that
the shorter side is 1500 pixels and only processed the images to remove any
identiﬁable and personal information from the photos (faces, license plates).
2.Was the “raw” data saved in addition to the preprocessed/cleaned/labeled
data ( e.g., to support unanticipated future uses)? If so, please provide a link
or other access point to the “raw” data. No, as we removed the data for
safety reasons and to respect privacy, we do not release the unaltered photos.
3.Is the software that was used to preprocess/clean/label the data avail-
able? If so, please provide a link or other access point. We used theRetinaFace [88, 89] model (https://github.com/serengil/retinaface) to detect
faces. The model used to blur license plates has not been made public.
Uses
1.Has the dataset been used for any tasks already? If so, please provide a
description. The dataset was used to train our segmentation model, SAM.
2.Is there a repository that links to any or all papers or systems that use the
dataset? If so, please provide a link or other access point. No. However, all
users of the dataset must cite it, so its use is trackable via citation explorers.
3.What (other) tasks could the dataset be used for? We intend the dataset
to be a large-scale segmentation dataset. However, we invite the research
community to gather additional annotations for the dataset.
4.Is there anything about the composition of the dataset or the way it was
collected and preprocessed/cleaned/labeled that might impact future uses?
For example, is there anything that a dataset consumer might need to know
to avoid uses that could result in unfair treatment of individuals or groups
(e.g., stereotyping, quality of service issues) or other risks or harms ( e.g.,
legal risks, ﬁnancial harms)? If so, please provide a description. Is there
anything a dataset consumer could do to mitigate these risks or harms? We
have an analysis of the approximate geographic and income level coverage
of our dataset in §6. While we believe our dataset to be more representative
than most of the publicly existing datasets at this time, we acknowledge
that we do not have parity across all groups, and we encourage users to be
mindful of potential biases their models have learned using this dataset.
5.Are there tasks for which the dataset should not be used? If so, please pro-
vide a description. Full terms of use for the dataset including prohibited use
cases can be found at https://ai.facebook.com/datasets/segment-anything.
6.Any other comments? No.
Distribution
1.Will the dataset be distributed to third parties outside of the entity ( e.g.,
company, institution, organization) on behalf of which the dataset was cre-
ated? If so, please provide a description. The dataset will be available for
the research community.
2.How will the dataset will be distributed ( e.g., tarball on website, API,
GitHub)? Does the dataset have a digital object identiﬁer (DOI)? The
dataset is available at https://ai.facebook.com/datasets/segment-anything.
3.When will the dataset be distributed? The dataset will be released in 2023.
4.Will the dataset be distributed under a copyright or other intellectual
property (IP) license, and/or under applicable terms of use (ToU)? If
so, please describe this license and/or ToU, and provide a link or other
access point to, or otherwise reproduce, any relevant licensing terms
or ToU, as well as any fees associated with these restrictions. Yes.
The license agreement and terms of use for the dataset can be found at
https://ai.facebook.com/datasets/segment-anything. Users must agree to the
terms of use before downloading or using the dataset.
5.Have any third parties imposed IP-based or other restrictions on the data
associated with the instances? If so, please describe these restrictions, and
provide a link or other access point to, or otherwise reproduce, any relevant
licensing terms, as well as any fees associated with these restrictions. Full
terms of use and restrictions on use of the SA-1B dataset can be found at
https://ai.facebook.com/datasets/segment-anything.
6.Do any export controls or other regulatory restrictions apply to the dataset
or to individual instances? If so, please describe these restrictions, and pro-
vide a link or other access point to, or otherwise reproduce, any supporting
documentation. The license and restrictions on use of the SA-1B dataset
can be found at https://ai.facebook.com/datasets/segment-anything.
7.Any other comments? No.
Maintenance
1.Who will be supporting/hosting/maintaining the dataset? The dataset will
be hosted at https://ai.facebook.com/datasets/segment-anything and main-
tained by Meta AI.
2.How can the owner/curator/manager of the dataset be contacted ( e.g., email
address)? Please email segment-anything@meta.com.
3.Is there an erratum? If so, please provide a link or other access point. No.
4.Will the dataset be updated ( e.g., to correct labeling errors, add new in-
stances, delete instances)? If so, please describe how often, by whom, and
how updates will be communicated to dataset consumers ( e.g., mailing list,
26GitHub)? To aid reproducibility of research using SA-1B, the only updates
will be to remove reported images.
5.If the dataset relates to people, are there applicable limits on the retention of
the data associated with the instances ( e.g., were the individuals in question
told that their data would be retained for a ﬁxed period of time and then
deleted)? If so, please describe these limits and explain how they will be
enforced. There are no limits on data retention. We took measures to remove
personally identiﬁable information from any images of people. Users may
report content for potential removal here: segment-anything@meta.com.
6.Will older versions of the dataset continue to be sup-
ported/hosted/maintained? If so, please describe how. If not, please
describe how its obsolescence will be communicated to dataset consumers.
No, as the only updates will be to remove potentially harmful content, we
will not keep older versions with the content.
7.If others want to extend/augment/build on/contribute to the dataset, is there
a mechanism for them to do so? If so, please provide a description. Will
these contributions be validated/veriﬁed? If so, please describe how. If not,
why not? Is there a process for communicating/distributing these contribu-
tions to dataset consumers? If so, please provide a description. We encour-
age users to gather further annotations for SA-1B. Any users who generate
annotations will be liable for hosting and distributing their annotations.
8.Any other comments? No.
F.2. Data Annotation Card
Task Formulation
1.At a high level, what are the subjective aspects of your task? Segmenting
objects present in an image is inherently a subjective task. For instance,
one annotator may segment two boots as one mask, whereas another may
segment each boot separately. Depending on annotators’s skills, the quality
of the mask and the number of masks per image are different between an-
notators. Despite these subjective aspects of the task, we believed efﬁcient
annotation was possible as the data was annotated in a per-mask fashion
with the main focus on the diversity of the data rather than completeness.
2.What assumptions do you make about annotators? Our annotators worked
full time on our annotation task with very small attrition rate. This made
it possible to train the annotators providing feedback and answering their
questions on a regular basis. Speciﬁcally: (1) By giving a clear understand-
ing of the goals of this work and providing clear guidelines, including vi-
suals and video recordings of the tasks, annotators had enough context to
understand and perform the tasks reasonably. (2) Sharing objectives and
key results and meeting weekly with annotators increased the likelihood
that annotators improved annotation quality and quantity over time.
3.How did you choose the speciﬁc wording of your task instructions? What
steps, if any, were taken to verify the clarity of task instructions and wording
for annotators? As our task was annotating images, the annotation guide-
lines included visual examples. Our research team completed 30 annotation
tasks to identify any obvious challenges using the annotation tool, collec-
tively decide how to handle complex cases, and reﬁne the guidelines. The
research team met with the annotators weekly for feedback sessions. Videos
of the research team performing the task were shared live with the annota-
tors, followed by Q&A sessions. Annotators were able to give feedback on
unclear aspects, both during the feedback session and asynchronously.
4.What, if any, risks did your task pose for annotators and were they informed
of the risks prior to engagement with the task? No identiﬁed risks. Images
were ﬁltered for objectionable content prior to the annotation phase.
5.What are the precise instructions that were provided to annotators? We
provide only high-level instructions: Given an image, we aim at segment-
ing every possible object. Annotators generate a mask for every potential
object they can identify. An object can be segmented using our interactive
segmentation tool either by using corrective foreground/background clicks
to add/remove parts of the mask or by drawing a bounding box around the
object. Masks can be reﬁned using pixel-precise tools.
Selecting Annotations
1.Are there certain perspectives that should be privileged? If so, how did you
seek these perspectives out? We chose to work with annotators that have
worked on other vision annotation tasks before.
2.Are there certain perspectives that would be harmful to include? If so, how
did you screen these perspectives out? No.3.Were sociodemographic characteristics used to select annotators for your
task? If so, please detail the process. No.
4.If you have any aggregated socio-demographic statistics about your anno-
tator pool, please describe. Do you have reason to believe that sociode-
mographic characteristics of annotators may have impacted how they an-
notated the data? Why or why not? We worked with 130 annotators. The
annotators were all based in Kenya. We do not believe sociodemographic
characteristics of annotators meaningfully impacted the annotated data.
5.Consider the intended context of use of the dataset and the individuals
and communities that may be impacted by a model trained on this dataset.
Are these communities represented in your annotator pool? The Segment
Anything 1B (SA-1B) dataset is to be used for research purposes only.
The SA-1B dataset is one of the most geographically diverse segmentation
dataset, as discussed in §6. In addition, we analyze the responsible AI axes
of a model trained on the dataset in §6.
Platform and Infrastructure Choices
1.What annotation platform did you utilize? At a high level, what considera-
tions informed your decision to choose this platform? Did the chosen plat-
form sufﬁciently meet the requirements you outlined for annotator pools?
Are any aspects not covered? We used a proprietary annotation platform.
2.What, if any, communication channels did your chosen platform offer to
facilitate communication with annotators? How did this channel of com-
munication inﬂuence the annotation process and/or resulting annotations?
We manually reviewed annotations and shared feedback with the annotators
on a weekly basis. We communicated common mistakes or inconsisten-
cies and the corresponding corrections. In addition, the annotators were
given feedback for improvements daily by the annotation QA team. Out-
side the weekly feedback sessions, annotators had access to a spreadsheet
and chat group to facilitate communication with the research team. This
process greatly improved the average speed and quality of the annotations.
3.How much were annotators compensated? Did you consider any partic-
ular pay standards, when determining their compensation? If so, please
describe. Annotators were compensated with an hourly wage set by the
vendor. The vendor is a Certiﬁed B Corporation.
Dataset Analysis and Evaluation
1.How do you deﬁne the quality of annotations in your context, and how did
you assess the quality in the dataset you constructed? Annotators were ﬁrst
placed into training. They followed a 1-day training session led by the ven-
dor and then were asked to annotate a large number of examples from a
training queue. Annotators graduated from training to production after the
vendor QA team, in collaboration with the research team, manually spot-
checked the annotator’s masks to ensure quality. On average, annotators
spent one week in training before graduating. Production quality assess-
ment followed a similar process: the vendor QA team and the research team
manually reviewed the annotations weekly, sharing feedback weekly.
2.Have you conducted any analysis on disagreement patterns? If so, what
analyses did you use and what were the major ﬁndings? Did you analyze
potential sources of disagreement? We pointed out common mistakes dur-
ing weekly meetings with the annotators.
3.How do the individual annotator responses relate to the ﬁnal labels released
in the dataset? The annotations were only used to train early versions of the
SAM model and we do not currently plan to release them.
Dataset Release and Maintenance
1.Do you have reason to believe the annotations in this dataset may change
over time? Do you plan to update your dataset? No, except to remove
objectionable images.
2.Are there any conditions or deﬁnitions that, if changed, could impact the
utility of your dataset? We do not believe so.
3.Will you attempt to track, impose limitations on, or otherwise inﬂuence how
your dataset is used? If so, how? The SA-1B dataset will be released under
a license agreement allowing use for certain research purposes and protec-
tions for researchers. Researchers must agree to the terms of the license
agreement to access the dataset.
4.Were annotators informed about how the data is externalized? If changes to
the dataset are made, will they be informed? No, we do not plan to release
the manual annotations at the moment.
5.Is there a process by which annotators can later choose to withdraw their
data from the dataset? If so, please detail. No.
27Model Overview
Name SAM or Segment Anything Model
Version 1.0
Date 2023
Organization The FAIR team of Meta AI
Mode type Promptable segmentation model
Architecture See §3
Repository https://github.com/facebookresearch/segment-anything
Citation https://research.facebook.com/publications/segment-anything
License Apache 2.0
Intended Use
Primary intended uses SAM is intended to be used for any prompt-based segmentation task. We explored its use in segmenting objects
from a point (§7.1), edge detection (§7.2), segmenting all objects (§7.3), and segmenting detected objects (§7.4).
We explored how SAM can integrate with other vision models to segment objects from text (§7.5).
Primary intended users SAM was primarily developed for research. The license for SAM can be found at
https://github.com/facebookresearch/segment-anything.
Out-of-scope use cases See terms of use for SAM found at https://github.com/facebookresearch/segment-anything. See Use Cases under
Ethical Considerations .
Caveats and recommendations SAM has impressive zero-shot performance across a wide range of tasks. We note, however, that in the zero-shot
setting there may be multiple valid ground truth masks for a given input. We recommend users take this into
consideration when using SAM for zero-shot segmentation. SAM can miss ﬁne structures and can hallucinate
small disconnected components. See §8 for a discussion of limitations.
Relevant Factors
Groups SAM was designed to segment any object. This includes stuff andthings .
Instrumentation and environment We benchmarked SAM on a diverse set of datasets and found that SAM can handle a variety of visual data including
simulations, paintings, underwater images, microscopy images, driving data, stereo images, ﬁsh-eye images . See
§D.1 and Table 7 for information on the benchmarks used.
Metrics
Model performance measures We evaluated SAM on a variety of metrics based on the downstream task in our experiments.
•mIoU : We used the mean intersection-over-union after a given number of prompts to evaluate the segmen-
tation quality of a mask when prompted with points.
•Human evaluation : We performed a human study (detailed in §E) to evaluate the real world performance
of SAM. We compared the masks generated by SAM to a baseline state-of-the-art interactive segmentation
model, RITM [92], using a perceptual quality scale from 1 to 10.
•AP: We used average precision to evaluate instance segmentation for a given box and edge detection.
•AR@1000 : We used average recall to evaluate object proposal generation.
•ODS, OIS, AP , R50 : We used the standard edge detection evaluation metrics from BSDS500 [72, 3].
Evaluation Data
Data sources See §D.1.
Training Data
Data source See Data Card in §F.1.
Ethical Considerations
Data We trained SAM on licensed images. The images were ﬁltered for objectionable content by the provider, but we
acknowledge the possibility of false negatives. We performed a geographic analysis of the SA-1B dataset in §6.
While SA-1B is more geographically diverse than many of its predecessors, we acknowledge that some geographic
regions and economic groups are underrepresented.
Cost and impact of compute SAM was trained on 256 A100 GPUS for 68 hours. We acknowledge the environmental impact and cost of training
large scale models. The environmental impact of training the released SAM model is approximately 6963 kWh
resulting in an estimated 2.8 metric tons of carbon dioxide given the speciﬁc data center used, using the calculation
described in [77] and the ML CO 2Impact calculator [61]. This is equivalent to 7k miles driven by the average
gasoline-powered passenger vehicle in the US [101]. We released the SAM models to both reduce the need for
retraining and lower the barrier to entry for large scale vision research.
Risks and harms We evaluated SAM for fairness in §6. Downstream use cases of SAM will create their own potential for biases
and fairness concerns. As such we recommend users run their own fairness evaluation when using SAM for their
speciﬁc use case.
Use cases We implore users to use their best judgement for downstream use of the model.
Table 9: Model Card for SAM, following the procedure detailed in [75].
28We have several models that, when provided with a click or a box as input, output a mask. We would
like to compare the quality of these models by rating the quality of their masks on many examples.
The interface will be different than for regular mask annotation.
• Each job reviews one mask in one image.
• On the right, there will be ﬁve image thumbnails in two rows. Each thumbnail can be moused-
over to show the image at a larger size. Clicking on the thumbnail will make it full screen, and
clicking again will return to the original screen.
– The images show the same mask in ﬁve different views. On the top row: (left) the image
without the mask, (middle) the mask overlaid on the image, and (right) the mask alone. On
the bottom row: (left) a zoomed in view of the object without a mask, and (right) a zoomed
in view of the mask overlaid on the image. These views are provided to make it easy to see
different types of mask errors.
– The mask will be in red when overlaid on the image.
– When shown by itself, the mask is yellow, and the background is purple.
– Each image will include either a blue dot or a blue and white box. This is the input to the
model, as if you had clicked at this location or drawn this box.
• On the left, there are buttons labeled 1-10. This is used to rate the quality of the shown mask.
Objective and Setup
 Example interface page. There will be ﬁve images on the
right and a question box on the left.
Mouse over an image to show the full image.
 Click on an image to make it full screen. The arrows will cy-
cle between images. Click again to return to previous view.
The ﬁrst image on the top row shows the image without a
mask. A blue point will be on the object of interest, or a
blue and white box will surround it.
The second image on the top row shows the mask for the
object in red.
The third image on the top row shows the mask only. The
mask is in yellow and the background is purple.
The ﬁrst image on the bottom row shows a zoomed in view
of the object without a mask.
The second image on the bottom row shows a zoomed in
view of the object with a mask. The mask is in red.
On the left are buttons to rate the mask quality, with selec-
tions 1-10.What we would like you to do for each job:
• Please aim to spend up to 30 seconds per job.
• Mouse-over or click each of the three images of the mask on the right to get a sense of the
quality of the mask. The thumbnail is too small to judge a mask, do not judge a mask by the
thumbnail alone. Each image can provide a different signal on possible mask errors:
– The unzoomed image can give context for the mask: does this mask correspond to an actual
object?
– The mask-only image can show if the mask has small holes or separated, incorrect pixels.
– The zoomed image can show if the mask boundaries make sense.
• Judge the quality of the mask on three criterion. Examples will follow.
– Does the mask correspond to an actual object?
– Does the mask have a good boundary?
– Does the mask correspond to the provided point or box?
• Rate the quality of the mask on a scale of 1-10 using the drop-down box on the left.
• Next are details and examples for judging mask quality according to the three criterion. These
are just examples and other cases may come up, please use your best judgment when deter-
mining if something is a good mask.
TaskDoes the mask correspond to an actual object?
• Valid objects can include:
– Entire single objects (such as a person, shirt, or tree)
– Logical parts of objects (a chair leg, a car door, a tabletop)
– Collections of objects (a stack of books, a crowd of people)
– ‘Stuff’ (the ground, the sky).
• Example errors a mask may have. The severity of these errors may be minor or major:
– Include a piece of another object (the mask of a person including the arm of a nearby
person)
– Miss part of an object (the mask covers only one part of a building obscured by a tree in
the foreground),
– Combine two unrelated things (a single mask covers both a mug and a pen on a desk)
– Include an arbitrary part of a collection for a point input (a point is on one apple, but
the mask covers three apples in a pile of many apples). If a box surrounds an arbitrary
collection, it is not an error to provide a mask for these objects.
• If you are unsure, a good rule-of-thumb is: can you name the object in question? However,
some things that are hard to name may still be good objects (an unusual component of a
machine, something at the edge of the image for which it is hard to determine what it is).
Judging Mask Quality (1 of 3)
Does the mask have a good boundary?
• Errors in the boundary can include:
– Incorrect holes in the mask
– Incorrect pixels included separated from the main part of the mask
– Poor edge quality, where the mask does not exactly match the edge of the object.
– Failure to consistently handle obscuring foreground objects (a mask that covers obscuring
objects is ﬁne, and a mask that doesn’t cover obscuring objects is ﬁne, but one that does
some of both has an error)
– Pixelation of a small mask is not an error, as long as the mask still matches the edges of
the object.
Judging Mask Quality (2 of 3)Does the mask correspond to the provided point or box?
• For points:
– The point needs to be on the mask.
– The size or position of the object with respect to the point does not matter (a point on
someone’s gloved hand can correspond to the glove or to the entire person, both are valid
masks).
• For boxes:
– The object needs to be the best object that is the size of the box (if a box is around some-
one’s entire head but the mask is of their hair, this is an error: their hair is in the box but is
not the correct object).
– If the box clearly corresponds to a given object but is slightly smaller than it, it is okay if
the mask goes slightly outside a box (if a box around a person misses their extended hand,
the mask can still include their hand even if the mask goes outside the box).
Judging Mask Quality (3 of 3)
 Example error of ‘Include a piece of another object’: The
elephant mask contains a piece of another nearby elephant.
Example error of ‘Missing a part of an object’: the mask is
missing a disconnected part of the object: the back half of
the zebra, and the right portion of the plate.
Example error of ‘Include an arbitrary part of a collection’:
In top top image, the point is on one orange rind, but the
mask covers two orange rinds. This is a mask error: the
mask covers an arbitrary number of objects in the collection,
and should either cover one orange rind or all of them. In
the bottom image, the box is around both vegetables. Since
this is the best match to the box, this is not a mask error.
Example error for ‘Incorrect holes in the mask’: This mask
has holes in the upper left and on the left sides (black ar-
rows). These holes are much easier to see on the ‘mask
only’ image.
Example error for ‘Incorrect pixels included separated from
the main part of the mask’: The ‘mask only’ view reveals a
few stray incorrect pixels on the clock face.
Example error for ‘Poor edge quality’: The mask has poor
edge quality, both along the edge of the umbrella, as well as
along the thin pole.
Figure 19: Here we provide the complete guidelines given to annotations for the human review of mask quality. Some images
been edited slightly and faces have been blurred to enable release. Best viewed with zoom (part 1 of 2).
G. Annotation Guidelines
We provide the complete guidelines given to annotations
for the human review of mask quality in Fig. 19 and Fig. 20.
29Example for ‘Combine two unrelated things’: The point in-
dicates the lizard, but the mask covers both the lizard and a
bird. This is a mask error.
Example error for ‘Failure to consistently handle obscuring
foreground objects’: The pole on the right (blue arrow) is
excluded from the mask, while the pole on the left is in-
cluded in the object (black arrow). The mask should either
include or exclude both of these.
Example of ‘Pixelation of a small mask’: this mask has an
imperfect boundary, since it extends beyond the object at
the black arrow. However, the ‘blocky’ pattern of the mask
is not an error, since, when zoomed in this much, the image
is also blocky the same way.
Example error for consistency with the provided point: The
mask does not agree with the blue point, so this is a mask
error.
Example for consistency with the provided point: For this
input point, but the logo (left) and the container (right) are
valid objects, since the blue point lies on both of them. Nei-
ther mask has a mask error.
Example for consistency with a box: The box surrounds the
bowl of oranges, but the mask is only of a single orange.
This is a mask error.
Example for consistency with a box: The box’s shape ﬁts
the zebra. Even though the mask extends slightly outside
the box to include the zebra’s left leg, this is not an error.Overall mask quality is subjective, each of the above errors may hurt mask quality only a little or a
lot, depending on how large the error is. Please use your best judgment when choosing mask scores,
and try to stay consistent from mask-to-mask. Here are some general guidelines for what different
scores should correspond to:
• A score of 1: It is not possible to tell what object this mask corresponds to. This includes the
case that there is no mask visible at all.
• A low score (2-4): The object is mostly identiﬁable, but the mask quality is extremely poor
(e.g. large regions of the mask cover other objects; large regions of the object missing; ex-
tremely splotchy mask boundaries that cut through the middle of the object).
• A mid score (5-6): The object is identiﬁable and the boundary is mostly correct, but there
are major errors (missing a signiﬁcant disconnected part of the object; containing a signiﬁcant
part of another object; very poor boundary quality in one area of the object but not the entire
object).
• A high score (7-9): The object is identiﬁable and errors are small and rare (missing a small,
heavily obscured disconnected component, having small regions where the mask boundary
does not quite match the object boundary).
• A score of 10: The mask is pixel-perfect; it has no identiﬁable errors at all.
Mask Scoring
Example of a mask with a score of 1: It is not clear what
object this mask corresponds to.
Example of a mask with a low score (2-4): The main ob-
ject is identiﬁable, but the mask includes a large, incorrect
portion of another object.
Example of a mask with a low score (2-4): The main ob-
ject is identiﬁable, but a large, random part of the object is
missing.
Example of a mask with a low-to-medium score (4-5): The
object is identiﬁable and the edges are all correct, but the
mask incorrectly includes the hand of the person on the left.
Example of a mask with a medium score (5-6): The mask
clearly corresponds to the plate, but the boundary with the
wafﬂe is quite poor.
Example of a mask with a medium score (5-6): the object
is easy to identify, and most of the edges make sense. How-
ever, there is a signiﬁcant disconnected part (their arm inside
the frame) that is mostly missing, as well as splotchy pixels
in this region.
Example of a mask with a medium-to-high score (6-8): the
mask has two small-ish regions of poor boundary, at the top
of the mask and on the bottom right.
Example of a mask with a medium-to-high score (6-8): The
wreath is a valid object that is the size of the box (the entire
wreath + clock would also be a valid object). However, there
are incorrect stray mask pixels on the clock.
Example of a mask with a high score (7-9): The boundary of
the horse is almost entirely correct, except for the right side
of its back leg. The mask consistently includes all of the
equipment that horse is wearing, and has logical boundaries.
Example of a mask with a very high score ( 9): There are
only minor errors around the edge of the mask. The blocky
‘pixelation’ is not an error, since the image is also blocky at
this scale.
Example of a mask with a very high score (9-10): the mask
has only very minor errors in the edge on the bottom right.
Example of a mask with a very high score (9-10): There are
only minor errors around the edge of the mask.
Figure 20: Here we provide the complete guidelines given to annotations for the human review of mask quality. Some images
been edited slightly and faces have been blurred to enable release. Best viewed with zoom (part 2 of 2).
30Denoising Diffusion Probabilistic Models
Jonathan Ho
UC Berkeley
jonathanho@berkeley.eduAjay Jain
UC Berkeley
ajayj@berkeley.eduPieter Abbeel
UC Berkeley
pabbeel@cs.berkeley.edu
Abstract
We present high quality image synthesis results using diffusion probabilistic models,
a class of latent variable models inspired by considerations from nonequilibrium
thermodynamics. Our best results are obtained by training on a weighted variational
bound designed according to a novel connection between diffusion probabilistic
models and denoising score matching with Langevin dynamics, and our models nat-
urally admit a progressive lossy decompression scheme that can be interpreted as a
generalization of autoregressive decoding. On the unconditional CIFAR10 dataset,
we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On
256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our imple-
mentation is available at https://github.com/hojonathanho/diffusion .
1 Introduction
Deep generative models of all kinds have recently exhibited high quality samples in a wide variety
of data modalities. Generative adversarial networks (GANs), autoregressive models, ﬂows, and
variational autoencoders (V AEs) have synthesized striking image and audio samples [ 14,27,3,
58,38,25,10,32,44,57,26,33,45], and there have been remarkable advances in energy-based
modeling and score matching that have produced images comparable to those of GANs [11, 55].
Figure 1: Generated samples on CelebA-HQ 256256(left) and unconditional CIFAR10 (right)
34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.arXiv:2006.11239v2  [cs.LG]  16 Dec 2020 !<latexit sha1_base64="7yFrn0YPyuP5dVIvc7Tl2zcbS/g=">AAAB+HicbVBNSwMxEJ2tX7V+dNWjl2ARPJXdKuix6MVjBfsB7VKyaXYbmk2WJKvU0l/ixYMiXv0p3vw3pu0etPXBwOO9GWbmhSln2njet1NYW9/Y3Cpul3Z29/bL7sFhS8tMEdokkkvVCbGmnAnaNMxw2kkVxUnIaTsc3cz89gNVmklxb8YpDRIcCxYxgo2V+m65x6WIFYuHBislH/tuxat6c6BV4uekAjkafferN5AkS6gwhGOtu76XmmCClWGE02mpl2maYjLCMe1aKnBCdTCZHz5Fp1YZoEgqW8Kgufp7YoITrcdJaDsTbIZ62ZuJ/3ndzERXwYSJNDNUkMWiKOPISDRLAQ2YosTwsSWYKGZvRWSIFSbGZlWyIfjLL6+SVq3qn1drdxeV+nUeRxGO4QTOwIdLqMMtNKAJBDJ4hld4c56cF+fd+Vi0Fpx85gj+wPn8AXOGk5o=</latexit>
xT !··· !xt     !xt 1 !··· !x0
<latexit sha1_base64="l4LvSgM7PR7I/kkuy5soikK4gpU=">AAAEoXictVLditNAFE7XqGv92a5eejOYLexKLU0VFKRQ9EYvhCrb3YUklOlk2g6dnzBzYrcb8zK+lU/gazhJK6atuiB4YODM+T/n+8YJZwY6nW+1vRvuzVu39+/U7967/+CgcfjwzKhUEzokiit9McaGcibpEBhwepFoisWY0/Px/G3hP/9MtWFKnsIyoZHAU8kmjGCwplHjeygwzAjThNM4Kz/jSXaZj05zFHIlp5pNZ4C1VgsUkliB2TX/oQLYCpe/4rJwZhJM6NPMJyLPt9IM0SwBA0tOUaVGBs/8/J8mWVRH6eSjhtdpd0pBu4q/VjxnLYPR4d7XMFYkFVQC4diYwO8kEGVYA7P183qYGmr3meMpDawqsaAmykpEctS0lhhNlLZPAiqt1YwMC2OWYmwjiynNtq8w/s4XpDB5FWVMJilQSVaNJilHoFABL4qZpgT40irYntTOisgMa0zAkqC+0QbY/MquIfCcYssbsBH1UNIFUUJgGVePGfhR1qyj1YETXAaH/SqAnp836/lGftUfdNcFiqbBT8L2jouQdvE9iVAoVUyDWONFa5XVYlJSjezEPT+BlmCSiVQgw65or2vBaE0Y5z1e4D/VeBmhstwJyo5C0YeZ53vdo/z19lhVjly71+K6xRb/ZbO/rbLCS8HMwmVZ7W9zeFc567b95+3uxxde/82a3/vOY+eJc+z4zkun77xzBs7QIbUPNVP7Ustdz33vDtxPq9C92jrnkbMhbvAD81mObw==</latexit>p✓(xt 1|xt)
<latexit sha1_base64="XVzP503G8Ma8Lkwk3KKGZcZJbZ0=">AAACEnicbVC7SgNBFJ2Nrxhfq5Y2g0FICsNuFEwZsLGMYB6QLMvsZDYZMvtg5q4Y1nyDjb9iY6GIrZWdf+Mk2SImHrhwOOde7r3HiwVXYFk/Rm5tfWNzK79d2Nnd2z8wD49aKkokZU0aiUh2PKKY4CFrAgfBOrFkJPAEa3uj66nfvmdS8Si8g3HMnIAMQu5zSkBLrlmO3R4MGZBSLyAw9Pz0YeKmcG5P8CNekKDsmkWrYs2AV4mdkSLK0HDN714/oknAQqCCKNW1rRiclEjgVLBJoZcoFhM6IgPW1TQkAVNOOntpgs+00sd+JHWFgGfq4kRKAqXGgac7p0eqZW8q/ud1E/BrTsrDOAEW0vkiPxEYIjzNB/e5ZBTEWBNCJde3YjokklDQKRZ0CPbyy6ukVa3YF5Xq7WWxXsviyKMTdIpKyEZXqI5uUAM1EUVP6AW9oXfj2Xg1PozPeWvOyGaO0R8YX7+bCp4F</latexit>q(xt|xt 1)
<latexit sha1_base64="eAZ87UuTmAQoJ4u19RGH5tA+bCI=">AAACC3icbVC7TgJBFJ31ifhatbSZQEywkOyiiZQkNpaYyCMBspkdZmHC7MOZu0ay0tv4KzYWGmPrD9j5N87CFgieZJIz59ybe+9xI8EVWNaPsbK6tr6xmdvKb+/s7u2bB4dNFcaSsgYNRSjbLlFM8IA1gINg7Ugy4ruCtdzRVeq37plUPAxuYRyxnk8GAfc4JaAlxyzclbo+gaHrJQ8TB/AjnvsmcGZPTh2zaJWtKfAysTNSRBnqjvnd7Yc09lkAVBClOrYVQS8hEjgVbJLvxopFhI7IgHU0DYjPVC+Z3jLBJ1rpYy+U+gWAp+p8R0J8pca+qyvTRdWil4r/eZ0YvGov4UEUAwvobJAXCwwhToPBfS4ZBTHWhFDJ9a6YDokkFHR8eR2CvXjyMmlWyvZ5uXJzUaxVszhy6BgVUAnZ6BLV0DWqowai6Am9oDf0bjwbr8aH8TkrXTGyniP0B8bXL+1hmu8=</latexit>Figure 2: The directed graphical model considered in this work.
This paper presents progress in diffusion probabilistic models [ 53]. A diffusion probabilistic model
(which we will call a “diffusion model” for brevity) is a parameterized Markov chain trained using
variational inference to produce samples matching the data after ﬁnite time. Transitions of this chain
are learned to reverse a diffusion process, which is a Markov chain that gradually adds noise to the
data in the opposite direction of sampling until signal is destroyed. When the diffusion consists of
small amounts of Gaussian noise, it is sufﬁcient to set the sampling chain transitions to conditional
Gaussians too, allowing for a particularly simple neural network parameterization.
Diffusion models are straightforward to deﬁne and efﬁcient to train, but to the best of our knowledge,
there has been no demonstration that they are capable of generating high quality samples. We
show that diffusion models actually are capable of generating high quality samples, sometimes
better than the published results on other types of generative models (Section 4). In addition, we
show that a certain parameterization of diffusion models reveals an equivalence with denoising
score matching over multiple noise levels during training and with annealed Langevin dynamics
during sampling (Section 3.2) [ 55,61]. We obtained our best sample quality results using this
parameterization (Section 4.2), so we consider this equivalence to be one of our primary contributions.
Despite their sample quality, our models do not have competitive log likelihoods compared to other
likelihood-based models (our models do, however, have log likelihoods better than the large estimates
annealed importance sampling has been reported to produce for energy based models and score
matching [ 11,55]). We ﬁnd that the majority of our models’ lossless codelengths are consumed
to describe imperceptible image details (Section 4.3). We present a more reﬁned analysis of this
phenomenon in the language of lossy compression, and we show that the sampling procedure of
diffusion models is a type of progressive decoding that resembles autoregressive decoding along a bit
ordering that vastly generalizes what is normally possible with autoregressive models.
2 Background
Diffusion models [ 53] are latent variable models of the form p(x0):=R
p(x0:T)dx1:T, where
x1;:::;xTare latents of the same dimensionality as the data x0q(x0). The joint distribution
p(x0:T)is called the reverse process , and it is deﬁned as a Markov chain with learned Gaussian
transitions starting at p(xT) =N(xT;0;I):
p(x0:T):=p(xT)TY
t=1p(xt 1jxt); p(xt 1jxt):=N(xt 1;(xt;t);(xt;t)) (1)
What distinguishes diffusion models from other types of latent variable models is that the approximate
posteriorq(x1:Tjx0), called the forward process ordiffusion process , is ﬁxed to a Markov chain that
gradually adds Gaussian noise to the data according to a variance schedule 1;:::;T:
q(x1:Tjx0):=TY
t=1q(xtjxt 1); q (xtjxt 1):=N(xt;p
1 txt 1;tI) (2)
Training is performed by optimizing the usual variational bound on negative log likelihood:
E[ logp(x0)]Eq
 logp(x0:T)
q(x1:Tjx0)
=Eq
 logp(xT) X
t1logp(xt 1jxt)
q(xtjxt 1)
=:L(3)
The forward process variances tcan be learned by reparameterization [ 33] or held constant as
hyperparameters, and expressiveness of the reverse process is ensured in part by the choice of
Gaussian conditionals in p(xt 1jxt), because both processes have the same functional form when
tare small [ 53]. A notable property of the forward process is that it admits sampling xtat an
arbitrary timestep tin closed form: using the notation t:= 1 tandt:=Qt
s=1s, we have
q(xtjx0) =N(xt;ptx0;(1 t)I) (4)
2Efﬁcient training is therefore possible by optimizing random terms of Lwith stochastic gradient
descent. Further improvements come from variance reduction by rewriting L(3) as:
Eq
DKL(q(xTjx0)kp(xT))|{z}
LT+X
t>1DKL(q(xt 1jxt;x0)kp(xt 1jxt))| {z }
Lt 1 logp(x0jx1)|{z}
L0
(5)
(See Appendix A for details. The labels on the terms are used in Section 3.) Equation (5) uses KL
divergence to directly compare p(xt 1jxt)against forward process posteriors, which are tractable
when conditioned on x0:
q(xt 1jxt;x0) =N(xt 1;~t(xt;x0);~tI); (6)
where ~t(xt;x0):=pt 1t
1 tx0+pt(1 t 1)
1 txtand ~t:=1 t 1
1 tt (7)
Consequently, all KL divergences in Eq. (5) are comparisons between Gaussians, so they can be
calculated in a Rao-Blackwellized fashion with closed form expressions instead of high variance
Monte Carlo estimates.
3 Diffusion models and denoising autoencoders
Diffusion models might appear to be a restricted class of latent variable models, but they allow a
large number of degrees of freedom in implementation. One must choose the variances tof the
forward process and the model architecture and Gaussian distribution parameterization of the reverse
process. To guide our choices, we establish a new explicit connection between diffusion models
and denoising score matching (Section 3.2) that leads to a simpliﬁed, weighted variational bound
objective for diffusion models (Section 3.4). Ultimately, our model design is justiﬁed by simplicity
and empirical results (Section 4). Our discussion is categorized by the terms of Eq. (5).
3.1 Forward process and LT
We ignore the fact that the forward process variances tare learnable by reparameterization and
instead ﬁx them to constants (see Section 4 for details). Thus, in our implementation, the approximate
posteriorqhas no learnable parameters, so LTis a constant during training and can be ignored.
3.2 Reverse process and L1:T 1
Now we discuss our choices in p(xt 1jxt) =N(xt 1;(xt;t);(xt;t))for1<tT. First,
we set (xt;t) =2
tIto untrained time dependent constants. Experimentally, both 2
t=tand
2
t=~t=1 t 1
1 tthad similar results. The ﬁrst choice is optimal for x0N (0;I), and the
second is optimal for x0deterministically set to one point. These are the two extreme choices
corresponding to upper and lower bounds on reverse process entropy for data with coordinatewise
unit variance [53].
Second, to represent the mean (xt;t), we propose a speciﬁc parameterization motivated by the
following analysis of Lt. Withp(xt 1jxt) =N(xt 1;(xt;t);2
tI), we can write:
Lt 1=Eq1
22
tk~t(xt;x0) (xt;t)k2
+C (8)
whereCis a constant that does not depend on . So, we see that the most straightforward parameteri-
zation of is a model that predicts ~t, the forward process posterior mean. However, we can expand
Eq. (8) further by reparameterizing Eq. (4) as xt(x0;) =ptx0+p1 tforN(0;I)and
applying the forward process posterior formula (7):
Lt 1 C=Ex0;"
1
22
t~t
xt(x0;);1pt(xt(x0;) p
1 t)
 (xt(x0;);t)2#
(9)
=Ex0;"
1
22
t1pt
xt(x0;) tp1 t
 (xt(x0;);t)2#
(10)
3Algorithm 1 Training
1:repeat
2:x0q(x0)
3:tUniform(f1;:::;Tg)
4:N(0;I)
5: Take gradient descent step on
r (ptx0+p1 t;t)2
6:until convergedAlgorithm 2 Sampling
1:xTN(0;I)
2:fort=T;:::; 1do
3:zN(0;I)ift>1, elsez=0
4:xt 1=1pt
xt 1 tp1 t(xt;t)
+tz
5:end for
6:return x0
Equation (10) reveals that must predict1pt
xt tp1 t
given xt. Since xtis available as
input to the model, we may choose the parameterization
(xt;t) =~t
xt;1pt(xt p
1 t(xt))
=1pt
xt tp1 t(xt;t)
(11)
where is a function approximator intended to predict fromxt. To sample xt 1p(xt 1jxt)is
to compute xt 1=1pt
xt tp1 t(xt;t)
+tz, where zN(0;I). The complete sampling
procedure, Algorithm 2, resembles Langevin dynamics with as a learned gradient of the data
density. Furthermore, with the parameterization (11), Eq. (10) simpliﬁes to:
Ex0;2
t
22
tt(1 t) (ptx0+p
1 t;t)2
(12)
which resembles denoising score matching over multiple noise scales indexed by t[55]. As Eq. (12)
is equal to (one term of) the variational bound for the Langevin-like reverse process (11), we see
that optimizing an objective resembling denoising score matching is equivalent to using variational
inference to ﬁt the ﬁnite-time marginal of a sampling chain resembling Langevin dynamics.
To summarize, we can train the reverse process mean function approximator to predict ~t, or by
modifying its parameterization, we can train it to predict . (There is also the possibility of predicting
x0, but we found this to lead to worse sample quality early in our experiments.) We have shown that
the-prediction parameterization both resembles Langevin dynamics and simpliﬁes the diffusion
model’s variational bound to an objective that resembles denoising score matching. Nonetheless,
it is just another parameterization of p(xt 1jxt), so we verify its effectiveness in Section 4 in an
ablation where we compare predicting against predicting ~t.
3.3 Data scaling, reverse process decoder, and L0
We assume that image data consists of integers in f0;1;:::; 255gscaled linearly to [ 1;1]. This
ensures that the neural network reverse process operates on consistently scaled inputs starting from
the standard normal prior p(xT). To obtain discrete log likelihoods, we set the last term of the reverse
process to an independent discrete decoder derived from the Gaussian N(x0;(x1;1);2
1I):
p(x0jx1) =DY
i=1Z+(xi
0)
 (xi
0)N(x;i
(x1;1);2
1)dx
+(x) =1 ifx= 1
x+1
255ifx<1 (x) = 1 ifx= 1
x 1
255ifx> 1(13)
whereDis the data dimensionality and the isuperscript indicates extraction of one coordinate.
(It would be straightforward to instead incorporate a more powerful decoder like a conditional
autoregressive model, but we leave that to future work.) Similar to the discretized continuous
distributions used in V AE decoders and autoregressive models [ 34,52], our choice here ensures that
the variational bound is a lossless codelength of discrete data, without need of adding noise to the
data or incorporating the Jacobian of the scaling operation into the log likelihood. At the end of
sampling, we display (x1;1)noiselessly.
3.4 Simpliﬁed training objective
With the reverse process and decoder deﬁned above, the variational bound, consisting of terms derived
from Eqs. (12) and (13), is clearly differentiable with respect to and is ready to be employed for
4Table 1: CIFAR10 results. NLL measured in bits/dim.
Model IS FID NLL Test (Train)
Conditional
EBM [11] 8:30 37 :9
JEM [17] 8:76 38 :4
BigGAN [3] 9:22 14 :73
StyleGAN2 + ADA (v1) [29] 10:06 2:67
Unconditional
Diffusion (original) [53] 5:40
Gated PixelCNN [59] 4:60 65 :93 3:03 (2:90)
Sparse Transformer [7] 2:80
PixelIQN [43] 5:29 49 :46
EBM [11] 6:78 38 :2
NCSNv2 [56] 31:75
NCSN [55] 8:870:12 25:32
SNGAN [39] 8:220:05 21:7
SNGAN-DDLS [4] 9:090:10 15:42
StyleGAN2 + ADA (v1) [29] 9:740:05 3:26
Ours (L, ﬁxed isotropic ) 7:670:13 13:513:70 (3:69)
Ours (Lsimple ) 9:460:11 3:173:75 (3:72)Table 2: Unconditional CIFAR10 reverse
process parameterization and training objec-
tive ablation. Blank entries were unstable to
train and generated poor samples with out-of-
range scores.
Objective IS FID
~prediction (baseline)
L, learned diagonal  7:280:10 23:69
L, ﬁxed isotropic  8:060:09 13:22
k~ ~k2– –
prediction (ours)
L, learned diagonal  – –
L, ﬁxed isotropic  7:670:13 13:51
k~ k2(Lsimple )9:460:11 3:17
training. However, we found it beneﬁcial to sample quality (and simpler to implement) to train on the
following variant of the variational bound:
Lsimple ():=Et;x0;h (ptx0+p
1 t;t)2i
(14)
wheretis uniform between 1andT. Thet= 1 case corresponds to L0with the integral in the
discrete decoder deﬁnition (13) approximated by the Gaussian probability density function times the
bin width, ignoring 2
1and edge effects. The t>1cases correspond to an unweighted version of
Eq. (12), analogous to the loss weighting used by the NCSN denoising score matching model [ 55].
(LTdoes not appear because the forward process variances tare ﬁxed.) Algorithm 1 displays the
complete training procedure with this simpliﬁed objective.
Since our simpliﬁed objective (14) discards the weighting in Eq. (12), it is a weighted variational
bound that emphasizes different aspects of reconstruction compared to the standard variational
bound [ 18,22]. In particular, our diffusion process setup in Section 4 causes the simpliﬁed objective
to down-weight loss terms corresponding to small t. These terms train the network to denoise data
with very small amounts of noise, so it is beneﬁcial to down-weight them so that the network can
focus on more difﬁcult denoising tasks at larger tterms. We will see in our experiments that this
reweighting leads to better sample quality.
4 Experiments
We setT= 1000 for all experiments so that the number of neural network evaluations needed
during sampling matches previous work [ 53,55]. We set the forward process variances to constants
increasing linearly from 1= 10 4toT= 0:02. These constants were chosen to be small
relative to data scaled to [ 1;1], ensuring that reverse and forward processes have approximately
the same functional form while keeping the signal-to-noise ratio at xTas small as possible ( LT=
DKL(q(xTjx0)kN(0;I))10 5bits per dimension in our experiments).
To represent the reverse process, we use a U-Net backbone similar to an unmasked PixelCNN++ [52,
48] with group normalization throughout [ 66]. Parameters are shared across time, which is speciﬁed
to the network using the Transformer sinusoidal position embedding [ 60]. We use self-attention at
the1616feature map resolution [63, 60]. Details are in Appendix B.
4.1 Sample quality
Table 1 shows Inception scores, FID scores, and negative log likelihoods (lossless codelengths) on
CIFAR10. With our FID score of 3.17, our unconditional model achieves better sample quality than
most models in the literature, including class conditional models. Our FID score is computed with
respect to the training set, as is standard practice; when we compute it with respect to the test set, the
score is 5.24, which is still better than many of the training set FID scores in the literature.
5Figure 3: LSUN Church samples. FID= 7:89
 Figure 4: LSUN Bedroom samples. FID= 4:90
Algorithm 3 Sending x0
1: Send xTq(xTjx0)usingp(xT)
2:fort=T 1;:::; 2;1do
3: Send xtq(xtjxt+1;x0)usingp(xtjxt+1)
4:end for
5: Send x0usingp(x0jx1)Algorithm 4 Receiving
1: Receive xTusingp(xT)
2:fort=T 1;:::; 1;0do
3: Receive xtusingp(xtjxt+1)
4:end for
5:return x0
We ﬁnd that training our models on the true variational bound yields better codelengths than training
on the simpliﬁed objective, as expected, but the latter yields the best sample quality. See Fig. 1 for
CIFAR10 and CelebA-HQ 256256samples, Fig. 3 and Fig. 4 for LSUN 256256samples [ 71],
and Appendix D for more.
4.2 Reverse process parameterization and training objective ablation
In Table 2, we show the sample quality effects of reverse process parameterizations and training
objectives (Section 3.2). We ﬁnd that the baseline option of predicting ~works well only when
trained on the true variational bound instead of unweighted mean squared error, a simpliﬁed objective
akin to Eq. (14). We also see that learning reverse process variances (by incorporating a parameterized
diagonal (xt)into the variational bound) leads to unstable training and poorer sample quality
compared to ﬁxed variances. Predicting , as we proposed, performs approximately as well as
predicting ~when trained on the variational bound with ﬁxed variances, but much better when trained
with our simpliﬁed objective.
4.3 Progressive coding
Table 1 also shows the codelengths of our CIFAR10 models. The gap between train and test is at
most 0.03 bits per dimension, which is comparable to the gaps reported with other likelihood-based
models and indicates that our diffusion model is not overﬁtting (see Appendix D for nearest neighbor
visualizations). Still, while our lossless codelengths are better than the large estimates reported for
energy based models and score matching using annealed importance sampling [ 11], they are not
competitive with other types of likelihood-based generative models [7].
Since our samples are nonetheless of high quality, we conclude that diffusion models have an inductive
bias that makes them excellent lossy compressors. Treating the variational bound terms L1++LT
as rate andL0as distortion, our CIFAR10 model with the highest quality samples has a rate of 1.78
bits/dim and a distortion of 1.97 bits/dim, which amounts to a root mean squared error of 0.95 on a
scale from 0 to 255. More than half of the lossless codelength describes imperceptible distortions.
Progressive lossy compression We can probe further into the rate-distortion behavior of our model
by introducing a progressive lossy code that mirrors the form of Eq. (5): see Algorithms 3 and 4,
which assume access to a procedure, such as minimal random coding [ 19,20], that can transmit a
sample xq(x)using approximately DKL(q(x)kp(x))bits on average for any distributions pand
q, for which only pis available to the receiver beforehand. When applied to x0q(x0), Algorithms 3
and 4 transmit xT;:::;x0in sequence using a total expected codelength equal to Eq. (5). The receiver,
6at any timet, has the partial information xtfully available and can progressively estimate:
x0^x0= 
xt p
1 t(xt)
=pt (15)
due to Eq. (4). (A stochastic reconstruction x0p(x0jxt)is also valid, but we do not consider
it here because it makes distortion more difﬁcult to evaluate.) Figure 5 shows the resulting rate-
distortion plot on the CIFAR10 test set. At each time t, the distortion is calculated as the root mean
squared errorp
kx0 ^x0k2=D, and the rate is calculated as the cumulative number of bits received
so far at time t. The distortion decreases steeply in the low-rate region of the rate-distortion plot,
indicating that the majority of the bits are indeed allocated to imperceptible distortions.
0200 400 600 8001;000020406080
Reverse process steps ( T t)Distortion (RMSE)
0200 400 600 8001;00000:511:5
Reverse process steps ( T t)Rate (bits/dim)
0 0:5 1 1:5020406080
Rate (bits/dim)Distortion (RMSE)
Figure 5: Unconditional CIFAR10 test set rate-distortion vs. time. Distortion is measured in root mean squared
error on a [0;255] scale. See Table 4 for details.
Progressive generation We also run a progressive unconditional generation process given by
progressive decompression from random bits. In other words, we predict the result of the reverse
process, ^x0, while sampling from the reverse process using Algorithm 2. Figures 6 and 10 show the
resulting sample quality of ^x0over the course of the reverse process. Large scale image features
appear ﬁrst and details appear last. Figure 7 shows stochastic predictions x0p(x0jxt)withxt
frozen for various t. Whentis small, all but ﬁne details are preserved, and when tis large, only large
scale features are preserved. Perhaps these are hints of conceptual compression [18].
Figure 6: Unconditional CIFAR10 progressive generation ( ^x0over time, from left to right). Extended samples
and sample quality metrics over time in the appendix (Figs. 10 and 14).
Figure 7: When conditioned on the same latent, CelebA-HQ 256256samples share high-level attributes.
Bottom-right quadrants are xt, and other quadrants are samples from p(x0jxt).
Connection to autoregressive decoding Note that the variational bound (5) can be rewritten as:
L=DKL(q(xT)kp(xT)) +Eq"X
t1DKL(q(xt 1jxt)kp(xt 1jxt))#
+H(x0) (16)
(See Appendix A for a derivation.) Now consider setting the diffusion process length Tto the
dimensionality of the data, deﬁning the forward process so that q(xtjx0)places all probability mass
onx0with the ﬁrst tcoordinates masked out (i.e. q(xtjxt 1)masks out the tthcoordinate), setting
p(xT)to place all mass on a blank image, and, for the sake of argument, taking p(xt 1jxt)to
7Figure 8: Interpolations of CelebA-HQ 256x256 images with 500 timesteps of diffusion.
be a fully expressive conditional distribution. With these choices, DKL(q(xT)kp(xT)) = 0 , and
minimizingDKL(q(xt 1jxt)kp(xt 1jxt))trainspto copy coordinates t+ 1;:::;T unchanged
and to predict the tthcoordinate given t+ 1;:::;T . Thus, training pwith this particular diffusion is
training an autoregressive model.
We can therefore interpret the Gaussian diffusion model (2) as a kind of autoregressive model with
a generalized bit ordering that cannot be expressed by reordering data coordinates. Prior work has
shown that such reorderings introduce inductive biases that have an impact on sample quality [ 38],
so we speculate that the Gaussian diffusion serves a similar purpose, perhaps to greater effect since
Gaussian noise might be more natural to add to images compared to masking noise. Moreover, the
Gaussian diffusion length is not restricted to equal the data dimension; for instance, we use T= 1000 ,
which is less than the dimension of the 32323or2562563images in our experiments.
Gaussian diffusions can be made shorter for fast sampling or longer for model expressiveness.
4.4 Interpolation
We can interpolate source images x0;x0
0q(x0)in latent space using qas a stochastic encoder,
xt;x0
tq(xtjx0), then decoding the linearly interpolated latent xt= (1 )x0+x0
0into image
space by the reverse process, x0p(x0jxt). In effect, we use the reverse process to remove
artifacts from linearly interpolating corrupted versions of the source images, as depicted in Fig. 8
(left). We ﬁxed the noise for different values of soxtandx0
tremain the same. Fig. 8 (right)
shows interpolations and reconstructions of original CelebA-HQ 256256images (t= 500 ). The
reverse process produces high-quality reconstructions, and plausible interpolations that smoothly
vary attributes such as pose, skin tone, hairstyle, expression and background, but not eyewear. Larger
tresults in coarser and more varied interpolations, with novel samples at t= 1000 (Appendix Fig. 9).
5 Related Work
While diffusion models might resemble ﬂows [ 9,46,10,32,5,16,23] and V AEs [ 33,47,37],
diffusion models are designed so that qhas no parameters and the top-level latent xThas nearly zero
mutual information with the data x0. Our -prediction reverse process parameterization establishes a
connection between diffusion models and denoising score matching over multiple noise levels with
annealed Langevin dynamics for sampling [ 55,56]. Diffusion models, however, admit straightforward
log likelihood evaluation, and the training procedure explicitly trains the Langevin dynamics sampler
using variational inference (see Appendix C for details). The connection also has the reverse
implication that a certain weighted form of denoising score matching is the same as variational
inference to train a Langevin-like sampler. Other methods for learning transition operators of Markov
chains include infusion training [ 2], variational walkback [ 15], generative stochastic networks [ 1],
and others [50, 54, 36, 42, 35, 65].
By the known connection between score matching and energy-based modeling, our work could have
implications for other recent work on energy-based models [ 67–69,12,70,13,11,41,17,8]. Our
rate-distortion curves are computed over time in one evaluation of the variational bound, reminiscent
of how rate-distortion curves can be computed over distortion penalties in one run of annealed
importance sampling [ 24]. Our progressive decoding argument can be seen in convolutional DRAW
and related models [ 18,40] and may also lead to more general designs for subscale orderings or
sampling strategies for autoregressive models [38, 64].
86 Conclusion
We have presented high quality image samples using diffusion models, and we have found connections
among diffusion models and variational inference for training Markov chains, denoising score
matching and annealed Langevin dynamics (and energy-based models by extension), autoregressive
models, and progressive lossy compression. Since diffusion models seem to have excellent inductive
biases for image data, we look forward to investigating their utility in other data modalities and as
components in other types of generative models and machine learning systems.
Broader Impact
Our work on diffusion models takes on a similar scope as existing work on other types of deep
generative models, such as efforts to improve the sample quality of GANs, ﬂows, autoregressive
models, and so forth. Our paper represents progress in making diffusion models a generally useful
tool in this family of techniques, so it may serve to amplify any impacts that generative models have
had (and will have) on the broader world.
Unfortunately, there are numerous well-known malicious uses of generative models. Sample gen-
eration techniques can be employed to produce fake images and videos of high proﬁle ﬁgures for
political purposes. While fake images were manually created long before software tools were avail-
able, generative models such as ours make the process easier. Fortunately, CNN-generated images
currently have subtle ﬂaws that allow detection [ 62], but improvements in generative models may
make this more difﬁcult. Generative models also reﬂect the biases in the datasets on which they
are trained. As many large datasets are collected from the internet by automated systems, it can be
difﬁcult to remove these biases, especially when the images are unlabeled. If samples from generative
models trained on these datasets proliferate throughout the internet, then these biases will only be
reinforced further.
On the other hand, diffusion models may be useful for data compression, which, as data becomes
higher resolution and as global internet trafﬁc increases, might be crucial to ensure accessibility of
the internet to wide audiences. Our work might contribute to representation learning on unlabeled
raw data for a large range of downstream tasks, from image classiﬁcation to reinforcement learning,
and diffusion models might also become viable for creative uses in art, photography, and music.
Acknowledgments and Disclosure of Funding
This work was supported by ONR PECASE and the NSF Graduate Research Fellowship under grant
number DGE-1752814. Google’s TensorFlow Research Cloud (TFRC) provided Cloud TPUs.
References
[1]Guillaume Alain, Yoshua Bengio, Li Yao, Jason Yosinski, Eric Thibodeau-Laufer, Saizheng Zhang, and
Pascal Vincent. GSNs: generative stochastic networks. Information and Inference: A Journal of the IMA ,
5(2):210–249, 2016.
[2]Florian Bordes, Sina Honari, and Pascal Vincent. Learning to generate samples from noise through infusion
training. In International Conference on Learning Representations , 2017.
[3]Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high ﬁdelity natural
image synthesis. In International Conference on Learning Representations , 2019.
[4] Tong Che, Ruixiang Zhang, Jascha Sohl-Dickstein, Hugo Larochelle, Liam Paull, Yuan Cao, and Yoshua
Bengio. Your GAN is secretly an energy-based model and you should use discriminator driven latent
sampling. arXiv preprint arXiv:2003.06060 , 2020.
[5]Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential
equations. In Advances in Neural Information Processing Systems , pages 6571–6583, 2018.
[6]Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. PixelSNAIL: An improved autoregres-
sive generative model. In International Conference on Machine Learning , pages 863–871, 2018.
[7]Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509 , 2019.
9[8]Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc’Aurelio Ranzato. Residual energy-based
models for text generation. arXiv preprint arXiv:2004.11714 , 2020.
[9]Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation.
arXiv preprint arXiv:1410.8516 , 2014.
[10] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv
preprint arXiv:1605.08803 , 2016.
[11] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In Advances in
Neural Information Processing Systems , pages 3603–3613, 2019.
[12] Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, and Ying Nian Wu. Learning generative ConvNets
via multi-grid modeling and sampling. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 9155–9164, 2018.
[13] Ruiqi Gao, Erik Nijkamp, Diederik P Kingma, Zhen Xu, Andrew M Dai, and Ying Nian Wu. Flow
contrastive estimation of energy-based models. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 7518–7528, 2020.
[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing
Systems , pages 2672–2680, 2014.
[15] Anirudh Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. Variational walkback: Learning a
transition operator as a stochastic recurrent net. In Advances in Neural Information Processing Systems ,
pages 4392–4402, 2017.
[16] Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, and David Duvenaud. FFJORD: Free-form
continuous dynamics for scalable reversible generative models. In International Conference on Learning
Representations , 2019.
[17] Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and
Kevin Swersky. Your classiﬁer is secretly an energy based model and you should treat it like one. In
International Conference on Learning Representations , 2020.
[18] Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards
conceptual compression. In Advances In Neural Information Processing Systems , pages 3549–3557, 2016.
[19] Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication
complexity of correlation. In Twenty-Second Annual IEEE Conference on Computational Complexity
(CCC’07) , pages 10–23. IEEE, 2007.
[20] Marton Havasi, Robert Peharz, and José Miguel Hernández-Lobato. Minimal random code learning:
Getting bits back from compressed model parameters. In International Conference on Learning Represen-
tations , 2019.
[21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs
trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances in Neural
Information Processing Systems , pages 6626–6637, 2017.
[22] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mo-
hamed, and Alexander Lerchner. beta-V AE: Learning basic visual concepts with a constrained variational
framework. In International Conference on Learning Representations , 2017.
[23] Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving ﬂow-based
generative models with variational dequantization and architecture design. In International Conference on
Machine Learning , 2019.
[24] Sicong Huang, Alireza Makhzani, Yanshuai Cao, and Roger Grosse. Evaluating lossy compression rates of
deep generative models. In International Conference on Machine Learning , 2020.
[25] Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, and
Koray Kavukcuoglu. Video pixel networks. In International Conference on Machine Learning , pages
1771–1779, 2017.
[26] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart,
Florian Stimberg, Aaron van den Oord, Sander Dieleman, and Koray Kavukcuoglu. Efﬁcient neural audio
synthesis. In International Conference on Machine Learning , pages 2410–2419, 2018.
[27] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved
quality, stability, and variation. In International Conference on Learning Representations , 2018.
[28] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial
networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages
104401–4410, 2019.
[29] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training
generative adversarial networks with limited data. arXiv preprint arXiv:2006.06676v1 , 2020.
[30] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and
improving the image quality of StyleGAN. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 8110–8119, 2020.
[31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations , 2015.
[32] Diederik P Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions. In
Advances in Neural Information Processing Systems , pages 10215–10224, 2018.
[33] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114 ,
2013.
[34] Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved
variational inference with inverse autoregressive ﬂow. In Advances in Neural Information Processing
Systems , pages 4743–4751, 2016.
[35] John Lawson, George Tucker, Bo Dai, and Rajesh Ranganath. Energy-inspired models: Learning with
sampler-induced distributions. In Advances in Neural Information Processing Systems , pages 8501–8513,
2019.
[36] Daniel Levy, Matt D. Hoffman, and Jascha Sohl-Dickstein. Generalizing Hamiltonian Monte Carlo with
neural networks. In International Conference on Learning Representations , 2018.
[37] Lars Maaløe, Marco Fraccaro, Valentin Liévin, and Ole Winther. BIV A: A very deep hierarchy of
latent variables for generative modeling. In Advances in Neural Information Processing Systems , pages
6548–6558, 2019.
[38] Jacob Menick and Nal Kalchbrenner. Generating high ﬁdelity images with subscale pixel networks and
multidimensional upscaling. In International Conference on Learning Representations , 2019.
[39] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In International Conference on Learning Representations , 2018.
[40] Alex Nichol. VQ-DRAW: A sequential discrete V AE. arXiv preprint arXiv:2003.01599 , 2020.
[41] Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and Ying Nian Wu. On the anatomy of MCMC-based
maximum likelihood learning of energy-based models. arXiv preprint arXiv:1903.12370 , 2019.
[42] Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-persistent
short-run MCMC toward energy-based model. In Advances in Neural Information Processing Systems ,
pages 5233–5243, 2019.
[43] Georg Ostrovski, Will Dabney, and Remi Munos. Autoregressive quantile networks for generative modeling.
InInternational Conference on Machine Learning , pages 3936–3945, 2018.
[44] Ryan Prenger, Rafael Valle, and Bryan Catanzaro. WaveGlow: A ﬂow-based generative network for
speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP) , pages 3617–3621. IEEE, 2019.
[45] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-ﬁdelity images with VQ-
V AE-2. In Advances in Neural Information Processing Systems , pages 14837–14847, 2019.
[46] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. In International
Conference on Machine Learning , pages 1530–1538, 2015.
[47] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approx-
imate inference in deep generative models. In International Conference on Machine Learning , pages
1278–1286, 2014.
[48] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical
image segmentation. In International Conference on Medical Image Computing and Computer-Assisted
Intervention , pages 234–241. Springer, 2015.
[49] Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. In Advances in Neural Information Processing Systems , pages 901–909,
2016.
[50] Tim Salimans, Diederik Kingma, and Max Welling. Markov Chain Monte Carlo and variational inference:
Bridging the gap. In International Conference on Machine Learning , pages 1218–1226, 2015.
11[51] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved
techniques for training gans. In Advances in Neural Information Processing Systems , pages 2234–2242,
2016.
[52] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the PixelCNN
with discretized logistic mixture likelihood and other modiﬁcations. In International Conference on
Learning Representations , 2017.
[53] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In International Conference on Machine Learning , pages
2256–2265, 2015.
[54] Jiaming Song, Shengjia Zhao, and Stefano Ermon. A-NICE-MC: Adversarial training for MCMC. In
Advances in Neural Information Processing Systems , pages 5140–5150, 2017.
[55] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In
Advances in Neural Information Processing Systems , pages 11895–11907, 2019.
[56] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. arXiv
preprint arXiv:2006.09011 , 2020.
[57] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal
Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A generative model for raw audio.
arXiv preprint arXiv:1609.03499 , 2016.
[58] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.
International Conference on Machine Learning , 2016.
[59] Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray
Kavukcuoglu. Conditional image generation with PixelCNN decoders. In Advances in Neural Information
Processing Systems , pages 4790–4798, 2016.
[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing
Systems , pages 5998–6008, 2017.
[61] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation ,
23(7):1661–1674, 2011.
[62] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros. Cnn-generated images
are surprisingly easy to spot...for now. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , 2020.
[63] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 7794–7803,
2018.
[64] Auke J Wiggers and Emiel Hoogeboom. Predictive sampling with forecasting autoregressive models.
arXiv preprint arXiv:2002.09928 , 2020.
[65] Hao Wu, Jonas Köhler, and Frank Noé. Stochastic normalizing ﬂows. arXiv preprint arXiv:2002.06707 ,
2020.
[66] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on Computer
Vision (ECCV) , pages 3–19, 2018.
[67] Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In International
Conference on Machine Learning , pages 2635–2644, 2016.
[68] Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Synthesizing dynamic patterns by spatial-temporal
generative convnet. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ,
pages 7093–7101, 2017.
[69] Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, and Ying Nian Wu. Learning
descriptor networks for 3d shape synthesis and analysis. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , pages 8629–8638, 2018.
[70] Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Learning energy-based spatial-temporal generative
convnets for dynamic patterns. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2019.
[71] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: Construction of a large-scale
image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365 , 2015.
[72] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,
2016.
12Extra information
LSUN FID scores for LSUN datasets are included in Table 3. Scores marked withare reported
by StyleGAN2 as baselines, and other scores are reported by their respective authors.
Table 3: FID scores for LSUN 256256datasets
Model LSUN Bedroom LSUN Church LSUN Cat
ProgressiveGAN [27] 8.34 6.42 37.52
StyleGAN [28] 2.65 4.218.53
StyleGAN2 [30] - 3.86 6.93
Ours (Lsimple ) 6.36 7.89 19.75
Ours (Lsimple , large) 4.90 - -
Progressive compression Our lossy compression argument in Section 4.3 is only a proof of concept,
because Algorithms 3 and 4 depend on a procedure such as minimal random coding [ 20], which is
not tractable for high dimensional data. These algorithms serve as a compression interpretation of the
variational bound (5) of Sohl-Dickstein et al. [53], not yet as a practical compression system.
Table 4: Unconditional CIFAR10 test set rate-distortion values (accompanies Fig. 5)
Reverse process time ( T t+ 1) Rate (bits/dim) Distortion (RMSE [0;255])
1000 1.77581 0.95136
900 0.11994 12.02277
800 0.05415 18.47482
700 0.02866 24.43656
600 0.01507 30.80948
500 0.00716 38.03236
400 0.00282 46.12765
300 0.00081 54.18826
200 0.00013 60.97170
100 0.00000 67.60125
A Extended derivations
Below is a derivation of Eq. (5), the reduced variance variational bound for diffusion models. This
material is from Sohl-Dickstein et al. [53]; we include it here only for completeness.
L=Eq
 logp(x0:T)
q(x1:Tjx0)
(17)
=Eq2
4 logp(xT) X
t1logp(xt 1jxt)
q(xtjxt 1)3
5 (18)
=Eq"
 logp(xT) X
t>1logp(xt 1jxt)
q(xtjxt 1) logp(x0jx1)
q(x1jx0)#
(19)
=Eq"
 logp(xT) X
t>1logp(xt 1jxt)
q(xt 1jxt;x0)q(xt 1jx0)
q(xtjx0) logp(x0jx1)
q(x1jx0)#
(20)
=Eq"
 logp(xT)
q(xTjx0) X
t>1logp(xt 1jxt)
q(xt 1jxt;x0) logp(x0jx1)#
(21)
13=Eq"
DKL(q(xTjx0)kp(xT)) +X
t>1DKL(q(xt 1jxt;x0)kp(xt 1jxt)) logp(x0jx1)#
(22)
The following is an alternate version of L. It is not tractable to estimate, but it is useful for our
discussion in Section 4.3.
L=Eq2
4 logp(xT) X
t1logp(xt 1jxt)
q(xtjxt 1)3
5 (23)
=Eq2
4 logp(xT) X
t1logp(xt 1jxt)
q(xt 1jxt)q(xt 1)
q(xt)3
5 (24)
=Eq2
4 logp(xT)
q(xT) X
t1logp(xt 1jxt)
q(xt 1jxt) logq(x0)3
5 (25)
=DKL(q(xT)kp(xT)) +Eq2
4X
t1DKL(q(xt 1jxt)kp(xt 1jxt))3
5+H(x0) (26)
B Experimental details
Our neural network architecture follows the backbone of PixelCNN++ [ 52], which is a U-Net [ 48]
based on a Wide ResNet [ 72]. We replaced weight normalization [ 49] with group normalization [ 66]
to make the implementation simpler. Our 3232models use four feature map resolutions ( 3232
to44), and our 256256models use six. All models have two convolutional residual blocks
per resolution level and self-attention blocks at the 1616resolution between the convolutional
blocks [ 6]. Diffusion time tis speciﬁed by adding the Transformer sinusoidal position embedding [ 60]
into each residual block. Our CIFAR10 model has 35.7 million parameters, and our LSUN and
CelebA-HQ models have 114 million parameters. We also trained a larger variant of the LSUN
Bedroom model with approximately 256 million parameters by increasing ﬁlter count.
We used TPU v3-8 (similar to 8 V100 GPUs) for all experiments. Our CIFAR model trains at 21
steps per second at batch size 128 (10.6 hours to train to completion at 800k steps), and sampling
a batch of 256 images takes 17 seconds. Our CelebA-HQ/LSUN (2562) models train at 2.2 steps
per second at batch size 64, and sampling a batch of 128 images takes 300 seconds. We trained on
CelebA-HQ for 0.5M steps, LSUN Bedroom for 2.4M steps, LSUN Cat for 1.8M steps, and LSUN
Church for 1.2M steps. The larger LSUN Bedroom model was trained for 1.15M steps.
Apart from an initial choice of hyperparameters early on to make network size ﬁt within memory
constraints, we performed the majority of our hyperparameter search to optimize for CIFAR10 sample
quality, then transferred the resulting settings over to the other datasets:
•We chose the tschedule from a set of constant, linear, and quadratic schedules, all
constrained so that LT0. We setT= 1000 without a sweep, and we chose a linear
schedule from 1= 10 4toT= 0:02.
•We set the dropout rate on CIFAR10 to 0:1by sweeping over the values f0:1;0:2;0:3;0:4g.
Without dropout on CIFAR10, we obtained poorer samples reminiscent of the overﬁtting
artifacts in an unregularized PixelCNN++ [ 52]. We set dropout rate on the other datasets to
zero without sweeping.
•We used random horizontal ﬂips during training for CIFAR10; we tried training both with
and without ﬂips, and found ﬂips to improve sample quality slightly. We also used random
horizontal ﬂips for all other datasets except LSUN Bedroom.
•We tried Adam [ 31] and RMSProp early on in our experimentation process and chose the
former. We left the hyperparameters to their standard values. We set the learning rate to
210 4without any sweeping, and we lowered it to 210 5for the 256256images,
which seemed unstable to train with the larger learning rate.
14•We set the batch size to 128 for CIFAR10 and 64 for larger images. We did not sweep over
these values.
•We used EMA on model parameters with a decay factor of 0.9999. We did not sweep over
this value.
Final experiments were trained once and evaluated throughout training for sample quality. Sample
quality scores and log likelihood are reported on the minimum FID value over the course of training.
On CIFAR10, we calculated Inception and FID scores on 50000 samples using the original code
from the OpenAI [ 51] and TTUR [ 21] repositories, respectively. On LSUN, we calculated FID
scores on 50000 samples using code from the StyleGAN2 [ 30] repository. CIFAR10 and CelebA-HQ
were loaded as provided by TensorFlow Datasets ( https://www.tensorflow.org/datasets ),
and LSUN was prepared using code from StyleGAN. Dataset splits (or lack thereof) are standard
from the papers that introduced their usage in a generative modeling context. All details can be found
in the source code release.
C Discussion on related work
Our model architecture, forward process deﬁnition, and prior differ from NCSN [ 55,56] in subtle but
important ways that improve sample quality, and, notably, we directly train our sampler as a latent
variable model rather than adding it after training post-hoc. In greater detail:
1.We use a U-Net with self-attention; NCSN uses a ReﬁneNet with dilated convolutions. We
condition all layers on tby adding in the Transformer sinusoidal position embedding, rather
than only in normalization layers (NCSNv1) or only at the output (v2).
2.Diffusion models scale down the data with each forward process step (by ap1 tfactor)
so that variance does not grow when adding noise, thus providing consistently scaled inputs
to the neural net reverse process. NCSN omits this scaling factor.
3.Unlike NCSN, our forward process destroys signal ( DKL(q(xTjx0)kN(0;I))0), ensur-
ing a close match between the prior and aggregate posterior of xT. Also unlike NCSN, our
tare very small, which ensures that the forward process is reversible by a Markov chain
with conditional Gaussians. Both of these factors prevent distribution shift when sampling.
4.Our Langevin-like sampler has coefﬁcients (learning rate, noise scale, etc.) derived rig-
orously from tin the forward process. Thus, our training procedure directly trains our
sampler to match the data distribution after Tsteps: it trains the sampler as a latent variable
model using variational inference. In contrast, NCSN’s sampler coefﬁcients are set by hand
post-hoc, and their training procedure is not guaranteed to directly optimize a quality metric
of their sampler.
D Samples
Additional samples Figure 11, 13, 16, 17, 18, and 19 show uncurated samples from the diffusion
models trained on CelebA-HQ, CIFAR10 and LSUN datasets.
Latent structure and reverse process stochasticity During sampling, both the prior xT
N(0;I)and Langevin dynamics are stochastic. To understand the signiﬁcance of the second source
of noise, we sampled multiple images conditioned on the same intermediate latent for the CelebA
256256dataset. Figure 7 shows multiple draws from the reverse process x0p(x0jxt)that
share the latent xtfort2f1000;750;500;250g. To accomplish this, we run a single reverse chain
from an initial draw from the prior. At the intermediate timesteps, the chain is split to sample multiple
images. When the chain is split after the prior draw at xT=1000 , the samples differ signiﬁcantly.
However, when the chain is split after more steps, samples share high-level attributes like gender,
hair color, eyewear, saturation, pose and facial expression. This indicates that intermediate latents
likex750encode these attributes, despite their imperceptibility.
Coarse-to-ﬁne interpolation Figure 9 shows interpolations between a pair of source CelebA
256256images as we vary the number of diffusion steps prior to latent space interpolation.
Increasing the number of diffusion steps destroys more structure in the source images, which the
15model completes during the reverse process. This allows us to interpolate at both ﬁne granularities
and coarse granularities. In the limiting case of 0diffusion steps, the interpolation mixes source
images in pixel space. On the other hand, after 1000 diffusion steps, source information is lost and
interpolations are novel samples.
SourceRec.λ=0.1λ=0.2λ=0.3λ=0.4λ=0.5λ=0.6λ=0.7λ=0.8λ=0.9Rec.Source1000 steps875 steps750 steps625 steps500 steps375 steps250 steps125 steps0 steps
Figure 9: Coarse-to-ﬁne interpolations that vary the number of diffusion steps prior to latent mixing.
0 200 400 600 800 1;000246810
Reverse process steps ( T t)Inception Score
0 200 400 600 800 1;0000100200300
Reverse process steps ( T t)FID
Figure 10: Unconditional CIFAR10 progressive sampling quality over time
16Figure 11: CelebA-HQ 256256generated samples
17(a) Pixel space nearest neighbors
(b) Inception feature space nearest neighbors
Figure 12: CelebA-HQ 256256nearest neighbors, computed on a 100100crop surrounding the
faces. Generated samples are in the leftmost column, and training set nearest neighbors are in the
remaining columns.
18Figure 13: Unconditional CIFAR10 generated samples
19Figure 14: Unconditional CIFAR10 progressive generation
20(a) Pixel space nearest neighbors
(b) Inception feature space nearest neighbors
Figure 15: Unconditional CIFAR10 nearest neighbors. Generated samples are in the leftmost column,
and training set nearest neighbors are in the remaining columns.
21Figure 16: LSUN Church generated samples. FID= 7:89
22Figure 17: LSUN Bedroom generated samples, large model. FID= 4:90
23Figure 18: LSUN Bedroom generated samples, small model. FID= 6:36
24Figure 19: LSUN Cat generated samples. FID= 19:75
25Neural Discrete Representation Learning
Aaron van den Oord
DeepMind
avdnoord@google.comOriol Vinyals
DeepMind
vinyals@google.comKoray Kavukcuoglu
DeepMind
korayk@google.com
Abstract
Learning useful representations without supervision remains a key challenge in
machine learning. In this paper, we propose a simple yet powerful generative
model that learns such discrete representations. Our model, the Vector Quantised-
Variational AutoEncoder (VQ-V AE), differs from V AEs in two key ways: the
encoder network outputs discrete, rather than continuous, codes; and the prior
is learnt rather than static. In order to learn a discrete latent representation, we
incorporate ideas from vector quantisation (VQ). Using the VQ method allows the
model to circumvent issues of “posterior collapse” -— where the latents are ignored
when they are paired with a powerful autoregressive decoder -— typically observed
in the V AE framework. Pairing these representations with an autoregressive prior,
the model can generate high quality images, videos, and speech as well as doing
high quality speaker conversion and unsupervised learning of phonemes, providing
further evidence of the utility of the learnt representations.
1 Introduction
Recent advances in generative modelling of images [ 38,12,13,22,10], audio [ 37,26] and videos
[20,11] have yielded impressive samples and applications [ 24,18]. At the same time, challenging
tasks such as few-shot learning [ 34], domain adaptation [ 17], or reinforcement learning [ 35] heavily
rely on learnt representations from raw data, but the usefulness of generic representations trained in
an unsupervised fashion is still far from being the dominant approach.
Maximum likelihood and reconstruction error are two common objectives used to train unsupervised
models in the pixel domain, however their usefulness depends on the particular application the
features are used in. Our goal is to achieve a model that conserves the important features of the
data in its latent space while optimising for maximum likelihood. As the work in [ 7] suggests, the
best generative models (as measured by log-likelihood) will be those without latents but a powerful
decoder (such as PixelCNN). However, in this paper, we argue for learning discrete and useful latent
variables, which we demonstrate on a variety of domains.
Learning representations with continuous features have been the focus of many previous work
[16,39,6,9] however we concentrate on discrete representations [ 27,33,8,28] which are potentially
a more natural ﬁt for many of the modalities we are interested in. Language is inherently discrete,
similarly speech is typically represented as a sequence of symbols. Images can often be described
concisely by language [ 40]. Furthermore, discrete representations are a natural ﬁt for complex
reasoning, planning and predictive learning (e.g., if it rains, I will use an umbrella). While using
discrete latent variables in deep learning has proven challenging, powerful autoregressive models
have been developed for modelling distributions over discrete variables [37].
In our work, we introduce a new family of generative models succesfully combining the variational
autoencoder (V AE) framework with discrete latent representations through a novel parameterisation
of the posterior distribution of (discrete) latents given an observation. Our model, which relies on
vector quantization (VQ), is simple to train, does not suffer from large variance, and avoids the
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1711.00937v2  [cs.LG]  30 May 2018“posterior collapse” issue which has been problematic with many V AE models that have a powerful
decoder, often caused by latents being ignored. Additionally, it is the ﬁrst discrete latent V AE model
that get similar performance as its continuous counterparts, while offering the ﬂexibility of discrete
distributions. We term our model the VQ-V AE.
Since VQ-V AE can make effective use of the latent space, it can successfully model important
features that usually span many dimensions in data space (for example objects span many pixels in
images, phonemes in speech, the message in a text fragment, etc.) as opposed to focusing or spending
capacity on noise and imperceptible details which are often local.
Lastly, once a good discrete latent structure of a modality is discovered by the VQ-V AE, we train
a powerful prior over these discrete random variables, yielding interesting samples and useful
applications. For instance, when trained on speech we discover the latent structure of language
without any supervision or prior knowledge about phonemes or words. Furthermore, we can equip
our decoder with the speaker identity, which allows for speaker conversion, i.e., transferring the
voice from one speaker to another without changing the contents. We also show promising results on
learning long term structure of environments for RL.
Our contributions can thus be summarised as:
Introducing the VQ-V AE model, which is simple, uses discrete latents, does not suffer from
“posterior collapse” and has no variance issues.
We show that a discrete latent model (VQ-V AE) perform as well as its continuous model
counterparts in log-likelihood.
When paired with a powerful prior, our samples are coherent and high quality on a wide
variety of applications such as speech and video generation.
We show evidence of learning language through raw speech, without any supervision, and
show applications of unsupervised speaker conversion.
2 Related Work
In this work we present a new way of training variational autoencoders [ 23,32] with discrete latent
variables [ 27]. Using discrete variables in deep learning has proven challenging, as suggested by
the dominance of continuous latent variables in most of current work – even when the underlying
modality is inherently discrete.
There exist many alternatives for training discrete V AEs. The NVIL [ 27] estimator use a single-sample
objective to optimise the variational lower bound, and uses various variance-reduction techniques to
speed up training. VIMCO [ 28] optimises a multi-sample objective [ 5], which speeds up convergence
further by using multiple samples from the inference network.
Recently a few authors have suggested the use of a new continuous reparemetrisation based on the
so-called Concrete [ 25] or Gumbel-softmax [ 19] distribution, which is a continuous distribution and
has a temperature constant that can be annealed during training to converge to a discrete distribution
in the limit. In the beginning of training the variance of the gradients is low but biased, and towards
the end of training the variance becomes high but unbiased.
None of the above methods, however, close the performance gap of V AEs with continuous latent
variables where one can use the Gaussian reparameterisation trick which beneﬁts from much lower
variance in the gradients. Furthermore, most of these techniques are typically evaluated on relatively
small datasets such as MNIST, and the dimensionality of the latent distributions is small (e.g., below
8). In our work, we use three complex image datasets (CIFAR10, ImageNet, and DeepMind Lab) and
a raw speech dataset (VCTK).
Our work also extends the line of research where autoregressive distributions are used in the decoder
of V AEs and/or in the prior [ 14]. This has been done for language modelling with LSTM decoders [ 4],
and more recently with dilated convolutional decoders [ 42]. PixelCNNs [ 29,38] are convolutional
autoregressive models which have also been used as distribution in the decoder of V AEs [15, 7].
Finally, our approach also relates to work in image compression with neural networks. Theis et. al.
[36] use scalar quantisation to compress activations for lossy image compression before arithmetic
encoding. Other authors [ 1] propose a method for similar compression model with vector quantisation.
2The authors propose a continuous relaxation of vector quantisation which is annealed over time
to obtain a hard clustering. In their experiments they ﬁrst train an autoencoder, afterwards vector
quantisation is applied to the activations of the encoder, and ﬁnally the whole network is ﬁne tuned
using the soft-to-hard relaxation with a small learning rate. In our experiments we were unable to
train using the soft-to-hard relaxation approach from scratch as the decoder was always able to invert
the continuous relaxation during training, so that no actual quantisation took place.
3 VQ-VAE
Perhaps the work most related to our approach are V AEs. V AEs consist of the following parts:
an encoder network which parameterises a posterior distribution q(zjx)of discrete latent random
variableszgiven the input data x, a prior distribution p(z), and a decoder with a distribution p(xjz)
over input data.
Typically, the posteriors and priors in V AEs are assumed normally distributed with diagonal covari-
ance, which allows for the Gaussian reparametrisation trick to be used [ 32,23]. Extensions include
autoregressive prior and posterior models [ 14], normalising ﬂows [ 31,10], and inverse autoregressive
posteriors [22].
In this work we introduce the VQ-V AE where we use discrete latent variables with a new way of
training, inspired by vector quantisation (VQ). The posterior and prior distributions are categorical,
and the samples drawn from these distributions index an embedding table. These embeddings are
then used as input into the decoder network.
3.1 Discrete Latent variables
We deﬁne a latent embedding space e2RKDwhereKis the size of the discrete latent space (i.e.,
aK-way categorical), and Dis the dimensionality of each latent embedding vector ei. Note that
there areKembedding vectors ei2RD,i21;2;:::;K . As shown in Figure 1, the model takes an
inputx, that is passed through an encoder producing output ze(x). The discrete latent variables z
are then calculated by a nearest neighbour look-up using the shared embedding space eas shown in
equation 1. The input to the decoder is the corresponding embedding vector ekas given in equation 2.
One can see this forward computation pipeline as a regular autoencoder with a particular non-linearity
that maps the latents to 1-of-K embedding vectors. The complete set of parameters for the model are
union of parameters of the encoder, decoder, and the embedding space e. For sake of simplicity we
use a single random variable zto represent the discrete latent variables in this Section, however for
speech, image and videos we actually extract a 1D, 2D and 3D latent feature spaces respectively.
The posterior categorical distribution q(zjx)probabilities are deﬁned as one-hot as follows:
q(z=kjx) =1for k = argmin jkze(x) ejk2,
0otherwise; (1)
whereze(x)is the output of the encoder network. We view this model as a V AE in which we
can bound logp(x)with the ELBO. Our proposal distribution q(z=kjx)is deterministic, and by
deﬁning a simple uniform prior over zwe obtain a KL divergence constant and equal to logK.
The representation ze(x)is passed through the discretisation bottleneck followed by mapping onto
the nearest element of embedding eas given in equations 1 and 2.
zq(x) =ek;wherek=argminjkze(x) ejk2 (2)
3.2 Learning
Note that there is no real gradient deﬁned for equation 2, however we approximate the gradient
similar to the straight-through estimator [ 3] and just copy gradients from decoder input zq(x)to
encoder output ze(x). One could also use the subgradient through the quantisation operation, but this
simple estimator worked well for the initial experiments in this paper.
3Figure 1: Left: A ﬁgure describing the VQ-V AE. Right: Visualisation of the embedding space. The
output of the encoder z(x)is mapped to the nearest point e2. The gradientrzL(in red) will push the
encoder to change its output, which could alter the conﬁguration in the next forward pass.
During forward computation the nearest embedding zq(x)(equation 2) is passed to the decoder, and
during the backwards pass the gradient rzLis passed unaltered to the encoder. Since the output
representation of the encoder and the input to the decoder share the same Ddimensional space,
the gradients contain useful information for how the encoder has to change its output to lower the
reconstruction loss.
As seen on Figure 1 (right), the gradient can push the encoder’s output to be discretised differently in
the next forward pass, because the assignment in equation 1 will be different.
Equation 3 speciﬁes the overall loss function. It is has three components that are used to train
different parts of VQ-V AE. The ﬁrst term is the reconstruction loss (or the data term) which optimizes
the decoder and the encoder (through the estimator explained above). Due to the straight-through
gradient estimation of mapping from ze(x)tozq(x), the embeddings eireceive no gradients from
the reconstruction loss logp(zjzq(x)). Therefore, in order to learn the embedding space, we use one
of the simplest dictionary learning algorithms, Vector Quantisation (VQ). The VQ objective uses
thel2error to move the embedding vectors eitowards the encoder outputs ze(x)as shown in the
second term of equation 3. Because this loss term is only used for updating the dictionary, one can
alternatively also update the dictionary items as function of moving averages of ze(x)(not used for
the experiments in this work). For more details see Appendix A.1.
Finally, since the volume of the embedding space is dimensionless, it can grow arbitrarily if the
embeddings eido not train as fast as the encoder parameters. To make sure the encoder commits to
an embedding and its output does not grow, we add a commitment loss, the third term in equation 3.
Thus, the total training objective becomes:
L= logp(xjzq(x)) +ksg[ze(x)] ek2
2+kze(x) sg[e]k2
2; (3)
where sg stands for the stopgradient operator that is deﬁned as identity at forward computation time
and has zero partial derivatives, thus effectively constraining its operand to be a non-updated constant.
The decoder optimises the ﬁrst loss term only, the encoder optimises the ﬁrst and the last loss terms,
and the embeddings are optimised by the middle loss term. We found the resulting algorithm to be
quite robust to , as the results did not vary for values of ranging from 0:1to2:0. We use= 0:25
in all our experiments, although in general this would depend on the scale of reconstruction loss.
Since we assume a uniform prior for z, the KL term that usually appears in the ELBO is constant
w.r.t. the encoder parameters and can thus be ignored for training.
In our experiments we deﬁne Ndiscrete latents (e.g., we use a ﬁeld of 32 x 32 latents for ImageNet,
or 8 x 8 x 10 for CIFAR10). The resulting loss Lis identical, except that we get an average over N
terms fork-means and commitment loss – one for each latent.
The log-likelihood of the complete model logp(x)can be evaluated as follows:
logp(x) = logX
kp(xjzk)p(zk);
Because the decoder p(xjz)is trained with z=zq(x)from MAP-inference, the decoder should not
allocate any probability mass to p(xjz)forz6=zq(x)once it has fully converged. Thus, we can write
4logp(x)logp(xjzq(x))p(zq(x)). We empirically evaluate this approximation in section 4. From
Jensen’s inequality, we also can write logp(x)logp(xjzq(x))p(zq(x)).
3.3 Prior
The prior distribution over the discrete latents p(z)is a categorical distribution, and can be made
autoregressive by depending on other zin the feature map. Whilst training the VQ-V AE, the prior is
kept constant and uniform. After training, we ﬁt an autoregressive distribution over z,p(z), so that
we can generate xvia ancestral sampling. We use a PixelCNN over the discrete latents for images,
and a WaveNet for raw audio. Training the prior and the VQ-V AE jointly, which could strengthen our
results, is left as future research.
4 Experiments
4.1 Comparison with continuous variables
As a ﬁrst experiment we compare VQ-V AE with normal V AEs (with continuous variables), as well as
VIMCO [ 28] with independent Gaussian or categorical priors. We train these models using the same
standard V AE architecture on CIFAR10, while varying the latent capacity (number of continuous or
discrete latent variables, as well as the dimensionality of the discrete space K). The encoder consists
of 2 strided convolutional layers with stride 2 and window size 44, followed by two residual
33blocks (implemented as ReLU, 3x3 conv, ReLU, 1x1 conv), all having 256 hidden units. The
decoder similarly has two residual 33blocks, followed by two transposed convolutions with stride
2 and window size 44. We use the ADAM optimiser [ 21] with learning rate 2e-4 and evaluate
the performance after 250,000 steps with batch-size 128. For VIMCO we use 50 samples in the
multi-sample training objective.
The V AE, VQ-V AE and VIMCO models obtain 4.51 bits/dim, 4.67 bits/dim and 5.14 respectively.
All reported likelihoods are lower bounds. Our numbers for the continuous V AE are comparable to
those reported for a Deep convolutional V AE: 4.54 bits/dim [13] on this dataset.
Our model is the ﬁrst among those using discrete latent variables which challenges the performance
of continuous V AEs. Thus, we get very good reconstructions like regular V AEs provide, with the
compressed representation that symbolic representations provide. A few interesting characteristics,
implications and applications of the VQ-V AEs that we train is shown in the next subsections.
4.2 Images
Images contain a lot of redundant information as most of the pixels are correlated and noisy, therefore
learning models at the pixel level could be wasteful.
In this experiment we show that we can model x= 1281283images by compressing them to a
z= 32321discrete space (with K=512) via a purely deconvolutional p(xjz). So a reduction of
12812838
3232942:6in bits. We model images by learning a powerful prior (PixelCNN) over z. This
allows to not only greatly speed up training and sampling, but also to use the PixelCNNs capacity to
capture the global structure instead of the low-level statistics of images.
Figure 2: Left: ImageNet 128x128x3 images, right: reconstructions from a VQ-V AE with a 32x32x1
latent space, with K=512.
5Reconstructions from the 32x32x1 space with discrete latents are shown in Figure 2. Even considering
that we greatly reduce the dimensionality with discrete encoding, the reconstructions look only slightly
blurrier than the originals. It would be possible to use a more perceptual loss function than MSE over
pixels here (e.g., a GAN [12]), but we leave that as future work.
Next, we train a PixelCNN prior on the discretised 32x32x1 latent space. As we only have 1 channel
(not 3 as with colours), we only have to use spatial masking in the PixelCNN. The capacity of the
PixelCNN we used was similar to those used by the authors of the PixelCNN paper [38].
Figure 3: Samples (128x128) from a VQ-V AE with a PixelCNN prior trained on ImageNet images.
From left to right: kit fox, gray whale, brown bear, admiral (butterﬂy), coral reef, alp, microwave,
pickup.
Samples drawn from the PixelCNN were mapped to pixel-space with the decoder of the VQ-V AE
and can be seen in Figure 3.
Figure 4: Samples (128x128) from a VQ-V AE with a PixelCNN prior trained on frames captured
from DeepMind Lab.
We also repeat the same experiment for 84x84x3 frames drawn from the DeepMind Lab environment
[2]. The reconstructions looked nearly identical to their originals. Samples drawn from the PixelCNN
prior trained on the 21x21x1 latent space and decoded to the pixel space using a deconvolutional
model decoder can be seen in Figure 4.
Finally, we train a second VQ-V AE with a PixelCNN decoder on top of the 21x21x1 latent space
from the ﬁrst VQ-V AE on DM-LAB frames. This setup typically breaks V AEs as they suffer from
"posterior collapse", i.e., the latents are ignored as the decoder is powerful enough to model x
perfectly. Our model however does not suffer from this, and the latents are meaningfully used. We use
only three latent variables (each with K=512 and their own embedding space e) at the second stage
for modelling the whole image and as such the model cannot reconstruct the image perfectly – which
is consequence of compressing the image onto 3 x 9 bits, i.e. less than a ﬂoat32. Reconstructions
sampled from the discretised global code can be seen in Figure 5.
6Figure 5: Top original images, Bottom: reconstructions from a 2 stage VQ-V AE, with 3 latents to
model the whole image (27 bits), and as such the model cannot reconstruct the images perfectly. The
reconstructions are generated by sampled from the second PixelCNN prior in the 21x21 latent domain
of ﬁrst VQ-V AE, and then decoded with standard VQ-V AE decoder to 84x84. A lot of the original
scene, including textures, room layout and nearby walls remain, but the model does not try to store
the pixel values themselves, which means the textures are generated procedurally by the PixelCNN.
Figure 6: Left: original waveform, middle: reconstructed with same speaker-id, right: reconstructed
with different speaker-id. The contents of the three waveforms are the same.
4.3 Audio
In this set of experiments we evaluate the behaviour of discrete latent variables on models of raw
audio. In all our audio experiments, we train a VQ-V AE that has a dilated convolutional architecture
similar to WaveNet decoder. All samples for this section can be played from the following url:
https://avdnoord.github.io/homepage/vqvae/ .
We ﬁrst consider the VCTK dataset, which has speech recordings of 109 different speakers [ 41].
We train a VQ-V AE where the encoder has 6 strided convolutions with stride 2 and window-size 4.
This yields a latent space 64x smaller than the original waveform. The latents consist of one feature
map and the discrete space is 512-dimensional. The decoder is conditioned on both the latents and a
one-hot embedding for the speaker.
First, we ran an experiment to show that VQ-V AE can extract a latent space that only conserves
long-term relevant information. After training the model, given an audio example, we can encode
it to the discrete latent representation, and reconstruct by sampling from the decoder. Because the
dimensionality of the discrete representation is 64 times smaller, the original sample cannot be
perfectly reconstructed sample by sample. As it can be heard from the provided samples, and as
shown in Figure 7, the reconstruction has the same content (same text contents), but the waveform
is quite different and prosody in the voice is altered. This means that the VQ-V AE has, without
any form of linguistic supervision, learned a high-level abstract space that is invariant to low-level
features and only encodes the content of the speech. This experiment conﬁrms our observations from
before that important features are often those that span many dimensions in the input data space (in
this case phoneme and other high-level content in waveform).
We have then analysed the unconditional samples from the model to understand its capabilities. Given
the compact and abstract latent representation extracted from the audio, we trained the prior on top of
this representation to model the long-term dependencies in the data. For this task we have used a
larger dataset of 460 speakers [ 30] and trained a VQ-V AE model where the resolution of discrete
space is 128 times smaller. Next we trained the prior as usual on top of this representation on chunks
of 40960 timesteps (2.56 seconds), which yields 320 latent timesteps. While samples drawn from even
the best speech models like the original WaveNet [ 37] sound like babbling , samples from VQ-V AE
contain clear words and part-sentences (see samples linked above). We conclude that VQ-V AE was
able to model a rudimentary phoneme-level language model in a completely unsupervised fashion
from raw audio waveforms.
7Next, we attempted the speaker conversion where the latents are extracted from one speaker and then
reconstructed through the decoder using a separate speaker id. As can be heard from the samples,
the synthesised speech has the same content as the original sample, but with the voice from the
second speaker. This experiment again demonstrates that the encoded representation has factored out
speaker-speciﬁc information: the embeddings not only have the same meaning regardless of details
in the waveform, but also across different voice-characteristics.
Finally, in an attempt to better understand the content of the discrete codes we have compared the
latents one-to-one with the ground-truth phoneme-sequence (which was not used any way to train the
VQ-V AE). With a 128-dimensional discrete space that runs at 25Hz (encoder downsampling factor
of640), we mapped every of the 128 possible latent values to one of the 41 possible phoneme values1
(by taking the conditionally most likely phoneme). The accuracy of this 41-way classiﬁcation was
49:3%, while a random latent space would result in an accuracy of 7:2%(prior most likely phoneme).
It is clear that these discrete latent codes obtained in a fully unsupervised way are high-level speech
descriptors that are closely related to phonemes.
4.4 Video
For our ﬁnal experiment we have used the DeepMind Lab [ 2] environment to train a generative model
conditioned on a given action sequence. In Figure 7 we show the initial 6frames that are input to the
model followed by 10frames that are sampled from VQ-V AE with all actions set to forward (top row)
andright (bottom row). Generation of the video sequence with the VQ-V AE model is done purely in
the latent space, ztwithout the need to generate the actual images themselves. Each image in the
sequencextis then created by mapping the latents with a deterministic decoder to the pixel space
after all the latents are generated using only the prior model p(z1;:::;z T). Therefore, VQ-V AE can
be used to imagine long sequences purely in latent space without resorting to pixel space. It can be
seen that the model has learnt to successfully generate a sequence of frames conditioned on given
action without any degradation in the visual quality whilst keeping the local geometry correct. For
completeness, we trained a model without actions and obtained similar results, not shown due to
space constraints.
Figure 7: First 6 frames are provided to the model, following frames are generated conditioned on an
action. Top: repeated action "move forward", bottom: repeated action "move right".
5 Conclusion
In this work we have introduced VQ-V AE, a new family of models that combine V AEs with vector
quantisation to obtain a discrete latent representation. We have shown that VQ-V AEs are capable of
modelling very long term dependencies through their compressed discrete latent space which we have
demonstrated by generating 128128colour images, sampling action conditional video sequences
and ﬁnally using audio where even an unconditional model can generate surprisingly meaningful
chunks of speech and doing speaker conversion. All these experiments demonstrated that the discrete
latent space learnt by VQ-V AEs capture important features of the data in a completely unsupervised
manner. Moreover, VQ-V AEs achieve likelihoods that are almost as good as their continuous latent
variable counterparts on CIFAR10 data. We believe that this is the ﬁrst discrete latent variable model
that can successfully model long range sequences and fully unsupervisedly learn high-level speech
descriptors that are closely related to phonemes.
1Note that the encoder/decoder pairs could make the meaning of every discrete latent depend on previous
latents in the sequence, e.g.. bi/tri-grams (and thus achieve a higher compression) which means a more advanced
mapping to phonemes would results in higher accuracy.
8References
[1]Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu Timofte, Luca Benini, and
Luc Van Gool. Soft-to-hard vector quantization for end-to-end learned compression of images and neural
networks. arXiv preprint arXiv:1704.00648 , 2017.
[2]Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew
Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801 ,
2016.
[3]Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through
stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 , 2013.
[4]Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio.
Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349 , 2015.
[5]Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv preprint
arXiv:1509.00519 , 2015.
[6]Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. CoRR ,
abs/1606.03657, 2016.
[7]Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever,
and Pieter Abbeel. Variational lossy autoencoder. arXiv preprint arXiv:1611.02731 , 2016.
[8]Aaron Courville, James Bergstra, and Yoshua Bengio. A spike and slab restricted boltzmann machine. In
Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics , pages
233–241, 2011.
[9]Emily Denton, Sam Gross, and Rob Fergus. Semi-supervised learning with context-conditional generative
adversarial networks. arXiv preprint arXiv:1611.06430 , 2016.
[10] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv preprint
arXiv:1605.08803 , 2016.
[11] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through
video prediction. In Advances in Neural Information Processing Systems , pages 64–72, 2016.
[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing
systems , pages 2672–2680, 2014.
[13] Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards
conceptual compression. In Advances In Neural Information Processing Systems , pages 3549–3557, 2016.
[14] Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. Deep autoregressive
networks. arXiv preprint arXiv:1310.8499 , 2013.
[15] Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vázquez, and
Aaron C. Courville. Pixelvae: A latent variable model for natural images. CoRR , abs/1611.05013, 2016.
[16] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks.
science , 313(5786):504–507, 2006.
[17] Judy Hoffman, Erik Rodner, Jeff Donahue, Trevor Darrell, and Kate Saenko. Efﬁcient learning of
domain-invariant image representations. arXiv preprint arXiv:1301.3224 , 2013.
[18] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional
adversarial networks. arXiv preprint arXiv:1611.07004 , 2016.
[19] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144 , 2016.
[20] Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, and
Koray Kavukcuoglu. Video pixel networks. arXiv preprint arXiv:1610.00527 , 2016.
[21] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
[22] Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved
variational inference with inverse autoregressive ﬂow. NIPS 2016 , 2016.
[23] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 ,
2013.
[24] Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Cunningham, Alejandro Acosta,
Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-
resolution using a generative adversarial network. arXiv preprint arXiv:1609.04802 , 2016.
9[25] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of
discrete random variables. arXiv preprint arXiv:1611.00712 , 2016.
[26] Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron
Courville, and Yoshua Bengio. Samplernn: An unconditional end-to-end neural audio generation model.
arXiv preprint arXiv:1612.07837 , 2016.
[27] Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. arXiv
preprint arXiv:1402.0030 , 2014.
[28] Andriy Mnih and Danilo Jimenez Rezende. Variational inference for monte carlo objectives. CoRR ,
abs/1602.06725, 2016.
[29] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv
preprint arXiv:1601.06759 , 2016.
[30] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus
based on public domain audio books. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE
International Conference on , pages 5206–5210. IEEE, 2015.
[31] Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. arXiv
preprint arXiv:1505.05770 , 2015.
[32] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approxi-
mate inference in deep generative models. arXiv preprint arXiv:1401.4082 , 2014.
[33] Ruslan Salakhutdinov and Geoffrey Hinton. Deep boltzmann machines. In Artiﬁcial Intelligence and
Statistics , pages 448–455, 2009.
[34] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. One-shot
learning with memory-augmented neural networks. arXiv preprint arXiv:1605.06065 , 2016.
[35] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction , volume 1. MIT press
Cambridge, 1998.
[36] Lucas Theis, Wenzhe Shi, Andrew Cunningham, and Ferenc Huszár. Lossy image compression with
compressive autoencoders. arXiv preprint arXiv:1703.00395 , 2017.
[37] Aäron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal
Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio.
CoRR abs/1609.03499 , 2016.
[38] Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional
image generation with pixelcnn decoders. In Advances in Neural Information Processing Systems , pages
4790–4798, 2016.
[39] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked
denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.
Journal of Machine Learning Research , 11(Dec):3371–3408, 2010.
[40] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image
caption generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ,
pages 3156–3164, 2015.
[41] Junichi Yamagishi. English multi-speaker corpus for cstr voice cloning toolkit. URL http://homepages. inf.
ed. ac. uk/jyamagis/page3/page58/page58. html , 2012.
[42] Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick. Improved variational
autoencoders for text modeling using dilated convolutions. CoRR , abs/1702.08139, 2017.
10A Appendix
A.1 VQ-VAE dictionary updates with Exponential Moving Averages
As mentioned in Section 3.2, one can also use exponential moving averages (EMA) to update the dictionary
items instead of the loss term from Equation 3:
ksg[ze(x)] ek2
2: (4)
Letfzi;1;zi;2;:::;z i;nigbe the set of nioutputs from the encoder that are closest to dictionary item ei, so that
we can write the loss as:niX
jkzi;j eik2
2: (5)
The optimal value for eihas a closed form solution, which is simply the average of elements in the set:
ei=1
niniX
jzi;j:
This update is typically used in algorithms such as K-Means.
However, we cannot use this update directly when working with minibatches. Instead we can use exponential
moving averages as an online version of this update:
N(t)
i:=N(t 1)
i+n(t)
i(1 ) (6)
m(t)
i:=m(t 1)
i+X
jz(t)
i;j(1 ) (7)
e(t)
i:=m(t)
i
N(t)
i; (8)
witha value between 0 and 1. We found = 0:99to work well in practice.
11Provided proper attribution is provided, Google hereby grants permission to
reproduce the tables and figures in this paper solely for use in journalistic or
scholarly works.
Attention Is All You Need
Ashish Vaswani∗
Google Brain
avaswani@google.comNoam Shazeer∗
Google Brain
noam@google.comNiki Parmar∗
Google Research
nikip@google.comJakob Uszkoreit∗
Google Research
usz@google.com
Llion Jones∗
Google Research
llion@google.comAidan N. Gomez∗ †
University of Toronto
aidan@cs.toronto.eduŁukasz Kaiser∗
Google Brain
lukaszkaiser@google.com
Illia Polosukhin∗ ‡
illia.polosukhin@gmail.com
Abstract
The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-
to-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature. We show that the Transformer generalizes well to
other tasks by applying it successfully to English constituency parsing both with
large and limited training data.
∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started
the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and
has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head
attention and the parameter-free position representation and became the other person involved in nearly every
detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and
tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and
efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and
implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating
our research.
†Work performed while at Google Brain.
‡Work performed while at Google Research.
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 20231 Introduction
Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks
in particular, have been firmly established as state of the art approaches in sequence modeling and
transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous
efforts have since continued to push the boundaries of recurrent language models and encoder-decoder
architectures [38, 24, 15].
Recurrent models typically factor computation along the symbol positions of the input and output
sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden
states ht, as a function of the previous hidden state ht−1and the input for position t. This inherently
sequential nature precludes parallelization within training examples, which becomes critical at longer
sequence lengths, as memory constraints limit batching across examples. Recent work has achieved
significant improvements in computational efficiency through factorization tricks [ 21] and conditional
computation [ 32], while also improving model performance in case of the latter. The fundamental
constraint of sequential computation, however, remains.
Attention mechanisms have become an integral part of compelling sequence modeling and transduc-
tion models in various tasks, allowing modeling of dependencies without regard to their distance in
the input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms
are used in conjunction with a recurrent network.
In this work we propose the Transformer, a model architecture eschewing recurrence and instead
relying entirely on an attention mechanism to draw global dependencies between input and output.
The Transformer allows for significantly more parallelization and can reach a new state of the art in
translation quality after being trained for as little as twelve hours on eight P100 GPUs.
2 Background
The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU
[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building
block, computing hidden representations in parallel for all input and output positions. In these models,
the number of operations required to relate signals from two arbitrary input or output positions grows
in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes
it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is
reduced to a constant number of operations, albeit at the cost of reduced effective resolution due
to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as
described in section 3.2.
Self-attention, sometimes called intra-attention is an attention mechanism relating different positions
of a single sequence in order to compute a representation of the sequence. Self-attention has been
used successfully in a variety of tasks including reading comprehension, abstractive summarization,
textual entailment and learning task-independent sentence representations [4, 27, 28, 22].
End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-
aligned recurrence and have been shown to perform well on simple-language question answering and
language modeling tasks [34].
To the best of our knowledge, however, the Transformer is the first transduction model relying
entirely on self-attention to compute representations of its input and output without using sequence-
aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate
self-attention and discuss its advantages over models such as [17, 18] and [9].
3 Model Architecture
Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].
Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence
of continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output
sequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive
[10], consuming the previously generated symbols as additional input when generating the next.
2Figure 1: The Transformer - model architecture.
The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.
3.1 Encoder and Decoder Stacks
Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two
sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-
wise fully connected feed-forward network. We employ a residual connection [ 11] around each of
the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is
LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer
itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding
layers, produce outputs of dimension dmodel = 512 .
Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two
sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head
attention over the output of the encoder stack. Similar to the encoder, we employ residual connections
around each of the sub-layers, followed by layer normalization. We also modify the self-attention
sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This
masking, combined with fact that the output embeddings are offset by one position, ensures that the
predictions for position ican depend only on the known outputs at positions less than i.
3.2 Attention
An attention function can be described as mapping a query and a set of key-value pairs to an output,
where the query, keys, values, and output are all vectors. The output is computed as a weighted sum
3Scaled Dot-Product Attention
 Multi-Head Attention
Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several
attention layers running in parallel.
of the values, where the weight assigned to each value is computed by a compatibility function of the
query with the corresponding key.
3.2.1 Scaled Dot-Product Attention
We call our particular attention "Scaled Dot-Product Attention" (Figure 2). The input consists of
queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the
query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the
values.
In practice, we compute the attention function on a set of queries simultaneously, packed together
into a matrix Q. The keys and values are also packed together into matrices KandV. We compute
the matrix of outputs as:
Attention( Q, K, V ) = softmax(QKT
√dk)V (1)
The two most commonly used attention functions are additive attention [ 2], and dot-product (multi-
plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor
of1√dk. Additive attention computes the compatibility function using a feed-forward network with
a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is
much faster and more space-efficient in practice, since it can be implemented using highly optimized
matrix multiplication code.
While for small values of dkthe two mechanisms perform similarly, additive attention outperforms
dot product attention without scaling for larger values of dk[3]. We suspect that for large values of
dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has
extremely small gradients4. To counteract this effect, we scale the dot products by1√dk.
3.2.2 Multi-Head Attention
Instead of performing a single attention function with dmodel-dimensional keys, values and queries,
we found it beneficial to linearly project the queries, keys and values htimes with different, learned
linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of
queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional
4To illustrate why the dot products get large, assume that the components of qandkare independent random
variables with mean 0and variance 1. Then their dot product, q·k=Pdk
i=1qiki, has mean 0and variance dk.
4output values. These are concatenated and once again projected, resulting in the final values, as
depicted in Figure 2.
Multi-head attention allows the model to jointly attend to information from different representation
subspaces at different positions. With a single attention head, averaging inhibits this.
MultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO
where head i= Attention( QWQ
i, KWK
i, V WV
i)
Where the projections are parameter matrices WQ
i∈Rdmodel×dk,WK
i∈Rdmodel×dk,WV
i∈Rdmodel×dv
andWO∈Rhdv×dmodel.
In this work we employ h= 8 parallel attention layers, or heads. For each of these we use
dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost
is similar to that of single-head attention with full dimensionality.
3.2.3 Applications of Attention in our Model
The Transformer uses multi-head attention in three different ways:
•In "encoder-decoder attention" layers, the queries come from the previous decoder layer,
and the memory keys and values come from the output of the encoder. This allows every
position in the decoder to attend over all positions in the input sequence. This mimics the
typical encoder-decoder attention mechanisms in sequence-to-sequence models such as
[38, 2, 9].
•The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.
•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property. We implement this
inside of scaled dot-product attention by masking out (setting to −∞) all values in the input
of the softmax which correspond to illegal connections. See Figure 2.
3.3 Position-wise Feed-Forward Networks
In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully
connected feed-forward network, which is applied to each position separately and identically. This
consists of two linear transformations with a ReLU activation in between.
FFN( x) = max(0 , xW 1+b1)W2+b2 (2)
While the linear transformations are the same across different positions, they use different parameters
from layer to layer. Another way of describing this is as two convolutions with kernel size 1.
The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality
dff= 2048 .
3.4 Embeddings and Softmax
Similarly to other sequence transduction models, we use learned embeddings to convert the input
tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-
mation and softmax function to convert the decoder output to predicted next-token probabilities. In
our model, we share the same weight matrix between the two embedding layers and the pre-softmax
linear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.
5Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations
for different layer types. nis the sequence length, dis the representation dimension, kis the kernel
size of convolutions and rthe size of the neighborhood in restricted self-attention.
Layer Type Complexity per Layer Sequential Maximum Path Length
Operations
Self-Attention O(n2·d) O(1) O(1)
Recurrent O(n·d2) O(n) O(n)
Convolutional O(k·n·d2) O(1) O(logk(n))
Self-Attention (restricted) O(r·n·d) O(1) O(n/r)
3.5 Positional Encoding
Since our model contains no recurrence and no convolution, in order for the model to make use of the
order of the sequence, we must inject some information about the relative or absolute position of the
tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the
bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel
as the embeddings, so that the two can be summed. There are many choices of positional encodings,
learned and fixed [9].
In this work, we use sine and cosine functions of different frequencies:
PE(pos,2i)=sin(pos/100002i/d model)
PE(pos,2i+1)=cos(pos/100002i/d model)
where posis the position and iis the dimension. That is, each dimension of the positional encoding
corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We
chose this function because we hypothesized it would allow the model to easily learn to attend by
relative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of
PEpos.
We also experimented with using learned positional embeddings [ 9] instead, and found that the two
versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version
because it may allow the model to extrapolate to sequence lengths longer than the ones encountered
during training.
4 Why Self-Attention
In this section we compare various aspects of self-attention layers to the recurrent and convolu-
tional layers commonly used for mapping one variable-length sequence of symbol representations
(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden
layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we
consider three desiderata.
One is the total computational complexity per layer. Another is the amount of computation that can
be parallelized, as measured by the minimum number of sequential operations required.
The third is the path length between long-range dependencies in the network. Learning long-range
dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the
ability to learn such dependencies is the length of the paths forward and backward signals have to
traverse in the network. The shorter these paths between any combination of positions in the input
and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare
the maximum path length between any two input and output positions in networks composed of the
different layer types.
As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially
executed operations, whereas a recurrent layer requires O(n)sequential operations. In terms of
computational complexity, self-attention layers are faster than recurrent layers when the sequence
6length nis smaller than the representation dimensionality d, which is most often the case with
sentence representations used by state-of-the-art models in machine translations, such as word-piece
[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving
very long sequences, self-attention could be restricted to considering only a neighborhood of size rin
the input sequence centered around the respective output position. This would increase the maximum
path length to O(n/r). We plan to investigate this approach further in future work.
A single convolutional layer with kernel width k < n does not connect all pairs of input and output
positions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,
orO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths
between any two positions in the network. Convolutional layers are generally more expensive than
recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity
considerably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable
convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,
the approach we take in our model.
As side benefit, self-attention could yield more interpretable models. We inspect attention distributions
from our models and present and discuss examples in the appendix. Not only do individual attention
heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic
and semantic structure of the sentences.
5 Training
This section describes the training regime for our models.
5.1 Training Data and Batching
We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million
sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-
target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT
2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece
vocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training
batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000
target tokens.
5.2 Hardware and Schedule
We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using
the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We
trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the
bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps
(3.5 days).
5.3 Optimizer
We used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning
rate over the course of training, according to the formula:
lrate =d−0.5
model·min(step_num−0.5, step _num·warmup _steps−1.5) (3)
This corresponds to increasing the learning rate linearly for the first warmup _steps training steps,
and decreasing it thereafter proportionally to the inverse square root of the step number. We used
warmup _steps = 4000 .
5.4 Regularization
We employ three types of regularization during training:
7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the
English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
ModelBLEU Training Cost (FLOPs)
EN-DE EN-FR EN-DE EN-FR
ByteNet [18] 23.75
Deep-Att + PosUnk [39] 39.2 1.0·1020
GNMT + RL [38] 24.6 39.92 2.3·10191.4·1020
ConvS2S [9] 25.16 40.46 9.6·10181.5·1020
MoE [32] 26.03 40.56 2.0·10191.2·1020
Deep-Att + PosUnk Ensemble [39] 40.4 8.0·1020
GNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021
ConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021
Transformer (base model) 27.3 38.1 3.3·1018
Transformer (big) 28.4 41.8 2.3·1019
Residual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the
sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the
positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of
Pdrop= 0.1.
Label Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This
hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.
6 Results
6.1 Machine Translation
On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)
in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0
BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is
listed in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model
surpasses all previously published models and ensembles, at a fraction of the training cost of any of
the competitive models.
On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,
outperforming all of the previously published single models, at less than 1/4the training cost of the
previous state-of-the-art model. The Transformer (big) model trained for English-to-French used
dropout rate Pdrop= 0.1, instead of 0.3.
For the base models, we used a single model obtained by averaging the last 5 checkpoints, which
were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We
used beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters
were chosen after experimentation on the development set. We set the maximum output length during
inference to input length + 50, but terminate early when possible [38].
Table 2 summarizes our results and compares our translation quality and training costs to other model
architectures from the literature. We estimate the number of floating point operations used to train a
model by multiplying the training time, the number of GPUs used, and an estimate of the sustained
single-precision floating-point capacity of each GPU5.
6.2 Model Variations
To evaluate the importance of different components of the Transformer, we varied our base model
in different ways, measuring the change in performance on English-to-German translation on the
5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.
8Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base
model. All metrics are on the English-to-German translation development set, newstest2013. Listed
perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to
per-word perplexities.
N d model dff h d k dvPdrop ϵlstrain PPL BLEU params
steps (dev) (dev) ×106
base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65
(A)1 512 512 5.29 24.9
4 128 128 5.00 25.5
16 32 32 4.91 25.8
32 16 16 5.01 25.4
(B)16 5.16 25.1 58
32 5.01 25.4 60
(C)2 6.11 23.7 36
4 5.19 25.3 50
8 4.88 25.5 80
256 32 32 5.75 24.5 28
1024 128 128 4.66 26.0 168
1024 5.12 25.4 53
4096 4.75 26.2 90
(D)0.0 5.77 24.6
0.2 4.95 25.5
0.0 4.67 25.3
0.2 5.47 25.7
(E) positional embedding instead of sinusoids 4.92 25.7
big 6 1024 4096 16 0.3 300K 4.33 26.4 213
development set, newstest2013. We used beam search as described in the previous section, but no
checkpoint averaging. We present these results in Table 3.
In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,
keeping the amount of computation constant, as described in Section 3.2.2. While single-head
attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.
In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This
suggests that determining compatibility is not easy and that a more sophisticated compatibility
function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,
bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our
sinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical
results to the base model.
6.3 English Constituency Parsing
To evaluate if the Transformer can generalize to other tasks we performed experiments on English
constituency parsing. This task presents specific challenges: the output is subject to strong structural
constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence
models have not been able to attain state-of-the-art results in small-data regimes [37].
We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the
Penn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,
using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences
[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens
for the semi-supervised setting.
We performed only a small number of experiments to select the dropout, both attention and residual
(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters
remained unchanged from the English-to-German base translation model. During inference, we
9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23
of WSJ)
Parser Training WSJ 23 F1
Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3
Petrov et al. (2006) [29] WSJ only, discriminative 90.4
Zhu et al. (2013) [40] WSJ only, discriminative 90.4
Dyer et al. (2016) [8] WSJ only, discriminative 91.7
Transformer (4 layers) WSJ only, discriminative 91.3
Zhu et al. (2013) [40] semi-supervised 91.3
Huang & Harper (2009) [14] semi-supervised 91.3
McClosky et al. (2006) [26] semi-supervised 92.1
Vinyals & Kaiser el al. (2014) [37] semi-supervised 92.1
Transformer (4 layers) semi-supervised 92.7
Luong et al. (2015) [23] multi-task 93.0
Dyer et al. (2016) [8] generative 93.3
increased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3
for both WSJ only and the semi-supervised setting.
Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-
prisingly well, yielding better results than all previously reported models with the exception of the
Recurrent Neural Network Grammar [8].
In contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-
Parser [29] even when training only on the WSJ training set of 40K sentences.
7 Conclusion
In this work, we presented the Transformer, the first sequence transduction model based entirely on
attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with
multi-headed self-attention.
For translation tasks, the Transformer can be trained significantly faster than architectures based
on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014
English-to-French translation tasks, we achieve a new state of the art. In the former task our best
model outperforms even all previously reported ensembles.
We are excited about the future of attention-based models and plan to apply them to other tasks. We
plan to extend the Transformer to problems involving input and output modalities other than text and
to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs
such as images, audio and video. Making generation less sequential is another research goals of ours.
The code we used to train and evaluate our models is available at https://github.com/
tensorflow/tensor2tensor .
Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful
comments, corrections and inspiration.
References
[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450 , 2016.
[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. CoRR , abs/1409.0473, 2014.
[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural
machine translation architectures. CoRR , abs/1703.03906, 2017.
[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine
reading. arXiv preprint arXiv:1601.06733 , 2016.
10[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,
and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical
machine translation. CoRR , abs/1406.1078, 2014.
[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv
preprint arXiv:1610.02357 , 2016.
[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation
of gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.
[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural
network grammars. In Proc. of NAACL , 2016.
[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-
tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.
[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint
arXiv:1308.0850 , 2013.
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-
age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 770–778, 2016.
[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in
recurrent nets: the difficulty of learning long-term dependencies, 2001.
[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,
9(8):1735–1780, 1997.
[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations
across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural
Language Processing , pages 832–841. ACL, August 2009.
[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring
the limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.
[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural
Information Processing Systems, (NIPS) , 2016.
[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference
on Learning Representations (ICLR) , 2016.
[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-
ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,
2017.
[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.
InInternational Conference on Learning Representations , 2017.
[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.
[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint
arXiv:1703.10722 , 2017.
[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen
Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint
arXiv:1703.03130 , 2017.
[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task
sequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.
[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-
based neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.
11[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.
[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In
Proceedings of the Human Language Technology Conference of the NAACL, Main Conference ,
pages 152–159. ACL, June 2006.
[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention
model. In Empirical Methods in Natural Language Processing , 2016.
[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive
summarization. arXiv preprint arXiv:1705.04304 , 2017.
[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,
and interpretable tree annotation. In Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July
2006.
[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv
preprint arXiv:1608.05859 , 2016.
[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words
with subword units. arXiv preprint arXiv:1508.07909 , 2015.
[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts
layer. arXiv preprint arXiv:1701.06538 , 2017.
[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine
Learning Research , 15(1):1929–1958, 2014.
[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory
networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,
Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,
Inc., 2015.
[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural
networks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.
[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.
Rethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.
[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In
Advances in Neural Information Processing Systems , 2015.
[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine
translation system: Bridging the gap between human and machine translation. arXiv preprint
arXiv:1609.08144 , 2016.
[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with
fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.
[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate
shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume
1: Long Papers) , pages 434–443. ACL, August 2013.
12Attention Visualizations
Input-Input Layer5
It
is
in
this
spirit
that
a
majority
of
American
governments
have
passed
new
laws
since
2009
making
the
registration
or
voting
process
more
difficult
.
<EOS>
<pad>
<pad>
<pad>
<pad>
<pad>
<pad>
It
is
in
this
spirit
that
a
majority
of
American
governments
have
passed
new
laws
since
2009
making
the
registration
or
voting
process
more
difficult
.
<EOS>
<pad>
<pad>
<pad>
<pad>
<pad>
<pad>
Figure 3: An example of the attention mechanism following long-distance dependencies in the
encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of
the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for
the word ‘making’. Different colors represent different heads. Best viewed in color.
13Input-Input Layer5
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
Input-Input Layer5
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:
Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5
and 6. Note that the attentions are very sharp for this word.
14Input-Input Layer5
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
Input-Input Layer5
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the
sentence. We give two such examples above, from two different heads from the encoder self-attention
at layer 5 of 6. The heads clearly learned to perform different tasks.
15MANNINGEli StevensLuca AntigaEssential Excerpts
Deep Learning with PyTorchEssential ExcerptsEli Stevens and Luca Antiga Manning Author Picks
    Copyright 2019 Manning PublicationsTo pre-order or learn more about this book go to www.manning.com
'PSPOMJOFJOGPSNBUJPOBOEPSEFSJOHPGUIJTBOEPUIFS.BOOJOHCPPLTQMFBTFWJTJUXXXNBOOJOHDPN5IFQVCMJTIFSPGGFSTEJTDPVOUTPOUIFTFCPPLTXIFOPSEFSFEJORVBOUJUZ'PSNPSFJOGPSNBUJPOQMFBTFDPOUBDU4QFDJBM4BMFT%FQBSUNFOU.BOOJOH1VCMJDBUJPOT$P#BMEXJO3PBE10#PY4IFMUFS*TMBOE/:&NBJM&SJO5XPIFZDPSQTBMFT!NBOOJOHDPN¥CZ.BOOJOH1VCMJDBUJPOT$P"MMSJHIUTSFTFSWFE/PQBSUPGUIJTQVCMJDBUJPONBZCFSFQSPEVDFETUPSFEJOBSFUSJFWBMTZTUFNPSUSBOTNJUUFEJOBOZGPSNPSCZNFBOTFMFDUSPOJDNFDIBOJDBMQIPUPDPQZJOHPSPUIFSXJTFXJUIPVUQSJPSXSJUUFOQFSNJTTJPOPGUIFQVCMJTIFS.BOZPGUIFEFTJHOBUJPOTVTFECZNBOVGBDUVSFSTBOETFMMFSTUPEJTUJOHVJTIUIFJSQSPEVDUTBSFDMBJNFEBTUSBEFNBSLT8IFSFUIPTFEFTJHOBUJPOTBQQFBSJOUIFCPPLBOE.BOOJOH1VCMJDBUJPOTXBTBXBSFPGBUSBEFNBSLDMBJNUIFEFTJHOBUJPOTIBWFCFFOQSJOUFEJOJOJUJBMDBQTPSBMMDBQT3FDPHOJ[JOHUIFJNQPSUBODFPGQSFTFSWJOHXIBUIBTCFFOXSJUUFOJUJT.BOOJOHTQPMJDZUPIBWFUIFCPPLTXFQVCMJTIQSJOUFEPOBDJEGSFFQBQFSBOEXFFYFSUPVSCFTUFGGPSUTUPUIBUFOE3FDPHOJ[JOHBMTPPVSSFTQPOTJCJMJUZUPDPOTFSWFUIFSFTPVSDFTPGPVSQMBOFU.BOOJOHCPPLTare printed on paper that is at least 15 percent recycled and processed without the use of elemental chlorine.Manning Publications Co. 20 Baldwin Road Technical PO Box 761Shelter Island, NY 11964Cover designer: Leslie HaimesISBN: 9781617297120Printed in the United States of America1 2 3 4 5 6 7 8 9 10 - EBM - 24 23 22 21 20 19iiicontents preface v1I n t r o d u c i n g  d e e p  l e a r n i n g  a n d  t h e  P y T o r c h  l i b r a r y  11.1 What is PyTorch? 21.2 What is this book? 21.3 Why PyTorch? 31.4 PyTorch has the batteries included 102I t  s t a r t s  w i t h  a  t e n s o r  1 52.1 Tensor fundamentals 182.2 Tensors and storages 222.3 Size, storage offset, and strides 242.4 Numeric types 302.5 Indexing tensors 312.6 NumPy interoperability 312.7 Serializing tensors 322.8 Moving tensors to the GPU 342.9 The tensor API 353R e a l - w o r l d  d a t a  r e p r e s e n t a t i o n  w i t h  t e n s o r s  3 93.1 Tabular data 403.2 Time series 493.3 Text 543.4 Images 603.5 Volumetric data 63iv CONTENTS4T h e  m e c h a n i c s  o f  l e a r n i n g  6 74.1 Learning is parameter estimation 704.2 PyTorch’s autograd: Backpropagate all things 835U s i n g  a  n e u r a l  n e t w o r k  t o  f i t  y o u r  d a t a  1 0 15.1 Artificial neurons 1025.2 The PyTorch nn module 1105.3 Subclassing nn.Module 120 index 127viabout the authorsEli Stevens has worked in Silicon Valley for the past 15 years as a software engineer, and the past 7 years as Chief Technical Officer of a startup making medical device software. Luca Antiga is co-founder and CEO of an AI engineering company located in Bergamo, Italy, and a regular contributor to PyTorch.Save 50% on the full book – eBook, pBook, and MEAP. Enter ebstevens50 in the Promotional Code box when you checkout. Only at manning.com.
Deep Learning with PyTorchby Eli Stevens and Luca Antiga  ISBN 9781617295263400 pages (estimated)$49.99Publication in Winter, 2019 (estimated)1Introducing deep learning  and the PyTorch library
We’re living through exciting times. The landscape of what computers can do is changing by the week. Tasks that only a few years ago were thought to require higher cognition are getting solved by machines at near-superhuman levels of per-formance. Tasks such as describing a photographic image with a sentence in idiom-atic English, playing complex strategy g a me, and diagnosing a tumor from a radiological scan are all approachable now by a computer. Even more impressively, computers acquire the ability to solve such tasks through examples, rather than human-encoded of handcrafted rules. It would be disingenuous to assert that machines are learning to “think” in any human sense of the word. Rather, a general class of algorithms is able to approxi-This chapter coversWhat this book will teach youPyTorch’s role as a library for building deep learning projectsThe strengths and weaknesses of PyTorchThe hardware you’ll need to follow along with the examples2CHAPTER 1 Introducing deep learning and the PyTorch librarymate complicated, nonlinear processes extremely effectively. In a way, we’re learning that intelligence, as we subjectively perceive it, is a notion that’s often conflated with self-awareness, and self-awareness definitely isn’t required to solve or carry out these kinds of problems. In the end, the question of computer intelligence may not even be important. As pioneering computer scientist Edsger W. Dijkstra said in “The Threats to Computing Science,”Alan M. Turing thought about . . . the question of whether Machines Can Think, a question . . . about as relevant as the question of whether Submarines Can Swim.That general class of algorithms we’re talking about falls under the category of deep learning, which deals with training mathematical entities named deep neural networks on the basis of examples. Deep learning leverages large amounts of data to approximate complex functions whose inputs and outputs are far apart, such as an image (input) and a line of text describing the input (output); a written script (input) and a natural-sounding voice reciting the script (output); or, even more simply, associating an image of a golden retriever with a flag that indicates that a golden retriever is present. This capability allows developers to create programs with functionality that until recently was the exclusive domain of human beings.1.1 What is PyTorch?PyTorch is a library for Python programs that facilitates building deep learning proj-ects. It emphasizes flexibility and allows deep learning models to be expressed in idi-omatic Python. This approachability and ease of use found early adopters in the research community, and in the years since the library’s release, it has grown into one of the most prominent deep learning tools for a broad range of applications. PyTorch provides a core data structure, the Tensor, a multidimensional array that has many similarities with NumPy arrays. From that foundation, a laundry list of fea-tures was built to make it easy to get a project up and running, or to design and train investigation into a new neural network architecture. Tensors accelerate mathematical operations (assuming that the appropriate combination of hardware and software is present), and PyTorch has packages for distributed training, worker processes for effi-cient data loading, and an extensive library of common deep learning functions. As Python is for programming, PyTorch is both an excellent introduction to deep learning and a tool usable in professional contexts for real-world, high-level work. W e  b e l i e v e  t h a t  P y T o r c h  s h o u l d  b e  t h e  f i r s t  d e e p  l e a r n i n g  l i b r a r y  y o u  l e a r n . Whether it should be the last is a decision that we’ll leave to you.1.2 What is this book?This book is intended to be a starting point for software engineers, data scientists, and motivated students who are fluent in Python and want to become comfortable using PyTorch to build deep learning projects. To that end, we take a hands-on approach; we encourage you to keep your computer at the ready so that you can play with the examples and take them a step further.3Why PyTorch? T h o u g h  w e  s t r e s s  t h e  p r a c t i c a l  a p p l i c a tions, we also believe that providing an accessible introduction to foundational deep learning tools like PyTorch is more than a way to facilitate the acquisition of new technical skills. It is also a step toward equip-ping a new generation of scientists, engineers, and practitioners from a wide range of disciplines with a working knowledge of the tools that will be the backbone of many software projects during the decades to come. To get the most out of this book, you need two things:Some experience programming in Python—We’re not going to pull any punches on that one: you’ll need to be up on Python data types, classes, floating-point num-bers, and the like.Willingness to dive in and get your hands dirty—It’ll be much easier for you to learn if you follow along with us.Deep learning is a huge space. In this book, we’ll be covering a tiny part of that space—specifically, using PyTorch for smaller-scope projects. Most of the motivating examples use image processing of 2D and 3D data sets. We focus on practical PyTorch, with the aim of covering enough ground to allow you to solve realistic problems with deep learning or explore new models as they pop up in research literature. A great resource for the latest publications related to deep learning research is the ArXiV public preprint repository, hosted at https://arxiv.org.11.3 Why PyTorch?As we’ve said, deep learning allows you to carry out a wide range of complicated tasks—such as performing machine translation, playing strategy games, and identify-ing objects in cluttered scenes—by exposing your model to illustrative examples. To do so in practice, you need tools that are flexible so that they can be adapted to your specific problem and efficient, to allow training to occur over large amounts of data in reasonable times. You also need the trained network to perform correctly in the pres-ence of uncertainty in the inputs. In this section, we take a look at some of the reasons why we decided to use PyTorch. PyTorch is easy to recommend because of its simplicity. Many researchers and prac-titioners find it easy to learn, use, extend, and debug. It’s Pythonic, and although (like any complicated domain) it has caveats and best practices, using the library generally feels familiar to developers who have used Python previously. For users who are familiar with NumPy arrays, the PyTorch Tensor class will be immediately familiar. PyTorch feels like NumPy, but with GPU acceleration and auto-matic computation of gradients, which makes it suitable for calculating backward pass data automatically starting from a forward expression. The Tensor API is such that the additional features of the class relevant to deep learning are unobtrusive; the user is mostly free to pretend that those features don’t exist until need for them arises.1We also recommed http://www.arxiv-sanity.com to help organize research papers of interest.4CHAPTER 1 Introducing deep learning and the PyTorch library A design driver for PyTorch is expressivity, allowing a developer to implement com-plicated models without undue complexity being imposed by the library. (The library isn’t a framework!) PyTorch arguably offers one of the most seamless translations of ideas into Python code in the deep learning landscape. For this reason, PyTorch has seen widespread adoption in research, as witnessed by the high citation counts in international conferences.2 PyTorch also has a compelling story for the transition from research and develop-ment to production. Although it initially focused on research workflows, PyTorch has been equipped with a high-performance C++ runtime that users can leverage to deploy models for inference without relying on Python, keeping most of the flexibility of PyTorch without paying the overhead of the Python runtime. Claims of ease of use and high performance are trivial to make, of course. We hope that by the time you’re in the thick of this book, you’ll agree that our claims here are well founded.1.3.1 The deep learning revolutionIn this section, we take a step back and provide some context for where PyTorch fits into the current and historical landscape of deep learning tools. U n t i l  t h e  l a t e  2 0 0 0 s ,  t h e  b r o a d e r  c l a s s  of systems that fell into the category “machine learning” relied heavily on feature engineering. Features are transformations of input data resulting in numerical features that facilitate a downstream algorithm, such as a classifier, to produce correct outcomes on new data. Feature engineering aims to take the original data and come up with representations of the same data that can be fed to an algorithm to solve a problem. To tell ones from zeros in images of handwritten digits, for example, you’d come up with a set of filters to estimate the direction of edges over the image and then train a classifier to predict the correct digit, given a dis-tribution of edge directions. Another useful feature could be the number of enclosed holes in a zero, an eight, or particularly loopy twos. Deep learning, on the other hand, deals with finding such representations auto-matically, from raw data, to perform a task successfully. In the ones-versus-zeros exam-ple, filters would be refined during training by iteratively looking at pairs of examples and target labels. This isn’t to say that feature engineering has no place in deep learn-ing; developers often need to inject some form of knowledge into a learning system. The ability of a neural network to ingest data and extract useful representations on the basis of examples, however, is what makes deep learning so powerful. The focus of deep learning practitioners is not so much on handcrafting those representations but on operating on a mathematical entity so that it discovers representations from the training data autonomously. Often, these automatically created features are better than those that are handcrafted! As in many disruptive technologies, this fact has led to a change in perspective.2At ICLR 2019, PyTorch appeared as a citation in 252 papers, up from 87 the previous year and at the same level as TensorFlow, which appeared in 266 papers.5Why PyTorch? On the left side of figure 1.1, a practitioner is busy defining engineering features and feeding them to a learning algorithm. The results of the task will be as good as the features he engineers. On the right side of the figure, with deep learning, the raw data is fed to an algorithm that extracts hierarchical features automatically, based on opti-mizing the performance of the algorithm on the task. The results will be as good as the practitioner’s ability to drive the algorithm toward its goal. 
Figure 1.1 The change in perspective brought by deep learning
1.3.2 Immediate versus deferred executionOne key differentiator for deep learning libraries is immediate versus deferred execu-tion. Much of PyTorch’s ease of use is due to how it implements immediate execution, so we briefly cover that implementation here. Consider the expression (a**2 + b**2) ** 0.5 that implements the Pythagorean theorem. If you want to execute this expression, you need to have an a and b handy, like so:>>> a = 3>>> b = 4>>> c = (a**2 + b**2) ** 0.5>>> c5.0Immediate execution like this consumes inputs and produces an output value(c here). PyTorch, like Python in general, defaults to immediate execution (referred to as eager mode i n  t h e  P y T o r c h  d o c u m e n t a t i o n ). Immediate execution is useful 6CHAPTER 1 Introducing deep learning and the PyTorch librarybecause if problems arise in executing the expression, the Python interpreter, debug-ger, and similar tools have direct access to the Python objects involved. Exceptions can be raised directly at the point where the issue occurred. Alternatively, you could define the Pythagorean expression even before knowing what the inputs are and use that definition to produce the output when the inputs are available. That callable function that you define can be used later, repeatedly, with var-ied inputs:>>> p = lambda a, b: (a**2 + b**2) ** 0.5>>> p(1, 2)2.23606797749979>>> p(3, 4)5.0In the second case, you defined a series of operations to perform, resulting in a out-put function (p in this case). You didn’t execute anything until later, when you passed in the inputs—an example of deferred execution. Deferred execution means that most exceptions are be raised when the function is called, not when it’s defined. For normal Python (as you see here), that’s fine, because the interpreter and debuggers have full access to the Python state at the time when the error occurred. Things get tricky when specialized classes that have heavy operator overloading are used, allowing what looks like immediate execution to be deferred under the hood. These classes can look like the following:>>> a = InputParameterPlaceholder()>>> b = InputParameterPlaceholder()>>> c = (a**2 + b**2) ** 0.5>>> callable(c)True>>> c(3, 4)5.0Often in libraries that use this form of function definition, the operations of squaring a and b, adding, and taking the square root aren’t recorded as high-level Python byte code. Instead, the point usually is to compile the expression into a static computation graph (a graph of basic operations) that has some advantage over pure Python (such as compiling the math directly to machine code for performance reasons). The  f a ct  t h at  t h e c o mp ut ati o n gra ph is  bu ilt  i n  o n e p lac e a n d us ed  in ano t he r makes debugging more difficult, because exceptions often lack specificity about what went wrong and Python debugging tools don’t have any visibility into the intermediate states of the data. Also, static graphs usually don’t mix well with standard Python flow control: they’re de-facto domain-specific languages implemented on top of a host lan-guage (Python in this case). N e x t ,  w e  t a k e  a  m o r e  c o n c r e t e  l o o k  a t  t h e  d i f f e r e n c e s  b e t w e e n  i m m e d i a t e  a n d deferred execution, specifically regarding issues that are relevant to neural networks. We won’t be teaching these concepts in any depth here, instead giving you a high-level introduction to the terminology and the relationships among these concepts. Under-standing those concepts and relationships lays the groundwork for understand how 7Why PyTorch?libraries like PyTorch that use immediate execution differ from deferred-execution frameworks, even though the underlying math is the same for both types. T h e  f u n d a m e n t a l  b u i l d i n g  b l o ck of a neural network is a  neuron. Neurons are strung together in large numbers to form the network. You see a typical mathematical expression for a single neuron in the first row of figure 1.2: o = tanh(w * x + b). As we explain the execution modes in the following figures, keep these facts in mind:x is the input to the single-neuron computation.w and b are the parameters or weights of the neuron and can be changed as needed.To update the parameters (to produce output that more closely matches what we desire), we assign error to each of the weights via backpropagation and then tweak the weights accordingly.Backpropagation requires computing the gradient of the output with respect to the weights (among other things).We use automatic differentiation to compute the gradient automatically, saving us the trouble of writing the calculations by hand.In figure 1.2, the neuron gets compiled into a symbolic graph in which each node rep-resents individual operations (second row),
Figure 1.2 Static graph for a simple computation corresponding to a single neuron
 u s i n g  p l a c e h o l d e r s  f o r  i n p u t s  a n d  o u t-puts. Then the graph is evaluated numerically (third row) when concrete numbers are plugged into the placeholders (in this case, the numbers are the values stored in w, 8CHAPTER 1 Introducing deep learning and the PyTorch libraryx, and b). The gradient of the output with respect to the weights is constructed sym-bolically by automatic differentiation, which traverses the graph backward and multi-plies the gradients at individual nodes (fourth row). The corresponding mathematical expression is shown in the fifth row.  One of the major competing deep learning frameworks is TensorFlow, which has a graph mode that uses a similar kind of deferred execution. Graph mode is the default mode of operation in TensorFlow 1.0. By c o n t r a s t ,  P y T o r c h  s p o r t s  a  d e f i n e - b y - r u n dynamic graph engine in which the computation graph is built node by node as the code is eagerly evaluated. T h e  t o p  h a l f  o f  f i g u r e  1 . 3  s h o w s  t h e  same calculation running under a dynamic graph engine. 
Figure 1.3 Dynamic graph for a simple computation corresponding to a single neuron
The computation is broken into individual expressions, which are greed-ily evaluated as they’re encountered. The program has no advance notion of the inter-connection between computations. The bottom half of the figure shows the behind-the-scenes construction of a dynamic computation graph for the same expression. The expression is still broken into individual operations, but here those operations are eagerly evaluated, and the graph is built incrementally. Automatic differentiation is achieved by traversing the resulting graph backward, similar to s t a t i c  c o m p u t a t i o n graphs. Note that this does not mean dynamic graph libraries are inherently more capa-9Why PyTorch?ble than static graph libraries, just that it’s often easier to accomplish looping or condi-tional behavior with dynamic graphs.  Dynamic graphs can change during successive forward passes. Different nodes can be invoked according to conditions on the outputs of the preceding nodes, for exam-ple, without a need for such conditions to be represented in the graph itself—a dis-tinct advantage over static graph approaches. The major frameworks are converging toward supporting both modes of opera-tion. PyTorch 1.0 gained the ability to record the execution of a model in a static com-putation graph or define it through a precompiled scripting language, with the goal of improved performance and ease of putting the model into production. TensorFlow has also gained “eager mode,” a new define-by-run API, increasing the library’s flexi-bility as we have discussed.1.3.3 The deep learning competitive landscapeAlthough all analogies are flawed, it seems that the release of PyTorch 0.1 in January 2017 marked the transition from a Cambrian Explosion–like proliferation of deep learning libraries, wrappers, and data exchange formats to an era of consolidation and unification.NOTE T h e  d e e p  l e a r n i n g  l a n d s c a p e  h a s  b e e n moving so quickly lately that by the time you read this book, some aspects may be out of date. If you’re unfa-miliar with some of the libraries mentioned here, that’s fine.At the time of PyTorch’s first beta releaseTheano and TensorFlow were the premiere low-level deferred-execution libraries.Lasagne and Keras were high-level wrappers around Theano, with Keras wrap-ping TensorFlow and CNTK as well.Caffe, Chainer, Dynet, Torch (the Lua-based precursor to PyTorch), mxnet, CNTK, DL4J, and others filled various niches in the ecosystem.In the roughly two years that followed, the landscape changed dramatically. The com-munity has largely consolidated behind PyTorch or TensorFlow, with the adoption of other libraries dwindling or filling specific niches:Theano, one of the first deep learning frameworks, has ceased active develop-ment.TensorFlow–C o n s u m e d  K e r a s ,  p r o m o t i n g  it to a first-class API–P r o v i d e d  a n  i m m e d i a t e  e x e c u t i o n  e a g e r  m o d e– Announced that TF 2.0 will enable eager mode by defaultPyTorch–C o n s u m e d  C a f f e 2  f o r  i t s  b a c k e n d–R e p l a c e d  m o s t  o f  t h e  l o w -level code reused from the Lua-based Torch project10CHAPTER 1 Introducing deep learning and the PyTorch library–A d d e d  s u p p o r t  f o r  O N N X ,  a  v e n d o r - n e utral model description and exchange format–A d d e d  a  d e l a y e d  e x e c u t i o n  g r a p h  mode runtime called TorchScript–R e l e a s e d  v e r s i o n  1 . 0TensorFlow has a robust pipeline to production, an extensive industrywide community, and massive mindshare. PyTorch has made huge inroads with the research and teaching community, thanks to its ease of use, and has picked up momentum as researchers and graduates train students and move to industry. Interestingly, with the advent of Torch-Script and eager mode, both libraries have seen their feature sets start to converge.1.4 PyTorch has the batteries includedWe’ve already hinted at a few components of PyTorch. Now we’ll take some time to formalize a high-level map of the main components that form PyTorch. First, PyTorch has the Py from Python, but there’s a lot of non-Python code in it. For performance reasons, most of PyTorch is written in C++ and CUDA3, a C++-like lan-guage from NVIDIA that can be compiled to run with massive parallelism on NVIDIA GPUs. There are ways to run PyTorch directly from C. One of the main motivations for this capability is providing a reliable strategy for deploying models in production. Most of the time, however, you’ll interact with PyTorch from Python, building models, training them, and using the trained models to solve problems. Depending on a given use case’s requirements for performance and scale, a pure-Python solution can be suf-ficient to put models into production. It can be perfectly viable to use a Flask web server to wrap a PyTorch model using the Python API, for example. Indeed, the Python API is where PyTorch shines in term of usability and integra-tion with the wider Python ecosystem. Next, we take a peek at the mental model of PyTorch. At its core, PyTorch is a library that provides multidimensional arrays, called tensors in PyTorch parlance, and an extensive library of operations on them is provided by the torch module. Both tensors and related operations can run on the CPU or GPU. Run-ning on the GPU results in massive speedups compared with CPU (especially if you’re willing to pay for a top-end GPU), and with PyTorch doing so, it doesn’t require more than an additional function call or two. The second core thing that PyTorch provides allows tensors to keep track of the operations performed on them and to compute derivatives of an output with respect to any of its inputs analytically via backpropagation. This capability is provided natively by tensors and further refined in torch.autograd. We could argue that by having tensors and the autograd-enabled tensor standard library, PyTorch could be used for more than neural networks, and we’d be correct: PyTorch can be used for physics, rendering, optimization, simulation, modeling, and so on. We’re likely to see PyTorch being used in creative ways across the spectrum of scientific applications.3https://www.geforce.com/hardware/technology/cuda11PyTorch has the batteries included But PyTorch is first and foremost a deep learning library, and as such, it provides all the building blocks needed to build and train neural networks. Figure 1.4 shows a stan-dard setup that loads data, trains a model, and then deploys that model to production. The core PyTorch modules for building neural networks are located in torch.nn, which provides common neural network layers and other architectural components. Fully connected layers, convolutional layers, activation functions, and loss functions can all be found here. These components can be used to build and initialize the untrained model shown in the center of figure 1.4.
Figure 1.4 Basic high-level structure of a PyTorch project, with data loading, training, and deployment to production
To train this model, you need a few things (besides the loop itself, which can be a stan-dard Python for loop): a source of training data, an optimizer to adapt the model to the training data, and a way to get the model and data to the hardware that will be per-forming the calculations needed for training the model. Utilities for data loading and handling can be found in torch.util.data. The two main classes you’ll work with are Dataset, which acts as the bridge between your cus-tom data (in whatever format it might be in), and a standardized PyTorch Tensor. The other class you’ll see a lot of is DataLoader, which can spawn child processes to load data from a Dataset in the background so that it’s ready and waiting for the training loop as soon as the loop can use it.12CHAPTER 1 Introducing deep learning and the PyTorch library In the simplest case, the model will be running the required calculations on the local CPU or on a single GPU, so when the training loop has the data, computation can start immediately. It’s more common, however, to want to use specialized hard-ware such as multiple GPUs or to have multiple machines contribute their resources to training the model. In those cases, torch.nn.DataParallel and torch.distrib-uted can be employed to leverage the additional hardware available. W h e n  y o u  h a v e  r e s u l t s  f r o m  r u n n i n g your model on the training data, torch.optim provides standard ways of updating the model so that the output starts to more closely resemble the answers specified in the training data. As mentioned earlier, PyTorch defaults to an immediate execution model (eager mode). Whenever an instruction involving PyTorch is executed by the Python inter-preter, the corresponding operation is immediately carried out by the underlying C++ or CUDA implementation. As more instructions operate on tensors, more operations are executed by the backend implementation. This process is as fast as it typically can be on the C++ side, but it incurs the cost o f  c a l l i n g  t h a t  i m p l e m e n t a t i o n  t h r o u g h Python. This cost is minute, but it adds up. To bypass the cost of the Python interpreter and offer the opportunity to run mod-els independently from a Python runtime, PyTorch also provides a deferred execution model named TorchScript. Using TorchScript, PyTorch can serialize a set of instruc-tions that can be invoked independently from Python. You can think of this model as being a virtual machine with a limited instruction set specific to tensor operations. Besides not incurring the costs of calling into Python, this execution mode gives PyTorch the opportunity to Just in Time (JIT) transform sequences of known opera-tions into more efficient fused operations. These features are the basis of the produc-tion deployment capabilities of PyTorch.1.4.1 Hardware for deep learningRunning a pretrained network on new data is within the capabilities of any recent lap-top or personal computer. Even retraining a small portion of a pretrained network to specialize it on a new data set doesn’t necessarily require specialized hardware. You can follow along with this book on a standard personal computer or laptop. We antici-pate, however, that completing a full training run for more-advanced examples will require a CUDA-capable graphical processing unit (GPU), such as a GPU with 8GB of RAM (we suggest an NVIDIA GTX 1070 or b e t t e r ) .  B u t  t h o s e  p a r a m e t e r s  c a n  b e adjusted if your hardware has less RAM available. To be clear: such hardware isn’t mandatory if you’re willing to wait, but running on a GPU cuts training time by at least an order of magnitude (and usually is 40 to 50 times faster). Taken individually, the operations required to compute parameter updates are fast (from fractions of a second to a few seconds) on modern hardware such as a typical laptop CPU. The issue is that training involves running these operations over and over, many times, incrementally updating the network parameters to minimize training error. Moderately large networks can take hours to days to train from scratch on large, real-world data sets on workstations equipped with good GPUs. That time can be 13PyTorch has the batteries includedreduced by using multiple GPUs on the same machine and even further by using clus-ters of machines equipped with multiple GPUs. These setups are less prohibitive to access than they sound thanks to the offerings of cloud computing providers. DAWN-Bench4 is an interesting initiative from Stanford University aimed at providing bench-marks on training time and cloud computing costs related to common deep learning tasks on publicly available data sets. If you have a GPU around, great. Otherwise, we suggest checking out the offerings of the various cloud platforms, many of which offer GPU-enabled Jupyter notebooks with PyTorch preinstalled, often with a free quota. Last consideration: the operating system (OS). PyTorch has supported Linux and macOS from its first release and gained Windows support during 2018. Because current Apple laptops don’t include GPUs that support CUDA, the precompiled macOS pack-ages for PyTorch are CPU-only. We try to avoid assuming that you run a particular OS; scripts’ command lines should convert to a Windows-compatible form readily. For con-venience, whenever possible we list code as though it’s running on a Jupyter Notebook. For installation information, please see the Getting Started guide on the official website.5 We suggest that Windows users install with Anaconda or Miniconda. Other operating systems, such as Linux, typically have a wider variety of workable options, with Pip being one of the most common installers. Experienced users, of course, are free to install packages in the way that’s most compatible with their preferred develop-ment environments.1.4.2 Using Jupyter NotebooksWe’re going to assume that you have PyTorch and the other dependencies installed and have verified that things are working. We’re going to be making heavy use of Jupy-ter Notebooks for example code. A Jupyter Notebook shows itself as a page in the browser through which you can run code interactively. The code gets evaluated by a kernel, a process running on a server that’s ready to receive code to execute and send back the results, which are rendered inline on the page. A notebook maintains the state of the kernel, such as variables defined during the evaluation of code, in memory until it’s terminated or restarted. The fundamental unit with which you interact with a notebook is a cell, a box on the page where you can type code and have the kernel evaluate it (by choosing the menu item or pressing Shift-Enter). You can add multiple cells to a notebook, and the new cells see the variables you created in the earlier cells. The value returned by the last line of a cell is printed below the cell after execution, and the same goes for plots. By mixing source code, results of evaluations, and Mark-down-formatted text cells, you can generate beautiful interactive documents. You can read everything about Jupyter Notebooks on the project website.64https://dawn.cs.stanford.edu/benchmark/index.html5https://pytorch.org/get-started/locally6https://jupyter.org14CHAPTER 1 Introducing deep learning and the PyTorch library At this point, you’ll need to start the notebook server from the root directory of the code checkout from GitHub. How starting the server looks depends on the details of your operating system and on how and where you installed Jupyter. If you have questions, feel free to ask on our forums.7 W h e n  t h e  n o t e b o o k  s e r v e r  s t a r t s ,  y o u r default browser pops up, showing a list of local notebook files. J u p y t e r  N o t e b o o k s  a r e  p o w e r f u l  t o o l s  f o r  e x p r e s s i n g  a n d  i n v e s t i g a ting ideas through code. Although we think that they make a good fit with our use case, they’re not for everyone. We would argue that it’s important to focus on removing friction and minimizing cognitive overhead, which is going to be different for everyone. Use what you like during your experimentation with PyTorch. You can find full working code for the listings in this book in our repository on GitHub.8ExercisesStart Python to get an interactive prompt.–W h a t  P y t h o n  v e r s i o n  a r e  y o u  u s i n g :  2 . x  o r  3 . x ?–C a n  y o u  import torch? What version of PyTorch do you get?–W h a t  i s  t h e  r e s u l t  o f  torch.cuda.is_available()? Does it match your expectation based on the hardware you’re using?Start the Jupyter Notebook server.–W h a t  v e r s i o n  o f  P y t h o n  i s  J u p y t e r  u s i n g ?–I s  t h e  l o c a t i o n  o f  t h e  torch library used by Jupyter the same as the one you imported from the interactive prompt?SummaryDeep learning models automatically learn to associate inputs and desired out-puts from examples.Libraries like PyTorch allow you to build and train neural network models efficiently.PyTorch minimizes cognitive overhead while focusing on flexibility and speed. It also defaults to immediate execution for operations.TorchScript is a precompiled deferred-execution mode that can be invoked from C++.Since the release of PyTorch in early 2017, the deep learning tooling ecosystem has consolidated significantly.PyTorch provides several utility libraries to facilitate deep learning projects.
7https://forums.manning.com/forums/deep-learning-with-pytorch8https://github.com/deep-learning-with-pytorch/dlwpt-code15It starts with a tensor
Deep learning enables many applications, which invariably consist of taking data in some form, such as images or text, and producing data in another form, such as labels, numbers, or more text. Taken from t h i s  a n g l e ,  d e e p  l e a r n i n g  c o n s i s t s  o f building a system that can transform data from one representation to another. This transformation is driven by extracting commonalities from a series of examples that demonstrate the desired mapping. The system might note the general shape of a dog and the typical colors of a golden retriever, for example. By combining the two image properties, the system can correctly map images with a given shape and color to the golden-retriever label instead of a black lab (or a tawny tomcat, for that mat-ter). The resulting system can consume broad swaths of similar inputs and produce meaningful output for those inputs. The first step of this process is converting the input into floating-point numbers, as you see in the first step of figure 2.1 (along with many other types of data). This chapter coversTensors, the basic data structure in PyTorchIndexing and operating on PyTorch tensors to explore and manipulate dataInteroperating with NumPy multidimensional arraysMoving computations to the GPU for speed16CHAPTER 2 It starts with a tensorBecause a network uses floating-point numbers to deal with information, you need a way to encode real-world data of the kind you want to process into something that’s digestible by a network and then decode the output back to something you can under-stand and use for a purpose. The transformation from one form of data to another is typically learned by a deep neural network in stages, which means that you can think of the partially transformed data between stages as being a sequence of intermediate representations. For image rec-ognition, early representations can be things (like edge detection) or textures (like fur). Deeper representations can capture more-complex structures (like ears, noses, or eyes). I n  g e n e r a l ,  s u c h  i n t e r m e d i a t e  r e p r e s e ntations are collections of floating-point numbers that characterize the input and capture the structure in the data, in a way that’s instrumental for describing how inputs are mapped to the outputs of the neural network. Such characterization is specific to the task at hand and is learned from rele-vant examples. These collections of floating-point numbers and their manipulation are at the heart of modern AI. It’s important to keep in mind that these intermediate representations (such as the ones shown in the second step of figure 2.1) are the results of combining the input with the weights of the previous layer of neurons. Each intermediate representation is unique to the inputs that preceded it. 
Figure 2.1 A deep neural network learns how to transform an input representation to an output representation. (Note: The number of neurons and outputs is not to scale.)
 Before you can begin the process of converting data to floating-point input, you must have a solid understanding of how PyTorch handles and stores data: as input, as 17intermediate representations, and as output. This chapter is devoted to providing pre-cisely that understanding. T o  t h i s  e n d ,  P y T o r c h  i n t r o d u c e s  a  f u n damental data structure: the tensor. For those who come from mathematics, physics, or engineering, the term tensor c o m e s bundled with the notion of spaces, reference systems, and transformations between them. For everyone else, tensor refers to the generalization of vectors and matrices to an arbitrary number of dimensions, as shown in figure 2.2. Another name for the same concept is multidimensional arrays. The dimensionality of a tensor coincides with the number of indexes used to refer to scalar values within the tensor. 
Figure 2.2 Tensors are the building blocks for representing data in PyTorch
 PyTorch isn’t not the only library that deals with multidimensional arrays. NumPy is by far the most popular multidimensional-array library, to the point that it has argu-ably become the lingua f r a n c a o f  d a t a  s c i e n c e .  I n  f a c t ,  P y T o r c h  f e a t u r e s  s e a m l e s s interoperability with NumPy, which brings with it first-class integration with the rest of the scientific libraries in Python, such as SciPy1, Scikit-learn2, and Pandas3. Compared with NumPy arrays, PyTorch tensors have a few superpowers, such as the ability to perform fast operations on graphical processing units (GPUs), to distrib-ute operations on multiple devices or machines, and to keep track of the graph of computations that created them. All these features are important in implementing a modern deep learning library. W e  s t a r t  t h e  c h a p te r  by  i n t r o d u c i n g  P y T orch tensors, covering the basics to set things in motion. We show you how to manipulate tensors by using the PyTorch tensor library, covering things such as how the data is stored in memory and how certain operations can be performed on arbitrarily large tensors in constant time; then we move on to the aforementioned NumPy interoperability and the GPU acceleration. Understanding the capabilities and API of tensors is important if they’re to be go-to tools in your programming toolbox.1https://www.scipy.org2https://scikit-learn.org/stable3https://pandas.pydata.org18CHAPTER 2 It starts with a tensor2.1 Tensor fundamentalsYou’ve already learned that tensors are the fundamental data structures in PyTorch. A tensor is an array—that is, a data structure storing collection of numbers that are accessible individually by means of an index and that can be indexed with multiple indices. T a k e  a  l o o k  a t  list i n d e x i n g  i n  a c t i o n  s o  t h a t  y o u  c a n  c o m p a r e  i t  w i t h  t e n s o r indexing. The following listing shows a list of three numbers in Python.# In[1]:a = [1.0, 2.0, 1.0]You can access the first element of the list by using the corresponding 0-based index:# In[2]:a[0]# Out[2]:1.0# In[3]:a[2] = 3.0a# Out[3]:[1.0, 2.0, 3.0]It’s not unusual for simple Python programs that deal with vectors of numbers, such as the coordinates of a 2D line, to use Python lists to store the vector. This practice can be suboptimal, however, for several reasons:Numbers in Python are full-fledged objects. Whereas a floating-point number might take only 32 bits to be represented on a computer, Python boxes them in a full-fledged Python object with reference counting and so on. This situation isn’t a problem if you need to store a small number of numbers, but allocating mil-lions of such numbers gets inefficient.Lists in Python are meant for sequential collections of objects. No operations are defined for, say, efficiently taking the dot product of two vectors or summing vectors. Also, Python lists have no way of optimizing the layout of their content in memory, as they’re indexable collections of pointers to Python objects (of any kind, not numbers alone). Finally, Python lists are one-dimensional, and although you can create lists of lists, again, this practice is inefficient.The Python interpreter is slow compared with optimized, compiled code. Performing mathematical operations on l a r g e  c o l l e c t i o n s  o f  n umerical data can be must faster using optimized code written in a compiled, low-level language like C.For these reasons, data science libraries rely on NumPy or introduce dedicated data structures such as PyTorch tensors that provide efficient low-level implementations of Listing 2.1 code/p1ch3/1_tensors.ipynb19Tensor fundamentalsnumerical data structures and related operations on them, wrapped in a convenient high-level API. Many types of data—from images to time series, audio, and even sentences—can be represented by tensors. By defining operations over tensors, some of which you explore in this chapter, you can slice and manipulate data expressively and efficiently at the same time, even from a high-level (and not particularly fast) language such as Python. Now you’re ready to construct your first PyTorch tensor to see what it looks like. This tensor won’t be particularly meaningful for now, being three ones in a column:# In[4]:import torcha = torch.ones(3)a# Out[4]:tensor([1., 1., 1.])# In[5]:a[1]# Out[5]:tensor(1.)# In[6]:float(a[1])# Out[6]:1.0# In[7]:a[2] = 2.0a# Out[7]:tensor([1., 1., 2.])Now take a look at what you did here. After importing the torch module, you called a function that creates a (one-dimensional) tensor of size 3 filled with the value 1.0. You can access an element by using its 0-based index or assign a new value to it. Although on the surface, this example doesn’t differ much from a list of number objects, under the hood, things are completely different. Python lists or tuples of num-bers are collections of Python objects that are individually allocated in memory, as shown on the left side of figure 2.3. PyTorch tensors or NumPy arrays, on the other hand, are views over (typically) contiguous memory blocks containing unboxed C numeric types, not Python objects. In this case, 32 bits (4 bytes) float, as you see on the right side of figure 2.3. So a 1D tensor of 1 million float numbers requires 4 million contiguous bytes to be stored, plus a small overhead for the metadata (dimensions, numeric type, and so on).Figure 2.3 Python object (boxed) numeric values versus tensor (unboxed array) numeric values
20CHAPTER 2 It starts with a tensor
Suppose that you have a list of 2D coordinates that you’d like to manage to represent a geometrical object, such as a triangle. The example isn’t particularly pertinent to deep learning, but it’s easy to follow. Instead of having coordinates as numbers in a Python list, you can use a one-dimensional tensor by storing xs in the even indices and ys in the odd indices, like so:# In[8]:points = torch.zeros(6) points[0] = 1.0 points[1] = 4.0points[2] = 2.0points[3] = 1.0points[4] = 3.0points[5] = 5.0You can also pass a Python list to the constructor to the same effect# In[9]:points = torch.tensor([1.0, 4.0, 2.0, 1.0, 3.0, 5.0])points# Out[9]:tensor([1., 4., 2., 1., 3., 5.])to get the coordinates of the first point:# In[10]:float(points[0]), float(points[1])# Out[10]:(1.0, 4.0)This technique is OK, although it would be practical to have the first index refer to individual 2D points rather than point coordinates. For this purpose, you can use a 2D tensor:The use of .zeros here is a way to get an appropriately sized array.Overwrite those zeros with the values you want.21Tensor fundamentals# In[11]:points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])points# Out[11]:tensor([[1., 4.],        [2., 1.],        [3., 5.]])Here, you passed a list of lists to the constructor. You can ask the tensor about its shape,# In[12]:points.shape# Out[12]:torch.Size([3, 2])which informs you of the size of the tensor along each dimension. You could also use zeros or ones to initialize the tensor, providing the size as a tuple:# In[13]:points = torch.zeros(3, 2)points# Out[13]:tensor([[0., 0.],        [0., 0.],        [0., 0.]])Now you can access an individual element in the tensor by using two indices:# In[14]:points = torch.FloatTensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])points# Out[14]:tensor([[1., 4.],        [2., 1.],        [3., 5.]])# In[15]:points[0, 1]# Out[15]:tensor(4.)This code returns the y c o o r d i n a t e of  th e  0th point in your data set. You can also access the first element in the tensor as you did before to get the 2D coordinates of the first point:# In[16]:points[0]# Out[16]:tensor([1., 4.])22CHAPTER 2 It starts with a tensorNote that what you get as the output is another tensor, but a 1D tensor of size 2 contain-ing the values in the first row of the points tensor. Does this output mean that a new chunk of memory was allocated, values were copied into it, and the new memory was returned wrapped in a new tensor object? No, because that process would be ineffi-cient, especially if you had millions of points. What you got back instead was a differ-ent view of the same underlying data, limited to the first row.2.2 Tensors and storagesIn this section, you start getting hints about the implementation under the hood. Val-ues are allocated in contiguous chunks of memory, managed by torch.Storageinstances. A storage is a one-dimensional array of numerical data, such as a contiguous block of memory containing numbers of a given type, perhaps a float o r  int32. A PyTorch Tensor is a view over such a Storage that’s capable of indexing into that storage by using an offset and per-dimension strides. Multiple tensors can index the same storage even if they index into the data differ-ently. You can see an example in figure 2.4. In fact, when you requested points[0] in the last snippet, what you got back was another tensor that indexes the same storage as the points tensor, but not all of it and with different dimensionality (1D versus 2D). The underlying memory is allocated only once, however, so creating alternative tensor views on the data can be done quickly, regardless of the size of the data managed by the Storage instance. 
Figure 2.4 Tensors are views over a Storage instance
Next, you see how indexing into the storage works in practice with 2D points. You can access the storage for a given tensor by using the .storage property:23Tensors and storages# In[17]:points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])points.storage()# Out[17]: 1.0 4.0 2.0 1.0 3.0 5.0[torch.FloatStorage of size 6]Even though the tensor reports itself as having three rows and two columns, the stor-age under the hood is a contiguous array of size 6. In this sense, the tensor knows how to translate a pair of indices into a location in the storage. You can also index into a storage manually:# In[18]:points_storage = points.storage()points_storage[0]# Out[18]:1.0# In[19]:points.storage()[1]# Out[19]:4.0You can’t index a storage of a 2D tensor by using two indices. The layout of a storage is always one-dimensional, irrespective of the dimensionality of any tensors that may refer to it. At this point, it shouldn’t come as a surprise that changing the value of a storage changes the content of its referring tensor:# In[20]:points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])points_storage = points.storage()points_storage[0] = 2.0points# Out[20]:tensor([[2., 4.],        [2., 1.],        [3., 5.]])You’ll seldom, if ever, use storage instances directly, but understanding the relation-ship between a tensor and the underlying storage is useful for understanding the cost (or lack thereof) of certain operations later. This mental model is a good one to keep in mind when you want to write effective PyTorch code.24CHAPTER 2 It starts with a tensor2.3 Size, storage offset, and stridesTo index into a storage, tensors rely on a few pieces of information that, together with their storage, unequivocally define them: size, storage offset, and stride (figure 2.5). The size (or shape, in NumPy parlance) is a tuple indicating how many elements across each dimension the tensor represents. The storage offset is the index in the storage that corresponds to the first element in the tensor. The stride is the number of elements in the storage that need to be skipped to obtain the next element along each dimension.
Figure 2.5 Relationship among a tensor’s offset, size, and stride
You can get the second point in the tensor by providing the corresponding index:# In[21]:points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])second_point = points[1]second_point.storage_offset()# Out[21]:2# In[22]:second_point.size()# Out[22]:torch.Size([2])25Size, storage offset, and stridesThe resulting tensor has offset 2 i n  t h e  s t o r a g e  ( b e c a u s e  w e  n e e d  t o  s k i p  t h e  f i r s t point, which has two items) and the size is an instance of the Size class containing one element because the tensor is one-dimensional. Important note: this information is the same information contained in the shape property of tensor objects:# In[23]:second_point.shape# Out[23]:torch.Size([2])Last, stride is a tuple indicating the number of elements in the storage that have to be skipped when the index is increased by 1 in each dimension. Your points tensor, for example, has a stride:# In[24]:points.stride()# Out[24]:(2, 1)Accessing an element i, j in a 2D tensor results in accessing the storage_offset + stride[0] * i + stride[1] * j element in the storage. The offset will usually be zero; if this tensor is a view into a storage created to hold a larger tensor the offset might be a positive value. T h i s  i n d i r e c t i o n  b e t w e e n  Tensor a n d  Storage l e a d s  s o m e  o p e r a t i o n s ,  s u c h  a s transposing a tensor or extracting a subtensor, to be inexpensive, as they don’t lead to memory reallocations; instead, they consist of allocating a new tensor object with a dif-ferent value for size, storage offset, or stride. You saw how to extract a subtensor when you indexed a specific point and saw the storage offset increasing. Now see what happens to size and stride:# In[25]:points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])second_point = points[1]second_point.size()# Out[25]:torch.Size([2])# In[26]:second_point.storage_offset()# Out[26]:2# In[27]:second_point.stride()# Out[27]:(1,)26CHAPTER 2 It starts with a tensorBottom line, the subtensor has one fewer dimension (as you’d expect) while still indexing the same storage as the original points tensor. Changing the subtensor has a side effect on the original tensor too:# In[28]:points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])second_point = points[1]second_point[0] = 10.0points# Out[28]:tensor([[ 1.,  4.],        [10.,  1.],        [ 3.,  5.]])This effect may not always be desirable, so you can eventually clone the subtensor into a new tensor:# In[29]:points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])second_point = points[1].clone()second_point[0] = 10.0points# Out[29]:tensor([[1., 4.],        [2., 1.],        [3., 5.]])Try transposing now. Take your points tensor, which has individual points in the rows and x and y coordinates in the columns, and turn it around so that individual points are along the columns:# In[30]:points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])points# Out[30]:tensor([[1., 4.],        [2., 1.],        [3., 5.]])# In[31]:points_t = points.t()points_t# Out[31]:tensor([[1., 2., 3.],        [4., 1., 5.]])You can easily verify that the two tensors share storage# In[32]:id(points.storage()) == id(points_t.storage())# Out[32]:True27Size, storage offset, and stridesand that they differ only in shape and stride:# In[33]:points.stride()# Out[33]:(2, 1)# In[34]:points_t.stride()# Out[34]:(1, 2)This result tells you that increasing the first index by 1 in points—that is, going from points[0,0] t o  points[1,0]—skips along the storage by two elements, and that increasing the second index from points[0,0] to points[0,1] skips along the stor-age by one. In other words, the storage holds the elements in the tensor sequentially row by row. You can transpose points into points_t as shown in figure 2.6. You change the order of the elements in the stride. After that, increasing the row (the first index of the tensor) skips along the storage by 1, as when you were moving along columns in points. This is the definition of transposing. No new memory is allocated: transpos-ing is obtained only by creating a new Tensor instance with different stride ordering from the original.
Figure 2.6 Transpose operation applied to a tensor
28CHAPTER 2 It starts with a tensorTransposing in PyTorch isn’t limited to matrices. You can transpose a multidimen-sional array by specifying the two dimensions along which transposing (such as flip-ping shape and stride) should occur:# In[35]:some_tensor = torch.ones(3, 4, 5)some_tensor_t = some_tensor.transpose(0, 2)some_tensor.shape# Out[35]:torch.Size([3, 4, 5])# In[36]:some_tensor_t.shape# Out[36]:torch.Size([5, 4, 3])# In[37]:some_tensor.stride()# Out[37]:(20, 5, 1)# In[38]:some_tensor_t.stride()# Out[38]:(1, 5, 20)A tensor whose values are laid out in the storage starting from the rightmost dimen-sion onward (moving along rows for a 2D tensor, for example) is defined as being con-tiguous. Contiguous tensors are convenient because you can visit them efficiently and in order without jumping around in the storage. (Improving data locality improves performance because of the way memory access works in modern CPUs.) In this case, points is contiguous but its transpose is not:# In[39]:points.is_contiguous()# Out[39]:True# In[40]:points_t.is_contiguous()# Out[40]:FalseYou can obtain a new contiguous tensor from a noncontiguous one by using the con-tiguous method. The content of the tensor stays the same, but the stride changes, as does the storage:29Size, storage offset, and strides# In[41]:points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])points_t = points.t()points_t# Out[41]:tensor([[1., 2., 3.],        [4., 1., 5.]])# In[42]:points_t.storage()# Out[42]: 1.0 4.0 2.0 1.0 3.0 5.0[torch.FloatStorage of size 6]# In[43]:points_t.stride()# Out[43]:(1, 2)# In[44]:points_t_cont = points_t.contiguous()points_t_cont# Out[44]:tensor([[1., 2., 3.],        [4., 1., 5.]])# In[45]:points_t_cont.stride()# Out[45]:(3, 1)# In[46]:points_t_cont.storage()# Out[46]: 1.0 2.0 3.0 4.0 1.0 5.0[torch.FloatStorage of size 6]Notice that the storage has been reshuffled for elements to be laid out row by row in the new storage. The stride has been changed to reflect the new layout.30CHAPTER 2 It starts with a tensor2.4 Numeric typesAll right, you know the basics of how tensors work. But we haven’t touched on the numeric types you can store in a Tensor. The dtype argument to tensor constructors (that is, functions such as tensor, zeros, and ones) specifies the numerical data type that will be contained in the tensor. The data type specifies the possible values that the tensor can hold (integers versus floating-point numbers) and the number of bytes per value.4 The dtype argument is deliberately similar to the standard NumPy argument of the same name. Here’s a list of the possible values for the dtype argument:torch.float32 or torch.float—32-bit floating-pointtorch.float64 or torch.double—64-bit, double-precision floating-pointtorch.float16 or torch.half—16-bit, half-precision floating-pointtorch.int8—Signed 8-bit integerstorch.uint8—Unsigned 8-bit integerstorch.int16 or torch.short—Signed 16-bit integerstorch.int32 or torch.int—Signed 32-bit integerstorch.int64 or torch.long—Signed 64-bit integersEach of torch.float, torch.double, and so on has a corresponding concrete class of torch.FloatTensor, torch.DoubleTensor, and so on. The class for  torch.int8  is torch.CharTensor, and the class for  torch.uint8  is torch.ByteTensor. torch.Ten-sor is an alias for torch.FloatTensor. The default data type is 32-bit floating-point. To allocate a tensor of the right numeric type, you can specify the proper dtype as an argument to the constructor, as follows:# In[47]:double_points = torch.ones(10, 2, dtype=torch.double)short_points = torch.tensor([[1, 2], [3, 4]], dtype=torch.short)You can find out about the dtype for a tensor by accessing the corresponding attri-bute:# In[48]:short_points.dtype# Out[48]:torch.int16You can also cast the output of a tensor-creation function to the right type by using the corresponding casting method, such as# In[49]:double_points = torch.zeros(10, 2).double()short_points = torch.ones(10, 2).short()4And signedness, in the case of uint831NumPy interoperabilityor the more convenient to method:# In[50]:double_points = torch.zeros(10, 2).to(torch.double)short_points = torch.ones(10, 2).to(dtype=torch.short)Under the hood, type a n d  to p e r f o r m  t h e  s a m e  t y p e  c h e c k - a n d - c o n v e r t - i f - n e e d e d operation, but the to method can take additional arguments. You can always cast a tensor of one type as a tensor of another type by using the type method:# In[51]:points = torch.randn(10, 2) short_points = points.type(torch.short)2.5 Indexing tensorsYou’ve seen that points[0] returns a tensor containing the 2D point at the first row of the tensor. What if you need to obtain a tensor that contains all points but the first? That task is easy when you use range indexing notation, the same kind that applies to standard Python lists:# In[53]:From element 1 inclusive to element 4 exclusive in steps of 2some_list = list(range(6))some_list[:]     some_list[1:4]   some_list[1:]    some_list[:4]    some_list[:-1]  some_list[1:4:2] To achieve your goal, you can use the same notation for PyTorch tensors, with the added benefit that as in NumPy and in other Python scientific libraries, we can use range indexing for each dimension of the tensor:# In[54]:points[1:]  points[1:, :]    points[1:, 0]  All rows after first, first columnIn addition to using ranges, PyTorch features a powerful form of indexing called advanced indexing. 2.6 NumPy interoperabilityAlthough we don’t consider experience in NumPy to be a prerequisite for reading this book, we strongly encourage you to get familiar with NumPy due to its ubiquity in the Python data science ecosystem. PyTorch tensors can be converted to NumPy arrays and vice versa efficiently. By doing so, you can leverage the huge swath of functionality in the wider Python ecosystem that has built up around the NumPy array type. This randn initializes the tensor elements to random numbers between 0 and 1. 
All elements in the listFrom element 1 inclusive to element 4 exclusiveFrom element 1 inclusive to the end of the listFrom the start of the list to element 4 exclusiveFrom the start of the list to one before the last elementAll rows after first, implicitly all columnsAll rows after first,  all columns32CHAPTER 2 It starts with a tensorzero-copy interoperability with NumPy arrays is due to the storage system that works with the Python buffer protocol.5 To get a NumPy array out of your points tensor, call# In[55]:points = torch.ones(3, 4)points_np = points.numpy()points_np# Out[55]:array([[1., 1., 1., 1.],       [1., 1., 1., 1.],       [1., 1., 1., 1.]], dtype=float32)which returns a NumPy multidimensional array of the right size, shape, and numeri-cal type. Interestingly, the returned array shares an underlying buffer with the tensor storage. As a result, the numpy method can be executed effectively at essentially no cost as long as the data sits in CPU RAM, and modifying the NumPy array leads to a change in the originating tensor. If the tensor is allocated on the GPU, PyTorch makes a copy of the content of the tensor into a NumPy array allocated on the CPU. Conversely, you can obtain a PyTorch tensor from a NumPy array this way# In[56]:points = torch.from_numpy(points_np)which uses the same buffer-sharing strategy.2.7 Serializing tensorsCreating a tensor on the fly is all well and good, but if the data inside it is of any value to you, you want to save it to a file and load it back at some point. After all, you don’t want to have to retrain a model from scratch every time you start running your pro-gram! PyTorch uses pickle under the hood to serialize the tensor object, as well as dedicated serialization code for the storage. Here’s how you can save your points ten-sor to a ourpoints.t file:# In[57]:torch.save(points, '../data/p1ch3/ourpoints.t')As an alternative, you can pass a file descriptor in lieu of the filename:# In[58]:with open('../data/p1ch3/ourpoints.t','wb') as f:   torch.save(points, f)Loading your points back is similarly a one-liner:# In[59]:points = torch.load('../data/p1ch3/ourpoints.t')5https://docs.python.org/3/c-api/buffer.html33Serializing tensorsThe equivalent is# In[60]:with open('../data/p1ch3/ourpoints.t','rb') as f:   points = torch.load(f)This technique allows you to save tensors quickly in case you only want to load them with PyTorch, but the file format itself isn’t interoperable. You can’t read the tensor with software other than PyTorch. Depending on the use case, this situation may not be a limitation, but you should learn how to save tensors interoperably for those times when it is. Although every use case is unique, we suspect that this one will be more common when you introduce PyTorch into existing systems that already rely on differ-ent libraries. New projects probably won’t need to save tensors interoperably as often. For those cases when you need to, however, you can use the HDF5 format and library.6 HDF5 is a portable, widely supported format for representing serialized multi-dimensional arrays, organized in a nested key-value dictionary. Python supports HDF5 through the h5py library7, which accepts and returns data under the form of NumPy arrays. You can install h5py by using$ conda install h5pyAt this point, you can save your points tensor by converting it to a NumPy array (at no cost, as noted earlier) and passing it to the create_dataset function:# In[61]:import h5pyf = h5py.File('../data/p1ch3/ourpoints.hdf5', 'w')dset = f.create_dataset('coords', data=points.numpy())f.close()Here, 'coords' is a key into the HDF5 file. You can have other keys, even nested ones. One interesting thing in HDF5 is that you can index the data set while on disk and access only the elements you’re interested in. Suppose that you want to load only the last two points in your data set:# In[62]:f = h5py.File('../data/p1ch3/ourpoints.hdf5', 'r')dset = f['coords']last_points = dset[1:]Here, data wasn’t loaded when the file was opened or the data set was required. Rather, data stayed on disk until you requested the second and last rows in the data set. At that point, h5py accessed those two columns and returned a NumPy array-like object encapsulating that region in that data set that behaves like a NumPy array and has the same API.6https://www.hdfgroup.org/solutions/hdf57http://www.h5py.org34CHAPTER 2 It starts with a tensor Owing to this fact, you can pass the returned object to the torch.from_numpy func-tion to obtain a tensor directly. Note that in this case, the data is copied over to the tensor’s storage:# In[63]:last_points = torch.from_numpy(dset[1:])f.close()>>> last_points = torch.from_numpy(dset[1:])When you finish loading data, close the file.2.8 Moving tensors to the GPUOne last point about PyTorch tensors is related to computing on the GPU. Every Torch tensor can be transferred to a GPUs to perform fast, massively parallel compu-tations. All operations to be performed on the tensor are carried out by GPU-specific routines that come with PyTorch.NOTE As of early 2019, main PyTorch releases have acceleration only on GPUs that have support for CUDA. Proof-of-concept versions of PyTorch run-ning on AMD’s ROCm8 platform exist, but full support hasn’t been merged into PyTorch as of version 1.0. Support for Google’s TPUs is a work in prog-ress9, with the current proof of concept available to the public in Google Colab.10 Implementation of data structures and kernels on other GPU tech-nology, such as OpenCL, wasn’t planned at the time we wrote this chapter.In addition to the dtype, a PyTorch tensor has a notion of device, which is where on the computer the tensor data is being placed. Here’s how to create a tensor on the GPU by specifying the corresponding argument to the constructor:# In[64]:points_gpu = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 4.0]], device='cuda')You could instead copy a tensor created on the CPU to the GPU by using the tomethod:# In[65]:points_gpu = points.to(device='cuda')This code returns a new tensor that has the same numerical data but is stored in the RAM of the GPU rather than in regular system RAM. Now that the data is stored locally on the GPU, you start to see speedups when per-forming mathematical operations on the tensor. Also, the class of this new GPU-backed tensor changes to torch.cuda.FloatTensor. (Given the starting type of torch.Float-Tensor; the corresponding set of torch.cuda.DoubleTensor a n d  s o  o n  e x i s t s . )  I n almost all cases, CPU- and GPU-based tensors expose the same user-facing API, making 8https://rocm.github.io9https://github.com/pytorch/xla10https://colab.research.google.com35The tensor APIit much easier to write code that is agnostic to where the heavy number-crunching pro-cess is running. In case your machine has more than one GPU, you can decide which GPU to allo-cate the tensor to by passing a zero-based integer identifying the GPU on the machine:# In[66]:points_gpu = points.to(device='cuda:0')At this point, any operation performed on the tensor, such as multiplying all elements by a constant, is carried out on the GPU:# In[67]:points = 2 * points  points_gpu = 2 * points.to(device='cuda')  Multiplication performed on the GPUNote that the points_gpu tensor isn’t brought back to the CPU when the result has been computed. Here’s what happened:1The points tensor was copied to the GPU.2A new tensor was allocated on the GPU and used to store the result of the mul-tiplication.3A handle to that GPU tensor was returned.Therefore, if you also add a constant to the result,# In[68]:points_gpu = points_gpu + 4the addition is still performed on the GPU, and no information flows to the CPU (except if you print or access the resulting tensor). To move the tensor back to the CPU, you need to provide a cpu argument to the to method:# In[69]:points_cpu = points_gpu.to(device='cpu')You can use the shorthand methods cpu and cuda instead of the to method to achieve the same goal:# In[70]:points_gpu = points.cuda() points_gpu = points.cuda(0)points_cpu = points_gpu.cpu()It’s worth mentioning that when you use the to method, you can change the place-ment and the data type simultaneously by providing device and dtype as arguments.2.9 The tensor APIAt this point, you know what PyTorch tensors are and how they work under the hood. Before we wrap up this chapter, we’ll take a look at the tensor operations that PyTorch offers. It would be of little use to list all of them all here. Instead, we’re going to give you a general feel for the API and show you where to find things in the online docu-mentation at http://pytorch.org/docs.Multiplication performed on the CPU
Defaults to GPU index 036CHAPTER 2 It starts with a tensor First, the vast majority of operations on and between tensors are available under the torch module and can also be called as methods of a tensor object. The trans-pose function that you encountered earlier, for example, can be used from the torchmodule# In[71]:a = torch.ones(3, 2)a_t = torch.transpose(a, 0, 1)or as a method of the a tensor:# In[72]:a = torch.ones(3, 2)a_t = a.transpose(0, 1)No difference exists between the two forms, which can be used interchangeably. A caveat, though: a small number of operations exist only as methods of the tensor object. They’re recognizable by the trailing underscore in their name, such as zero_, which indicates that the method operates in-place by modifying the input instead of creating a new output tensor and returning it. The zero_ method, for example, zeros out all the elements of the input. Any method without the trailing underscore leaves the source tensor unchanged and returns a new tensor:# In[73]:a = torch.ones(3, 2)# In[74]:a.zero_()a# Out[74]:tensor([[0., 0.],        [0., 0.],        [0., 0.]])Earlier, we mentioned the online docs11, which are exhaustive and well organized with the tensor operations divided into groups:Creation ops—Functions for constructing a tensor, such as ones and from_numpyIndexing, slicing, joining, and mutating ops—Functions for changing the shape, stride, or content of a tensor, such as transposeMath ops—Functions for manipulating the content of the tensor through com-putations:–Pointwise ops—Functions for obtaining a new tensor by applying a function to each element independently, such as abs and cos–Reduction ops—Functions for computing aggregate values by iterating through tensors, such as mean, std, and norm11http://pytorch.org/docs37The tensor API–Comparison ops—Functions for evaluating numerical predicates over tensors, such as equal and max–Spectral ops—Functions for transforming in and operating in the frequency domain, such as stft and hamming_window–Other ops—Special functions operating on vectors, such as cross, or matrices, such as trace–BLAS and LAPACK ops—Functions that follow the BLAS (Basic Linear Alge-bra Subprograms) specification for scalar, vector-vector, matrix-vector, and matrix-matrix operationsRandom sampling ops—Functions for generating values by drawing randomly from probability distributions, such as randn and normalSerialization ops—Functions for saving and loading tensors, such as load a n d saveParallelism ops—Functions for controlling the number of threads for parallel CPU execution, such as set_num_threadsIt’s useful to play with the general tensor API. This chapter should provide all the pre-requisites for this kind of interactive exploration.ExercisesCreate a tensor a from list(range(9)). Predict then check what the size, off-set, and strides are.Create a tensor b = a.view(3, 3). What is the value of b[1,1]?Create a tensor c = b[1:,1:]. Predict then check what the size, offset, and strides are.Pick a mathematical operation like cosine or square root. Can you find a corre-sponding function in the torch library?Is there a version of your function that operates in-place?SummaryNeural networks transform floating-point r e p r e s e n t a t i o n s  i n t o  o t h e r  f l o a t i n g -point representations, with the starting a n d  e n d i n g  r e p r e s e n t a t i o n s  t y p i c a l l y being human-interpretable. The intermediate representations are less so.These floating-point representations are stored in tensors.Tensors are multidimensional arrays and the basic data structure in PyTorch.PyTorch has a comprehensive standard library for tensor creation and manipu-lation and for mathematical operations.Tensors can be serialized to disk and loaded back.All tensor operations in PyTorch can execute on the CPU as well as on the GPU with no change in the code.PyTorch uses a trailing underscore to indicate that a function operates in-place on a tensor (such as Tensor.sqrt_).39Real-world data  representation with tensors
Tensors are the building blocks for data in PyTorch. Neural networks take tensors in input and produce tensors as outputs. In fact, all operations within a neural net-work and during optimization are operations between tensors, and all parameters (such as weights and biases) in a neural network are tensors. Having a good sense of how to perform operations on tensors and index them effectively is central to using tools like PyTorch successfully. Now that you know the basics of tensors, your dexterity with them will grow.This chapter coversRepresenting different types of real-world data as PyTorch tensorsWorking with range of data types, including spreadsheet, time series, text, image, and medical imagingLoading data from fileConverting data to tensorsShaping tensors so that they can be used as inputs for neural network models40CHAPTER 3 Real-world data representation with tensors We can address one question at this point: how do you take a piece of data, a video, or text, and represent it with a tensor, and do that in a way that’s appropriate for train-ing a deep learning model? The answer is what you’ll learn in this chapter. We cover different types of data and show you how to get them represented as tensors. Then we show you how to load the data from the most common on-disk formats and also get a feeling for those data types structure so that you can see how to prepare them for training a neural network. Often, your raw data won’t be perfectly formed for the problem you’d like to solve, so you’ll have a chance to practice your tensor manipulation skills on a few more inter-esting tensor operations. You’ll be using a lot of image and volumetric data because those data types are common and reproduce well in book format. We also cover tabu-lar data, time series, and text, which are also of interest to many readers. Each section of the chapter describes a data type, and each comes with its own data set. Although we’ve structured the chapter so that each data type builds on the pre-ceding one, you should feel free to skip around a bit if you’re so inclined. We start with tabular data of data about wines, as you’d find in a spreadsheet. Next, we move to ordered tabular data, with a time-series data set from a bike-sharing pro-gram. After that, we show you how to work with text data from Jane Austen. Text data retains the ordered aspect but introduces the problem of representing words as arrays of numbers. Because a picture is worth a thousand words, we demonstrate how to work with image data. Finally, we dip into medical data with a 3D array that represents a volume containing patient anatomy. I n  e v e r y  s e c t i o n ,  w e  s t o p  w h e r e  a  d e e p  l e a r n i n g  r e s e a r c h e r  w o u l d  s t a r t :  r i g h t before feeding the data to a model. We encourage you to keep these data sets around. They’ll constitute excellent material when you start learning how to train neural net-work models.3.1 Tabular dataThe simplest form of data you’ll encounter in your machine learning job is sitting in a spreadsheet, in a CSV (comma-separated values) file, or in a database. Whatever the medium, this data is a table containing one row per sample (or record), in which col-umns contain one piece of information about the sample. At first, assume that there’s no meaning in the order in which samples appear in the table. Such a table is a collection of independent samples, unlike a time-series, in which samples are related by a time dimension. Columns may contain numerical values, such as temperatures at specific locations, or labels, such as a string expressing an attribute of the sample (like "blue"). There-fore, tabular data typically isn’t homogeneous; different columns don’t have the same type. You might have a column showing the weight of apples and another encoding their color in a label. PyTorch tensors, on the other hand, are homogeneous. Other data science pack-ages, such as Pandas, have the concept of the data frame, an object representing a data set with named, heterogenous columns. By contrast, information in PyTorch is encoded 41Tabular dataas a number, typically floating-point (though integer types are supported as well). Numeric encoding is deliberate, because neural networks are mathematical entities that take real numbers as inputs and produce real numbers as output through successive application of matrix multiplications and nonlinear functions. Your first job as a deep learning practitioner, therefore, is to encode heterogenous, real-world data in a tensor of floating-point numbers, ready for consumption by a neu-ral network. A  l a r g e  n u m b e r  o f  t a b u l a r  d a t a  s e t s  is freely available on the internet. See https://github.com/caesar0301/awesome-public-data sets, for example. We start with something fun: wine. The Wine Quality data set is a freely available table containing chemical characterizations of samples of vinho verde ( a  w i n e  f r o m northern Portugal) together with a sensory quality score. You can download the data set for white wines at https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv. For convenience, we created a copy of the data set on the Deep Learning with PyTorch Git repository, under data/p1ch4/tabular-wine. The file contains a comma-separated collection of values organized in 12 columns preceded by a header line containing the column names. The first 11 columns con-tain values of chemical variables; the last column contains the sensory quality score from 0 (worst) to 10 (excellent). Following are the column names in the order in which they appear in the data set:fixed acidityvolatile aciditycitric acidresidual sugarchloridesfree sulfur dioxidetotal sulfur dioxidedensitypHsulphatesalcoholqualityA possible machine learning task on this data set is predicting the quality score from chemical characterization alone. Don’t worry, though—machine learning isn’t going to kill wine tasting anytime soon. We have to get the training data from somewhere! As shown in figure 3.1, you hope to find a relationship between one of the chemi-cal columns in your data and the quality column. Here, you’re expecting to see quality increase as sulfur decreases.  Before you can get to that observation, however, you need to be able to examine the data in a more usable way than opening the file in a text editor. We’ll show you how to load the data by using Python and then turn it into a PyTorch tensor. Python offers several options for loading a CSV file quickly. Three popular options areThe csv module that ships with PythonNumPyPandasFigure 3.1 The relationship between sulfur and quality in wine42CHAPTER 3 Real-world data representation with tensors
The third option is the most time- and memory-efficient, but we’ll avoid introducing an additional library into your learning trajectory merely to load a file. Because we’ve already introduced NumPy and PyTorch has excellent NumPy interoperability, you’ll go with it. Load your file and turn the resulting NumPy array into a PyTorch tensor, as shown in the following listing.# In[2]:import csvwine_path = "../data/p1ch4/tabular-wine/winequality-white.csv"wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=";", skiprows=1)wineq_numpy# Out[2]:array([[ 7.  ,  0.27,  0.36, ...,  0.45,  8.8 ,  6.  ],       [ 6.3 ,  0.3 ,  0.34, ...,  0.49,  9.5 ,  6.  ],       [ 8.1 ,  0.28,  0.4 , ...,  0.44, 10.1 ,  6.  ],       ...,       [ 6.5 ,  0.24,  0.19, ...,  0.46,  9.4 ,  6.  ],       [ 5.5 ,  0.29,  0.3 , ...,  0.38, 12.8 ,  7.  ],       [ 6.  ,  0.21,  0.38, ...,  0.32, 11.8 ,  6.  ]], dtype=float32)Here, you prescribed the type of the 2D array (32-bit floating-point) and the delimiter used to separate values in each row, and stated that the first line shouldn’t be read because it contains the column names. Next, check that all the data has been read,Listing 3.1 code/p1ch4/1_tabular_wine.ipynb43Tabular data# In[3]:col_list = next(csv.reader(open(wine_path), delimiter=';'))wineq_numpy.shape, col_list# Out[3]:((4898, 12), ['fixed acidity',  'volatile acidity',  'citric acid',  'residual sugar',  'chlorides',  'free sulfur dioxide',  'total sulfur dioxide',  'density',  'pH',  'sulphates',  'alcohol',  'quality'])and proceed to convert the NumPy array to a PyTorch tensor:# In[4]:wineq = torch.from_numpy(wineq_numpy)wineq.shape, wineq.type()# Out[4]:(torch.Size([4898, 12]), 'torch.FloatTensor')At this point, you have a torch.FloatTensor containing all columns, including the last, which refers to the quality score.  Interval, ordinal, and categorical valuesYou should be aware of three kinds of numerical values as you attempt to make sense of your data.The first kind is continuous values. These values are the most intuitive when repre-sented as numbers; they’re strictly ordered, and a difference between various values has a strict meaning. Stating that package A is 2 kilograms heavier than package B or that package B came from 100 miles farther away than package A has a fixed meaning, no matter whether package A weighs 3 kilograms or 10, or whether B came from 200 miles away or 2,000. If you’re counting or measuring something with units, the value probably is a continuous value.Next are ordinal values. The strict ordering of continuous values remains, but the fixed relationship between values no longer applies. A good example is ordering a small, medium, or large drink, with small mapped to the value 1, medium to 2, and large to 3. The large drink is bigger than the medium, in the same way that 3 is bigger than 2, but it doesn’t tell you anything about how much bigger. If you were to convert 1, 2, and 3 to the actual volumes (say, 8, 12, and 24 fluid ounces), those values would switch to inter-val values. It’s important to remember that you can’t do math on the values beyond order-ing them; trying to average large=3 and small=1 does not result in a medium drink!44CHAPTER 3 Real-world data representation with tensors
You could treat the score as a continuous variable, keep it as a real number, and per-form a regression task, or treat it as a label and try to guess such label from the chemi-cal analysis in a classification task. In both methods, you typically remove the score from the tensor of input data and keep it in a separate tensor, so that you can use the score as the ground truth without it being input to your model:# In[5]:data = wineq[:, :-1] data, data.shape# Out[5]:(tensor([[ 7.0000,  0.2700,  ...,  0.4500,  8.8000],         [ 6.3000,  0.3000,  ...,  0.4900,  9.5000],         ...,         [ 5.5000,  0.2900,  ...,  0.3800, 12.8000],         [ 6.0000,  0.2100,  ...,  0.3200, 11.8000]]), torch.Size([4898, 11]))# In[6]:target = wineq[:, -1] target, target.shape# Out[6]:(tensor([6., 6.,  ..., 7., 6.]), torch.Size([4898]))If you want to transform the target tensor in a tensor of labels, you have two options, depending on the strategy or how you want to use the categorical data. One option is to treat a label as an integer vector of scores:# In[7]:target = wineq[:, -1].long()target# Out[7]:tensor([6, 6,  ..., 7, 6])If targets were string labels (such as wine color), assigning an integer number to each string would allow you to follow the same approach. The other approach is to build a one-hot encoding of the scores—that is, encode each of the ten scores in a vector of ten elements, with all elements set to zero but one, at a different index for each score. This way, a score of 1 could be mapped to the vector (1,0,0,0,0,0,0,0,0,0), a score of 5 to (0,0,0,0,1,0,0,0,0,0) and so on. (continued)Finally, categorical values have neither ordering nor numerical meaning. These values are often enumerations of possibilities, assigned arbitrary numbers. Assigning water to 1, coffee to 2, soda to 3, and milk to 4 is a good example. Placing water first and milk last has no real logic; you simply need distinct values to differentiate them. You could assign coffee to 10 and milk to –3 with no significant change (although assigning values in the range 0..N-1 will have advantages when we discuss one-hot encoding later).
Select all rows and all columns except the last.
Select all rows and the last column.45Tabular dataThe fact that the score corresponds to the index of the nonzero element is purely inci-dental; you could shuffle the assignment, and nothing would change from a classifica-tion standpoint. The two approaches have marked differences. Keeping wine-quality scores in an integer vector of scores induces an ordering of the scores, which may be appropriate in this case because a score of 1 is lower than a score of 4. It also induces some dis-tance between scores. (The distance between 1 a n d  3 i s  t h e  s a m e  a s  t h e  d i s t a n c e between 2 and 4, for example.) If this holds for your quantity, great. If, on the other hand, scores are purely qualitative, such as color, one-hot encoding is a much better fit, as no implied ordering or distance is involved. One-hot encoding is appropriate for quantitative scores when fractional values between integer scores (such as 2.4) make no sense for the application (when score is either this or that). You can achieve one-hot encoding by using the scatter_ method, which fills the tensor with values from a source tensor along the indices provided as arguments.# In[8]:target_onehot = torch.zeros(target.shape[0], 10)target_onehot.scatter_(1, target.unsqueeze(1), 1.0)# Out[8]:tensor([[0., 0.,  ..., 0., 0.],        [0., 0.,  ..., 0., 0.],        ...,        [0., 0.,  ..., 0., 0.],        [0., 0.,  ..., 0., 0.]])Now take a look at what scatter_ does. First, notice that its name ends with an under-score. This convention in PyTorch indicates that the method won’t return a new ten-sor but modify the tensor in place. The arguments for scatter_ areThe dimension along which the following two arguments are specifiedA column tensor indicating the indices of the elements to scatterA tensor containing the elements to scatter or a single scalar to scatter (1, in this case)In other words, the preceding invocation reads this way: “For each row, take the index of the target label (which coincides with the score in this case), and use it as the column index to set the value 1.0. The result is a tensor encoding categorical information.” The second argument of scatter_, the index tensor, is required to have the same number of dimensions as the tensor you scatter into. Because target_onehot has two dimensions (4898x10), you need to add an extra dummy dimension to target b y using unsqueeze:# In[9]:target_unsqueezed = target.unsqueeze(1)target_unsqueezed# Out[9]:46CHAPTER 3 Real-world data representation with tensorstensor([[6],        [6],        ...,        [7],        [6]])The call to unsqueeze adds a singleton dimension, from a 1D tensor of 4898 elements to a 2D tensor of size (4898x1), without changing its contents. No elements were added; you decided to use an extra index to access the elements. That is, you accessed the first element of target as target[0] and the first element of its unsqueezed coun-terpart as target_unsqueezed[0,0]. PyTorch allows you to use class indices directly as targets while training neural net-works. If you want to use the score as a categorical input to the network, however, you’d have to transform it to a one-hot encoded tensor. Now go back to your data tensor, containing the 11 variables associated with the chemical analysis. You can use the functions in the PyTorch Tensor API to manipulate your data in tensor form. First, obtain means and standard deviations for each column:# In[10]:data_mean = torch.mean(data, dim=0)data_mean# Out[10]:tensor([6.8548e+00, 2.7824e-01, 3.3419e-01, 6.3914e+00, 4.5772e-02, 3.5308e+01,        1.3836e+02, 9.9403e-01, 3.1883e+00, 4.8985e-01, 1.0514e+01])# In[11]:data_var = torch.var(data, dim=0)data_var# Out[11]:tensor([7.1211e-01, 1.0160e-02, 1.4646e-02, 2.5726e+01, 4.7733e-04, 2.8924e+02,        1.8061e+03, 8.9455e-06, 2.2801e-02, 1.3025e-02, 1.5144e+00])In this case, dim=0 indicates that the reduction is performed along dimension 0. At this point, you can normalize the data by subtracting the mean and dividing by the standard deviation, which helps with the learning process.# In[12]:data_normalized = (data - data_mean) / torch.sqrt(data_var)data_normalized# Out[12]:tensor([[ 1.7209e-01, -8.1764e-02,  ..., -3.4914e-01, -1.3930e+00],        [-6.5743e-01,  2.1587e-01,  ...,  1.3467e-03, -8.2418e-01],        ...,        [-1.6054e+00,  1.1666e-01,  ..., -9.6250e-01,  1.8574e+00],        [-1.0129e+00, -6.7703e-01,  ..., -1.4882e+00,  1.0448e+00]])Next, look at the data with an eye to finding an easy way to tell good and bad wines apart at a glance. First, use the torch.le function to determine which rows in targetcorrespond to a score less than or equal to 3:47Tabular data# In[13]:bad_indexes = torch.le(target, 3)bad_indexes.shape, bad_indexes.dtype, bad_indexes.sum()# Out[13]:(torch.Size([4898]), torch.uint8, tensor(20))Note that only 20 of the bad_indexes entries are set to 1! By leveraging a feature in PyTorch called advanced indexing, you can use a binary tensor to index the data tensor. This tensor essentially filters data to be only items (or rows) that correspond to 1 in the indexing tensor. The bad_indexes tensor has the same shape as target, with a value of 0 or 1 depending on the outcome of the comparison between your threshold and each element in the original target tensor:# In[14]:bad_data = data[bad_indexes]bad_data.shape# Out[14]:torch.Size([20, 11])Note that the new bad_data tensor has 20 rows, the same as the number of rows with a 1 in the bad_indexes tensor. It retains all 11 columns. Now you can start to get information about wines grouped into good, middling, and bad categories. Take the .mean() of each column:# In[15]:bad_data = data[torch.le(target, 3)]mid_data = data[torch.gt(target, 3) & torch.lt(target, 7)] good_data = data[torch.ge(target, 7)]bad_mean = torch.mean(bad_data, dim=0)mid_mean = torch.mean(mid_data, dim=0)good_mean = torch.mean(good_data, dim=0)for i, args in enumerate(zip(col_list, bad_mean, mid_mean, good_mean)):    print('{:2} {:20} {:6.2f} {:6.2f} {:6.2f}'.format(i, *args))# Out[15]: 0 fixed acidity          7.60   6.89   6.73 1 volatile acidity       0.33   0.28   0.27 2 citric acid            0.34   0.34   0.33 3 residual sugar         6.39   6.71   5.26 4 chlorides              0.05   0.05   0.04 5 free sulfur dioxide   53.33  35.42  34.55 6 total sulfur dioxide 170.60 141.83 125.25 7 density                0.99   0.99   0.99 8 pH                     3.19   3.18   3.22 9 sulphates              0.47   0.49   0.5010 alcohol               10.34  10.26  11.42It looks as though you’re on to something here. At first glance, the bad wines seem to have higher total sulfur dioxide, among other differences. You could use a threshold on total sulfur dioxide as a crude criterion for discriminating good wines from bad For numpy arrays and PyTorch tensors, the & operator does a logical and operation.48CHAPTER 3 Real-world data representation with tensorsones. Now get the indexes in which the total sulfur dioxide column is below the mid-point you calculated earlier, like so:# In[16]:total_sulfur_threshold = 141.83total_sulfur_data = data[:,6]predicted_indexes = torch.lt(total_sulfur_data, total_sulfur_threshold)predicted_indexes.shape, predicted_indexes.dtype, predicted_indexes.sum()# Out[16]:(torch.Size([4898]), torch.uint8, tensor(2727))Your threshold implies that slightly more than half of the wines are going to be high-quality. Next, you need to get the indexes of the good wines:# In[17]:actual_indexes = torch.gt(target, 5)actual_indexes.shape, actual_indexes.dtype, actual_indexes.sum()# Out[17]:(torch.Size([4898]), torch.uint8, tensor(3258))Because you have about 500 more good wines than your threshold predicted, you already have hard evidence that the threshold isn’t perfect. Now you need to see how well your predictions line up with the actual rankings. Perform a logical and b e t w e e n  y o u r  p r e d i c t i o n  i ndexes and the good indexes (remembering that each index is an array of 0s and 1s), and use that intersection of wines in agreement to determine how well you did:# In[18]:n_matches = torch.sum(actual_indexes & predicted_indexes).item()n_predicted = torch.sum(predicted_indexes).item()n_actual = torch.sum(actual_indexes).item()n_matches, n_matches / n_predicted, n_matches / n_actual# Out[18]:(2018, 0.74000733406674, 0.6193984039287906)You got around 2,000 wines right! Because you had 2,700 wines predicted, a 74 per-cent chance exists that if you predict a wine to be high-quality, it is. Unfortunately, you have 3,200 good wines and identified only 61 percent of them. Well, we guess you got what you signed up for; that result is barely better than random. This example is naïve, of course. You know for sure that multiple variables contrib-ute to wine quality and that the relationships between the values of these variables and the outcome (which could be the actual score rather than a binarized version of it) is likely to be more complicated than a simple threshold on a single value. Indeed, a simple neural network would overcome all these limitations, as would a lot of other basic machine learning methods. You’ll have the tools to tackle this problem after completing chapters 5 and 6, in which you build your first neural network from scratch.49Time series3.2 Time seriesIn the preceding section, we covered how to represent data organized in a flat table. As we noted, every row in the table was independent from the others; their order did not matter. Equivalently, no column encoded information on what rows came before and what rows came after. Going back to the wine data set, you could have had a Year column that allowed you to look at how wine quality evolved year over year. (Unfortunately, we don’t have such data at hand, but we’re working hard on collecting the data samples manually, bottle by bottle.) In the meantime, we’ll switch to another interesting data set: data from a Washing-ton, D.C., bike sharing system reporting the hourly count of rental bikes between 2011 and 2012 in the Capital bike-share system with the corresponding weather and seasonal information.1 The goal is to take a flat 2D data set and transform it into a 3D one, as shown in figure 3.2. In the source data, each row is a separate hour of data (Figure 3.2 shows a trans-posed version of this to better fit on the printed page). 
Figure 3.2 Transforming a 1D multichannel data set into a 2D multichannel data set by separating the date and hour of each sample into separate axesWe want to change the row-per-
1https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset50CHAPTER 3 Real-world data representation with tensorshour organization so that you have one axis that increases at a rate of one day per index increment and another axis that represents hour of day (independent of the date). The third axis is different columns of data (weather, temperature, and so on).Load the data, as shown in the following listing.# In[2]:bikes_numpy = np.loadtxt("../data/p1ch4/bike-sharing-data set/hour-fixed.csv",                         dtype=np.float32,                         delimiter=",",                         skiprows=1,                         converters={1: lambda x: float(x[8:10])}) bikes = torch.from_numpy(bikes_numpy)bikes# Out[2]:tensor([[1.0000e+00, 1.0000e+00,  ..., 1.3000e+01, 1.6000e+01],        [2.0000e+00, 1.0000e+00,  ..., 3.2000e+01, 4.0000e+01],        ...,        [1.7378e+04, 3.1000e+01,  ..., 4.8000e+01, 6.1000e+01],        [1.7379e+04, 3.1000e+01,  ..., 3.7000e+01, 4.9000e+01]])For every hour, the data set reports the following variables:instant      # index of recordday          # day of monthseason       # season (1: spring, 2: summer, 3: fall, 4: winter)yr           # year (0: 2011, 1: 2012)mnth         # month (1 to 12)hr           # hour (0 to 23)holiday      # holiday statusweekday      # day of the weekworkingday   # working day statusweathersit   # weather situation             # (1: clear, 2:mist, 3: light rain/snow, 4: heavy rain/snow)temp         # temperature in Catemp        # perceived temperature in Chum          # humiditywindspeed    # windspeedcasual       # number of causal usersregistered   # number of registered userscnt          # count of rental bikesIn a time-series data set such as this one, r o w s  r e p r e s e n t  s u c c essive time points: a dimension along which they’re ordered. Sure, you could treat each row as indepen-dent and try to predict the number of circulating bikes based on, say, a particular time of day regardless of what happened earlier. This existence of an ordering, however, gives you the opportunity to exploit causal relationships across time. You can predict bike rides at one time based on the fact that it was raining at an earlier time, for example. For the time being, you’re going to focus Listing 3.2 code/p1ch4/2_time_series_bikes.ipynb
Convert date strings to numbers corresponding to the day of the month in column 1.51Time serieson learning how to turn your bike-sharing data set into something that your neural network can ingest in fixed-size chunks. T h i s  n e u r a l  n e t w o r k  m o d e l  needs to see sequences of values for each quantity, such as ride count, time of day, temperature, and weather conditions, so N parallel sequences of size C. C stands for channel, in neural network parlance, and is the same as column for 1D data like you have here. The N dimension represents the time axis—here, one entry per hour. You may want to break up the 2-year data set in wider observation periods, such as days. This way, you’ll have N (for number of samples) collections of C sequences of length L. In other words, your time-series data set is a tensor of dimension 3 and shape N x C x L. The C remains your 17 channels, and L would be 24, one per hour of the day. There’s no particular reason why we must use chunks of 24 hours, though the general daily rhythm is likely to give us patterns we can exploit for predictions. We could instead use 7*24=168 hour blocks to chunk by week instead, if we desired. N o w  g o  b a c k  t o  y o u r  b i k e - s h a r i n g  d a t a  s e t .  T h e  f i r s t  c o l u m n  i s  t h e  i n d e x  ( t h e global ordering of the data); the second is the date; the sixth is the time of day. You have everything you need to create a data set of daily sequences of ride counts and other exogenous variables. Your data set is already sorted, but if it weren’t, you could use torch.sort on it to order it appropriately.NOTE T h e  v e r s i o n  o f  t h e  file you’re using here, hour-fixed.csv, has had some processing done to include rows t h a t  w e r e  m i s s i n g  f r o m  t h e  o r i g i n a l data set. We presumed that the missing hours had zero bikes active (typically the early-morning hours).All you have to do to obtain your daily hours data set is view the same tensor in batches of 24 hours. Take a look at the shape and strides of your bikes tensor:# In[3]:bikes.shape, bikes.stride()# Out[3]:(torch.Size([17520, 17]), (17, 1))That’s 17,520 hours, 17 columns. Now reshape the data to have three axes (day, hour, and then your 17 columns):# In[4]:daily_bikes = bikes.view(-1, 24, bikes.shape[1])daily_bikes.shape, daily_bikes.stride()# Out[4]:(torch.Size([730, 24, 17]), (408, 17, 1))What happened here? First, the bikes.shape[1] is 17, which is the number of columns in the bikes tensor. But the real crux of the code is the call to view, which is important: it changes the way that the tensor looks at the same data as contained in storage. Calling view on a tensor returns a new tensor that changes the number of dimen-sions and the striding information without changing the storage. As a result, you can 52CHAPTER 3 Real-world data representation with tensorsrearrange your tensor at zero cost because no data is copied at all. Your call to viewrequires you to provide the new shape for the returned tensor. Use the -1 as a place-holder for “however many indexes are left, given the other dimensions and the origi-nal number of elements.”  Remember that Storage is a contiguous, linear container for numbers—floating-point, in this case. Your bikes tensor has rows stored one after the other in corre-sponding storage, as confirmed by the output from the call to bikes.stride() earlier. For daily_bikes, stride is telling you that advancing by 1 along the hour dimen-sion (the second) requires you to advance by 17 places in the storage (or one set of columns), whereas advancing along the day dimension (the first) requires you to advance by a number of elements equal to the length of a row in the storage times 24 (here, 408, which is 17 * 24). The rightmost dimension is the number of columns in the original data set. In the middle dimension, you have time split into chunks of 24 sequential hours. In other words, you now have N sequences of L hours in a day for C channels. To get to your desired NxCxL ordering, you need to transpose the tensor:# In[5]:daily_bikes = daily_bikes.transpose(1, 2)daily_bikes.shape, daily_bikes.stride()# Out[5]:(torch.Size([730, 17, 24]), (408, 1, 17))We mentioned earlier that the weather-situation variable is ordinal. In fact, it has 4 lev-els: 1 for the best weather and 4 for the worst. You could treat this variable as categori-cal, with levels interpreted as labels, or continuous. If you choose categorical, you turn the variable into a one-hot encoded vector and concatenate the columns with the data set. To make rendering your data easier, limit yourself to the first day for now. First, initialize a zero-filled matrix with a number of rows equal to the number of hours in the day and a number of columns equal to the number of weather levels:# In[6]:first_day = bikes[:24].long()weather_onehot = torch.zeros(first_day.shape[0], 4)first_day[:,9]# Out[6]:tensor([1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2])Then scatter ones into our matrix according to the corresponding level at each row. Remember the use of unsqueeze to add a singleton dimension earlier:# In[7]:weather_onehot.scatter_(    dim=1,    index=first_day[:,9].unsqueeze(1) - 1,     value=1.0)# Out[7]:You’re decreasing the values by 1 because the weather situation ranges from 1 to 4, whereas indices are 0-based.53Time seriestensor([[1., 0., 0., 0.],        [1., 0., 0., 0.],        ...,        [0., 1., 0., 0.],        [0., 1., 0., 0.]])The day started with weather 1 and ended with 2, so that seems right. L a s t ,  c o n c a t e n a t e  y o u r  m a t r i x  t o  your original data set, using the cat f u n c t i o n . Look at the first of your results:# In[8]:torch.cat((bikes[:24], weather_onehot), 1)[:1]# Out[8]:tensor([[ 1.0000,  1.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  6.0000,          0.0000,  1.0000,  0.2400,  0.2879,  0.8100,  0.0000,  3.0000, 13.0000,         16.0000,  1.0000,  0.0000,  0.0000,  0.0000]])Here, you prescribed your original bikes data set and your one-hot encoded weather-situation matrix to be concatenated along the column dimension (such as 1). In other words, the columns of the two data sets are stacked together, or the new one-hot encoded columns are appended to the original data set. For cat to succeed, the ten-sors must have the same size along the other dimensions (the row dimension, in this case). Note that your new last four columns are 1, 0, 0, 0—exactly what you’d expect with a weather value of 1. Y o u  c o u l d  h a v e  d o n e  t h e  s a m e  t h i n g  w i t h  t h e  r e s h a p e d  daily_bikes t e n s o r . Remember that it’s shaped (B, C, L), where L = 24. First, create the zero tensor, with the same B and L but with the number of additional columns as C:# In[9]:daily_weather_onehot = torch.zeros(daily_bikes.shape[0], 4, daily_bikes.shape[2])daily_weather_onehot.shape# Out[9]:torch.Size([730, 4, 24])Then scatter the one-hot encoding into the tensor in the C dimension. Because opera-tion is performed in place, only the content of the tensor changes:# In[10]:daily_weather_onehot.scatter_(1, daily_bikes[:,9,:].long().unsqueeze(1) - 1, 1.0)daily_weather_onehot.shape# Out[10]:torch.Size([730, 4, 24])Concatenate along the C dimension:# In[11]:daily_bikes = torch.cat((daily_bikes, daily_weather_onehot), dim=1)54CHAPTER 3 Real-world data representation with tensorsWe mentioned earlier that this method isn’t the only way to treat the weather-situation variable. Indeed, its labels have an ordinal relationship, so you could pretend that they’re special values of a continuous variable. You might transform the variable so that it runs from 0.0 to 1.0:# In[12]:daily_bikes[:, 9, :] = (daily_bikes[:, 9, :] - 1.0) / 3.0As we mention in section 4.1, rescaling variables to the [0.0, 1.0] interval or the [-1.0, 1.0] interval is something that you’ll want to do for all quantitative variables, such as temperature (column 10 in your data set). You’ll see why later; for now, we’ll say that it’s beneficial to the training process. You have multiple possibilities for rescaling variables. You can map their range to [0.0, 1.0]# In[13]:temp = daily_bikes[:, 10, :]temp_min = torch.min(temp)temp_max = torch.max(temp)daily_bikes[:, 10, :] = (daily_bikes[:, 10, :] - temp_min) / (temp_max - temp_min)or subtract the mean and divide by the standard deviation:# In[14]:temp = daily_bikes[:, 10, :]daily_bikes[:, 10, :] = (daily_bikes[:, 10, :] - torch.mean(temp)) / torch.std(temp)In this latter case, the variable has zero mean and unitary standard deviation. If the variable were drawn from a Gaussian distribution, 68 percent of the samples would sit in the [-1.0, 1.0] interval.  Great—you’ve built another nice data set that you’ll get to use later. For now, it’s important only that you got an idea of how a time series is laid out and how you can wrangle the data into a form that a network will digest. Other kinds of data look like a time series, in that strict ordering exists. The top two in that category are text and audio. 3.3 TextDeep learning has taken the field of natural language processing (NLP) by storm, par-ticularly by using models that repeatedly consume a combination of new input and previous model output. These models are called recurrent neural networks, and they’ve been applied with great success to text categorization, text generation, and automated translation systems. Previous NLP workloads were characterized by sophisticated mul-tistage pipelines that included rules encoding the grammar of a language.2 3 N o w , 2Nadkarni et al., “Natural language processing: an introduction”. JAMIA https://www.ncbi.nlm.nih.gov/pmc/arti-cles/PMC31683283Wikipedia entry for natural language processing: https://en.wikipedia.org/wiki/Natural-language_processing55Textstate-of-the-art work trains n e t w o r k s  e n d  t o  e n d  o n  l a r g e  c o r p u s e s  s t a r t i n g  f r o m scratch, letting those rules emerge from data. For the past several years, the most-used automated translation systems available as services on the internet have been based on deep learning. Your goal in this chapter is to turn text into something that a neural network can process, which, like the previous cases, is a tensor of numbers. If you can do that and later choose the right architecture for your text processing job, you’ll be in the posi-tion of doing NLP with PyTorch. You see right away how powerful this capability is: you can achieve state-of-the-art performance on tasks in different domains with the same PyTorch tools if you cast your problem in the right form. The first part of this job is reshaping data. Networks operate on text at two levels: at character level, by processing one charac-ter at a time, and at word level, in which individual words are the finest-grained enti-ties seen by the network. The technique you u s e  t o  e n c o d e  t ext information into tensor form is the same whether you operate at character level or at word level. This technique is nothing magic; you stumbled upon it earlier. It’s one-hot encoding. Start with a character-level example. First, get some text to process. An amazing resource is Project Gutenberg4, a volunteer effort that digitizes and archives cultural work and makes it available for free in open formats, including plain-text files. If you’re aiming at larger-scale corpora, the Wikipedia corpus stands out: it’s the complete collection of Wikipedia articles containing 1.9 billion words and more than 4.4 million articles. You can find several other corpora at the English Corpora website.5 Load Jane Austen’s Pride and Prejudice from the Project Gutenberg website.6 Save the file and read it in, as shown in the following listing.# In[2]:with open('../data/p1ch4/jane-austin/1342-0.txt', encoding='utf8') as f:    text = f.read()You need to take care of one more detail before you proceed: encoding. Encoding is a vast subject, so all we’ll do now is touch on it. Every written character is represented by a code, a sequence of bits of appropriate length that allow each character to be uniquely identified. The simplest such encoding is ASCII (American Standard Code for Information Interchange), dating back to the 1960s. ASCII encodes 128 characters using 128 integers. Letter a, for example, corresponds to binary 1100001 or decimal 97; letter b corresponds to binary 1100010 or decimal 98, and so on. The encoding would fit 8 bits, which was a big bonus in 1965.4http://www.gutenberg.org5https://www.english-corpora.org6http://www.gutenberg.org/files/1342/1342-0.txtListing 3.3 code/p1ch4/3_text_jane_austin.ipynb56CHAPTER 3 Real-world data representation with tensorsNOTE Clearly, 128 characters aren’t enough to account for all the glyphs, accents, ligatures, and other features that are needed to properly represent written text in languages other than English. To this end, other encodings have been developed, using a larger number of bits as a code for a wider range of characters. That wider range of characters got standardized as Uni-code, which maps all known characters to numbers, with the representation in bits of those numbers being provided by a specific encoding. Popular encod-ings include UTF-8, UTF-16 and UTF-32, in which the numbers are a sequence of 8-, 16-, or 32-bit integers. Strings in Python 3.x are Unicode strings.You’re going to one-hot encode your characters to limit the one-hot encoding to a character set that’s useful for the text being analyzed. In this case, because you loaded text in English, it’s quite safe to use ASCII and deal with a small encoding. You could also make all characters lowercase to reduce the number of characters in your encod-ing. Similarly, you could screen out punctuation, numbers, and other characters that aren’t relevant to the expected kinds of text, which may or may not make a practical difference to your neural network, depending on the task at hand. At this point, you need to parse the characters in the text and provide a one-hot encod-ing for each of them. Each character will be represented by a vector of length equal to the number of characters in the encoding. This vector will contain all zeros except for a 1 at the index corresponding to the location of the character in the encoding. First, split your text into a list of lines and pick an arbitrary line to focus on:# In[3]:lines = text.split('\n')line = lines[200]line# Out[3]:'“Impossible, Mr. Bennet, impossible, when I am not acquainted with him'Create a tensor that can hold the total number of one-hot encoded characters for the whole line:# In[4]:letter_tensor = torch.zeros(len(line), 128) letter_tensor.shape# Out[4]:torch.Size([70, 128])Note that letter_tensor holds a one-hot encoded character per row. Now set a 1 on each row in the right position so that each row represents the right character. The index where the 1 has to be set corresponds to the index of the character in the encoding:# In[5]:for i, letter in enumerate(line.lower().strip()):    letter_index = ord(letter) if ord(letter) < 128 else 0      letter_tensor[i][letter_index] = 1128 hardcoded due to the limits of ASCII\
The text uses directional double quotes,   which aren’t valid ASCII, so screen them out here.  57TextYou’ve one-hot encoded your sentence into a representation that a neural network can digest. You could do word-level encoding the same way by establishing a vocabu-lary and one-hot encoding sentences, sequences of words, along the rows of your ten-sor. Because a vocabulary contains many words, this method produces wide encoded vectors that may not be practical. Later in this chapter, you see a more efficient way to represent text at word level by using embeddings. For now, stick with one-hot encod-ings to see what happens. Define clean_words, which takes text and returns it lowercase and stripped of punc-tuation. When you call it on your “Impossible, Mr. Bennet” line, you get the following:# In[6]:def clean_words(input_str):    punctuation = '.,;:"!?”“_-'    word_list = input_str.lower().replace('\n',' ').split()    word_list = [word.strip(punctuation) for word in word_list]    return word_listwords_in_line = clean_words(line)line, words_in_line# Out[6]:('“Impossible, Mr. Bennet, impossible, when I am not acquainted with him', ['impossible',  'mr',  'bennet',  'impossible',  'when',  'i',  'am',  'not',  'acquainted',  'with',  'him'])Next, build a mapping of words to indexes in your encoding:# In[7]:word_list = sorted(set(clean_words(text)))word2index_dict = {word: i for (i, word) in enumerate(word_list)}len(word2index_dict), word2index_dict['impossible']# Out[7]:(7261, 3394)Note that all_words i s  n o w  a  d i c t i o n a r y  w i t h  w o r d s  as keys and an integer as value. You’ll use this dictionary to efficiently find the index of a word as you one-hot encode it. Now focus on your sentence. Break it into words and one-hot encode it—that is, populate a tensor with one one-hot encoded vector per word. Create an empty vector, and assign the one-hot encoded values of the word in the sentence:# In[8]:word_tensor = torch.zeros(len(words_in_line), len(word2index_dict))for i, word in enumerate(words_in_line):58CHAPTER 3 Real-world data representation with tensors    word_index = word2index_dict[word]    word_tensor[i][word_index] = 1    print('{:2} {:4} {}'.format(i, word_index, word))print(word_tensor.shape)# Out[8]: 0 3394 impossible 1 4305 mr 2  813 bennet 3 3394 impossible 4 7078 when 5 3315 i 6  415 am 7 4436 not 8  239 acquainted 9 7148 with10 3215 himtorch.Size([11, 7261])At this point, tensor represents one sentence of length 11 in an encoding space of size 7261—the number of words in your dictionary. 3.3.1 Text embeddingsOne-hot encoding is a useful technique for representing categorical data in tensors. As you may have anticipated, however, one-hot encoding starts to break down when the number of items to encode is effectively unbound, as with words in a corpus. In one book, you had more than 7,000 items! You certainly could do some work to deduplicate words, condense alternative spell-ings, collapse past and future tenses into a single token, and that kind of thing. Still, a general-purpose English-language encoding is going to be huge. Worse, every time you encounter a new word, you have to add a new column to the vector, which means add-ing a new set of weights to the model to account for that new vocabulary entry, which is going to be painful from a training perspective. How can you compress your encoding to a more manageable size and put a cap on the size growth? Well, instead of using vectors of many zeros and a single 1, you could use vectors of floating-point numbers. A vector of, say, 100 floating-point numbers can indeed represent a large number of words. The trick is to find an effective way to map individual words to this 100-dimensional space in a way that facilitates downstream learning. This technique is called embedding. In principle, you could iterate over your vocabulary and generate a set of 100 ran-dom floating-point numbers for each word. This method would work, in that you could cram a large vocabulary into 100 numbers, but it would forgo any concept of distance between words based on meaning or context. A model that used this word embedding would have to deal with little structure in its input vectors. An ideal solu-tion would be to generate the embedding in such a way that words used in similar con-texts map to nearby regions of the embedding.59Text If you were to design a solution to this problem by hand, you might decide to build your embedding space by mapping basic nouns and adjectives along the axes. You can generate a 2D space in which axes map to nouns "fruit" (0.0–0.33), "flower" (0.33–0.66), and "dog" (0.66–1.0), and to adjectives "red" (0.0–0.2), "orange" (0.2–0.4), "yellow" (0.4–0.6), "white" (0.6–0.8), and "brown" (0.8–1.0). Your goal now is to take actual fruit, flowers, and dogs and lay them out in the embedding. As you start embedding words, you can map "apple" to a number in the "fruit"and "red" quadrant. Likewise, you can easily map "tangerine", "lemon", "lychee", and "kiwi" (to round out your list of colorful fruits). Then you can start on flowers, assigning "rose", "poppy", "daffodil", "lily", and . . . well, there aren’t many brown flowers out there. Well, "sunflower" c a n  g e t  "flower", "yellow", and "brown", and "daisy" can get "flower"  "white", and "yellow". Perhaps you should update "kiwi" to map close to "fruit", "brown", and "green". For dogs and color, you can embed "redbone", "fox" perhaps for "orange", "golden retriever", "poo-dles" for "white", and . . . most kinds of dogs are "brown". Although doing this mapping manually isn’t feasible for a large corpus, you should note that although you had an embedding size of 2, you described 15 different words besides the base 8 and probably could cram quite a few more in if you take the time to be creative. As you’ve probably guessed, this kind of work can be automated. By processing a large corpus of organic text, you can generate embeddings similar to this one. The main differences are that the embedding vector has 100 to 1,000 elements and that axes don’t map directly to concepts, but conceptually similar words map to neigh-boring regions of an embedding space whose axes are arbitrary floating-point dimensions. Although the exact algorithms7 used are a bit out of scope for what we wanting to focus on here, we’d like to mention that embeddings are often generated by using neural networks, trying to predict a word from nearby words (the context) in a sen-tence. In this case, you could start from one-hot encoded words and use a (usually rather shallow) neural network to generate the embedding. When the embedding is available, you could use it for downstream tasks. One interesting aspect of the resulting embeddings is that similar words end up not only clustered together, but also with consistent spatial relationships with other words. If you were to take the embedding vector for "apple" and begin to add and subtract the vectors for other words, you could begin to perform analogies such as apple - red - sweet + yellow + sour and end up with a vector similar to the one for "lemon". We won’t be using text embeddings here, but they’re essential tools when a large number of entries in a set has to be represented with numeric vectors. 7One example is https://en.wikipedia.org/wiki/Word2vec60CHAPTER 3 Real-world data representation with tensors3.4 ImagesThe introduction of convolutional neural networks revolutionized computer vision8, and image-based systems have since acquired a new set of capabilities. Problems that required complex pipelines of highly tuned algorithmic building blocks became solv-able at unprecedented levels of performance by training end-to-end networks with paired input-and-desired-output examples. To participate in this revolution, you need to be able to load images from common image formats and then transform the data into a tensor representation that has the various parts of the image arranged in the way that PyTorch expects. An image is represented as a collection of scalars arranged in a regular grid, hav-ing a height and a width (in pixels). You might have a single scalar per grid point (the pixel), which would be represented as a grayscale image, or multiple scalars per grid point, which typically represent different colors or different features, such as depth from a depth camera. Scalars representing values at individual pixels are often encoded with 8-bit inte-gers, as in consumer cameras, for example. In medical, scientific, and industrial appli-cations, you not infrequently find pixels with higher numerical precision, such as 12-bit and 16-bit. This precision provides a wider range or increased sensitivity in cases in which the pixel encodes information on a physical property, such a s  b o n e  d e n s i t y , temperature, or depth. Y o u  h a v e  s e v e r a l  w a y s  o f  e n c o d i n g  n u m b e r s  i n t o  c o l o r s .9 T h e  m o s t  c o m m o n  i s RGB, which defines a color with three numbers that represent the intensity of red, green and blue. You can think of a color channel as being a grayscale intensity map of only the color in question, similar to what you’d see if you looked at the scene in ques-tion through a pair of pure-red sunglasses. Figure 3.3 shows a rainbow in which each of the RGB channels captures a certain portion of the spectrum. (The figure is simpli-fied, in that it elides things. The orange and yellow bands, for example, are repre-sented as a combination of red and green.)  Images come in several file formats, but luckily, you have plenty of ways to load images in Python. Start by loading a PNG image with the imageio module. You’ll use imageio throughout the chapter because it handles different data types with a uni-form API. Now load an image, as in the following listing.# In[2]:import imageioimg_arr = imageio.imread('../data/p1ch4/image-dog/bobby.jpg')img_arr.shape# Out[2]:(720, 1280, 3)8https://en.wikipedia.org/wiki/Convolutional_neural_network#History9Something of an understatement: https://en.wikipedia.org/wiki/Color_modelListing 3.4 code/p1ch4/5_image_dog.ipynbFigure 3.3 A rainbow broken into red, green, and blue channels61Images
At this point, img i s  a  N u m P y  a r r a y - l i k e  o b j e c t  w i t h  t h r e e  d i m e n s i o n s :  t w o  s p a t i a l dimensions (width and height) and a third dimension corresponding to the channels red, green, and blue. Any library that outputs a NumPy array does so to obtain a PyTorch tensor. The only thing to watch out for is the layout of dimensions. PyTorch modules that deal with image data require tensors to be laid out as C x H x W (chan-nels, height, and width, respectively). You can use the transpose function to get to an appropriate layout. Given an input tensor W x H x C, you get to a proper layout by swapping the first and last channels:# In[3]:img = torch.from_numpy(img_arr)out = torch.transpose(img, 0, 2)You’ve seen this example before, but note that this operation doesn’t make a copy of the tensor data. Instead, out uses the same underlying storage as img and plays with the size and stride information at the tensor level. This arrangement is convenient because the operation is cheap, but (heads up) changing a pixel in img leads to a change in out. Also note that other deep learning frameworks use different layouts. Originally, TensorFlow kept the channel dimension last, resulting in a H x W x C layout. (Now it supports multiple layouts.) This strategy has pros and cons from a low-level perfor-mance standpoint, but it doesn’t make a difference to you as long as you reshape your tensors properly. So far, you’ve described a single image. Following the same strategy that you used for earlier data types, to create a data set of multiple images to use as an input for your neural networks, you store the images in a batch along the first dimension to obtain a N x C x H x W tensor. As a more efficient alternative to using stack to build up the tensor, you can preal-locate a tensor of appropriate size and fill it with images loaded from a directory,62CHAPTER 3 Real-world data representation with tensors# In[4]:batch_size = 100batch = torch.zeros(100, 3, 256, 256, dtype=torch.uint8)which indicates that your batch will consist of 100 RGB images 256 pixels in height and 256 pixels in width. Notice the type of the tensor: you’re expecting each color to be represented as a 8-bit integer, as in most photographic formats from standard con-sumer cameras. Now you can load all png images from an input directory and store them in the tensor:# In[5]:import osdata_dir = '../data/p1ch4/image-cats/'filenames = [name for name in os.listdir(data_dir) if os.path.splitext(name) == '.png']for i, filename in enumerate(filenames):    img_arr = imageio.imread(filename)    batch[i] = torch.transpose(torch.from_numpy(img_arr), 0, 2)As we mentioned earlier, neural networks usually work with floating-point tensors as their input. As you’ll also see in upcoming chapters, neural networks exhibit the best training performance when input data ranges from roughly 0 to 1 or –1 to 1 (an effect of how their building blocks are defined). A typical thing that you’ll want to do is cast a tensor to floating-point and normal-ize the values of the pixels. Casting to floating-point is easy, but normalization is trick-ier, as it depends on what range of the input you decide should lie between 0 and 1(or –1 and 1). One possibility is to divide the values of pixels by 255 (the maximum representable number in 8-bit unsigned):# In[6]:batch = batch.float()batch /= 255.0Another possibility is to compute mean and standard deviation of the input data and scale it so that the output has zero mean a n d  u n i t  s t a n d a r d  d e v i a t i o n  a c r o s s  e a c h channel:# In[7]:n_channels = batch.shape[1]for c in range(n_channels):    mean = torch.mean(batch[:, c])    std = torch.std(batch[:, c])    batch[:, c] = (batch[:, c] - mean) / stdYou can perform several other operations on inputs, including geometric transfor-mations such as rotation, scaling, and cropping. These operations may help with training or may be required to make an arbitrary input conform to the input requirements of a network, such as the size of the image. You’ll stumble onto quite a few of these strategies. For now, just remember that you have image manipulation options available. 63Volumetric data3.5 Volumetric dataYou’ve learned how to load and represent 2D images, like the ones you take with your camera. In contexts such as medical imaging applications involving, say, CT (Com-puted Tomography) scans, you typically deal with sequences of images stacked along the head-to-feet direction, each corresponding to a slice across the body. In CT scans, the intensity represents the density of the different parts of the body: lungs, fat, water, muscle, bone, in order of increasing density, mapped from dark to bright when CT scans are displayed on clinical workstations. The density at each point is computed from the amount of x-ray reaching a detector after passing through the body, with some complex math used to deconvolve the raw sensor data into the full volume. CTs have a single intensity channel, similar to a grayscale image. Often, in native data formats, the channel dimension is left out, so the raw data typically has three dimensions. By stacking individual 2D slices into a 3D tensor, you can build volumetric data representing the 3D anatomy of a subject. Unlike figure 3.3, the extra dimension in figure 3.4 represents an offset in physical space rather than a particular band of the visible spectrum.
Figure 3.4 Slices of a CT scan, from the top of the head to the jawline We won’t go into detail here on medical imaging data formats. For now, it suffices to say that no fundamental difference exists between a tensor that stores volumetric data and one that stores image data. You have an extra dimension, depth, after the channel dimension, leading to a 5D tensor of shape N x C x D x H x W. Load a sample CT scan by using the volread function in the imageio module, which takes a directory as argument and assembles all DICOM (Digital Imaging Communica-tion and Storage) files10 in a series in a NumPy 3D array, as shown in the following listing.10https://wiki.cancerimagingarchive.net/display/Public/CPTAC-LSCC#dd4a08a246524596add33b9f8f00f28864CHAPTER 3 Real-world data representation with tensors# In[2]:import imageiodir_path = "../data/p1ch4/volumetric-dicom/2-LUNG 3.0  B70f-04083"vol_arr = imageio.volread(dir_path, 'DICOM')vol_arr.shape# Out[2]:Reading DICOM (examining files): 1/99 files (1.0%99/99 files (100.0%)  Found 1 correct series.Reading DICOM (loading data): 87/99  (87.999/99  (100.0%)# Out[2]:(99, 512, 512)Also in this case, the layout is different from what PyTorch expects, due to the lack of channel information. You’ll have to make room for the channel dimension by using unsqueeze:# In[3]:vol = torch.from_numpy(vol_arr).float()vol = torch.transpose(vol, 0, 2)vol = torch.unsqueeze(vol, 0)vol.shape# Out[3]:torch.Size([1, 512, 512, 99])At this point, you could assemble a 5D data set by stacking multiple volumes along the batch direction, as you did earlier in the chapter.ConclusionYou covered a lot of ground in this chapter. You learned to load the most common types of data and shape them up for consumption by a neural network. There are more data formats in the wild than we could hope to describe in a single volume, of course. Some, like medical histories, are too complex to cover in this volume. For the interested reader, however, we do provide short examples of audio and video tensor creation in bonus Jupyter notebooks in our code repository11.ExercisesTake several pictures of red, blue, and green items with your phone or other digital camera.12–L o a d  e a c h  i m a g e ,  a n d  c o n v e r t  i t  t o  a  t e n s o r .– For each image tensor, use the .mean() method to get a sense of how bright the image is.Listing 3.5 code/p1ch4/6_volumetric_ct.ipynb
11https://github.com/deep-learning-with-pytorch/dlwpt-code/tree/master/p1ch412Or download some from the internet if a camera isn’t available.65Volumetric data–N o w  t a k e  t h e  m e a n  o f  e a c h  c h a n n e l  o f  y o u r  i m a g e s .  C a n  y o u  i d e n t i f y  t h e  r e d , green, and blue items from only the channel averages?Select a relatively large file containing Python source code.–B u i l d  a n  i n d e x  o f  a l l  t h e  w o r d s  i n  t he source file. (Feel free to make your tokenization as simple or as complex as you like; we suggest starting by replacing r"[^a-zA-Z0-9_]+" with spaces.)–C o m p a r e  y o u r  i n d e x  w i t h  t h e  o n e  y o u  m a d e  f o r  Pride and Prejudice. Which is larger?– Create the one-hot encoding for the source code file.–W h a t  i n f o r m a t i o n  i s  l o s t  w i t h  t h i s  e n c o d i n g ?  H o w  d o e s  t h a t  i n f o r m a t i o n compare with what’s lost in the Pride and Prejudice encoding?SummaryNeural networks require data to be represented as multidimensional numerical tensors, often 32-bit floating-point.Thanks to how the PyTorch libraries interact with the Python standard library and surrounding ecosystem, loading the most common types of data and con-verting them to PyTorch tensors is convenient.In general, PyTorch expects data to be laid out along specific dimensions, according to the model architecture (such as convolutional versus recurrent). Data reshaping can be achieved effectively with the PyTorch tensor API.Spreadsheets can be straightforward to convert to tensors. Categorical- and ordi-nal-valued columns should be handled differently from interval-valued columns.Text or categorical data can be encoded to a one-hot representation through the use of dictionaries.Images can have one or many channels. The most common are the red, green, and blue channels of typical digital photos.Single-channel data formats sometimes omit an explicit channel dimension.Volumetric data is similar to 2D image data, with the exception of adding a third dimension: depth.Many images have a per-channel bit depth of 8, though 12 and 16 bits per chan-nel are not uncommon. These bit-depths can be stored in a 32-bit floating-point number without loss of precision.67The mechanics of learning
With the blooming of machine learning that has occurred over the past decade, the notion of machines that learn from experience has become a mainstream theme in both technical and journalistic circles. Now, how is it exactly that a machine learns? What are the mechanics of it, or the algorithm behind it? From the point of view of an outer observer, a learning algorithm is presented input data that is paired with desired outputs. When learning has occurred, that algorithm is capa-ble of producing correct outputs when it’s fed new data that is similar enough to the input data on which it was trained. With deep learning, this process works even when the input data and the desired output are far from each other—when they come from different domains, such as an image and a sentence describing it. As a matter of fact, models that allow you to explain input/output relationships date back centuries. When Johannes Kepler, a German mathematical astronomer This chapter coversUnderstanding how algorithms can learn from dataReframing learning as parameter estimation, using differentiation and gradient descentWalking through a simple learning algorithm from scratchSeeing how PyTorch supports learning with autograd68CHAPTER 4 The mechanics of learningwho lived between 1571 and 1630, figured out his three laws of planetary motion in the early 1600s, he based them on data collected by his mentor, Tycho Brahe, during naked-eye observations (yep, naked eye and a piece of paper). Not having Newton’s Law of gravitation at his disposal (in fact, Newton used Kepler’s work to figure things out), he extrapolated the simplest possible geometric model that could fit the data. By the way, it took him six years of staring at data that didn’t make sense to him, as well as incremental realizations, to formulate these laws.1 You can see this process in figure 4.1. The first law reads: “The orbit of every planet is an ellipse with the Sun at one of the two foci.” He didn’t know what caused orbits to be ellipses, but given a set of obser-vations for a planet (or a moon of a large planet, such as Jupiter), he could at that point estimate the shape (the eccentricity) and size (the semi-latus rectum) of the ellipse. With those two parameters computed from the data, he could tell where the planet could possibly be during its journey in the sky. When he figured out the second law—“A line joining a planet and the Sun sweeps out equal areas during equal intervals of time—he could also tell when a planet would be at a particular point in space given observations in time.2
Figure 4.1 Johannes Kepler considers multiple candidate models that might fit the data at hand, settling on an ellipse.
1As recounted by Michael Fowler: http://galileoandeinstein.physics.virginia.edu/1995/lectures/morekepl.html2Understanding the details of Kepler’s laws isn’t needed for understanding the chapter, but you can find more information at https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion.69 How would Kepler estimate the eccentricity and size of the ellipse without comput-ers, pocket calculators or even calculus, none of which had been invented yet? You learn the answer from Kepler’s own recollection in his book New Astronomy or from how J.V. Field put it in his The Origins of Proof series:3Essentially, Kepler had to try different shapes, using a certain number of observations to find the curve, then use the curve to find some more positions, for times when he had observations available, and then check whether these calculated positions agreed with the observed ones. To sum things up, over those six years, Kepler1Got lots of good data from his friend Brahe (not without some struggle).2Tried to visualize the heck out of that data because he felt that something fishy was going on.3Chose the simplest possible model that had a chance to fit the data (an ellipse).4Split the data so that he could work on part of it and keep an independent set for validation.5Started with a tentative eccentricity and size, and iterated until the model fit the observations.6Validated his model on the independent observations.7Looked back in disbelief.There’s a data-science handbook for you, all the way from 1609. The history of science is constructed on these seven steps, and as scientists have learned over the centuries, deviating from them is a recipe for disaster.4 T h e s e  s t e p s  a r e  e x a c t l y  w h a t  y o u ’ l l  f o l l o w  t o  learn s o m e t h i n g  f r o m  d a t a .  H e r e , there’s virtually no difference between saying that you’ll fit the data and saying that you’ll make an algorithm learn from data. The process always involves a function with unknown parameters whose values are estimated from data—in short, a model. Y o u  c a n  a r g u e  t h a t  learning from data p r e s u m e s  t h a t  t h e  u n d e r l y i n g  m o d e l  i s n ’ t engineered to solve a specific problem (as was the ellipse in Kepler’s work) and is capable of approximating a much wider family of functions. A neural network would have predicted Tycho Brahe’s trajectories without requiring Kepler’s flash of insight to try fitting the data to an ellipse. Sir Isaac Newton, however, would have had a much harder time deriving his laws of gravitation from a generic model. You’re interested here in the latter kinds of models: one that aren’t engineered to solve specific narrow tasks and can be adapted automatically to specialize in solving many similar tasks, using input and output pairs—in other words, general models trained on data relevant to the task at hand. In particular, PyTorch is designed to make it easy to create models for which the derivatives of the fitting error, with respect to the parameters, can be expressed analytically. Don’t worry if the last sentence didn’t make sense; section 1.1 should clear it up for you.3https://plus.maths.org/content/origins-proof-ii-keplers-proofs4Unless you’re a theoretical physicist ;)70CHAPTER 4 The mechanics of learning This chapter is about how to automate this generic function-fitting, which is all you do with deep learning, deep neural networks being the generic functions, and PyTorch makes this process as simple and transparent as possible. To make sure that you get the key concepts right and to allow you to understand the mechanics of learn-ing algorithms from first principles, we’ll start with a model that’s a lot simpler than a deep neural network.4.1 Learning is parameter estimationIn this section, you learn how you can take data, choose a model, and estimate the parameters of the model so that it gives good predictions on new data. To do so, you’ll leave the intricacies of planetary motion and divert your attention to the second-hard-est problem in physics: calibrating instruments. Figure 4.2 shows a high-level overview of what you’ll have implemented by the end of the chapter. Given input data and the corresponding desired outputs (ground truth), as well as initial values for the weights, the model is fed input data (forward pass), and a measure of the error is evaluated by comparing the resulting outputs with the ground truth. To optimize the parameter of the model, its weights—the change in the error following a unit change in weights (the gradient of the error with respect to the parameters)—is computed by using the chain rule for the derivative of a compos-ite function (backward pass). Then the value of the weights is updated in the direc-tion that leads to a decrease in the error. The procedure is repeated until the error, evaluated on unseen data, falls below an acceptable level. I f  t h i s  s o u n d s  o b s c u r e ,  w e ’ v e  g o t  a  w h o l e  c h a p t e r  t o  c l a r i f y  t h i n g s .  B y  t h e  t i m e we’re done, all the pieces will fall into place, and the preceding paragraph will make perfect sense to you. Next, you take a problem with a noisy data set, build a model, and implement a learning algorithm for it. You’ll start by doing everything by hand, but by the end of the chapter, you’ll be letting PyTorch do all the heavy lifting. By the end of the chap-ter, we’ll have covered many of the essential concepts that underlie training deep neu-ral networks, even if the motivating example is simple and the model isn’t a neural network (yet!).4.1.1 A hot problemSuppose that you took a trip to some obscure location and brought back a fancy, wall-mounted analog thermometer. It looks great, it’s a perfect fit for your living room. Its only flaw is that it doesn’t show units. Not to worry; you’ve got a plan. You’ll build a data set of readings and corresponding temperature values in your favorite units, choose a model, and adjust its weights iteratively until a measure of the error is low enough, and you’ll finally be able to interpret the new readings in units you understand. S t a r t  b y  m a k i n g  a  n o t e  o f  t e m p e r a t u r e  d a t a  i n  g o o d  o l d  C e l s i u s5 a n d  m e a s u r e-ments from your new thermometer.5Luca is Italian, so please forgive him for using sensible units.Figure 4.2 Mental model of the learning process
71Learning is parameter estimation
After a couple of weeks, here’s the data:# In[2]:t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]t_c = torch.tensor(t_c)t_u = torch.tensor(t_u)t_c are temperatures in Celsius, and t_u are the unknown units. You can expect noise in both measurements coming from the devices themselves and from your approxi-mate readings. For convenience, the data is already in tensors, which you’ll use soon.4.1.2 Choosing a linear model as a first tryIn the absence of further knowledge, assume the simplest possible model for convert-ing between the two sets of measurements, as Kepler might have done. The two sets may be linearly related. That is, multiplying t_u by a factor and adding a constant, you may get the temperature in Celsius:t_c = w * t_u + bIs this assumption reasonable? Probably; you’ll see how well the final model performs. (You chose to name w and b after weight and bias, two common terms for linear scaling and the additive constant, which you’ll bump into all the time.)Listing 4.1 code/p1ch5/1_parameter_estimation.ipynb72CHAPTER 4 The mechanics of learningNOTE S p o i l e r  a l e r t :  W e  k n o w  t h a t  a  l i n e ar model is correct because the prob-lem and data have been fabricated, but please bear with us; this model is a useful motivating example to build your understanding of what PyTorch is doing under the hood.Now you need to estimate w and b, the parameters in the model, based on the data you have. You must do this so that the temperatures you obtain from running the unknown temperatures t_u through the model are close to temperatures you mea-sured in Celsius. If that process sounds like fitting a line through a set of measure-ments, that’s exactly what you’re doing. A s  y o u  g o  t h r o u gh this simple example using PyTorch, realize that training a neural network essentially involves changing the model for a slightly more elaborate one with a few (or a metric ton) more parameters. To flesh out the example again, you have a model with some unknown parameters, and you need to estimate those parameters so that the error between predicted out-puts and measured values is as low as possible. You notice that you still need to define a measure of such error. Such measure, which we refer to as the loss function, should be high if the error is high and should ideally be as low as possible for a perfect match. Your optimization process, therefore, should aim at finding w and b so that the loss function is at a minimum level.4.1.3 Less loss is what you wantA loss function (or cost function) is a function that computes a single numerical value that the learning process attempts to minimize. The calculation of loss typically involves taking the difference between the desired outputs for some training samples and those produced by the model when fed those samples—in this case, the differ-ence between the predicted temperatures t_p o u t p u t  b y  t h e  m o d e l  a n d  t h e  a c t u a l measurements, so t_p - t_c. You need to make sure the loss function makes the loss positive both when t_p is above and when below the true t_c, because the goal is to minimize this value. (Being able to push the loss infinitely negative isn’t useful.) You have a few choices, the most straightforward being |t_p - t_c| and (t_p - t_c)^2. Based on the mathematical expression you choose, you can emphasize or discount certain errors. Conceptually, a loss function is a way of prioritizing which errors to fix from your training samples, so that your parameter updates result in adjustments to the outputs for the highly weighted sam-ples instead of changes to some other samples’ output that had a smaller loss. Both of the example loss functions have a clear minimum in zero and grow mono-tonically as the predicted value moves farther from the true value in either direction. For this reason, both functions are said to be convex. Because your model is linear, the loss as a function of w and b is also convex. Cases in which the loss is a convex function of the model parameters are usually great to deal with because you can find a mini-mum in an efficient way through specialized algorithms. Deep neural networks don’t exhibit a convex loss, however, so those methods aren’t generally useful to you.73Learning is parameter estimation For the two loss functions |t_p - t_c| and (t_p - t_c)^2, as shown in figure 4.3, notice that the square of differences behaves more nicely around the minimum: the derivative of the error-squared loss with respect to t_p is zero when t_p equals t_c. The absolute value, on the contrary, has an undefined derivative right where you’d like to converge. This issue is less important than it looks in practice, but stick to the square of differences for the time being.
Figure 4.3 Absolute difference versus difference squared
It’s worth noting that the square difference also penalizes wildly wrong results more than the absolute difference. Often, having more slightly wrong results is better than having a few wildly wrong ones, and the squared difference helps prioritize those results as desired. 4.1.4 From problem to PyTorchYou’ve figured out the model and the loss function, so you’ve already got a good part of the high-level picture figured out. Now you need to set the learning process in motion and feed it actual data. Also, enough with math notation already, so now switch to PyTorch. After all, you came here for the fun. You’ve already created your data tensors, so write out the model as a Python function# In[3]:def model(t_u, w, b):    return w * t_u + bin which you’re expecting t_u, w, and b to be the input tensor, weight parameter, and bias parameter, respectively. In your model, the parameters will be PyTorch scalars (aka zero-dimensional tensors), and the product operation will use broadcasting to yield the returned tensors. Now define your loss:# In[4]:def loss_fn(t_p, t_c):    squared_diffs = (t_p - t_c)**2    return squared_diffs.mean()74CHAPTER 4 The mechanics of learningNote that you’re building a tensor of differences, taking their square elementwise and finally producing a scalar loss function by averaging all elements in the resulting ten-sor. The loss is a mean square loss. Now you can initialize the parameters, invoke the model,# In[5]:w = torch.ones(1)b = torch.zeros(1)t_p = model(t_u, w, b)t_p# Out[5]:tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,        48.4000, 60.4000, 68.4000])and check the value of the loss:# In[6]:loss = loss_fn(t_p, t_c)loss# Out[6]:tensor(1763.8846)In this section, you implemented the model and the loss. The meat of the section is how to estimate the w and b such that the loss reaches a minimum. First, you work things out by hand; then you learn how to leverage PyTorch superpowers to solve the same problem in a more general, off-the-shelf way.4.1.5 Down along the gradientIn this section, you optimize the loss function with respect to the parameters by using the so-called gradient descent algorithm and build your intuition about how gradient descent works from first principles, which will help you a lot in the future. There are ways to solve this particular example more e f f i c i e n t l y ,  b u t  t h o s e  a p p r o a c h e s  a r e n ’ t applicable to most deep learning tasks. Gradient descent is a simple idea that scales up surprisingly well to large neural network models with millions of parameters. Start with the mental image conveniently sketched out in figure 4.4. Suppose that you’re in front of a machine sporting two knobs, labeled w and b. You’re allowed to see the value of the loss on a screen and are told to minimize that value. Not knowing the effect of the knobs on the loss, you’d probably start fiddling with them and decide for each knob what direction makes the loss decrease. You’d probably decide to rotate both knobs in their direction of decreasing loss. If you’re far from the opti-mal value, you’re likely to see the loss decrease quickly and then slow as it gets closer to the minimum. You’d notice that at some point, the loss climbs back up again, so you’d invert the direction of rotation for one or both knobs. You’d also learn that when the loss changes slowly, it’s a good idea to adjust the knobs more finely to avoid reaching the point where the loss goes back up. After a while, eventually, you’d con-verge to a minimum.Figure 4.4 A cartoon depiction of the optimization process, in which a person with knobs for w and b searches for the direction that makes the loss decrease
75Learning is parameter estimation
Gradient descent isn’t too different. The idea is to compute the rate of change of the loss with respect to each parameter and apply a change to each parameter in the direction of decreasing loss. As when you were fiddling with the knobs, you could esti-mate such rate of change by applying a small change to w and b to see how much the loss is changing in that neighborhood:# In[7]:delta = 0.1loss_rate_of_change_w = \    (loss_fn(model(t_u, w + delta, b), t_c) -     loss_fn(model(t_u, w - delta, b), t_c)) / (2.0 * delta)This code is saying that in a small neighborhood of the current values of w and b, a unit increase in w leads to some change in the loss. If the change is negative, you need to increase w t o  m i n i m i z e  t h e  l o s s ,  w h e r e a s  i f  the change is positive, you need to decrease w. By how much? Applying a change to w that’s proportional to the rate of change of the loss is a good idea, especially when the loss has several parameters: you’d apply a change to those that exert a significant change on the loss. It’s also wise to change the parameters slowly in general, because the rate of change could be dra-matically different at a distance from the neighborhood of the current w value. There-fore, you should scale the rate of change by a typically small factor. This scaling factor has many names; the one used in machine learning is learning_rate.# In[8]:learning_rate = 1e-2w = w - learning_rate * loss_rate_of_change_wYou can do the same with b:# In[9]:loss_rate_of_change_b = \    (loss_fn(model(t_u, w, b + delta), t_c) -     loss_fn(model(t_u, w, b - delta), t_c)) / (2.0 * delta)b = b - learning_rate * loss_rate_of_change_b76CHAPTER 4 The mechanics of learningThis code represents the basic parameter update step for gradient descent. By reiter-ating these evaluations (provided that you choose a small-enough learning rate), you’d converge to an optimal value of the parameters for which the loss computed on the given data is minimal. We’ll show you the complete iterative process soon, but this method of computing rates of change is rather crude and needs an upgrade. In the next section, you see why and how.4.1.6 Getting analyticalComputing the rate of change by using repeated evaluations of model and loss to probe the behavior of the loss function in the neighborhood of w and b doesn’t scale well to models with many parameters. Also, it isn’t always clear how large that neigh-borhood should be. You chose delta equal to 0.1 earlier, but everything depends on the shape of the loss as a function of w and b. If the loss changes too quickly compared with delta, you won’t have a good idea of where downhill is. What if you could make the neighborhood infinitesimally small, as in figure 4.5? That’s exactly what happens when you take the derivative of the loss with respect to a parameter analytically. In a model with two or more parameters, you compute the individual derivatives of the loss with respect to each parameter and put them in a vec-tor of derivatives: the gradient. 
Figure 4.5 Differences in the estimated downhill directions when evaluating them at discrete locations versus analytically
To compute the derivative of the loss with respect to a parameter, you can apply the chain rule and compute the derivative of the loss with respect to its input (which is the output of the model) times the derivative of the model with respect to the parameter:d loss_fn / d w = (d loss_fn / d t_p) * (d t_p / d w)Recall that the model is a linear function and the loss is a sum of squares. Now figure out the expressions for the derivatives. Recalling the expression for the loss# In[4]:def loss_fn(t_p, t_c):    squared_diffs = (t_p - t_c)**2    return squared_diffs.mean()77Learning is parameter estimationand remembering that d x^2 / d x = 2 x, you get# In[10]:def dloss_fn(t_p, t_c):    dsq_diffs = 2 * (t_p - t_c)    return dsq_diffsAs for the model, recalling that the model is# In[3]:def model(t_u, w, b):    return w * t_u + byou get derivatives of# In[11]:def dmodel_dw(t_u, w, b):    return t_u# In[12]:def dmodel_db(t_u, w, b):    return 1.0Putting all this together, the function that returns the gradient of the loss with respect to w and b is# In[13]:def grad_fn(t_u, t_c, t_p, w, b):    dloss_dw = dloss_fn(t_p, t_c) * dmodel_dw(t_u, w, b)    dloss_db = dloss_fn(t_p, t_c) * dmodel_db(t_u, w, b)    return torch.stack([dloss_dw.mean(), dloss_db.mean()])The same idea expressed in mathematical notation is shown in figure 4.6.
Figure 4.6 The derivative of the loss function with respect to the weights
 Again, you’re averaging (summing and dividing by a constant) over all data points to get a single scalar quantity for each partial derivative of the loss.78CHAPTER 4 The mechanics of learning4.1.7 The training loopNow you have everything in place to optimize your parameters. Starting from a tenta-tive value for a parameter, you can iteratively apply updates to it for a fixed number of iterations or until w and b stop changing. You can use several stopping criteria, but stick to a fixed number of iterations for now. While we’re at it, we’ll introduce you to another piece of terminology. A training iteration during which you update the parameters for all your training samples is called an epoch. The complete training loop looks like this:# In[14]:def training_loop(n_epochs, learning_rate, params, t_u, t_c):    for epoch in range(1, n_epochs + 1):        w, b = params        t_p = model(t_u, w, b)          loss = loss_fn(t_p, t_c)        grad = grad_fn(t_u, t_c, t_p, w, b)         params = params - learning_rate * grad        print('Epoch %d, Loss %f' % (epoch, float(loss)))     return paramsThe actual logging logic used for the output in this text is more complicated (see cell 15 in the same notebook),6 but the differences are unimportant for understanding the core concepts in this chapter. Now invoke your training loop:# In[16]:training_loop(    n_epochs = 100,    learning_rate = 1e-2,    params = torch.tensor([1.0, 0.0]),    t_u = t_u,    t_c = t_c)# Out[16]:Epoch 1, Loss 1763.884644    Params: tensor([-44.1730,  -0.8260])    Grad:   tensor([4517.2964,   82.6000])Epoch 2, Loss 5802484.500000    Params: tensor([2568.4011,   45.1637])    Grad:   tensor([-261257.4062,   -4598.9707])Epoch 3, Loss 19408031744.000000    Params: tensor([-148527.7344,   -2616.3933])    Grad:   tensor([15109615.0000,   266155.7188])...6https://github.com/deep-learning-with-pytorch/dlwpt-code/blob/master/p1ch5/1_parameter_estimation.ipynbThis is the forward pass.And this is the backward pass.This logging line can be verbose.79Learning is parameter estimationEpoch 10, Loss 90901075478458130961171361977860096.000000    Params: tensor([3.2144e+17, 5.6621e+15])    Grad:   tensor([-3.2700e+19, -5.7600e+17])Epoch 11, Loss inf    Params: tensor([-1.8590e+19, -3.2746e+17])    Grad:   tensor([1.8912e+21, 3.3313e+19])tensor([-1.8590e+19, -3.2746e+17])Wait—what happened? Your training process blew up, leading to losses becoming inf. This result is a clear sign that params is receiving updates that are too large; their val-ues start oscillating back and forth as each update overshoots, and the next overcor-rects even more. The optimization process is unstable; it diverges instead of converging to a minimum. You want to see smaller and smaller updates to params, not larger, as shown in figure 4.7.
Figure 4.7 Top: Diverging optimization on convex function (parabolalike) due to large steps. Bottom: Converging optimization with small steps.
How can you limit the magnitude of the learning_rate * grad? Well, that process looks easy. You could simply choose a smaller learning_rate. You usually change learn-ing rates by order of magnitude, so you might try 1e-3 or 1e-4, which would decrease the magnitude of updates by orders of magnitude. Go with 1e-4 to see how it works out:# In[17]:training_loop(    n_epochs = 100,    learning_rate = 1e-4,80CHAPTER 4 The mechanics of learning    params = torch.tensor([1.0, 0.0]),    t_u = t_u,    t_c = t_c)# Out[17]:Epoch 1, Loss 1763.884644    Params: tensor([ 0.5483, -0.0083])    Grad:   tensor([4517.2964,   82.6000])Epoch 2, Loss 323.090546    Params: tensor([ 0.3623, -0.0118])    Grad:   tensor([1859.5493,   35.7843])Epoch 3, Loss 78.929634    Params: tensor([ 0.2858, -0.0135])    Grad:   tensor([765.4666,  16.5122])...Epoch 10, Loss 29.105242    Params: tensor([ 0.2324, -0.0166])    Grad:   tensor([1.4803, 3.0544])Epoch 11, Loss 29.104168    Params: tensor([ 0.2323, -0.0169])    Grad:   tensor([0.5780, 3.0384])...Epoch 99, Loss 29.023582    Params: tensor([ 0.2327, -0.0435])    Grad:   tensor([-0.0533,  3.0226])Epoch 100, Loss 29.022669    Params: tensor([ 0.2327, -0.0438])    Grad:   tensor([-0.0532,  3.0226])tensor([ 0.2327, -0.0438])Nice. The behavior is stable now. But there’s another problem: updates to parameters are small, so the loss decreases slowly and eventually stalls. You could obviate this issue by making the learning_rate adaptive—that is, change according to the magnitude of updates. You can use several optimization schemes for that purpose; you see one toward the end of this chapter, in section “Optimizers a-la Carte”. Another potential troublemaker exists in the update term: the gradient itself. Go back to look at grad at epoch 1 during optimization. You see that the first-epoch gradi-ent for the weight is about 50 times larger than the gradient for the bias, so the weight and bias live in differently scaled spaces. In this case, a learning rate that’s large enough to meaningfully update one is so large that it’s unstable for the other, or a rate that’s appropriate for the second one won’t be large enough to change the first mean-ingfully. You’re not going to be able to update your parameters unless you change your formulation of the problem. You could have individual learning rates for each parameter, but for models with many parameters, this approach would be too much to bother with; it’s babysitting of the kind you don’t like. You have a simpler way to keep things in check: change the inputs so that the gra-dients aren’t so different. You can make sure that the range of the input doesn’t get too far from the range of -1.0 to 1.0, roughly speaking. In this case, you can achieve something close enough to that example by multiplying t_u by 0.1:81Learning is parameter estimation# In[18]:t_un = 0.1 * t_uHere, you denote the normalized version of t_u by appending n to the variable name. At this point, you can run the training loop on your normalized input:# In[19]:training_loop(    n_epochs = 100,    learning_rate = 1e-2,    params = torch.tensor([1.0, 0.0]),    t_u = t_un,     t_c = t_c)# Out[19]:Epoch 1, Loss 80.364342    Params: tensor([1.7761, 0.1064])    Grad:   tensor([-77.6140, -10.6400])Epoch 2, Loss 37.574917    Params: tensor([2.0848, 0.1303])    Grad:   tensor([-30.8623,  -2.3864])Epoch 3, Loss 30.871077    Params: tensor([2.2094, 0.1217])    Grad:   tensor([-12.4631,   0.8587])...Epoch 10, Loss 29.030487    Params: tensor([ 2.3232, -0.0710])    Grad:   tensor([-0.5355,  2.9295])Epoch 11, Loss 28.941875    Params: tensor([ 2.3284, -0.1003])    Grad:   tensor([-0.5240,  2.9264])...Epoch 99, Loss 22.214186    Params: tensor([ 2.7508, -2.4910])    Grad:   tensor([-0.4453,  2.5208])Epoch 100, Loss 22.148710    Params: tensor([ 2.7553, -2.5162])    Grad:   tensor([-0.4445,  2.5165])tensor([ 2.7553, -2.5162])Even though you set your learning rate back to 1e-2, parameters didn’t blow up during iterative updates. Now take a look at the gradients; they were of similar magni-tudes, so using a single learning_rate for both parameters worked fine. You probably could do a better job of normalization than rescaling by a factor of ten, but because doing so is good enough for your needs, stick it for now.NOTE T h e  n o r m a l i z a t i o n  h e r e  h e l p s  y o u  get the network trained, but you could make an argument that it’s not strictly needed to optimize the parame-ters for this problem. That’s absolutely true! This problem is small enough that you have numerous ways to beat the parameters into submission. For larger, more sophisticated problems, however, normalization is an easy and effective (if not crucial!) tool to use to improve model convergence.You’ve updated t_u to your new, rescaled t_un.82CHAPTER 4 The mechanics of learningNext, run the loop for enough iterations to see the changes in params g e t  s m a l l . Change n_epochs to 5000:# In[20]:params = training_loop(    n_epochs = 5000,    learning_rate = 1e-2,    params = torch.tensor([1.0, 0.0]),    t_u = t_un,    t_c = t_c,    print_params = False)params# Out[20]:Epoch 1, Loss 80.364342Epoch 2, Loss 37.574917Epoch 3, Loss 30.871077...Epoch 10, Loss 29.030487Epoch 11, Loss 28.941875...Epoch 99, Loss 22.214186Epoch 100, Loss 22.148710...Epoch 4000, Loss 2.927680Epoch 5000, Loss 2.927648tensor([  5.3671, -17.3012])Good. You saw the loss decrease while you were changing parameters along the direc-tion of gradient descent. The loss didn’t go to zero, which could mean that iterations weren’t enough to converge to zero or that the data points aren’t sitting on a line. As anticipated, your measurements weren’t perfectly accurate or noise was involved in the reading. But look: the value for w and b looks an awful lot like the numbers you need to use to convert Celsius to Fahrenheit (after accounting for the earlier normalization when you multiplied your inputs by 0.1). The exact values are w=5.5556 and b=-17.7778. Your fancy thermometer was showing temperatures in Fahrenheit the whole time, which is no big discovery, but it proves that your gradient descent optimization pro-cess works. Next, do something that you should have done right from the start: plot your data. We didn’t introduce this topic until now for the sake of drama (the surprise effect). But seriously, the first thing that anyone doing data science should do is plot the heck out of the data.# In[21]:%matplotlib inlinefrom matplotlib import pyplot as pltt_p = model(t_un, *params)  Remember that you’re training on the normalized unknown units.83PyTorch’s autograd: Backpropagate all thingsfig = plt.figure(dpi=600)plt.xlabel("Fahrenheit")plt.ylabel("Celsius")plt.plot(t_u.numpy(), t_p.detach().numpy())plt.plot(t_u.numpy(), t_c.numpy(), 'o')This code produces Figure 4.8. 
Figure 4.8 The plot of your linear-fit model (solid line) versus input data (circles)
The linear model is a good model for the data, it seems. It also seems that your mea-surements are somewhat erratic. You should e i t h er c a l l  y o u r  op to m e tr i s t  f o r  a  n e w pair of glasses or think about returning your fancy thermometer.4.2 PyTorch’s autograd: Backpropagate all thingsIn your little adventure so far, you saw a simple example of backpropagation. You com-puted the gradient of a composition of f u n c tions—the model and the loss—with respect to their innermost parameters—w and b—by propagating derivatives backward via the chain rule. The basic requirement is that all functions you’re dealing with are differentiable analytically. In this case, you can compute the gradient (which we called “the rate of change of the loss” earlier) with respect to the parameters in one sweep. Should you have a complicated model with millions of parameters, as long as the model is differentiable, computing the gradient the loss with respect to parameters amounts to writing the analytical expression for the derivatives and evaluating them once. Granted, writing the analytical expression for the derivatives of a deep composi-tion of linear and nonlinear functions isn’t a lot of fun.7 It isn’t particularly quick, either.7Or maybe it is; we won’t judge how you spend your weekend!But you’re plotting the raw unknown values.84CHAPTER 4 The mechanics of learning This situation is where PyTorch tensors come to the rescue, with a PyTorch compo-nent called autograd. PyTorch tensors can remember where they come from in terms of the operations and parent tensors that originated them, and they can provide the chain of derivatives of such operations with respect to their inputs automatically. You won’t need to derive your model by hand;8 given a forward expression, no matter how nested, PyTorch provides the gradient of that expression with respect to its input parameters automatically. At this point, the best way to proceed is to rewrite the thermometer calibration code, this time using autograd, and see what happens. First, recall your model and loss function, as shown in the following listing.# In[3]:def model(t_u, w, b):    return w * t_u + b# In[4]:def loss_fn(t_p, t_c):    squared_diffs = (t_p - t_c)**2    return squared_diffs.mean()Again initialize a parameters tensor:# In[5]:params = torch.tensor([1.0, 0.0], requires_grad=True)Notice the requires_grad=True argument to the tensor constructor? That argument is telling PyTorch to track the entire family tree of tensors resulting from operations on params. In other words, any tensor that has params as an ancestor has access to the chain of functions that were called to get from params to that tensor. In case these functions are differentiable (and most PyTorch tensor operations are), the value of the derivative is automatically populated as a grad attribute of the params tensor. In general, all PyTorch tensors have an attribute named grad, normally None:# In[6]:params.grad is None# Out[6]:TrueAll you have to do to populate it is start with a tensor with requires_grad set to True, call the model, compute the loss, and then call backward on the loss tensor:# In[7]:loss = loss_fn(model(t_u, *params), t_c)loss.backward()8Bummer! What are we going to do on Saturdays now?Listing 4.2 code/p1ch5/2_autograd.ipynb85PyTorch’s autograd: Backpropagate all thingsparams.grad# Out[7]:tensor([4517.2969,   82.6000])At this point, the grad a t t r i b u t e  o f  params c o n t a i n s  t h e  d e r i v a t i v e s  o f  t h e  l o s s  w i t h respect to each element of params (figure 4.9). 
Figure 4.9 The forward graph and backward graph of model as computed with autograd You could have any number of tensors with requires_grad set to True and any composition of functions. In this case, PyTorch would compute derivatives of the loss throughout the chain of functions (the computation graph) and accumulate their val-ues in the grad attribute of those tensors (the leaf nodes of the graph). Alert: Big gotcha ahead. This is one thing that PyTorch newcomers (and a lot of more experienced folks) trip up on regularly. We wrote accumulate, not store.WARNING C a l l i n g  backward leads derivatives to accumulate at leaf nodes. You need to zero the gradient explicitly after using it for parameter updates.86CHAPTER 4 The mechanics of learningTo repeat, calling backward leads derivatives to accumulate at leaf nodes. So if back-ward has been called earlier, the loss is evaluated again, and backward is called again (as in any training loop), the gradient at each leaf is accumulated (summed) on top of the one computed at the preceding iteration, which leads to an incorrect value for the gradient. To prevent this situation from occurring, you need to zero the gradient explicitly at each iteration. You can do so easily by using the in-place zero_ method:# In[8]:if params.grad is not None:    params.grad.zero_()NOTE Y o u  m a y  b e  c u r i o u s  w h y  z e r o i n g  t h e  g radient is a required step instead of automatic whenever you call backward. The reason is to provide more flex-ibility and control for working with gradients in complicated models.Having this reminder drilled into your h e a d ,  now see how your autograd-enabled training code looks like, start to end:# In[9]:def training_loop(n_epochs, learning_rate, params, t_u, t_c):    for epoch in range(1, n_epochs + 1):        if params.grad is not None:              params.grad.zero_()        t_p = model(t_u, *params)        loss = loss_fn(t_p, t_c)        loss.backward()        params = (params - learning_rate * params.grad).detach().requires_grad_()        if epoch % 500 == 0:            print('Epoch %d, Loss %f' % (epoch, float(loss)))	    return paramsNotice that when you updated params, you also did an odd .detach().requi-res_grad_() d a n c e .  T o  u n d e r s t a n d  w h y ,  t h i nk about the computation graph that you’ve built. Reformulate your params update line a little so that you’re not reusing vari-able names: p1 = (p0 * lr * p0.grad) Here, p0 is the random weights with which you initialized the model. p0.grad is computed from a combination of p0 and your training data via the loss function. So far, so good. Now you need to look at the second iteration of the loop: p2 = (p1 * lr * p1.grad). As you’ve seen, the computation graph for p1 goes back to p0,which is problematic because (a) you have to keep p0 in memory (until you’re done with training), and (b) it confuses the matter of where you should be assigning error via backpropagation.This could be done at any point in the loop prior to calling loss.backward()It’s somewhat cumbersome, but as you’ll see in “Optimizers a-la Carte,” it’s not an issue in practice.87PyTorch’s autograd: Backpropagate all things I n s t e a d ,  d e t a c h  t h e  n e w  params t e n s o r  f r o m  t h e  c o m putation graph associated with its update expression by calling .detatch(). This way, params effectively loses the memory of the operations that generated it. Then you can reenable tracking by call-ing .requires_grad_(), an in_place operation (see the trailing _) that reactivates autograd for the tensor. Now you can release the memory held by old versions of params and need to backpropagate through only your current weights. See whether this code works:# In[10]:training_loop(    n_epochs = 5000,    learning_rate = 1e-2,    params = torch.tensor([1.0, 0.0], requires_grad=True),     t_u = t_un,     t_c = t_c)# Out[10]:Epoch 500, Loss 7.860116Epoch 1000, Loss 3.828538Epoch 1500, Loss 3.092191Epoch 2000, Loss 2.957697Epoch 2500, Loss 2.933134Epoch 3000, Loss 2.928648Epoch 3500, Loss 2.927830Epoch 4000, Loss 2.927679Epoch 4500, Loss 2.927652Epoch 5000, Loss 2.927647tensor([  5.3671, -17.3012], requires_grad=True)You get the same result that you got previously. Good for you! Although you’re capableof computing derivatives by hand, you no longer need to.4.2.1 Optimizers a la carteThis code uses vanilla gradient descent for optimization, which works fine for this sim-ple case. Needless to say, several optimization strategies and tricks can help conver-gence, especially when models get complicated. Now is the right time to introduce the way that PyTorch abstracts the optimization strategy away from user code, such as the training loop, sparing you from the boiler-plate busywork of having to update every parameter in your model yourself. The torch module has an optim submodule where you can find classes that implement dif-ferent optimization algorithms. Here’s an abridged listing:# In[5]:import torch.optim as optimdir(optim)# Out[5]:Listing 4.3 code/p1ch5/3_optimizers.ipynbAdding this requires_grad=True is key.Note that again, you’re using the normalized t_un instead of t_u.88CHAPTER 4 The mechanics of learning['ASGD', 'Adadelta', 'Adagrad', 'Adam', 'Adamax', 'LBFGS', 'Optimizer', 'RMSprop', 'Rprop', 'SGD', 'SparseAdam',...]Every optimizer constructor takes a list of parameters (aka PyTorch tensors, typically with requires_grad set to True) as the first input. All parameters passed to the opti-mizer are retained inside the optimizer object so that the optimizer can update their values and access their grad attribute, as represented in figure 4.10.
Figure 4.10 Conceptual representation of how an optimizer holds a reference to parameters (A), and after a loss is computed from inputs (B), a call to .backward leads to .grad being populated on parameter (C). At that point, the optimizer can access .grad and compute the parameter updates (D).
 Each optimizer exposes two methods: zero_grad and step. The former zeros the grad attribute of all the parameters passed to the optimizer upon construction. The latter updates the value of those parameters according to the optimization strategy implemented by the specific optimizer.89PyTorch’s autograd: Backpropagate all thingsNow create params and instantiate a gradient descent optimizer:# In[6]:params = torch.tensor([1.0, 0.0], requires_grad=True)learning_rate = 1e-5optimizer = optim.SGD([params], lr=learning_rate)Here, SGD stands for Stochastic Gradient Descent. The optimizer itself is a vanilla gradient descent (as long as the momentum argument is set to 0.0, which is the default). The term stochastic comes from the fact that the gradient is typically obtained by averaging over a random subset of all input samples, called a minibatch. The optimizer itself, however, doesn’t know whether the loss was evaluated on all the samples (vanilla) or a random subset thereof (stochastic), so the algorithm is the same in the two cases. Anyway, take your fancy new optimizer for a spin:# In[7]:t_p = model(t_u, *params)loss = loss_fn(t_p, t_c)loss.backward()optimizer.step()params# Out[7]:tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True)The value of params was updated when step was called, and you didn’t have to touch it yourself! What happened was that the optimizer looked into params.grad a n d updated params by subtracting learning_rate times grad from it, exactly as in your former hand-rolled code. Are you ready to stick this code in a training loop? Nope! The big gotcha almost got you: you forgot to zero out the gradients. Had you called the preceding code in a loop, gradients would have accumulated in the leaves at every call to backward, and your gradient descent would have been all over the place! Here’s the loop-ready code, with the extra zero_grad in the right spot (before the call to backward):# In[8]:params = torch.tensor([1.0, 0.0], requires_grad=True)learning_rate = 1e-2optimizer = optim.SGD([params], lr=learning_rate)t_p = model(t_un, *params)loss = loss_fn(t_p, t_c)optimizer.zero_grad() loss.backward()optimizer.step()params# Out[8]:tensor([1.7761, 0.1064], requires_grad=True)As before, the placement of this call is somewhat arbitrary. It could be earlier in the loop as well.90CHAPTER 4 The mechanics of learningPerfect! See how the optim module helped you abstract away the specific optimization scheme? All you have to do is provide a list of params to it (that list can be extremely long, as needed for deep neural network models) and then forget about the details. Update your training loop accordingly:# In[9]:def training_loop(n_epochs, optimizer, params, t_u, t_c):    for epoch in range(1, n_epochs + 1):        t_p = model(t_u, *params)        loss = loss_fn(t_p, t_c)        optimizer.zero_grad()        loss.backward()        optimizer.step()        if epoch % 500 == 0:            print('Epoch %d, Loss %f' % (epoch, float(loss)))    return params# In[10]:params = torch.tensor([1.0, 0.0], requires_grad=True)learning_rate = 1e-2optimizer = optim.SGD([params], lr=learning_rate) training_loop(    n_epochs = 5000,    optimizer = optimizer,    params = params,     t_u = t_un,    t_c = t_c)# Out[10]:Epoch 500, Loss 7.860116Epoch 1000, Loss 3.828538Epoch 1500, Loss 3.092191Epoch 2000, Loss 2.957697Epoch 2500, Loss 2.933134Epoch 3000, Loss 2.928648Epoch 3500, Loss 2.927830Epoch 4000, Loss 2.927679Epoch 4500, Loss 2.927652Epoch 5000, Loss 2.927647tensor([  5.3671, -17.3012], requires_grad=True)Again, you get the same result as before. Great. You have further confirmation that you know how to descend a gradient by hand! To test more optimizers, all you have to do is instantiate a different optimizer, such as Adam, instead of SGD. The rest of the code stays as is. This stuff is pretty handy. We won’t go into much detail on Adam, but it suffices to say that it’s a more sophis-ticated optimizer in which the learning rate is set adaptively. In addition, it’s a lot less sensitive to the scaling of the parameters—so insensitive that you can go back to use It’s important that both params here are the same object; otherwise, the optimizer won’t know what parameters the model used.91PyTorch’s autograd: Backpropagate all thingsthe original (non-normalized) input t_u and even increase the learning rate to 1e-1. Adam won’t even blink:# In[11]:params = torch.tensor([1.0, 0.0], requires_grad=True)learning_rate = 1e-1optimizer = optim.Adam([params], lr=learning_rate) training_loop(    n_epochs = 2000,    optimizer = optimizer,    params = params,    t_u = t_u,     t_c = t_c)# Out[11]:Epoch 500, Loss 7.612901Epoch 1000, Loss 3.086700Epoch 1500, Loss 2.928578Epoch 2000, Loss 2.927646tensor([  0.5367, -17.3021], requires_grad=True)The optimizer isn’t the only flexible part of your training loop. Turn your attention to the model. To train a neural network on the same data and the same loss, all you’d need to change is the model function. Doing this wouldn’t make sense in this case, because you know that converting Celsius to Fahrenheit amounts to a linear transfor-mation. Neural networks allow you to remove your arbitrary assumptions about the shape of the function you should be approximating. Even so, neural networks manage to be trained even when the underlying processes are highly nonlinear (such in the case of describing an image with a sentence). We’ve touched on a lot of the essential concepts that will enable you to train com-plicated deep learning models while knowing what’s going on under the hood: back-propagation to estimate gradients, autograd, and optimizing weights of models by using gradient descent or other optimizers. We don’t have a whole lot more to cover. The rest is mostly filling in the blanks, however extensive they are. Next, we discuss how to split up samples, which sets up a perfect use case for learn-ing to control autograd better.4.2.2 Training, validation, and overfittingJohannes Kepler kept a part of the data on the side so that he could validate his mod-els on independent observations—a vital thing to do, especially when the model you adopt could potentially approximate functions of any shape, as in the case of neural networks. In other words, a highly adaptable model tends to use its many parameters to make sure that the loss is minimal at the data points, but you’ll have no guarantee that the model behaves well away from or between the data points. After all, that’s all you’re asking the optimizer to do: minimize the loss at the data points. Sure enough, if you had independent data points that you didn’t use to evaluate your loss or New optimizer class here.Note that you’re back to the original t_u as input.92CHAPTER 4 The mechanics of learningdescend along its negative gradient, you’d soon find out that evaluating the loss at those independent data points would yield a higher-than-expected loss. We’ve already mentioned this phenomenon, called overfitting. The first action you can take to combat overfitting is to recognize that it might happen. To do so, as Kepler figured out in 1600, you must take a few data points out of your data set (the validation set) and fit our model to the remaining data points (the training set), as shown in figure 4.11. Then, while you’re fitting the model, you can evaluate the loss once on the training set and once on the validation set. When you’re trying to decide whether you’ve done a good job of fitting your model to the data, you must look at each data set!
Figure 4.11 Conceptual representation of a data-producing process and the collection and use of training data and independent validation data The first figure, the training loss, tells you whether your model can fit the training set at all—in other words, whether your model has enough capacity to process the rele-vant information in the data. If your mysterious thermometer somehow managed to measure temperatures by using a logarithmic scale, your poor linear model wouldn’t have had a chance to fit those measurements and provide a sensible conversion to Cel-sius. In that case, your training loss (the loss you were printing in the training loop) would stop decreasing well before approaching zero. A deep neural network can potentially approximate complicated functions, pro-vided that the number of neurons—and, therefore, parameters—is high enough. The fewer the parameters, the simpler the shape of the function your network will be able to approximate. So here’s rule one: if the training loss isn’t decreasing, chances are 93PyTorch’s autograd: Backpropagate all thingsthat the model is too simple for the data. The other possibility is that your data doesn’t contain meaningful information for it to explain the output. If the nice folks at the shop sold you a barometer instead of a thermometer, you’d have little chance to pre-dict temperature in Celsius from pressure alone, even if you used the latest neural net-work architecture from Quebec.9 W h a t  a b o u t  t h e  v a l i d a t i o n  s e t ?  W e l l ,  i f t h e  l o s s  e v a l u a t e d  i n  t h e  v a l i d a t i o n  s e t doesn’t decrease along with the training set, your model is improving its fit of the sam-ples it’s seeing during training, but it isn’t generalizing to samples outside this precise set. As soon as you evaluate the model at new, previously unseen points, the values of the loss function are poor. Here’s rule two: if the training loss and the validation loss diverge, you’re overfitting. We’ll delve into this phenomenon a little here, going back to the thermometer example. You could have decided to fit the data with a more complicated function, such as a piecewise polynomial or a large neural network. This function could gener-ate a model that meanders its way through the data points, as in figure 4.12, because it pushes the loss close to zero. Because the behavior of the function away from the data points doesn’t increase the loss, there’s nothing to keep the model away from the training data points in check.
Figure 4.12 Rather extreme example of overfitting
9https://www.umontreal.ca/en/artificialintelligence94CHAPTER 4 The mechanics of learningWhat’s the cure, though? Good question. Overfitting looks like a problem of making sure that the behavior of the model in between data points is sensible for the process you’re trying approximate. First, you should make sure that you get enough data for the process. If you collected data from a sinusoidal process by sampling it regularly at a low frequency, you’d have a hard time fitting a model to it. Assuming that you have enough data points, you should make sure that the model that’s capable of fitting the training data is as regular as possible between the data points. You have several ways to achieve this goal. One way is to add so-called penaliza-tion terms t o  t h e l o s s  f u n c t i o n  t o  m a k e  i t  c h e a p e r  f o r  t h e  m o d e l  t o  b e h a v e  m o r e smoothly and change more slowly (up to a point). Another way is to add noise to the input samples, to artificially create new data points between training data samples and force the model to try to fit them too. Several other ways are somewhat related to the preceding ones. But the best favor you can do for yourself, at least as a first move, is to make your model simpler. From an intuitive standpoint, a simpler model may not fit the training data as perfectly as a more complicated model would do, but it will likely behave more regularly between data points. You’ve got some nice tradeoffs here. On one hand, you need to model to have enough capacity for it to fit the training set. On the other hand, you need the model to avoid overfitting. Therefore, the process for choosing the right size of a neural net-work model, in terms of parameters, is based on two steps: increase the size until it fits and then scale it down until it stops overfitting. Your life will be a balancing act between fitting and overfitting. You can split the data into a training set and a validation set by shuffling t_u and t_c in the same way and then splitting the resulting shuffled tensors into two parts. Shuffling the elements of a tensor amounts to finding a permutation of its indices. The randperm function does this:# In[12]:n_samples = t_u.shape[0]n_val = int(0.2 * n_samples)shuffled_indices = torch.randperm(n_samples)train_indices = shuffled_indices[:-n_val]val_indices = shuffled_indices[-n_val:]train_indices, val_indices  # Out[12]:(tensor([ 8,  0,  3,  6,  4,  1,  2,  5, 10]), tensor([9, 7]))You get index tensors that you can use to build training and validation sets starting from the data tensors:# In[13]:train_t_u = t_u[train_indices]train_t_c = t_c[train_indices]val_t_u = t_u[val_indices]Because these values are random, don’t be surprised if your values end up being different from here on.95PyTorch’s autograd: Backpropagate all thingsval_t_c = t_c[val_indices]train_t_un = 0.1 * train_t_uval_t_un = 0.1 * val_t_uYour training loop doesn’t change. You want to evaluate the validation loss at every epoch to have a chance to recognize whether you’re overfitting:# In[14]:def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):    for epoch in range(1, n_epochs + 1):        train_t_p = model(train_t_u, *params)          train_loss = loss_fn(train_t_p, train_t_c)        val_t_p = model(val_t_u, *params)               val_loss = loss_fn(val_t_p, val_t_c)        optimizer.zero_grad()        train_loss.backward()         optimizer.step()        if epoch <= 3 or epoch % 500 == 0:            print('Epoch {}, Training loss {}, Validation loss {}'.format(                epoch, float(train_loss), float(val_loss)))    return params# In[15]:params = torch.tensor([1.0, 0.0], requires_grad=True)learning_rate = 1e-2optimizer = optim.SGD([params], lr=learning_rate)training_loop(    n_epochs = 3000,    optimizer = optimizer,    params = params,    train_t_u = train_t_un,     val_t_u = val_t_un,       train_t_c = train_t_c,    val_t_c = val_t_c)# Out[15]:Epoch 1, Training loss 88.59708404541016, Validation loss 43.31699752807617Epoch 2, Training loss 34.42190933227539, Validation loss 35.03486633300781Epoch 3, Training loss 27.57990264892578, Validation loss 40.214229583740234Epoch 500, Training loss 9.516923904418945, Validation loss 9.02982234954834Epoch 1000, Training loss 4.543173789978027, Validation loss 2.596876621246338Epoch 1500, Training loss 3.1108808517456055, Validation loss 2.9066450595855713Epoch 2000, Training loss 2.6984243392944336, Validation loss 4.1561737060546875Epoch 2500, Training loss 2.579646348953247, Validation loss 5.138668537139893Epoch 3000, Training loss 2.5454416275024414, Validation loss 5.755766868591309tensor([  5.6473, -18.7334], requires_grad=True)These two pairs of lines are the same except for the train_* vs. val_* inputs.Note that you have no val_loss.backward() here because you don’t want to train the model on the validation data.
Because you’re using SGD again, you’re back to using normalized inputs.96CHAPTER 4 The mechanics of learningHere, we’re not being entirely fair to the model. The validation set is small, so the val-idation loss will be meaningful only up to a point. In any case, note that the validation loss is higher than your training loss, although not by an order of magnitude. The fact that a model performs better on the training set is expected since the model parame-ters are being shaped by the training set. Your main goal is to also see both the train-ing loss and the validation loss decreasing. Although ideally, both losses would be roughly the same value, as long as validation loss stays reasonably close to the training loss, you know that your model is continuing to learn generalized things about your data. In figure 4.13, case C is ideal, and D is acceptable. In case A, the model isn’t learning at all, and in case B, you see overfitting.
Figure 4.13 Overfitting scenarios for the training (blue) and validation (red) losses. (A) Training and validation losses don’t decrease; the model isn’t learning due to no information in the data or insufficient capacity of the model. (B) Training loss decreases while validation increases (overfitting). (C) Training and validation losses decrease in tandem; performance may be improved further, as the model isn’t at the limit of overfitting. (D) Training and validation losses have different absolute values but similar trends; overfitting is under control.
4.2.3 Nits in autograd and switching it offFrom the training loop, you can appreciate that you only ever call backward on the train_loss. Therefore, errors only ever backpropagate based on the training set. The validation set is used to provide an independent evaluation of the accuracy of the model’s output on data that wasn’t used for training. The curious reader will have an embryo of a question at this point. The model is evaluated twice—once on train_t_u and then on val_t_u—after which backward is 97PyTorch’s autograd: Backpropagate all thingscalled. Won’t this confuse the hell out of autograd? Won’t backward be influenced by the values generated during the pass on the validation set? Luckily, this isn’t the case. The first line in the training loop evaluates model on train_t_u t o  p r o d u c e  train_t_p. Then train_loss i s  e v a l u a t e d  from train_t_p, creating a computation graph that links train_t_u t o  train_t_p t o  train_loss. When model is evaluated again on val_t_u, it produces val_t_p and val_loss. In this case, a separate computation graph is created that links val_t_u t o  val_t_p t o val_loss. Separate tensors have been run through the same functions, model a n d loss_fn, generating separate computation graphs, as shown in figure 4.14.
Figure 4.14 Diagram showing how gradients propagate through a graph with two losses when .backward is called on one of them
The only tensors that these two graphs have in common are the parameters. When you call backward on train_loss, you run the backward on the first graph. In other words, you accumulate the derivatives of the train_loss with respect to the parame-ters based on the computation generated from train_t_u. If you (incorrectly) called backward on val_loss as well, you’d have accumulated the derivatives of the val_loss with respect to the parameters on the same leaf nodes. Remember the zero_grad thing, whereby gradients would be accumulated on top of each other every time you called backward unless you zeroed out gradients explicitly? Well, here something similar would happen: calling backward o n  val_loss w o u l d lead to gradients accumulating in the params tensor, on top of those generated during the train_loss.backward() call. In this case, you’d effectively train your model on the whole data set (both training and validation), because the gradient would depend on both. Pretty interesting.98CHAPTER 4 The mechanics of learning Here’s another element for discussion: because you’re never calling backward on val_loss, why are you building the graph in the first place? You could in fact call model and loss_fn as plain functions without tracking history. However optimized, tracking history comes with additional costs that you could forgo during the validation pass, especially when the model has millions of parameters. To address this situation, PyTorch allows you to switch off autograd when you don’t need it by using the torch.no_grad context manager. You won’t see any meaningful advantage in terms of speed or memory consumption on your small problem. But for larger models, the differences can add up. You can make sure that this context manager works by checking the value of the requires_grad attribute on the val_loss tensor:# In[16]:def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):    for epoch in range(1, n_epochs + 1):        train_t_p = model(train_t_u, *params)        train_loss = loss_fn(train_t_p, train_t_c)        with torch.no_grad():             val_t_p = model(val_t_u, *params)            val_loss = loss_fn(val_t_p, val_t_c)            assert val_loss.requires_grad == False         optimizer.zero_grad()        train_loss.backward()        optimizer.step()Using the related set_grad_enabled context, you can also condition code to run with autograd enabled or disabled, according to a Boolean expression, typically indicating whether you’re running in training or inference. You could define a calc_forwardfunction that takes data in input and runs model and loss_fn with or without auto-grad, according to a Boolean train_is argument:# In[17]:def calc_forward(t_u, t_c, is_train):    with torch.set_grad_enabled(is_train):        t_p = model(t_u, *params)        loss = loss_fn(t_p, t_c)    return lossConclusionThis chapter started with a big question: how can a machine learn from examples? We spent the rest of the chapter describing the mechanism by which a model can be opti-mized to fit data. We chose to stick with a simple model to show all the moving parts without unneeded complications.Context manager here.All requires_grad args are forced to False inside this block.99PyTorch’s autograd: Backpropagate all thingsExercisesRedefine the model to be w2 * t_u ** 2 + w1 * t_u + b.–W h a t  p a r t s  o f  t h e  t r a i n i n g  l o o p  a n d  s o on must be changed to accommodate this redefinition?•W h a t  p a r t s  a r e  a g n o s t i c  t o  s w a p p i n g  o u t  t h e  m o d e l ?–I s  t h e  r e s u l t i n g  l o s s  h i g h e r or lower after training?•I s  t h e  r e s u l t  b e t t e r  o r  w o r s e ?SummaryLinear models are the simplest reasonable model to use to fit data.Convex optimization techniques can be used for linear models, but they don’t generalize to neural networks, so this chapter focuses on parameter estimation.Deep learning can be used for generic models that aren’t engineered to solve a specific task but can be adapted automatically to specialize in the problem at hand.Learning algorithms amount to optimizing parameters of models based on observations. Loss function is a measure of the error in carrying out a task, such as the error between predicted outputs and measured values. The goal is to get loss function as low as possible.The rate of change of the loss function with respect to model parameters can be used to update the same parameters in the direction of decreasing loss.The optim module in PyTorch provides a collection of ready-to-use optimizers for updating parameters and minimizing loss functions.Optimizers use the autograd feature of PyTorch to compute the gradient for each parameter, depending on how that parameter contributed to the final out-put. This feature allows users to rely on the dynamic computation graph during complex forward passes.Context managers such as with torch.no_grad(): can be used to control auto-grad behavior.Data is often split into separate sets of training samples and validation samples, allowing a model to be evaluated on data it wasn’t trained on.Overfitting a model happens when the model’s performance continues to improve on the training set but degrades on the validation set. This situation usually occurs when the model doesn’t generalize and instead memorizes the desired outputs for the training set.101Using a neural network  to fit your data
You’ve taken a close look at how a linear model can learn and how to make it hap-pen in PyTorch, focusing on a simple regression problem that required a linear model with one input and one output. This simple example allowed you to dissect the mechanics of a model that learns without getting overly distracted by the imple-mentation of the model itself. Backpropagating error to the parameters and then updating those parameters by taking the gradient with respect to the loss is going to be the same no matter what the underlying model is (figure 5.1). I n  t h i s  c h a p t e r ,  y o u ’ r e  g o i n g  t o  m ake changes in your model architecture. You’re going to implement a full artificial neural network to solve your problem.This chapter coversThe use of nonlinear activation functions as the key difference from linear modelsThe many kinds of activation functions in common usePyTorch’s nn module, containing neural network building blocksSolving a simple linear-fit problem with a neural networkFigure 5.1 Mental model of the learning process
102CHAPTER 5 Using a neural network to fit your data
Your thermometer conversion training loop and Fahrenheit-to-Celsius samples split into training and validation sets remain. You could start to use a quadratic model, rewriting your model as a quadratic function of its input (such as y = a * x**2 + b * x + c). Because such a model would be differentiable, PyTorch would take care of computing gradients, and the training loop would work as usual. That wouldn’t be too interesting for you, though, because you’d still be fixing the shape of the function. This chapter is where you start to hook the foundational work you’ve put in with the PyTorch features you’ll be using day in and day out as you work on your projects. You’ll have an understanding of what’s going on underneath the porcelain of the PyTorch API rather than thinking that it’s so much black magic. Before we get into the implementation of the new model, though, we’ll explain what we mean by artificial neural network.5.1 Artificial neuronsAt the core of deep learning are neural networks, mathematical entities capable of rep-resenting complicated functions through a composition of s i m p l e r  f u n c t i o n s .  T h e term neural network obviously suggests a link to the way the human brain works. As a matter of fact, although the initial models were inspired by neuroscience,1 modern 1http://psycnet.apa.org/doiLanding?doi=10.1037%2Fh0042519103Artificial neuronsartificial neural networks bear only a vague resemblance to the mechanisms of neu-rons in the brain. It seems likely that artificial and physiological neural networks use vaguely similar mathematical strategies for approximating complicated functions because that family of strategies works effectively.NOTE W e ’ r e  g o i n g  t o  d r o p  artificial and refer to these constructs as neural net-works from here on.The basic building block of these complicated functions is the neuron, pictured in figure 5.2. At its core, a neuron is nothing but a linear transformation of the input (such as multiplication of the input by a number [the weight] and the addition of a constant [the bias]) followed by the application of a fixed nonlinear function (referred to as the activation function). 
Figure 5.2 An artificial neuron: a linear transformation enclosed in a nonlinear function
 Mathematically, you can write this as o = f(w * x + b), with x as the input, w as the weight or scaling factor, and b as the bias or offset. f is the activation function, set to the hyperbolic tangent or “tanh” function here. In general, x and hence o can be simple scalars, or vector-valued (holding many scalar values). Similarly, w can be a sin-gle scalar, or a matrix, whereas b is a scalar or vector (the dimensionality of the inputs and weights must match, however). In the latter case, the expression is referred to as a 104CHAPTER 5 Using a neural network to fit your datalayer of neurons because it represents many neurons via the multidimensional weights and biases. A multilayer neural network, as represented in figure 5.3, is a composition of the preceding functions:x_1 = f(w_0 * x + b_0)x_2 = f(w_1 * x_1 + b_1)...y = f(w_n * x_n + b_n)where the output of a layer of neurons is used as an input for the following layer. Remember that w_0 here is a matrix, and x is a vector! Using a vector here allows w_0to hold an entire layer of neurons, not just a single weight.
Figure 5.3 A neural network with three layers
An important difference between the earlier linear model and what you’ll be using for deep learning is the shape of the error function. The linear model and error-squared loss function had a convex error curve with a singular, clearly defined minimum. If you were to use other methods, you could solve for it automatically and definitively. Your parame-ter updates were attempting to estimate that singular correct answer as best they could. N e u r a l  n e t w o r k s  d o n ’ t  h a v e that same property of a convex error surface, even when using the same error-squared loss function. There’s no single right answer for each parameter that you’re attempting to approximate. Instead, you’re trying to get all the parameters, when acting in concert, to produce useful output. Since that useful 105Artificial neuronsoutput is only going to approximate the truth, there will be some level of imperfec-tion. Where and how those imperfections manifest is somewhat arbitrary, and by implication the parameters that control the output (and hence the imperfections) are somewhat arbitrary as well. This output results in neural network training’s looking much like parameter estimation from a mechanical perspective, but you must remem-ber that the theoretical underpinnings are quite different. A big part of the reason why neural networks have nonconvex error surfaces is due to the activation function. The ability of an ensemble of neurons to approximate a wide range of useful functions depends on the combination of the linear and nonlin-ear behavior inherent to each neuron.5.1.1 All you need is activationThe simplest unit in (deep) neural networks is a linear operation (scaling + offset) fol-lowed by an activation function. You had a linear operation in your latest model; the linear operation was the entire model. The activation function has the role of concen-trating the outputs of the preceding linear operation into a given range. Suppose that you’re assigning a “good doggo” score to images. Pictures of retriev-ers and spaniels should have a high score; images of airplanes and garbage trucks should have a low score. Bear pictures should have a low-ish score too, though higher than garbage trucks. The problem is that you have to define a high score. Because you’ve got the entire range of float32 to work with, you can go pretty high. Even if you say “It’s a ten point scale,” sometimes your model is going to produce a score of 11 out of 10. Remember that under the hood, it’s all sum of w*x+b matrix multiplications, which won’t natu-rally limit themselves to a specific range of outputs. What you want to do is firmly constrain the output of your linear operation to a specific range so that the consumer of this output isn’t having to handle numerical inputs of puppies at 12/10, bears at -10, and garbage trucks at -1000. One possibility is to cap the output values. Anything below zero is set to zero, and anything above 10 is set to 10. You use a simple activation function called torch.nn.Hardtanh.2 Another family of functions that works well is torch.nn.Sigmoid, which is 1 / (1 + e ** -x), torch.tanh, and others that you’ll see in a moment. These functions have a curve that asymptotically approaches zero or negative one as x goes to negative infinity, approaches one as x increases, and has a mostly constant slope at x == 0. Con-ceptually, functions shaped this way work well, because it means that your neuron (which, again, is a linear function followed by an activation) will be sensitive to an area in the middle of your linear function’s output; everything else gets lumped up next to the boundary values. As you see in figure 5.4, a garbage truck gets a score of -0.97, whereas bears, foxes, and wolves may end up somewhere in the -0.3 to 0.3 range.2See https://pytorch.org/docs/stable/nn.html#hardtanh, but note that the default range is -1 to +1.Figure 5.4 Dogs, bears, and garbage trucks being mapped to “how doglike” via the tanh activation function
106CHAPTER 5 Using a neural network to fit your data
Garbage trucks are flagged as “not dogs,” the good dog maps to “clearly a dog,” and the bear ends up somewhere in the middle. In code, you see the exact values:>>> import math>>> math.tanh(-2.2)  -0.9757431300314515>>> math.tanh(0.1)   0.09966799462495582>>> math.tanh(2.5)   0.9866142981514303With the bear in the sensitive range, small changes to the bear result in a noticeable change in the result. You could swap from a  g r i z z l y  t o  a  p o l a r  b e a r  ( w h i c h  h a s  a vaguely more traditionally canine face) and see a jump up the Y axis as you slide toward the “very much a dog” end of the graph. Conversely, a koala bear would regis-ter as less doglike and would see a drop in the activated output. There isn’t much you could do to the garbage truck to make it register as doglike, though. Even with drastic changes, you might see a shift only from -0.97 to -0.8 or so. Quite a few activation functions are available, some of which are pictured in figure 5.5. In the first column, you see the continuous functions Tanh and Softplus; the sec-ond column has “hard” versions of the activation functions to their left, Hardtanh and ReLU. ReLU (Rectified Linear Unit) deserves special note, as it is currently considered to be one of the best-performing general activation functions, as many state-of-the-art results have used it. The Sigmoid activation function, also known as the logistic func-Garbage truckBearGood doggo107Artificial neurons
tion, was widely used in early deep learning work but has fallen out of common use. Finally, the LeakyReLU function modifies the standard ReLU to have a small positive slope rather than being strictly zero for negative inputs. (Typically, this slope is 0.01, but it’s shown here with slope 0.1 for clarity.)Figure 5.5 A collection of common and not-so-common activation functions
 Activation functions are curious, because with such a wide variety of proven-suc-cessful ones (many more than pictured in figure 5.5), it’s clear that there are few, if any, strict requirements. As such, we’re going to discuss some generalities about activa-tion functions that can probably be disproved in the specific. That said, by definition, activation functions3Are nonlinear—Repeated applications of w*x+b w i t h o u t  a n  a c t i v a t i o n  f u n c t i o n results in a polynomial. The nonlinearity allows the overall network to approxi-mate more complex functions.Are differentiable—They’re differentiable so that gradients can be computed through them. Point discontinuities, as you see in Hardtanh or ReLU, are fine.Without those functions, the network either falls back to being a complicated polyno-mial or becomes difficult to train.3Even these statements aren’t always true, of course: See https://openai.com/blog/nonlinear-computation-in-linear-networks.108CHAPTER 5 Using a neural network to fit your data Also generally true (though less so), the functionsHave at least one sensitive range, where no-trivial changes to the input result in a corresponding nontrivial change in the output.Have at least one insensitive (or saturated) range, where changes to the input result in little to no change in the output.By way of example, the Hardtanh function could easily be used to make piecewise-lin-ear approximations of a function due to combining the sensitive range with different weights and biases on the input. Often (but far from universally so), the activation function has at least one of the following:A lower bound that is approached (or met) as the input goes to negative infinityA similar-but-inverse upper bound for positive infinityThinking about what you know about how backpropagation works, you can figure out that the errors will propagate backward through the activation more effectively when the inputs are in the response range, whereas errors won’t greatly affect neurons for which the input is saturated (because the gradient will be close to zero due to the flat area around the output). All put together, this mechanism is pretty powerful. What we’re saying is that in a network built out of linear + activation units, when different inputs are presented to the network, (a) different units respond in different ranges for the same inputs, and (b) the errors associated with those inputs will primarily affect the neurons operating in the sensitive range, leaving other units more or less unaffected by the learning pro-cess. In addition, thanks to the fact that derivatives of the activation with respect to its inputs are often close to one in the sensitive range, estimating the parameters of the linear transformation through gradient descent for the units that operate in that range will look a lot like the linear fit. You’re starting to get a deeper intuition about how joining many linear + activation units in parallel and stacking them one after the other leads to a mathematical object that is capable of approximating complicated functions. Different combinations of units respond to inputs in different ranges and for those parameters are relatively easy to optimize through gradient descent, because learning will behave a lot like that of a linear function until the output saturates.5.1.2 What learning means for a neural networkBuilding models out of stacks of linear transformations followed by differentiable acti-vations leads to models that can approximate highly nonlinear processes whose parameters you can estimate surprisingly well through gradient descent. This fact remains true even when you’re dealing with models with millions of parameters. What makes using deep neural networks so attractive is that it allows you not to worry too much about the exact function that represents your data, whether it’s quadratic, piecewise polynomial, or something else. With a deep neural network model, you 109Artificial neuronshave a universal approximator and a method to estimate its parameters. This approxi-mator can be customized to your needs, in terms of model capacity and its ability to model complicated input/output relationships, by composing simple building blocks. Figure 5.6 shows some examples.
Figure 5.6 Composing multiple linear units and tanh activation functions to produce nonlinear outputs
 The four top-left graphs show four neurons—A, B, C, and D—each with its own (arbitrarily chosen) weight and bias. Each neuron uses the Tanh activation function, with a minimum of -1 and a maximum of 1. The varied weights and biases move the center point and change how drastically the transition from min to max goes, but they clearly are all the same general shape. The columns to the right show both pairs of neurons added together (A+B and then C+D). Here, you start to see some interesting properties that mimic a single layer of neurons. A+B shows a slight S curve, with the extremes approaching zero, but both a positive and negative bump in the middle. Conversely, C+D has only a large positive bump, which peaks at a higher value than the single-neuron max of 1. In the third row, you start to compose your neurons as they would be in a two-layer network. Both C(A+B) and D(A+B) have the same positive-and-negative-bumps that A+B shows, but the positive peak is more subtle. The composition of C(A+B)+D(A+B) shows a new property: two clear negative bumps and possibly a subtle second positive peak to the left of the main area of interest. All this occurs with only four neurons in two layers!110CHAPTER 5 Using a neural network to fit your data Again, these neurons’ parameters were chosen only to create a visually interesting result. Training consists of finding acceptable values for these weights and biases so that the resulting network carries out a task correctly, such as predicting likely tem-peratures given geographic coordinates and time of the year. By carrying out a task suc-cessfully, we mean obtaining a correct output on unseen data produced by the same data-generating process used for training data. A successfully trained network, through the value of its weights and biases, captures the inherent structure of the data in the form of meaningful numerical representations that work correctly for previ-ously unseen data. Here’s another step in your realization of the mechanics of learning: deep neural networks allow you to approximate highly nonlinear phenomena without having an explicit model for them. Instead, starting from a generic, untrained model, you spe-cialize it on a task by providing it a set of inputs and outputs and a loss function to backpropagate from. Specializing a generic model to a task by using examples is what we refer to as learning, because the model wasn’t built with that specific task in mind; no rules describing how that task worked were encoded in the model. F o r  y o u r  t h e r m o m e t e r  e x p e r i e n c e ,  y ou assumed that both thermometers mea-sured temperatures linearly. That assumption is where we implicitly encoded a rule for our task: we hard-coded the shape of our input/output function; we couldn’t have approximated anything other than data points sitting around a line. As the dimen-sionality of a problem grows (many inputs to many outputs) and input/output rela-tionships get complicated, assuming a shape for the input/output function is unlikely to work. The job of a physicist or an applied mathematician is often to come up with a functional description of a phenomenon from first principles so that it’s possible to estimate the unknown parameters from measurements and get an accurate model of the world. Deep neural networks, at the other end, are families of functions that can approximate a wide range of input/output relationships without necessarily requiring one to come up with an explanatory model of a phenomenon. In a way, you’re renouncing an explanation in exchange for the possibility of tackling increasingly complicated problems. In another way, you sometimes lack the ability, information, or computational resources to build an explicit model of what you’re presented with, so data-driven methods are your only way forward.5.2 The PyTorch nn moduleAll this talk about neural networks may be getting you curious about building one from scratch with PyTorch. The first step is replacing your linear model with a neural network unit. This step is a somewhat-useless step backward from a correctness per-spective, because you’ve already verified that your calibration required only a linear function, but it’ll still be instrumental for starting a sufficiently simple problem and scaling up later. PyTorch has a whole submodule dedicated to neural networks: torch.nn. This sub-module contains the building blocks needed to create all sorts of neural network 111The PyTorch nn modulearchitectures. Those building blocks are called modules in PyTorch parlance (and layersin other frameworks). A  P y T o r c h  m o d u l e  i s  a  P y t h o n  c l a s s  d e r i v i n g  f r o m  t h e  nn.Module b a s e  c l a s s . A Module can have one or more Parameter instances as attributes, which are tensors whose values are optimized during t h e  t r a i n i n g  p r o c e s s .  ( T h i n k  w and b in your linear model.) A Module can also have one or more submodules (subclasses of nn.Module) as attributes, and it can track their Parameters as well.NOTE T h e  s u b m o d u l e s  m u s t  b e  t o p - l e v e l  attributes, not buried inside list or dict instances! Otherwise, the optimizer won’t be able to locate the submod-ules (and, hence, their parameters). For situations in which your model requires a list or dict of submodules, PyTorch provides nn.ModuleList and nn.ModuleDict.Unsurprisingly, you can find a subclass of nn.Module called nn.Linear, which applies an affine transformation to its input (via the parameter attributes weight and bias); it’s equivalent to what you implemented earlier in your thermometer experiments. Now start precisely where you left off and convert your previous code to a form that uses nn. A l l  P y T o r c h - p r o v i d e d  s u b c l a s s e s  o f  nn.Module h a v e  t h e i r  call m e t h o d  d e f i n e d , which allows you to instantiate an nn.Linear and call it as though it were a function, as in the following listing.# In[5]:import torch.nn as nnlinear_model = nn.Linear(1, 1) linear_model(t_un_val)# Out[5]:tensor ([[-0.9852],        [-2.6876]], grad_fn=<AddmmBackward>)Calling an instance of nn.Module with a set of arguments ends up calling a method named forward with the same arguments. The forward method executes the forward computation; call d o e s  o t h e r  r a t h e r  i m p o r t a n t  c h o r e s  b e f o r e  a n d  a f t e r  c a l l i n g forward. So it’s technically possible to call forward directly, and it produces the same output as call, but it shouldn’t be done from user code:>>> y = model(x)         Correct!>>> y = model.forward(x) Silent error. Don’t do it!The following listing shows the implementation of Module.call (with some simplifi-cations made for clarity).Listing 5.1 code/p1ch6/1_neural_networks.ipynbYou look into the constructor arguments in a moment.112CHAPTER 5 Using a neural network to fit your datadef __call__(self, *input, **kwargs):    for hook in self._forward_pre_hooks.values():        hook(self, input)    result = self.forward(*input, **kwargs)    for hook in self._forward_hooks.values():        hook_result = hook(self, input, result)        # ...    for hook in self._backward_hooks.values():        # ...    return resultAs you can see, a lot of hooks won’t get called properly if you use .forward(…)directly. Now turn back to the linear model. The constructor to nn.Linear accepts three arguments: the number of input features, the number of output features, and whether the linear model includes a bias (defaulting to True here).# In[5]:import torch.nn as nnlinear_model = nn.Linear(1, 1)	linear_model(t_un_val)# Out[5]:tensor([[-0.9852],         [-2.6876]], grad_fn=<AddmmBackward>) The number of features in this case refers to the size of the input and the output tensor for the module, so 1 and 1. If you used both temperature and barometric pres-sure in input, for example, you’d have two features in input and one feature in out-put. As you’ll see, for more complex models with several intermediate modules, the number of features is associated with the capacity of the model. You have an instance of nn.Linear with one input and one output feature, which requires one weight and one bias:# In[6]:linear_model.weight# Out[6]:Parameter containing:tensor([[-0.4992]], requires_grad=True)# In[7]:linear_model.bias# Out[7]:Parameter containing:tensor([0.1031], requires_grad=True)Listing 5.2 torch/nn/modules/module.py, line 483, class: Module
The arguments are input size, output size, and bias defaulting to True.113The PyTorch nn moduleYou can call the module with some input:# In[8]:x = torch.ones(1)linear_model(x)# Out[8]:tensor([-0.3961], grad_fn=<AddBackward0>) Although PyTorch let you get away with it, you didn’t provide an input with the right dimensionality. You have a model that takes one input and produces one output, but PyTorch nn.Module and its subclasses are designed to do so on multiple samples at the same time. To accommodate multiple samples, modules expect the zeroth dimen-sion of the input to be the number of samples in the batch. Any module in nn is written to produce outputs for a batch of multiple inputs at the same time. Thus, assuming that you need to run nn.Linear on 10 samples, you can create an input tensor of size B x Nin, where B is the size of the batch and Nin the number of input features, and run it once through the model:# In[9]:x = torch.ones(10, 1)linear_model(x)# Out[9]:tensor([[-0.3961],        [-0.3961],        [-0.3961],        [-0.3961],        [-0.3961],        [-0.3961],        [-0.3961],        [-0.3961],        [-0.3961],        [-0.3961]], grad_fn=<AddmmBackward>)Figure 5.7 shows a similar situation with batched image data. The input is BxCxHxWwith a batch size of three (say, images of a dog, bird, and then car), three channel dimensions (red, green, and blue), and an unspecified number of pixels for height and width. As you can see, the output is a tensor of size B x Nout, where Nout is the number of output features—four, in this case. The reason we want to do this batching is multi-faceted. One big motivation is to make sure that the computation we’re asking for is big enough to saturate the comput-ing resources we’re using to perform the computation. GPUs in particular are highly parallelized, so a single input on a small model will leave most of the computing units idle. By providing batches of inputs, the calculation can be spread across the otherwise-idle units, which means that the batched results come back just as quickly as a single one would. Another benefit is that some advanced models will use statistical informa-tion from the entire batch, and those statistics get better with larger batch sizes.Figure 5.7 Three RGB images batched together and fed into a neural network. The output is a batch of three vectors of size 4.
114CHAPTER 5 Using a neural network to fit your data
Now turn back to the thermometer data. Your t_u and t_c were two 1D tensors of size B. Thanks to broadcasting, you could write your linear model as w * x + b, where w and b are two scalar parameters. This model works because you have one input fea-ture; if you had two, you’d need to add an extra dimension to turn that 1D tensor into a matrix with samples in the rows and features in the columns. That’s exactly what you need to do to switch to using nn.Linear. You reshape your B inputs to B x Nin, where Nin is 1. You can easily do this with unsqueeze:# In[2]:t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]t_c = torch.tensor(t_c).unsqueeze(1) t_u = torch.tensor(t_u).unsqueeze(1) t_u.shape# Out[2]:torch.Size([11, 1])You’re done. Now update your training code. First, replace your handmade model with nn.Linear(1,1); then pass the linear model parameters to the optimizer:# In[10]:linear_model = nn.Linear(1, 1) optimizer = optim.SGD(    linear_model.parameters(),     lr=1e-2)Here, you add the extra dimension at axis 1.
A redefinition from above.You replace [params] with this method call.115The PyTorch nn moduleEarlier, it was your responsibility to create parameters and pass them as the first argu-ment to optim.SGD. Now you can ask any nn.Module for a list of parameters owned by it or any of its submodules by using the parameters method:# In[11]:linear_model.parameters()# Out[11]:<generator object Module.parameters at 0x0000020A2B022D58># In[12]:list(linear_model.parameters())# Out[12]:[Parameter containing: tensor([[0.3791]], requires_grad=True), Parameter containing: tensor([-0.5349], requires_grad=True)]This call recurses into submodules defined in the module’s init c o n s t r u c t o r  a n d returns a flat list of all parameters encountered, so you can conveniently pass it to the optimizer constructor as you did earlier. You can already figure out what happens in the training loop. The optimizer is pro-vided a list of tensors that were defined with requires_grad = True. All Parameters are defined this way, by definition, because they need to be optimized by gradient descent. When training_loss.backward() is called, grad is accumulated on the leaf nodes of the graph, which are precisely the parameters that were passed to the optimizer. At this point, the SGD optimizer has everything it needs. When optimizer.step()is called, it iterates through each Parameter a n d  c h a n g e s  i t  b y  a n  a m o u n t  p r o p o r-tional to what is stored in its grad attribute, which is clean design. Take a look at the training loop now:# In[13]:def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val, t_c_train, t_c_val):    for epoch in range(1, n_epochs + 1):        t_p_train = model(t_un_train)         loss_train = loss_fn(t_p_train, t_c_train)        t_p_val = model(t_un_val)         loss_val = loss_fn(t_p_val, t_c_val)        optimizer.zero_grad()        loss_train.backward()        optimizer.step()        if epoch == 1 or epoch % 1000 == 0:            print('Epoch {}, Training loss {}, Validation loss {}'.format(                epoch, float(loss_train), float(loss_val)))The training loop hasn’t changed practically except that now you don’t pass paramsexplicitly to model because the model itself holds its Parameters internally.Now the model is passed in instead of the individual params.The loss function is also passed in. You’ll use it in a moment.116CHAPTER 5 Using a neural network to fit your data You can use one last bit from torch.nn: the loss. Indeed, nn comes with several common loss functions, among which nn.MSELoss (MSE stands for Mean Square Error), which is exactly what you defined earlier as your loss_fn. Loss functions in nn are still subclasses of nn.Module, so create an instance and call it as a function. In this case, you get rid of the handwritten loss_fn and replace it:# In[15]:linear_model = nn.Linear(1, 1)optimizer = optim.SGD(linear_model.parameters(), lr=1e-2)training_loop(    n_epochs = 3000,    optimizer = optimizer,    model = linear_model,    loss_fn = nn.MSELoss(),     t_u_train = t_un_train,    t_u_val = t_un_val,    t_c_train = t_c_train,    t_c_val = t_c_val)print()print(linear_model.weight)print(linear_model.bias)# Out[15]:Epoch 1, Training loss 92.3511962890625, Validation loss 57.714385986328125Epoch 1000, Training loss 4.910993576049805, Validation loss 1.173926591873169Epoch 2000, Training loss 3.014694929122925, Validation loss 2.8020541667938232Epoch 3000, Training loss 2.857640504837036, Validation loss 4.464878559112549Parameter containing:tensor([[5.5647]], requires_grad=True)Parameter containing:tensor([-18.6750], requires_grad=True)Everything else input into our training loop stays the same. Even our results remain the same as before. Of course, getting the same results is expected, as a difference would imply a bug in one of the two implementations. It’s been a long journey, with a lot to explore for these twenty-something lines of code. We hope that by now, the magic has vanished and left room for the mechanics. What you learn in this chapter will allow you to own the code you write instead of merely poking at a black box when things get more complicated. You have one last step left to take: replacing your linear model with a neural net-work as your approximating function. As we said earlier, using a neural network won’t result in a higher-quality model, because the process underlying the calibration prob-lem is fundamentally linear. It’s good to make the leap from linear to neural network in a controlled environment, however, so that you won’t feel lost later on.You’re no longer using your handwritten loss function from earlier.117The PyTorch nn module This section keeps everything else fixed, including the loss function, and redefines only the model. You’ll build the simplest possible neural network: a linear module fol-lowed by an activation function feeding into another linear module. The first linear + activation layer is commonly referred to as a hidden l a y e r  f o r  h i s t o r i c a l  r e a s o n s , because its outputs aren’t observed directly but fed into the output layer. Whereas the input and the output of the model are both of size 1 (they have one input and one output feature), the size of the output of the first linear module usually is larger than one. Recalling the earlier explanation on the role of activations, this situation can lead different units to respond to different ranges of the input, which increases the capacity of the model. The last linear layer takes the output of activations and com-bines them linearly to produce the output value. nn provides a simple way to concatenate modules through the nn.Sequential con-tainer:# In[16]:seq_model = nn.Sequential(            nn.Linear(1, 13),             nn.Tanh(),            nn.Linear(13, 1)) seq_model# Out[16]:Sequential(  (0): Linear(in_features=1, out_features=13, bias=True)  (1): Tanh()  (2): Linear(in_features=13, out_features=1, bias=True))The result is a model that takes the inputs expected by the first module specified as an argument of nn.Sequential, passes intermediate outputs to subsequent modules, and produces the output returned by the last module. The model fans out from 1 input feature to 13 hidden features, passes them through a tanh a c t i v a t i o n ,  a n d  l i n e a r l y combines the resulting 13 numbers into 1 output feature. Calling model.parameters() collects weight and bias from both the first and the second linear modules. It’s instructive to inspect the parameters in this case by print-ing their shapes:# In[17]:[param.shape for param in seq_model.parameters()]# Out[17]:[torch.Size([13, 1]), torch.Size([13]), torch.Size([1, 13]), torch.Size([1])]These are the tensors that the optimizer will get. Again, after model.backward() is called, all parameters are populated with their grad, and then the optimizer updates their values accordingly during the optimizer.step() call, which isn’t too different from the previous linear model. After all, both models are differentiable models that can be trained with gradient descent.13 was chosen arbitrarily. We wanted to pick a number that was a different size from the other various tensor shapes floating around.This 13 must match the first size, however.118CHAPTER 5 Using a neural network to fit your data A few notes on parameters of nn.Modules: when you’re inspecting parameters of a model made up of several submodules, it’s handy to be able to identify parameters by their names. There’s a method for that, called named_parameters:# In[18]:for name, param in seq_model.named_parameters():    print(name, param.shape)# Out[18]:0.weight torch.Size([13, 1])0.bias torch.Size([13])2.weight torch.Size([1, 13])2.bias torch.Size([1])In fact, the name of each module in Sequential is the ordinal with which the module appeared in the arguments. Interestingly, Sequential also accepts an OrderedDict4 in which you can name each  module passed to Sequential:# In[19]:from collections import OrderedDictseq_model = nn.Sequential(OrderedDict([    ('hidden_linear', nn.Linear(1, 8)),    ('hidden_activation', nn.Tanh()),    ('output_linear', nn.Linear(8, 1))]))seq_model# Out[19]:Sequential(  (hidden_linear): Linear(in_features=1, out_features=8, bias=True)  (hidden_activation): Tanh()  (output_linear): Linear(in_features=8, out_features=1, bias=True))This code allows you to get more explanatory names for submodules:# In[20]:for name, param in seq_model.named_parameters():    print(name, param.shape)# Out[20]:hidden_linear.weight torch.Size([8, 1])hidden_linear.bias torch.Size([8])output_linear.weight torch.Size([1, 8])output_linear.bias torch.Size([1])You can also get to a particular Parameter by accessing submodules as though they were attributes:4Not all versions of Python specify the iteration order for dict, so we’re using OrderedDict here to ensure the ordering of the layers and emphasize that the order of the layers matters.119The PyTorch nn module# In[21]:seq_model.output_linear.bias# Out[21]:Parameter containing:tensor([-0.2194], requires_grad=True)This code is useful for inspecting parameters or their gradients, such as to monitor gradients during training, as you did the beginning of this chapter. Suppose that you want to print out the gradients of weight of the linear portion of the hidden layer. You can run the training loop for the new neural network model and then look at the resulting gradients after the last epoch:# In[22]:optimizer = optim.SGD(seq_model.parameters(), lr=1e-3) training_loop(    n_epochs = 5000,    optimizer = optimizer,    model = seq_model,    loss_fn = nn.MSELoss(),    t_u_train = t_un_train,    t_u_val = t_un_val,    t_c_train = t_c_train,    t_c_val = t_c_val)print('output', seq_model(t_un_val))print('answer', t_c_val)print('hidden', seq_model.hidden_linear.weight.grad)# Out[22]:Epoch 1, Training loss 207.2268524169922, Validation loss 106.6062240600586Epoch 1000, Training loss 6.121204376220703, Validation loss 2.213937759399414Epoch 2000, Training loss 5.273784637451172, Validation loss 0.0025627268478274345Epoch 3000, Training loss 2.4436306953430176, Validation loss 1.9463319778442383Epoch 4000, Training loss 1.6909029483795166, Validation loss 4.027190685272217Epoch 5000, Training loss 1.4900192022323608, Validation loss 5.368413925170898output tensor([[-1.8966],        [11.1774]], grad_fn=<AddmmBackward>)answer tensor([[-4.],        [14.]])hidden tensor([[-0.0073],        [ 4.0584],        [-4.5080],        [-4.4498],        [ 0.0127],        [-0.0073],        [-4.1530],        [-0.6572]])Note that the learning rate has dropped a bit to help with stability.120CHAPTER 5 Using a neural network to fit your dataYou can also evaluate the model on the whole data to see how different it is from a line:# In[23]:from matplotlib import pyplot as pltt_range = torch.arange(20., 90.).unsqueeze(1)fig = plt.figure(dpi=600)plt.xlabel("Fahrenheit")plt.ylabel("Celsius")plt.plot(t_u.numpy(), t_c.numpy(), 'o')plt.plot(t_range.numpy(), seq_model(0.1 * t_range).detach().numpy(), 'c-')plt.plot(t_u.numpy(), seq_model(0.1 * t_u).detach().numpy(), 'kx')This code produces figure 5.8.
Figure 5.8 The plot of the neural network model, with input data (circles), desired output (Xs), and continuous line showing behavior between samples
You can appreciate that the neural network has a tendency to overfit because it tries to chase the measurements, including the noisy ones. It doesn’t do a bad job overall, though.5.3 Subclassing nn.ModuleFor larger and more complex projects, you need to leave nn.Sequential behind in favor of something that gives you more flexibility: subclassing nn.Module. To subclass nn.Module, at a minimum you need to define a .forward(…)function that takes the input to the module and returns the output. If you use standard torch operations, autograd takes care of the backward pass automatically.NOTE O f t e n ,  y o u r  e n t i r e  model i s  i m p l e m e n t e d  a s  a  s u b c l a s s  o f  nn.Module, which can in turn contain submodules that are also subclasses of nn.Module.121Subclassing nn.ModuleWe’ll show you three ways to implement the same network structure, using increas-ingly more complex PyTorch functionality to do so and varying the number of neu-rons in the hidden layer to make it easier to differentiate among the approaches. The first method is a simple instance of nn.Sequential, as shown in the following listing.# In[2]:seq_model = nn.Sequential(            nn.Linear(1, 11),             nn.Tanh(),            nn.Linear(11, 1)) seq_model# Out[2]:Sequential(  (0): Linear(in_features=1, out_features=11, bias=True)  (1): Tanh()  (2): Linear(in_features=11, out_features=1, bias=True))Although this code works, you don’t have any semantic information about what the various layers are intended to be. You can rectify that situation by giving each layer a label, using an ordered dictionary instead of a list as input:# In[3]:from collections import OrderedDictnamedseq_model = nn.Sequential(OrderedDict([    ('hidden_linear', nn.Linear(1, 12)),    ('hidden_activation', nn.Tanh()),    ('output_linear', nn.Linear(12 , 1))]))namedseq_model# Out[3]:Sequential(  (hidden_linear): Linear(in_features=1, out_features=12, bias=True)  (hidden_activation): Tanh()  (output_linear): Linear(in_features=12, out_features=1, bias=True))Much better. You don’t have any ability to control the flow of data through the net-work, however, aside from the purely sequential pass-through provided by the (aptly named!) nn.Sequential c l a s s .  Y o u  c a n  t a k e  f u l l  c o n t r o l  o f  t h e  p r o c e s s i n g  o f  i n p u t data by subclassing nn.Module yourself:# In[4]:class SubclassModel(nn.Module):    def __init__(self):        super().__init__()Listing 5.3 code/p1ch6/3_nn_module_subclassing.ipynbThe choice of 11 is somewhat arbitrary, but the sizes of the two layers must match.122CHAPTER 5 Using a neural network to fit your data        self.hidden_linear = nn.Linear(1, 13)        self.hidden_activation = nn.Tanh()        self.output_linear = nn.Linear(13, 1)    def forward(self, input):        hidden_t = self.hidden_linear(input)        activated_t = self.hidden_activation(hidden_t)        output_t = self.output_linear(activated_t)        return output_tsubclass_model = SubclassModel()subclass_model# Out[4]:SubclassModel(  (hidden_linear): Linear(in_features=1, out_features=13, bias=True)  (hidden_activation): Tanh()  (output_linear): Linear(in_features=13, out_features=1, bias=True))This code ends up being somewhat more verbose, because you have to define the layers you want to have and then define how and in what order they should be applied in the forward function. That repetition grants you an incredible amount of flexibility in the sequential models, however, as you’re now free to do all sorts of interesting things inside the forward f u n c t i o n .  A l t h o u g h  t h i s  e x a m p l e  i s  u n l i k e l y  t o  m a k e  s e n s e ,  y o u  c o u l d implement activated_t = self.hidden_activation(hidden_t) if random.ran-dom() > 0.5 else hidden_t t o  a p p l y  t h e  a c t i v a t i o n  f unction only half the time! Because PyTorch uses a dynamic graph-based autograd, gradients would flow properly through the sometimes-present activation, no matter what random.random() returned! Typically, you want to use the constructor of the module to define the submodules that we call in the forward function so that they can hold their parameters through-out the lifetime of your module. You might instantiate two instances of nn.Linear in the constructor and use them in forward, for example. Interestingly, assigning an instance of nn.Module to an attribute in a nn.Module, as you did in the constructor here, automatically registers the module as a submodule, which gives modules access to the parameters of its submodules without further action by the user. G o i n g  b a c k  t o  t h e  n o n r a n d o m  SubclassModel, you see that the printed output for that class is similar to the output for the sequential model with named parame-ters. This makes sense, because you used the same names and intended to implement the same architecture. If you look at the parameters of all three models, you also see similarities there (except, again, for the differences in the number of hidden neurons):# In[5]:for type_str, model in [('seq', seq_model), ('namedseq', namedseq_model), ('subclass', subclass_model)]:    print(type_str)    for name_str, param in model.named_parameters():        print("{:21} {:19} {}".format(name_str, str(param.shape), param.numel()))    print()123Subclassing nn.Module# Out[5]:seq0.weight              torch.Size([11, 1]) 110.bias                torch.Size([11])    112.weight              torch.Size([1, 11]) 112.bias                torch.Size([1])     1namedseqhidden_linear.weight  torch.Size([12, 1]) 12hidden_linear.bias    torch.Size([12])    12output_linear.weight  torch.Size([1, 12]) 12output_linear.bias    torch.Size([1])     1subclasshidden_linear.weight  torch.Size([13, 1]) 13hidden_linear.bias    torch.Size([13])    13output_linear.weight  torch.Size([1, 13]) 13output_linear.bias    torch.Size([1])     1What happens here is that the named_parameters() call delves into all submodules assigned as attributes in the constructor and recursively calls named_parameters() on them. No matter how nested the submodule is, any nn.Module can access the list of all child parameters. By accessing their grad attribute, which will have been popu-lated by autograd, the optimizer knows how to change parameters so as to minimize the loss. NOTE C h i l d  m o d u l e s  c o n t a i n e d  i n s i d e  P y t h o n  list or dict instances won’t be registered automatically! Subclasses can register those modules manually with the add_module(name, module) method of nn.Module5 or can use the provided nn.ModuleList a n d  nn.ModuleDict c l a s s e s  ( w h i c h  p r o v i d e  a u t o -matic registration for contained instances).Looking back at the implementation of the SubclassModel class, and thinking about the utility of registering submodules in the constructor so that you can access their parameters, it appears to be a bit of a waste to also register submodules that have no parameters, such as nn.Tanh. Wouldn’t it be easier to call them directly in the forwardfunction?6 It certainly would. P y T o r c h  h a s  functional c o u n t e r p a r t s  o f  e v e r y  nn m o d u l e .  B y  functional, we mean “having no internal state” or “whose output value is solely and fully determined by the value input arguments.” Indeed, torch.nn.functional p r o v i d e s  m a n y  o f  t h e  s a m e modules you find in nn, but with all eventual parameters moved as an argument to the function call. The functional counterpart of nn.Linear, for example, is nn.func-tional.linear, which is a function that has signature linear(input, weight, bias=None). The weight and bias parameters are arguments to the function.5https://pytorch.org/docs/stable/nn.html#torch.nn.Module.add_module6Aren’t rhetorical questions great?124CHAPTER 5 Using a neural network to fit your data To get back to your model, it makes sense to keep using nn modules for nn.Linearso that SubclassModel can to manage all its Parameter instances during training. You can safely switch to the functional counterparts of Tanh, however, because it has no parameters:# In[6]:class SubclassFunctionalModel(nn.Module):    def __init__(self):        super().__init__()        self.hidden_linear = nn.Linear(1, 14)                                                        self.output_linear = nn.Linear(14, 1)    def forward(self, input):        hidden_t = self.hidden_linear(input)        activated_t = torch.tanh(hidden_t)         output_t = self.output_linear(activated_t)        return output_tfunc_model = SubclassFunctionalModel()func_model# Out[6]:SubclassFunctionalModel(  (hidden_linear): Linear(in_features=1, out_features=14, bias=True)  (output_linear): Linear(in_features=14, out_features=1, bias=True))The functional version is a bit more concise and fully equivalent to the non-functional version (as your models get more complicated, the saved lines of code start to add up!) Note that it would still make sense to instantiate modules that require arguments for their initialization in the constructor. HardTanh, for example, takes optional min_val and max_val arguments, and rather than repeatedly state those arguments in the body of forward, you could create a HardTanh instance and reuse it.TIP A l t h o u g h  g e n e r a l - p u r p o s e  s c i e n t i f i c  f u n c t i o n s  l i k e  tanh s t i l l  e x i s t  i n torch.nn.functional i n  v e r s i o n  1 . 0 ,  t h o s e  e n t r y  p o i n t s  a r e  d e p r e c a t e d  i n favor of ones in the top-level torch namespace. More niche functions remain in torch.nn.functional.ConclusionWe covered a lot in this chapter, although we dealt with a simple problem. We dis-sected building differentiable models and training them by using gradient descent, using raw autograd first and then relying on nn. By now, you should have confidence in your understanding of what’s going on behind the scenes. We hope that this taste of PyTorch has given you an appetite for more!The self.hidden_activation = … line is missing here.That line was replaced with the equivalent functional call here.125Subclassing nn.Module
ExercisesExperiment with the number of hidden neurons in your simple neural networkmodel, as well as the learning rate.–W h a t  c h a n g e s  r e s u l t  i n  a  m o r e  linear output from the model?–C a n  y o u  g e t  t h e  m o d e l  t o  o b v i o u s l y  o v e r f i t  t h e  d a t a ?The third-hardest problem in physics is finding a proper wine to celebrate dis-coveries. Load the wine data from chapter 3 and create a new model with theappropriate number of input parameters.–H o w  l o n g  d o e s  i t  t a k e  t o  t r a i n  c o m p a r e d  t o  t h e  t e m p e r a t u r e  d a t a  y o u ’ v ebeen using?–C a n  y o u  e x p l a i n  w h a t  f a c t o r s  c ontribute to the training times?–C a n  y o u  g e t  t h e  l o s s  t o  d e c r e a s e  w h i l e  t r a i n i n g  o n  t h i s  d a t a  s e t ?–H o w  w o u l d  y o u  g o  a b o u t  g r a p h i n g  t h i s  d a t a  s e t ?SummaryNeural networks can be automatically adapted to specialize in the problem athand.Neural networks allow easy access to the analytical derivatives of the loss withrespect to any parameter in the model, which makes evolving the parametersefficient. Thanks to its automated differentiation engine, PyTorch providessuch derivatives effortlessly.Activation functions around linear transformations make neural networks capa-ble of approximating highly nonlinear functions, at the same time keepingthem simple enough to optimize.More resourcesA tremendous number of books and other resources are available to help teach deeplearning. We recommend the following:The official PyTorch website: https://pytorch.orgGrokking Deep Learning, by Andrew W. Traska, is a great resource for developinga strong mental model and intuition on the mechanism underlying deep neural net-works.For a thorough introduction and reference to the field, we direct you to DeepLearning, by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.bLast but not least, the full version of this book is available in Early Access now, withan estimated print date in late 2019: https://www.manning.com/books/deep-learning-with-pytorch.a)https://www.manning.com/books/grokking-deep-learningb)https://www.deeplearningbook.org126CHAPTER 5 Using a neural network to fit your dataThe nn module, together with the tensor standard library, provides all the build-ing blocks for creating neural networks.To recognize overfitting, it’s essential to maintain the training set of data points separate from the validation set. There’s no one recipe to combat overfitting, but getting more data (or more variability in the data) and resorting to simpler models are good starts.Anyone who does data science should be plotting data all the time.127Symbols.storage property 22Aabsolute value, loss function and 73activation function 103and noncovex error surfaces of neural networks 105continuous 106differentiable 107insensitive range 108linear operation and role of 105nonlinearity 107sensitive range 108algorithm, behind machine learning 67applications, enabled by deep learning 15ArXiV public preprint repository, PyTorch and 3ASCII (American Standard Code for Information Interchange) 55automated translation systems, recurrent neural networks and 54automatic differentiation, and computing gradient automatically 7Bbackpropagation 7autograd feature 84basic requirement for 83calling backward 85chain rule 83batching, modules and 113bias parameteradditive constant 71estimation 72neuron 103, 109Brahe, Tycho 68CC++, PyTorch and 10calc_forward function 98cat function, time series and 53categorical data 44and one-hot encoding 58categorical values 44, 65cell, Jupyter Notebook and 13chain rule 83and computing derivative of the loss 76channel dimension, single-channel data formats and 63channel, single, CT scans and 63codeand large collections of numerical data 18updating training code 114color channel 60computation graph 85dynamic 8–9static 6–7computationsfast and massively parallel 34single-neuron 7computer intelligence 2computers, complicated tasks and level of performance 1context manager 98–99contiguous method 28 index128INDEXcontinuous values 43converging optimization 79convex function 72cost function. See loss functionCPU, copying tensor to GPU 34creation ops, tensor API and 36csv module, loading CSV file and 41CT (Computed Tomography) scans 63sample, volread function and loading 63CUDA 10Ddatadeep learning and large amounts of 2different types of 40handled and stored by PyTorch 16linear models and fitting 71, 99loading from file 41numerical values 43–44plotting 82–83Python and loading CSV file 41reshaping 55, 65tabular 40–48time series and making rendering easier 52transformation from one representation to another 15–16utilities for loading and handling 11volumetric 63–64DataLoader class 11Dataset class 11deep learningand applications enabled by 15and deep neural networks as generic functions 70and natural language processing (NLP) 54and wide range of complicated tasks 3as disruptive technology 4as general class of algorithms 2automatically created features and 4change of perspective 4–5competitive landscape of 9–10described 2hardware for 12–13historical overview and current landscape 4–5immediate vs. deferred execution 5–9learning algorithm and producing correct outputs 67models expressed in idiomatic Python 2operation system and 13sources available to help teaching 125Deep Learning with PyTorch Git repository 41derivativesaccumulating at leaf nodes 85computing individual 76gradients 76DICOM (Digital Imaging Communication and Storage) 63dictionaries, one-hot encoding and 57, 65differentiable activations, models and 108Dijkstra, Edsger W. 2diverging optimization 79dtype argument 30–31dynamic graph engine 8Eeager mode 5ecosystem, various niches in 9encodingand embedding 58–59compressing to more manageable size 58numbers into colors 60one-hot encoding. See one-hot encodingpopular encodings 56epoch 78errors, propagating backward through activation 108executiondeferred 6immediatedeep learning libraries and 5Pythagorean theorem and example of 5vs. deffered 5neural networks and 6–9PyTorch and 12Ffeature engineeringdeep learning and 4machine learning and 4features, defined 4file descriptor 32fixed nonlinear function. See activation functionfloating-point numbersand intermediate representations 16and numbers in Python 18and transformation from one form of data to another 15forward method 111functions, deep learning and approximating complex 2129INDEXGGoogle Colab 34GPUs (graphics processing units)and PyTorch tensors transferred to 34moving tensors to 34–35PyTorch tensors and 17support for CUDA 34using multiple 13grad attribute 84–85gradient descentand estimating parameters of highly nonlinear processes 108and large neural network models 74and loss decrease 75differentiable models 117estimating parameters of linear transformation through 108stochastic 89vanilla gradient descent 87gradientscomputing automatically 7dynamic graph-based autograd 122monitoring during training 119zeroing explicitly 85graph libraries, static vs. dynamic 8graph mode 8graphsdynamicadvantage over static graph approaches 9changes during successive forward passes 9symbolic 7Hh5py library 33Hardtanh activation function 106, 108Iimagesand pixels with higher numerical precision 60and the most common channels 60, 65and transpose function 61casting tensor to floating-point 62collection of scalars 60file formats 60geometric transformations 62loading into Python 60multiple scalars per grid point 60normalizing values of pixels 62single scalar per grid point 60tensor preallocation 61image recognition, and transformation from one form of data to another 16imageio module 60indexingadvanced 31, 47range indexing 31tensors and 31indexing, slicing, joining, and mutating ops, tensor API and 36init constructor 115input, converting into floating numbers 15insensitive (saturated) range 108interval values 43, 65JJupyter Notebook 13PyTorch and 13KKepler, Johannesand history of science 69and laws on planetary motion 67–69and learning from data 69Keras 9kernel, code evaluation and 13LLasagne 9LeakyReLU activation function 107learning algorithm 67, 99learning process, mental model of 71, 102learning rates 79learning_rate, as scaling factor in machine learning 75linear operationas the simplest units in neural networks 105constraining output to specific range 105lists, in Pythonas collections of objects 18–19one-dimensional 18passing to the constructor 20losscomputing derivative of 76–77repeated evaluations of 76loss decrease 74–75loss functionand calculation of loss 72130INDEXconvex 72defined 72emphasizing and discounting errors 72in nn 116nn.MSELoss 116penalization terms 94rate of change with respect to model parameters 75, 99repeated evaluations 76scalar 74square difference vs. absolute difference 73Mmachine learningalgorithm behind 67scaling factor 75math ops, tensor API and 36max_val argument 124mean square loss 74minibatch 89min_val argument 124modelsand training neural networks 72as subclasses of nn.Module 120deep learning and generic 69, 99differentiable 83, 102highly adaptable 91linear 71–72convex function 72replacing with neural network units 110, 116loss function 72parameter estimation 72plot of neural network models 120repeated evaluations of 76Module.call, implementation of 111modulesand accommodating multiple samples 113child modules 123in PyTorch 111named_parameters method 118nn and concatenating 117nn.Moduleidentifying parameters by their names 118subclassing 120–124nn.Module base class 111OrderedDict 118submodules 118momentum argument 89multidimensional arrayslibraries dealing with 17transposing and 28See also tensorsNnatural language processing (NLP) 54network structure, ways to implement the same 121–123network, pretrained, running on new data 12neural networksand approximating highly nonlinear phenomena 110and automatic differentiation 7and differences between immediate and deferred execution 6–9and error-squared loss function 104and neurons 7and nonconvex error surfaces 104–105and time series 49–54artificial 102channel 51deep 2and approximating complicated functions 92and exhibiting convex loss 72and transformation from one form of data to another 16and universal approximator 108as families of functions 110as generic functions 70linear operation 105described 102embeddings generated by using 59gradient descent and large neural network models 74input data range and best training performance 62introduction of convolutional 60model function 91moderately large 12multilayer 104operations and parameters in 39recurrent 54successfully trained 110tensors and outputs 39two levels of operation 55what learning means for 108–110neuronsactivation function 103and typical mathematical expression for single 7artificial 103defined 103layer of neurons 104neural networks and 7sensitive range and errors affecting 108131INDEXsingle-neuron computation and dynamic graph 8single-neuron computation and static graph 7symbolic graph 7See also neural networksneuroscience, modern artificial neural networks and 102NLP. See natural language processingnn.Linear, subclass of nn.Module 111–112nn.Sequential container, concatenating modules 117normalization 81numeric encoding, PyTorch and 41numeric typesallocating tensors of the right 30and tensors 30–31dtype argument and 30NumPyand loading SCV file 41as the most popular multidimensional-array library 17interoperability 31–32PyTorch and seamless interoperability with 17NumPy argument, standard, similarity with dtype argument 30NumPy arraysand PyTorch Tensor class 3and similarities with Tensor 2converting to PyTorch tensors 43numpy method 32Oone-hot encoding 52and parsing characters in text 56representing categorical data in tensors and 58OpenCL 34operations, as methods of tensor objects 36optim module 90, 99optimization process 78changing inputs 80loss decrease 75optim submodule 87unstable 79vanilla gradient descent 87optimizers 88–89, 99, 115optimizer.step() 115, 117SGD (Stochastic Gradient Descent) 89, 115testing more 90two methods exposed by 88vanilla gradient descent 89ordinal values 43, 65overfitting 92, 99extreme example of 93how to recognize 126scenarios for training and validation losses 96PPandasand loading CSV file 41concept of data frame 40parallelism ops, tensor API and 37parametersadaptive learning_rate 80and scaling rate of change 75applying updates iteratively 78as PyTorch scalars 73backpropagation and updating 7estimating 72initializing 74small updates 80updating, potential problem 80parameters method 115penalization terms 94points tensor 22Project Gutenberg 55Pythagorean theorem, example od immediate execution and 5PythonHDF5 33computation graph and 6lists as sequential collections of objects 18numbers as full-fledged objects 18PyTorch, described 2Python interpreterPyTorchand non-Python code 10and significant consolidation of deep learning tooling ecosystem 9as introduction to deep learning 2as deep learning library 11autograd 83–87, 99switching off 98autograd-enabled tensor standard library 10automation of generic function-fitting 70creative use 10described 2dynamic graph engine 8functional counterparts 123high-performance C++ runtime 4immediate execution 5–9implementing complicated models 4minimizing cognitive overhead 4, 14main components of 10–12132INDEXmodules as building blocks for creating neural networks 11, 111nn module 110–120official website 125production deployment capabilities 12reasons for using 3–10running directly from C 10seamless interoperability with NumPy 17, 42simplicity of 3smaller-scope projects and 3tensor operations offered by 35Tensor, as multidimensional array 2tensors as building blocks for representing data in 17, 39transposing in 26–28use of class indices 46Rrandom sampling ops, tensor API and 37randperm function 94rate of change 75, 99computing 76recurrent neural networks 54ReLU (Rectified Linear Unit) activation function 106representationdeeper, and capturing more-complex structures 16intermediate 16transforming from one to another 15requires_grad=True argument 84RGB channels 60Sscatter_ methodarguments for 45–46one-hot encoding and 45scoreas continuous variable 44distance between scores 45keeping in separate tensor 44one-hot encoding 44sensitive range 108serialization ops, tensor API and 37shape property 25Sigmoid activation function 106singleton dimension 46size, defined 24Size class 25Softplus activation function 106Stochastic Gradient Descent (SGD) 89Storage, as contiguous, linear container for numbers 52storages.storage property 22accessing for given tensor 22and direct use of storage instances 23changing values of 23defined 22indexing manually 23layout of 23muliple tensors and indexing the same 22Storage instance 22storage offset, defined 24stridechanging the order of elements 27defined 24submodules, registering 123subtensorschanging 26cloning 26extracting 25systems, image-based 60Ttabular dataas the simplest form of data 40as typically non-homogeneous 40described 40sets freely available on the internet 41Tanh activation function 106TensorFlow 8eager mode of 9TensorFlow library 9tensors2D 20accessing 21advanced indexing 31and acceleration of mathematical operations 2and data types represented by 19and defining operations over 19as building blocks for representing data in PyTorch 17as fundamental data structures 17as multidimensional arrays provided by PyTorch 10binary tensor 47compared with NumPy arrays 17contiguous 28conversion into NumPy arrays 31converting data to 43–54CPU- and GPU-based 34defined 18dimensionality of 17133INDEXencoding real-world data, example of 41–48fundamentals 18–22grad attribute 84groups of operations 36–37homogeneous, versus tabular data 40keeping score in separate 44list indexing compared to tensor indexing 18obtaining PyTorch tensors from NumPy arrays 32params as an ancestor 84params receiving too large updates 79relationship among offset, size and stride 24saving 32–34serialization 32–34shuffling elements of 94size, indexing into a storage and 24stride, indexing into a storage and 24subtensors 26tensor numeric values vs. Python object numeric values 20transpose operation applied to 27use of zeros or ones to initialize 21verification of shared storage 26zero-dimensional 73textand recurrent neural networks 54character-level encoding, example of 55–56embedding 58–59encoding 55one-hot encoding 56–58word-level encoding, example of 57Theano library 9time series 49–54breaking up data set in wider observation periods 51calling view on tensors 51concatenation 52data set 50number of samples 51options for rescaling variables 54rearranging tensors 52separate axes 49transformation of 2D data set into 3D data set, example of 49–54torch module 10, 19and operations on and between tensors 36torch.autograd 10torch.distributed 12torch.from_numpy function 34torch.le function 46torch.nn submodule 110torch.nn, and modules for building neural networks 11torch.nn.DataParallel 12torch.nn.Hardtanh activation function 105torch.nn.Sigmoid activation function 105torch.optim 12torch.Storage instances 22torch.tanh activation function 105torch.util.data 11TorchScript, deferred execution model 12training loopepoch 78invoking 78training loss 92overfitting, scenarios for 96training samples 72, 78, 99training set 92, 99and model performance 96backpropagation 96splitting data 94train_is argument 98train_loss, calling backward on 96–97transpose function 36Uuniversal approximator, and modeling complex input/output relationships 109unsqueeze, adding a singleton dimension 46, 52Vvalidation lossevaluating at every epoch 95overfitting 93scenarios for 96training loss and 96validation samples 99validation set 92–93, 96, 99val_loss, calling backward on 97volumetric data 63, 65 Wweight parameterderivative of loss function 77estimation 72linear scaling 71neuron 103, 109updates 80Wikipedia 55Zzero_ method 86Sigmoid Loss for Language Image Pre-Training
Xiaohua Zhai?Basil Mustafa Alexander Kolesnikov Lucas Beyer?
Google DeepMind, Z ¨urich, Switzerland
fxzhai, basilm, akolesnikov, lbeyer g@google.com
Abstract
We propose a simple pairwise Sigmoid loss for
Language-Image Pre-training (SigLIP). Unlike standard
contrastive learning with softmax normalization, the sig-
moid loss operates solely on image-text pairs and does not
require a global view of the pairwise similarities for nor-
malization. The sigmoid loss simultaneously allows fur-
ther scaling up the batch size, while also performing bet-
ter at smaller batch sizes. Combined with Locked-image
Tuning, with only four TPUv4 chips, we train a SigLiT
model that achieves 84.5% ImageNet zero-shot accuracy
in two days. The disentanglement of the batch size from
the loss further allows us to study the impact of exam-
ples vs pairs and negative to positive ratio. Finally, we
push the batch size to the extreme, up to one million, and
ﬁnd that the beneﬁts of growing batch size quickly dimin-
ish, with a more reasonable batch size of 32 k being suf-
ﬁcient. We release our models at https://github.
com/google-research/big_vision and hope our
research motivates further explorations in improving the
quality and efﬁciency of language-image pre-training.
1. Introduction
Contrastive pre-training using weak supervision from
image-text pairs found on the web is becoming the go-to
method for obtaining generic computer vision backbones,
slowly replacing pre-training on large labelled multi-class
datasets. The high-level idea is to simultaneously learn
an aligned representation space for images and texts using
paired data. Seminal works CLIP [36] and ALIGN [23] es-
tablished the viability of this approach at a large scale, and
following their success, many large image-text datasets be-
came available privately [59, 13, 21, 49] and publicly [40,
6, 15, 7, 41].
The standard recipe to pre-train such models leverages
the image-text contrastive objective. It aligns the image and
?equal contributionTable 1: SigLiT and SigLIP results . Sigmoid loss is mem-
ory efﬁcient, allows larger batch sizes (BS) that unlocks
language image pre-training with a small number of chips.
SigLiT model with a frozen public
 B/8 checkpoint [42],
trained on the LiT image-text dataset [59] using four TPU-
v4 chips for one day, achieves 79.7% 0-shot accuracy on
ImageNet. The same setup with a g/14 checkpoint [58]
leads to 84.5% accuracy, trained for two days. With a pub-
lic unlocked
 B/16 image checkpoint [42], trained on the
WebLI dataset [13], SigLIP achieves 71.0% 0-shot accu-
racy using 16 TPU-v4 chips for three days. The last two
rows show results with randomly initialized models.
Image Text BS #TPUv4 Days INet-0
SigLiT
 B/8 L32 k 4 1 79.8
SigLiT
 g/14 L 20 k 4 2 84.5
SigLIP
 B/16 B 16 k 16 3 71.0
SigLIP B/16 B 32 k 32 2 72.1
SigLIP B/16 B 32 k 32 5 73.4
We use a variant of the L model with 12 layers.
text embeddings for matching (positive) image-text pairs
while making sure that unrelated (negative) image-text pairs
are dissimilar in the embedding space. This is achieved via a
batch-level softmax-based contrastive loss, applied twice to
normalize the pairwise similarity scores across all images,
then all texts. A naive implementation of the softmax is
numerically unstable; it is usually stabilized by subtracting
the maximum input value before applying the softmax [18],
which requires another pass over the full batch.
In this paper, we propose a simpler alternative: the sig-
moid loss. It does not require any operation across the full
batch and hence greatly simpliﬁes the distributed loss im-
plementation and boosts efﬁciency. Additionally, it con-
ceptually decouples the batch size from the deﬁnition of
the task. We compare the proposed sigmoid loss with the
standard softmax loss across multiple setups. In partic-
ular, we investigate sigmoid-based loss with two promi-
1nent approaches for image-text learning: CLIP [36] and
LiT [59], which we call sigmoid language image pre-
training ( SigLIP ) and sigmoid LiT ( SigLiT ), respectively.
We ﬁnd that the sigmoid loss performs signiﬁcantly better
than the softmax loss when the batch size is smaller than
16 k. As the train batch size grows, the gap closes. Impor-
tantly, the sigmoid loss is symmetric, requires just a single
pass, and a typical implementation requires less memory
than the softmax loss. This enables successful training of a
SigLiT model at a batch size of one million . However, we
ﬁnd that the performance saturates with growing batch size,
both for softmax and sigmoid. The good news is that a rea-
sonable batch size, i.e. 32 k, is sufﬁcient for image-text pre-
training. This conclusion also holds for multilingual SigLIP
training on over 100 languages.
In Table 1, we present setups for image-text pre-training
that require a moderate amount of TPUv4 chips for training.
SigLiT is surprisingly efﬁcient, reaching 79.7% zero-shot
accuracy on ImageNet in just a single day on four chips.
SigLIP’s more demanding from-scratch training reaches
73.4% zero-shot accuracy in 5 days with 32 TPUv4 chips.
This compares favorably to prior works such as FLIP [30]
and CLIP [36], which require approximately 5 and 10 days
respectively on 256 TPUv3 cores. When ﬁne-tuning a pre-
trained vision backbone in SigLIP, denoted as
 in Table 1,
we found that disabling the weight decay on the pre-trained
backbone leads to better results (see Figure 4 for details).
We hope our work paves the way for making the nascent
language-image pre-training ﬁeld more accessible.
2. Related Work
Contrastive learning with the sigmoid loss. One prior
work proposes a similar sigmoid loss for the task of unsu-
pervised dimensionality reduction [19]; in the scope of con-
trastive image-text learning, the vast majority of works rely
on the softmax-based InfoNCE loss as popularized by [46].
In supervised classiﬁcation, the sigmoid loss has already
been shown to be slightly more effective and robust than
the softmax loss [3, 51].
Contrastive language-image pre-training has become
popular since CLIP [36] and ALIGN [23] applied softmax
contrastive learning [60, 46, 10, 24] to large-scale image-
text datasets. Both models perform very well on zero-shot
transfer tasks, including classiﬁcation and retrieval. Follow-
up works show that contrastively pre-trained models pro-
duce good representations for ﬁne-tuning [53, 16], linear
regression [23], object detection [31], semantic segmenta-
tion [33] and video tasks [57].
Generative language-image pre-training Besides soft-
max contrastive pre-training, various alternatives have been
proposed. GIT [49], SimVLM [50], and LEMON [21] suc-
cessfully pre-train models using a generative text decoderAlgorithm 1 Sigmoid loss pseudo-implementation.
1# img_emb : image model embedding [n, dim]
2# txt_emb : text model embedding [n, dim]
3# t_prime, b : learnable temperature and bias
4# n : mini-batch size
5
6t = exp(t_prime)
7zimg = l2_normalize(img_emb)
8ztxt = l2_normalize(txt_emb)
9logits = dot(zimg, ztxt.T) *t + b
10labels = 2 *eye(n) - ones(n) # -1 with diagonal 1
11l = -sum(log_sigmoid(labels *logits)) / n
instead, while CoCa [56] adds such a decoder to the dis-
criminative CLIP/ALIGN setup, thus combining the pros
and cons of both approaches into a single very capable
model. BLIP [28] further proposes CapFilt which uses the
generative decoder to create better captions and the discrim-
inative part of the model to ﬁlter pairs. Language-Image
pre-training is a very active ﬁeld and surveys [8] rapidly be-
come outdated.
Efﬁcient language-image pre-training On the other hand,
few works have tried making language image pre-training
more efﬁcient. LiT [59] and FLIP [30] are notable attempts,
the former requires a pre-trained and locked backbone, and
the latter sacriﬁces quality by randomly dropping visual to-
kens. BASIC [35] and LAION [52] look at scaling batch-
size but only go up to 16 k and 160 k respectively, by using
many hundreds of chips, and for the former also mixing in a
large private classiﬁcation dataset [35, 55]. The recent Lion
optimizer [12] claims to be able to reduce the training cost
to reach similar quality.
3. Method
In this section, we ﬁrst review the widely-used softmax-
based contrastive loss. We then introduce the pairwise sig-
moid loss and discuss its efﬁcient implementation.
Given a mini-batch B=f(I1;T1);(I2;T2);:::gof
image-text pairs, the contrastive learning objective encour-
ages embeddings of matching pairs (Ii;Ti)to align with
each other, while pushing embeddings of unmatched pairs
(Ii;Tj6=i)apart. For practical purposes, it is assumed that
for all images i, the text associated with a different image j
is not related to i, and vice-versa. This assumption is usu-
ally noisy and imperfect.
3.1. Softmax loss for language image pre-training
When using the softmax loss to formalize this objective,
an image model f()and a text model g()are trained to
2Device 1 Device 2 Device 3
I₁I₂I₃I₄I₅I₆I₇I₈I₉I₁₀I₁₁I₁₂Device 1T₁
T₂
T₃
T₄Device 2T₅
T₆
T₇
T₈Device 3T₉
T₁₀
T₁₁
T₁₂(a) Initially each device holds 4
image and 4 text representations.
Each device needs to see the rep-
resentations from other devices
to calculate the full loss.
Device 1 Device 2 Device 3
I₁I₂I₃I₄I₅I₆I₇I₈I₉I₁₀I₁₁I₁₂Device 1T₁ + –––
T₂ –+ ––
T₃ ––+ –
T₄ –––+Device 2T₅ + –––
T₆ –+ ––
T₇ ––+ –
T₈ –––+Device 3T₉ + –––
T₁₀ –+ ––
T₁₁ ––+ –
T₁₂ –––+
↓↓↓↓↓↓↓↓↓↓↓↓
loss33% 33% 33% 33% 33% 33% 33% 33% 33% 33% 33% 33%
Device 1 Device 2 Device 3(b) They each compute the com-
ponent of the loss (highlighted)
for their representations, which
includes the positives.
Device 1 Device 2 Device 3
I₁I₂I₃I₄I₅I₆I₇I₈I₉I₁₀I₁₁I₁₂Device 3T₁ ✓ ✓ ✓ ✓ ––––
T₂ ✓ ✓ ✓ ✓ ––––
T₃ ✓ ✓ ✓ ✓ ––––
T₄ ✓ ✓ ✓ ✓ ––––Device 1T₅ –––– ✓ ✓ ✓ ✓
T₆ –––– ✓ ✓ ✓ ✓
T₇ –––– ✓ ✓ ✓ ✓
T₈ –––– ✓ ✓ ✓ ✓Device 2T₉ –––– ✓ ✓ ✓ ✓
T₁₀ –––– ✓ ✓ ✓ ✓
T₁₁ –––– ✓ ✓ ✓ ✓
T₁₂ –––– ✓ ✓ ✓ ✓
↓↓↓↓↓↓↓↓↓↓↓↓
loss66% 66% 66% 66% 66% 66% 66% 66% 66% 66% 66% 66%
Device 1 Device 2 Device 3(c) Texts are swapped across the
devices, so device 1 now has I1:4
andT5:8etc. The new loss is
computed and accumulated with
the previous.
Device 1 Device 2 Device 3
I₁I₂I₃I₄I₅I₆I₇I₈I₉I₁₀I₁₁I₁₂Device 2T₁ ✓ ✓ ✓ ✓ –––– ✓ ✓ ✓ ✓
T₂ ✓ ✓ ✓ ✓ –––– ✓ ✓ ✓ ✓
T₃ ✓ ✓ ✓ ✓ –––– ✓ ✓ ✓ ✓
T₄ ✓ ✓ ✓ ✓ –––– ✓ ✓ ✓ ✓Device 3T₅ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ––––
T₆ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ––––
T₇ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ––––
T₈ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ––––Device 1T₉ –––– ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
T₁₀ –––– ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
T₁₁ –––– ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
T₁₂ –––– ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
↓↓↓↓↓↓↓↓↓↓↓↓
loss✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
Device 1 Device 2 Device 3
↘ ↓ ↙
Cross Device Σ(d) This repeats till every image
& text pair have interacted, e.g.
device 1 has the loss of I1:4and
T1:12. A ﬁnal cross-device sum
brings everything together.
Figure 1: Efﬁcient loss implementation demonstrated via a mock setup with 3 devices and a global batch size of 12. There
are no all-gathers, and at any point in time only the bright yellow square (size 44) is materialized in memory.
minimize the following objective:
 1
2jBjjBjX
i=10
BBB@image!text softmaxz}| {
logetxiyi
PjBj
j=1etxiyj+text!image softmaxz}| {
logetxiyi
PjBj
j=1etxjyi1
CCCA
where xi=f(Ii)
kf(Ii)k2andyi=g(Ti)
kg(Ti)k2. In this paper, we
adopt the vision transformer architecture [17] for images
and the transformer architecture [47] for texts. Note that
due to the asymmetry of the softmax loss, the normalization
is independently performed two times: across images and
across texts [36]. The scalar tis parametrized as exp(t0),
wheret0is a global freely learnable parameter.
3.2. Sigmoid loss for language image pre-training
Instead of the softmax-based contrastive loss, we pro-
pose a simpler alternative that does not require computing
global normalization factors. The sigmoid-based loss pro-
cesses every image-text pair independently, effectively turn-
ing the learning problem into the standard binary classiﬁca-
tion on the dataset of all pair combinations, with a positive
labels for the matching pairs (Ii;Ti)and negative labels for
all other pairs (Ii;Tj6=i). It is deﬁned as follows:
 1
jBjjBjX
i=1jBjX
j=1log1
1 +ezij( txiyj+b)
| {z }
Lij
wherezijis the label for a given image and text input, which
equals 1 if they are paired and  1otherwise. At initial-ization, the heavy imbalance coming from the many nega-
tives dominates the loss, leading to large initial optimization
steps attempting to correct this bias. To alleviate this, we
introduce an additional learnable bias term bsimilar to the
temperature t. We initialize t0andbtolog 10 and 10re-
spectively. This makes sure the training starts roughly close
to the prior and does not require massive over-correction.
Algorithm 1 presents a pseudocode implementation of the
proposed sigmoid loss for language image pre-training.
3.3. Efﬁcient “chunked” implementation
Contrastive training typically utilizes data parallelism.
Computing the loss when data is split across Ddevices
necessitates gathering all embeddings [59] with expensive
all-gathers and, more importantly, the materialization of a
memory-intensivejBjjBj matrix of pairwise similarities.
The sigmoid loss, however, is particularly amenable to
a memory efﬁcient, fast, and numerically stable implemen-
tation that ameliorates both these issues. Denoting the per-
device batch size as b=jBj
D, the loss is reformulated as:
 1
jBjDX
di=1|{z}
A:8device diB:swap negs
across devicesz}|{
DX
dj=1C:per device
lossz }| {
b(di+1)X
i=bdi|{z}
all local
positivesb(dj+1)X
j=bdj|{z}
negs from
next deviceLij
This is particularly simple for the sigmoid loss as each pair
is an independent term in the loss. Figure 1 illustrates this
32 8 32 262 1024
Batch Size (k)8182838485ImageNet 0-shot
SigLiT
4 8 16 32 98 307
Batch Size (k)6668707274
SigLIP
Sigmoid
Softmax
16 32 65 131 245
Batch Size (k)30313233343536XM T→I 36 lang. avg.
mSigLIPFigure 2: The effect of pre-training batch size. Left: SigLiT results , trained for 18B seen examples. Sigmoid loss outper-
forms the softmax loss signiﬁcantly with small batch sizes, and performs similarly at larger batch sizes. We successfully
trained an SigLiT model with up to one million batch size. However, performance for both sigmoid and softmax saturate at
around 32 k batch size. Middle: SigLIP results , trained for 9B seen examples. Both sigmoid loss and softmax loss saturate
at a reasonable batch size, while the peak of the sigmoid loss comes earlier and slightly outperforms the peak of the softmax
loss. A very large batch size hurts both losses. Right: mSigLIP results , trained for 30B seen examples. With a multilingual
setup using over 100 languages, 32 k batch size is surprisingly sufﬁcient and scaling beyond that hurts performance on a
36-language cross-modal retrieval task.
method. In words, we ﬁrst compute the component of the
loss corresponding to the positive pairs, and b 1nega-
tive pairs. We then permute representations across devices,
so each device takes negatives from its neighbouring de-
vice (next iteration of sum B). The loss is then calculated
with respect to this chunk (sum C). This is done indepen-
dently in each device, such that each device computes the
loss with respect to its local batch b. Losses can then simply
be summed across all devices (sum A). Individual collec-
tive permutes (for sum B) are fast (and indeed Dcollective
permutes is typically faster than two all-gathers between D
devices), and the memory cost at any given moment is re-
duced fromjBj2tob2(for sum C). Usuallybis constant as
scalingjBjis achieved by increasing the number of accel-
erators. Due to being quadratic with respect to the batch
size, the vanilla loss computation rapidly bottlenecks scal-
ing up. This chunked approach enabled training with batch
sizes over 1 million on relatively few devices.
4. Results
In this section, we evaluate the proposed SigLiT and
SigLIP models across a wide range of batch sizes. We dis-
cuss what can be achieved with a small number of accel-
erator chips, using both SigLiT and SigLIP recipes. We
also brieﬂy discuss the impact of batch size on multilin-
gual language image pre-training. We ablate the importance
of our large-batch stabilization modiﬁcation and the intro-
duced learned bias term and present a study on the effect of
positive and negative pairs ratio in the sigmoid loss. Lastly,we explore SigLIP’s data noise robustness.
To validate our models, we report zero-shot transfer re-
sults on the ImageNet dataset [14] and zero-shot retrieval
results across 36 languages on the XM3600 dataset [44].
We use the ScalingViT-Adafactor optimizer [58] by default
for all our experiments.
4.1. SigLiT: Scaling batch size to the limit
Following [59], we use the same precomputed embed-
dings for the images using a ViT-g vision model, and train
a base size text tower from scratch with the same hyperpa-
rameters using the LiT image-text dataset [59].
We perform a study over a wide range of batch sizes,
from 512 to 1M, demonstrating the impact of batch size
for contrastive learning. Results are presented in Figure 2
(left). When the batch size is smaller than 16k, sigmoid loss
outperforms softmax loss by a large margin. With growing
batch sizes, we observe that softmax loss quickly catches
up and potentially slightly underperforms sigmoid loss with
a large enough batch size. Overall, we recommend using
the SigLIP recipe for large batch sizes as well, due to the
simplicity, compute savings, and straightforward memory
efﬁcient implementation.
There is a consensus that contrastive learning beneﬁts
from large batch sizes, while most of the existing studies
stop at 64 k batch size [59, 35, 10]. We successfully trained
an SigLiT model at one million batch size, to explore the
limit of contrastive learning. To our surprise, the perfor-
mance saturates at 32 k batch size, further scaling up the
batch size only gives a minor boost, and the model peaks at
4450 900 3000 18'000
Examples Seen [M]7879808182838485ImageNet 0-shot
    8k
262kSigmoid
SoftmaxFigure 3: SigLiT ImageNet 0-shot transfer results with
different training durations. Large batch size results in a
big performance boost, but needs a sufﬁciently long sched-
ule to ramp up, as for short schedules, very large batch size
results in a small number of gradient update steps.
256 k batch size. Our best SigLiT with a B-sized text mode
achieves 84.7% zero-shot transfer accuracy on ImageNet,
while the original LiT paper reports a slightly better 85.2%
score with a 10 times larger g-sized text model. Figure 3
presents the impact of training duration for different batch
sizes. It demonstrates that large, 262kbatch size signiﬁ-
cantly outperforms smaller 8kbatch size when trained for
a sufﬁciently long time. Note, that for short training dura-
tions, large batch size leads to the fewer absolute number of
update steps and thus needs more time to ramp up.
4.2. SigLIP: Sigmoid loss is beneﬁcial for language-
image pre-training
We pre-train SigLIP models on the WebLI dataset [13],
using only English image and text pairs. We use CLIP (We-
bLI) to denote the CLIP baseline pre-trained on WebLI with
the standard softmax loss. We use moderately-sized mod-
els: B/16 ViT for image embeddings and B-sized trans-
former for text embeddings. The input images are resized to
224224 resolution. The text is tokenized by a 32 k vocab-
ulary sentencepiece tokenizer [27] trained on the English
C4 dataset [37], and a maximum of 16 text tokens are kept.
Figure 2 middle plot shows SigLIP results, With less than
32 k batch size, SigLIP outperforms CLIP (WebLI) base-
lines. On the other end of the scale, the memory efﬁciency
of the sigmoid loss enabled much larger batch sizes. For ex-
ample, with four TPU-v4 chips, we could ﬁt a batch size of
4096 with a Base SigLIP but only 2048 with a correspond-
ing CLIP model. The two advantages together demonstrate
signiﬁcant beneﬁts of the sigmoid loss for language image
pre-training with ﬁxed resources, which will be discussed
in Section 4.5.16 k 32 k 64 k 128 k 240 k
INet-0 71.6 73.2 73.2 73.2 73.1
XM avg 34.8 34.9 34.4 33.6 32.7
XM de 54.7 54.8 55.4 54.3 54.7
XM en 46.5 46.2 46.5 46.6 46.6
XM hi 9.1 8.5 7.9 8.1 7.3
XM ru 50.1 49.9 49.7 48.6 49.3
XM zh 30.7 32.5 32.0 30.6 23.7
Table 2: Multilingual SigLIP results with various batch
sizes, pre-trained for 30 billion seen examples. We report
zero-shot transfer results on ImageNet (INet-0) and aver-
aged text to image retrieval results across 36 languages on
the crossmodal 3600 dataset (XM). The full table on 36 lan-
guages can be found in Appendix.
As batch size increases, the gap between the sigmoid and
the softmax losses diminish. SigLIP performs best at batch
size 32 k, whereas the softmax loss required 98 k for optimal
performance and still didn’t outperform the sigmoid based
variant. Scaling further, a larger batch size like 307 k hurts
both losses.
4.3. mSigLIP: Multi-lingual pre-training
We further scale up the training data by keeping all the
100 languages from the WebLI dataset [13]. With multi-
lingual data, one usually needs to use a larger international
vocabulary. We ﬁrst verify the impact of two tokenizers: a
small multilingual vocabulary with 32 k tokens [37], and a
large multilingual vocabulary with 250 k tokens [54]. We
train B-sized ViT and text models for 900Mtotal exam-
ples seen, and observe slightly more than 1% improvement
when using a larger vocabulary.
However, the token embeddings become huge for very
large vocabulary sizes. Following the standard setup, we
would need to store a NWtoken embedding lookup table
to train the multilingual model, where Nis the vocabulary
size mentioned above and Wis the embedding dimension
of the text model. To save memory, we propose to use a
“bottlenecked” token embedding. We use NKembed-
ding matrix and additional KWprojection, where the
bottleneckKis much smaller than W.
In our experiments, we observed that using a large mul-
tilingual vocabulary with a bottleneck can be scaled up as
efﬁciently as using a small multilingual vocabulary. Specif-
ically, by enabling the bottleneck of size K= 96 for Base
architecture with W= 768 , we only see about a half per-
cent quality drop on ImageNet zero-shot transfer, compared
to using the full 250kvocabulary.
512 4 8 16 24010203040506070INet 0-shot
12 4 8 16 24
Examples Seen [100M]010203040506070INet 10-shotfrom-scratch
fine-tune
fine-tune w/o enc.wdFigure 4: Top: SigLIP with pre-trained encoders ramps up
quickly. However, only disabling weight decay on the pre-
trained encoder weights leads to stable behavior and good
ImageNet 0-shot transfer results. Bottom : ImageNet 10-
shot transfer results, where decaying the pre-trained weights
leads to deterioration of the pre-trained model visual repre-
sentation quality. Disabling weight decay ﬂattens the curve.
With the memory improvements, we train mSigLIP
models for various batch sizes, for a total of 30 billion ex-
amples seen. Table 2 and Figure 2 (right plot) show the
results. We were expecting a large batch size to improve
multilingual pre-training, where the model sees more ex-
amples from the same language as hard negatives in a sin-
gle mini-batch. However, we didn’t observe clear improve-
ments with a batch size larger than 32 k. A batch size of
32 k is sufﬁcient for a multilingual setup as well. On the
XM3600 cross-modal retrieval tasks, we found that going
beyond 32 k batch size leads to worse results on average
while on ImageNet zero-shot transfer it stays ﬂat. mSigLIP
sets the new state-of-the-art on XM3600 text to image re-
trieval task, with only a Base size model. Our best result is
34.9%, which is more than 6% higher than the previously
reported result 28.5% [13] with a standard LiT model [59]
using a much larger four billion ViT-e model. We further
scale up mSigLIP training in Section 4.6.
4.4. SigLiT with four TPU-v4 chips
For many practitioners, the important question usually is
“what can be trained with a limited amount of resources?”
We explore the usage of SigLiT models in this section with
only four TPU-v4 chips, as the memory efﬁcient sigmoid
loss is suitable for this application scenario.
3456Loss Lβ2=0.999
β2=0.95
110||∇wL||
1B 2B 3B 4B 5B
Examples seen24||Δw||Figure 5: The effect of Adam and AdaFactor’s 2.As
we increase batch-size, we observe more frequent training
instability. This instability seen in the loss curves (top) is
caused by spikes in gradient norm (middle) leading to large
parameter updates (bottom). Decreasing the 2momentum
stabilizes training. Occasional gradient spikes still happen
(see step at 2B), but do not destabilize the training process.
We follow the same setup as in section 4.1. We use
the publicly available ViT-AugReg-B/8 [42] model as the
frozen (
 ) vision tower, and precompute embeddings to ac-
celerate the training [59]. The text model is a Large Trans-
former, but with a depth of only 12 layers (instead of 24).
It is trained using the LION [12] optimizer with decoupled
weight decay 110 7, linearly warm-up of learning rate
over 6.5k steps up to a peak of 110 4, followed by a co-
sine decay to 0. We train for a total of 65 000 steps with a
batch size of 32k – this leads to just under one day of train-
ing. Table 1 shows the results when training a model on four
chips for one day, achieving 79.7% 0-shot ImageNet classi-
ﬁcation accuracy; very competitive in this limited resource
regime. With a ViT-g/14 [58] model as the vision tower and
a Large text tower, we can train at 20 k batch size on four
chips for 107 k steps in under two days. This further pushes
the 0-shot ImageNet classiﬁcation accuracy up to 84.5%.
4.5. SigLIP with a small amount of TPU-v4 chips
It’s resource demanding to train a CLIP model from-
scratch in general, with SigLIP it’s possible to ﬁt a larger
train batch size with fewer amount of chips. In this section,
we explore ways to train SigLIP models efﬁciently with pre-
trained weights. We use pre-trained weights to initialize the
image model to accelerate the pre-training, which was orig-
61 : 16k 1 : 1.6k 1 : 164 1 : 16 1 : 1.6607080
ImageNet 0-shot
Random
Hard
Hard, matched pairs
Easy
1 : 16k 1 : 1.6k 1 : 164 1 : 16 1 : 1.6−15−10−50
Learned bias
1 : 16k 1 : 1.6k 1 : 164 1 : 16 1 : 1.6−20−15−10−505
Average logit of pos and negFigure 6: The effect of batch composition. We simulate various batch compositions by masking out negatives, either
randomly, keeping only the hardest, or the easiest. With no masking, we have 16 k negatives for each positive in the batch
(1:16 k) and the strongest masking we apply (1:1.6) results in almost balanced minibatches. In one setting we match total
pairs seen by training for signiﬁcantly longer. We observe ImageNet 0-shot score, the ﬁnal value of the learned bias, and the
average logits of positive and negative pairs. Overall, the imbalance does not seem to be detrimental, but ﬁnding an efﬁcient
way of mining negatives might be beneﬁcial.
inally discussed in [59]. We use the public and unlocked
ViT-AugReg-B/16 [42] model to initialize our vision tower
and ﬁne-tune on the same WebLI English data as used for
SigLIP. In all the experiments, we apply a 0.1 learning rate
multiplier to the pre-trained image tower to make it suitable
for ﬁne-tuning.
Figure 4 presents unlocked
 ﬁne-tuning results along-
side from-scratch randomly initialized baselines. We used
16 TPU-v4 chips and train at 16 k batch size for 2.4 B ex-
amples seen. We found that the ﬁne-tuning setup doesn’t
perform well out-of-the-box; this is consistent with prior
works [59] where ﬁnetuning image models degraded visual
representation quality. This is evidenced by ImageNet 10-
shot linear classiﬁcation, where in Figure 4 the ﬁne-tuned
setup is barely better than the from-scratch baseline.
We hypothesize that the default weight decay applied to
the pre-trained weights reduces their effectiveness. Moti-
vated by the ﬁne-tuning recipe from [17, 58, 25], that uses
no weight decay, we also propose disabling weight decay on
the pre-trained weights for SigLIP training. Weight decay
is therefore only applied to the randomly initialized weights
in the text model. This simple modiﬁcation signiﬁcantly
improved SigLIP results. Figure 4 shows that with our im-
proved recipe, SigLIP reaches 71% 0-shot accuracy on Im-
ageNet, using 16kbatch size, trained on 16 chips for three
days. We also present from-scratch results in the bottom
rows of Table 1: with 32 TPUv4 chips for only two days,
SigLIP achieves 72.1% 0-shot accuracy. This presents a
signiﬁcant training cost reduction e.g. compared to CLIP
(approx. 2500 TPUv3-days for 72.6%) reported in [30].4.6. Scaling up SigLIP and mSigLIP
In this section, we scale up SigLIP by “overtraining” the
model [45, 1]. We present results in Table 3 using ViT-B,
ViT-L or So-400m [1] as the vision encoder, with a text en-
coder of the same size (B, L and So-400m respectively).
Following the recipe described in Section 4.2, we train both
models for 40 billion examples seen at batch size 32 k, but
use(256=16)2= 256 image patches and 64 text tokens (in-
stead of 16). To get SigLIP models for different resolutions,
we train for 5 billion more examples at the target resolution,
with a 100x smaller learning rate and no weight decay. In
Table 3, we report zero-shot classiﬁcation results on Im-
ageNet [14], ObjectNet [2], ImageNet-v2 [39], ImageNet
ReaL [3], and zero-shot image-to-text (I !T) retrieval, text-
to-image (I!T) retrieval results on MSCOCO [11].
We also scale up the multilingual mSigLIP ViT-B model
in the same way. We report image-text retrieval results
across 36 languages on the XM3600 benchmark [44]. The
scaled-up mSigLIP ViT-B model achieves the state-of-the-
art42.6% image retrieval recall@1 and 54.1% text retrieval
recall@1 for a Base model. This is slightly outperformed
by the Large model in [48] getting 42.96% image retrieval
recall@1. Detailed results are provided in Appendix Table 9
and Figure 8, denoted as *32 k.
4.7. Stabilizing large-batch training
As we move to large batch sizes, the language image pre-
training using transformers becomes increasingly more un-
stable, even when using a modestly-sized model (e.g. Base
size). The reason for these instabilities is large spikes in the
7MethodImage Encoder ImageNet-1k COCO R@1
ViT size # Patches Validation v2 ReaL ObjectNet I !T T!I
CLIP B 196 68.3 61.9 - 55.3 52.4 33.1
OpenCLIP B 196 70.2 62.3 - 56.0 59.4 42.3
EV A-CLIP B 196 74.7 67.0 - 62.3 58.7 42.2
SigLIP B 196 76.2 69.6 82.8 70.7 64.4 47.2
SigLIP B 256 76.7 70.0 83.1 71.3 65.1 47.4
SigLIP B 576 78.6 72.1 84.5 73.8 67.5 49.7
SigLIP B 1024 79.2 73.0 84.9 74.7 67.6 50.4
CLIP L 256 75.5 69.0 - 69.9 56.3 36.5
OpenCLIP L 256 74.0 61.1 - 66.4 62.1 46.1
CLIPA-v2 L 256 79.7 72.8 - 71.1 64.1 46.3
EV A-CLIP L 256 79.8 72.9 - 75.3 63.7 47.5
SigLIP L 256 80.5 74.2 85.9 77.9 69.5 51.1
CLIP L 576 76.6 72.0 - 70.9 57.9 37.1
CLIPA-v2 L 576 80.3 73.5 - 73.1 65.5 47.2
EV A-CLIP L 576 80.4 73.8 - 78.4 64.1 47.9
SigLIP L 576 82.1 75.9 87.0 81.0 70.6 52.7
OpenCLIP G (2B) 256 80.1 73.6 - 73.0 67.3 51.4
CLIPA-v2 H (630M) 576 81.8 75.6 - 77.4 67.2 49.2
EV A-CLIP E (5B) 256 82.0 75.7 - 79.6 68.8 51.1
SigLIP SO (400M) 729 83.2 77.2 87.5 82.9 70.2 52.0
Table 3: Comparison with other publicly released models. Our SigLIP models outperform all prior models, e.g. Open-
CLIP [22] and CLIP [36], by a signiﬁcant margin on both zero-shot classiﬁcation and retrieval tasks. Compared to the concur-
rent EV A-CLIP [43] and CLIPA-v2 [29], our SigLIP-L performs better across the board, in both the low and high resolution
cases. Especially noteworthy is the Shape-Optimized 400M parameter ViT [1] architecture, which outperforms all signiﬁ-
cantly larger models. We publicly release our models: https://github.com/google-research/big_vision .
gradient norms, which translate to large-magnitude changes
in the weights that may destabilize the training process,
see Figure 5. We observe that reducing 2in Adam and
AdaFactor from its default 0.999 to 0.95 (which was sug-
gested in [20, 9]) is enough to stabilize the training. Intu-
itively, this allows recovering from gradient spikes quicker.
We opt for setting 2= 0:95for all our experiments.
4.8. Negative ratio in sigmoid loss
One question which arises when shifting the perspective
from the softmax’s “pick the right class” view to the sig-
moid’s “rate this pair” view, is the imbalance in positive
versus negative pairs. For a batch size jBj, the batch con-
tainsjBjpositive pairs, but jBj2 jBj negative examples.
In the modest batch-size of 16 k, there are actually 268 M
negative examples for only 16 k positive ones. At the same
time, because the sigmoid loss decomposes into a sum of
per-example losses, we can perform controlled experiments
to study the effect of the mini-batch composition and dis-tribution of examples visited. We run experiments in the
SigLiT setup at batch-size 16 k for 900 M steps and vary
the composition of the batch by masking out ( i.e. ignoring)
enough negative examples to reach a target “positive : neg-
ative” ratio, masking in the following ways:
•Random: Randomly choose negative pairs to mask.
•Hard: Keep hardest negative pairs (highest loss).
•Easy: Keep easiest negatives pairs (lowest loss).
•Hard + matching total pairs seen: Masking exam-
ples while training for a ﬁxed number of steps does
decrease the total number of pairs seen during train-
ing. Hence in the matched pairs setting, we increase
the number of training steps by the masking ratio in
order to keep the number of pairs seen constant.
Figure 6 shows the effect of the various masking strate-
gies. Randomly removing negatives to rebalance does dete-
riorate performance. Keeping the easiest examples does not
work at all, while keeping the hardest negatives does almost
80.0 0.2 0.40.500.520.540.56ImageNet 0shot
Image
0.0 0.2 0.4
Text
Sigmoid
Softmax
0.0 0.2 0.4
p(corruption)
Batch
0.0 0.1 0.2
Image & Text
0.0 0.1 0.2
Image, Text & BatchFigure 7: Sigmoid-training increases robustness to data noise. Titles show the type of corruption applied, and x-axes show
the probability with which they are applied. With increasing corruption severity, M-scale models trained with sigmoid loss
for 3.6 billion examples retain superiority over corresponding softmax baseline.
maintain the quality, indicating that, as could be expected,
a lot of the learning on the negative side comes from the
harder examples. This is further conﬁrmed by the slightly
increased performance of training longer on the hardest ex-
amples in order to match the total pairs seen.
We also look at the value of the learned bias at the end of
training as well as the average logit value for positive and
negative examples across these settings, and ﬁnd the result
mostly follows what one would expect: as fewer negatives
are present, the bias and logits become more positive over-
all. Interestingly, when training with more hard negative
pairs, the average logits of positive pairs stays mostly ﬂat.
This study conﬁrms that (1) the imbalance does not seem
to be a major reason for concern, while at the same time (2)
coming up with an efﬁcient way of including more negative
examples can be promising but is not trivial.
4.9. Bias term in sigmoid loss
We ablate the bias term in the loss function, using the
Base architecture with an 8 k batch size, trained for 900M
examples with the SigLIP setup. Zero-shot transfer results
are reported on ImageNet [14], Oxford-iiit pet [34] and Ci-
far100 [26]. Table 4 presents results with and without a bias
term in the sigmoid loss.
Table 4: Bias (b) and temperature (t0) initialization. Re-
sults are reported using Base architecture, 8 k batch size,
trained for 900M examples. Enabling the bias term b with
 10initialization improves results consistently.
b t0INet-0 Pet-0 C100-0
n/a log 10 62.0 81.8 59.9
-10 log 10 63.0 82.4 61.0
-10 log 1 61.0 80.0 60.4
0 log 10 61.7 79.9 59.0
0 log 1 53.7 73.2 53.8Enabling the bias term with a  10initialization consis-
tently improves performance across all tasks. This is be-
cause the bias term ensures that the training starts close to
the prior, preventing dramatic over-correction in early op-
timization. In contrast, a randomly chosen bias term ini-
tialization, such as the 0 initialization in Table 4, fails to
address the over-correction issue, leading to signiﬁcantly
worse results. This effect is particularly noticeable when
using a small temperature t0initialization. We set the bias
and temperature initialization to b= 10andt0= log 10
(hencet= 10 ) as the default for all experiments.
4.10. Label noise robustness
Prior works demonstrated improved robustness against
label noise when using the sigmoid loss for classiﬁcation
models [3]. This property would be particularly useful here
in the face of the famously noisy nature of popular large-
scale image-text datasets. In order to study this for SigLIP,
we train M/16 image models alongside an M text model at
batch size 16384 for 3.6 billion seen examples. We corrupt
the training data using one of the following methods:
•Image : With probability p, replace the image with uni-
form random noise.
•Text: With probability p, replace tokenized text with a
new sequence of randomly sampled tokens, up to some
(sampled) sequence length.
•Batch alignment : Randomly shufﬂe the ordering of
p% of the batch.
•Image & text : Apply both with probability peach.
•Image, text & batch : Alongside (4), also shufﬂe frac-
tionpof alignments.
Results from varying the likelihood of the corruption are
shown in Figure 7. Models trained with sigmoid loss are
increasingly robust to all kinds of added noise.
95. Conclusion
We conducted a study on two language-image pre-
training instances that used the sigmoid loss: SigLiT and
SigLIP. Our results demonstrate that the sigmoid loss per-
forms better than the softmax baseline, particularly for
small train batch sizes. This loss function is also more mem-
ory efﬁcient, which allows larger train batch sizes without
requiring additional resources. We performed a thorough
investigation of the batch size in contrastive learning. Sur-
prisingly, we found that a relatively modest batch size of
32 k yielded nearly optimal performance. Further studies
have been performed to understand better the introduced
bias term in the sigmoid loss, robustness to data noises and
the impact of positive and negative pairs ratio in the sigmoid
loss. We hope this work will facilitate language-image pre-
training research with limited resources.
Acknowledgements. We thank Daniel Keysers, Ilya Tol-
stikhin, Olivier Bousquet and Michael Tschannen for their
valuable feedback and discussions on this paper. We thank
Joan Puigcerver, Josip Djolonga and Black Hechtman for
discussions on efﬁcient implementations of the chunked
contrastive loss. We thank Kaiming He and Xinlei Chen for
the discussion of 2to stabilize the training. We also thank
Ross Wightman for spotting a mistake in the pseudocode in
the ﬁrst version of this paper, Boris Dayma and Krzysztof
Maziarz for spotting typos in the second and third versions
which made tvst0confusing. We thank the Google Deep-
mind team for providing a supportive research environment.
We use the bigvision codebase [5, 4] for all experi-
ments in this project.
10References
[1] Ibrahim Alabdulmohsin, Xiaohua Zhai, Alexander
Kolesnikov, and Lucas Beyer. Getting vit in shape:
Scaling laws for compute-optimal model design. In
NeurIPS , 2023. 7, 8, 17
[2] Andrei Barbu, David Mayo, Julian Alverio, William Luo,
Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and
Boris Katz. ObjectNet: A large-scale bias-controlled dataset
for pushing the limits of object recognition models. In
NeurIPS , 2019. 7, 17
[3] Lucas Beyer, Olivier J. H ´enaff, Alexander Kolesnikov, Xi-
aohua Zhai, and A ¨aron van den Oord. Are we done with
imagenet? CoRR , abs/2006.07159, 2020. 2, 7, 9, 17
[4] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Bet-
ter plain vit baselines for imagenet-1k, 2022. 10, 17
[5] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Big
vision. https://github.com/google-research/
big_vision , 2022. 10, 17
[6] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun
Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-
700m: Image-text pair dataset. https://github.com/
kakaobrain/coyo-dataset , 2022. 1
[7] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
Soricut. Conceptual 12M: Pushing web-scale image-text
pre-training to recognize long-tail visual concepts. In CVPR ,
2021. 1
[8] Feilong Chen, Duzhen Zhang, Minglun Han, Xiu-Yi Chen,
Jing Shi, Shuang Xu, and Bo Xu. VLP: A survey on vision-
language pre-training. Int. J. Autom. Comput. , 20(1):38–56,
2023. 2
[9] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-
woo Jun, David Luan, and Ilya Sutskever. Generative pre-
training from pixels. In Proceedings of the 37th Interna-
tional Conference on Machine Learning, ICML 2020, 13-18
July 2020, Virtual Event , volume 119 of Proceedings of Ma-
chine Learning Research , pages 1691–1703. PMLR, 2020.
8
[10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey E. Hinton. A simple framework for contrastive learn-
ing of visual representations. In ICML , 2020. 2, 4
[11] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-
tam, Saurabh Gupta, Piotr Doll ´ar, and C. Lawrence Zitnick.
Microsoft COCO captions: Data collection and evaluation
server. CoRR , abs/1504.00325, 2015. 7, 17
[12] Xiangning Chen, Chen Liang, Da Huang, Esteban Real,
Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang
Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V . Le. Symbolic
discovery of optimization algorithms, 2023. 2, 6
[13] Xi Chen, Xiao Wang, Soravit Changpinyo, A. J. Piergio-
vanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,
Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander
Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Has-
san Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal,
James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini,
Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas
Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, andRadu Soricut. Pali: A jointly-scaled multilingual language-
image model. CoRR , abs/2209.06794, 2022. 1, 5, 6, 17
[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR , 2009. 4, 7, 9, 17
[15] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin John-
son. Redcaps: Web-curated image-text data created by the
people, for the people. In Joaquin Vanschoren and Sai-
Kit Yeung, editors, Proceedings of the Neural Information
Processing Systems Track on Datasets and Benchmarks 1,
NeurIPS Datasets and Benchmarks 2021, December 2021,
virtual , 2021. 1
[16] Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen,
Shuyang Gu, Weiming Zhang, Lu Yuan, Dong Chen, Fang
Wen, and Nenghai Yu. Clip itself is a strong ﬁne-tuner:
Achieving 85.7% and 88.0% top-1 accuracy with vit-b and
vit-l on imagenet. CoRR , abs/2212.06138, 2022. 2
[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 1616 words: Transformers for image recognition at
scale. In ICLR , 2021. 3, 7, 17
[18] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
Deep Learning . MIT Press, 2016. http://www.
deeplearningbook.org . 1
[19] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimension-
ality reduction by learning an invariant mapping. In CVPR ,
volume 2, 2006. 2
[20] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Pi-
otr Doll ´ar, and Ross B. Girshick. Masked autoencoders
are scalable vision learners. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition, CVPR 2022, New
Orleans, LA, USA, June 18-24, 2022 , pages 15979–15988.
IEEE, 2022. 8
[21] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang,
Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up
vision-language pre-training for image captioning. CoRR ,
abs/2111.12233, 2021. 1, 2
[22] Gabriel Ilharco, Mitchell Wortsman, Nicholas Carlini,
Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok
Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi,
and Ludwig Schmidt. OpenCLIP. Zenodo, 2021. 8
[23] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc V . Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representation
learning with noisy text supervision. In ICML , 2021. 1, 2
[24] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,
Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and
Dilip Krishnan. Supervised contrastive learning. In Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-
Florina Balcan, and Hsuan-Tien Lin, editors, Advances in
Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual , 2020. 2
[25] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan
Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.
11Big transfer (BiT): General visual representation learning. In
ECCV , 2020. 7
[26] Alex Krizhevsky. Learning multiple layers of features from
tiny images. Technical report, Univ. of Toronto, 2009. 9
[27] Taku Kudo and John Richardson. SentencePiece: A sim-
ple and language independent subword tokenizer and detok-
enizer for neural text processing. In EMNLP , 2018. 5, 14
[28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.
Hoi. BLIP: bootstrapping language-image pre-training for
uniﬁed vision-language understanding and generation. In
Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba
Szepesv ´ari, Gang Niu, and Sivan Sabato, editors, Interna-
tional Conference on Machine Learning, ICML 2022, 17-
23 July 2022, Baltimore, Maryland, USA , volume 162 of
Proceedings of Machine Learning Research , pages 12888–
12900. PMLR, 2022. 2
[29] Xianhang Li, Zeyu Wang, and Cihang Xie. Clipa-v2: Scal-
ing CLIP training with 81.1% zero-shot imagenet accuracy
within a $10, 000 budget; an extra $4, 000 unlocks 81.8%
accuracy. CoRR , abs/2306.15658, 2023. 8
[30] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichten-
hofer, and Kaiming He. Scaling language-image pre-training
via masking. CoRR , abs/2212.00794, 2022. 2, 7
[31] Matthias Minderer, Alexey A. Gritsenko, Austin Stone,
Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy,
Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani,
Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf,
and Neil Houlsby. Simple open-vocabulary object detection.
In Shai Avidan, Gabriel J. Brostow, Moustapha Ciss ´e, Gio-
vanni Maria Farinella, and Tal Hassner, editors, Computer
Vision - ECCV 2022 - 17th European Conference, Tel Aviv,
Israel, October 23-27, 2022, Proceedings, Part X , volume
13670 of Lecture Notes in Computer Science , pages 728–
755. Springer, 2022. 2
[32] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker
Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer,
Inioluwa Deborah Raji, and Timnit Gebru. Model cards
for model reporting. In danah boyd and Jamie H. Morgen-
stern, editors, Proceedings of the Conference on Fairness,
Accountability, and Transparency, FAT* 2019, Atlanta, GA,
USA, January 29-31, 2019 , pages 220–229. ACM, 2019. 17
[33] Jishnu Mukhoti, Tsung-Yu Lin, Omid Poursaeed, Rui Wang,
Ashish Shah, Philip H. S. Torr, and Ser-Nam Lim. Open
vocabulary semantic segmentation with patch aligned con-
trastive learning, 2022. 2
[34] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and
C. V . Jawahar. Cats and dogs. In IEEE Conference on Com-
puter Vision and Pattern Recognition , 2012. 9
[35] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu,
Adams Wei Yu, Minh-Thang Luong, Mingxing Tan, and
Quoc V . Le. Combined scaling for zero-shot transfer learn-
ing. CoRR , abs/2111.10050, 2021. 2, 4
[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In ICML , 2021.
1, 2, 3, 8[37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J. Liu. Exploring the limits of transfer learning with a
uniﬁed text-to-text transformer. arXiv e-prints , 2019. 5, 14
[38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J. Liu. Exploring the limits of transfer learning with
a uniﬁed text-to-text transformer. J. Mach. Learn. Res. ,
21:140:1–140:67, 2020. 17
[39] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and
Vaishaal Shankar. Do ImageNet classiﬁers generalize to Im-
ageNet? In ICML , 2019. 7, 17
[40] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, Patrick Schramowski, Srivatsa Kundurthy, Katherine
Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia
Jitsev. LAION-5B: an open large-scale dataset for training
next generation image-text models. CoRR , abs/2210.08402,
2022. 1
[41] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael
Bendersky, and Marc Najork. WIT: wikipedia-based image
text dataset for multimodal multilingual machine learning.
CoRR , abs/2103.01913, 2021. 1
[42] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross
Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train
your ViT? Data, augmentation, and regularization in vision
transformers. CoRR , abs/2106.10270, 2021. 1, 6, 7
[43] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue
Cao. EV A-CLIP: improved training techniques for CLIP at
scale. CoRR , abs/2303.15389, 2023. 8
[44] Ashish V . Thapliyal, Jordi Pont-Tuset, Xi Chen, and Radu
Soricut. Crossmodal-3600: A massively multilingual mul-
timodal evaluation dataset. In Yoav Goldberg, Zornitsa
Kozareva, and Yue Zhang, editors, Proceedings of the 2022
Conference on Empirical Methods in Natural Language Pro-
cessing, EMNLP 2022, Abu Dhabi, United Arab Emirates,
December 7-11, 2022 , pages 715–729. Association for Com-
putational Linguistics, 2022. 4, 7, 17
[45] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste
Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur ´elien
Rodriguez, Armand Joulin, Edouard Grave, and Guillaume
Lample. Llama: Open and efﬁcient foundation language
models. CoRR , abs/2302.13971, 2023. 7
[46] A ¨aron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-
sentation learning with contrastive predictive coding. CoRR ,
abs/1807.03748, 2018. 2
[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS , 2017. 3,
17
[48] Alexander Visheratin. Nllb-clip – train performant multilin-
gual image retrieval model on a budget, 2023. 7
[49] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,
Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.
GIT: A generative image-to-text transformer for vision and
language. CoRR , abs/2205.14100, 2022. 1, 2
12[50] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia
Tsvetkov, and Yuan Cao. Simvlm: Simple visual language
model pretraining with weak supervision. In The Tenth In-
ternational Conference on Learning Representations, ICLR
2022, Virtual Event, April 25-29, 2022 . OpenReview.net,
2022. 2
[51] Ross Wightman, Hugo Touvron, and Herv ´e J´egou. Resnet
strikes back: An improved training procedure in timm.
CoRR , abs/2110.00476, 2021. 2
[52] Mitchell Wortsman. Reaching 80% zero-shot accuracy with
OpenCLIP: VIT-G/14 trained on LAION-2B. https:
//web.archive.org/web/20230127012732/
https://laion.ai/blog/giant-openclip/ . 2
[53] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim,
Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gon-
tijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok
Namkoong, and Ludwig Schmidt. Robust ﬁne-tuning of
zero-shot models. In IEEE/CVF Conference on Computer
Vision and Pattern Recognition, CVPR 2022, New Orleans,
LA, USA, June 18-24, 2022 , pages 7949–7961. IEEE, 2022.
2
[54] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,
Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin
Raffel. mT5: A massively multilingual pre-trained text-to-
text transformer. In NAACL-HLT , 2021. 5, 17
[55] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce
Liu, Lu Yuan, and Jianfeng Gao. Uniﬁed contrastive learn-
ing in image-text-label space. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition, CVPR 2022, New
Orleans, LA, USA, June 18-24, 2022 , pages 19141–19151.
IEEE, 2022. 2
[56] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung,
Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Con-
trastive captioners are image-text foundation models. CoRR ,
abs/2205.01917, 2022. 2
[57] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,
Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu,
Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao,
Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and
Pengchuan Zhang. Florence: A new foundation model for
computer vision. CoRR , abs/2111.11432, 2021. 2
[58] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu-
cas Beyer. Scaling vision transformers. CVPR , 2022. 1, 4,
6, 7, 14
[59] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner,
Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer.
Lit: Zero-shot transfer with locked-image text tuning. In
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, CVPR 2022, New Orleans, LA, USA, June 18-
24, 2022 , pages 18102–18112. IEEE, 2022. 1, 2, 3, 4, 6, 7,
14
[60] Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D.
Manning, and Curtis P. Langlotz. Contrastive learning of
medical visual representations from paired images and text.
In Zachary C. Lipton, Rajesh Ranganath, Mark P. Sendak,
Michael W. Sjoding, and Serena Yeung, editors, Proceed-
ings of the Machine Learning for Healthcare Conference,MLHC 2022, 5-6 August 2022, Durham, NC, USA , volume
182 of Proceedings of Machine Learning Research , pages
2–25. PMLR, 2022. 2
13A. More results for SigLiT
In section 4.1, we use the same precomputed embed-
dings for the images using a ViT-g vision model from [59].
Only resize augmentation is applied, to a ﬁxed 288288
resolution. We train a standard base size text tower, using
the ScalingViT-Adafactor optimizer [58] with 1= 0:9and
2= 0:95. We use 0.001 learning rate with a linear warmup
schedule for the ﬁrst 200 M examples seen, and then the
learning rate is decayed to zero with a cosine learning rate
schedule. Weight decay is set to 0.0001 for all the experi-
ments. The text is tokenized by a 32 k vocabulary sentence-
piece tokenizer [27] trained on the English C4 dataset [37],
and a maximum of 16 text tokens are kept. Table 8 shows
results with multiple train examples seen and batch sizes,
for both the sigmoid loss and the softmax loss baseline.
For training SigLiT in under a day with 4 chips (Sec-
tion 4.4), we used the LION optimizer with peak learning
rate110 4and weight decay 110 7. The learning rate
was warmed linearly to the peak in 6.5 k steps, then cosine
decayed to zero for the remaining 58.5 k steps.
B. More results for SigLIP
In Table 5, we present more results for SigLIP Base with
multiple train examples seen: 3 billion examples and 9 bil-
lion examples respectively.
Batch Size3 B 9 B
sigmoid softmax sigmoid softmax
512 51.5 47.7 - -
1 k 57.3 53.2 - -
2 k 62.1 59.3 - -
4 k 65.3 63.8 68.4 66.6
8 k 68.6 66.6 70.6 69.4
16 k - - 72.3 71.7
32 k 69.9 69.9 73.4 72.9
98 k 69.5 69.7 73.0 73.2
307 k - - 71.6 72.6
Table 5: SigLIP zeor-shot accuracy (%) on the ImageNet
benchmark. Both the sigmoid loss and the softmax loss
baseline are presented. Experiments are performed on mul-
tiple train examples seen (3 B, 9 B) and train batch sizes
(from 512 to 307 k). When trained for 9 B examples, the
peak of the sigmoid loss comes earlier at 32 k than the peak
of the softmax loss at 98 k. Together with the memory ef-
ﬁcient advantage for the sigmoid loss, it allows one to train
the best language-image model with much fewer amount of
accelerators.BS Default Best Best LR Best WD
8 k 70.1 70.1 0.001 0.0001
16 k 70.0 70.0 0.001 0.0001
32 k 68.2 69.0 0.0003 0.00003
Table 6: Default hyperparameters across different batch
sizes, perform either the best or close to the best hyperpa-
rameter from a sweep. Zero-shot accuracy on ImageNet
is reported. BS=batch size, LR=learning rate, WD=weight
decay.
C. Robustness of SigLIP results
Hyperparameters for different batch sizes. Sigmoid
loss doesn’t require tuning hyperparameters for different
batch sizes. For example, in both the SigLiP and SigLiT
setup, we only used default 0.001 learning rate and 0.0001
weight decay across a wide range of batch sizes (from 512
to 1024k). We further performed a sweep of 9 hyperparam-
eters across 3 batch sizes on the from-scratch SigLIP setup
for 3B seen examples: learning rate f0.0003, 0.001, 0.003 g
weight decayf0.00003, 0.0001, 0.0003 g batch size
f8 k, 16 k, 32 kg. We observed in Table 6 that the default
LR/WD is either the best or close to the best.
Standard deviation. We repeat SigLIP training ﬁve
times, using the recommended 32k batch size and 3B seen
examples. We report the average and std in Table 7. The std
of the ﬁve runs is very small for both sigmoid and softmax.
Alternative optimizers. We repeat the same experiment
with AdamW optimizer ﬁve times and got very similar re-
sults and std as reported in Table 7. We tested a linear learn-
ing rate scheduler instead of the default cosine learning rate
scheduler, it achieves 69.9% accuracy.
D. More results for mSigLIP
We present the mSigLIP Base crossmodal retrieval re-
sults on the Crossmodal-3600 dataset, across all the 36 lan-
gauges in Figure 8 and Table 9.
Loss Optimizer Results (%)
Softmax ViT-Adafactor 69.9 0.1
Sigmoid ViT-Adafactor 70.1 0.2
Sigmoid AdamW 70.3 0.1
Table 7: Mean and standard deviation of ﬁve repeated ex-
periments. Zero-shot accuracy on ImageNet is reported.
14Batch Size450 M 900 M 3 B 18 B
sigmoid softmax sigmoid softmax sigmoid softmax sigmoid softmax
512 72.5 69.5 75.0 72.8 77.2 74.6 - -
1 k 75.5 73.6 77.2 76.0 79.6 77.9 - -
2 k 77.1 76.3 79.3 78.1 81.3 80.1 82.2 81.2
4 k 79.2 78.3 80.8 79.8 82.4 81.2 83.0 82.0
8 k 80.8 79.7 82.0 81.0 83.1 82.6 83.6 83.1
16 k 81.2 81.2 82.7 82.1 83.8 83.5 84.2 84.1
32 k 81.9 81.4 83.1 82.7 84.2 84.0 84.6 84.4
64 k 81.6 81.6 83.0 82.8 84.3 84.1 84.7 84.4
128 k 80.5 80.0 83.1 83.2 84.2 84.4 84.7 84.6
256 k 72.8 72.2 82.1 81.7 84.3 84.2 84.7 84.6
1024 k - - - - - - 84.7 -
Table 8: SigLiT zero-shot accuracy (%) on the ImageNet benchmark. Both the sigmoid loss and the softmax loss
baseline are presented. Extensive experiments are performed on multiple train examples seen (450 M, 900 M, 3 B, 18 B) and
train batch sizes (from 512 to 1 M).
arbncsdadeelenesfafifilfrhihrhuiditiwjakominlnoplptquz rorusvswtethtrukvizhavg0.00.10.20.30.40.50.60.70.8 16 k 32 k 64 k 128 k 240 k *32 k
arbncsdadeelenesfafifilfrhihrhuiditiwjakominlnoplptquz rorusvswtethtrukvizhavg0.00.10.20.30.40.50.616 k 32 k 64 k 128 k 240 k *32 k
Figure 8: Image-to-text and text-to-image zero-shot retrieval recall@1 results on all 36 languages of Crossmodal-3600 .
Top: Image to text. Bottom: text to image. Colors are batch sizes. *32 k represents the scaled up results as described in
Section 4.6.
E. Label noise experiments
All models had an M/16 image tower and a M text tower.
They were trained from random initialisation for 3.6B ex-
amples seen, with a batch size of 16384. A cosine learning
rate schedule was used, with an initial linear warmup for
10% of steps up to a peak learning rate of 0.001.
15Lang.Image-to-text Text-to-image
16 k 32 k 64 k 128 k 240 k *32 k 16 k 32 k 64 k 128 k 240 k *32 k
ar 52.4 51.3 51.5 51.5 51.1 59.7 37.6 37.4 37.1 36.3 36.0 44.9
bn 11.4 10.8 10.4 10.3 9.9 30.1 5.5 6.2 4.9 5.1 4.4 20.0
cs 54.1 53.7 53.7 52.8 51.8 58.9 41.8 41.6 41.5 39.9 39.4 47.0
da 62.7 62.4 62.0 60.4 59.3 68.4 47.0 47.0 45.6 43.0 43.5 52.9
de 70.3 71.4 71.2 71.1 70.2 79.7 54.7 54.8 55.4 54.3 54.7 65.3
el 36.9 35.8 35.1 34.5 33.8 47.4 22.4 22.8 22.0 21.3 20.8 32.2
en 50.1 50.5 50.2 49.9 50.7 52.5 46.5 46.2 46.5 46.6 46.6 47.6
es 64.7 64.9 67.2 65.3 65.6 66.3 54.8 55.0 55.5 54.5 55.2 57.0
fa 57.0 57.8 56.1 55.3 54.6 66.2 39.6 40.2 38.4 38.4 38.3 50.0
ﬁ 54.9 54.1 53.8 51.7 51.7 59.1 37.7 37.1 36.4 34.0 34.5 44.0
ﬁl 23.2 22.8 22.9 21.4 21.2 29.2 12.8 12.9 12.4 12.2 11.3 20.4
fr 65.7 66.9 67.0 66.1 66.5 71.2 55.9 57.1 55.5 54.4 54.3 61.8
hi 19.9 18.8 19.9 19.5 17.4 32.2 9.1 8.5 7.9 8.1 7.3 17.3
hr 52.7 53.0 53.0 49.9 49.6 62.6 38.2 37.1 36.4 35.2 34.3 47.2
hu 57.0 57.1 56.3 54.8 53.0 62.9 41.4 40.2 40.2 38.6 38.2 51.2
id 64.8 67.1 66.6 65.4 64.7 73.7 48.5 49.4 49.5 47.8 47.3 60.5
it 65.9 66.4 67.1 65.2 66.1 72.3 55.5 56.4 55.8 54.8 54.1 62.3
iw 48.4 47.9 47.7 46.1 45.2 62.2 31.8 31.8 31.9 30.1 30.1 48.0
ja 46.4 45.9 42.9 43.7 30.2 55.1 31.0 31.3 29.2 28.9 18.5 42.3
ko 50.8 49.5 49.4 50.2 46.8 61.4 34.4 34.7 33.2 33.1 31.5 45.9
mi 0.4 0.4 0.6 0.6 0.4 0.3 0.2 0.2 0.2 0.2 0.2 0.3
nl 59.6 60.4 58.9 58.3 57.9 63.6 48.9 49.5 48.9 48.4 47.9 53.6
no 61.4 62.4 62.0 60.9 59.9 65.3 45.3 46.2 45.0 43.5 43.7 50.0
pl 62.2 62.0 62.0 61.1 60.5 67.1 48.8 47.4 48.7 46.8 46.7 56.7
pt 63.1 63.6 64.9 64.3 63.2 65.4 52.4 52.3 52.3 51.9 52.4 57.3
quz 6.8 6.4 6.4 6.6 6.7 6.8 2.7 2.6 2.7 2.7 2.8 2.9
ro 52.1 51.4 51.0 50.6 49.3 61.0 37.2 35.6 34.3 34.5 32.5 49.3
ru 62.2 63.6 63.1 62.7 63.1 68.4 50.1 49.9 49.7 48.6 49.3 59.9
sv 62.3 63.5 63.5 63.1 61.2 67.7 47.9 48.2 47.6 46.2 46.2 52.0
sw 14.8 14.4 14.3 14.2 13.8 17.4 7.8 7.2 7.1 6.9 6.3 10.7
te 1.2 1.2 1.2 1.7 1.1 8.4 0.4 0.3 0.3 0.5 0.3 4.3
th 36.1 35.8 35.6 35.6 28.3 39.0 21.6 23.1 22.2 21.6 16.8 24.6
tr 53.1 54.5 53.7 52.9 51.2 62.0 37.3 37.4 37.8 37.0 36.1 48.1
uk 51.4 51.5 51.2 49.9 49.2 61.2 34.5 33.2 33.8 32.5 32.4 48.3
vi 59.6 59.8 59.5 58.5 58.8 68.4 41.4 41.9 41.9 40.6 40.3 52.3
zh 44.1 45.7 44.1 41.9 36.1 53.9 30.7 32.5 32.0 30.6 23.7 46.8
avg 47.2 47.4 47.1 46.3 45.0 54.1 34.8 34.9 34.4 33.6 32.7 42.6
Table 9: Image-to-text (text retrieval) and text-to-image (image retrieval) zero-shot recall@1 results on all 36 languages
of Crossmodal-3600 , with mSigLIP models trained at different batch sizes for 30 B total examples seen. *32 k represents
the scaled up results as described in Section 4.6.
16F. Model Card
We provide a description of our models following [32].
•Model Architecture: The model is trained using the
contrastive pre-training technique with sigmoid loss
as described in this paper. This contrastive model
contains two encoders, i.e. vision transformer en-
coder [17] and language transformer encoder [47]. The
vision and language encoders always have the same
size, one of ViT-B, ViT-L and SoViT-400M [1].
•Inputs: The vision encoder takes an image ( 224
2243,2562563,3843843,5125123) as
input. The text encoder takes a tokenized text [38, 54]
cropped to the ﬁrst 64 tokens as input.
•Outputs: The vision and text encoders both output a d
dimensional feature vector, where dis 768, 1024 and
1152 for ViT-B, ViT-L and SoViT-400M, respectively.
•Intended Use: The models are designed for multi-
modal research purposes. The models can be used
for zero-shot image classiﬁcation and zero-shot image-
text retrieval by comparing both feature vectors. We
provide both en-only and i18n-trained models to en-
courage research on the impact of this choice.
•Training Data: The contrastive model is pre-trained
from-scratch using the WebLI [13] dataset. SigLIP
models are pre-trained on a WebLI subset ﬁltered to
contain mostly English. mSigLIP models are pre-
trained on the WebLI dataset without language ﬁlters.
•Evaluation Data: Zero-shot classiﬁcation is per-
formed on ImageNet [14], ImageNet v2 [39], Ima-
geNet Real [3], and ObjectNet [2]. Zero-shot re-
trieval is performed on COCO [11] and the multilin-
gual XM3600 dataset [44].
•Hardware & Software : The models are developed
in the bigvision codebase [5, 4] and trained on
Google Cloud TPUs.
17