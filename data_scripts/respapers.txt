Auffusion Leveraging the Power of Diffusion and Large Language Models for
TexttoAudio Generation
Jinlong Xue Yayue Deng Yingming Gao Ya Li
Beijing University of Posts and Telecommunications
jinlong xue yayuedeng yingminggao yli01 bupteducn
Abstract
Recent advancements in diffusion models and large lan
guage models LLMs have significantly propelled the field
of AIGC TexttoAudio TTA a burgeoning AIGC appli
cation designed to generate audio from natural language
prompts is attracting increasing attention However ex
isting TTA studies often struggle with generation quality
and textaudio alignment especially for complex textual
inputs Drawing inspiration from stateoftheart Textto
Image T2I diffusion models we introduce Auffusion a
TTA system adapting T2I model frameworks to TTA task
by effectively leveraging their inherent generative strengths
and precise crossmodal alignment Our objective and sub
jective evaluations demonstrate that Auffusion surpasses
previous TTA approaches using limited data and computa
tional resource Furthermore previous studies in T2I rec
ognizes the significant impact of encoder choice on cross
modal alignment like finegrained details and object bind
ings while similar evaluation is lacking in prior TTA works
Through comprehensive ablation studies and innovative
crossattention map visualizations we provide insightful as
sessments of textaudio alignment in TTA Our findings re
veal Auffusions superior capability in generating audios
that accurately match textual descriptions which further
demonstrated in several related tasks such as audio style
transfer inpainting and other manipulations Project page
is available at httpsauffusiongithubio
1 Introduction
Texttoaudio TTA generation is an emerging application
that focuses on synthesizing diverse audio outputs based
on text prompts With the integration of artificial intelli
gence into the realm of AIGC the scope of TTA appli
cations has expanded significantly covering areas such as
movie dubbing and musical composition Early TTA mod
els as referenced in 22 29 primarily relied on onehot
labels leading to the generation of monotonous audio con
Corresponding authorstrained by limited label space and generative capacity In
contrast natural descriptive text delivers more comprehen
sive and finegrained information Thereby the following
works 9 16 19 27 28 55 develop their models based on
textual content
Recent advancements in diffusion generative models
15 48 and large language models 6 30 39 40 47
have showcased remarkable capabilities in content gener
ation and understanding Leveraging these advancements
the first diffusion based TTA Diffsound 55 outperforms
previous TTA systems by generating discrete tokens quan
tized from melspectrograms using a diffusion model Later
Diffsound is surpassed by AudioGen 24 using an autore
gressive model in a discrete space of waveform Inspired
by 43 AudioLDM 27 is the first to utilize a contin
uous latent diffusion model LDM 43 achieving better
quality and computational efficiency compared to other dis
crete token based TTA systems 24 55 Similarly many
wellperformed TTA systems including AudioLDM2 28
Tango 9 MakeanAudio 12 16 19 integrate LDM into
their TTA frameworks to facilitate denoising processes in
the latent space However these models still require ex
tensive computational resources and largescale datasets for
training from scratch Moreover these models only em
phasis coarsegrained performance and neglect finegrained
textaudio alignment Our work concentrates on addressing
these two critical challenges providing effective solutions
and valuable insights
In developing a powerful TTA model two primary ob
jectives are paramount 1 mastering the distribution of
natural audio and 2 achieving precise textaudio align
ment These competencies are also paralleled in T2I 2 45
tasks where robust generative abilities and accurate text
image alignment are similarly crucial To this end we in
troduce our TTA system Auffusion that adapts Latent Dif
fusion Model LDM originally pretrained for T2I tasks
This adaptation enables Auffusion to leverage the LDMs
inherent generative strength and transfer it alignment under
standing effectively for textaudio alignment in TTA appli
cation The comprehensive subjective and objective evalua
1arXiv240101044v1  csSD  2 Jan 2024tion metrics show our proposed system Auffusion achieves
better quality and alignment Moreover Auffusion demon
strates performance comparable to other baseline models
trained on datasets 60 times larger Riffusion 7 an app
for 5second music generation also uses a pretrained LDM
43 However it is specifically designed for music and does
not extend to broader TTA tasks Additionally Riffusion
simply quantizes audio signals into images a nonreversible
process that leads to a great precision loss In contrast our
Auffusion model features a carefully designed feature space
transformation pipeline enabling lossless audio conversion
On the other hand the text encoder serves as a critical
bridge between text and audio representing a key compo
nent in TTA systems Different from the extensive stud
ies 1 17 45 conducted in the T2I domain where the im
pact of text encoders on aspects such as finegrained de
tails and object bindings has been widely explored their
influence in TTA has not been thoroughly explored Gen
erally text encoders in existing TTA systems fall into two
categories 1 multimodal contrastive learningbased mod
els such as CLIP 39 and CLAP 6 and 2 textonly
large language models LLMs like BERT 4 and T5 40
However findings from previous studies can sometimes be
contradictory Diffsound 55 employs CLIP pretrained
on textimage pairs claiming superior performance over
textonly model BERT Conversely AudioLDM 27 uses
CLAP model pretrained on textaudio pairs suggesting ad
vantages using audio only features over using combined
audiotext features or textonly features Building upon
the same LDM used in AudioLDM Tango 9 owes dif
ferent opinions They advocate for instructiontuned LLMs
FlanT5 47 to better grasp textual descriptions and cross
modal correlations challenging the notion of embedding
audio and text in a shared space However comprehensive
ablation studies using these text encoders are lacking
To address the debates outlined above our study con
ducts a thorough investigation into the performance of var
ious text encoders and baseline models Moving beyond
traditional evaluation metrics we innovatively assess text
audio alignment by visualizing the crossattention map to
provide intuitive observation for the first time in TTA task
We find that our model achieves better finegrained text
audio alignment Additionally we demonstrate versatile
audio manipulations enabled by our models generative ca
pacity and clear textaudio alignment
In summary the contributions of our work are 1 We
propose Auffusion a TTA model that integrates a pow
erful pretrained LDM from T2I in order to inherit gen
erative strengths and enhance crossmodal alignment and
our method demonstrates superior performance compared
to existing TTA systems 2 We conduct an extensive in
vestigation into the performance between different text en
coders and we provide a novel and insightful demonstrationin TTA task to assess textaudio alignment utilizing visual
izations of crossattention maps across different models
2 Related Work
21 TexttoImage Synthesis
TexttoImage T2I synthesis particularly through diffu
sion models has seen significant advancements in recent
years Pioneering models like DALLE 41 treats T2I as
a sequencetosequence translation task encoding images
into discrete latent tokens using pretrained VQV AE 52
DALLE 2 42 employs the CLIP text encoder and two
diffusion models first predicting CLIP 39 visual features
from text and then synthesizing the image Another famous
model Imagen 45 uses the T5 encoder 40 for text feature
extraction and a cascade of diffusion models for initial im
age synthesis and subsequent superresolution Stable Dif
fusion 43 optimizes computational efficiency by mapping
images from pixel to compressed latent space using a con
tinuous V AE trained with discriminators followed by dif
fusion in this latent space These models 36 42 43 45
demonstrate remarkable diversity and quality in image gen
eration guided by text prompts and operating either directly
in image space or within a latent space
22 TexttoAudio Synthesis
TexttoAudio TTA synthesis has witnessed significant
advancements Diffsound 55 leverages VQV AE 52
model trained on melspectrograms and convert them into
discrete codes where a nonautoregressive token based dif
fusion model is then used to generate audio signals Sim
ilarly AudioGen 24 uses a VQV AE based approach 3
but focuses on encoding raw waveform data and employs
an autoregressive model for generation Other studies
include the use of Latent Diffusion Models LDMs as
seen in AudioLDM 12 27 28 MakeAnAudio 12 16
19 and Tango 9 AudioLDM 27 utilizes audio fea
tures extracted by a pretrained contrastive textaudio model
CLAP 6 as a condition during training while leveraging
text features during inference This approach benefits from
CLAPs ability to map audio and captions to a shared latent
space AudioLDM2 28 first employs an autoregressive
model AR to generate AudioMAE 18 features from text
then uses them to condition the LDM These two methods
both alleviate the reliance of audiotext pair data Other
methods 9 16 19 on the other hand employ text features
in both training and inference stages MakeAnAudio 19
LDM is similar to AudioLDM MakeAnAudio2 16 em
phasize the temporal information by changing 2D spatial
structure to 1D temporal structure and they additionally
replace UNet design to transformer However neither of
these two models is open source
2Figure 1 An overview of Auffusion architecture The whole training and inference process include backandforth transformation between
four feature spaces audio spectrogram pixel and latent space Note that UNet is initialized with pretrained texttoimage LDM
3 Auffusion
31 Overview
Our proposed method Auffusion as depicted in Fig 1 has
four main components 1 text encoder 2 latent diffusion
model LDM 3 pixel V AE 4 HiFiGAN vocoder In or
der to achieve the TTA task and utilize the powerful pre
trained model from T2I task the whole process involves
conversion between four feature spaces audio spectro
gram pixel and latent space The spectrogram feature is a
key proxy that bridges the audio space and pixel space Dur
ing training audio is first converted into melspectrogram
and normalize for image space then LDM is conditioned on
textual embeddings extracted by textual condition encoder
and trained in the pixel space learned by V AE In inference
this process is reversed starting from standard Gaussian
noise the latent representation is generated through reverse
diffusion process conditioned on text embeddings There
after the pixel V AE decoder reconstructs the pixel space and
generated image is denormalized into melspectrogram Fi
nally the pretrained HiFiGAN vocoder synthesizes the au
dio from this melspectrogram
32 Feature Space Transformation
Given an audiotext pair we first convert the audio signal
xaudioRTinto melspectrogram xmelRdl where d
andlrepresent the melchannels and the number of frames
respectively In order to transform melspectrogram into
imagelike data without precision loss we conduct nor
malization by utilizing the mean µand variance δcalcu
lated from the whole dataset rather than on individual mel
spectrogram 7 The normalized spectrogram xnorm canbe viewed as grayscale image and then converted into RGB
image data xRcdl where cdandlare referred to as
the image channel height and width respectively
33 Latent Diffusion Model
To guide the construction of the audio signals pixel dis
tribution z0using a text prompt τ we finetune the UNet
diffusion module by minimizing mean squared error MSE
in the noise space The objective function is defined as
ℓθϵθzt t τϵ2
2 1
Here ϵ N 0 Irepresents Gaussian noise tis a ran
dom time step and ϵθis the textguided denoising network
comprising a UNet with a crossattention component for
text guidance τ
In this process the V AE encoder processes the image
like input xinto compressed latent vector z0 The diffusion
process then operates in this latent space gradually trans
forming z0into Gaussian noise zT The model is trained
to reverse this transformation recovering the original data
distribution from the noise This process involves two key
steps the forward process that converts z0intozTand the
reverse process that recovers z0from zT
Forward process is defined by a fixed Markov chain
from data z0to the latent variable zT
qz1     z Tz0 TY
t1qztzt1 2
The entire procedure transforms the initial latent data z0
into noise latent variables zTin accordance with a prede
termined noise schedule β1     β T
qztzt1 Nztp
1βtzt1 βtI 3
3where βtis a small positive constant qztzt1represents
a function where a small Gaussian noise is added to the dis
tribution of zt1
Reverse process converts the latent variables from zTto
z0with learnable parameters θ aimed at recovering samples
from Gaussian noise zT N0 I
pθz0     z T1zT TY
t1pθzt1zt 4
pθzt1zt Nzt1µθzt tΣθzt t 5
Note that µθtakes the diffusion step tNand variable
ztas inputs and outputs zt1for each iteration
34 Conditioning Processes
The previous work AudioLDM 27 adopts concatenation
operation between pooled text embedding extracted from
CLAP and time embedding to guide the generation pro
cess in LDM By contrast we turn diffusion model gener
ation into more flexible and understandable by conducting
a crossattention mechanism 53 between conditional em
bedding sequence and latent vectors in the UNet backbone
More formally we donates ϑiztRNdi
ϵa interme
diate representation of the ithlayer of UNet estimation
ϵθ Then a linear projection is applied to the deep spatial
features of the noisy data ϑizt
QWi
qϑizt 6
The conditional embedding τis also projected via learned
linear projections
KWi
kτ V Wi
vτ 7
where Wi
qRddτWi
vRddi
ϵandWi
kRddτ
are learnable matrices The attention value and attention
score are calculated as follows
Attention Q K V  score Q KV
score Q K softmax QKT

d8
The condition approach allows us to visualize the 2D at
tention map 12 35 50 by reshaping attention score back
to latent image shape Furthermore it provides an intuitive
measurement to access the understanding ability of various
text encoders We discuss the compared results in Sec 43
Meanwhile based on the visualized attention map we find
that the pretrained LDM is capable of adequately transfer
ring crossmodal understanding ability from T2I to TTA
task resulting in better alignment Overall we highlight
the importance of the conditioning process in enhancing the
audiotext models ability to extract key information from
text descriptions and accurately match the desired audio as
demonstrated in Fig 235 Text Encoder
Inspired by eDiffI 1 who uses an ensemble of encoders
to provide multisource information to LDM we combine
CLAP and FlanT5 text encoders as conditions We use ran
dom dropout on each of these embeddings independently
during training When all two embeddings are dropped it
corresponds to unconditional training which is useful for
performing classifierfree guidance 14 We conduct com
prehensive comparison for various text encoders and results
are shown in Sec 43
36 ClassifierFree Guidance
To guide the reverse diffusion process we utilize classifier
free guidance 14 based on the text input τusing
ˆϵθzt t τ  1  wϵzt t τwϵzt t9
At the inference stage the guidance scale wdetermines how
much the text input influences the noise estimation ˆϵθcom
pared to the unconditional estimation We randomly discard
the text condition at a rate of 10 during training
4 Experiments
41 Experimental Setup
Dataset We follow previous works 9 16 27 and use a va
riety of different audio datasets with audio caption or audio
labels to train our model including AudioCaps AC 20
WavCaps 33 MACS 32 Clotho 5 ESC50 38 Urban
Sound 46 Music Instruments dataset1and GTZAN 51
The WavCaps dataset consists of ChatGPTassisted weakly
labeled audio captions for the FreeSound2 BBC Sound Ef
fects SFX3 SoundBible4and the AudioSet strongly la
beled subset 11 containing 403050 audio clips with an
average duration of 68 seconds AudioCaps is a subset of
AudioSet AS 8 with handcrafted captions and it contains
about 46K tensecond audio clips This results in a dataset
composed of 047 million audio text pairs with a total du
ration of approximately 77K hours
It is noted that the duration of the audio samples in
AudioSet and AudioCaps is 10 seconds while it is much
longer in FreeSound and BBC SFX datasets 86s and 115s
in average To avoid the imbalance caused by longer au
dio which often contains repeated sounds like background
sounds we only use the first thirty seconds of audios for
all datasets and randomly select tensecond segments dur
ing training Finally we have in total 04M audio samples
with a total duration of around 2K hours for model training
1httpswwwkagglecomdatasetssoumendraprasadmusical
instrumentssounddataset
2httpsfreesoundorg
3httpssoundeffectsbbcrewindcouk
4httpssoundbiblecom
4Table 1 The comparison between our model Auffusion and baseline TTA models Although our model Auffusion is only trained on a
much smaller dataset AC our model outperforms other baselines on AC test set and has comparable zeroshot result in Clotho test set
Model Pretrain Durationh ParamsAudioCaps Clotho
FD FADKL IS CLAP FD FADKL IS CLAP
Riffusion  1990 11B 2628 468 157 721 476 3163 611 266 650 475
AudioGenv2medium  6824 15B 1786 173 159 931 485 2326 255 256 719 467
AudioLDMSfullv2  9031 421M 3058 440 179 696 421 2651 354 262 658 499
AudioLDMLfull  9031 975M 2977 404 178 750 429 2413 302 256 749 510
AudioLDM2  29510 11B 2605 194 176 731 460 2353 306 247 905 481
AudioLDM2large  29510 15B 2559 219 170 783 479 2331 300 241 888 490
Tango  145 13B 2482 177 143 720 550 3167 322 257 718 466
TangoFull  3400 13B 3068 367 163 479 519 2583 317 235 651 503
Auffusion  145 11B 2445 225 139 1014 547 2901 267 266 946 476
Auffusion  145 11B 2199 163 136 1057 553 2564 235 259 901 482
AuffusionFull  1990 11B 2411 167 146 839 516 1914 199 242 1033 528
AuffusionFull  1990 11B 2308 176 136 1028 556 1797 196 238 1129 550
Training Setup We utilize the pretrained Stable Dif
fusion v155 including its V AE and UNet and later fine
tune the UNet on audio datasets All datasets are re
sampled to 16kHz sampling rate and mono format with
samples padded to 1024 seconds We then extract mel
spectrograms from audios using parameters of 256 mel fil
ter bands 1024 window length 2048 FFT and 160 hop
size resulting in 12561024 melspectrograms akin to
grayscale images with 256 height and 1024 width in 1 chan
nels These are normalized and channelrepeated to cre
ate RGBlike images suitable for V AE encoder input For
highfidelity audio conversion previous methods adopt neu
ral vocoder 21 25 In order to match our need using our
specific melspectrogram parameters we train a new HiFi
GAN vocoder 21 using the same datasets described above
This training employs the AdamW optimizer 31 with a 2e
4 learning rate and 16 batch size on one A6000 GPU Fi
nally we freeze the text encoder and finetune the pretrained
Stable Diffusions UNet using AdamW optimizer with a
3e5 learning rate at a 20 batch size for 100K steps Our
model can be trained only taking a total of 48 hours on one
A6000 GPU
Objective Evaluation In our experimental evalua
tion we follow previous evaluation methods 9 16 27
and employ a suite of objective metrics to assess the
quality and fidelity of generated audio samples includ
ing Frechet Distance FD Frechet Audio Distance FAD
KullbackLeibler KL divergence Inception Score IS
and CLAP score Analogous to the Frechet Inception Dis
tance FID 13 used in image synthesis the FD score in
audio domain quantifies the global similarity between cre
ated audio samples and the target samples without the need
of using paired reference audio samples The IS score is ef
5httpshuggingfacecorunwaymlstablediffusionv15fective at assessing both the quality and variety of samples
The KL score is calculated using paired samples and it mea
sures the divergence between two probability distributions
These three metrics are all grounded in the advanced audio
classifier PANNs 23 FAD score has a similar idea to FD
but it uses VGGish 10 as feature extractor The evaluate
suite that we uses for FD FAD KL and IS is in project6 Be
sides we also use pretrained CLAP7model to compute the
similarity of the text caption and generated audio to evaluate
the textaudio alignment similar to CLIP score
Subjective Evaluation Following previous evaluation
method 9 24 in TTA field we ask five human evaluators
to assess two aspects of the generated audio including over
all audio quality OVL and relevance to the text caption
REL We randomly select 30 audio samples from each of
the AC and Clotho test sets and ask participants to rate them
on a scale from 1 to 100 with 10point intervals Results are
shown in Table 3
Baseline Models To comprehensively compare our
models with others our study employs five baseline mod
els including three diffusion based models Riffusion 7
AudioLDM 27 AudioLDM2 28 Tango 9 and one
autoregressive generative model based on discrete audio
token AudioGen 24 We reimplement Riffusion8 orig
inally trained on music datasets for only 5s audio to gen
erate 10s audio using a 160 hop length and trained it on
our datasets We use the other baseline models released
by authors on huggingface respectively AudioLDMS
fullv29and AudioLDMLfull10have 412M and 975M pa
rameters and trained them on AudioCaps AudioSet and
6httpsgithubcomhaoheliuaudioldm eval
7httpshuggingfacecolaionclaphtsatunfused
8httpshuggingfacecoriffusionriffusionmodelv1
9httpshuggingfacecocvsspaudioldmsfullv2
10httpshuggingfacecocvsspaudioldmlfull
5other 2 datasets including 9031h audio data for more than
15M train steps AudioLDM211and AudioLDM2large12
have 11B and 15B parameters respectively and trained on
29510h diverse audio data Tango is trained on Audio
Caps dataset and TangoFull is trained on datasets similar
with our datasets settings but using different preprocessing
AudioGenv2medium13has 15B parameters and is trained
on AudioCaps AudioSet and eight other datasets around
4K hours data
42 Results
Evaluation Setup We compare our model Auffusion
trained on single dataset AudioCaps AC and Auffusion
Full trained on whole datasets with other baselines in both
AudioCaps test set and Clotho test set We also conduct ab
lation studies on the impact of pretrained SD models Both
Auffusion and AuffusionFull use CLIP as default text en
coder We report our result in Table 1
Objective Evaluation When trained solely on AC
dataset our model Auffusionwithpretrained outperforms
the previous stateoftheart Tango in AC test set with 2199
FD 163 FAD 136 KL 1057 IS and 553 in CLAP
score and achieve comparable zeroshot Clotho test set re
sults to other baseline models trained on datasets over 60
times larger Notably Tango and Auffusionnopretrained
both trained solely on AC dataset exhibit a huge drop on
Clotho test set indicating a problem of overfitting In con
trast our Auffusionwithpretrained still maintain its perfor
mance demonstrating generalization ability This suggests
that the generative capacity and crossmodal alignment of
the pretrained SD model can be effectively transferred to
melspectrogram domain even with a small dataset
When trained in much larger datasets using same training
steps AuffusionFullwithpretrained achieves the state
oftheart performance in Clotho test set Besides it
shows a negligible decrease in AC test set compared to
Auffusionwithpretrained and records a slight increase in
CLAP score This indicates the robustness and strong
generalization ability of the pretrained SD even when
dealing with different data distributions Additionally
both our Auffusionwithpretrained and AuffusionFull
withpretrained models significantly outperform other base
lines in terms of IS and CLAP scores A higher IS score
implies that our model can generate melspectrograms with
both high fidelity and diversity A higher CLAP score indi
cates our models enhanced capability to adhere to textual
descriptions and produce more relevant audio We also find
that our reimplement Riffusion yields much inferior re
sults indicating that the precision loss caused by the quan
tization transform has a great impact
11httpshuggingfacecocvsspaudioldm2
12httpshuggingfacecocvsspaudioldm2large
13httpshuggingfacecofacebookaudiogenmediumSubjective Evaluation Our subjective human evalua
tion results are presented in Table 3 In the All Event
column our model Auffusionwclip demonstrates superior
performance over other baseline models achieving an OVL
score of 6936 and a REL score of 7025 Additionally the
REL has significant gains compared to other models show
ing strong textaudio alignment We delve deeper into the
impact of numbers of events and provide intuitive visualiza
tion for textaudio alignment in Sec 43
43 Analysis
Effect of Text Encoder To assess the performance of dif
ferent text encoders and explore the effectiveness of a dual
text encoder approach in TTA applications we compared
several encoder options including CLIP CLAP FlanT5
base FlanT5large and a combined CLAP and FlanT5
large encoder The original SDv15 uses the CLIP L14
model trained on textimage pairs while AudioLDM em
ploys the CLAP model trained on textaudio pairs Tango
suggests that FlanT5 an instructiontuned LLM enhances
textual understanding but lacks an encoder ablation study
Inspired by eDiffI 1 which uses an ensemble of encoders
to provide multisource information we experimented with
a combined CLAP and FlanT5large encoder by concatena
tion The results are shown in Table 2
Our findings reveal that FlanT5large surpasses FlanT5
base in all evaluation metrics underscoring the importance
of the text encoders size for understanding textual cap
tions FlanT5large shows results comparable to CLIP with
CLIP excelling in IS score and FlanT5large in FAD score
This mirrors Imagen 45 findings where T5XXL matches
CLIP in objective scores but exceeds smaller T5 models
Notably the CLAP model outperforms both text encoders
especially in FAD and CLAP scores demonstrating its ad
vanced audio domain expertise The elevated CLAP score
may be attributed to the use of same model during train
ing Combining CLAP and FlanT5large encoders lever
ages both acoustic and rich semantic knowledge yielding
the best overall objective performance
TextAudio Alignment In our investigation into the im
pact of varying text encoders on TTA alignmenta crucial
aspect of TTA taskswe are the first to examine the cross
attention mechanisms between text and LDM outputs using
method 12 This approach allows us to intuitively observe
the focal points of the LDM during the TTA process How
ever due to the global conditioning approach employed by
AudioLDM and the use of GPT for generating AudioMAE
features in AudioLDM2 these models do not provide a
direct correlation between text and LDM Therefore we
present a comparative visualization of crossattention maps
using various encoders within the Auffusion framework
and the Tango models who also adopt cross attention as
depicted in Figure 2 For consistency we standardize the
6Figure 2 The visualization of cross attention maps for Auffusion with different text encoders and Tango model Auffusionnopretrain use
fixed CLIP encoder and LDM is trained from scratch The LDMs in 2 to 4 rows are initialized with SDv15 with different encoders The
last row shows the Tangos cross attention map and Tango uses FlanT5large as condition encoder
Table 2 The results evaluated on AudioCaps test set and Clotho test set with different settings of conditional encoder
AudioCaps Clotho
AuffusionFullwith FD FADKL IS CLAP FD FADKL IS CLAP
CLIP 2308 176 136 1028 556 1797 196 238 1129 550
CLAP 2192 157 135 1001 583 1779 182 232 1102 591
FlanT5base 2300 155 150 1011 530 2005 203 250 1088 529
FlanT5large 2231 141 142 937 546 1809 162 235 1016 556
ClapFlanT5large 2255 150 132 1034 574 1759 187 225 1093 595
diffusion steps to 50 and adjust all crossattention maps to a
uniform square dimension for clear comparison
Upon comparing the attention maps of the first and sec
ond lines it is evident that using pretrained LDM exhibits
superior distinguishability with clear attention across al
most all tokens Notably the gunshot token within the
highlighted red area is prominent and the dogs and gun
shot sounds in the generated audio overlap in the latter
section This suggests that the pretrained LDM possesses
advanced prior knowledge enabling it to effectively trans
fer its textimage alignment capabilities to textaudio align
ment tasks Although the CLAP model achieves higher
objective scores it surprisingly produces similar attention
maps for each token Furthermore the dogs and gun
shot sounds are indistinguishable occurring simultane
ously in the generated audio which indicates that the CLAP
model struggles to differentiate and isolate finegrained
events or entities We believe the reason is that CLAP model
can only gather global acoustic information and have issue capturing temporal and local information in the text
Furthermore all the objective evaluation also only focus
and compute global features therefore such inconsistency
exists Recent study 54 corroborates that current CLAP
models do not truly comprehend natural language focusing
instead on global information
In contrast when comparing the attention maps of the
last two lines where Tango also employs FlanT5large as
a text encoder the Tangos attention maps appear muddled
particularly for the dog token highlighted in blue area
which results in the omission of the dog sound in the gen
erated audio Additionally we find that Tango often pro
duces extraneous sounds such as unintended bird noises
that are not present in the text captions These findings high
light that Auffusion by leveraging the robust textimage
alignment capabilities of pretrained LDMs can generate
audio that more accurately reflects the given captions
Performance against Number of Events To better as
sess finegrained textaudio alignment we evaluate perfor
7Table 3 Subjective evaluation for all baseline models and different encoders used in Auffusion categorized by the number of events in the
text OVL measures the overall quality and REL shows the relevance ACC represents the mean accuracy of audio events matching the text
in multievent conditions indicating the finegrained alignment between text and audio
ModelAll Event Single Event Two Events Multi Events
OVL REL RELCLAPRELCLAPRELCLAPACC
Groundtruth 7156 7401 7370 509 7550 515 7285 487 846
AudioGenv2medium 6386 5980 5975 472 5850 459 6115 483 685
AudioLDMLfull 6033 5736 5800 511 6040 508 5370 511 532
AudioLDM2large 6553 5923 6150 506 6020 483 5600 469 614
TangoFull 6778 6505 6375 482 6775 523 6265 536 696
Auffusionwclip 6936 7025 7065 525 7345 532 6665 553 739
Auffusionwclap 6976 6776 6895 556 7125 575 6310 589 710
Auffusionwflant5 7013 7065 6955 511 7460 530 6780 543 733
Auffusionwclapflant5 6980 7186 7210 553 7390 568 6960 589 741
mance across varying event numbers in the AudioCaps and
Clotho test set For instance a sequence like A man talking
followed by plastic clacking then a power tool drilling com
prises three distinct events We categorize the test sets into
three groups single event two events and multiple events
three or more randomly selecting 20 captions from each
category for generation Human raters are asked to rate
the relevance between text and audio for each group using
REL metric Besides we select an additional 80 samples
for multiple events We ask raters to count the number of
events in the audios that accurately appear in the text and
calculate the mean accuracy denoted as ACC to reflect the
finegrained textaudio alignment These results for base
lines and Auffusion with various encoders trained on whole
datasets are presented in Table 3 Additionally we used the
objective CLAP score for comparison
We assume that human raters can directly and faithfully
represent true performance We find that the CLAP score
can not accurately reflect detailed alignment ability par
ticularly in multievent evaluation compared with human
evaluation Our analysis concludes that CLAP primarily
extracts global features lacking in finegrained evaluation
capacity Additionally we observe that AudioLDM Au
dioLDM2 and AudioGen exhibit inferior performance in
REL and we can tell from ACC score that they fail to gen
erate matching audio in multievent scenarios This is at
tributed to AudioLDM using globally pooled CLAP em
beddings for conditioning while AudioLDM2 first employs
an autoregressive model AR to generate AudioMAE fea
tures from text then uses these to condition the LDM Con
sequently finegrained information is lost in AudioLDM
and the AR model in AudioLDM2 introduces error accu
mulation In contrast Tango and our Auffusion which
adopt crossattention between text embedding sequences
and LDM demonstrate better alignment Moreover our
findings across various encoders align with the attention
map results illustrated in previous part Despite Auffusionscombination with the CLAP encoder yielding a higher
CLAP score evaluations using ACC and REL especially
in multievent scenarios reveal that the CLAP encoder cap
tures less finegrained information compared to CLIP and
FlanT5 encoders
Applications Leveraging our systems exceptional text
comprehension capabilities and robust textaudio align
ment we demonstrate its versatile applications inspired by
T2I tasks 34 44 49 56 These include audio style trans
fer audio inpainting and attentionbased techniques such as
word swap and text attention reweighting We demonstrate
these capabilities in Appendix E Our method offers a sig
nificantly more controllable and finegrained manipulation
compared to previous methods9 24 27 28
5 Conclusion
In this study we introduce Auffusion a texttoaudio TTA
generation model that harnesses the robust generative capa
bilities and precise crossmodal alignment abilities of pre
trained texttoimage T2I models Our extensive objec
tive and subjective evaluations demonstrate that Auffusion
surpasses other stateoftheart models achieving superior
performance with limited data and computational resources
Recognizing the significant impact of different encoders
on crossmodal alignment in T2I we pioneer in the TTA
field by conducting comprehensive investigations and inno
vatively adopting crossattention map visualization This
approach offers an intuitive evaluation of textaudio align
ment Our findings demonstrate that Auffusion exhibits an
exceptional ability to generate audio that accurately aligns
with text descriptions surpassing existing methods which
further evidenced in several audio manipulations includ
ing audio style transfer inpainting word swapping and re
weighting In the future we aim to delve into a broader
spectrum of innovative audio applications based on the ro
bust textaudio alignment capabilities of our system
8References
1 Yogesh Balaji Seungjun Nah Xun Huang Arash Vahdat
Jiaming Song Karsten Kreis Miika Aittala Timo Aila
Samuli Laine Bryan Catanzaro Tero Karras and MingYu
Liu ediffi Texttoimage diffusion models with an ensem
ble of expert denoisers CoRR  abs221101324 2022 2 4
6
2 Junsong Chen Jincheng Yu Chongjian Ge Lewei Yao Enze
Xie Yue Wu Zhongdao Wang James T Kwok Ping Luo
Huchuan Lu and Zhenguo Li Pixart α Fast training of dif
fusion transformer for photorealistic texttoimage synthesis
CoRR  abs231000426 2023 1
3 Alexandre D efossez Jade Copet Gabriel Synnaeve and
Yossi Adi High fidelity neural audio compression CoRR 
abs221013438 2022 2
4 Jacob Devlin MingWei Chang Kenton Lee and Kristina
Toutanova BERT pretraining of deep bidirectional trans
formers for language understanding In NAACLHLT 1 
pages 41714186 Association for Computational Linguis
tics 2019 2
5 Konstantinos Drossos Samuel Lipping and Tuomas Virta
nen Clotho an audio captioning dataset In 2020 IEEE
International Conference on Acoustics Speech and Signal
Processing ICASSP 2020 Barcelona Spain May 48 2020 
pages 736740 IEEE 2020 4 1
6 Benjamin Elizalde Soham Deshmukh Mahmoud Al Ismail
and Huaming Wang CLAP learning audio concepts from
natural language supervision CoRR  abs220604769 2022
1 2
7 Seth Forsgren and Hayk Martiros Riffusion  Stable dif
fusion for realtime music generation 2022 2 3 5
8 Jort F Gemmeke Daniel P W Ellis Dylan Freedman Aren
Jansen Wade Lawrence R Channing Moore Manoj Plakal
and Marvin Ritter Audio set An ontology and human
labeled dataset for audio events In 2017 IEEE Interna
tional Conference on Acoustics Speech and Signal Process
ing ICASSP 2017 New Orleans LA USA March 59 2017 
pages 776780 IEEE 2017 4
9 Deepanway Ghosal Navonil Majumder Ambuj Mehrish
and Soujanya Poria Texttoaudio generation using
instructiontuned LLM and latent diffusion model CoRR 
abs230413731 2023 1 2 4 5 8
10 Shawn Hershey Sourish Chaudhuri Daniel P W Ellis
Jort F Gemmeke Aren Jansen R Channing Moore Manoj
Plakal Devin Platt Rif A Saurous Bryan Seybold Mal
colm Slaney Ron J Weiss and Kevin W Wilson CNN ar
chitectures for largescale audio classification In 2017 IEEE
International Conference on Acoustics Speech and Signal
Processing ICASSP 2017 New Orleans LA USA March
59 2017  pages 131135 IEEE 2017 5
11 Shawn Hershey Daniel P W Ellis Eduardo Fonseca Aren
Jansen Caroline Liu R Channing Moore and Manoj Plakal
The benefit of temporallystrong labels in audio event clas
sification In IEEE International Conference on Acoustics
Speech and Signal Processing ICASSP 2021 Toronto ON
Canada June 611 2021  pages 366370 IEEE 2021 4 112 Amir Hertz Ron Mokady Jay Tenenbaum Kfir Aberman
Yael Pritch and Daniel CohenOr Prompttoprompt im
age editing with crossattention control In ICLR  OpenRe
viewnet 2023 4 6
13 Martin Heusel Hubert Ramsauer Thomas Unterthiner
Bernhard Nessler and Sepp Hochreiter Gans trained by a
two timescale update rule converge to a local nash equilib
rium In Advances in Neural Information Processing Systems
30 Annual Conference on Neural Information Processing
Systems 2017 December 49 2017 Long Beach CA USA 
pages 66266637 2017 5
14 Jonathan Ho and Tim Salimans Classifierfree diffusion
guidance CoRR  abs220712598 2022 4
15 Jonathan Ho Ajay Jain and Pieter Abbeel Denoising diffu
sion probabilistic models In NeurIPS  2020 1
16 Jiawei Huang Yi Ren Rongjie Huang Dongchao Yang
Zhenhui Ye Chen Zhang Jinglin Liu Xiang Yin Zejun Ma
and Zhou Zhao Makeanaudio 2 Temporalenhanced text
toaudio generation CoRR  abs230518474 2023 1 2 4
5
17 Kaiyi Huang Kaiyue Sun Enze Xie Zhenguo Li and Xi
hui Liu T2icompbench A comprehensive benchmark for
openworld compositional texttoimage generation CoRR 
abs230706350 2023 2
18 PoYao Huang Hu Xu Juncheng Li Alexei Baevski
Michael Auli Wojciech Galuba Florian Metze and
Christoph Feichtenhofer Masked autoencoders that listen
InNeurIPS  2022 2
19 Rongjie Huang Jiawei Huang Dongchao Yang Yi Ren
Luping Liu Mingze Li Zhenhui Ye Jinglin Liu Xiang Yin
and Zhou Zhao Makeanaudio Texttoaudio generation
with promptenhanced diffusion models In ICML  pages
1391613932 PMLR 2023 1 2
20 Chris Dongjoo Kim Byeongchang Kim Hyunmin Lee and
Gunhee Kim Audiocaps Generating captions for audios
in the wild In Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computa
tional Linguistics Human Language Technologies NAACL
HLT 2019 Minneapolis MN USA June 27 2019 Volume
1 Long and Short Papers  pages 119132 Association for
Computational Linguistics 2019 4 1
21 Jungil Kong Jaehyeon Kim and Jaekyoung Bae Hifigan
Generative adversarial networks for efficient and high fi
delity speech synthesis In Advances in Neural Information
Processing Systems 33 Annual Conference on Neural Infor
mation Processing Systems 2020 NeurIPS 2020 December
612 2020 virtual  2020 5 1
22 Qiuqiang Kong Yong Xu Turab Iqbal Yin Cao Wenwu
Wang and Mark D Plumbley Acoustic scene generation
with conditional samplernn In ICASSP  pages 925929
IEEE 2019 1
23 Qiuqiang Kong Yin Cao Turab Iqbal Yuxuan Wang
Wenwu Wang and Mark D Plumbley Panns Largescale
pretrained audio neural networks for audio pattern recogni
tion IEEE ACM Trans Audio Speech Lang Process  28
28802894 2020 5
24 Felix Kreuk Gabriel Synnaeve Adam Polyak Uriel Singer
Alexandre D efossez Jade Copet Devi Parikh Yaniv Taig
9man and Yossi Adi Audiogen Textually guided audio gen
eration In ICLR  OpenReviewnet 2023 1 2 5 8
25 Sanggil Lee Wei Ping Boris Ginsburg Bryan Catanzaro
and Sungroh Yoon Bigvgan A universal neural vocoder
with largescale training In The Eleventh International Con
ference on Learning Representations ICLR 2023 Kigali
Rwanda May 15 2023  OpenReviewnet 2023 5
26 Benjamin Lefaudeux Francisco Massa Diana Liskovich
Wenhan Xiong Vittorio Caggiano Sean Naren Min Xu
Jieru Hu Marta Tintore Susan Zhang Patrick Labatut
and Daniel Haziza xformers A modular and hackable
transformer modelling library httpsgithubcom
facebookresearchxformers  2022 1
27 Haohe Liu Zehua Chen Yi Yuan Xinhao Mei Xubo Liu
Danilo P Mandic Wenwu Wang and Mark D Plumbley
Audioldm Texttoaudio generation with latent diffusion
models In ICML  pages 2145021474 PMLR 2023 1
2 4 5 8
28 Haohe Liu Qiao Tian Yi Yuan Xubo Liu Xinhao Mei Qi
uqiang Kong Yuping Wang Wenwu Wang Yuxuan Wang
and Mark D Plumbley Audioldm 2 Learning holistic
audio generation with selfsupervised pretraining CoRR 
abs230805734 2023 1 2 5 8
29 Xubo Liu Turab Iqbal Jinzheng Zhao Qiushi Huang
Mark D Plumbley and Wenwu Wang Conditional sound
generation using neural discrete timefrequency representa
tion learning In MLSP  pages 16 IEEE 2021 1
30 Yinhan Liu Myle Ott Naman Goyal Jingfei Du Mandar
Joshi Danqi Chen Omer Levy Mike Lewis Luke Zettle
moyer and Veselin Stoyanov Roberta A robustly optimized
BERT pretraining approach CoRR  abs190711692 2019
1
31 Ilya Loshchilov and Frank Hutter Decoupled weight decay
regularization In 7th International Conference on Learning
Representations ICLR 2019 New Orleans LA USA May
69 2019  OpenReviewnet 2019 5
32 Irene Mart ınMorat o and Annamaria Mesaros What is the
ground truth reliability of multiannotator data for audio
tagging In 29th European Signal Processing Conference
EUSIPCO 2021 Dublin Ireland August 2327 2021  pages
7680 IEEE 2021 4 1
33 Xinhao Mei Chutong Meng Haohe Liu Qiuqiang Kong
Tom Ko Chengqi Zhao Mark D Plumbley Yuexian Zou
and Wenwu Wang Wavcaps A chatgptassisted weakly
labelled audio captioning dataset for audiolanguage multi
modal research CoRR  abs230317395 2023 4
34 Chenlin Meng Yutong He Yang Song Jiaming Song Jiajun
Wu JunYan Zhu and Stefano Ermon Sdedit Guided im
age synthesis and editing with stochastic differential equa
tions In The Tenth International Conference on Learn
ing Representations ICLR 2022 Virtual Event April 2529
2022  OpenReviewnet 2022 8 4
35 Ron Mokady Amir Hertz Kfir Aberman Yael Pritch and
Daniel CohenOr Nulltext inversion for editing real images
using guided diffusion models In IEEECVF Conference on
Computer Vision and Pattern Recognition CVPR 2023 Van
couver BC Canada June 1724 2023  pages 60386047
IEEE 2023 436 Alexander Quinn Nichol Prafulla Dhariwal Aditya Ramesh
Pranav Shyam Pamela Mishkin Bob McGrew Ilya
Sutskever and Mark Chen GLIDE towards photorealis
tic image generation and editing with textguided diffusion
models In International Conference on Machine Learning
ICML 2022 1723 July 2022 Baltimore Maryland USA 
pages 1678416804 PMLR 2022 2
37 Nathana el Perraudin P eter Bal azs and Peter L
Søndergaard A fast griffinlim algorithm In IEEE
Workshop on Applications of Signal Processing to Audio
and Acoustics WASPAA 2013 New Paltz NY USA October
2023 2013  pages 14 IEEE 2013 2
38 Karol J Piczak ESC dataset for environmental sound clas
sification In Proceedings of the 23rd Annual ACM Con
ference on Multimedia Conference MM 15 Brisbane Aus
tralia October 26  30 2015  pages 10151018 ACM 2015
4 1
39 Alec Radford Jong Wook Kim Chris Hallacy Aditya
Ramesh Gabriel Goh Sandhini Agarwal Girish Sastry
Amanda Askell Pamela Mishkin Jack Clark Gretchen
Krueger and Ilya Sutskever Learning transferable visual
models from natural language supervision In ICML  pages
87488763 PMLR 2021 1 2
40 Colin Raffel Noam Shazeer Adam Roberts Katherine Lee
Sharan Narang Michael Matena Yanqi Zhou Wei Li and
Peter J Liu Exploring the limits of transfer learning with
a unified texttotext transformer J Mach Learn Res  21
140114067 2020 1 2
41 Aditya Ramesh Mikhail Pavlov Gabriel Goh Scott Gray
Chelsea V oss Alec Radford Mark Chen and Ilya Sutskever
Zeroshot texttoimage generation In ICML  pages 8821
8831 PMLR 2021 2
42 Aditya Ramesh Prafulla Dhariwal Alex Nichol Casey Chu
and Mark Chen Hierarchical textconditional image gener
ation with CLIP latents CoRR  abs220406125 2022 2
43 Robin Rombach Andreas Blattmann Dominik Lorenz
Patrick Esser and Bj orn Ommer Highresolution image syn
thesis with latent diffusion models In CVPR  pages 10674
10685 IEEE 2022 1 2
44 Chitwan Saharia William Chan Huiwen Chang Chris A
Lee Jonathan Ho Tim Salimans David J Fleet and Mo
hammad Norouzi Palette Imagetoimage diffusion mod
els In SIGGRAPH 22 Special Interest Group on Computer
Graphics and Interactive Techniques Conference Vancouver
BC Canada August 7  11 2022  pages 1511510 ACM
2022 8
45 Chitwan Saharia William Chan Saurabh Saxena Lala
Li Jay Whang Emily L Denton Seyed Kamyar Seyed
Ghasemipour Raphael Gontijo Lopes Burcu Karagol Ayan
Tim Salimans Jonathan Ho David J Fleet and Mohammad
Norouzi Photorealistic texttoimage diffusion models with
deep language understanding In NeurIPS  2022 1 2 6
46 Justin Salamon Christopher Jacoby and Juan Pablo Bello A
dataset and taxonomy for urban sound research In Proceed
ings of the ACM International Conference on Multimedia
MM 14 Orlando FL USA November 03  07 2014  pages
10411044 ACM 2014 4 1
1047 Sheng Shen Le Hou Yanqi Zhou Nan Du Shayne Long
pre Jason Wei Hyung Won Chung Barret Zoph William
Fedus Xinyun Chen Tu Vu Yuexin Wu Wuyang Chen Al
bert Webson Yunxuan Li Vincent Zhao Hongkun Yu Kurt
Keutzer Trevor Darrell and Denny Zhou Flanmoe Scal
ing instructionfinetuned language models with sparse mix
ture of experts CoRR  abs230514705 2023 1 2
48 Jascha SohlDickstein Eric A Weiss Niru Mah
eswaranathan and Surya Ganguli Deep unsupervised
learning using nonequilibrium thermodynamics In ICML 
pages 22562265 JMLRorg 2015 1
49 Roman Suvorov Elizaveta Logacheva Anton Mashikhin
Anastasia Remizova Arsenii Ashukha Aleksei Silvestrov
Naejin Kong Harshith Goka Kiwoong Park and Victor
Lempitsky Resolutionrobust large mask inpainting with
fourier convolutions In IEEECVF Winter Conference on
Applications of Computer Vision WACV 2022 Waikoloa HI
USA January 38 2022  pages 31723182 IEEE 2022 8
50 Raphael Tang Linqing Liu Akshat Pandey Zhiying Jiang
Gefei Yang Karun Kumar Pontus Stenetorp Jimmy Lin
and Ferhan Ture What the DAAM interpreting stable dif
fusion using cross attention In Proceedings of the 61st An
nual Meeting of the Association for Computational Linguis
tics Volume 1 Long Papers ACL 2023 Toronto Canada
July 914 2023  pages 56445659 Association for Compu
tational Linguistics 2023 4
51 George Tzanetakis and Perry R Cook Musical genre clas
sification of audio signals IEEE Trans Speech Audio Pro
cess  105293302 2002 4 1
52 A aron van den Oord Oriol Vinyals and Koray
Kavukcuoglu Neural discrete representation learning
InNIPS  pages 63066315 2017 2
53 Ashish Vaswani Noam Shazeer Niki Parmar Jakob Uszko
reit Llion Jones Aidan N Gomez Lukasz Kaiser and Illia
Polosukhin Attention is all you need In NIPS  pages 5998
6008 2017 4
54 HoHsiang Wu Oriol Nieto Juan Pablo Bello and Justin
Salamon Audiotext models do not yet leverage natural
language In IEEE International Conference on Acoustics
Speech and Signal Processing ICASSP 2023 Rhodes Island
Greece June 410 2023  pages 15 IEEE 2023 7
55 Dongchao Yang Jianwei Yu Helin Wang Wen Wang Chao
Weng Yuexian Zou and Dong Yu Diffsound Discrete dif
fusion model for texttosound generation IEEE ACM Trans
Audio Speech Lang Process  3117201733 2023 1 2
56 Lvmin Zhang and Maneesh Agrawala Adding condi
tional control to texttoimage diffusion models CoRR 
abs230205543 2023 8
11Auffusion Leveraging the Power of Diffusion and Large Language Models for
TexttoAudio Generation
Supplementary Material
A Dataset Details
Table 4 Statistics for the all datasets used in the this paper
Dataset Samples Hours Original Hours Processed Source
AudioCaps 41K 110h 110h 20
MACS 3930 11h 11h 32
Clotho 5929 37h 37h 5
ESC50 2000 3h 3h 38
UrbanSound8K 8266 9h 9h 46
Music Instruments 4587 11h 10h 9
GTZAN 2997 8h 8h 51
BBC Sound Effects 31K 997h 232h httpssoundeffectsbbcrewindcouk
FreeSound 206K 6246h 1283h httpsfreesoundorg
AudioSet SL 108K 296h 296h 11
SoundBible 612 4h 3h httpssoundbiblecom
Total 402K 7732h 1990h 
As indicated in Table 4 we collect a largescale audiotext dataset comprising approximately 047M audio samples
amounting to a total duration of about 77K hours This dataset encompasses a diverse range of sounds including musical
instruments sound effects human voices and sounds from nature and everyday life We only utilize the first 30 seconds
of each audio sample for long duration audio and exclude any samples shorter than 1 second Consequently our model is
trained on approximately 04M audio samples collectively amounting to about 2K hours
B Experiment Details
B1 Vocoder
In this work we adopt HiFiGAN vocoder 21 as a converter from V AE decoder output to finally generated audio It is widely
used for speech waveform and audio sample reconstructed from melspectrogram However the default and pretrained HiFi
GAN14use 80 mel filter bands 256 hop size and trained on 22050 sample rate speech resulting in 180882 melspectogram
for 10second audio which does not suit V AE input requirements Therefore we train our own HiFiGAN vocoder using 256
mel filter bands 1024 window length 2048 FFT and 160 hop size resulting in 12561024 melspectrograms Then we
repeat by channel and convert grayscale image to RGB image to match V AE encoder input The 32561024 image is then
8x downsampled by V AE as 432128 latent features sent to LDM We train this vocoder using AdaW optimizer with 2e4
learning rate and 16 batch size on one A6000 GPU We release this pretrained vocoder in our opensource implementation
B2 Configuration
We utilize the pretrained Stable Diffusion v1515 including its V AE and 860Mparameter UNet we freeze the text encoder
and finetune UNet using AdamW optimizer with a 3e5 learning rate and a constant scheduler at a 20 batch size for 100K
steps in both Auffusion and AuffusionFull setups For GPU memory efficiency we use xformer 26 and mixed precision
and our model can be trained only taking a total of 48 hours on one A6000 GPU In comparison AudioGen 24 utilizes 64
A100 GPU with a batch size of 256 AudioLDM 27 use one A100 GPU for 15M train steps and AudioLDM2 28 use 8
A100 GPU for the same steps Tango use 4 A6000 GPU for 200K steps
14httpsgithubcomjik876hifigan
15httpshuggingfacecorunwaymlstablediffusionv15
1B3 Riffusion Reimplementation
Riffusion 7 is originally trained on music datasets for only 5second sound clips using a 44100 sample rate a 441 hop size
and 512 mel filter bands We changed the sample rate to 16000 and the hop size to 160 to match audio generation setup After
converting audio to a melspectrogram Riffusion simply applies minmax normalization to each individual melspectrogram
and quantizes it into an image Therefore it is a nonreversible process that leads to information loss To convert back to
audio Riffusion uses the GriffinLim 37 algorithm which is not sensitive to initial data range and iteratively estimates the
missing phase information However the quality is not comparable to that of deeplearningbased vocoders We use the same
training setup to reimplement Riffusion with same whole datasets
C Effect of Guidance Scale and Inference Steps
The number of inference steps and the classifierfree guidance scale are of crucial importance for sampling from latent dif
fusion models We report the effect of varying these parameters on audio generation using AudioCaps test set in Table 5 On
the left with a fixed guidance scale of 75 we explore inference steps ranging from 10 to 200 Unlike previous studies 9 27
using 200 steps we find that our Auffusion model have competent performance even with fewer inference steps suggesting
strong generative capabilities Therefore we choose using 100 inference steps for Auffusion model On the right we fix the
steps at 100 and adjust the guidance scale We find that guidance 5 has the best FD and FAD score and guidance 10 excels
in IS and CLAP score therefore we choose balanced guidance scale 75 for Auffusion model to generate audio
Table 5 Effect on the objective evaluation metrics with a varying number of inference steps and classifierfree guidance scale
Varying Steps Varying Guidance
Guidance Steps FD  FADKL IS CLAPSteps Guidance FD  FADKL IS CLAP
7510 2115 248 154 789 489
1001 3232 517 247 456 322
25 2252 203 136 988 553 2 2344 286 163 64 471
50 2335 201 135 1009 555 5 2124 172 137 929 549
100 2344 196 136 1021 556 75 2344 196 136 1021 556
200 2334 192 136 1020 556 10 2469 231 136 1050 559
D Evaluation
Despite many objective evaluation method exits they can only assess global performance Subjective method is a much more
direct approach and can be tailored to specific needs We design three scoring tasks to evaluate performance We first ask
human raters to evaluate the overall qaulity OVL from 0 to 100 with 10point intervals Then they need to rate the relevance
REL to the text caption evaluate the global textaudio alignment Finally we ask testers to count the number of events that
appear in the audio to assess the finegrained textaudio alignment Our designed subjective evaluation is shown in Figure 3
Figure 3 Screenshot of subjective evaluation
2E Demos
E1 TexttoAudio Generation
Figure 4 Demo of audio generation with the AuffusionFull model
3E2 Audio Style Transfer Examples
We show the audio style transfer ability of our Auffusion We adopt the similar imagetoimage manipulation method first
introduced in T2I task by using shallow reverse process 34 to audio domain In the figures below we show the original
audio samples on the left and six transferred audios guided by textual descriptions using different starting points of the
reverse process n Asnincreases more noise will be added to the original audio Diffusion model will pay more attention
on text guidance and the generated audio will become less similar to the original one When n 1 the added noise is at its
maximum and information from the original audio will not be retained We gradually increase nand set n 07for the last
generated sample For instance the original sound of a baby crying gradually transitions into the sound of a cat meowing
in Figure 5
Figure 5 Audio style transfer gradually from baby crying tocat meowing 
Figure 6 Audio style transfer gradually from cat screaming tocar racing 
Figure 7 Audio style transfer gradually from bird chirping toambulance siren 
4E3 Audio Inpainting Examples
In the figure below we demonstrate the audio inpainting capability of our Auffusion model Each audio sample is extended to
a duration of 10 seconds In the first row of each example we present the groundtruth samples For the unprocessed samples
we mask a segment from 25 to 75 seconds in the original audio and use these masked samples as input for inpainting The
inpainting results are produced using the same text prompt as the original groundtruth sample We observe that our model
can comprehend the textual description and audio context thereby generating appropriate content for the masked segment
Figure 8 The examples of audio inpainting ability of Auffusion
5E4 Word Swap Examples
Showing the replacement ability of attention map in TTA task In below cases we swap tokens in the original prompt with
others By changing huge tosmall  we observe that the sound effect in the corresponding part changes to a less echoic and
clearer sound By replacing gunshots tospeech  the corresponding sound is replaced with a human voice
Figure 9 Demo of word swapping manipulation
E5 Attention Reweighting Examples
Showing the reweighting ability of attention map in TTA task By increasing the cross attention of specific words marked
with an arrow we control the effect only on specific words without significant change the image We find that increasing
the weight on the verb chopping enhances the frequency of the action sound while amplifying the adjective huge affects the
sound effect
Figure 10 Demo of attention reweighting manipulation
6
  VeCLIP Improving CLIP Training via Visualenriched Captions
Zhengfeng Lai1 Haotian Zhang2 Bowen Zhang2 Wentao Wu2 Haoping Bai2 Aleksei Timofeev2
Xianzhi Du2 Zhe Gan2 Jiulong Shan2 ChenNee Chuah1 Yinfei Yang2 Meng Cao2
1University of California Davis2Apple AIML
lzhengfeng chuah ucdavisedu
haotian zhang2 yinfeiy mengcao applecom
Abstract
Largescale webcrawled datasets are fundamental for
the success of pretraining visionlanguage models such as
CLIP  However the inherent noise and potential irrelevance
of webcrawled AltTexts pose challenges in achieving pre
cise imagetext alignment Existing methods utilizing large
language models LLMs for caption rewriting have shown
promise on small curated datasets like CC3M and CC12M
This study introduces a scalable pipeline for noisy caption
rewriting Unlike recent LLM rewriting techniques we em
phasize the incorporation of visual concepts into captions
termed as Visual enriched Captions VeCap To ensure data
diversity we propose a novel mixed training scheme that op
timizes the utilization of AltTexts alongside newly generated
VeCap We showcase the adaptation of this method for train
ing CLIP on largescale webcrawled datasets termed Ve
CLIP  Employing this costeffective pipeline we effortlessly
scale our dataset up to 300 million samples named VeCap
dataset Our results show significant advantages in image
text alignment and overall model performance For example
VeCLIP achieves up to 252 gain in COCO and Flickr30k
retrieval tasks under the 12M setting For data efficiency
VeCLIP achieves 3 gain while only using 14 of the data
employed in the vanilla CLIP and 11 in ALIGN We also
note the VeCap data is complementary with other well cu
rated datasets good for zeroshot classification tasks When
combining VeCap and DFN our model can achieve strong
performance on both of imagetext retrieval and zeroshot
classification tasks eg831 accuracy1 on ImageNet
zeroshot for a H14 model We release the pretrained mod
els at httpsgithubcomapplemlveclip 
Work done during an internship at AppleEqual contribution
Corresponding author1 Introduction
Largescale visionlanguage representation learning exem
plified by CLIP  32 has gained wide attention due to the
transferability of knowledge learned from imagetext pairs
to diverse downstream tasks such as zeroshot image classi
fication and imagetext retrieval  171920 CLIP training
is straightforward via the imagetext contrastive loss but
involves a largescale dataset of 400 million imagetext pairs
crawled from the Web Consequently CLIP embeddings
lead to consistent improvement across various downstream
tasks compared to other vision pretraining methods such as
SimCLR  6 and MAE  15 CLIP achieves success via two
scalable paradigms data and computational resources First
the massive webcrawled data  3536 enable the training
to be scalable and meet the requirements of datahungry
backbones  eg ViT  11 Second the simple imagetext
contrastive loss grants favorable scaling properties to the
computational resources
Despite the availability of largescale webcrawled data
their quality can be low or noisy For example AltTexts
suffer from two major issues 1 they can be noisy uninfor
mative or irrelevant to the images 2 they may not describe
all visual contents in the image For example as shown
in Figure 1 in the first image we observe a house with a
white roof and a porch However the corresponding caption
only describes the address which proves overly abstract for
effective visionlanguage alignment in training Our obser
vations demonstrate that caption quality plays a pivotal role
in CLIPs performance as detailed in Table 6b and the Ap
pendix  eg CC3M vs our webcrawled 3M It is worth
noting that the captions in CC3M are derived from human an
notations which may require heavy resources when further
scaling up This motivates the main open research question
addressed in this work Can we devise a scalable and cost
effective pipeline to improve captions within these noisy
datasets at scale eg up to million or billion level
One natural direction is to deploy Large Language Mod
els LLMs to rephrase captions  12 However the major
1arXiv231007699v3  csCV  13 Mar 2024Figure 1 Noisy webcrawled data and the limitation of LLM rewrite AltTexts can be noisy and uninformative it may not describe all visual objects
present in the image Simple LLM rewrite  12 on such raw and noisy captions cannot introduce new imagerelevant information After applying our proposed
VeCap new captions are enriched with more imagespecific concepts We keep all imagetext pairs for pretraining rather than filtering out those with noisy
AltTexts as images of rich visual objects still contribute effectively to the training process
limitation of such methods lies in the inability of LLMs to
generate and introduce new imagespecific details LLMs
can only modify sentence syntax in this scenario For exam
ple we follow a recent work  12 and use LLM to rewrite the
raw captions from the Web as shown in Fig 1 LLM rewrite
cannot introduce any new information and thus the new cap
tion remains similar to AltText In other words if the origi
nal AltTexts are noisy the benefits brought by LLM rewrite
might yield only trivial improvements In essence the re
liance on highquality captions within pretraining datasets
limits the effectiveness of simple LLM rewrite However
sourcing such highquality datasets like manually curated
CC3M and CC12M  5 remains challenging and further
scaling up to larger datasets becomes both timeconsuming
and laborintensive to meet the prerequisites for CLIP pre
training Therefore in this work we focus on building a
scalable and costeffective pipeline tailored to raw and noisy
webcrawled data to improve CLIP
In addition to data quality the diversity of data signifi
cantly impacts VLM pretraining  227 Methods relying
on LLMbased rewriting may diminish data variety given
that LLMs tend to apply a uniform style in their sentence
rephrasing Moreover existing works mainly focus on image
augmentations while texts are disregarded and unaltered dur
ing training without augmentation  12 This may also incur
overfitting issues as the text encoders repeatedly encounter
the same texts in each epoch Since these techniques have
exclusively undergone assessment on meticulously curated
datasets like CC3M and CC12M  5 their suitability for
extensive uncensored webcrawled data remains uncertainConsequently there is a pressing need to build a scalable
approach to enhance data quality diversity and training
methodologies to improve pretraining for VLMs on both
model performance and data efficiency
Concurrently alongside the evolution of CLIP there has
been substantial progress in the development of instruction
finetuned LLMs These models and their multimodal ex
tensions have demonstrated outstanding performance sur
passing human capabilities in various natural language and
vision tasks Inspired by these models we investigate the
potential of utilizing them to improve the noisy captions
gathered from the Internet Specifically we initially employ
LLaV A  24 a LanguageVision Assistant to leverage visual
concepts extracted from the images Given that AltTexts may
lack informativeness our objective is to integrate the newly
derived visual concepts into the caption However it is worth
noting that LLaV A  24 finetuned its language decoder on
its own generated dataset potentially losing its ability to ac
commodate comprehensive instructions Consequently we
further propose to utilize an LLM to refine the sentence by
fusing the generated caption from LLaV A and the original
AltText This process aims to maximize imagespecific infor
mation for optimal visionlanguage alignment We denote
the caption generated from LLM as LLM Visual enriched
Captions VeCapap or VeCap for short For data variety
we propose VeCLIP and introduce a mixed training scheme
alternating between VeCap and the original AltText This
strategy ensures that the model captures all pertinent informa
tion without oversight We generalize this scalable pipeline
to curate five pretraining datasets ranging from smallscale
2to largescale up to 300M Overall our contributions are
summarized below
We present a visualenriched recaptioning technique for
CLIP training This marks the initial endeavor to leverage
visual concepts extracted from images and inject them into
the captioning process
Our pipeline is costeffective and capable of processing
data at a scale exceeding 300M named VeCap Then we
propose VeCLIP with a mixed training scheme that uses
VeCap to improve CLIP training on model performance
VeCLIP can achieve up to 252 improvement over CLIP
in retrival tasks For training data efficiency eg we use
only 5data in training but achieve competitive results
in imagetext retrieval tasks
VeCap data is also complementary with other well curated
datasets A CLIPH14 model trained on the combination
of VeCap and DFN achieves strong performance on both of
imagetext retrieval and zeroshot classification tasks with
an impressive 831 zeroshot accuracy1 on ImageNet
2 Related Work
Contrastive languageimage pretraining CLIP  32 has
shown its effectiveness in acquiring transferable image repre
sentations via text supervision after largescale pretraining
Similar models such as ALIGN  17 Florence  44 BA
SIC  30 and OpenCLIP  8 have shown impressive zero
shot image classification and imagetext retrieval capabili
ties SLIP  26 and DeCLIP  21 incorporate selfsupervised
training techniques to improve performance CoCa  43
introduces an additional decoder alongside the contrastive
loss LiT  47 proposes to keep a pretrained image encoder
frozen and finetune text encoders to improve the zeroshot
transferability Nevertheless the majority of these subse
quent studies incorporate supplementary training inputs and
losses potentially exerting adverse effects on both training
efficiency and memory usage
Improving imagetext datasets Given the importance
of the pretraining data many works focus on improving
the datasets such as filtering less informative imagetext
pairs  141225 However these methods may disre
gard a large amount of data even though some images have
rich visual concepts An alternative approach is to rewrite
the caption to enhance the alignment between texts and im
ages For example LaCLIP  12 employs LLMs to perform
rewriting Nevertheless their evaluation was conducted on
smallscale and meticulously curated datasets like CC3M
and CC12M  5 where the initial captions were already of
high quality As shown in Fig 1 the advantage of employing
LLM rewrite on noisy webcrawled data is marginal if the
AltText is noisy3 Methodology
31 Preliminary
CLIP The Contrastive LanguageImage Pretraining CLIP
method has shown its effectiveness in training vision models
via language supervision Specifically a batch of Nimage
text pairs xI xTis sampled from the massive training data
during each training iteration We apply data augmentations
to the images before inputting them into the vision encoder
We denote fIandfTas the normalized features extracted
by the vision and text encoders respectively We use the
contrastive loss to train the model where the paired images
and texts are treated as positive pairs and the remaining as
negative samples The training loss iterating over images
can be formulated as follows
LINX
i1logexp
simfIaugxi
I fTxi
Tτ
PN
k1exp
simfIaugxi
I fTxk
Tτ
1
where xi
I xi
Tis the ithimagetext pair in the batch and
augrefers to image augmentations simis the simi
larity measurement function We set τas a learnable tem
perature parameter that scales the logits in experiments The
loss iterating over texts is symmetrical and denoted as LT
Finally the training loss is L LILT2
32 Recaptioning with Visual Concept Exploitation
Webcrawled captions AltTexts can be noisy and uninfor
mative to the images LaCLIP  12 used LLM to rewrite
the caption As shown in Fig 1 this may not be applicable
if the captions are noisy as LLM can only reconstruct the
sentence but cannot introduce new information without any
information provided by the image Given the inherent noise
in AltTexts we advocate for the utilization of pretrained
multimodal models to generate augmented captions with
richer visual concepts derived from the images In this sub
section we use LLaV A  24 as one example and present a
scalable and costeffective pipeline for scaling up
LLaV A and image captioning for Visualenriched Cap
tions VeCap As a multimodal model LLaV A connects
the openset visual encoder of CLIP  32 with an LLM such
as LLaMA  38 then finetune them on a visual instruction
tuning dataset LLaV A shows its effectiveness in leveraging
the capabilities of pretrained LLM and vision foundation
models Given an input image xI we get fIfrom CLIPs
vision encoder Then LLaV A applies a trainable projection
matrixWto convert fIinto language embedding tokens
to achieve the imagelanguage alignment To mitigate the
influence of AltText we have devised AltTextindependent
prompts tailored for LLaV A ensuring the full exploitation
of visual concepts We refrain from incorporating AltText
information into LLaV A while acknowledging the potential
loss of pretrained knowledge during finetuning of the LLM
3Figure 2 An overview of the scalable VeCap recaptioning piepline First we focus on exploiting visual concepts in images via leveraging a multimodal
LLM LLaV A to describe the image with a designed prompt independent of AltText to generate Visualenriched Captions VeC Second we leverage an
LLM to do ethical check and fuse the concepts from both AltText and VeC to generate the final caption denoted as VeCap
component on the generated dataset This tradeoff however
may limit its capacity to comprehend more intricate instruc
tions Thus we adopt a straightforward yet potent prompt
Describe the image concisely less than 20 words  allowing
LLaV A to generate visual concepts directly from the image
autonomously We denote this captions generated by LLaV A
asxTv Subsequently the imagetext pair is converted as
xI xTv
33 Scalable LLM Rewrite for Concept Fusion
Given the limited language capacity of LLaV A we only
use LLaV A to extract all possible visual clues Then we
employ LLMs to refine the caption by fusing both the knowl
edge from AltText xTand the novel visual concepts from
xTv This step has three main advantages 1 It ensures
the retention of information delineated in AltText thereby
amplifying the informativeness of the caption 2 It can serve
as a form of strong augmentation in textual data character
ized by a profound restructuring of sentence syntax instead
of focusing on wordlevel modifications used in existing lan
guage augmentation techniques  3740 3 It can mitigate
the hallucination issue arising from large visionlanguage
models eg LLaV A to ensure that the entity described in
the ultimate caption is present in the image
Generating rewrites for a vast corpus of texts using closed
source models like ChatGPT or Bard is impractical consider
ing the substantial financial costs and time incurred through
API utilization Therefore to facilitate the rewriting tasks ona largescale dataset we turn to opensource stateoftheart
LLMs Due to the license issue we select Vicuna11  48
renowned for its robust performance in text completion tasks
as one example of LLM rewriting in this study We formulate
a context input as the following three components First we
include a sentence designed to apprise the LLM of the task
specifically rewriting and fusing two attached sentences
This serves as an initial contextual cue to orient the LLM
towards comprehending the overarching objective Second
we impose several constraints on the ultimate output For in
stance our goal is to position attributes prior to noun entities
all while refraining from introducing any novel semantic
interpretations Furthermore it is essential that the sentence
refrains from commencing with the phrase The image and
instead directly expounds upon allencompassed concepts
Finally the last part of the context includes two sentences  xv
andxTv that require fusing and rewriting followed by the
separation symbol This ensures that the LLM is furnished
with the specific texts to be fused and rewritten as part of
its context input By integrating these three components we
establish an allencompassing context that steers the LLM
towards proficiently crafting diverse and knowledgefused
text rewrites
Scalable batchinference process Employing the
crafted context input as a prompt Vicuna showcases its profi
ciency in executing text completion and producing rephrased
renditions of the associated text samples However single
item inference may be timeconsuming and not scalable for
4massive data Therefore we conduct this process in a batch
inference process instead of a singleitem inference as shown
in Fig 2 we group our data into batches and implement a
batchinference process to achieve up to 64 times faster on
Nvidia A100 Specifically we use Vicuna1113B model
to generate the final output as xTl The final prompt is as
follows  Rephrase the following two sentences into one
short sentence while adhering to the provided instructions
Place attributes before noun entities without introducing new
meaning Do not start with The image  1 AltText 2
model generated caption  We denote the caption from LLM
as LLMVeCap or VeCap for short
Potential ethics of LLM and failure cases processing
While upscaling the LLM rewriting process we identify two
scenarios in which LLM encounters difficulties in executing
the designated task 1 Ethical Concerns  If the AltText
contains content either illegal or violent LLM may reply
I am sorry that I cannot 2 Length Constraint  In cases
where the AltText exceeds an optimal length the processing
time of the LLM may be significantly prolonged thus im
peding largescale rewriting To address the first scenario
we use the model generation captions as the only input to
be rewritten via LLM to form VeCap thereby preemptively
excluding potentially unlawful or aggressive content In the
second scenario we mitigate this issue by preserving the
generated catpion but truncating the AltText to conform to
the maximum allowable length thus we have more visual
concepts aligned with the image
34 VeCLIP Mixed Training Scheme with Visual
enriched Captions for CLIP
As LLM rewriting may introduce a consistent style there
could be a decline in data diversity for largescale pre
training even if data quality is enhanced To enhance data
diversity we propose a mixed training scheme to serve as
additional text augmentations applied in pretraining
mixxtUniform xT xTl 2
Then the training loss iterating over the images becomes
LIPN
i1logexp
simfIaugxi
IfTmixxi
tτ
PN
k1exp
simfIaugxi
IfTmixxk
tτ
The only difference with the original CLIP training is that
we alternate the AltTexts with our rephrased sentences with
all other components remaining unaltered This modifica
tion does not incur additional computational complexity or
parameter overheads compared to the standard CLIP train
ing process Through the strategic alternation of AltTexts
and our captions we improve both the quality and diversity
of the pretraining dataset without filtering any data points
This approach empowers the model to glean insights from
both AltText and VeCap This simple yet effective strategy
elevates the training regimen for CLIP offering a scalableframework for optimizing other visionlanguage pretraining
efforts utilizing extensive webcrawled data
4 Experiments
41 Pretraining Datasets and Downstream Tasks
Pretraining datasets and training setup We conduct pre
training experiments on four scales of our datasets named
VeCap to show the efficiency and scalability of our method
Specifically we set 3M as small scale 12M as medium
scale and 100M as large scale We use ViTB16  11
as the vision encoder of CLIP training Our batch size is
8192 for smallmedium scales 3M12M and 32768 for
large scales 100M For efficiency purposes we employ
JAX 3 and train models on 64 TPUs for the 3M12M
settings whereas we utilize 512 TPUs for the 100M200M
pretraining configurations All models are trained with the
AXLearn framework1More details can be found in the
Appendix A To show its generalizability and effectiveness
we also evaluate it on wellcurated CC3MCC12M besides
our crawled noisy WIT data as shown in our ablation studies
and Appendix C2 We evaluate all pretrained models on
the following three tasks
Zeroshot image classification We evaluate all the mod
els on ImageNet  10 ImageNetV2  33 and VTAB  46
We select 9 tasks 6 from natural sets and 3 from special
ized sets that are suitable for zeroshot classification tasks
such as Flowers102  28 and Caltech101  14 as zeroshot
classification tasks We list the details in the Appendix
Zeroshot imagetext retrieval We evaluate the pre
trained models on COCO  23 and Flickr30k  31 cross
modal retrieval tasks ImagetoText denoted as I2T and
TexttoImage T2I retrieval For Flickr30k we evaluate
them on the standard 1K test set We report the results in
terms of Recall kas R1 R5 and R10
Zeroshot imagetoimage retrieval We select
GPR1200  34 for imagetoimage retrieval GPR1200  34
serves as a generalpurpose benchmark for contentbased
image retrieval encompassing subsets drawn from six dif
ferent domains It includes 1200 categories 10 images per
category Following  34 we do not split images as query
and index sets for evaluation Instead we perform retrieval
of the nearest neighbor for each image and utilize the remain
ing images as the index set We report the mean Average
Precision mAP
42 Results on Retrieval Tasks
I2T and T2I retrieval We summarize the main results
in Table 1 We show consistent improvements across
Recall kmetrics in both COCO and Flickr30k datasets
for both I2T and T2I retrieval tasks Specifically for small
and medium scales 3M12M we attain an improvement
1httpsgithubcomappleaxlearn
5Table 1 Results Recall k on zeroshot imagetotext and texttoimage retrieval tasks on COCO and Flickr30k 14BCLIP denotes the
inhouse CLIP pretrained on 14B webcrawled imagetext pairs We use ViTB16 as the vision encoder of CLIP  Denote FLIP uses
ViTL14
Data ModelCOCO Flickr30k
ImagetoText TexttoImage ImagetoText TexttoImage
R1 R5 R10 R1 R5 R10 R1 R5 R10 R1 R5 R10
18B ALIGN 17 5860 8300 8970 4560 6980 7860 8860 9870 9970 7570 9380 9680
400M FLIP 22 6020 8260 8990 4420 6920 7840 8910 9850 9960 7540 9250 9590
400M OpenAI CLIP 5376 7792 8553 3309 5842 6890 8800 9870 9940 6870 9060 9520
14B Inhouse CLIP 6138 8280 8978 4448 6919 7828 8760 9790 9880 7170 9130 9524
3MCLIP 546 1534 2242 328 1044 1596 1220 2780 3750 636 1916 2758
VeCLIP 2230 4500 5616 1301 3161 4242 4060 6730 7670 2758 5244 6320
Performance Gain 1684 2966 3374 973 2117 2646 2840 3950 3920 2122 3328 3562
12MCLIP 2452 4828 5982 1428 3452 4629 4470 7180 8040 2906 5862 7000
VeCLIP 4778 7254 8156 3162 5719 6847 7390 9230 9590 5568 8078 8764
Performance Gain 2326 2426 2174 1734 2267 2218 2920 2050 1550 2662 2216 1764
100MCLIP 4724 7234 8156 3061 5649 6791 7440 9320 9670 5716 8812 8898
VeCLIP 6482 8556 9198 4612 7119 8023 8930 9770 9920 7310 8912 9314
Performance Gain 1758 1322 1042 1551 1470 1232 1490 450 250 1594 100 416
200MCLIP 5220 7622 8504 3497 6042 7108 8090 9490 9760 6326 8658 9226
VeCLIP 6720 8728 9270 4840 7326 8179 9110 9850 9970 7632 9350 9640
Performance Gain 1500 1106 766 1343 1284 1071 1020 360 210 1306 692 414
300MCLIP 5424 7814 8648 3698 6232 7270 8130 9580 9780 6580 8828 9316
VeCLIP 6780 8794 9284 4891 7354 8211 9120 9910 9980 7630 9300 9644
Performance Gain 1356 980 636 1193 1122 941 990 330 200 1050 472 328
of 16842326 in Recall1 for COCO imageto
text retrieval respectively Notably the strides made in
Flickr30k are particularly noteworthy with a remarkable
28402920 improvement in Recall1 Subsequently
we scale our approach to 100M and 200M where we ob
serve sustained and substantial improvements Notably we
achieve a noteworthy 17581500 enhancement in
COCO imagetotext retrieval performance using 100M and
200M respectively Furthermore we observe a diminishing
improvement margin as we scale up the dataset Initially we
achieve a substantial 2840 improvement in imagetotext
retrieval for Flickr30k with the 3M dataset which subse
quently decreases to 1020 when employing the 200M
dataset These findings show the advantages of our proposed
pipeline for enhancing CLIP pretraining By demonstrating
its scalability from 3M to 300M we provide compelling evi
dence of its applicability in realworld scenarios particularly
for training CLIP from scratch using WIT datasets
Imagetoimage retrieval We use GPR1200  34 with
6 domains for this setting Google Landmarks V2 natural
and architectural landmarks denoted as Land IMDB Faces
denoted as Faces iNat plants animals insects and fungi
INSTRE planar images and photographs of logostoys de
noted as INST ImageNet Sketch denoted as Sketch and SOP
products and objects partly isolated The results mAP
are summarized in Table 2 We attain a performance gain of
522392 under smallmedium scales 3M12M Evenupon upscaling the dataset to 200M we observe a notable
184 increase in average score across six domains Notably
our primary performance boost is derived from the Sketch do
main underlining the crucial role of visual concepts in zero
shot transferability Consequently our visuallyenriched
captions play a pivotal role in learning such transferability
towards downstream tasks
Data efficiency for pretraining To show the data ef
ficiency of VeCLIP we include ALIGN  17 pretrained
on 18B data denoted as 18BALIGN and our inhouse
CLIP  32 model trained on 14B data denoted as 14B
CLIP as baselines trained at a significantly larger scale We
use these models utilizing over tenfold more data compared
to our setting to show the data efficiency of VeCLIP training
VeCLIP can outperform 14BCLIP model when scaling up
to 100M representing approximately 7 of its size across
nearly all downstream tasks Specifically in COCO we
achieve 344164 gain in Recall1 for both retrieval
tasks Upon further scaling to 200M the improvement be
comes even more pronounced reaching 582392
Furthermore we achieve a notable 860280 gain in
COCO retrieval as well as a 250062 improvement
in Flickr30k when compared to the 18BALIGN model
Remarkably these improvements are achieved with only
111 of the data utilized in the pretraining process These
results show the data efficacy of VeCLIP When we scale
it to 300M the results are similar to 200M The results on
6Table 2 Imagetoimage retrieval results mAP on 6domain GPR1200  34
Data ModelDomain Name
Land Faces iNat INST Sketch SOP All
3MCLIP 5798 2076 1761 3114 1823 7429 3667
VeCLIP 6655 2351 2043 3863 2459 7765 4189
12MCLIP 7447 3065 2360 5215 3068 8425 4930
VeCLIP 7930 3172 2553 5665 4142 8469 5322
100MCLIP 8564 5168 2966 6819 4245 9038 6133
VeCLIP 8559 4283 3072 7196 5259 9054 6237
200MCLIP 8696 5654 3095 7151 4603 9095 6383
VeCLIP 8640 4848 3172 7374 5652 9116 6567
Table 3 Zeroshot classification results Top kAccuracy on ImageNet and
ImageNetV2 33
Data ModelImageNet ImageNetV2
Top1 Top5 Top10 Top1 Top5 Top10
3MCLIP 546 2105 2870 709 1852 2583
VeCLIP 1598 3411 4323 1351 3003 3893
12MCLIP 3160 5880 6949 2703 5268 6337
VeCLIP 3811 6674 7636 3253 6016 7050
100MCLIP 5864 8582 9179 5096 7977 8691
VeCLIP 6077 8777 9316 5417 8251 8924
200MCLIP 6372 8926 9411 5684 8350 8979
VeCLIP 6462 9027 9490 5767 8524 9162
Figure 3 Performance gain on downstream tasks across
different data scales
Table 4 Zeroshot classification accuracy Top1 accuracies   of VTAB  46 across 9 tasks 6 from natural and 3 from specialized sets
are reported Full table can be found in Appendix Table A7
Data ModelNatural Sets Specialized SetsAverageCaltech101 CIFAR100 SVHN DTD OxPet Flowers102 EuroSAT RESISC45 Camelyon
3MCLIP 3950 983 2089 742 744 1040 1194 793 5065 1845
VeCLIP 5430 1774 1874 1123 1009 2275 735 1654 5252 2348
12MCLIP 7043 3006 3011 3069 3451 3367 887 3005 5346 3576
VeCLIP 7058 4510 2361 3090 3622 4394 2746 3809 5554 4127
100MCLIP 8144 5475 3870 5728 7051 5171 3445 4856 5387 5459
VeCLIP 8164 6462 4649 5751 6481 6641 4623 5175 5851 5978
200MCLIP 8230 6187 4283 6429 7560 5867 4673 5559 5930 6079
VeCLIP 8314 6814 4493 6195 7261 6851 4736 5510 6259 6270
300M can be found in Appendix Therefore we stop further
scaling up the dataset
43 Results on Image Classification
ImageNet We use the same prompt as CLIP A photo
of a classname for zeroshot evaluation on both Im
ageNet  10 and ImageNetV2  33 The main results are
summarized in Table 3 We report Top1 Top5 and Top
10 accuracies In small and mediumscale settings we ob
serve a substantial improvement 1052642 gains
in Top1 accuracy on ImageNetImageNetV2 under the 3M
setting and 651550 gains under the 12M setting
While the improvement becomes marginal upon scaling to100M200M we still achieve noteworthy 207321
and 090083 gains on 100M and 200M across Ima
geNet and ImageNetV2 respectively This shows the data
efficiency of our pretraining approach
Visual Task Adaptation Benchmark VTAB Besides
ImageNetImageNetV2 we also select VTAB  46 for eval
uation Table 4 summarizes zeroshot image classification
results for both the original CLIP models and our models
utilizing the identical prompt set from CLIP Our approach
consistently achieves comparable or superior performance
to CLIP across the majority of datasets For instance we
observe an average accuracy gain of over 5 under settings
of 3M 12M and 100M Even upon scaling up to 200M
7we maintain a notable gain of 191 These results show
great robustness on zeroshot classification tasks across dif
ferent data distributions We show the overall trend of the
performance gain over the data scale in Figure 3
44 Performance trend across scales
Besides the performance gain we also visualize the perfor
mance trend across data scales in pretraining As shown
in Figure 4 the performance of CLIP utilizing original Alt
Texts exhibits a marked surge with the increased data size
while its starting point is poor at 3M it demonstrates swift
progression up to 12M and 100M However once scaled
beyond 100 million the performance trend exhibits a grad
ual and eventually saturated growth On the other hand
commencing with a higher baseline VeCLIP employing Ve
Cap demonstrates substantial improvement in comparison to
CLIP within small to medium scales 3M and 12M As we
progress beyond 300M the performance gains of VeCLIP
become relatively incremental but still noticeable in retrieval
tasks Both CLIP and VeCLIP reach a saturation point when
scaled up to 100M once over 100M the performance gain
becomes gradual and marginal
45 Complementary to other datasets to achieve
stateoftheart performance
Our VeCap datasets with visualenriched captions can also be
complementary to other wellcurated dataset For example
DFN  13 has shown benefits on CLIP To demonstrate that
we train CLIP models with VeCap and DFN separately and
also a combination with them All the models are trained
under same configuration for learning rate maximum steps
and so on
We summarize the results in Table 5 The highquality
descriptive captions from VeCap can achieve superior results
compared to DFN in retrieval tasks However the perfor
mance on classification tasks are inferior After we combine
DFN and VeCap for training CLIP can achieve the most
improvements for all model sizes
We also train a H14 model with resolution 336x336
and compare it with the stateoftheart models like Meta
CLIP  42 and DFN  13 The results are summarized in
row 6 to 8 of table 5 Albeit trained on different resolutions
and recipes the CLIP model with VeCapDFN is compat
ible with other models and provide yet another option for
downstream tasks2
Our VeCLIP with DFN  13 can outperform FLIP  22
and OpenAI CLIP with different backbones as shown in
Table A10 in Appendix Specifically our ViTH14 model
achieves impressive 831 of accuracy on ImageNet We
leave the further study of combing the synthetic data VeCap
with other data curation approaches as a future work
2Note we took the DFNH14 model from its original paper which is
trained 7 epochs our model is only trained roughly around 2 epochs46 Ablation Study
Importance of visualenriched concepts Different from
previous rewriting methods our primary emphasis lies in fus
ing visualenriched concepts extracted from images The ab
lation findings are summarized in Table 6a We use 3M12M
as examples to show the performance gain in smallmedium
scales Original AltTexts shows its limitation in retrieval
tasks due to its noise and limited imagespecific information
VeC generated from LLaV A can boost the performance on
retrieval tasks but may hurt the performance on ImageNet
zeroshot task Introducing VeCap can further improve Ve
Cap in all settings Intriguingly the zeroshot ImageNet
results still lag behind the original AltText In essence our
VeCap exerts a profound influence on retrieval prowess yet
exerts a negative effect on classification tasks We posit
that this phenomenon arises from the following two rea
sons 1 there can be a distributional shift in prompts from
pretraining to zeroshot inference in ImageNet particularly
noteworthy given the extended length and augmented visual
content of VeCap 2 the data diversity is hurt by LLM rewrit
ing as LLM uses the same writingparaphrasing style to fuse
VeCap and AltText
Importance of mixed training strategies To mitigate
the aforementioned issues we propose a mixed training
scheme to alternate between AltTexts and VeCap to provide
more data variety during pretraining We summarize the
ablation results of VeCLIP in Table 6b First we observe a
slight performance improvement by randomly selecting one
AltText in cases where multiple AltTexts are associated with
an image This practice augments data diversity during pre
training Second interchanging between AltText and VeCap
proves to be advantageous not only in retaining substantial
performance gains in retrieval tasks but also in markedly
elevating zeroshot results on ImageNet Lastly leveraging
all AltTexts and VeCap within the mixed training approach in
VeCLIP achieves superior results across nearly all settings
Larger backbone architecture We also investigate a
larger backbone architecture eg ViTL14 and ViTH14
The detailed results can be found in both Table 5 and Ap
pendix C1 VeCLIP scaled up in backbone size can consis
tently outperform the original CLIP in all downstream tasks
Besides a larger backbone ViTL14 can also achieve up
to 587 improvement compared to ViTB16 These find
ings support the effectiveness of VeCLIP in improving CLIP
pretraining regardless of the specific underlying backbone
architecture
Generalizability of VeCap on wellcurated datasets
Besides our WIT datasets we evaluate VeCap on well
curated CC3MCC12M Table 6b shows CLIP achieves bet
ter performance when pretrained on CC3M compared to
pretrained on WIT3M indicating the importance of high
quality captions for pretraining With VeCap to further
improve the quality of CC3Ms captions CLIP can achieve
8a CLIP b VeCLIP
c CLIP d VeCLIP
Figure 4 Performance trend with ViTB16 as the vision backbone a and c show the trend of CLIP with original AltTexts while b and
d show the trend of VeCLIP with LLMVeC The performance is improved significantly when we scale pretraining data up to 100M Once
over 100M the performance gain becomes gradual and incremental
Table 5 CLIP training with VeCap and DFN 13 and its comparison with the stateoftheart models
Model Resolution DataCOCO R1 Flickr30k R1ImageNet
I2T T2I I2T T2I
B16 224DFN 13 630 432 871 704 762
VeCapDFN 663 451 888 736 762
Comparison to other stateoftheart models
DFN 13 685 485 892 751 810
L14 224 FLIP 22 602 442 891 754 746
VeCapDFN 708 495 924 784 811
224 MetaCLIP 42 672 495 921 785 805
H14 378 DFN 13 718 556 940 821 844
336 VeCapDFN 728 523 936 826 831
9Data CaptionPrompt
ConstraintCOCO R1 Flickr30k R1ImageNet ImageNetV2I2T T2I I2T T2I
WIT3MAltText  518 340 1050 688 802 688
VeC  1676 957 3260 2006 731 658
VeCap  1734 952 3730 2162 812 683
VeCap  1810 951 4000 2194 820 739
WIT12MAltText  2258 1423 4440 3090 3114 2591
VeC  4006 2459 6410 4346 729 1474
VeCap  4452 2746 7090 5046 2105 1811
VeCap  4682 2661 7260 5094 2099 1841
a Importance of visualenriched concepts for data quality We use the AltText with Highest CLIP Score HCS if multiple
AltTexts exist on the same image in all settings
Data AltText VeCapTraining
SamplingCOCO R1 Flickr30k R1ImageNet ImageNetV2I2T T2I I2T T2I
WIT3M  HCS 518 340 1050 688 802 688
  random 546 328 1220 636 826 709
  HCS 1810 951 4000 2194 820 739
  HCSmixed 1970 1214 3930 2560 1483 1236
  randommixed 2230 1301 4060 2758 1598 1351
WIT12M  HCS 2258 1423 4440 3090 3114 2591
  random 2332 1428 4470 2906 3160 2703
  HCS 4682 2661 7260 5094 2099 1841
  HCSmixed 4600 3110 7250 5682 3745 3241
  randommixed 4778 3162 7390 5568 3811 3251
CC3M   1388 964 2630 1804 1459 1252
  randommixed 3204 2207 5720 3654 2073 1790
b Importance of the mixed training scheme for data variety HCS refers to using the AltText with Highest CLIP Score while random refers to
randomly selecting one if multiple AltTexts exist
Table 6 Ablation study of VeCLIP The highest score is bold and the second is underlined mixed is our proposed mixed training scheme
to alternate among captions
significant improvement since the captions of CC3M are
of higher quality than our noisy WIT dataset CC3M with
its original captions can outperform the performance of our
WIT3M with AltTexts indicating CC3M is of higher quality
VeCap can significantly improve CLIP under CC3M settings
eg 1816 on the I2T task of COCO and 614 on Ima
geNet showing its generalizability on wellcurated datasets
More results are in Appendix C2
5 Discussion
Conclusion We present a simple yet effective approach
to improve CLIP pretraining with leveraging LLaV A and
LLMs to rewrite the captions with more visualenriched
concepts VeCLIP is intentionally designed to be scalable
and adaptable for handling extensive imagetext datasets
obtained from web crawling We conduct a thorough evalua
tion of VeCLIP on a diverse range of raw and noisy datasets
spanning small medium and large scales The results re
veal a substantial performance boost providing compelling
evidence for the effectiveness of our strategy in enhancing
largescale VLM pretraining VeCLIP can significantly re
duce the computational cost and the size of training data forlarge models to reach competitive results as vanilla CLIP
Future work We employ CLIP as an illustrative in
stance to highlight the importance of aligning text and im
ages within the training dataset For future work we plan
to use the collected largescale dataset to improve the pre
training of other types of VLMs Further LLM can generate
outputs that encompass factual inaccuracies and hallucina
tions Thus we also plan to delve into more sophisticated
filtering techniques to remove such descriptions
Limitation We only leverage LLaV A to exploit the
visual concepts However the quality measurement metric
for such generative AI is still under study
References
1Amro Abbas Kushal Tirumala D aniel Simig Surya Ganguli
and Ari S Morcos Semdedup Dataefficient learning at
webscale through semantic deduplication arXiv preprint
arXiv230309540  2023 3
2James Betker Gabriel Goh Li Jing Tim Brooks Jianfeng
Wang Linjie Li Long Ouyang Juntang Zhuang Joyce Lee
Yufei Guo Wesam Manassra Prafulla Dhariwal Casey Chu
Yunxin Jiao and Aditya Ramesh Improving image generation
with better captions OpenAI  2023 2
103James Bradbury Roy Frostig Peter Hawkins Matthew James
Johnson Chris Leary Dougal Maclaurin George Necula
Adam Paszke Jake VanderPlas Skye WandermanMilne
and Qiao Zhang JAX composable transformations of
PythonNumPy programs Github  2018 5 1
4Liangliang Cao Bowen Zhang Chen Chen Yinfei Yang
Xianzhi Du Wencong Zhang Zhiyun Lu and Yantao Zheng
Less is more Removing textregions improves clip training
efficiency and robustness arXiv preprint arXiv230505095 
2023 3
5Soravit Changpinyo Piyush Sharma Nan Ding and Radu
Soricut Conceptual 12m Pushing webscale imagetext pre
training to recognize longtail visual concepts In Proceedings
of the IEEECVF Conference on Computer Vision and Pattern
Recognition  pages 35583568 2021 2 3 1
6Ting Chen Simon Kornblith Mohammad Norouzi and Ge
offrey Hinton A simple framework for contrastive learning
of visual representations In International conference on
machine learning  pages 15971607 PMLR 2020 1
7Gong Cheng Junwei Han and Xiaoqiang Lu Remote sensing
image scene classification Benchmark and state of the art
Proceedings of the IEEE  1051018651883 2017 2
8Mehdi Cherti Romain Beaumont Ross Wightman Mitchell
Wortsman Gabriel Ilharco Cade Gordon Christoph Schuh
mann Ludwig Schmidt and Jenia Jitsev Reproducible scal
ing laws for contrastive languageimage learning In Proceed
ings of the IEEECVF Conference on Computer Vision and
Pattern Recognition  pages 28182829 2023 3
9M Cimpoi S Maji I Kokkinos S Mohamed  and A
Vedaldi Describing textures in the wild In Proceedings of
the IEEE Conf on Computer Vision and Pattern Recognition
CVPR  2014 2
10 Jia Deng Wei Dong Richard Socher LiJia Li Kai Li and Li
FeiFei Imagenet A largescale hierarchical image database
In2009 IEEE conference on computer vision and pattern
recognition  pages 248255 Ieee 2009 5 7 1
11 Alexey Dosovitskiy Lucas Beyer Alexander Kolesnikov
Dirk Weissenborn Xiaohua Zhai Thomas Unterthiner
Mostafa Dehghani Matthias Minderer Georg Heigold Syl
vain Gelly et al An image is worth 16x16 words Transform
ers for image recognition at scale In International Conference
on Learning Representations  2020 1 5
12 Lijie Fan Dilip Krishnan Phillip Isola Dina Katabi and Yon
glong Tian Improving clip training with language rewrites
arXiv preprint arXiv230520088  2023 1 2 3
13 Alex Fang Albin Madappally Jose Amit Jain Ludwig
Schmidt Alexander T Toshev and Vaishaal Shankar Data fil
tering networks In NeurIPS 2023 Workshop on Distribution
Shifts New Frontiers with Foundation Models  2023 8 9 2
5
14 Li FeiFei Rob Fergus and Pietro Perona Learning gener
ative visual models from few training examples An incre
mental bayesian approach tested on 101 object categories In
2004 conference on computer vision and pattern recognition
workshop  pages 178178 IEEE 2004 5 2
15 Kaiming He Xinlei Chen Saining Xie Yanghao Li Piotr
Dollar and Ross Girshick Masked autoencoders are scalablevision learners In Proceedings of the IEEECVF conference
on computer vision and pattern recognition  pages 16000
16009 2022 1
16 Patrick Helber Benjamin Bischke Andreas Dengel and
Damian Borth Introducing eurosat A novel dataset and
deep learning benchmark for land use and land cover classifi
cation In IGARSS 20182018 IEEE International Geoscience
and Remote Sensing Symposium  pages 204207 IEEE 2018
2
17 Chao Jia Yinfei Yang Ye Xia YiTing Chen Zarana Parekh
Hieu Pham Quoc Le YunHsuan Sung Zhen Li and Tom
Duerig Scaling up visual and visionlanguage representation
learning with noisy text supervision In International confer
ence on machine learning  pages 49044916 PMLR 2021
1 3 6
18 Alex Krizhevsky Learning multiple layers of features from
tiny images Canadian Institute for Advanced Research  2009
2
19 Gukyeong Kwon Zhaowei Cai Avinash Ravichandran Er
han Bas Rahul Bhotika and Stefano Soatto Masked vision
and language modeling for multimodal representation learn
ing In The Eleventh International Conference on Learning
Representations  2023 1
20 Junnan Li Dongxu Li Caiming Xiong and Steven Hoi Blip
Bootstrapping languageimage pretraining for unified vision
language understanding and generation In International Con
ference on Machine Learning  pages 1288812900 PMLR
2022 1
21 Yanghao Li Haoqi Fan Ronghang Hu Christoph Feichten
hofer and Kaiming He Scaling languageimage pretraining
via masking In Proceedings of the IEEECVF Conference
on Computer Vision and Pattern Recognition  pages 23390
23400 2023 3
22 Yanghao Li Haoqi Fan Ronghang Hu Christoph Feichten
hofer and Kaiming He Scaling languageimage pretraining
via masking In Proceedings of the IEEECVF Conference
on Computer Vision and Pattern Recognition  pages 23390
23400 2023 6 8 9 2 5
23 TsungYi Lin Michael Maire Serge Belongie James Hays
Pietro Perona Deva Ramanan Piotr Doll ar and C Lawrence
Zitnick Microsoft coco Common objects in context In
Computer VisionECCV 2014 13th European Conference
Zurich Switzerland September 612 2014 Proceedings Part
V 13 pages 740755 Springer 2014 5
24 Haotian Liu Chunyuan Li Qingyang Wu and Yong Jae Lee
Visual instruction tuning arXiv preprint arXiv230408485 
2023 2 3
25 Pratyush Maini Sachin Goyal Zachary C Lipton J Zico
Kolter and Aditi Raghunathan Tmars Improving visual
representations by circumventing text feature learning arXiv
preprint arXiv230703132  2023 3
26 Norman Mu Alexander Kirillov David Wagner and Sain
ing Xie Slip Selfsupervision meets languageimage pre
training In European Conference on Computer Vision  pages
529544 Springer 2022 3
27 Thao Nguyen Samir Yitzhak Gadre Gabriel Ilharco Se
woong Oh and Ludwig Schmidt Improving multi
11modal datasets with image captioning arXiv preprint
arXiv230710350  2023 2
28 MariaElena Nilsback and Andrew Zisserman Automated
flower classification over a large number of classes In 2008
Sixth Indian conference on computer vision graphics  image
processing  pages 722729 IEEE 2008 5 2
29 Omkar M Parkhi Andrea Vedaldi Andrew Zisserman and
CV Jawahar Cats and dogs In 2012 IEEE conference on
computer vision and pattern recognition  pages 34983505
IEEE 2012 2
30 Hieu Pham Zihang Dai Golnaz Ghiasi Hanxiao Liu
Adams Wei Yu MinhThang Luong Mingxing Tan and
Quoc V Le Combined scaling for zeroshot transfer learning
arXiv preprint arXiv211110050  2021 3
31 Bryan A Plummer Liwei Wang Chris M Cervantes
Juan C Caicedo Julia Hockenmaier and Svetlana Lazebnik
Flickr30k entities Collecting regiontophrase correspon
dences for richer imagetosentence models In Proceedings
of the IEEE international conference on computer vision 
pages 26412649 2015 5
32 Alec Radford Jong Wook Kim Chris Hallacy Aditya
Ramesh Gabriel Goh Sandhini Agarwal Girish Sastry
Amanda Askell Pamela Mishkin Jack Clark et al Learning
transferable visual models from natural language supervision
InICML  pages 87488763 2021 1 3 6
33 Benjamin Recht Rebecca Roelofs Ludwig Schmidt and
Vaishaal Shankar Do imagenet classifiers generalize to im
agenet In International conference on machine learning 
pages 53895400 PMLR 2019 5 7
34 Konstantin Schall Kai Uwe Barthel Nico Hezel and Klaus
Jung Gpr1200 a benchmark for generalpurpose content
based image retrieval In International Conference on Multi
media Modeling  pages 205216 Springer 2022 5 6 7
35 Christoph Schuhmann Richard Vencu Romain Beaumont
Robert Kaczmarczyk Clayton Mullis Aarush Katta Theo
Coombes Jenia Jitsev and Aran Komatsuzaki Laion400m
Open dataset of clipfiltered 400 million imagetext pairs
arXiv preprint arXiv211102114  2021 1
36 Christoph Schuhmann Romain Beaumont Richard Vencu
Cade Gordon Ross Wightman Mehdi Cherti Theo Coombes
Aarush Katta Clayton Mullis Mitchell Wortsman et al
Laion5b An open largescale dataset for training next gen
eration imagetext models Advances in Neural Information
Processing Systems  352527825294 2022 1
37 Rico Sennrich Barry Haddow and Alexandra Birch Improv
ing neural machine translation models with monolingual data
arXiv preprint arXiv151106709  2015 4
38 Hugo Touvron Thibaut Lavril Gautier Izacard Xavier Mar
tinet MarieAnne Lachaux Timoth ee Lacroix Baptiste
Rozi ere Naman Goyal Eric Hambro Faisal Azhar et al
Llama Open and efficient foundation language models arXiv
preprint arXiv230213971  2023 3
39 Bastiaan S Veeling Jasper Linmans Jim Winkens Taco Co
hen and Max Welling Rotation equivariant cnns for digital
pathology In Medical Image Computing and Computer As
sisted InterventionMICCAI 2018 21st International Confer
ence Granada Spain September 1620 2018 Proceedings
Part II 11  pages 210218 Springer 2018 240 Jason Wei and Kai Zou Eda Easy data augmentation tech
niques for boosting performance on text classification tasks
arXiv preprint arXiv190111196  2019 4
41 Wentao Wu Aleksei Timofeev Chen Chen Bowen Zhang
Kun Duan Shuangning Liu Yantao Zheng Jon Shlens Xi
anzhi Du Zhe Gan et al Mofi Learning image represen
tations from noisy entity annotated images arXiv preprint
arXiv230607952  2023 1
42 Hu Xu Saining Xie Xiaoqing Ellen Tan PoYao Huang Rus
sell Howes Vasu Sharma ShangWen Li Gargi Ghosh Luke
Zettlemoyer and Christoph Feichtenhofer Demystifying clip
data arXiv preprint arXiv230916671  2023 8 9
43 Jiahui Yu Zirui Wang Vijay Vasudevan Legg Yeung Mo
jtaba Seyedhosseini and Yonghui Wu Coca Contrastive
captioners are imagetext foundation models arXiv preprint
arXiv220501917  2022 3
44 Lu Yuan Dongdong Chen YiLing Chen Noel Codella
Xiyang Dai Jianfeng Gao Houdong Hu Xuedong Huang
Boxin Li Chunyuan Li et al Florence A new foundation
model for computer vision arXiv preprint arXiv211111432 
2021 3
45 Netzer Yuval Reading digits in natural images with unsuper
vised feature learning In Proceedings of the NIPS Workshop
on Deep Learning and Unsupervised Feature Learning  2011
2
46 Xiaohua Zhai Joan Puigcerver Alexander Kolesnikov Pierre
Ruyssen Carlos Riquelme Mario Lucic Josip Djolonga An
dre Susano Pinto Maxim Neumann Alexey Dosovitskiy et al
A largescale study of representation learning with the visual
task adaptation benchmark arXiv preprint arXiv191004867 
2019 5 7 1 4
47 Xiaohua Zhai Xiao Wang Basil Mustafa Andreas Steiner
Daniel Keysers Alexander Kolesnikov and Lucas Beyer Lit
Zeroshot transfer with lockedimage text tuning In Proceed
ings of the IEEECVF Conference on Computer Vision and
Pattern Recognition  pages 1812318133 2022 3
48 Lianmin Zheng WeiLin Chiang Ying Sheng Siyuan
Zhuang Zhanghao Wu Yonghao Zhuang Zi Lin Zhuohan Li
Dacheng Li Eric P Xing Hao Zhang Joseph E Gonzalez
and Ion Stoica Judging llmasajudge with mtbench and
chatbot arena 2023 4
121 Appendix
We provide additional details for datasets experimental set
tings results and analysis in the supplementary material
A Dataset details
Pretraining datasets Instead of using wellcurated
datasets we use imageAltText pairs sampled from a web
crawled dataset  41 We collect 300M imagetext pairs
from the Web and denote it as WIT300M Based on WIT
300M we build four subsets to cover from small to large
scales Specifically WIT200M is a subset of WIT300M
WIT100M is a subset of WIT200M WIT12M is a subset
of WIT100M WIT3M is a subset of WIT12M
VTAB datasets We choose 9 classification datasets
suitable for zeroshot evaluation from VTAB  46 Table A1
summarizes zeroshot image classification datasets For both
original CLIP models and our models we use the identi
cal prompt set from CLIP Every class label is expanded
using a collection of prompt templates as defined by CLIP
including examples like A photo of a classname The
class embedding is then computed by taking the average
of the embeddings of all such templates followed by L2
normalization
B Implementation details
Pretraining hyperparameters We summarize the pre
training hyperparameters for CLIP training in Table A2 We
pretrain models on up to 512 TPUs with JAX 3
C More experimental results
In this section we present more detailed experimental results
and our ablation studies eg generalization of VeCLIP with
a large backbone public and wellcurated datasets for pre
training
C1 Larger backbone architectures
We also investigate the performance of VeCLIP using a larger
backbone architecture ViTL14 The comparison results
are summarized in Table A3 First VeCLIP shows a consis
tent improvement over CLIP employing ViTL14 across all
downstream tasks Second VeCLIP utilizing ViTL14 sur
passes its counterpart employing ViTB16 notably excelling
in image classification tasks achieving a notable improve
ment of over 5 on both ImageNet and ImageNetV2 This
shows that VeCLIP has the potential to be scalable with
larger backbone architectures and largerscale datasets
C2 Generalization on wellcurated datasets
CC3M and CC12M
Besides our crawled noisy WIT datasets we also use a well
curated dataset eg CC3M and CC12M  5 to show the effectiveness and generalizability of our proposed approach on
wellcurated datasets CC3M and CC12M  5 were curated
via several rounds of comprehensive refining and filtering to
get highquality imagecaption pairs We show highquality
examples of CC3M and the comparison of CC3Ms captions
and WIT3Ms AltTexts in Appendix D We present an ex
perimental comparison between our crawled WIT datasets
and wellcurated CC3MCC12M 5 in this subsection
3M As shown in Table A4 CC3M outperforms WIT3M
when coupled with CLIP pretraining yielding a notable
increase of 1070 on the COCO I2T task Additionally
VeCLIP exhibits substantial improvement for both WIT3M
and CC3M Notably we achieve a remarkable over 30
improvement on the I2T task in Flickr30K and an impressive
over 5 boost on ImageNet and ImageNetV2
12M Similar to 3M settings CC12M exhibits superior
quality and attains better results in contrast to WIT12M
when utilized with CLIP and original AltTexts VeCLIP
demonstrates notable improvements for both WIT12M and
CC12M For instance VeCLIP yields a remarkable 1227
increase in the I2T task of COCO along with an impressive
over 5 improvement on both ImageNet and ImageNetV2
These findings emphasize the effectiveness and generaliz
ability of VeCLIP in both noisy webcrawled datasets and
meticulously curated datasets where a richer set of visual
concepts is harnessed for pretraining
C3 Complete visual descriptions vs simplified en
tity representations
In Table 6b of the main paper we note that sole training on
VeCap might detriment zeroshot performance in compari
son to the original AltText Conversely our mixed training
approach yields optimal outcomes This intriguing finding
propels us toward a more profound investigation of zeroshot
classification tasks Following established works  1232
we employ an identical set of prompting templates such as
a photo of a CLS for ImageNet  10 It is conceivable
that this direct and uncomplicated prompt may diverge sig
nificantly from VeCaps pretraining which encompasses a
more extensive and intricate set of visual concepts To ad
dress this we reformulate VeCap into a format as Simplified
Entity Representation SER Specifically we employ the
NLTK package to extract entities from VeCap and subse
quently apply filtering to retain only noun entities denoted
asA B C U This transformation results in VeCap
being presented as a photo of  U offering a concise repre
sentation of all extracted entities The results are summarized
in Table A5 Surprisingly we find that even with SERstyle
captions the zeroshot performance remains inferior to that
achieved with the original AltText We hypothesize that this
discrepancy may arise from a lack of data diversity When
all sentences adhere to the same distribution there exists
a risk of overfitting in the pretrained model resulting in
1Table A1 Details of 9 VTAB zeroshot classification datasets
Dataset Metric Categories Train Size Test Size
CIFAR100 18 Accuracy 100 50000 10000
SVHN 45 Accuracy 10 73257 26032
DTD 9 Accuracy 47 3760 1880
Oxford Pets 29 Mean per class 37 3680 3669
Caltech101 14 Mean per class 102 3060 6085
Flowers102 28 Mean per class 102 2040 6149
EuroSAT 16 Accuracy 10 10000 5000
RESISC45 7 Accuracy 45 25200 6300
Camelyon 39 Accuracy 2 262144 32768
Table A2 Details of the pretraining hyperparameters for CLIP training on our webcrawled datasets
aPretraining hyperparameters on 3M
Config Value
Batch size 8192
Optimizer AdamW
Learning rate 5104
Weight decay 05
Adam β β1 β2 09098
Adam ϵ 1108
Total epochs 40
Warm up epochs 1
Learning rate schedule cosine decaybPretraining hyperparameters on 12M
Config Value
Batch size 8192
Optimizer AdamW
Learning rate 5104
Weight decay 05
Adam β β1 β2 09098
Adam ϵ 1108
Total epochs 35
Warm up epochs 1
Learning rate schedule cosine decay
cPretraining hyperparameters on 100M
Config Value
Batch size 32768
Optimizer AdamW
Learning rate 5104
Weight decay 02
Adam β β1 β2 09098
Adam ϵ 1106
Total epochs 32
Warm up iterations 2000
Learning rate schedule cosine decaydPretraining hyperparameters on 200M
Config Value
Batch size 32768
Optimizer AdamW
Learning rate 5104
Weight decay 02
Adam β β1 β2 09098
Adam ϵ 1106
Total epochs 32
Warm up iterations 2000
Learning rate schedule cosine decay
suboptimal performance in downstream tasks
C4 Main results with WIT300M
We show the detailed results with the Webcrawled Image
Text 300M dataset WIT300M here We summarize the
results on various downstream tasks in Table A8 There are
two major observations First we observe that the results ob
tained with a dataset size of 300M are close to those achieved
with 200M for both CLIP and VeCLIP models This suggests
that a dataset scale of 200 million is sufficient for effectively
training a ViTB16based CLIP model Second VeCLIP
achieves significant improvement on retrieval tasks even un
der 300M settings Nevertheless the improvement observedin ImageNetImageNetV2 is marginal
As shown in Table A9 our VeCLIP with DFN  13 can
outperform FLIP  22 and OpenAI CLIP with different back
bones Specifically our ViTH14 model achieves impressive
831 of accuracy on ImageNet We leave the further study
of combing the synthetic data VeCap with other data cura
tion approaches as a future work
D Caption quality comparison between well
curated Datasets and WIT datasets
In Appendix C2 we find CLIP performs notably better
when pretrained on CC3M compared to the case of being
pretrained on noisy crawled WIT datasets due to several
2Table A3 Ablation studies on different backbones with VeCLIP We use 200M as the pretraining dataset
Model BackboneCOCO R1 Flickr30k R1ImageNet ImageNetV2I2T T2I I2T T2I
CLIP ViTB16 5220 3497 8090 6323 6372 5684
VeCLIP ViTB16 6720 4840 9110 7632 6462 5767
Performance Gain 1500 1343 1020 1306 090 081
CLIP ViTL14 5392 3786 8460 6678 6851 6113
VeCLIP ViTL14 6992 5132 9260 7904 6985 6354
Performance Gain 1600 1346 800 1226 134 241
VeCLIP ViTL14 vs B16 272 292 150 272 523 587
Table A4 Ablation studies on wellcurated datasets CC3M and CC12M  5 and the effect of data quality with ViTB16 as the vision
backbone
Model ModelCOCO R1 Flickr30k R1ImageNet ImageNetV2I2T T2I I2T T2I
WIT3MCLIP 518 340 1050 688 802 688
VeCLIP 2230 1301 4060 2758 1598 1351
Performance Gain 1712 961 3010 2070 796 663
CC3MCLIP 1388 964 2630 1804 1459 1252
VeCLIP 3204 2207 5720 3654 2073 1790
Performance Gain 1816 1243 3090 1850 614 538
WIT12MCLIP 2258 1423 4440 3090 3114 2591
VeCLIP 4778 3162 7390 5568 3811 3251
Performance Gain 2520 1739 2950 2478 697 660
CC12MCLIP 3796 2440 5970 4490 3924 3441
VeCLIP 5323 3690 7520 6210 4532 4021
Performance Gain 1527 1250 1550 1720 608 580
rounds of filtering and refining involved in the curation
of CC3M and CC12M In this section we show detailed
captions from CC3M and compare them with AltTexts from
WIT datasets
Here we provide more examples of AltText and LLM
VeC from WIT3M
1AltText Ring Capri Pomellato  Pomellato Online
Boutique
VeCap Pomellatos Ring Capri features a delicate and
elegant white stone or possibly three pearls set against a
white background
2AltText Fiamma F45 L 450 Royal Blue Awning
VeCap The Fiamma F45 L 450 Royal Blue Awning is
featured on a white car with a visible red logo for perfect
closing parked in a driveway under a tree with a house
in the background
3AltText Union votes for strike on pensions
VeCap The man with white hair dressed in a suit and
tie exhibits a surprised or expressive look on his facewith his mouth open and hand near his face creating a
dynamic and energetic expression
4AltText rreallifedoodles  I can show you the world
VeCap The large orange and black drone hovers in the
air carrying two small teddy bears attached to it above a
patio area as seen in the image
5AltText 20 Amazon Skincare Products That Keep Sell
ing Out
VeCap 20 Amazon skincare products that keep selling
out feature a happy woman with dark skin wearing a
white shirt and covering her face with her hands with a
white spot or patch on her skin
6AltText Durable White Arcane Dining Console Table
With 6 Hidden Chairs
VeCap A durable white arcane dining console table with
6 hidden chairs is visually appealing and ready for use
as seen in the image featuring a dining set with a white
table and two benches surrounded by black chairs
7AltText Peaceful apartment with wi fi internet access
near old Quebec
3Table A5 Ablation studies on VeCap and Simplied Entities Representation SER We use ViTB16 as the backbone and use 200M as the
pretrained dataset
Model CaptionCOCO R1 Flickr30k R1ImageNet ImageNetV2I2T T2I I2T T2I
CLIP AltText 5220 3497 8090 6323 6372 5684
VeCLIP SER 6588 4904 8920 7596 5858 5289
VeCLIP VeCap 6720 4840 9110 7632 6462 5767
Table A6 Zeroshot classification accuracy Top1 Accuracies   of VTAB  46 across 9 tasks 6 from natural and 3 from specialized sets
are reported
Data ModelNatural Sets Specialized SetsAverageCaltech101 CIFAR100 SVHN DTD OxPet Flowers102 EuroSAT RESISC45 Camelyon
Model Architecture ViTB16
3MCLIP 3950 983 2089 742 744 1040 1194 793 5065 1845
VeCLIP 5430 1774 1874 1123 1009 2275 735 1654 5252 2348
Performance Gain 1480 791 215 381 265 1235 459 861 187 503
12MCLIP 7043 3006 3011 3069 3451 3367 887 3005 5346 3576
VeCLIP 7058 4510 2361 3090 3622 4394 2746 3809 5554 4127
Performance Gain 015 1504 650 021 171 1027 1859 804 208 551
100MCLIP 8144 5475 3870 5728 7051 5171 3445 4856 5387 5459
VeCLIP 8164 6462 4649 5751 6481 6641 4623 5175 5851 5978
Performance Gain 020 987 779 023 570 1470 1178 319 464 519
200MCLIP 8230 6187 4283 6429 7560 5867 4673 5559 5930 6079
VeCLIP 8314 6814 4493 6195 7261 6851 4736 5510 6259 6270
Performance Gain 084 627 210 234 299 984 063 049 329 191
300MCLIP 8358 6336 5004 6616 7430 6181 3995 5644 5394 6106
VeCLIP 8307 6837 5007 6598 7536 6971 4828 5809 5194 6343
Performance Gain 051 501 003 018 106 790 833 165 200 237
VeCap Experience a peaceful stay in a cozy apartment
with WiFi internet access located near historic Old Que
bec featuring a charming dining room with a set table
and chairs on a hardwood floor complete with a white
refrigerator in the background
8AltText CABLE BUJIA CHEVROLET CORSA 10 14
EFI FERRAZZI CABLE BUJIA CHEVROLET CORSA
10 14 EFI FERRAZZI
VeCap An array of cords and wires comprising a black
rubber cable is displayed on a pristine surface featuring
diverse configurations and orientations with some lying
horizontally and others positioned at angles
Here we provide more examples of original caption and
VeCap from CC3M
1CC3M Caption person runs with the ball during their
training session on friday
VeCap A group of soccer players clad in red and black
jerseys are energetically engaging in a game on a vast
field with some running and others immersed in the
action dispersed across the terrain
2CC3M Caption a house with red roof with some bushes
and a lamp post in frontVeCap A prominent twostory beige building with a
distinctive tile roof stands out in the area illuminated by
a nearby lamp post The building appears to be a complex
with several houses or apartments adding a touch of
complexity to the surroundings
3CC3M Caption eating a big sweet cupcake with choco
late at cafe
VeCap A person holds a halfeaten blueberry muffin on
a plate standing next to a dining table with a cup while
eating a big sweet cupcake with chocolate at a cafe
4CC3M Caption paper heart with red ribbon and a bow
VeCap A pink background showcases a heartshaped
box with a bow adorned in white with the message
Happy Valentines Day positioned centrally within the
image
5CC3M Caption person andactor at the premiere
VeCap Two individuals a man and a woman are de
picted standing together both attired in formal attire The
man is donning a tuxedo with a black bow tie while the
woman is wearing a long dress They seem to be posi
tioning themselves for a photograph possibly at a formal
event
4Table A7 ImagetoImage retrieval results mAP on 6domain
GPR1200
Data ModelDomain Name
Land Faces iNat INST Sketch SOP All
3MCLIP 5798 2076 1761 3114 1823 7429 3667
VeCLIP 6655 2351 2043 3863 2459 7765 4189
12MCLIP 7447 3065 2360 5215 3068 8425 4930
VeCLIP 7930 3172 2553 5665 4142 8469 5322
100MCLIP 8564 5168 2966 6819 4245 9038 6133
VeCLIP 8559 4283 3072 7196 5259 9054 6237
200MCLIP 8696 5654 3095 7151 4603 9095 6383
VeCLIP 8640 4848 3172 7374 5652 9116 6567
300MCLIP 8717 5709 3183 7280 4703 9130 6454
VeCLIP 8622 4851 3205 7529 5618 9125 6691Table A8 Zeroshot classification results Top kAccuracy on
ImageNet and ImageNetV2
Data ModelImageNet ImageNetV2
Top1 Top5 Top10 Top1 Top5 Top10
3MCLIP 546 2105 2870 709 1852 2583
VeCLIP 1598 3411 4323 1351 3003 3893
12MCLIP 3160 5880 6949 2703 5268 6337
VeCLIP 3811 6674 7636 3253 6016 7050
100MCLIP 5864 8582 9179 5096 7977 8691
VeCLIP 6077 8777 9316 5417 8251 8924
200MCLIP 6372 8926 9411 5684 8350 8979
VeCLIP 6462 9027 9490 5767 8524 9162
300MCLIP 6570 9055 9487 5858 8532 9135
VeCLIP 6571 9115 9536 5876 8631 9195
Table A9 Comparison bwetween VeCLIP and other models
Backbone Model DataCOCO R1 Flickr30k R1ImageNetI2T T2I I2T T2I
ViTB16OpenAI CLIP OpenAI400M 538 331 880 687 686
FLIP 22 LAION400M     680
VeCLIP DFN 13  VeCap 663 451 888 736 762
ViTL14OpenAI CLIP OpenAI400M 584 378 880 687 753
FLIP 22 LAION400M 602 442 891 754 746
VeCLIP DFN 13  VeCap 711 511 931 810 820
ViTH14 VeCLIP DFN 13  VeCap 728 523 936 826 831
6CC3M Caption wedding ceremony on the beach
VeCap A picturesque wedding ceremony unfolds on a
stunning white sandy beach where perfectly arranged
chairs accommodate guests in formal attire The groom
and bride exude joy and love basking in the warm sun
light
7CC3M Caption revenge is a dish best served cold 
with lots of lettuce 
VeCap A large possibly turtle tortoise with an angry
expression sits on rocks displaying a saying or text mes
sage that reads Revenge is a dish best cold served with
lots of lettuce
8CC3M Caption interior of an abandoned factory
VeCap The sunlit interior of an industrial building stands
in contrast to its darker exterior with numerous windows
allowing natural light to flood the space giving it an
empty and open appearance devoid of people or personal
touches
Examining the aforementioned instances it becomes evi
dent that CC3Ms captions exhibit a notable level of preci
sion and high quality displaying a closer alignment with the
corresponding images Conversely WIT3Ms AltTexts tend
to be more cluttered signaling a comparatively subpar per
formance in contrast to CC3M Upon implementing VeCap
even though CC3Ms captions are of high quality they are
enhanced with more visual concepts leveraged via VeCapSuch integration of enriched visual concepts accounts for the
significant improvement we achieve in retrieval tasks the
results are shown in Table A4
E More examples of WIT with VeCap
We conduct our scalable pipeline over 200 million image
text pairs We randomly select more examples below to
show the advantages of VeCap against the original AltText
in terms of visual concepts The examples are visualized in
Figure A1
5Figure A1 More examples of VeCap captions and AltTexts
6
  TACOTRON  T OWARDS ENDTOENDSPEECH SYN
THESIS
Yuxuan Wang RJ SkerryRyan Daisy Stanton Yonghui Wu Ron J Weissy Navdeep Jaitly
Zongheng Yang Ying Xiao Zhifeng Chen Samy Bengioy Quoc Le Yannis Agiomyrgiannakis
Rob Clark Rif A Saurous
Google Inc
fyxwangrjryanrif ggooglecom
ABSTRACT
A texttospeech synthesis system typically consists of multiple stages such as a
text analysis frontend an acoustic model and an audio synthesis module Build
ing these components often requires extensive domain expertise and may contain
brittle design choices In this paper we present Tacotron an endtoend genera
tive texttospeech model that synthesizes speech directly from characters Given
text audio pairs the model can be trained completely from scratch with ran
dom initialization We present several key techniques to make the sequenceto
sequence framework perform well for this challenging task Tacotron achieves a
382 subjective 5scale mean opinion score on US English outperforming a pro
duction parametric system in terms of naturalness In addition since Tacotron
generates speech at the frame level its substantially faster than samplelevel au
toregressive methods
1 I NTRODUCTION
Modern texttospeech TTS pipelines are complex Taylor 2009 For example it is common for
statistical parametric TTS to have a text frontend extracting various linguistic features a duration
model an acoustic feature prediction model and a complex signalprocessingbased vocoder Zen
et al 2009 Agiomyrgiannakis 2015 These components are based on extensive domain expertise
and are laborious to design They are also trained independently so errors from each component
may compound The complexity of modern TTS designs thus leads to substantial engineering efforts
when building a new system
There are thus many advantages of an integrated endtoend TTS system that can be trained on text
audiopairs with minimal human annotation First such a system alleviates the need for laborious
feature engineering which may involve heuristics and brittle design choices Second it more easily
allows for rich conditioning on various attributes such as speaker or language or highlevel features
like sentiment This is because conditioning can occur at the very beginning of the model rather
than only on certain components Similarly adaptation to new data might also be easier Finally
a single model is likely to be more robust than a multistage model where each components errors
can compound These advantages imply that an endtoend model could allow us to train on huge
amounts of rich expressive yet often noisy data found in the real world
TTS is a largescale inverse problem a highly compressed source text is decompressed into
audio Since the same text can correspond to different pronunciations or speaking styles this is a
particularly difﬁcult learning task for an endtoend model it must cope with large variations at the
signal level for a given input Moreover unlike endtoend speech recognition Chan et al 2016
These authors really like tacos
yThese authors would prefer sushi
1arXiv170310135v2  csCL  6 Apr 2017Attention 
Prenet CBHG 
Character embeddings Attention 
RNNDecoder 
RNN 
Prenet Attention 
RNNDecoder 
RNN
Prenet Attention 
RNNDecoder 
RNN
Prenet CBHG Linearscale 
spectrogram 
Seq2seq target 
with r3 GriffinLim reconstruction 
Attention is applied 
to all decoder steps 
GO frame Figure 1 Model architecture The model takes characters as input and outputs the corresponding
raw spectrogram which is then fed to the GrifﬁnLim reconstruction algorithm to synthesize speech
or machine translation Wu et al 2016 TTS outputs are continuous and output sequences are
usually much longer than those of the input These attributes cause prediction errors to accumulate
quickly In this paper we propose Tacotron an endtoend generative TTS model based on the
sequencetosequence seq2seq Sutskever et al 2014 with attention paradigm Bahdanau et al
2014 Our model takes characters as input and outputs raw spectrogram using several techniques
to improve the capability of a vanilla seq2seq model Given text audio pairs Tacotron can
be trained completely from scratch with random initialization It does not require phonemelevel
alignment so it can easily scale to using large amounts of acoustic data with transcripts With a
simple waveform synthesis technique Tacotron produces a 382 mean opinion score MOS on an
US English eval set outperforming a production parametric system in terms of naturalness1
2 R ELATED WORK
WaveNet van den Oord et al 2016 is a powerful generative model of audio It works well for TTS
but is slow due to its samplelevel autoregressive nature It also requires conditioning on linguistic
features from an existing TTS frontend and thus is not endtoend it only replaces the vocoder and
acoustic model Another recentlydeveloped neural model is DeepV oice Arik et al 2017 which
replaces every component in a typical TTS pipeline by a corresponding neural network However
each component is independently trained and its nontrivial to change the system to train in an
endtoend fashion
To our knowledge Wang et al 2016 is the earliest work touching endtoend TTS using seq2seq
with attention However it requires a pretrained hidden Markov model HMM aligner to help the
seq2seq model learn the alignment Its hard to tell how much alignment is learned by the seq2seq
per se Second a few tricks are used to get the model trained which the authors note hurts prosody
Third it predicts vocoder parameters hence needs a vocoder Furthermore the model is trained on
phoneme inputs and the experimental results seem to be somewhat limited
Char2Wav Sotelo et al 2017 is an independentlydeveloped endtoend model that can be trained
on characters However Char2Wav still predicts vocoder parameters before using a SampleRNN
neural vocoder Mehri et al 2016 whereas Tacotron directly predicts raw spectrogram Also their
seq2seq and SampleRNN models need to be separately pretrained but our model can be trained
1Sound demos can be found at httpsgooglegithubiotacotron
2from scratch Finally we made several key modiﬁcations to the vanilla seq2seq paradigm As
shown later a vanilla seq2seq model does not work well for characterlevel inputs
3 M ODEL ARCHITECTURE
The backbone of Tacotron is a seq2seq model with attention Bahdanau et al 2014 Vinyals et al
2015 Figure 1 depicts the model which includes an encoder an attentionbased decoder and a
postprocessing net At a highlevel our model takes characters as input and produces spectrogram
frames which are then converted to waveforms We describe these components below
Conv1D layers Highway layers 
Conv1D bank  stacking Maxpool along time stride1 Bidirectional RNN 
Residual connection 
Conv1D projections 
Figure 2 The CBHG 1D convolution bank  highway network  bidirectional GRU module
adapted from Lee et al 2016
31 CBHG MODULE
We ﬁrst describe a building block dubbed CBHG illustrated in Figure 2 CBHG consists of a
bank of 1D convolutional ﬁlters followed by highway networks Srivastava et al 2015 and a
bidirectional gated recurrent unit GRU Chung et al 2014 recurrent neural net RNN CBHG
is a powerful module for extracting representations from sequences The input sequence is ﬁrst
convolved with Ksets of 1D convolutional ﬁlters where the kth set contains Ckﬁlters of width
kiek 12     K  These ﬁlters explicitly model local and contextual information akin to
modeling unigrams bigrams up to Kgrams The convolution outputs are stacked together and
further max pooled along time to increase local invariances Note that we use a stride of 1 to
preserve the original time resolution We further pass the processed sequence to a few ﬁxedwidth
1D convolutions whose outputs are added with the original input sequence via residual connections
He et al 2016 Batch normalization Ioffe  Szegedy 2015 is used for all convolutional layers
The convolution outputs are fed into a multilayer highway network to extract highlevel features
Finally we stack a bidirectional GRU RNN on top to extract sequential features from both forward
and backward context CBHG is inspired from work in machine translation Lee et al 2016
where the main differences from Lee et al 2016 include using noncausal convolutions batch
normalization residual connections and stride1 max pooling We found that these modiﬁcations
improved generalization
32 E NCODER
The goal of the encoder is to extract robust sequential representations of text The input to the
encoder is a character sequence where each character is represented as a onehot vector and em
3Table 1 Hyperparameters and network architectures conv kcReLU denotes 1D convolution
with width kandcoutput channels with ReLU activation FC stands for fullyconnected
Spectral analysis preemphasis  097 frame length  50 ms
frame shift  125 ms window type  Hann
Character embedding 256D
Encoder CBHG Conv1D bank K16 conv k128ReLU
Max pooling  stride1 width2
Conv1D projections  conv3128ReLU
conv3128Linear
Highway net  4 layers of FC128ReLU
Bidirectional GRU  128 cells
Encoder prenet FC256ReLUDropout05
FC128ReLUDropout05
Decoder prenet FC256ReLUDropout05
FC128ReLUDropout05
Decoder RNN 2layer residual GRU 256 cells
Attention RNN 1layer GRU 256 cells
Postprocessing net Conv1D bank K8 convk128ReLU
CBHG Max pooling  stride1 width2
Conv1D projections  conv3256ReLU
conv380Linear
Highway net  4 layers of FC128ReLU
Bidirectional GRU  128 cells
Reduction factor  r 2
bedded into a continuous vector We then apply a set of nonlinear transformations collectively
called a prenet to each embedding We use a bottleneck layer with dropout as the prenet in this
work which helps convergence and improves generalization A CBHG module transforms the pre
net outputs into the ﬁnal encoder representation used by the attention module We found that this
CBHGbased encoder not only reduces overﬁtting but also makes fewer mispronunciations than a
standard multilayer RNN encoder see our linked page of audio samples
33 D ECODER
We use a contentbased tanh attention decoder see eg Vinyals et al 2015 where a stateful recur
rent layer produces the attention query at each decoder time step We concatenate the context vector
and the attention RNN cell output to form the input to the decoder RNNs We use a stack of GRUs
with vertical residual connections Wu et al 2016 for the decoder We found the residual con
nections speed up convergence The decoder target is an important design choice While we could
directly predict raw spectrogram its a highly redundant representation for the purpose of learning
alignment between speech signal and text which is really the motivation of using seq2seq for this
task Because of this redundancy we use a different target for seq2seq decoding and waveform syn
thesis The seq2seq target can be highly compressed as long as it provides sufﬁcient intelligibility
and prosody information for an inversion process which could be ﬁxed or trained We use 80band
melscale spectrogram as the target though fewer bands or more concise targets such as cepstrum
could be used We use a postprocessing network discussed below to convert from the seq2seq
target to waveform
We use a simple fullyconnected output layer to predict the decoder targets An important trick we
discovered was predicting multiple nonoverlapping output frames at each decoder step Predicting
rframes at once divides the total number of decoder steps by r which reduces model size training
time and inference time More importantly we found this trick to substantially increase convergence
speed as measured by a much faster and more stable alignment learned from attention This is
likely because neighboring speech frames are correlated and each character usually corresponds to
multiple frames Emitting one frame at a time forces the model to attend to the same input token for
multiple timesteps emitting multiple frames allows the attention to move forward early in training
A similar trick is also used in Zen et al 2016 but mainly to speed up inference
4The ﬁrst decoder step is conditioned on an allzero frame which represents a GOframe In
inference at decoder step t the last frame of the rpredictions is fed as input to the decoder at step
t 1 Note that feeding the last prediction is an adhoc choice here  we could use all rpredictions
During training we always feed every rth ground truth frame to the decoder The input frame is
passed to a prenet as is done in the encoder Since we do not use techniques such as scheduled
sampling Bengio et al 2015 we found it to hurt audio quality the dropout in the prenet is
critical for the model to generalize as it provides a noise source to resolve the multiple modalities
in the output distribution
34 P OSTPROCESSING NET AND WAVEFORM SYNTHESIS
As mentioned above the postprocessing nets task is to convert the seq2seq target to a target that
can be synthesized into waveforms Since we use GrifﬁnLim as the synthesizer the postprocessing
net learns to predict spectral magnitude sampled on a linearfrequency scale Another motivation of
the postprocessing net is that it can see the full decoded sequence In contrast to seq2seq which
always runs from left to right it has both forward and backward information to correct the prediction
error for each individual frame In this work we use a CBHG module for the postprocessing net
though a simpler architecture likely works as well The concept of a postprocessing network is
highly general It could be used to predict alternative targets such as vocoder parameters or as a
WaveNetlike neural vocoder van den Oord et al 2016 Mehri et al 2016 Arik et al 2017 that
synthesizes waveform samples directly
We use the GrifﬁnLim algorithm Grifﬁn  Lim 1984 to synthesize waveform from the predicted
spectrogram We found that raising the predicted magnitudes by a power of 12 before feeding
to GrifﬁnLim reduces artifacts likely due to its harmonic enhancement effect We observed that
GrifﬁnLim converges after 50 iterations in fact about 30 iterations seems to be enough which
is reasonably fast We implemented GrifﬁnLim in TensorFlow Abadi et al 2016 hence its also
part of the model While GrifﬁnLim is differentiable it does not have trainable weights we do not
impose any loss on it in this work We emphasize that our choice of GrifﬁnLim is for simplicity
while it already yields strong results developing a fast and highquality trainable spectrogram to
waveform inverter is ongoing work
4 M ODEL DETAILS
Table 1 lists the hyperparameters and network architectures We use log magnitude spectrogram
with Hann windowing 50 ms frame length 125 ms frame shift and 2048point Fourier transform
We also found preemphasis 097 to be helpful We use 24 kHz sampling rate for all experiments
We use r 2 output layer reduction factor for the MOS results in this paper though larger rvalues
egr 5 also work well We use the Adam optimizer Kingma  Ba 2015 with learning rate
decay which starts from 0001 and is reduced to 00005 00003 and 00001 after 500K 1M and 2M
global steps respectively We use a simple 1 loss for both seq2seq decoder melscale spectrogram
and postprocessing net linearscale spectrogram The two losses have equal weights
We train using a batch size of 32 where all sequences are padded to a max length Its a com
mon practice to train sequence models with a loss mask which masks loss on zeropadded frames
However we found that models trained this way dont know when to stop emitting outputs causing
repeated sounds towards the end One simple trick to get around this problem is to also reconstruct
the zeropadded frames
5 E XPERIMENTS
We train Tacotron on an internal North American English dataset which contains about 246 hours
of speech data spoken by a professional female speaker The phrases are text normalized eg 16
is converted to sixteen
50 50 100 150 200 250 300 350
Decoder timesteps020406080Encoder states
00010203040506070809
a Vanilla seq2seq  scheduled sampling
0 10 20 30 40 50 60 70
Decoder timesteps020406080Encoder states
00010203040506070809
b GRU encoder
0 10 20 30 40 50 60 70
Decoder timesteps020406080Encoder states
00010203040506070809
c Tacotron proposed
Figure 3 Attention alignments on a test phrase The decoder length in Tacotron is shorter due to
the use of the output reduction factor r5
51 A BLATION ANALYSIS
We conduct a few ablation studies to understand the key components in our model As is common
for generative models its hard to compare models based on objective metrics which often do not
correlate well with perception Theis et al 2015 We mainly rely on visual comparisons instead
We strongly encourage readers to listen to the provided samples
First we compare with a vanilla seq2seq model Both the encoder and decoder use 2 layers of
residual RNNs where each layer has 256 GRU cells we tried LSTM and got similar results No
prenet or postprocessing net is used and the decoder directly predicts linearscale log magnitude
spectrogram We found that scheduled sampling sampling rate 05 is required for this model to
learn alignments and generalize We show the learned attention alignment in Figure 3 Figure 3a
reveals that the vanilla seq2seq learns a poor alignment One problem is that attention tends to
60 20 40 60 80 100 120 140
Frame02004006008001000DFT bina Without postprocessing net
0 20 40 60 80 100 120 140
Frame02004006008001000DFT bin
b With postprocessing net
Figure 4 Predicted spectrograms with and without using the postprocessing net
get stuck for many frames before moving forward which causes bad speech intelligibility in the
synthesized signal The naturalness and overall duration are destroyed as a result In contrast our
model learns a clean and smooth alignment as shown in Figure 3c
Second we compare with a model with the CBHG encoder replaced by a 2layer residual GRU
encoder The rest of the model including the encoder prenet remain exactly the same Comparing
Figure 3b and 3c we can see that the alignment from the GRU encoder is noisier Listening to
synthesized signals we found that noisy alignment often leads to mispronunciations The CBHG
encoder reduces overﬁtting and generalizes well to long and complex phrases
Figures 4a and 4b demonstrate the beneﬁt of using the postprocessing net We trained a model
without the postprocessing net while keeping all the other components untouched except that the
decoder RNN predicts linearscale spectrogram With more contextual information the prediction
from the postprocessing net contains better resolved harmonics eg higher harmonics between
bins 100 and 400 and high frequency formant structure which reduces synthesis artifacts
52 M EAN OPINION SCORE TESTS
We conduct mean opinion score tests where the subjects were asked to rate the naturalness of the
stimuli in a 5point Likert scale score The MOS tests were crowdsourced from native speakers
7100 unseen phrases were used for the tests and each phrase received 8 ratings When computing
MOS we only include ratings where headphones were used We compare our model with a para
metric based on LSTM Zen et al 2016 and a concatenative system Gonzalvo et al 2016
both of which are in production As shown in Table 2 Tacotron achieves an MOS of 382 which
outperforms the parametric system Given the strong baselines and the artifacts introduced by the
GrifﬁnLim synthesis this represents a very promising result
Table 2 5scale mean opinion score evaluation
mean opinion score
Tacotron 3820085
Parametric 3690109
Concatenative 4090119
6 D ISCUSSIONS
We have proposed Tacotron an integrated endtoend generative TTS model that takes a character
sequence as input and outputs the corresponding spectrogram With a very simple waveform syn
thesis module it achieves a 382 MOS score on US English outperforming a production parametric
system in terms of naturalness Tacotron is framebased so the inference is substantially faster
than samplelevel autoregressive methods Unlike previous work Tacotron does not need hand
engineered linguistic features or complex components such as an HMM aligner It can be trained
from scratch with random initialization We perform simple text normalization though recent ad
vancements in learned text normalization Sproat  Jaitly 2016 may render this unnecessary in the
future
We have yet to investigate many aspects of our model many early design decisions have gone
unchanged Our output layer attention module loss function and GrifﬁnLimbased waveform
synthesizer are all ripe for improvement For example its well known that GrifﬁnLim outputs
may have audible artifacts We are currently working on fast and highquality neuralnetworkbased
spectrogram inversion
ACKNOWLEDGMENTS
The authors would like to thank Heiga Zen and Ziang Xie for constructive discussions and feedback
REFERENCES
Mart ın Abadi Ashish Agarwal Paul Barham Eugene Brevdo Zhifeng Chen Craig Citro Greg S
Corrado Andy Davis Jeffrey Dean Matthieu Devin et al TensorFlow Largescale machine
learning on heterogeneous distributed systems arXiv preprint arXiv160304467  2016
Yannis Agiomyrgiannakis V ocaine the vocoder and applications in speech synthesis In Acoustics
Speech and Signal Processing ICASSP 2015 IEEE International Conference on  pp 4230
4234 IEEE 2015
Sercan Arik Mike Chrzanowski Adam Coates Gregory Diamos Andrew Gibiansky Yongguo
Kang Xian Li John Miller Jonathan Raiman Shubho Sengupta and Mohammad Shoeybi Deep
voice Realtime neural texttospeech arXiv preprint arXiv170207825  2017
Dzmitry Bahdanau Kyunghyun Cho and Yoshua Bengio Neural machine translation by jointly
learning to align and translate arXiv preprint arXiv14090473  2014
Samy Bengio Oriol Vinyals Navdeep Jaitly and Noam Shazeer Scheduled sampling for sequence
prediction with recurrent neural networks In Advances in Neural Information Processing Sys
tems pp 11711179 2015
William Chan Navdeep Jaitly Quoc Le and Oriol Vinyals Listen attend and spell A neural
network for large vocabulary conversational speech recognition In Acoustics Speech and Signal
Processing ICASSP 2016 IEEE International Conference on  pp 49604964 IEEE 2016
8Junyoung Chung Caglar Gulcehre KyungHyun Cho and Yoshua Bengio Empirical evaluation of
gated recurrent neural networks on sequence modeling arXiv preprint arXiv14123555  2014
Xavi Gonzalvo Siamak Tazari Chunan Chan Markus Becker Alexander Gutkin and Hanna Silen
Recent advances in Google realtime HMMdriven unit selection synthesizer In Proc Inter
speech  pp 22382242 2016
Daniel Grifﬁn and Jae Lim Signal estimation from modiﬁed shorttime fourier transform IEEE
Transactions on Acoustics Speech and Signal Processing  322236243 1984
Kaiming He Xiangyu Zhang Shaoqing Ren and Jian Sun Deep residual learning for image recog
nition In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pp
770778 2016
Sergey Ioffe and Christian Szegedy Batch normalization Accelerating deep network training by
reducing internal covariate shift arXiv preprint arXiv150203167  2015
Diederik Kingma and Jimmy Ba Adam A method for stochastic optimization Proceedings of the
3rd International Conference on Learning Representations ICLR  2015
Jason Lee Kyunghyun Cho and Thomas Hofmann Fully characterlevel neural machine translation
without explicit segmentation arXiv preprint arXiv161003017  2016
Soroush Mehri Kundan Kumar Ishaan Gulrajani Rithesh Kumar Shubham Jain Jose Sotelo
Aaron Courville and Yoshua Bengio SampleRNN An unconditional endtoend neural audio
generation model arXiv preprint arXiv161207837  2016
Jose Sotelo Soroush Mehri Kundan Kumar Jo ao Felipe Santos Kyle Kastner Aaron Courville and
Yoshua Bengio Char2Wav Endtoend speech synthesis In ICLR2017 workshop submission 
2017
Richard Sproat and Navdeep Jaitly RNN approaches to text normalization A challenge arXiv
preprint arXiv161100068  2016
Rupesh Kumar Srivastava Klaus Greff and J urgen Schmidhuber Highway networks arXiv preprint
arXiv150500387  2015
Ilya Sutskever Oriol Vinyals and Quoc V Le Sequence to sequence learning with neural networks
InAdvances in neural information processing systems  pp 31043112 2014
Paul Taylor Texttospeech synthesis  Cambridge university press 2009
Lucas Theis A aron van den Oord and Matthias Bethge A note on the evaluation of generative
models arXiv preprint arXiv151101844  2015
Aaron van den Oord Sander Dieleman Heiga Zen Karen Simonyan Oriol Vinyals Alex Graves
Nal Kalchbrenner Andrew Senior and Koray Kavukcuoglu WaveNet A generative model for
raw audio arXiv preprint arXiv160903499  2016
Oriol Vinyals Łukasz Kaiser Terry Koo Slav Petrov Ilya Sutskever and Geoffrey Hinton Gram
mar as a foreign language In Advances in Neural Information Processing Systems  pp 2773
2781 2015
Wenfu Wang Shuang Xu and Bo Xu First step towards endtoend parametric TTS synthesis
Generating spectral parameters with neural attention In Proceedings Interspeech  pp 22432247
2016
Yonghui Wu Mike Schuster Zhifeng Chen Quoc V Le Mohammad Norouzi Wolfgang Macherey
Maxim Krikun Yuan Cao Qin Gao Klaus Macherey et al Googles neural machine trans
lation system Bridging the gap between human and machine translation arXiv preprint
arXiv160908144  2016
Heiga Zen Keiichi Tokuda and Alan W Black Statistical parametric speech synthesis Speech
Communication  511110391064 2009
9Heiga Zen Yannis Agiomyrgiannakis Niels Egberts Fergus Henderson and Przemysław Szczepa
niak Fast compact and high quality LSTMRNN based statistical parametric speech synthesizers
for mobile devices Proceedings Interspeech  2016
10
  Grounded Compositional Semantics
for Finding and Describing Images with Sentences
Richard Socher Andrej Karpathy Quoc V  Le Christopher D Manning Andrew Y Ng
Stanford University Computer Science Department Google Inc
richardsocherorg karpathycsstanfordedu 
qvlgooglecom manningstanfordedu angcsstanfordedu
Abstract
Previous work on Recursive Neural Networks
RNNs shows that these models can produce
compositional feature vectors for accurately
representing and classifying sentences or im
ages However the sentence vectors of previ
ous models cannot accurately represent visu
ally grounded meaning We introduce the DT
RNN model which uses dependency trees to
embed sentences into a vector space in order
to retrieve images that are described by those
sentences Unlike previous RNNbased mod
els which use constituency trees DTRNNs
naturally focus on the action and agents in
a sentence They are better able to abstract
from the details of word order and syntactic
expression DTRNNs outperform other re
cursive and recurrent neural networks kernel
ized CCA and a bagofwords baseline on the
tasks of ﬁnding an image that ﬁts a sentence
description and vice versa They also give
more similar representations to sentences that
describe the same image
1 Introduction
Single word vector spaces are widely used Turney
and Pantel 2010 and successful at classifying sin
gle words and capturing their meaning Collobert
and Weston 2008 Huang et al 2012 Mikolov et
al 2013 Since words rarely appear in isolation
the task of learning compositional meaning repre
sentations for longer phrases has recently received a
lot of attention Mitchell and Lapata 2010 Socher
et al 2010 Socher et al 2012 Grefenstette et al
2013 Similarly classifying whole images into aﬁxed set of classes also achieves very high perfor
mance Le et al 2012 Krizhevsky et al 2012
However similar to words objects in images are of
ten seen in relationships with other objects which are
not adequately described by a single label
In this work we introduce a model illustrated in
Fig 1 which learns to map sentences and images
into a common embedding space in order to be able
to retrieve one from the other We assume word and
image representations are ﬁrst learned in their re
spective single modalities but ﬁnally mapped into a
jointly learned multimodal embedding space
Our model for mapping sentences into this space
is based on ideas from Recursive Neural Networks
RNNs Pollack 1990 Costa et al 2003 Socher
et al 2011b However unlike all previous RNN
models which are based on constituency trees CT
RNNs our model computes compositional vector
representations inside dependency trees The com
positional vectors computed by this new dependency
tree RNN DTRNN capture more of the meaning
of sentences where we deﬁne meaning in terms of
similarity to a visual representation of the textual
description DTRNN induced vector representa
tions of sentences are more robust to changes in the
syntactic structure or word order than related mod
els such as CTRNNs or Recurrent Neural Networks
since they naturally focus on a sentences action and
its agents
We evaluate and compare DTRNN induced rep
resentations on their ability to use a sentence such as
A man wearing a helmet jumps on his bike near a
beach  to ﬁnd images that show such a scene The
goal is to learn sentence representations that captureA man wearing a helmet jumps on his bike near a beach 
Compositional Sentence Vectors
Two airplanes parked in an airport 
A man jumping his downhill bike 
Image Vector Representation
A small child sits on a cement wall near white flower 
Multi Modal 
RepresentationsFigure 1 The DTRNN learns vector representations for sentences based on their dependency trees We learn to
map the outputs of convolutional neural networks applied to images into the same space and can then compare both
sentences and images This allows us to query images with a sentence and give sentence descriptions to images
the visual scene described and to ﬁnd appropriate
images in the learned multimodal sentenceimage
space Conversely when given a query image we
would like to ﬁnd a description that goes beyond a
single label by providing a correct sentence describ
ing it a task that has recently garnered a lot of at
tention Farhadi et al 2010 Ordonez et al 2011
Kuznetsova et al 2012 We use the dataset intro
duced by Rashtchian et al 2010 which consists of
1000 images each with 5 descriptions On all tasks
our model outperforms baselines and related mod
els
2 Related Work
The presented model is connected to several areas of
NLP and vision research each with a large amount
of related work to which we can only do some justice
given space constraints
Semantic Vector Spaces and Their Composition
ality The dominant approach in semantic vec
tor spaces uses distributional similarities of single
words Often cooccurrence statistics of a word and
its context are used to describe each word Turney
and Pantel 2010 Baroni and Lenci 2010 such
as tfidf Most of the compositionality algorithms
and related datasets capture twoword compositions
For instance Mitchell and Lapata 2010 use two
word phrases and analyze similarities computed by
vector addition multiplication and others Compo
sitionality is an active ﬁeld of research with many
different models and representations being explored
Grefenstette et al 2013 among many others We
compare to supervised compositional models thatcan learn taskspeciﬁc vector representations such as
constituency tree recursive neural networks Socher
et al 2011b Socher et al 2011a chain structured
recurrent neural networks and other baselines An
other alternative would be to use CCG trees as a
backbone for vector composition KM Hermann
2013
Multimodal Embeddings Multimodal embed
ding methods project data from multiple sources
such as sound and video Ngiam et al 2011 or im
ages and text Socher et al Socher and FeiFei
2010 project words and image regions into a com
mon space using kernelized canonical correlation
analysis to obtain state of the art performance in an
notation and segmentation Similar to our work they
use unsupervised large text corpora to learn seman
tic word representations Among other recent work
is that by Srivastava and Salakhutdinov 2012 who
developed multimodal Deep Boltzmann Machines
Similar to their work we use techniques from the
broad ﬁeld of deep learning to represent images and
words
Recently single word vector embeddings have
been used for zero shot learning Socher et al
2013c Mapping images to word vectors enabled
their system to classify images as depicting objects
such as cat without seeing any examples of this
class Related work has also been presented at NIPS
Socher et al 2013b Frome et al 2013 This work
moves zeroshot learning beyond single categories
per image and extends it to unseen phrases and full
length sentences making use of similar ideas of se
mantic spaces grounded in visual knowledgeDetailed Image Annotation Interactions be
tween images and texts is a growing research ﬁeld
Early work in this area includes generating single
words or ﬁxed phrases from images Duygulu et al
2002 Barnard et al 2003 or using contextual in
formation to improve recognition Gupta and Davis
2008 Torralba et al 2010
Apart from a large body of work on single object
image classiﬁcation Le et al 2012 there is also
work on attribute classiﬁcation and other midlevel
elements Kumar et al 2009 some of which we
hope to capture with our approach as well
Our work is close in spirit with recent work in de
scribing images with more detailed longer textual
descriptions In particular Yao et al 2010 describe
images using hierarchical knowledge and humans in
the loop In contrast our work does not require hu
man interactions Farhadi et al 2010 and Kulkarni
et al 2011 on the other hand use a more automatic
method to parse images For instance the former ap
proach uses a single triple of objects estimated for an
image to retrieve sentences from a collection written
to describe similar images It forms representations
to describe 1 object 1 action and 1 scene Kulkarni
et al 2011 extends their method to describe an im
age with multiple objects None of these approaches
have used a compositional sentence vector repre
sentation and they require speciﬁc language gener
ation techniques and sophisticated inference meth
ods Since our model is based on neural networks in
ference is fast and simple Kuznetsova et al 2012
use a very large parallel corpus to connect images
and sentences Feng and Lapata 2013 use a large
dataset of captioned images and experiments with
both extractive search and abstractive generation
models
Most related is the very recent work of Hodosh et
al 2013 They too evaluate using a ranking mea
sure In our experiments we compare to kernelized
Canonical Correlation Analysis which is the main
technique in their experiments
3 DependencyTree Recursive Neural
Networks
In this section we ﬁrst focus on the DTRNN model
that computes compositional vector representations
for phrases and sentences of variable length and syntactic type In section 5 the resulting vectors will
then become multimodal features by mapping im
ages that show what the sentence describes to the
same space and learning both the image and sen
tence mapping jointly
The most common way of building representa
tions for longer phrases from single word vectors is
to simply linearly average the word vectors While
this bagofwords approach can yield reasonable
performance in some tasks it gives all the words the
same weight and cannot distinguish important dif
ferences in simple visual descriptions such as The
bike crashed into the standing car vsThe car
crashed into the standing bike 
RNN models Pollack 1990 Goller and K uchler
1996 Socher et al 2011b Socher et al 2011a pro
vided a novel way of combining word vectors for
longer phrases that moved beyond simple averag
ing They combine vectors with an RNN in binary
constituency trees which have potentially many hid
den layers While the induced vector representations
work very well on many tasks they also inevitably
capture a lot of syntactic structure of the sentence
However the task of ﬁnding images from sentence
descriptions requires us to be more invariant to syn
tactic differences One such example are active
passive constructions which can collapse words such
as by in some formalisms de Marneffe et al
2006 relying instead on the semantic relationship
of agent For instance The mother hugged her
child and The child was hugged by its mother
should map to roughly the same visual space Cur
rent Recursive and Recurrent Neural Networks do
not exhibit this behavior and even bag of words rep
resentations would be inﬂuenced by the words was
andby The model we describe below focuses more
on recognizing actions and agents and has the po
tential to learn representations that are invariant to
activepassive differences
31 DTRNN Inputs Word Vectors and
Dependency Trees
In order for the DTRNN to compute a vector repre
sentation for an ordered list of mwords a phrase or
sentence we map the single words to a vector space
and then parse the sentence
First we map each word to a ddimensional vec
tor We initialize these word vectors with the unA man wearing a helmet jumps on his bike near a beachdetnsubj
partmod detdobjroot
prep posspobjprep
detpobj
Figure 2 Example of a full dependency tree for a longer sentence The DTRNN will compute vector representations
at every word that represents that word and an arbitrary number of child nodes The ﬁnal representation is computed
at the root node here at the verb jumps  Note that more important activity and object words are higher up in this tree
structure
supervised model of Huang et al 2012 which can
learn single word vector representations from both
local and global contexts The idea is to construct a
neural network that outputs high scores for windows
and documents that occur in a large unlabeled corpus
and low scores for windowdocument pairs where
one word is replaced by a random word When
such a network is optimized via gradient descent the
derivatives backpropagate into a word embedding
matrixAwhich stores word vectors as columns In
order to predict correct scores the vectors in the ma
trix capture cooccurrence statistics We use d 50
in all our experiments The embedding matrix X
is then used by ﬁnding the column index iof each
word w iand retrieving the corresponding col
umnxwfromX Henceforth we represent an input
sentencesas an ordered list of wordvector pairs
s w1xw1 wmxwm
Next the sequence of words w1wmis
parsed by the dependency parser of de Marneffe
et al 2006 Fig 2 shows an example We can
represent a dependency tree dof a sentence sas
an ordered list of childparent indices ds 
fijg where every child word in the sequence
i 1m is present and has any word j2
f1mgf 0gas its parent The root word has
as its parent 0and we notice that the same word can
be a parent between zero and mnumber of times
Without loss of generality we assume that these in
dices form a tree structure To summarize the input
to the DTRNN for each sentence is the pair sd
the words and their vectors and the dependency tree
32 Forward Propagation in DTRNNs
Given these two inputs we now illustrate how the
DTRNN computes parent vectors We will use the
following sentence as a running example Students 1
ride 2bikes 3at4night 5Fig 3 shows its tree
and computed vector representations The depen
Students                 bikes           nightride 
at          x1x2
x3x4
x5h1h2
h3h4
h5Figure 3 Example of a DTRNN tree structure for com
puting a sentence representation in a bottom up fashion
dency tree for this sentence can be summarized by
the following set of child parent edges d
f1220324254g
The DTRNN model will compute parent vectors
at each word that include all the dependent chil
dren nodes in a bottom up fashion using a com
positionality function gwhich is parameterized by
all the model parameters  To this end the algo
rithm searches for nodes in a tree that have either
i no children or ii whose children have already
been computed and then computes the correspond
ing vector
In our example the words x1x3x5are leaf
nodes and hence we can compute their correspond
ing hidden nodes via
hcgxc fWvxcforc 1351
where we compute the hidden vector at position c
via our general composition function g In the case
of leaf nodes this composition function becomes
simply a linear layer parameterized by Wv2Rnd
followed by a nonlinearity We crossvalidate over
using no nonlinearity  f id tanh sigmoid or
rectiﬁed linear units  f max0x but generally
ﬁndtanh to perform best
The ﬁnal sentence representation we want to com
pute is ath2 however since we still do not have h4we compute that one next
h4gx4h5 fWvx4Wr1h52
where we use the same Wvas before to map the
word vector into hidden space but we now also have
a linear layer that takes as input h5 the only child
of the fourth node The matrix Wr12Rnnis used
because node 5 is the ﬁrst child node on the right
side of node 4 Generally we have multiple matri
ces for composing with hidden child vectors from
the right and left sides Wr Wr1Wrkrand
Wl Wl1Wlkl The number of needed ma
trices is determined by the data by simply ﬁnding
the maximum numbers of left kland rightkrchil
dren any node has If at test time a child appeared
at an even large distance this does not happen in
our test set the corresponding matrix would be the
identity matrix
Now that all children of h2have their hidden vec
tors we can compute the ﬁnal sentence representa
tion via
h2gx2h1h3h4  3
fWvx2Wl1h1Wr1h3Wr2h4
Notice that the children are multiplied by matrices
that depend on their location relative to the current
node
Another modiﬁcation that improves the mean
rank by approximately 6 in image search on the dev
set is to weight nodes by the number of words under
neath them and normalize by the sum of words under
all children This encourages the intuitive desidera
tum that nodes describing longer phrases are more
important Let ibe the number of leaf nodes
words under node iandCiybe the set of child
nodes of node iin dependency tree y The ﬁnal com
position function for a node vector hibecomes
hif0
1
i0
WvxiX
j2CijWposijhj1
A1
A
4
where by deﬁnition i  1 P
j2Cijand
posijis the relative position of child jwith re
spect to node i egl1orr2in Eq 3
33 Semantic Dependency Tree RNNs
An alternative is to condition the weight matrices
on the semantic relations given by the dependencyparser We use the collapsed tree formalism of
the Stanford dependency parser de Marneffe et al
2006 With such a semantic untying of the weights
the DTRNN makes better use of the dependency
formalism and could give activepassive reversals
similar semantic vector representation The equation
for this semantic DTRNN  SDTRNN  is the same
as the one above except that the matrices Wposij
are replaced with matrices based on the dependency
relationship There are a total of 141 unique such
relationships in the dataset However most are very
rare For examples of semantic relationships see
Fig 2 and the model analysis section 67
This forward propagation can be used for com
puting compositional vectors and in Sec 5 we will
explain the objective function in which these are
trained
34 Comparison to Previous RNN Models
The DTRNN has several important differences to
previous RNN models of Socher et al 2011a and
Socher et al 2011b Socher et al 2011c These
constituency tree RNNs CTRNNs use the follow
ing composition function to compute a hidden par
ent vectorhfrom exactly two child vectors c1c2
in a binary tree hf
Wc1
c2
 whereW2
Rd2dis the main parameter to learn This can be
rewritten to show the similarity to the DTRNN as
hfWl1c1Wr1c2 However there are several
important differences
Note ﬁrst that in previous RNN models the par
ent vectors were of the same dimensionality to be
recursively compatible and be used as input to the
next composition In contrast our new model ﬁrst
maps single words into a hidden space and then par
ent nodes are composed from these hidden vectors
This allows a higher capacity representation which
is especially helpful for nodes that have many chil
dren
Secondly the DTRNN allows for nary nodes in
the tree This is an improvement that is possible even
for constituency tree CTRNNs but it has not been
explored in previous models
Third due to computing parent nodes in con
stituency trees previous models had the problem
that words that are merged last in the tree have a
larger weight or importance in the ﬁnal sentence repFigure 4 The architecture of the visual model This model has 3 sequences of ﬁltering pooling and local contrast
normalization layers The learnable parameters are the ﬁltering layer The ﬁlters are not shared ie the network is
nonconvolutional
resentation This can be problematic since these are
often simple noncontent words such as a leading
But While such single words can be important for
tasks such as sentiment analysis we argue that for
describing visual scenes the DTRNN captures the
more important effects The dependency tree struc
tures push the central content words such as the main
action or verb and its subject and object to be merged
last and hence by construction the ﬁnal sentence
representation is more robust to less important ad
jectival modiﬁers word order changes etc
Fourth we allow some untying of weights de
pending on either how far away a constituent is from
the current word or what its semantic relationship is
Now that we can compute compositional vector
representations for sentences the next section de
scribes how we represent images
4 Learning Image Representations with
Neural Networks
The image features that we use in our experiments
are extracted from a deep neural network replicated
from the one described in Le et al 2012 The net
work was trained using both unlabeled data random
web images and labeled data to classify 22000 cat
egories in ImageNet Deng et al 2009 We then
used the features at the last layer before the classi
ﬁer as the feature representation in our experiments
The dimension of the feature vector of the last layer
is 4096 The details of the model and its training
procedures are as follows
The architecture of the network can be seen in
Figure 4 The network takes 200x200 pixel images
as inputs and has 9 layers The layers consist ofthree sequences of ﬁltering pooling and local con
trast normalization Jarrett et al 2009 The pooling
function is L2 pooling of the previous layer taking
the square of the ﬁltering units summing them up
in a small area in the image and taking the square
root The local contrast normalization takes inputs
in a small area of the lower layer subtracts the mean
and divides by the standard deviation
The network was ﬁrst trained using an unsuper
vised objective trying to reconstruct the input while
keeping the neurons sparse In this phase the net
work was trained on 20 million images randomly
sampled from the web We resized a given image
so that its short dimension has 200 pixels We then
cropped a ﬁxed size 200x200 pixel image right at the
center of the resized image This means we may dis
card a fraction of the long dimension of the image
After unsupervised training we used Ima
geNet Deng et al 2009 to adjust the features in the
entire network The ImageNet dataset has 22000
categories and 14 million images The number of
images in each category is equal across categories
The 22000 categories are extracted from WordNet
To speed up the supervised training of this net
work we made a simple modiﬁcation to the algo
rithm described in Le et al 2012 adding a bottle
neck layer in between the last layer and the classi
ﬁer to reduce the number of connections We added
one bottleneck layer which has 4096 units in be
tween the last layer of the network and the softmax
layer This newlyadded layer is fully connected to
the previous layer and has a linear activation func
tion The total number of connections of this net
work is approximately 136 billionThe network was trained again using the super
vised objective of classifying the 22000 classes in
ImageNet Most features in the networks are local
which allows model parallelism Data parallelism
by asynchronous SGD was also employed as in Le
et al 2012 The entire training both unsupervised
and supervised took 8 days on a large cluster of ma
chines This network achieves 183 precision1
on the full ImageNet dataset Release Fall 2011
We will use the features at the bottleneck layer as
the feature vector zof an image Each scaled and
cropped image is presented to our network The net
work then performs a feedforward computation to
compute the values of the bottleneck layer This
means that every image is represented by a ﬁxed
length vector of 4096 dimensions Note that during
training no aligned sentenceimage data was used
and the ImageNet classes do not fully intersect with
the words used in our dataset
5 Multimodal Mappings
The previous two sections described how we can
map sentences into a d 50 dimensional space and
how to extract high quality image feature vectors of
4096 dimensions We now deﬁne our ﬁnal multi
modal objective function for learning joint image
sentence representations with these models Our
training set consists of Nimages and their feature
vectorsziand each image has 5 sentence descrip
tionssi1si5for which we use the DTRNN to
compute vector representations See Fig 5 for ex
amples from the dataset For training we use a max
margin objective function which intuitively trains
pairs of correct image and sentence vectors to have
high inner products and incorrect pairs to have low
inner products Let viWIzibe the mapped image
vector andyijDTRNN sijthe composed sen
tence vector We deﬁne Sto be the set of all sentence
indices andSithe set of sentence indices corre
sponding to image i SimilarlyIis the set of all im
age indices andIjis the image index of sentence
j The setPis the set of all correct imagesentence
training pairs ij The ranking cost function to
minimize is then JWI 
X
ij2PX
c2SnS imax0vT
iyjvT
iyc
X
ij2PX
c2InI jmax0vT
iyjvT
cyj5whereare the language composition matrices
and both second sums are over other sentences com
ing from different images and vice versa The hyper
parameter is the margin The margin is found via
cross validation on the dev set and usually around 1
The ﬁnal objective also includes the regulariza
tion termleft kk2
2kWIkF Both the visual
model and the word vector learning require a very
large amount of training data and both have a huge
number of parameters Hence to prevent overﬁtting
we assume their weights are ﬁxed and only train the
DTRNN parameters WI If larger training corpora
become available in the future training both jointly
becomes feasible and would present a very promis
ing direction We use a modiﬁed version of Ada
Grad Duchi et al 2011 for optimization of both
WIand the DTRNN as well as the other baselines
except kCCA Adagrad has achieved good perfor
mance previously in neural networks models Dean
et al 2012 Socher et al 2013a We modify it
by resetting all squared gradient sums to 1 every 5
epochs With both images and sentences in the same
multimodal space we can easily query the model for
similar images or sentences by ﬁnding the nearest
neighbors in terms of negative inner products
An alternative objective function is based on the
squared loss JWI P
ij2Pkviyjk2
2This
requires an alternating minimization scheme that
ﬁrst trains only WI then ﬁxes WIand trains the
DTRNN weights and then repeats this several
times We ﬁnd that the performance with this ob
jective function paired with ﬁnding similar images
using Euclidean distances is worse for all models
than the margin loss of Eq 5 In addition kCCA
also performs much better using inner products in
the multimodal space
6 Experiments
We use the dataset of Rashtchian et al 2010 which
consists of 1000 images each with 5 sentences See
Fig 5 for examples
We evaluate and compare the DTRNN in three
different experiments First we analyze how well
the sentence vectors capture similarity in visual
meaning Then we analyze Image Search with
Query Sentences  to query each model with a sen
tence in order to ﬁnd an image showing that sen1 A woman and her dog watch the cameraman in their living with wooden floors 
2 A woman sitting on the couch while a black faced dog runs across the floor 
3 A woman wearing a backpack sits on a couch while a small dog runs on the hardwood floor next to her 
4 A women sitting on a sofa while a small Jack Russell walks towards the camera 
5 White and black small dog walks toward the camera while woman sits on couch  desk and computer seen 
    in the background as well as a pillow  teddy bear and moggie toy on the wood floor 
1 A man in a cowboy hat check approaches a small red sports car 
2 The back and left side of a red Ferrari and two men admiring it 
3 The sporty car is admired by passer by 
4 Two men next to a red sports car in a parking lot 
5 Two men stand beside a red sports car Figure 5 Examples from the dataset of images and their sentence descriptions Rashtchian et al 2010 Sentence
length varies greatly and different objects can be mentioned ﬁrst Hence models have to be invariant to word ordering
tences visual meaning The last experiment De
scribing Images by Finding Suitable Sentences does
the reverse search where we query the model with an
image and try to ﬁnd the closest textual description
in the embedding space
In our comparison to other methods we focus on
those models that can also compute ﬁxed continu
ous vectors for sentences In particular we compare
to the RNN model on constituency trees of Socher
et al 2011a a standard recurrent neural network
a simple bagofwords baseline which averages the
words All models use the word vectors provided by
Huang et al 2012 and do not update them as dis
cussed above Models are trained with their corre
sponding gradients and backpropagation techniques
A standard recurrent model is used where the hidden
vector at word index tis computed from the hidden
vector at the previous time step and the current word
vectorhtfWhht1Wxxt During training
we take the last hidden vector of the sentence chain
and propagate the error into that It is also this vector
that is used to represent the sentence
Other possible comparisons are to the very differ
ent models mentioned in the related work section
These models use a lot more taskspeciﬁc engineer
ing such as running object detectors with bounding
boxes attribute classiﬁers scene classiﬁers CRFs
for composing the sentences etc Another line of
work uses large sentenceimage aligned resources
Kuznetsova et al 2012 whereas we focus on eas
ily obtainable training data of each modality sepa
rately and a rather small multimodal corpus
In our experiments we split the data into 800 train
ing 100 development and 100 test images Since
there are 5 sentences describing each image wehave 4000 training sentences and 500 testing sen
tences The dataset has 3020 unique words half of
which only appear once Hence the unsupervised
pretrained semantic word vector representations are
crucial Word vectors are not ﬁne tuned during train
ing Hence the main parameters are the DTRNNs
WlWror the semantic matrices of which there are
141 and the image mapping WI For both DTRNNs
the weight matrices are initialized to block identity
matrices plus Gaussian noise Word vectors and hid
den vectors are set o length 50 Using the develop
ment split we found  008and the learning rate
of AdaGrad to 00001  The best model uses a mar
gin of   3 
Inspired by Socher and FeiFei 2010 and Ho
dosh et al 2013 we also compare to kernelized
Canonical Correlation Analysis kCCA We use the
average of word vectors for describing sentences and
the same powerful image vectors as before We
use the code of Socher and FeiFei 2010 Tech
nically one could combine the recently introduced
deep CCA Andrew et al 2013 and train the re
cursive neural network architectures with the CCA
objective We leave this to future work With lin
ear kernels kCCA does well for image search but
is worse for sentence self similarity and describing
images with sentences closeby in embedding space
All other models are trained by replacing the DT
RNN function in Eq 5
61 Similarity of Sentences Describing the
Same Image
In this experiment we ﬁrst map all 500 sentences
from the test set into the multimodal space Then
for each sentence we ﬁnd the nearest neighbor senSentences Similarity for Image
Model Mean Rank
Random 1011
BoW 118
CTRNN 158
Recurrent NN 185
kCCA 107
DTRNN 111
SDTRNN 105Image Search
Model Mean Rank
Random 521
BoW 146
CTRNN 161
Recurrent NN 192
kCCA 159
DTRNN 136
SDTRNN 125Describing Images
Model Mean Rank
Random 921
BoW 211
CTRNN 239
Recurrent NN 271
kCCA 180
DTRNN 192
SDTRNN 169
Table 1 Left Comparison of methods for sentence similarity judgments Lower numbers are better since they indicate
that sentences describing the same image rank more highly are closer The ranks are out of the 500 sentences in the
test set Center Comparison of methods for image search with query sentences Shown is the average rank of the
single correct image that is being described Right Average rank of a correct sentence description for a query image
tences in terms of inner products We then sort
these neighbors and record the rank or position of
the nearest sentence that describes the same im
age If all the images were very unique and the vi
sual descriptions closeparaphrases and consistent
we would expect a very low rank However usually
a handful of images are quite similar for instance
there are various images of airplanes ﬂying parking
taxiing or waiting on the runway and sentence de
scriptions can vary greatly in detail and speciﬁcity
for the same image
Table 1 left shows the results We can see that
averaging the high quality word vectors already cap
tures a lot of similarity The chain structure of a
standard recurrent neural net performs worst since
its representation is dominated by the last words in
the sequence which may not be as important as ear
lier words
62 Image Search with Query Sentences
This experiment evaluates how well we can ﬁnd im
ages that display the visual meaning of a given sen
tence We ﬁrst map a query sentence into the vector
space and then ﬁnd images in the same space using
simple inner products As shown in Table 1 center
the new DTRNN outperforms all other models
63 Describing Images by Finding Suitable
Sentences
Lastly we repeat the above experiments but with
roles reversed For an image we search for suitable
textual descriptions again simply by ﬁnding close
by sentence vectors in the multimodal embedding
space Table 1 right shows that the DTRNN again
outperforms related models Fig 2assigned to imImage Search
Model mRank
BoW 247
CTRNN 222
Recurrent NN 284
kCCA 137
DTRNN 133
SDTRNN 158Describing Images
Model mRank
BoW 307
CTRNN 294
Recurrent NN 314
kCCA 380
DTRNN 268
SDTRNN 375
Table 2 Results of multimodal ranking when models are
trained with a squared error loss and using Euclidean dis
tance in the multimodal space Better performance is
reached for all models when trained in a maxmargin loss
and using inner products as in the previous table
ages The average ranking of 253 for a correct sen
tence description is out of 500 possible sentences A
random assignment would give an average ranking
of 100
64 Analysis Squared Error Loss vs Margin
Loss
We analyze the inﬂuence of the multimodal loss
function on the performance In addition we com
pare using Euclidean distances instead of inner prod
ucts Table 2 shows that performance is worse for all
models in this setting
65 Analysis Recall at nvs Mean Rank
Hodosh et al 2013 and other related work use re
call atnas an evaluation measure Recall at ncap
tures how often one of the top nclosest vectors were
a correct image or sentence and gives a good intu
ition of how a model would perform in a ranking
task that presents nsuch results to a user Below we
compare three commonly used and high performing
models bag of words kCCA and our SDTRNN onA gray convertible sports car is parked in front of the trees 
A close up view of the headlights of a blue old fashioned car 
Black shiny sports car parked on concrete driveway 
Five cows grazing on a patch of grass between two roadways 
A jockey rides a brown and white horse in a dirt corral 
A young woman is riding a Bay hose in a dirt riding ring
A white bird pushes a miniature teal shopping cart 
A person rides a brown horse 
A motocross bike with rider flying through the air 
White propeller plane parked in middle of grassy field 
The white jet with its landing gear down flies in the blue sky 
An elderly woman catches a ride on the back of the bicycle 
A green steam train running down the tracks 
Steamy locomotive speeding thou the forest 
A steam engine comes down a train track near trees 
A double decker bus is driving by Big Ben in London 
People in an outrigger canoe sail on emerald green water 
Two people sailing a small white sail boat 
behind a cliff  a boat sails away
Tourist move in on Big Ben on a typical overcast London day 
A group of people sitting around a table on a porch 
A group of four people walking past a giant mushroom 
A man and women smiling for the camera in a kitchen 
A group of men sitting around a table drinking while a man behind 
stands pointing 
Figure 6 Images and their sentence descriptions assigned by the DTRNN
Image Search
Model mRank4 R15 R55 R105
BoW 146 158 422 600
kCCA 159 164 414 580
SDTRNN 125 164 466 656
Describing Images
BoW 211 190 380 570
kCCA 180 210 470 610
SDTRNN 169 230 450 630
Table 3 Evaluation comparison between mean rank of
the closest correct image or sentence lower is better 4
with recall at different thresholds higher is better 5
With one exception R5 bottom table the SDTRNN
outperforms the other two models and all other models
we did not include here
this different metric Table 3 shows that the mea
sures do correlate well and the SDTRNN also per
forms best on the multimodal ranking tasks when
evaluated with this measure
66 Error Analysis
In order to understand the main problems with the
composed sentence vectors we analyze the sen
tences that have the worst nearest neighbor rank be
tween each other We ﬁnd that the main failure mode
of the SDTRNN occurs when a sentence that should
describe the same image does not use a verb but the
other sentences of that image do include a verb For
example the following sentence pair has vectors that
are very far apart from each other even though they
are supposed to describe the same image
1 A blue and yellow airplane ﬂying straight down
while emitting white smoke
2 Airplane in dive positionGenerally as long as both sentences either have a
verb or do not the SDTRNN is more robust to dif
ferent sentence lengths than bag of words represen
tations
67 Model Analysis Semantic Composition
Matrices
The best model uses composition matrices based on
semantic relationships from the dependency parser
We give some insights into what the model learns
by listing the composition matrices with the largest
Frobenius norms Intuitively these matrices have
learned larger weights that are being multiplied with
the child vector in the tree and hence that child will
have more weight in the ﬁnal composed parent vec
tor In decreasing order of Frobenius norm the re
lationship matrices are nominal subject possession
modiﬁer eg their passive auxiliary preposition
at preposition in front of passive auxiliary passive
nominal subject object of preposition preposition
in and preposition on
The model learns that nouns are very important as
well as their spatial prepositions and adjectives
7 Conclusion
We introduced a new recursive neural network
model that is based on dependency trees For eval
uation we use the challenging task of mapping sen
tences and images into a common space for ﬁnding
one from the other Our new model outperforms
baselines and other commonly used models that can
compute continuous vector representations for sen
tences In comparison to related models the DT
RNN is more invariant and robust to surface changes
such as word orderReferences
G Andrew R Arora K Livescu and J Bilmes 2013
Deep canonical correlation analysis In ICML  At
lanta Georgia
K Barnard P Duygulu N de Freitas D Forsyth
D Blei and M Jordan 2003 Matching words and
pictures JMLR 
M Baroni and A Lenci 2010 Distributional mem
ory A general framework for corpusbased semantics
Computational Linguistics  364673721
R Collobert and J Weston 2008 A uniﬁed archi
tecture for natural language processing deep neural
networks with multitask learning In Proceedings of
ICML  pages 160167
F Costa P Frasconi V  Lombardo and G Soda 2003
Towards incremental parsing of natural language using
recursive neural networks Applied Intelligence 
M de Marneffe B MacCartney and C D Manning
2006 Generating typed dependency parses from
phrase structure parses In LREC 
J Dean G S Corrado R Monga K Chen M Devin
Q V  Le M Z Mao M Ranzato A Senior P Tucker
K Yang and AY  Ng 2012 Large scale distributed
deep networks In NIPS 
J Deng W Dong R Socher LJ Li K Li and L Fei
Fei 2009 ImageNet A LargeScale Hierarchical Im
age Database In CVPR 
J Duchi E Hazan and Y  Singer 2011 Adaptive sub
gradient methods for online learning and stochastic op
timization JMLR  12 July
P Duygulu K Barnard N de Freitas and D Forsyth
2002 Object recognition as machine translation In
ECCV 
A Farhadi M Hejrati M A Sadeghi P Young
C Rashtchian J Hockenmaier and D Forsyth 2010
Every picture tells a story Generating sentences from
images In ECCV 
Y  Feng and M Lapata 2013 Automatic caption gen
eration for news images IEEE Trans Pattern Anal
Mach Intell  35
A Frome G Corrado J Shlens S Bengio J Dean
M Ranzato and T Mikolov 2013 Devise A deep
visualsemantic embedding model In NIPS 
C Goller and A K uchler 1996 Learning task
dependent distributed representations by backpropaga
tion through structure In Proceedings of the Interna
tional Conference on Neural Networks 
E Grefenstette G Dinu Y Z Zhang M Sadrzadeh and
M Baroni 2013 Multistep regression learning for
compositional distributional semantics In IWCS 
A Gupta and L S Davis 2008 Beyond nouns Exploit
ing prepositions and comparative adjectives for learn
ing visual classiﬁers In ECCV M Hodosh P Young and J Hockenmaier 2013 Fram
ing image description as a ranking task Data mod
els and evaluation metrics J Artif Intell Res JAIR 
47853899
E H Huang R Socher C D Manning and A Y  Ng
2012 Improving Word Representations via Global
Context and Multiple Word Prototypes In ACL
K Jarrett K Kavukcuoglu MA Ranzato and Y  Le
Cun 2009 What is the best multistage architecture
for object recognition In ICCV 
P Blunsom KM Hermann 2013 The role of syntax
in vector space models of compositional semantics In
ACL
A Krizhevsky I Sutskever and G E Hinton 2012
Imagenet classiﬁcation with deep convolutional neural
networks In NIPS 
G Kulkarni V  Premraj S Dhar S Li Y  Choi A C
Berg and T L Berg 2011 Baby talk Understanding
and generating image descriptions In CVPR 
N Kumar A C Berg P N Belhumeur  and S K Na
yar 2009 Attribute and simile classiﬁers for face ver
iﬁcation In ICCV 
P Kuznetsova V  Ordonez A C Berg T L Berg and
Yejin Choi 2012 Collective generation of natural
image descriptions In ACL
Q V  Le MA Ranzato R Monga M Devin K Chen
GS Corrado J Dean and A Y  Ng 2012 Build
ing highlevel features using large scale unsupervised
learning In ICML 
T Mikolov W Yih and G Zweig 2013 Linguistic
regularities in continuous spaceword representations
InHLTNAACL 
J Mitchell and M Lapata 2010 Composition in dis
tributional models of semantics Cognitive Science 
34813881429
J Ngiam A Khosla M Kim J Nam H Lee and AY 
Ng 2011 Multimodal deep learning In ICML 
V  Ordonez G Kulkarni and T L Berg 2011 Im2text
Describing images using 1 million captioned pho
tographs In NIPS 
J B Pollack 1990 Recursive distributed representa
tions Artiﬁcial Intelligence  46 November
C Rashtchian P Young M Hodosh and J Hocken
maier 2010 Collecting image annotations using
Amazons Mechanical Turk In Workshop on Creat
ing Speech and Language Data with Amazons MTurk 
R Socher and L FeiFei 2010 Connecting modalities
Semisupervised segmentation and annotation of im
ages using unaligned text corpora In CVPR 
R Socher C D Manning and A Y  Ng 2010 Learning
continuous phrase representations and syntactic pars
ing with recursive neural networks In Proceedings of
the NIPS2010 Deep Learning and Unsupervised Fea
ture Learning Workshop R Socher E H Huang J Pennington A Y  Ng and
C D Manning 2011a Dynamic Pooling and Unfold
ing Recursive Autoencoders for Paraphrase Detection
InNIPS 
R Socher C Lin A Y  Ng and CD Manning 2011b
Parsing Natural Scenes and Natural Language with
Recursive Neural Networks In ICML 
R Socher J Pennington E H Huang A Y  Ng and
C D Manning 2011c SemiSupervised Recursive
Autoencoders for Predicting Sentiment Distributions
InEMNLP 
R Socher B Huval C D Manning and A Y  Ng
2012 Semantic Compositionality Through Recursive
MatrixVector Spaces In EMNLP 
R Socher J Bauer C D Manning and A Y  Ng 2013a
Parsing With Compositional Vector Grammars In
ACL
R Socher M Ganjoo C D Manning and A Y  Ng
2013b ZeroShot Learning Through CrossModal
Transfer In NIPS 
R Socher M Ganjoo H Sridhar O Bastani and
A Y  Ng C D Manning and 2013c Zeroshot learn
ing through crossmodal transfer In Proceedings of
the International Conference on Learning Representa
tions ICLR Workshop Track 
N Srivastava and R Salakhutdinov 2012 Multimodal
learning with deep boltzmann machines In NIPS 
A Torralba K P Murphy and W T Freeman 2010
Using the forest to see the trees exploiting context for
visual object detection and localization Communica
tions of the ACM 
P D Turney and P Pantel 2010 From frequency to
meaning Vector space models of semantics Journal
of Artiﬁcial Intelligence Research  37141188
B Yao X Yang L Lin M W Lee and SC Zhu 2010
I2timage parsing to text description IEEE Xplore 
  Chameleon MixedModal EarlyFusion Foundation
Models
Chameleon Team1
1FAIR at Meta
See Contributions section for full author list
We present Chameleon a family of earlyfusion tokenbased mixedmodal models capable of under
standing and generating images and text in any arbitrary sequence We outline a stable training
approach from inception an alignment recipe and an architectural parameterization tailored for the
earlyfusion tokenbased mixedmodal setting The models are evaluated on a comprehensive range
of tasks including visual question answering image captioning text generation image generation and
longform mixed modal generation Chameleon demonstrates broad and general capabilities including
stateoftheart performance in image captioning tasks outperforms Llama2 in textonly tasks while
being competitive with models such as Mixtral 8x7B and GeminiPro and performs nontrivial image
generation all in a single model It also matches or exceeds the performance of much larger models
including Gemini Pro and GPT4V according to human judgments on a new longform mixedmodal
generation evaluation where either the prompt or outputs contain mixed sequences of both images and
text Chameleon marks a significant step forward in a unified modeling of full multimodal documents
DateMay 17 2024
1 Introduction
Recent multimodal foundation models are very widely adopted but still model different modalities separately
often using modality specific encoders or decoders This can limit their ability to integrate information across
modalities and generate multimodal documents that can contain arbitrary sequences of images and text In
this paper we present Chameleon  a family of mixedmodal foundation models capable of generating and
reasoning with mixed sequences of arbitrarily interleaved textual and image content Figures 24 This
allows for full multimodal document modeling which is a direct generalization of standard multimodal tasks
such as image generation understanding and reasoning over images and textonly LLMs Chameleon is
instead designed to be mixedmodel from inception and uses a uniform architecture trained from scratch in an
endtoend fashion on an interleaved mixture of all modalities ie images text and code
Our unified approach uses fully tokenbased representations for both image and textual modalities Figure 1
By quantizing images into discrete tokens analogous to words in text we can apply the same transformer
architecture to sequences of both image and text tokens without the need for separate imagetext encoders
Alayrac et al 2022 Liu et al 2023b Laurençon et al 2023 or domainspecific decoders Ramesh et al
2022 Jin et al 2023 Betker et al 2023 This earlyfusion approach where all modalities are projected into
a shared representational space from the start allows for seamless reasoning and generation across modalities
However it also presents significant technical challenges particularly in terms of optimization stability and
scaling
We address these challenges through a combination of architectural innovations and training techniques We
introduce novel modifications to the transformer architecture such as querykey normalization and revised
placement of layer norms which we find to be crucial for stable training in the mixedmodal setting Section
23 We further show how to adapt the supervised finetuning approaches used for textonly LLMs to the
mixedmodal setting enabling strong alignment at scale Section 3 Using these techniques we successfully
train Chameleon34B on 5x the number of tokens as Llama2  enabling new mixedmodal applications while
still matching or even outperforming existing LLMs on unimodal benchmarks
1arXiv240509818v1  csCL  16 May 2024MixedModal AutoRegressive LM 
TEXT PROMPT What can I bake 
with this TEXT OUTPUT Here is a recipe for 
banana bread 
Image Tokenizer Image DeTokenizer 
IMAGE PROMPT 
IMAGE OUTPUT 
Start 
ImageStart 
Image End
Image
End
ImageStart 
Image 
Start 
ImageEnd
Image
Mixed Modal AutoRegressive LM 
a MixedModal PreTraining b MixedModal Generation Figure 1 Chameleon represents all modalities  images text and code as discrete tokens and uses a uniform
transformerbased architecture that is trained from scratch in an endtoend fashion on 10T tokens of interleaved
mixedmodal data As a result Chameleon can both reason over as well as generate arbitrary mixedmodal documents
Text tokens are represented in green and image tokens are represented in blue
Extensive evaluations demonstrate that Chameleon is a broadly capable model on a diverse set of tasks
On visual question answering and image captioning benchmarks Chameleon34B achieves stateoftheart
performance outperforming models like Flamingo IDEFICS and Llava15 Section 52 At the same time
it maintains competitive performance on textonly benchmarks matching models like Mixtral 8x7B and
GeminiPro on commonsense reasoning and reading comprehension tasks Section 51 But perhaps most
impressively Chameleon unlocks entirely new capabilities in terms of mixedmodal reasoning and generation
As using only static public benchmarks to evaluate model performance could be limited Schaeffer 2023 we
also conduct a carefully designed human evaluation experiment by measuring the quality of mixedmodal
long form responses to openended prompts Chameleon34B substantially outperforms strong baselines like
GeminiPro and GPT4V Section 4 achieving a 604 preference rate against GeminiPro and a 516
preference rate against GPT4V in pairwise comparisons
In summary we present the following contributions
We present Chameleon  a family of earlyfusion tokenbased mixedmodal models capable of reasoning
over and generating interleaved imagetext documents setting a new bar for open multimodal foundation
models
We introduce architectural innovations and training techniques that enable the stable and scalable
training of earlyfusion tokenbased models addressing key challenges in mixedmodal learning
Through extensive evaluations we demonstrate stateoftheart performance across a diverse set of
visionlanguage benchmarks while maintaining competitive performance on textonly tasks and high
quality image generation all in the same model
We conduct the first largescale human evaluation on openended mixedmodal reasoning and generation
demonstrating the unique capabilities of Chameleon in this new setting
Chameleon represents a significant step towards realizing the vision of unified foundation models capable of
flexibly reasoning over and generating multimodal content
2Prompt Im bored Could you show me some cool quirkylooking birds Also tell me about them in 23
sentences
Response Absolutely Here are three birds with some unique features that
you might find interesting
KeelBilled Toucan The KeelBilled Toucan is a brightly colored bird
native to South American rainforests and woodlands Its most distinctive
feature is its colorful beak which can grow up to seven inches long and has a
distinctive curved shape The beak is used for plucking fruit from trees and
for social displays Despite its large size the KeelBilled Toucan is a skilled
flyer and can often be seen flitting through the treetops Here is an image of
this vibrant bird img
PuffinPuffins are small aquatic birds that are found in the northern hemisphere
They have distinctive black and white feathers as well as a brightly colored
beak that is used for attracting mates and defending their territory Puffins
are known for their distinctive puffin call which sounds like a loud guttural
chuckle They feed on fish which they catch by diving into the water from the
air Puffins are colonial birds and they breed in large colonies on rocky cliffs
and islands Here is an image of this adorable bird img
Golden Pheasant The Golden Pheasant is a brightly colored bird native to the
forests of China It is a member of the pheasant family and is known for its
striking plumage which features a goldenyellow body red face and green tail
feathers The Golden Pheasant is a grounddwelling bird and it feeds on a diet
of insects seeds and fruits It is a popular game bird and is often hunted for
its meat and feathers In captivity the Golden Pheasant is sometimes kept as
an ornamental bird due to its striking appearance Here is an image of this
beautiful bird img
I hope these images and descriptions have helped to brighten your day If you have any other questions
or requests please dont hesitate to ask
Figure 2 Sample interleaved image and text generation from Chameleon The corresponding images are generated in
locations marked by img
32 PreTraining
Chameleon represents images in addition to text as a series of discrete tokens and takes advantage of the
scaling properties of autoregressive Transformers Ramesh et al 2021 Aghajanyan et al 2022 2023 Yu
et al 2023 We present any ordering of images and text during training ranging from textonly to single
textimage pairs to full interleaved textimage documents
Prompt img While chameleons are masters of camouflage in the natural
world lets see how they might fare in a manmade setting
1 Analyze the colors patterns and textures in this image and identify the
most challenging spot for a chameleon to blend in
2 Then generate an image of a chameleon with the optimal pattern to
camouflage itself
3 Then rate and explain how successful each attempt is on a scale from 110
Response The image is of a stop sign in a forested area with some sunlight filtering through the trees
The sign has a red octagonal shape with a white border and the word STOP written in white letters
In the background there are green leaves and trees
Challenging Spot for a Chameleon The stop sign would be the most challenging spot for a chameleon
to blend in due to its bright red color and bold white letters which would stand out in the forested
environment
Optimal Pattern for Camouflage To camouflage itself on the stop sign a chameleon would need
to have a pattern that would break up its outline and make it less conspicuous A pattern with shades
of brown and red would help the chameleon blend in more effectively
Additionally a mottled or dappled pattern would help to scatter
light and further reduce the chameleons visibility
img
Rating and Explanation A rating of 210 would be given for
the chameleons ability to blend in on the stop sign While the
chameleons green and brown pattern helps to break up its outline
the bright red color of the stop sign will still make it stand out in
the forested environment
Figure 3 Sample generation from Chameleon
21 Tokenization
Image Tokenization We train a new image tokenizer based on Gafni et al 2022 which encodes a 512512
image into 1024discrete tokens from a codebook of size 8192 For training this tokenizer we use only licensed
images Given the importance of generating human faces we upsample the percentage of images with faces
during pretraining by 2 times A core weakness of our tokenizer is in reconstructing images with a large
amount of text therefore upper bounding the capability of our models when it comes to heavy OCRrelated
tasks
Tokenizer We train a new BPE tokenizer Sennrich et al 2016 over a subset of the training data outlined
belowwithavocabularysizeof65536 whichincludesthe8192imagecodebooktokens usingthe sentencepiece
library Kudo and Richardson 2018
4Prompt What can I bake with this Give me the full recipe and show me an
image with the final dish img
Response Bananas are a versatile fruit that can be used in various baked
goods such as muffins bread cakes and cookies They add natural sweetness
moisture and a subtle banana flavor to the finished product Additionally
bananas are rich in potassium fiber and other nutrients making them a
healthy addition to your baking repertoire
Here is a simple recipe for banana bread that uses 3 ripe bananas
Ingredients
3 ripe bananas mashed
1 12 cups allpurpose flour
1 tsp baking powder
12 tsp baking soda
12 tsp salt
14 cup unsalted butter softened
1 cup granulated sugar
2 large eggs
1 tsp vanilla extract
12 cup chopped walnuts optional
Instructions
Preheat your oven to 350 F 180 C Grease a 9x5inch loaf pan and set it aside
In a medium bowl whisk together the flour baking powder baking soda and salt Set aside
In a large mixing bowl use an electric mixer to cream together the butter and sugar until light and
fluffy about 23 minutes
Beat in the eggs one at a time followed by the mashed bananas and vanilla extract
Gradually mix in the dry ingredients until just combined being careful
not to overmix
If using walnuts fold them into the batter
Pour the batter into the prepared loaf pan and smooth the top
Bake for 5060 minutes or until a toothpick inserted into the center of
the bread comes out clean
Remove the bread from the oven and let it cool in the pan for 10 minutes
before transferring it to a wire rack to cool completely img
Figure 4 Sample Chameleon outputs
22 PreTraining Data
We delineate the pretraining stage into two separate stages The first stage takes up the first 80 of training
while the second stage takes the last 20 For all TextToImage pairs we rotate so that 50 of the time the
image comes before the text ie captioning
221 First Stage
In the first stage we use a data mixture consisting of the following very large scale completely unsupervised
datasets
TextOnly We use a variety of textual datasets including a combination of the pretraining data used to
train LLaMa2 Touvron et al 2023 and CodeLLaMa Roziere et al 2023 for a total of 29 trillion textonly
tokens
50k 5k 10k 15k 20k 25k 30k
Step0050100150200250300350Output Normw QKnorm and dropout
wo dropout
wo QKnorm or dropoutaUncontrolled growth of output
norms is a strong indicator of future
training divergence
0k 25k 50k 75k 100k 125k 150k 175k
Step34353536363737Training Losswo QKnorm
w QKnormbAn ablation with Chameleon 7B
with and without QKNorm 
0k 20k 40k 60k 80k
Step343536373839Training Losswo dropout
w dropoutcAn ablation with Chameleon 7B
with and without dropout
Figure 5 Output norm and training loss curves for Chameleon models under various settings
TextImage The textimage data for pretraining is a combination of publicly available data sources and
licensed data The images are then resized and center cropped into 512512images for tokenization In
total we include 14 billion textimage pairs which produces 15 trillion textimage tokens
TextImage Interleaved We procure data from publicly available web sources not including data from Metas
products or services for a total of 400 billion tokens of interleaved text and image data similar to Laurençon
et al 2023 We apply the same filtering for images as was applied in TextToImage 
222 Second Stage
In the second stage we lower the weight of the first stage data by 50 and mix in higher quality datasets
while maintaining a similar proportion of image text tokens
We additionally include a filtered subset of the train sets from a large collection of instruction tuning sets
23 Stability
It was challenging to maintain stable training when scaling the Chameleon models above 8B parameters
and 1T tokens with instabilities often only arising very late in training We adopted to following recipe for
architecture and optimization to achieve stability
Architecture Our architecture largely follows LLaMa2 Touvron et al 2023 For normalization we continue
to use RMSNorm Zhang and Sennrich 2019 we use the SwiGLU Shazeer 2020 activation function and
rotary positional embeddings RoPE Su et al 2021
We found that the standard LLaMa architecture showed complex divergences due to slow norm growth in the
midtolate stages of training We narrowed down the cause of the divergence to the softmax operation being
problematic when training with multiple modalities of significantly varying entropy due to the translation
invariant property of softmax ie softmax z softmax zc Because we share all weights of the model
across modalities each modality will try to compete with the other by increasing its norms slightly while
not problematic at the beginning of training it manifests in divergences once we get outside the effective
representation range of bf16 In Figure 6b we show that ablations without image generation did not diverge
In a unimodal setting this problem has also been named the logit drift problem Wortsman et al 2023 In
Figure 5a we plot the norms of the output of the last transformer layer as training progresses and we find
that although training divergences can manifest after as much as even 2030 of training progress monitoring
uncontrolled growth of output norms is strongly correlated with predicting future loss divergence
The softmax operation appears in two places in transformers the core attention mechanism and the softmax
over the logits As inspired by Dehghani et al 2023 and Wortsman et al 2023 we first deviate from
the Llama architecture by using querykey normalization QKNorm QKNorm directly controls the norm
growth of input to the softmax by applying layer norm to the query and key vectors within the attention
60k 100k 200k 300k 400k 500k 600k
Step2829303132Training Loss7b
34baTraining Curves for 600k steps for
Chameleon7B and Chameleon34B
over MixedModal Data
0k 50k 100k 150k 200k 250k
Step095100105110115Training Loss7B wo image generationbTraining loss curve with image gen
eration disabled does not suffer from
instability issues
0k 2k 4k 6k 8k 10k
Step354045505560Training Losswo norm reordering
w norm reorderingcFor Chameleon34B  using
dropout does not fix divergences
both with and without norm
reordering
Figure 6 Training loss curves for Chameleon models under various settings
In Figure 5b we show training loss curves for Chameleon7B with and without QKNorm and the latter
diverges after approximately 20 of a training epoch
We found that to stabilize Chameleon7B by controlling norm growth it was necessary to introduce dropout
after the attention and feedforward layers in addition to QKnorm see Figure 5c However this recipe was
not enough to stabilitize Chameleon34B  which required an additional reordering of the norms Specifically
we use the strategy of normalization proposed in Liu et al 2021 within the transformer block The benefit
of the Swin transformer normalization strategy is that it bounds the norm growth of the feedforward block
which can become additionally problematic given the multiplicate nature of the SwiGLU activation function
Ifhrepresents the hidden vector at timestep tafter selfattention is applied to input x
Chameleon34B hxattention_norm attention x
output hffn_norm feed_forward h
Llama2 hxattention attention_norm x
output hfeed_forward ffn_norm h
There was no difference in perplexity when training a model from scratch with and without the normalization
reordering until the divergence of the LLaMa2 parameterization Additionally we found that this type of
normalization did not work well in combination with dropout and therefore we train Chameleon34B without
dropout Figure 6c Furthermore we retroactively found that Chameleon7B can also be stably trained
without dropout when using normreordering but QKnorm is essential in both cases We plot training
curves for the first 600k steps for both Chameleon7B andChameleon34B in Figure 6a
Optimization Our training process uses the AdamW optimizer Loshchilov and Hutter 2017 with β1set
to 09 and β2to 095 with an ϵ 105 We use a linear warmup of 4000 steps with an exponential decay
schedule of the learning rate to 0 Additionally we apply a weight decay of 01 and global gradient clipping at
a threshold of 10 We use a dropout of 01 Srivastava et al 2014 for Chameleon7B for training stability
but not for Chameleon34B see Figure 5c and 6c
The application of QKNorm while helping the inner softmaxes within the Transformer does not solve the
problem of logit shift in the final softmax Following Chowdhery et al 2022 Wortsman et al 2023
we apply zlossregularization Specifically we regularize the partition function Zof the softmax function
σxiexi
Zwhere ZP
iexiby adding 105log2Zto our loss function
ForChameleon 7B it was important to use both dropout and zlossto achieve stability while Chameleon 34B
only required zlossFigure 6c
Chameleon7B was trained with a global batch size of 2238M tokens and Chameleon34B was trained
with a global batch size of 322212M tokens We do 21 epochs over our full training dataset for a total
7of 92 trillion tokens seen during training We show the first 600k steps of training 55 for Chameleon7B
and 80 for Chameleon34B  in Figure 6a
Table 1Summary of core architecture and optimization decisions made in Chameleon in contrast to LLaMa1 and
LLaMa2
Model Params Context Length GQA Tokens LR Epochs Dropout Zloss Qknorm
LLaMa1 7B 2k 10T 3010410 00 00 
33B 2k 14T 1510410 00 00 
LLaMa2 7B 4k 20T 3010410 00 00 
34B 4k 20T 1510410 00 00 
Chameleon 7B 4k 44T 1010421 01 105
34B 4k 44T 1010421 00 105
PreTraining Hardware Our model pretraining was conducted on Metas Research Super Cluster RSC Lee
and Sengupta 2022 and our alignment was done on other internal research clusters NVIDIA A100 80
GB GPUs power both environments The primary distinction is the interconnect technology RSC employs
NVIDIA Quantum InfiniBand whereas our research cluster utilizes Elastic Fabric We report our GPU usage
for pretraining in Table 2
Table 2 Chameleon Model PreTraining Resource Usage
Chameleon Concurrent GPUs GPU Hours
7B 1024 856481
34B 3072 4282407
24 Inference
To support alignment and evaluation both automated and human and to demonstrate the application
readiness of our approach we augment the inference strategy with respect to interleaved generation to improve
throughput and reduce latency
Autoregressive mixedmodal generation introduces unique performancerelated challenges at inference time
These include
Datadependencies perstep  given that our decoding formulation changes depending on whether the
model is generating images or text at a particular step tokens must be inspected at each step ie
copied from the GPU to the CPU in a blocking fashion to guide control flow
Masking for modalityconstrained generation  to facilitate exclusive generation for a particular modality
eg imageonly generation tokens that do not fall in a particular modality space must be masked and
ignored when detokenizing
Fixedsized text units  unlike textonly generation which is inherently variablelength tokenbased
image generation produces fixedsize blocks of tokens corresponding to an image
Given these unique challenges we built a standalone inference pipeline based on PyTorch Paszke et al 2019
supported with GPU kernels from xformers Lefaudeux et al 2022
Our inference implementation supports streaming for both text and images When generating in a streaming
fashion tokendependent conditional logic is needed at each generation step Without streaming however
blocks of image tokens can be generated in a fused fashion without conditional computation In all cases
token masking removes branching on the GPU Even in the nonstreaming setting however while generating
text each output token must be inspected for imagestart tokens to condition imagespecific decoding
augmentations
8Table 3Supervised FineTuning Dataset Statistics
Category  of Samples  of Tokens  of Images
ChameleonSFTText 16M 9400M 
Code 141K 11M 
Visual Chat 156K 194M 167K
Image Generation 643K 680M 643K
Interleaved Generation 169K 358M 307K
Safety 953K 386M 16K
3 Alignment
We follow recent work in using a light weight alignment stage based on supervised fine tuning on carefully
curated high quality datasets Zhou et al 2023 We include a range of different types of data targeting
both exposing model capabilities and improving safety
31 Data
We separate our supervised finetuning SFT dataset into the following categories TextCodeVisual
ChatImage Generation Interleaved TextImage Generation  and Safety We include examples from each
category from the ChameleonSFT dataset in Figure 7
We inherit the TextSFT dataset from LLaMa2 Touvron et al 2023 and the CodeSFT from CodeLLaMa
Roziere et al 2023 For the Image Generation SFT dataset we curate highly aesthetic images by applying
and filtering each image in our licensed data with an aesthetic classifier from Schuhmann et al 2022 We
first select images rated as at least six from the aesthetic classifier and then select the top 64K images closest
in size and aspect ratio to 512512the native resolution of our image tokenizer
For both Visual Chat andInterleaved TextImage Generation SFT data we focused on very highquality
data collection using thirdparty vendors following a similar strategy recommended by Touvron et al 2023
Zhou et al 2023 We do not include any Meta user data We present our datasets statistics in Table 3
Safety Data We include a collection of prompts that can potentially provoke the model to produce unsafe
content and match them with a refusal response eg I cant help with that These prompts cover a
wide variety of sensitive topics such as violence controlled substances privacy and sexual content Our
collection of safety tuning data includes examples from LLaMa2Chat Touvron et al 2023 synthetic
textbased examples generated with Rainbow Teaming Samvelyan et al 2024 image generation prompts
manually selected from PickAPic Kirstain et al 2023 for safety testing examples for cyber security
safety Roziere et al 2023 as well as mixedmodal prompts collected internally through manual annotation
and automatic expansion Honovich et al 2022 Collecting mixedmodal prompts is of particular importance
since it addresses potential multimodal attack vectors which are outside the distribution of textonly and
texttoimage safety tuning datasets
32 FineTuning Strategy
Data Balancing We found that balancing modalities within the SFT stage is important for high quality
alignment Specifically during the SFT stage if there is a severe imbalance between pairings of modalities or
when a specific modality should trigger the model learns an unconditional prior of generating that modality
which can either mute or over exaggerate the generation of a single modality
Optimization Our supervised finetuning strategy incorporates a cosine learning rate schedule starting at an
initial rate of 1e5 combined with a weight decay set at 01 We maintain a batch size of 128 accommodating
sequences up to 4096 tokens During finetuning each dataset instance comprises a paired prompt and its
corresponding answer To improve efficiency we pack as many prompts and answers as possible into each
9Prompt Can you write a poem using
rhyming couplets
Response Sure heres a short poem
using rhyming couplets
The sun sets in the sky
A beautiful sight to the eye
The stars come out to play
A magical sight to behold
The wind whispers through the
trees
A gentle breeze that brings ease
The birds sing their sweet melodies
A soothing sound that calms the soul
The world is full of wonder
A place of beauty and thunder
So let us cherish every moment
And bask in the beauty all around us
aTextPrompt Given a list of integers
find the first number that ap
pears at least twice
Response
def find_duplicatemy_list
seen  set
for num in my_list
if num in seen
return num
seenaddnum
return None
bCodePrompt Renaissance square of
Stepanakert
Response
cImage Generation
Prompt Which elements contribute to this flower
arrangements distinctiveness and what sets its sur
roundings apart
Response The image features an autumnalthemed
floral arrangement in a classically designed vase
The arrangement stands out with its striking com
bination of red maple leaves purple flowers and
pinkred roses The vases rosettes give it an antique
design In the background blurry green vegetation
is visible creating a contrast to the autumnal theme
of the bouquet The red maple leaves both in the
arrangement and scattered around the vase evoke
autumn while the green vegetation suggests a more
summerlike setting The outdoor setting is bright
and sunny adding to the contrast between the ar
rangement and its settings
dVisual ChatGyumri is an urban municipal community and the
secondlargest city in Armenia serving as the ad
ministrative center of Shirak Province in the north
western part of the country
Archaeological excavations conducted throughout
the Soviet period have shown that the area of
modernday Gyumri has been populated since at
least the third millennium BC
eInterleaved TextImage Generation
Figure 7 Example alignment data for different categories
sequence inserting a distinct token to delineate the end of a prompt and the beginning of an answer We
use an autoregressive training objective selectively masking the loss for the prompt tokens This targeted
approach allows us to optimize the model exclusively based on the answer tokens which provides slight gains
overall We also apply a dropout of 005 Additionally we maintain the same zloss that was used during
10Advice 102 
Howto 125 
How do I properly clean my TV 
screen I used Windex and now 
there are towel fibers and wipe 
marks all over Show me some 
reference photos 
Explanation 144 
Ive been studying 
classical French art 
and my favorite 
so far is his painting 
seen here img  
Could you please 
give me a few images of other 
contemporary artworks that have 
this same aesthetic 
Hypothetical 56 
What would the modernday 
vehicle look like if oil had never 
been discovered Brainstorming 186 
Show me a Middle Eastern alternative to 
these dishes img1 img2 
Reasoning 21 Identification 93  
Is the below image a 
Shetland Pony If 
not what is it and 
can you show me a 
Shetland Pony 
img 
Article 31 
Write me an introduction to a story about 
knickknacks and finish the story by 
shifting the focus with an image Report 54 
Who designed the church in the image below 
and whats the name of the 
Church img Can you 
please provide me with 
additional photos of famous 
landmarks designed 
by the same architect Story 39 
Can you create and illustrate a short story 
for children about an octopus that cant 
stop eating pizza 
Other 52 
Create a decal for my truck that features 
running horses as well as the TRD insignia Use 
black to gray gradients Comparison 96 
Please tell me what the difference between 
these two creatures is and show me some 
more examples img1 img2 
What does a meningitis rash look 
like What are the other 
symptoms I should be on the 
lookout for 
What is typically found at a construction site 
Show me a construction site that has a crane 
Figure 8 Task categories and examples of prompts Image attributions Seguin 2010 Agriflanders 2009 Tuszyński
2015 Sokolov 2022
pretraining During supervised finetuning images in the prompt are resized with border padding to ensure
that all the information is available in the image whereas images in the answer are centercropped to ensure
visually good image generation quality
4 Human Evaluations and Safety Testing
Chameleon has significant new mixed modal understanding and generation abilities that cannot be measured
with existing benchmarks In this section we detail how we conduct human evaluations on large multimodal
language models responses to a set of diverse prompts that regular users may ask daily We first introduce
how we collect the prompts and then describe our baselines and evaluation methods along with the evaluation
results and analysis A safety study is also included in this section
41 Prompts for Evaluation
We work with a thirdparty crowdsourcing vendor to collect a set of diverse and natural prompts from human
annotators Specifically we ask annotators to creatively think about what they want a multimodal model
to generate for different reallife scenarios For example for the scenario of imagine you are in a kitchen
annotators may come up with prompts like How to cook pasta or How should I design the layout of my
island Show me some examples The prompts can be textonly or text with some images and the expected
responses should be mixedmodal containing both text and images
After collecting an initial set of prompts we ask three random annotators to evaluate whether the prompts
are clear and whether they expect the responses to contain images We use a majority vote to filter unclear
prompts and prompts that dont expect mixedmodal responses In the end our final evaluation set contains
1048 prompts 441 421 are mixedmodal ie containing both text and images and the remaining 607
579 are textonly
To better understand the tasks users would like a multimodal AI system to fulfill we manually examine
11Fulfills Partially fulfills Does not fulfill
T ask Fulfillment Rate010203040506070Percentage Model
Chameleon
Gemini
GPT4V
Gemini
GPT4VaThe prompt task fulfillment rates
0 20 40 60 80 100
Percent GPT4VGeminiGPT4VGemini
460535358415
314312316345
226153326240Wins Ties Loses bChameleon vs the baselines Gemini GPT4V
Gemini GPT4V
Figure 9 Performance of Chameleon vs baselines on mixedmodal understanding and generation on a set of diverse
and natural prompts from human annotators
the prompts and classify them into 12 categories The description of these task categories1 as well as their
example prompts can be found in Figure 8
42 Baselines and Evaluations
We compare Chameleon 34B with OpenAI GPT4V and Google Gemini Pro by calling their APIs While these
models can take mixedmodal prompts as input their responses are textonly We create additional baselines
by augmenting GPT4V and Gemini responses with images to have even stronger baselines Specifically we
instruct these models to generate image captions by adding the following sentence at the end of each original
input prompt If the question requires an image to be generated then generate an image caption instead
and enclose the caption in a pair of caption  caption tags We then use OpenAI DALLE 3 to generate
images conditioned on these captions and replace the captions in the original responses with those generated
images We denote the enhanced responses as GPT4V and Gemini in this section Working with the same
thirdparty crowdsourcing vendor we conduct two types of evaluations to measure the model performance
absolute andrelative
421 Absolute Evaluation
For absolute evaluations the output of each model is judged separately by asking three different annotators
a set of questions regarding the relevance and quality of the responses Below we give detailed results and
analysis on the most critical question whether the response fulfills the task described in the prompt 
On task fulfillment we ask annotators whether the response fulfillspartially fulfills  ordoes not fulfill the
task described in the prompt As shown in Figure 9a much more of Chameleon s responses are considered
to have completely fulfilled the tasks 552 for Chameleon vs 376 of Gemini and 447 of GPT4V
When judging the original responses of Gemini and GPT4V the annotators consider much fewer prompts to
be fully fulfilled Gemini completely fulfills 176 of the tasks and GPT4V 231 We suspect that because
all the prompts expect mixedmodal output the textonly responses from Gemini and GPT4V might be
viewed as only partially completing the tasks by the annotators
The task fulfillment rates in each category and in each input modality can be found in Appendix B The
task categories that Chameleon performs well include Brainstorming Comparison  andHypothetical  and the
1While not instructed specifically certain image understanding tasks that require identifying the text in an image such as
OCR Optical character recognition do not appear in our evaluation set of prompts
12categories Chameleon needs to improve include Identification andReasoning  On the other hand we dont
see that the model performance differs a lot when comparing mixedmodality and textonly prompts although
Chameleon seems to perform slightly better on textonly prompts while Gemini and GPT4V are slightly
better on mixedmodal ones Figure 2 shows an example of Chameleon s response to a brainstorming prompt
422 Relative Evaluation
For relative evaluations we directly compare Chameleon with each baseline model by presenting their
responses to the same prompt in random order and asking human annotators which response they prefer The
options include the firstresponse the secondresponse and about the same  Figure 9b shows Chameleon s
win rates2over the baselines Compared with Gemini Chameleon s responses are better in 415 of the
cases 345 are tie and 240 are inferior Annotators also think that Chameleon s responses are slightly
more often better than GPT4V with 358 win 316 tie and 326 loss Overall Chameleon has win
rates of 604 and 516 over Gemini and GPT4V respectively When compared with the original
responses from Gemini without the augmented images Chameleon s responses are considered better in 535
of the cases 312 are tied and 153 are inferior Chameleon s responses are also considered better than
GPT4V more frequently with 460 win 314 tie and 226 loss Chameleon s win rates over Gemini
and GPT4V are 691 and 617 respectively
43 Interannotator Agreement
Every question in our evaluation is answered by three different human annotators and we take the majority
votes as the final answer To understand the quality of the human annotators and whether the questions we
asked are reasonably designed we examine the level of agreement between different annotators
The levels of agreement on each question in the absolute evaluation are shown in Figure 10
0
5001000 1500 2000 2500 3000
CountContaining images
Image quality
Image relevance
Language quality
Objectionable content
Relevance
T ask fulfillment
AccuracyAgreement
All
T wo
None
Figure 10 The interannotator agreement on the questions in the absolute evaluation
For questions about simple objective properties of the responses we very rarely see three annotators disagree
with each other For example annotators have unanimous judgments on whether the model responses contain
objectionable content eg hate speech in this case all models produce safe responses For some questions
such as whether the response fulfills the task or whether the model interprets the prompt correctly when one
2The win rate is calculated by adding 1 point for a win and 05 points for a tie
13annotators judgment differs from the other twos the decision is usually still close eg fulfillsvspartially
fulfills rather than opposite eg fulfillsvsdoes not fulfill 3
Table 4The interannotator agreement on relative evaluations
All 3 annotators agree 2 of 3 annotators agree No Agreement
Chameleon vs Gemini 331 315 609 581 108 103
Chameleon vs GPT4V 371 354 579 552 98 93
Chameleon vs Gemini 317 302 621 593 110 105
Chameleon vs GPT4V 300 286 611 583 137 131
For the relative evaluation Table 4 shows the numbers of cases where all three annotators agree two annotators
agree and there is no agreement For each model pair we have a bit higher than 10 of the cases where there
is no agreement among the three annotators considered as a tie in our evaluation On about 28 to 35 of
the pairs all annotators have unanimous judgments and in about 55 to 60 of the pairs one annotator
differs from other two This may be interpreted as Chameleon performing similarly to other baselines in
many cases making the relative evaluation challenging4
44 Safety Testing
We crowdsource prompts that provoke the model to create unsafe content in predefined categories such as
selfharm violence and hate and criminal planning These prompts cover both text and mixedmodal inputs
as well as intents to produce unsafe text images or mixedmodal outputs We generate the models response
to each prompt and ask annotators to label whether the response is safeorunsafewith respect to each
categorys definition of safety an unsureoption is also provided for borderline responses Table 5 shows
that an overwhelming majority of Chameleon s responses are considered safe with only 78 039 unsafe
responses for the 7B model and 19 0095 for the 30B model
We also evaluate the models ability to withstand adversarial prompting in an interactive session For
that purpose an internal red team probed the 30B model over 445 promptresponse interactions including
multiturn interactions Table 5 shows that of those responses 7 16 were considered unsafe and 20 45
were labeled as unsure While further safety tuning using RLHFRLAIF has been shown to further harden
the model against jailbreaking and intentional malicious attacks these results demonstrate that our current
safety tuning approach provides significant protection for reasonable benign usage of this research artifact
45 Discussion
Compared to Gemini and GPT4V Chameleon is very competitive when handling prompts that expect
interleaving mixedmodal responses The images generated by Chameleon are usually relevant to the context
making the documents with interleaving text and images very appealing to users However readers should
be aware of the limitations of human evaluation First the prompts used in the evaluation came from
crowdsourcing instead of real users who interact with a model While we certainly have a diverse set of
prompts the coverage may still be limited given the size of the dataset Second partially because our
prompts focus on the mixedmodal output certain visual understanding tasks such as OCR or Infographics
ie interpreting a given chart or plot are naturally excluded from our evaluation Finally at this moment
the APIs of existing multimodal LLMs provide only textual responses While we strengthen the baselines by
augmenting their output with separately generated images it is still preferred if we can compare Chameleon
to other native mixedmodal models
3For the question of task fulfillment the interrater reliability derived by Krippendorffs Alpha Krippendorff 2018 Marzi
et al 2024 is 0338 the 95 confidence interval is 0319 0356 based on bootstrap sampling of 1000 iterations
4When comparing Chameleon with Gemini and GPT4V the Krippendorffs Alpha values are 0337 0 293 0378and
0396 0 353 0435 respectively
14Table 5Safety testing on 20000 crowdsourced prompts and 445 red team interactions provoking the model to produce
unsafe content
Dataset Params Safe Unsafe Unsure
Crowdsourced7B 992 04 04
34B 997 01 02
Red Team 34B 939 16 45
5 Benchmark Evaluations
Given the general capabilities of Chameleon  there is not a single model that we can directly evaluate against
therefore we evaluate against the best models in every category within our capabilities
51 Text
We evaluate the general textonly capabilities of our pretrained not SFTd model against other stateofthe
art textonly large language models We follow the evaluation protocol outlined by Touvron et al 2023
Specifically we evaluate all models using an inhouse evaluation platform on the areas of commonsense
reasoning reading comprehension math problems and world knowledge We report our results in Table 6
Table 6Comparison of overall performance on collective academic benchmarks against opensource foundational models
Evaluated using our frameworkusing API For GSM8kMATH we report maj1 unless mentioned otherwise
From Gemini et al 2023
Chameleon Llama2 Mistral Gemini
ProGPT
4
7B 34B 7B 34B 70B 7B 8x7B  
Commonsense Reasoning and Reading Comprehension
PIQA 796 833 788 819 828 830 836  
SIQA 570 633 483 509 507    
HellaSwag 742 827 772 833 853 813 844  
756
10shot851
10shot  871
10shot839
10shot867
10shot847
10shot953
10shot
WinoGrande 704 785 692 767 802 753 772  
ArcE 761 841 752 794 802 800 831  
ArcC 465 597 459 545 574 555 597  
OBQA 510 540 586 582 602    
BoolQ 814 860 774 837 850 847  
Math and World Knowledge
GSM8k 416 614 146 422 568 521
maj8744
maj8865
maj32
CoT920
SFT
CoT509
maj8770
maj32    751
maj32 
MATH 115
maj1225
maj125 624 135 131
maj4284
maj4326 529
129
maj4247
maj4      
MMLU 521 658 453 626 689 601 706 718 864
Commonsense Reasoning and Reading Comprehension We report 0shot performance on the following
benchmarks that measure commonsense reasoning and reading comprehension capabilities PIQABisk
et al 2020 SIQASap et al 2019 HellaSwag Zellers et al 2019 WinoGrande Sakaguchi et al
2021 ARCEasy Clark et al 2018 ARCChallenge Clark et al 2018 OpenBookQA Mihaylov
et al 2018 and BoolQClark et al 2019 We score the prompt with each candidate answer and
compute accuracy using the candidate with the highest score All baseline model performances except a
few are taken directly from the reported sources We observe that Chameleon7B andChameleon34B
15are competitive with the corresponding Llama2 models with Chameleon34B even outperforming
Llama2 70B on 58tasks and performing on par with Mixtral 8x7B
MATH and World Knowledge We report 8shot performance on GSM8K Cobbe et al 2021 ie grade
school math word problems and 4shot performance on the MATHHendrycks et al 2021 benchmark
We report majN exact match accuracy for both benchmarks by sampling N generations from the model
greedy sampling for N1 and choosing the answer via majority voting Despite training for additional
modalities both Chameleon models demonstrate strong math capabilities On GSM8kChameleon7B
outperforms the corresponding Llama2 models with performance comparable to Mistral 7B 509 vs
521 maj8 Furthermore Chameleon34B can outperform Llama270B on maj1 614 vs 568 and
Mixtral 8x7B on maj32 770 vs 751 Similarly on MATH Chameleon7B outperforms Llama2
and matches Mistral 7B on maj4 while Chameleon34B outperforms Llama270B approaching the
performance of Mixtral 8x7B on maj4 247 vs 284
We also report performance on MMLU Hendrycks et al 2020 which measures worldindomain
knowledge and problemsolving abilities using 57 subjects including elementary mathematics US
history computer science and law Both Chameleon models outperform their Llama2 counterparts
with Chameleon34B approaching the performance of Mixtral 8x7BGeminiPro 658 vs 706718
Overall Chameleon outperformsLLaMa2acrosstheboard withperformanceapproachingMistral7BMixtral
8x7B Jiang et al 2023 2024 on some tasks These gains are likely due to multiple factors First we do
two epochs over the LLaMa2 pretraining data and in general use more compute for pretraining Second
including code data significantly improves performance on textonly reasoning tasks Lastly having higher
quality data in the last 20 of pretraining significantly improves performance
52 ImageToText
We next evaluate Chameleon on the segment of tasks that requires text generation conditioned on an image
specifically on image captioning and visual questionanswering tasks and present results of Chameleon34B
in Table 7 Together with our pretrained model we also present results with a model finetuned on all tasks
together  Chameleon34B MultiTask as well as models exclusively finetuned for the specific evaluation
tasks  Chameleon34B SFT
We evaluate against available opensource latefusion models specifically Flamingo 80B Alayrac et al 2022
IDEFICS 80B Laurençon et al 2023 and Llava15 Liu et al 2023a as well as recent closedsource
models such as Gemini Gemini et al 2023 and GPT4V OpenAI 2023 We note that we did not take
any special care when formatting the pretraining data to ensure that 0shot inference can be effectively done
Therefore we augment the input images or questions with the published prompts used by other models This
was purposefully done to maintain the fidelity of the pretraining data
Image Captioning For image captioning evaluations we report CiDER Vedantam et al 2015 scores
on the Karpathy test split of MSCOCO Lin et al 2014 and the Karpathy test split of Flickr30k
Plummer et al 2015 using the pycocoevalcap Chen et al 2020 package For Chameleon models
we restrict captions to 30tokens We evaluated GPT4V and Gemini models using several prompts and
generation lengths via their APIs and report the best performance that we were able to achieve
In the opensource pretrained category Chameleon34B 2shot outperforms the larger 80B models of
both Flamingo and IDEFICS on COCO with 32shots while matching their performance on Flickr30k
With respect to finetunedclosedsource models both multitask and SFT variants of Chameleon34B
outperform all other models on COCO while for Flickr30k the SFT model outperforms other models
with the multitask model being a close competitor
Visual Question Answering For visual question answering VQA we report performance on the test
dev split of VQAv2 Goyal et al 2017 For VQAv2 the pretrained Chameleon34B model with
2shots matches the 32shot performance of the larger Flamingo and IDEFICS models while for fine
tunedclosed models Chameleon34B Multitask approaches the performance of IDEFICS80BInstruct
and Gemini Pro but trails larger models such as Flamingo80BFT GPT4V and Gemini Ultra
Llava15 outperforms Chameleon34B on VQAv2 potentially owing to its additional finetuning on
16Table 7Model Performances on ImagetoText CapabilitiesEvaluated using API
Model Model Size COCO Flickr30k VQAv2
PretrainedFlamingo80B 80B 1138
32shot751
4shot676
32shot
IDEFICS80B 80B 1166
32shot737
4shot659
32shot
ChameleonChameleon 34B 1202
2shot747
2shot660
2shot
ChameleonSFT 34B 1408
0shot823
2shot
ChameleonMultiTask 34B 1391
2shot762
2shot696
FinetunedFlamingo80BFT 80B 1381  820
IDEFICS80BInstruct 80B 1232
32shot784
32shot688
32shot
Closed Source
finetuning
status unknownGPT4V  785
8shot553
8shot772
Gemini Nano 2    675
Gemini Pro  998
2shot822
4shot712
Gemini Ultra    778
conversations from GPT4 ShareGPT ShareGPT 2023 GQA Hudson and Manning 2019 and
regionlevel VQA datasets but significantly trails behind on the other tasks
In general we find Chameleon is fairly competitive on both image captioning and VQA tasks It rivals other
models by using much fewer incontext training examples and with smaller model sizes in both pretrained
and finetuned model evaluations
6 Related Work
Chameleon builds upon the lineage of works exploring tokenbased approaches for multimodal learning The
idea of using discrete tokens to represent continuous modalities like images was first explored in works like
BEiT Bao et al 2021 which proposed a selfsupervised vision representation learning method based on
tokenized image patches Aghajanyan et al 2022 extended this idea to learning from mixedmodal documents
through interleaved image and text tokens allowing for joint reasoning over both modalities within a unified
architecture CM3Leon Yu et al 2023 further scaled up this approach to autoregressive texttoimage
generation building on the initial proposal of tokenbased image generation in DALLE Ramesh et al 2021
As a fully tokenbased earlyfusion model Chameleon differs from latefusion approaches like Flamingo
Alayrac et al 2022 which encode images and text separately before combining them at a later stage
Other models like LLaVA Liu et al 2023a IDEFICS Laurençon et al 2023 and VisualGPT Chen
et al 2022 also maintain separate image and text encoders In contrast Chameleons unified token space
allows it to seamlessly reason over and generate interleaved image and text sequences without the need for
modalityspecific components This earlyfusion approach however comes with significant challenges in terms
of representation learning and alignment as discussed in Baltrušaitis et al 2018
The most similar model to Chameleon is Gemini Gemini et al 2023 which also uses an earlyfusion
tokenbased approach However a key difference is that Gemini uses separate image decoders whereas
Chameleon is an endtoend dense model without any routing components This makes Chameleon a more
generalpurpose model for both multimodal understanding and generation tasks similar in spirit to the
Perceiver Jaegle et al 2021 architecture which also aims for a unified model across modalities and tasks
In summary Chameleon builds on a rich history of work in multimodal learning and tokenbased architectures
while pushing the boundaries in terms of model scale and architecture design By demonstrating strong
performance across a wide range of visionlanguage tasks and enabling new capabilities in mixedmodal
reasoning and generation Chameleon represents a significant step towards realizing the vision of general
17purpose multimodal foundation models
7 Conclusion
In this paper we introduced Chameleon  a new family of earlyfusion tokenbased foundation models that
set a new bar for multimodal machine learning By learning a unified representation space over interleaved
image and text tokens Chameleon is a single model that achieves strong performance across a wide range of
visionlanguage benchmarks while enabling new mixedmodal reasoning and generation capabilities
The key to Chameleon s success is its fully tokenbased architecture which allows for seamless information
integration across modalities By quantizing images into discrete tokens and training on mixedmodal data
from scratch Chameleon learns to jointly reason over image and text in a way that is impossible with
latefusion architectures or models that maintain separate encoders for each modality At the same time
Chameleon introduces novel techniques for stable and scalable training of earlyfusion models addressing key
optimization and architectural design challenges that have previously limited the scale of such approaches On
tasks such as image captioning and visual question answering Chameleon34B outperforms models such as
Flamingo and IDEFICS while maintaining competitive performance on textonly benchmarks Chameleon
also unlocks entirely new possibilities for multimodal interaction as demonstrated by its strong performance
on our new benchmark for mixedmodal openended QA
Acknowledgements
We thank Naren Briar for her invaluable contribution to manually curating safety prompts which were crucial
for our safety tuning efforts We also thank Pierre Fernandez for his indispensable support with the Chameleon
release Shelly Sheynin for her work on the Chameleon image tokenizer Puxin Xu and David for helping us
with datasets Additionally we thank Mitchell Wortsman for engaging in insightful discussions about stability
in largescale language models and Mike Lewis for general discussions and advice throughout the project We
thank Aaron Grattafiori Firat Ozgenel Divya Shah Danny Livshits Cristian Canton Ferrer Saghar Hosseini
Ramon Calderer Joshua Saxe Daniel Song and Manish Bhatt for their help with the safety and red teaming
efforts
Contributors
We attribute credit separated by bucket of work Additionallyindicates joint first authorsindicates key
contributorsindicates workstream leads andindicates project leads
PreTraining Srinivasan Iyer Bernie Huang Lili Yu Arun Babu Chunting Zhou Kushal Tirumala Xi
Victoria Lin Hu Xu Xian Li Akshat Shrivastava Omer Levy Armen Aghajanyan
Alignment and Safety Ram Pasunuru Andrew Cohen Aram H Markosyan Koustuv Sinha Xiaoqing
Ellen Tan Ivan Evtimov Ping Yu Tianlu Wang Olga Golovneva Asli Celikyilmaz
Inference and Evaluation Pedro Rodriguez Leonid Shamis Vasu Sharma Christine Jou Karthik Padthe
ChingFeng Yeh Mingda Chen Bapi Akula Jacob Kahn Daniel Li Scott Yih
Overall Project Barlas Oguz Morteza Behrooz Benjamin Muller Carleigh Wood Mary Williamson Ramya
Raghavendra Barbara Usher William Ngan Nikolay Bashlykov Lukas Blecher Sony Theakanath Lead
PM Ammar Rizvi Lead TPM Gargi Ghosh Luke Zettlemoyer
18References
Armen Aghajanyan Bernie Huang Candace Ross Vladimir Karpukhin Hu Xu Naman Goyal Dmytro Okhonko
Mandar Joshi Gargi Ghosh Mike Lewis et al Cm3 A causal masked multimodal model of the internet arXiv
preprint arXiv220107520  2022
Armen Aghajanyan Lili Yu Alexis Conneau WeiNing Hsu Karen Hambardzumyan Susan Zhang Stephen Roller
Naman Goyal Omer Levy and Luke Zettlemoyer Scaling laws for generative mixedmodal language models arXiv
preprint arXiv230103728  2023
Agriflanders Miniatuurpaardjes prijskamp  Agriflanders 2009 2009 httpsenwikipediaorgwikiFile
Miniatuurpaardjejpg  CCBYSA 20 httpscreativecommonsorglicensesby20deeden 
JeanBaptiste Alayrac Jeff Donahue Pauline Luc Antoine Miech Iain Barr Yana Hasson Karel Lenc Arthur Mensch
Katherine Millican Malcolm Reynolds et al Flamingo a visual language model for fewshot learning Advances in
Neural Information Processing Systems  352371623736 2022
Tadas Baltrušaitis Chaitanya Ahuja and LouisPhilippe Morency Multimodal machine learning A survey and
taxonomy IEEE transactions on pattern analysis and machine intelligence  412423443 2018
Hangbo Bao Li Dong Songhao Piao and Furu Wei Beit Bert pretraining of image transformers arXiv preprint
arXiv210608254  2021
James Betker Gabriel Goh Li Jing Tim Brooks Jianfeng Wang Linjie Li Long Ouyang Juntang Zhuang Joyce
Lee Yufei Guo et al Improving image generation with better captions Computer Science httpscdn openai
compapersdalle3 pdf  238 2023
Yonatan Bisk Rowan Zellers Jianfeng Gao Yejin Choi et al Piqa Reasoning about physical commonsense in natural
language In Proceedings of the AAAI conference on artificial intelligence  pages 74327439 2020
Jun Chen Han Guo Kai Yi Boyang Li and Mohamed Elhoseiny Visualgpt Dataefficient adaptation of pretrained
language models for image captioning In Proceedings of the IEEECVF Conference on Computer Vision and
Pattern Recognition  pages 1803018040 2022
Xinlei Chen Hao Fang TsungYi Lin and Ramakrishna Vedantam httpsgithubcomsalanizpycocoevalcap  2020
Aakanksha Chowdhery Sharan Narang Jacob Devlin Maarten Bosma Gaurav Mishra Adam Roberts Paul Barham
Hyung Won Chung Charles Sutton Sebastian Gehrmann et al PaLM Scaling language modeling with pathways
arXiv preprint arXiv220402311  2022
Christopher Clark Kenton Lee MingWei Chang Tom Kwiatkowski Michael Collins and Kristina Toutanova Boolq
Exploring the surprising difficulty of natural yesno questions arXiv preprint arXiv190510044  2019
Peter Clark Isaac Cowhey Oren Etzioni Tushar Khot Ashish Sabharwal Carissa Schoenick and Oyvind Tafjord
Think you have solved question answering try arc the ai2 reasoning challenge arXiv preprint arXiv180305457 
2018
Karl Cobbe Vineet Kosaraju Mohammad Bavarian Mark Chen Heewoo Jun Lukasz Kaiser Matthias Plappert
Jerry Tworek Jacob Hilton Reiichiro Nakano et al Training verifiers to solve math word problems arXiv preprint
arXiv211014168  2021
Mostafa Dehghani Josip Djolonga Basil Mustafa Piotr Padlewski Jonathan Heek Justin Gilmer Andreas Peter
Steiner Mathilde Caron Robert Geirhos Ibrahim Alabdulmohsin et al Scaling vision transformers to 22 billion
parameters In International Conference on Machine Learning  pages 74807512 PMLR 2023
Oran Gafni Adam Polyak Oron Ashual Shelly Sheynin Devi Parikh and Yaniv Taigman Makeascene Scenebased
texttoimage generation with human priors arXiv preprint arXiv220313131  2022
Gemini Rohan Anil Sebastian Borgeaud Yonghui Wu JeanBaptiste Alayrac Jiahui Yu Radu Soricut Johan
Schalkwyk Andrew M Dai Anja Hauth et al Gemini a family of highly capable multimodal models arXiv
preprint arXiv231211805  2023
Yash Goyal Tejas Khot Douglas SummersStay Dhruv Batra and Devi Parikh Making the V in VQA matter
Elevating the role of image understanding in visual question answering In Proceedings of the IEEE conference on
computer vision and pattern recognition  pages 69046913 2017
19Dan Hendrycks Collin Burns Steven Basart Andy Zou Mantas Mazeika Dawn Song and Jacob Steinhardt Measuring
massive multitask language understanding In International Conference on Learning Representations  2020
Dan Hendrycks Collin Burns Saurav Kadavath Akul Arora Steven Basart Eric Tang Dawn Song and Jacob
Steinhardt Measuring mathematical problem solving with the math dataset arXiv preprint arXiv210303874  2021
Or Honovich Thomas Scialom Omer Levy and Timo Schick Unnatural instructions Tuning language models with
almost no human labor 2022
Drew A Hudson and Christopher D Manning Gqa A new dataset for realworld visual reasoning and compositional
question answering In Proceedings of the IEEECVF conference on computer vision and pattern recognition  pages
67006709 2019
Andrew Jaegle Sebastian Borgeaud JeanBaptiste Alayrac Carl Doersch Catalin Ionescu David Ding Skanda
Koppula Daniel Zoran Andrew Brock Evan Shelhamer et al Perceiver io A general architecture for structured
inputs  outputs arXiv preprint arXiv210714795  2021
Albert Q Jiang Alexandre Sablayrolles Arthur Mensch Chris Bamford Devendra Singh Chaplot Diego de las
Casas Florian Bressand Gianna Lengyel Guillaume Lample Lucile Saulnier et al Mistral 7b arXiv preprint
arXiv231006825  2023
Albert Q Jiang Alexandre Sablayrolles Antoine Roux Arthur Mensch Blanche Savary Chris Bamford Devendra Singh
Chaplot Diego de las Casas Emma Bou Hanna Florian Bressand et al Mixtral of experts arXiv preprint
arXiv240104088  2024
Yang Jin Kun Xu Liwei Chen Chao Liao Jianchao Tan Bin Chen Chenyi Lei An Liu Chengru Song Xiaoqiang Lei
et al Unified languagevision pretraining with dynamic discrete visual tokenization arXiv preprint arXiv230904669 
2023
Yuval Kirstain Adam Polyak Uriel Singer Shahbuland Matiana Joe Penna and Omer Levy Pickapic An open
dataset of user preferences for texttoimage generation 2023
Klaus Krippendorff Content analysis An introduction to its methodology  Sage publications 2018
Taku Kudo and John Richardson Sentencepiece A simple and language independent subword tokenizer and detokenizer
for neural text processing arXiv preprint arXiv180806226  2018
Hugo Laurençon Lucile Saulnier Léo Tronchon Stas Bekman Amanpreet Singh Anton Lozhkov Thomas Wang
Siddharth Karamcheti Alexander M Rush Douwe Kiela et al Obelisc An open webscale filtered dataset of
interleaved imagetext documents arXiv preprint arXiv230616527  2023
Kevin Lee and Shubho Sengupta Introducing the ai research supercluster  metas cuttingedge ai supercomputer for
ai research httpsaifacebookcomblogairsc  2022
Benjamin Lefaudeux Francisco Massa Diana Liskovich Wenhan Xiong Vittorio Caggiano Sean Naren Min Xu Jieru
Hu Marta Tintore Susan Zhang Patrick Labatut Daniel Haziza Luca Wehrstedt Jeremy Reizenstein and Grigory
Sizov xformers A modular and hackable transformer modelling library httpsgithubcomfacebookresearch
xformers  2022
TsungYi Lin Michael Maire Serge Belongie James Hays Pietro Perona Deva Ramanan Piotr Dollár and C Lawrence
Zitnick Microsoft coco Common objects in context In European conference on computer vision  pages 740755
Springer 2014
Haotian Liu Chunyuan Li Yuheng Li and Yong Jae Lee Improved baselines with visual instruction tuning arXiv
preprint arXiv231003744  2023a
HaotianLiu ChunyuanLi QingyangWu andYongJaeLee Visualinstructiontuning arXiv preprint arXiv230408485 
2023b
Ze Liu Yutong Lin Yue Cao Han Hu Yixuan Wei Zheng Zhang Stephen Lin and Baining Guo Swin transformer
Hierarchical vision transformer using shifted windows In Proceedings of the IEEECVF international conference on
computer vision  pages 1001210022 2021
Ilya Loshchilov and Frank Hutter Decoupled weight decay regularization arXiv preprint arXiv171105101  2017
Giacomo Marzi Marco Balzano and Davide Marchiori Kalpha calculatorkrippendorffs alpha calculator A user
friendly tool for computing krippendorffs alpha interrater reliability coefficient MethodsX  12102545 2024 ISSN
2022150161 doi httpsdoiorg101016jmex2023102545 httpswwwsciencedirectcomsciencearticlepii
S2215016123005411 
Todor Mihaylov Peter Clark Tushar Khot and Ashish Sabharwal Can a suit of armor conduct electricity a new
dataset for open book question answering arXiv preprint arXiv180902789  2018
OpenAI GPTV System Card httpscdnopenaicompapersGPTV_System_Cardpdf  2023
Adam Paszke Sam Gross Francisco Massa Adam Lerer James Bradbury Gregory Chanan Trevor Killeen Zeming
Lin Natalia Gimelshein Luca Antiga et al PyTorch An imperative style highperformance deep learning library
InNeurIPS  2019
Bryan A Plummer Liwei Wang Chris M Cervantes Juan C Caicedo Julia Hockenmaier and Svetlana Lazebnik
Flickr30k entities Collecting regiontophrase correspondences for richer imagetosentence models In Proceedings
of the IEEE international conference on computer vision  pages 26412649 2015
Aditya Ramesh Mikhail Pavlov Gabriel Goh Scott Gray Chelsea Voss Alec Radford Mark Chen and Ilya Sutskever
Zeroshot texttoimage generation arXiv preprint arXiv210212092  2021
Aditya Ramesh Prafulla Dhariwal Alex Nichol Casey Chu and Mark Chen Hierarchical textconditional image
generation with clip latents arXiv preprint arXiv220406125  2022
Baptiste Roziere Jonas Gehring Fabian Gloeckle Sten Sootla Itai Gat Xiaoqing Ellen Tan Yossi Adi Jingyu Liu
Tal Remez Jérémy Rapin et al Code llama Open foundation models for code arXiv preprint arXiv230812950 
2023
Keisuke Sakaguchi Ronan Le Bras Chandra Bhagavatula and Yejin Choi Winogrande An adversarial winograd
schema challenge at scale Communications of the ACM  64999106 2021
Mikayel Samvelyan Sharath Chandra Raparthy Andrei Lupu Eric Hambro Aram H Markosyan Manish Bhatt
Yuning Mao Minqi Jiang Jack ParkerHolder Jakob Foerster Tim Rocktäschel and Roberta Raileanu Rainbow
teaming Openended generation of diverse adversarial prompts 2024
Maarten Sap Hannah Rashkin Derek Chen Ronan LeBras and Yejin Choi Socialiqa Commonsense reasoning about
social interactions arXiv preprint arXiv190409728  2019
Rylan Schaeffer Pretraining on the test set is all you need arXiv preprint arXiv230908632  2023
Christoph Schuhmann Romain Beaumont Richard Vencu Cade Gordon Ross Wightman Mehdi Cherti Theo
Coombes Aarush Katta Clayton Mullis Mitchell Wortsman et al Laion5b An open largescale dataset for
training next generation imagetext models arXiv preprint arXiv221008402  2022
Georges Seguin Millefeuille 2010 httpsenwikipediaorgwikiFileMillefeuille_20100916jpg  CCBYSA 30
httpscreativecommonsorglicensesbysa30deeden 
Rico Sennrich Barry Haddow and Alexandra Birch Neural machine translation of rare words with subword units In
ACL Berlin Germany 2016 httpsaclanthologyorgP161162 
ShareGPT GPTV System Card httpssharegptcom  2023
Noam Shazeer Glu variants improve transformer arXiv preprint arXiv200205202  2020
Maksim Sokolov Sagrada Familia July 2022 2022 httpsenwikipediaorgwikiFileSagrada_Familia_28July_
202229_08jpg  CCBYSA40 httpscreativecommonsorglicensesbysa40deeden 
Nitish Srivastava Geoffrey Hinton Alex Krizhevsky Ilya Sutskever and Ruslan Salakhutdinov Dropout a simple
way to prevent neural networks from overfitting The journal of machine learning research  15119291958 2014
Jianlin Su Yu Lu Shengfeng Pan Ahmed Murtadha Bo Wen and Yunfeng Liu Roformer Enhanced transformer
with rotary position embedding arxiv eprints art arXiv preprint arXiv210409864  2021
Hugo Touvron Louis Martin Kevin Stone Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov
Soumya Batra Prajjwal Bhargava Shruti Bhosale et al Llama 2 Open foundation and finetuned chat models
arXiv preprint arXiv230709288  2023
JarekTuszyński AmericantoadBufoamericanusfoundinFairfax Virginia 2015 httpsenwikipediaorgwikiFile
Miniatuurpaardjejpg  CCBYSA40 httpscreativecommonsorglicensesbysa40deeden 
Ramakrishna Vedantam C Lawrence Zitnick and Devi Parikh Cider Consensusbased image description evaluation
InProceedings of the IEEE conference on computer vision and pattern recognition  pages 45664575 2015
21Mitchell Wortsman Peter J Liu Lechao Xiao Katie Everett Alex Alemi Ben Adlam John D CoReyes Izzeddin
Gur Abhishek Kumar Roman Novak et al Smallscale proxies for largescale transformer training instabilities
arXiv preprint arXiv230914322  2023
Lili Yu Bowen Shi Ramakanth Pasunuru Benjamin Muller Olga Golovneva Tianlu Wang Arun Babu Binh Tang
Brian Karrer Shelly Sheynin et al Scaling autoregressive multimodal models Pretraining and instruction tuning
arXiv preprint arXiv230902591  2023
Rowan Zellers Ari Holtzman Yonatan Bisk Ali Farhadi and Yejin Choi Hellaswag Can a machine really finish your
sentence arXiv preprint arXiv190507830  2019
Biao Zhang and Rico Sennrich Root mean square layer normalization Advances in Neural Information Processing
Systems 32 2019
Chunting Zhou Pengfei Liu Puxin Xu Srini Iyer Jiao Sun Yuning Mao Xuezhe Ma Avia Efrat Ping Yu Lili Yu
et al Lima Less is more for alignment arXiv preprint arXiv230511206  2023
22Appendix
A Samples
Prompt My mom just adopted a dog called a mountain cur I thought I
knew a lot about dogs but I have never heard of this breed Can you give
me some basic information as well as show me a few pictures of what they
look like
Response Sure Mountain Cur dogs are a type of hunting dog
that originated in the Appalachian Mountains They are known for
their intelligence loyalty and strong prey drive They are typically
mediumsized dogs with a muscular build and a short easytomaintain
coat that comes in a variety of colors including black brown tan red
and white Here is an image of a black and white mountain cur dog img
They have a long muzzle floppy ears and a bushy tail They are generally
friendly and outgoing but can be wary of strangers if not socialized
properly They are highly energetic dogs that require regular exercise and
mental stimulation to prevent boredom and destructive behavior
Here is another image of a mountain cur dog in the forest img
Mountain Cur dogs are often used for hunting particularly for small game
such as squirrels raccoons and rabbits They are also popular family
pets due to their loyal and affectionate nature However they do require
regular exercise and training to ensure they remain happy and healthy
With proper care and attention Mountain Cur dogs can make wonderful
companions for active families or individuals who enjoy the outdoors
Prompt What color is a polar bears fur Show me a photograph of the
polar bear in the wild
Response Polar bears have white fur which helps them blend in with
their snowy and icy surroundings This camouflage allows them to
hunt more effectively as they can sneak up on their prey without being
detected Additionally the white fur reflects sunlight keeping the polar
bear cool in the harsh Arctic environment Here is an image of a polar
bear in the wild img
23Prompt img Myplantlookslikethis Isthisaproblem Ifso howdoIfixit
Response The image you provided shows a pothos plant with yellowing leaves
which can be caused by a variety of factors including overwatering underwater
ing lack of nutrients too much direct sunlight or pests Here are some steps
you can take to troubleshoot and potentially fix the issue
Check the soil Pothos plants prefer welldraining soil that is neither too
wet nor too dry If the soil feels soggy or waterlogged stop watering for a
while Conversely if the soil feels dry to the touch its time to water
Adjust watering Overwatering is a common cause of yellowing leaves on
pothos plants Reduce the frequency of watering and allow the soil to dry
out slightly between waterings
Provide adequate light Pothos plants prefer bright indirect light but can tolerate low light
conditions If your plant is not getting enough light move it to a brighter location
Fertilize Pothos plants benefit from regular fertilization Use a balanced watersoluble fertilizer
and follow the instructions on the label
Prune Yellow leaves can be removed to improve the appearance of the plant Cut back any yellow
leaves to the nearest healthy stem using clean pruning shears
Check for pests Inspect the plant for pests such as mealybugs spider mites or scale If present
treat the plant with insecticidal soap or neem oil according to the products instructions
Repot If the plant is potbound meaning the roots have filled the container consider repotting it
in a larger container with fresh welldraining soil
By following these steps you may be able to identify and address the underlying cause of the yellowing
leaves on your pothos plant
B Additional Information of Human Evaluations
Table 8Descriptions of the prompt task categories
Task category Description
Howto Instructions on how to complete a task or accomplish a goal
Advice Informed problemsolving
Explanation A more detailed exposition of a topic eg how batteries work why animals hibernate or
how to apply rules of composition to photography
Hypothetical Responses to imaginative what if questions
Brainstorming Generating ideas options or possibilities
Reasoning Deducing the answer to a question using commonsense or information provided in the prompt
Comparison Describes the similarities  differences between multiple things like products places foods
etc
Identification Identifying objects in the input image
Article Asking for the creation of content such as blog posts
Report Generating a summary of real events
Story Creating fictional narratives
Other Other miscellaneous requests
For the twelve task categories of the prompts we collected for human evaluation a short description of each
category can be found in Table 8
The task fulfillment rates broken down by each task category and modality are shown in Table 9 and Table 10
Chameleon s win rates broken down by task category and modality are shown in Table 11 Table 12 Table 13
and Table 14
24Table 9Task fulfillment breakdown
Chameleon Gemini GPT4V
Task Type Fulfills Partially
fulfillsDoes not
fulfillFulfills Partially
fulfillsDoes not
fulfillFulfills Partially
fulfillsDoes not
fulfill
Advice 692 262 47 421 561 19 439 486 75
Article 594 375 31 406 531 63 625 375 00
Brainstorming 579 364 56 333 615 51 477 472 51
Comparison 604 347 50 475 465 59 436 446 119
Explanation 530 377 93 338 616 46 417 503 79
Howto 527 405 69 435 527 38 481 412 107
Hypothetical 559 390 51 390 475 136 424 441 136
Identification 557 330 113 330 660 10 351 557 93
Other 418 400 182 382 418 200 509 400 91
Reasoning 500 136 364 273 591 136 318 545 136
Report 491 404 105 298 614 88 386 474 140
Story 317 634 49 390 561 49 537 439 24
Gemini GPT4V
Task Type Fulfills Partially
fulfillsDoes not
fulfillFulfills Partially
fulfillsDoes not
fulfill
Advice 215 701 84 234 757 09
Article 125 844 31 94 906 00
Brainstorming 185 718 97 272 667 62
Comparison 149 762 89 198 723 79
Explanation 152 781 66 199 775 26
Howto 198 740 61 313 672 15
Hypothetical 305 492 203 322 610 68
Identification 186 753 62 227 680 93
Other 145 600 255 182 673 145
Reasoning 91 773 136 136 818 45
Report 123 772 105 228 684 88
Story 98 829 73 73 902 24
Table 10 Modality fulfillment breakdown
Chameleon Gemini GPT4V
Fulfills Partially
fulfillsDoes not
fulfillFulfills Partially
fulfillsDoes not
fulfillFulfills Partially
fulfillsDoes not
fulfill
Mixedmodality 553 367 79 392 578 29 426 524 50
Textonly 577 384 40 364 555 81 461 427 112
Gemini GPT4V
Fulfills Partially
fulfillsDoes not
fulfillFulfills Partially
fulfillsDoes not
fulfill
Mixedmodality 197 760 43 243 726 32
Textonly 183 727 91 236 720 44
25Table 11Complete Win Rates Chameleon vs Gemini
Wins Ties Loses Win rate
Overall 435 362 251 588
Advice 48 35 24 612
Article 14 14 4 656
Brainstorming 101 60 34 672
Comparison 41 38 22 594
Explanation 65 46 40 583
Howto 53 51 27 599
Hypothetical 17 24 18 492
Identification 39 33 25 572
Other 24 17 14 591
Reasoning 7 8 7 500
Report 16 22 19 474
Story 10 14 17 415
Mixedmodal Prompts 194 145 102 604
Textonly Prompts 241 217 149 576
Table 12 Complete Win Rates Chameleon vs GPT4V
Wins Ties Loses Win rate
Overall 375 331 342 516
Advice 54 27 26 631
Article 9 11 12 453
Brainstorming 78 57 60 546
Comparison 35 35 31 520
Explanation 53 56 42 536
Howto 49 46 36 550
Hypothetical 23 19 17 551
Identification 31 26 40 454
Other 16 13 26 409
Reasoning 11 5 6 614
Report 16 21 20 465
Story 0 15 26 183
Mixedmodal Prompts 149 119 173 473
Textonly Prompts 226 212 169 547
26Table 13 Complete Win Rates Chameleon vs Gemini
Wins Ties Loses Win rate
Overall 561 327 160 691
Advice 59 25 23 668
Article 18 11 3 734
Brainstorming 133 42 20 790
Comparison 54 29 18 678
Explanation 78 51 22 685
Howto 65 42 24 656
Hypothetical 27 26 6 678
Identification 45 30 22 619
Other 27 23 5 700
Reasoning 11 6 5 636
Report 30 21 6 711
Story 14 21 6 598
Mixedmodal Prompts 240 123 78 684
Textonly Prompts 321 204 82 697
Table 14 Complete Win Rates Chameleon vs GPT4V
Wins Ties Loses Win rate
Overall 482 329 237 617
Advice 53 30 24 636
Article 18 9 5 703
Brainstorming 107 53 35 685
Comparison 44 35 22 609
Explanation 75 36 40 616
Howto 51 49 31 576
Hypothetical 20 25 14 551
Identification 40 29 28 562
Other 20 22 13 564
Reasoning 10 6 6 591
Report 25 18 14 596
Story 19 17 5 671
Mixedmodal Prompts 191 125 125 575
Textonly Prompts 291 204 112 647
27
  Jukebox A Generative Model for Music
Prafulla Dhariwal 1Heewoo Jun 1Christine Payne 1Jong Wook Kim1Alec Radford1Ilya Sutskever1
Abstract
We introduce Jukebox a model that generates
music with singing in the raw audio domain We
tackle the long context of raw audio using a multi
scale VQV AE to compress it to discrete codes
and modeling those using autoregressive Trans
formers We show that the combined model at
scale can generate highﬁdelity and diverse songs
with coherence up to multiple minutes We can
condition on artist and genre to steer the musical
and vocal style and on unaligned lyrics to make
the singing more controllable We are releasing
thousands of non cherrypicked samples along
with model weights and code
1 Introduction
Music is an integral part of human culture existing from the
earliest periods of human civilization and evolving into a
wide diversity of forms It evokes a unique human spirit in
its creation and the question of whether computers can ever
capture this creative process has fascinated computer scien
tists for decades We have had algorithms generating piano
sheet music Hiller Jr  Isaacson 1957 Moorer 1972
Hadjeres et al 2017 Huang et al 2017 digital vocoders
generating a singers voice Bonada  Serra 2007 Saino
et al 2006 Blaauw  Bonada 2017 and also synthesizers
producing timbres for various musical instruments Engel
et al 2017 2019 Each captures a speciﬁc aspect of music
generation melody composition timbre and the human
voice singing However a single system to do it all remains
elusive
The ﬁeld of generative models has made tremendous
progress in the last few years One of the aims of gen
erative modeling is to capture the salient aspects of the data
and to generate new instances indistinguishable from the
true data The hypothesis is that by learning to produce the
data we can learn the best features of the data1 We are
surrounded by highly complex distributions in the visual
audio and text domain and in recent years we have devel
Equal contribution1OpenAI San Francisco Correspondence
to jukeboxopenaicomoped advances in text generation Radford et al speech
generation Xie et al 2017 and image generation Brock
et al 2019 Razavi et al 2019 The rate of progress in
this ﬁeld has been rapid where only a few years ago we
had algorithms producing blurry faces Kingma  Welling
2014 Goodfellow et al 2014 but now we now can gener
ate highresolution faces indistinguishable from real ones
Zhang et al 2019b
Generative models have been applied to the music genera
tion task too Earlier models generated music symbolically
in the form of a pianoroll which speciﬁes the timing pitch
velocity and instrument of each note to be played Yang
et al 2017 Dong et al 2018 Huang et al 2019a Payne
2019 Roberts et al 2018 Wu et al 2019 The symbolic
approach makes the modeling problem easier by working
on the problem in the lowerdimensional space However it
constrains the music that can be generated to being a speciﬁc
sequence of notes and a ﬁxed set of instruments to render
with In parallel researchers have been pursuing the non
symbolic approach where they try to produce music directly
as a piece of audio This makes the problem more challeng
ing as the space of raw audio is extremely high dimensional
with a high amount of information content to model There
has been some success with models producing piano pieces
either in the raw audio domain Oord et al 2016 Mehri
et al 2017 Yamamoto et al 2020 or in the spectrogram
domain Vasquez  Lewis 2019 The key bottleneck is
that modeling the raw audio directly introduces extremely
longrange dependencies making it computationally chal
lenging to learn the highlevel semantics of music A way to
reduce the difﬁculty is to learn a lowerdimensional encod
ing of the audio with the goal of losing the less important
information but retaining most of the musical information
This approach has demonstrated some success in generat
ing short instrumental pieces restricted to a set of a few
instruments Oord et al 2017 Dieleman et al 2018
In this work we show that we can use stateoftheart deep
generative models to produce a single system capable of gen
erating diverse highﬁdelity music in the raw audio domain
with longrange coherence spanning multiple minutes Our
approach uses a hierarchical VQV AE architecture Razavi
1Richard Feynmann famously said What I cannot create I
do not understandarXiv200500341v1  eessAS  30 Apr 2020Jukebox A Generative Model for Music
et al 2019 to compress audio into a discrete space with
a loss function designed to retain the maximum amount of
musical information while doing so at increasing levels
of compression We use an autoregressive Sparse Trans
former Child et al 2019 Vaswani et al 2017 trained with
maximumlikelihood estimation over this compressed space
and also train autoregressive upsamplers to recreate the lost
information at each level of compression
We show that our models can produce songs from highly
diverse genres of music like rock hiphop and jazz They
can capture melody rhythm longrange composition and
timbres for a wide variety of instruments as well as the
styles and voices of singers to be produced with the mu
sic We can also generate novel completions of existing
songs Our approach allows the option to inﬂuence the
generation process by swapping the top prior with a con
ditional prior we can condition on lyrics to tell the singer
what to sing or on midi to control the composition We
release our model weights and training and sampling code
at httpsgithubcomopenaijukebox
2 Background
We consider music in the raw audio domain represented as
a continuous waveform x211T where the number
of samplesTis the product of the audio duration and the
sampling rate typically ranging from 16 kHz to 48 kHz For
music CD quality audio 441 kHz samples stored in 16
bit precision is typically enough to capture the range of
frequencies perceptible to humans As an example a four
minutelong audio segment will have an input length of 10
million where each position can have 16 bits of information
In comparison a highresolution RGB image with 1024
1024 pixels has an input length of 3million and each
position has 24 bits of information This makes learning
a generative model for music extremely computationally
demanding with increasingly longer durations we have to
capture a wide range of musical structures from timbre to
global coherence while simultaneously modeling a large
amount of diversity
21 VQV AE
To make this task feasible we use the VQV AE Oord et al
2017 Dieleman et al 2018 Razavi et al 2019 to compress
raw audio to a lowerdimensional space A onedimensional
VQV AE learns to encode an input sequence xhxtiT
t1
using a sequence of discrete tokens zhzs2KiS
s1
whereKdenotes the vocabulary size and we call the ratio
TS the hop length It consists of an encoder Exwhich
encodes xinto a sequence of latent vectors hhhsiS
s1
a bottleneck that quantizes hs7ezsby mapping each hs
to its nearest vector ezsfrom a codebook CfekgK
k1
and a decoder Dethat decodes the embedding vectorsback to the input space It is thus an autoencoder with a
discretization bottleneck The VQV AE is trained using the
following objective
LLrecons Lcodebook Lcommit 1
Lrecons 1
TP
tkxtDeztk2
2 2
Lcodebook 1
SP
sksg hsezsk2
2 3
Lcommit 1
SP
skhssg ezsk2
2 4
where sgdenotes the stopgradient operation which passes
zero gradient during backpropagation The reconstruction
lossLrecons penalizes for the distance between the input x
and the reconstructed output bxDez andLcodebook pe
nalizes the codebook for the distance between the encodings
hand their nearest neighbors ezfrom the codebook To
stabilize the encoder we also add Lcommit to prevent the
encodings from ﬂuctuating too much where the weight 
controls the amount of contribution of this loss To speed up
training the codebook loss Lcodebook instead uses EMA up
dates over the codebook variables Razavi et al 2019
extends this to a hierarchical model where they train a sin
gle encoder and decoder but break up the latent sequence
hinto a multilevel representation h1hLwith de
creasing sequence lengths each learning its own codebook
Cl They use nonautoregressive encoderdecoders and
jointly train all levels with a simple meansquared loss
3 Music VQV AE
Inspired by the results from the hierarchical VQV AE model
Razavi et al 2019 for images we consider applying the
same technique to model raw audio using three different
levels of abstraction as illustrated in Figure 1 At each level
we use residual networks consisting of WaveNetstyle non
causal 1D dilated convolutions interleaved with downsam
pling and upsampling 1D convolutions to match different
hop lengths A detailed description of the architecture is
provided in Appendix B1 We make a number of modiﬁca
tions to our VQV AE compared to the ones in Oord et al
2017 Razavi et al 2019 as described in the following
subsections
31 Random restarts for embeddings
VQV AEs are known to suffer from codebook collapse
wherein all encodings get mapped to a single or few em
bedding vectors while the other embedding vectors in the
codebook are not used reducing the information capacity
of the bottleneck To prevent this we use random restarts
when the mean usage of a codebook vector falls below a
threshold we randomly reset it to one of the encoder out
puts from the current batch This ensures all vectors in theJukebox A Generative Model for Music
V ector
QuantizationV ector
QuantizationV ector
Quantization
EncodeEncode
Encode
ht		E  xt  xt zt		 ar gmink	ǁ	 ht		 ek 	ǁ
DecodeDecodeDecode
ez t x t		D  ez t 
Codebook
LookupCodebook
LookupCodebook
LookupCodebook  ek
Figure 1 We ﬁrst train three separate VQV AE models with different temporal resolutions At each level the input audio is segmented
and encoded into latent vectors ht which are then quantized to the closest codebook vectors ezt The code ztis a discrete representation
of the audio that we later train our prior on The decoder takes the sequence of codebook vectors and reconstructs the audio The top
level learns the highest degree of abstraction since it is encoding longer audio per token while keeping the codebook size the same
Audio can be reconstructed using the codes at any one of the abstraction levels where the least abstract bottomlevel codes result in the
highestquality audio as shown in Figure 4 For the detailed structure of each component see Figure 7
codebook are being used and thus have a gradient to learn
from mitigating codebook collapse
32 Separated Autoencoders
When using the hierarchical VQV AE from Razavi et al
2019 for raw audio we observed that the bottlenecked top
level is utilized very little and sometimes experiences a com
plete collapse as the model decides to pass all information
through the less bottlenecked lower levels To maximize
the amount of information stored at each level we simply
train separate autoencoders with varying hop lengths Dis
crete codes from each level can be treated as independent
encodings of the input at different levels of compression
33 Spectral Loss
When using only the samplelevel reconstruction loss the
model learns to reconstruct low frequencies only To capture
midtohigh frequencies we add a spectral loss which is
deﬁned as
LspeckjSTFT xjjSTFT bxjk2
It encourages the model to match the spectral components
without paying attention to phase which is more difﬁcult
to learn This is similar to the use of power loss Oord
et al 2018 and spectral convergence Arık et al 2018b
when training parallel decoders for raw audio One differ
ence between the latter approach and ours is that we are no
longer optimizing the spectral signaltonoise ratio dividing
by the magnitude of the signal results in numerical insta
bility for mostly silent inputs To prevent the model from
overﬁtting to a particular choice of the STFT parameterswe use the sum of the spectral losses Lspeccalculated over
multiple STFT parameters that tradeoff time and frequency
resolutions Yamamoto et al 2020
4 Music Priors and Upsamplers
After training the VQV AE we need to learn a prior pz
over the compressed space to generate samples We break
up the prior model as
pz pztopzmiddlezbottom 5
pztoppzmiddlejztoppzbottomjzmiddleztop6
and train separate models for the toplevel prior pztop and
upsamplers pzmiddlejztopandpzbottomjzmiddleztop Each
of these is an autoregressive modeling problem in the dis
crete token space produced by the VQV AE We use Trans
formers with sparse attention Vaswani et al 2017 Child
et al 2019 as they are currently the SOTA in autoregressive
modeling We propose a simpliﬁed version which we call
the Scalable Transformer that is easier to implement and
scale see Appendix A for details
For the upsamplers we need to provide the autoregres
sive Transformers with conditioning information from the
codes of the upper levels To do so we use a deep resid
ual WaveNet Xie et al 2017 followed by an upsampling
strided convolution and a layer norm Ba et al 2016 and
add the output as extra positional information to the embed
dings of the current level We condition the lower levels
only on the chunk of upper level codes that correspond to
the same segment of raw audioJukebox A Generative Model for Music
At each level we use Transformers over the same context
length of discrete codes which correspond to increasing
the raw audio length with larger hop lengths and modeling
longer temporal dependencies at the higher levels while
keeping the same computational footprint for training each
level As our VQV AE is convolutional we can use the
same VQV AE to produce codes for arbitrary lengths of
audio
41 Artist Genre and Timing Conditioning
Our generative model can be made more controllable by
providing additional conditioning signals while training For
our ﬁrst models we provide artist and genre labels for the
songs This has two advantages ﬁrst it reduces the entropy
of the audio prediction so the model is able to achieve
better quality in any particular style Second at generation
time we are able to steer the model to generate in a style
of our choosing Additionally we attach a timing signal
for each segment at training time This signal includes the
total duration of the piece the start time of that particular
sample and how much fraction of the song that has elapsed
This allows the model to learn audio patterns that depend
on the overall structure such as spoken or instrumental
introductions and applause at the end of a piece
42 Lyrics Conditioning
While the conditioned models above are able to generate
songs of diverse genres and artistic styles singing voices
generated by those models while often sung in a compelling
melody are mostly composed of babbling rarely producing
recognizable English words In order to be able to control
the generative model with lyrics we provide more context
at training time by conditioning the model on the lyrics
corresponding to each audio segment allowing the model
to produce singing simultaneosly with the music
Lyricstosinging LTS task  The conditioning signal
only includes the text of the lyrics without timing or vocal
isation information We thus have to model the temporal
alignment of lyrics and singing the artists voice and also
the diversity of ways one can sing a phrase depending on the
pitch melody rhythm and even genre of the song The con
ditioning data isnt precise as the lyrics data often contains
textual references to repeated sections like chorus or mis
matching portions of lyrics with the corresponding music
There is also no separation between lead vocals accompa
nying vocals and the background music in target audio This
makes the Lyricstosinging LTS task signiﬁcantly more
challenging than the corresponding Texttospeech TTS
task
Providing lyrics for chunks of audio  Our dataset includes
songlevel lyrics but to make the task easier we train on
shorter 24 sec chunks of audio To provide the lyrics cor
Middle Upsampler
Bottom Upsampler
VQV AE DecoderTopLevel Prior
Conditioning
InformationaAncestral sampling  Priors for the VQV AE codes are trained
using a cascade of Transformer models shown in blue Each model
takes conditioning information such as genre artist timing and
lyrics and the upsampler models are also conditioned on the codes
from the upper levels To generate music the VQV AE codes are
sampled from top to bottom using the conditioning information
for control after which the VQV AE decoder can convert the
bottomlevel codes to audio
time
new samples
bWindowed sampling  To generate music longer than the
models context length 12 in this ﬁgure we repeatedly sample
continuations at each level using overlapping windows of previous
codes as the context The overlap amount is a hyperparameter and
the ﬁgure shows an example of 75 overlap with hop length 3
Primed Audio Generated AudioEncodeGenerateDecode
cPrimed sampling  The model can generate continuations of
an existing audio signal by converting it into the VQV AE codes
and sampling the subsequent codes in each level
Figure 2 Sampling methods for generating musicJukebox A Generative Model for Music
responding to the audio during training we began with a
simple heuristics of aligning the characters of the lyrics to
linearly span the duration of each song and pass a ﬁxedside
window of characters centered around the current segment
during training While this simple strategy of linear align
ment worked surprisingly well we found that it fails for
certain genres such as hiphop with fast lyrics To address
this we use Spleeter Hennequin et al 2019 to extract vo
cals from each song and run NUS AutoLyricsAlign Gupta
et al 2020 on the extracted vocals to obtain a wordlevel
alignments of the lyrics allowing us to more accurately
provide the lyrics for a given chunk of audio We choose a
large enough window so that the actual lyrics have a high
probability of being inside the window
Encoderdecoder model  We use an encoderdecoder style
model to condition on the characters of the lyrics with
the encoder producing features from the lyrics which are
attended to by the decoder which produces the top level
music tokens The lyrics encoder is a Transformer with an
autoregressive modeling loss for lyrics and its last level is
used as features of the lyrics In the music decoder we inter
leave a few additional layers with encoderdecoder attention
where the queries from the music tokens are only allowed
to attend to keys and values from the lyrics tokens These
layers attend on the activation from the last layer of the
lyrics encoder see Figure 8c In Figure 3 we see that the
attention pattern learned by one of these layers corresponds
to the alignment between the lyrics and the singing
43 Decoder Pretraining
To reduce computation required to train the lyrics condi
tional model we use a pretrained unconditional toplevel
prior as our decoder and introduce the lyrics encoder using
model surgery Berner et al 2019 We initialize the output
projection weights in the MLP and the attention layers of
these residual blocks to zeros Zhang et al 2019a so that
the added layers perform the identity function at initializa
tion Thus at initialization the model behaves identically
as the pretrained decoder but there is still a gradient with
respect to the encoder state and parameters2 allowing the
model to learn to use the encoder
44 Sampling
After we have trained our VQV AE upsamplers and top
level priors we can then use them to sample novel songs
Ancestral sampling  We ﬁrst generate the top level codes
one token at a time by the usual ancestral sampling process
see Figure 2a generating the ﬁrst token then passing all
2The gradient also needs to break symmetry with the encoder
output features which is the case here since the weights of the
input projections in the attention are not zero
0 1600 3200 4800 6400 8000
Music token position0100200300400500Lyrics token position
000204060810Figure 3 Lyricssinging alignment learned by one of the encoder
decoder attention layers The xaxis is the position of music
queries and the yaxis is the position of lyric keys The positions
attended to by the decoder correspond to the characters being sung
previously generated tokens into the model as inputs and
outputting the next token conditioned on all previous tokens
We then run our conditioning wavenet on the top level codes
to produce the conditioning information for the middle level
and sample ancestrally from it too and do the same for the
bottom level
Windowed sampling  To sample segments longer than the
context length we use windowed sampling where we move
ahead our sampling window by half our context and con
tinue sampling conditioned on this half context See Figure
2b We can trade off speed for quality by using a smaller
hop length here
Primed sampling  Instead of sampling the entire token
sequence from the model we can also run a forward pass
of the VQV AE to obtain the top middle and bottom level
codes corresponding to a segment from an actual song as
shown in Figure 2c We can use these as the initial tokens in
our ancestral sampling process and continue sampling from
these to produce novel completions of the song
5 Experiments
51 Dataset
We scraped a new dataset of 12 million songs 600k of
which in English paired with the lyrics and metadata from
LyricWiki LyricWiki The metadata includes artist album
genre and year of the release along with common moods or
playlist keywords associated with each song We train on 32
bit 441 kHz raw audio and perform data augmentation by
randomly downmixing the right and left channels to produce
mono channel audio
52 Training Details
For the music VQV AE we use 3 levels of bottlenecks com
pressing 44 kHz audio in dimensionality by 8x 32x andJukebox A Generative Model for Music
128x respectively with a codebook size of 2048 for each
level The VQV AE has 2 million parameters and is trained
on 9second audio clips on 256 V100 for 3 days We used
exponential moving average to update the codebook fol
lowing Razavi et al 2019 For our prior and upsampler
models we use a context of 8192 tokens of VQV AE codes
which corresponds to approximately 24 6 and 15 seconds
of raw audio at the top middle and bottom level respec
tively The upsamplers have one billion parameters and are
trained on 128 V100s for 2 weeks and the toplevel prior
has 5 billion parameters and is trained on 512 V100s for 4
weeks We use Adam with learning rate 000015 and weight
decay of 0002 For lyrics conditioning we reuse the prior
and add a small encoder after which we train the model on
512 V100s for 2 weeks The detailed hyperparameters for
our models and training are provided in Appendix B3
53 Samples
We trained a sequence of models with increasing sample
quality Our ﬁrst model was trained on the MAESTRO
dataset using 22 kHz VQV AE codes and relatively small
prior models We observed that this could generate high
ﬁdelity classical music samples with piano and occasional
violin We then collected a larger and more diverse dataset
of songs with genre and artist labels The same model when
trained on this new dataset was able to produce diverse sam
ples other than classical music and demonstrated musicality
and coherence over more than a minute
Despite the novelty of being able to generate generally high
ﬁdelity and coherent songs sample quality was still limited
by a number of factors First the use of 22 kHz sampling
rate along with small upsamplers introduced noise both in
the upsampling and decoding steps which we hear as grainy
texture We improved ﬁdelity by using 44 kHz VQV AE
and 1B parameter upsamplers in all subsequent experiments
at the expense of longer rendering time
Second the 1B toplevel prior was not big enough to pro
duce singing and diverse musical timbres We ﬁrst explored
increasing the model size to 5 billion parameters Larger
capacity allowed better modeling of the broader distribu
tion of songs resulting in samples with better musicality
longer coherence and initial singing While there is an over
all qualitative improvement the unconditional model still
struggled to sing recognizable words Training a seq2seq
model with lyric conditioning and limiting the dataset only
to songs primarily in English made singing both intelligible
and controllable
The ﬁnal model which we call Jukebox uses all these
improvements Because everyone experiences music dif
ferently it is generally tricky and not very meaningful to
evaluate samples by the mean opinion score or FIDlike
metrics We manually evaluate coherence musicality diversity and novelty of generated samples The links to curated
examples are embedded in text
Coherence We ﬁnd the samples stay very coherent musi
cally through the context length of the toplevel prior ap
proximately 24 seconds and they maintain similar har
monies and textures as we slide the window to generate
longer samples However because the toplevel does not
have the context of the entire song we do not hear long
term musical patterns and we would never hear choruses or
melodies that repeat
The generations progress through beginnings of songs for
example applause or slow instrumental warmups through
sections that sound choruslike through instrumental inter
ludes and then fading or otherwise wrapping up at the end
The toplevel prior always knows what fraction of the song
is complete timewise so it is able to imitate appropriate
beginnings middles and ends
Musicality The samples frequently imitate familiar mu
sical harmonies and the lyrics are usually set in ways that
are very natural Frequently the highest or longest notes of
the melody match words that a human singer would choose
to emphasize and the lyrics are almost always rendered
in ways that capture the prosody of the phrases This is
noticeable in hip hop generations where the model reliably
captures the rhythm of spoken text We do ﬁnd that the
generated melodies are usually less interesting than human
composed melodies In particular we do not hear the an
tecedent and consequent pattern familiar to many human
melodies and we rarely hear choruses that are melodically
memorable
Diversity Likelihood training encourages covering of all
modes so we expect the model to produce diverse samples
 Rerenditions We generate multiple samples conditioned
on artist and lyrics combinations that exist in our training
data While occasionally drum and bass lines or melodic
intervals echo the original versions we ﬁnd that none of
the generated samples is noticeably similar to the original
songs
We also generate multiple songs conditioned on the same
artist and lyrics as Sample 1 to obtain Samples 912 All ﬁve
sound interesting in their own ways with different moods
and melodies with Sample 10 playing a harmonic at 0014
as part of a blues riff showing that the model has learned a
wide range of singing and playing styles
 Completions We prime the model with 12 seconds of
existing songs and ask it to complete them in the same
styles When the priming samples include singing the con
tinuations are more likely to imitate the original tunes and
rhythms Songs primed with more generic or common intros
tend to be more diverse Even generated samples that areJukebox A Generative Model for Music
close to the originals early on deviate completely into new
musical material after about 30 seconds
Rerenditions and completions are interesting and diverse
but overall there is still room for improvement in music
quality compared to the original songs
 Full tree To understand diversity in a more systematic
way we generate multiple continuations from the same seg
ment We start with a oneminute sample and independently
sample four times per oneminute extension By the three
minute mark there are 16 completions We can think of this
branching tree as exploring different possibilities obtained
by ancestral sampling In the generated songs in the link
we hear diversity in singing and development even when the
same initial segment is used We note that this particular
sample follows the lyrics more successfully than many For
certain genres like hip hop and rap where linearly moving
the window does not yield good lyrics alignment the chance
of obtaining plausible singing is lower
Novelty With the ability to condition on various styles
lyrics and raw audio we would like Jukebox to be a useful
tool for both professional musicians and music enthusiasts
alike In this section we are interested in exploring capabil
ities and applications of Jukebox
 Novel styles We generate songs in an unusual genre typi
cally not associated with an artist In general we ﬁnd that
it is fairly difﬁcult to generalize to a novel style of singing
while using the same voice as the artist embedding overpow
ers other information In Joe Bonamassa and Frank Sinatra
samples we hear a modest variation in instrumentation
energy and ambience depending on the genre embedding
However our attempts to mix country singer Alan Jackson
with unusual genres like hip hop and punk did not seem to
move the samples away from a country style in meaningful
ways
 Novel voices We pick artists whose voices are reproduced
reasonably well by the model and interpolate their style
embeddings to synthesize new voices Some blending for
instance between Frank Sinatra and Alan Jackson in Sample
4 still sounds similar to Frank Sinatra In most cases the
model renders in a vaguely recognizable but distinct voice
that preserves different vocal attributes Samples 1 and
2 conditioned on the Céline Dion embeddings divided by
two have slightly different timbre and tone but capture her
unique vibrato
We also experiment with changing the style embedding in
the middle of a song to create a duet Sample 7 This is
another way of guiding generation during sampling Con
tinuing in another voice works best when the segment ends
in an interlude otherwise the model blends voices in the
middle of a word or a sentence Novel lyrics We ask Jukebox to sing poems and novel
verses generated by GPT2 Radford et al to demonstrate
that it can indeed sing new lyrics While the training data
consists of song lyrics with limited vocabulary and con
strained structure the model has learned to follow along
most prompts and sing even new words that are reasonably
pronounceable including technical terms from the deep
learning literature To get the best results however we ﬁnd
that it is useful to spell out difﬁcult words or acronyms as
they are spoken The generations are noticeably higher qual
ity if the text matches the distribution of lyrics for the given
artist both in terms of length and of rhyming or rhythmic
qualities For example hip hop lyrics tend to be longer than
most other genres and the commonly emphasized syllables
easily form clear rhythms
 Novel riffs Another useful application of Jukebox is the
ability to record an incomplete idea and explore various
continuations without ever needing to tabulate in symbolic
representations which would lose details of timbre and
mood We curate recordings of novel riffs by our inhouse
musicians and prime the model during sampling Sample 6
starts with a musical style not widely used in Elton Johns
songs The model still carries out the tune and develops
it further Similarly the beginning of Sample 1 is a pro
gressive jazz piece with a 54 polymeter which has never
been used in hip hop Despite this novelty the rhythm per
sists throughout the song and is incorporated naturally with
rapping
54 VQV AE Ablations
Spectral convergence dB
Level Hop length Without restart With restart
Bottom 8 211230
Middle 32 124124
Top 128 8383
Table 1 Reconstruction ﬁdelity degrades with higher compression
Restarting dead codes near random encoder outputs mitigates learn
ing suboptimal codes
Codebook size Spectral convergence dB
256 159
2048 230
No quantization 405
Table 2 Bottomlevel VQV AE reconstruction results with differ
ent codebook sizes Using larger codebooks helps reconstruction
because it allows more information to be encoded at the bottleneck
layers Removing the bottleneck entirely yields almost perfect
reconstructionJukebox A Generative Model for Music
5001k2k4k8k16k
5001k2k4k8k16k
5001k2k4k8k16k
5001k2k4k8k16k
5001k2k4k8k16k
5001k2k4k8k16k
Figure 4 Comparison of reconstructions from different VQV AEs xaxis is time and yaxis is frequency The columns from left to
right are bottom middle and toplevel reconstructions at hop lengths 8 32 and 128 respectively visualized as Mel spectrograms
The ﬁrst row is the groundtruth and the second row shows the spectrograms of audio outputs from our VQV AE In the third row we
remove the spectral loss and see that the middle and top level lose highfrequency information In the fourth row we use a hierarchical
VQV AE Razavi et al 2019 instead of separate autoencoders Figure 1 and we see the middle and top levels are not used for encoding
pertinent information Finally the ﬁfth row shows a baseline with the Opus codec that encodes audio at constant bitrates comparable to
our VQV AE It also fails to capture higher frequencies and adds noticeable artifacts at the highest level of compression
Ablation Spectral convergence dB
None 83
Without spectral loss 63
With single autoencoder 29
Table 3 Toplevel codes are generally difﬁcult to train well without
spectral loss or with a single hierarchical autoencoder Resulting
reconstructions may lose some to most of information
We compare raw audio VQV AEs when trained with varying
compression ratios objectives and architectures As we
use nonautoregressive decoders with continuous represen
tation for output we report spectral convergence Sturmel Daudet 2011 which measures the amount of spectral
error relative to signal as test error and proxy for reconstruc
tion ﬁdelity We evaluate on 5000 heldout 3second audio
segments and report the average in decibels All models in
this section are trained with a batch size of 32 3second
audio clips sampled at 44 kHz As before we use hop
lengths of 8 32 and 128 for the bottom middle and top
level respectively
In Table 1 we see that increasing the hop size results in
higher reconstruction error Figure 4 indeed shows that a
signiﬁcant amount of information especially higher frequen
cies is missing at middle and top levels across all ablations
we ran This is expected as audio is compressed more withJukebox A Generative Model for Music
0 100k 200k 300k 400k 500k
Number of training steps8910Codebook entropy bitswith restart
without restart
Figure 5 Entropy of codebook with 2048 codes ie 11 bits over
training Reviving dead codes near random encoder outputs en
sures good codebook utilization from the start of training
larger hop sizes To mitigate codebook collapse we restart
dead codes near random encoder embeddings In Figure 5
we see that this yields higher codebook usage even from
early on in training Models trained without random restarts
can converge to the same test error and codebook usage but
require more training steps With poor initialization these
models sometimes end up with suboptimal codes hurting
reconstruction ﬁdelity
Codebook size also matters as it sets a limit on channel ca
pacity through the bottleneck layers In Table 2 we ﬁnd that
reconstruction error increases considerably when the code
book size is reduced from 2048 to 256 We also compare
with a model that uses continuous representations without
vector quantization We can think of this model as using a
vastly large codebook with all encoder embeddings This
achieves almost perfect reconstruction with negligible spec
tral error
When the model is trained with L2 loss only reconstruc
tions tend to sound muddy from missing high frequencies
and this problem is exacerbated as hop size is increased In
Figure 4 we see that toplevel codes trained without spec
tral loss do not capture much information beyond 2 kHz
and obtain worse reconstructions Table 3 However we
observe that while spectral loss helps encode more infor
mation it also adds distortion artifacts which we hear as
scratchy noise
Lastly we train a raw audio hierarchical VQV AE Razavi
et al 2019 and ﬁnd that it is generally difﬁcult to push
information to higher levels This model is trained twice as
long as the previous models but middle and toplevel recon
structions as shown in Figure 4 are not capturing much It is
possible that higher level codes may have collapsed before
bottom level starts to reconstruct the audio well Making
the bottom layers explicitly model residuals pushed more
information to the top But we found separate autoencoders
to be cleaner and more effective6 Related Work
Generative modeling in deep learning Generative mod
els aim to learn the distribution of data by either explicitly
by modeling the distribution or implicitly by constructing
means to sample from it Goodfellow 2016 Modeling
the interdependency within highdimensional data was tra
ditionally considered extremely difﬁcult but starting with
Deep Boltzmann Machines Salakhutdinov  Hinton 2009
various kinds of deep generative models have been intro
duced Generative Adversarial Networks GANs Good
fellow et al 2014 use generator and discriminator net
works that contest each other to make the generated samples
as indistinguishable as possible from the data and they
are renowned for their ability to generate highquality pic
tures Zhang et al 2019b Brock et al 2019 Autoregres
sive generative models such as NADE Uria et al 2016
PixelCNN Van den Oord et al 2016 and Transformers
Vaswani et al 2017 use the chain rule of probability to
factorize the joint distribution of data into a product of
simpler distributions and ﬂowbased models Dinh et al
2015 2017 Rezende  Mohamed 2015 Kingma  Dhari
wal 2018 learn a series of invertible transformations that
maps the data distribution with a simpler one such as a
Gaussian distribution Autoregressive ﬂows Papamakarios
et al 2017 Kingma et al 2016 combine the two ideas to
achieve faster density estimation or data generation Varia
tional autoencoders V AEs Rezende et al 2014 Kingma
 Welling 2014 impose a Gaussian prior on the latent
code in an encoderdecoder setup from which data can be
sampled
Generative models for music Generative modeling of
symbolic music dates back to more than half a century when
Hiller Jr  Isaacson 1957 introduced the ﬁrst computer
generated music based on Markov chains There exists
a variety of earlier approaches using rulebased systems
Moorer 1972 chaos and selfsimilarity Pressing 1988
cellular automata Beyls 1989 concatenative synthesis
Jehan 2005 and constraint programming Anders  Mi
randa 2011 More recent datadriven approaches include
DeepBach Hadjeres et al 2017 and Coconet Huang et al
2017 which use Gibbs sampling to produce notes in the
style of Bach chorals MidiNet Yang et al 2017 and
MuseGAN Dong et al 2018 which use generative ad
versarial networks MusicV AE Roberts et al 2018 and
HRNN Wu et al 2019 which use hierarchical recurrent
networks and Music Transformer Huang et al 2019a
and MuseNet Payne 2019 which use Transformers to au
toregressively predict MIDI note events There also have
been a number of approaches for synthesizing music con
ditioned on symbolic music information such as NSynth
Engel et al 2017 which uses WaveNetstyle autoen
coder Mel2Mel Kim et al 2019 and Wave2Midi2Wave
Hawthorne et al 2019 which synthesize music usingJukebox A Generative Model for Music
WaveNet conditioned on a piano roll representation and
GanSynth Engel et al 2019 which uses generative adver
sarial networks to produce magnitude spectrograms together
with instananeous frequencies for easier spectrogram inver
sion Generative models for music can also be used for
music style transfer as seen in MidiV AE Brunner et al
2018 which uses a variational autoencoder to transfer styles
between classical and jazz music LakhNES Donahue et al
2019 which uses a Transformer architecture to generate
chiptune music and Universal Music Translator Network
Mor et al 2019 which uses a denoising autoencoder that
can disentangle musical style and content
Samplelevel generation of audio In recent years a vari
ety of generative models for raw audio have been introduced
WaveNet Oord et al 2016 performs autoregressive sample
bysample probabilistic modeling of raw waveform using a
series of dilated convolutions to exponentially increase the
context length It can produce realistic audio either uncon
ditionally or by conditioning on acoustic features or spec
trograms The autoregressive nature of WaveNet makes the
sampling notoriously slow and it uses a categorical distribu
tion for audio samples which introduces quantization noise
Parallel WaveNet Oord et al 2018 improves upon this
by instead using a mixture of logistics distribution a con
tinuous probability distribution and performing probabil
ity density distillation which learns a parallel feedforward
network from a pretrained autoregressive model allow
ing faster sampling of high ﬁdelity audio ClariNet Ping
et al 2019 achieves similar audio quality using a simple
Gaussian distribution instead and thus having a closedform
loss function eliminating the need for MonteCarlo sam
pling SampleRNN Mehri et al 2017 uses a multiscale
hierarchical recurrent neural network with convolutional
upsampling to model longrange complex structures Wa
veRNN Kalchbrenner et al 2018 uses recurrent neural
networks that operate separately on the most signiﬁcant and
the least signiﬁcant bytes which can be efﬁciently deployed
in mobile devices while having comparable audio quality to
WaveNet WaveGlow Prenger et al 2019 is a ﬂowbased
model for parallel samplelevel audio synthesis which can
be trained with a straightforward maximumlikelihood esti
mation and thus is advantageous to the twostage training
process needed for distillation Parallel WaveGAN Ya
mamoto et al 2020 and MelGAN Kumar et al 2019
are GANbased approaches directly modeling audio wave
forms achieving similar quality as WaveNet and WaveGlow
models with signiﬁcantly fewer parameters While the ap
proaches above serve as sophisticated generative models for
raw audio to be conditioned on a compact and controllable
representation of audio such as Mel spectrograms Mel
Net Vasquez  Lewis 2019 takes a different approach of
hierarchically generating accurate highresolution Mel spectrograms after which a simple gradientbased optimization
can produce highﬁdelity audio
VQV AE Oord et al 2017 introduced VQV AE an ap
proach of downsampling extremely long context inputs to a
shorterlength discrete latent encoding using a vector quan
tization and they showed that it can generate both high
quality images and audio as well as learn unsupervized
representations of phonemes Razavi et al 2019 extended
the above model by introducing a hierarchy of discrete rep
resentations for images and showed that the resulting model
can learn to separate highlevel semantics into the highest
level of discrete codes which have the largest receptive ﬁeld
while capturing local features like textures in the lower lev
els with smaller receptive ﬁelds They used the hierarchical
model to generate highdiversity and highﬁdelity images
for the conditional ImageNet and FFHQ datasets Dieleman
et al 2018 tried variants of this approach where instead
of a single encoder there are successive encoders that each
further compress the lossy discrete encodings from the previ
ous levels A downside of this approach is that information
is lost at each step and requires separate training for each
VQV AE level and it leads to a hierarchy collapse problem
De Fauw et al 2019 used AR decoders which are known to
cause the problem of ignoring the latent variables and they
suggested ways to mitigate it The feedforward decoders
from Razavi et al 2019 do not suffer from this issue and
thus we use their approach
Speech synthesis Producing natural human voice entails
an understanding of linguistic features mapping of sounds
and steerability of expression Many texttospeech TTS
systems rely on highly engineered features Klatt 1980
carefully curated sound segments Hunt  Black 1996
statistical parametric modeling Zen et al 2009 and of
ten complex pipelines as described in Arık et al 2017
These approaches are fairly involved and produce unnatural
or inarticulate voices More recent works like Deep V oice
3 Ping et al 2018 Tacotron 2 Shen et al 2018 and
Char2Wav Sotelo et al 2017 learn speech synthesis end
toend using sequencetosequence architecture Sutskever
et al 2014 The design space is vast but in general typical
approaches comprise of a bidirectional encoder a decoder
and a vocoder to build text representations audio features
and the ﬁnal raw waveforms To generate multiple voices
texttospeech models can also condition on the speaker
identity Oord et al 2016 Gibiansky et al 2017 Jia et al
2018 as well as text prompt By learning and manipulat
ing auxiliary embeddings models can mimic a new voice
Arık et al 2018a Taigman et al 2018 at test time These
methods however require labeled data Ideas like clus
tering Dehak et al 2011 priming Wang et al 2018
and variational autoencoders Hsu et al 2019 Akuzawa
et al 2018 have been used to learn broader styles of speech
and control expressivity in an unsupervised way There areJukebox A Generative Model for Music
also works on synthesizing singing by additionally con
trolling pitch and timbre Similar to TTS literature early
works use concatenative methods Bonada  Serra 2007
that join short segments of curated singing and statistical
parametric methods Saino et al 2006 Oura et al 2010
which allow modeling of timbre from training data Both
approaches impose fairly strong assumptions resulting in
noticeable artifacts Blaauw  Bonada 2017 train a neural
TTS model with a parametric vocoder to separate pitch and
timbre which can be controlled at generation time
7 Future work
While our approach represents a step forward in the ability
to generate coherent long raw audio music samples we rec
ognize several directions for future work Great music gen
eration should be high quality over all time scales it should
have a developing musical and emotional structure across
the entire piece local notes and harmonies that always make
sense nuanced and appropriate small timbral and textural
details and audio recording quality that balances and blends
the multiple voices well and without unwanted noise We
view our current model as stronger on the midrange time
scales often the model generates samples that locally sound
very good with interesting and diverse harmonies rhythms
instruments and voices We have frequently been very
impressed how the melody and rhythm generated suits a
particular lyric extremely well However while the samples
stay consistent over longer time scales we notice they dont
have traditional larger music structures such as choruses
that repeat or melodies that have a question and answer
form Additionally on the smallest scale we sometimes
hear audio noise or scratchiness
Beyond the quality of the samples we also would look
to diversify the languages and styles the model is able to
generate Our current model has been trained only on songs
whose primary language as detected by Sites 2013 is
English In the future we would look to include other
languages and artists We believe this will be of interest
both for generating strictly in those styles and because
historically we have seen much creativity and development
coming from unusual blends of existing musical styles
Finally we consider it very important that computer music
generation also serves as a tool for human musicians and
increasingly those interested in music but without formal
training While we are able to steer our current model some
what through lyric and midi conditioning we can imagine
many other possible ways for humans to inﬂuence the gener
ations including indicating the mood or dynamic at various
sections or controlling when drums singers or other instru
ments should playThe current model takes around an hour to generate 1 minute
of top level tokens The upsampling process is very slow
as it proceeds sequentially through the sample Currently it
takes around 8 hours to upsample one minute of top level
tokens We can create a humanintheloop cocomposition
process at the top level only using the VQV AE decoders
to get a fast upsampling of the top level tokens to hear a
very rough sense of what the model generates The toplevel
model generates multiple samples the person picks a fa
vorite listening to the rough VQV AE decoding and then
the model continues generating multiple samples continuing
the favorite This process would be signiﬁcantly improved
with faster generation and Transformer upsampling steps
Our models have fast parallel evaluation of likelihood but
slow autoregressive sampling We can instead use a model
with fast parallel sampling but slow autoregressive likeli
hood evaluation Kingma et al 2016 and distill the infor
mation from our current model into it Oord et al 2018
The distillation works by generating samples from the paral
lel sampler and evaluating it likelihood and entropy using
the parallel likelihood evaluator and then optimising the
sampler by minimising the KL divergence of it from our
current model
8 Conclusion
We have introduced Jukebox a model that generates raw
audio music imitating many different styles and artists We
can condition this music on speciﬁc artists and genres and
can optionally specify the lyrics for the sample We laid
out the details necessary to train a Hierarchical VQV AE to
compress the music effectively into tokens While previous
work has generated raw audio music in the 2030 second
range our model is capable of generating pieces that are
multiple minutes long and with recognizable singing in
naturalsounding voices
9 Acknowledgement
We would like to thank John Schulman and Will Guss for
producing and performing novel riffs for our sampling ex
periments and Rewon Child Aditya Ramesh Ryan Lowe
and Jack Clark for providing feedback for initial drafts of
this paper
References
Akuzawa K Iwasawa Y  and Matsuo Y  Expressive
speech synthesis via modeling expressions with varia
tional autoencoder In INTERSPEECH  2018
Anders T and Miranda E R Constraint programming
systems for modeling music theories and composition
ACM Computing Surveys CSUR  434138 2011Jukebox A Generative Model for Music
Arık S Ö Chrzanowski M Coates A Diamos G Gib
iansky A Kang Y  Li X Miller J Ng A Raiman
J Sengupta S and Shoeybi M Deep Voice Realtime
neural texttospeech In International Conference on
Machine Learning  pp 195204 2017
Arık S Ö Chen J Peng K Ping W and Zhou Y 
Neural voice cloning with a few samples In Advances
in Neural Information Processing Systems  pp 10019
10029 2018a
Arık S Ö Jun H and Diamos G Fast spectrogram
inversion using multihead convolutional neural networks
IEEE Signal Processing Letters  2619498 2018b
Ba J L Kiros J R and Hinton G E Layer normalization
arXiv preprint arXiv160706450  2016
Berner C Brockman G Chan B Cheung V  D ebiak P
Dennison C Farhi D Fischer Q Hashme S Hesse
C et al Dota 2 with large scale deep reinforcement
learning arXiv preprint arXiv191206680  2019
Beyls P The musical universe of cellular automata In
International Computer Music Conference  pp 3441
1989
Blaauw M and Bonada J A neural parametric singing
synthesizer In INTERSPEECH  2017
Bonada J and Serra X Synthesis of the singing voice by
performance sampling and spectral models IEEE signal
processing magazine  2426779 2007
Brock A Donahue J and Simonyan K Large scale
GAN training for high ﬁdelity natural image synthesis In
International Conference on Learning Representations 
2019
Brunner G Konrad A Wang Y  and Wattenhofer R
MIDIV AE modeling dynamics and instrumentation of
music with applications to style transfer In International
Society for Music Information Retrieval Conference  pp
747754 2018
Child R Gray S Radford A and Sutskever I Gen
erating long sequences with sparse transformers arXiv
preprint arXiv190410509  2019
De Fauw J Dieleman S and Simonyan K Hierarchi
cal autoregressive image models with auxiliary decoders
arXiv preprint arXiv190304933  2019
Dehak N Kenny P J Dehak R Dumouchel P and
Ouellet P Frontend factor analysis for speaker veriﬁca
tion IEEE Transactions on Audio Speech and Language
Processing  194788798 2011Dieleman S van den Oord A and Simonyan K The chal
lenge of realistic music generation modelling raw audio
at scale In Advances in Neural Information Processing
Systems  pp 79897999 2018
Dinh L Krueger D and Bengio Y  NICE Nonlinear in
dependent components estimation In International Con
ference in Learning Representations  Workshop 2015
Dinh L SohlDickstein J and Bengio S Density esti
mation using Real NVP In International Conference in
Learning Representations  2017
Donahue C Mao H H Li Y  E Cottrell G W and
McAuley J J LakhNES Improving multiinstrumental
music generation with crossdomain pretraining In In
ternational Society for Music Information Retrieval Con
ference  pp 685692 2019
Dong HW Hsiao WY  Yang LC and Yang Y H
MuseGAN Multitrack sequential generative adversarial
networks for symbolic music generation and accompani
ment In ThirtySecond AAAI Conference on Artiﬁcial
Intelligence  2018
Engel J Resnick C Roberts A Dieleman S Norouzi
M Eck D and Simonyan K Neural audio synthesis
of musical notes with wavenet autoencoders In Interna
tional Conference on Machine Learning  pp 10681077
2017
Engel J Agrawal K K Chen S Gulrajani I Donahue
C and Roberts A GANSynth Adversarial neural au
dio synthesis In International Conference on Learning
Representations  2019
Gibiansky A Arık S Ö Diamos G Miller J Peng K
Ping W Raiman J and Zhou Y  Deep Voice 2 Multi
speaker neural texttospeech In Advances in neural
information processing systems  pp 29622970 2017
Goodfellow I NIPS 2016 tutorial Generative adversarial
networks In Neural Information Processing Systems 
Tutorial 2016
Goodfellow I PougetAbadie J Mirza M Xu B
WardeFarley D Ozair S Courville A and Bengio
Y  Generative adversarial nets In Advances in neural
information processing systems  pp 26722680 2014
Gupta C Yılmaz E and Li H Automatic lyrics tran
scription in polyphonic music Does background music
help In International Conference on Acoustics Speech
and Signal Processing  2020
Hadjeres G Pachet F and Nielsen F Deepbach a steer
able model for bach chorales generation In International
Conference on Machine Learning  pp 13621371 JMLR
org 2017Jukebox A Generative Model for Music
Hawthorne C Stasyuk A Roberts A Simon I Huang
CZ A Dieleman S Elsen E Engel J and Eck D
Enabling factorized piano music modeling and generation
with the MAESTRO dataset In International Conference
on Learning Representations  2019
Hennequin R Khlif A V oituret F and Moussallam M
Spleeter A fast and stateofthe art music source separa
tion tool with pretrained models LateBreakingDemo
ISMIR 2019 November 2019 Deezer Research
Hiller Jr L A and Isaacson L M Musical composition
with a high speed digital computer In Audio Engineering
Society Convention 9  Audio Engineering Society 1957
Ho J Kalchbrenner N Weissenborn D and Salimans T
Axial attention in multidimensional transformers arXiv
preprint arXiv191212180  2019
Hsu WN Zhang Y  Weiss R J Zen H Wu Y  Wang
Y  Cao Y  Jia Y  Chen Z Shen J Nguyen P and
Pang R Hierarchical generative modeling for control
lable speech synthesis In International Conference on
Learning Representations  2019
Huang C A Cooijmans T Roberts A Courville A C
and Eck D Counterpoint by convolution In Interna
tional Society for Music Information Retrieval Confer
ence pp 211218 2017
Huang CZ A Vaswani A Uszkoreit J Shazeer N
Simon I Hawthorne C Dai A M Hoffman M D
Dinculescu M and Eck D Music Transformer Gen
erating music with longterm structure In International
Conference on Learning Representations  2019a
Huang Y  Cheng Y  Bapna A Firat O Chen D Chen
M Lee H Ngiam J Le Q V  Wu Y  et al Gpipe
Efﬁcient training of giant neural networks using pipeline
parallelism In Advances in Neural Information Process
ing Systems  pp 103112 2019b
Hunt A J and Black A W Unit selection in a con
catenative speech synthesis system using a large speech
database In IEEE International Conference on Acoustics
Speech and Signal Processing Conference  pp 373376
1996
Jehan T Creating music by listening  PhD thesis Mas
sachusetts Institute of Technology School of Architecture
and Planning Program in Media Arts and Sciences 2005
Jia Y  Zhang Y  Weiss R Wang Q Shen J Ren F
Chen z Nguyen P Pang R Lopez Moreno I and
Wu Y  Transfer learning from speaker veriﬁcation to
multispeaker texttospeech synthesis In Advances in
Neural Information Processing Systems  pp 44804490
2018Kalchbrenner N Elsen E Simonyan K Noury S
Casagrande N Lockhart E Stimberg F Oord A
Dieleman S and Kavukcuoglu K Efﬁcient neural au
dio synthesis In International Conference on Machine
Learning  pp 24102419 2018
Kim J W Bittner R Kumar A and Bello J P Neural
music synthesis for ﬂexible timbre control In IEEE In
ternational Conference on Acoustics Speech and Signal
Processing  pp 176180 2019
Kingma D P and Dhariwal P Glow Generative ﬂow
with invertible 1x1 convolutions In Advances in Neural
Information Processing Systems  pp 1021510224 2018
Kingma D P and Welling M Autoencoding variational
bayes In International Conference on Learning Repre
sentations  2014
Kingma D P Salimans T Jozefowicz R Chen X
Sutskever I and Welling M Improved variational in
ference with inverse autoregressive ﬂow In Advances in
neural information processing systems  pp 47434751
2016
Klatt D H Software for a cascadeparallel formant synthe
sizer Journal of the Acoustical Society of America  67
3971995 1980
Kumar K Kumar R de Boissiere T Gestin L Teoh
W Z Sotelo J de Brébisson A Bengio Y  and
Courville A C MelGAN Generative adversarial net
works for conditional waveform synthesis In Advances
in Neural Information Processing Systems  pp 14881
14892 2019
LyricWiki URL httpslyricsfandomcom
wikiLyricWiki 
Mehri S Kumar K Gulrajani I Kumar R Jain S
Sotelo J Courville A and Bengio Y  SampleRNN An
unconditional endtoend neural audio generation model
InInternational Conference on Learning Representations 
2017
Moorer J A Music and computer composition Communi
cations of the ACM  152104113 1972
Mor N Wolf L Polyak A and Taigman Y  Autoencoder
based music translation In International Conference on
Learning Representations  2019
Oord A v d Dieleman S Zen H Simonyan K
Vinyals O Graves A Kalchbrenner N Senior A
and Kavukcuoglu K WaveNet A generative model for
raw audio arXiv preprint arXiv160903499  2016Jukebox A Generative Model for Music
Oord A v d Vinyals O and Kavukcuoglu K Neural
discrete representation learning In Neural Information
Processing Systems  2017
Oord A v d Li Y  Babuschkin I Simonyan K Vinyals
O Kavukcuoglu K van den Driessche G Lockhart E
Cobo L Stimberg F Casagrande N Grewe D Noury
S Dieleman S Elsen E Kalchbrenner N Zen H
Graves A King H Walters T Belov D and Hassabis
D Parallel WaveNet Fast highﬁdelity speech synthesis
InInternational Conference on Machine Learning  pp
39183926 2018
Oura K Mase A Yamada T Muto S Nankaku Y 
and Tokuda K Recent development of the HMMbased
singing voice synthesis system  Sinsy 2010
Papamakarios G Pavlakou T and Murray I Masked
autoregressive ﬂow for density estimation In Advances in
Neural Information Processing Systems  pp 23382347
2017
Payne C Musenet OpenAI blog  2019 URL https
openaicomblogmusenet 
Ping W Peng K Gibiansky A Arik S O Kannan
A Narang S Raiman J and Miller J Deep Voice
3 2000speaker neural texttospeech In International
Conference on Learning Representations  2018
Ping W Peng K and Chen J Clarinet Parallel wave
generation in endtoend texttospeech In International
Conference on Learning Representations  2019
Prenger R Valle R and Catanzaro B WaveGlow A
ﬂowbased generative network for speech synthesis In
IEEE International Conference on Acoustics Speech and
Signal Processing  pp 36173621 2019
Pressing J Nonlinear maps as generators of musical design
Computer Music Journal  1223546 1988
Radford A Wu J Child R Luan D Amodei D and
Sutskever I Language models are unsupervised multitask
learners
Razavi A van den Oord A and Vinyals O Generating
diverse highﬁdelity images with vqvae2 In Advances
in Neural Information Processing Systems  pp 14837
14847 2019
Rezende D and Mohamed S Variational inference with
normalizing ﬂows In International Conference on Ma
chine Learning  pp 15301538 2015
Rezende D J Mohamed S and Wierstra D Stochastic
backpropagation and approximate inference in deep gen
erative models In International Conference on Machine
Learning  pp 12781286 2014Roberts A Engel J Raffel C Hawthorne C and Eck
D A hierarchical latent vector model for learning long
term structure in music In International Conference on
Machine Learning  pp 43644373 2018
Saino K Zen H Nankaku Y  Lee A and Tokuda
K An HMMbased singing voice synthesis system In
INTERSPEECH  2006
Salakhutdinov R and Hinton G Deep boltzmann machines
InArtiﬁcial intelligence and statistics  pp 448455 2009
Shen J Pang R Weiss R J Schuster M Jaitly N
Yang Z Chen Z Zhang Y  Wang Y  SkerrvRyan
R Saurous R A Agiomvrgiannakis Y  and Wu Y 
Natural TTS synthesis by conditioning wavenet on mel
spectrogram predictions In IEEE International Confer
ence on Acoustics Speech and Signal Processing  pp
47794783 2018
Sites D Compact language detector 2 2013 URL https
githubcomCLD2Ownerscld2 
Sotelo J Mehri S Kumar K Santos J F Kastner K
Courville A C and Bengio Y  Char2Wav Endto
end speech synthesis In International Conference on
Learning Representations  2017
Sturmel N and Daudet L Signal reconstruction from stft
magnitude A state of the art International Conference
on Digital Audio Effects DAFx  2011
Sutskever I Vinyals O and Le Q V  Sequence to se
quence learning with neural networks In Advances in
neural information processing systems  pp 31043112
2014
Taigman Y  Wolf L Polyak A and Nachmani E
V oiceLoop V oice ﬁtting and synthesis via a phonological
loop In International Conference on Learning Represen
tations  2018
Uria B Côté MA Gregor K Murray I and
Larochelle H Neural autoregressive distribution esti
mation The Journal of Machine Learning Research  17
171847220 2016
Van den Oord A Kalchbrenner N Espeholt L Vinyals
O Graves A et al Conditional image generation with
pixelcnn decoders In Advances in neural information
processing systems  pp 47904798 2016
Vasquez S and Lewis M MelNet A generative model
for audio in the frequency domain arXiv preprint
arXiv190601083  2019
Vaswani A Shazeer N Parmar N Uszkoreit J Jones
L Gomez A N Kaiser Ł and Polosukhin I Atten
tion is all you need In Advances in Neural Information
Processing Systems  pp 59986008 2017Jukebox A Generative Model for Music
Wang Y  Stanton D Zhang Y  SkerryRyan R Batten
berg E Shor J Xiao Y  Ren F Jia Y  and Saurous
R A Style Tokens Unsupervised style modeling control
and transfer in endtoend speech synthesis In Interna
tional Conference on Machine Learning  2018
Wu J Hu C Wang Y  Hu X and Zhu J A hierarchical
recurrent neural network for symbolic melody generation
IEEE Transactions on Cybernetics  2019
Xie S Girshick R Dollár P Tu Z and He K Aggre
gated residual transformations for deep neural networks
InIEEE Conference on Computer Vision and Pattern
Recognition  pp 14921500 2017
Yamamoto R Song E and Kim JM Parallel Wave
GAN A fast waveform generation model based on gener
ative adversarial networks with multiresolution spectro
gram In International Conference on Acoustics Speech
and Signal Processing  2020
Yang L Chou S and Yang Y  Midinet A convolutional
generative adversarial network for symbolicdomain mu
sic generation In International Society for Music Infor
mation Retrieval Conference  pp 324331 2017
Zen H Tokuda K and Black A W Review Statistical
parametric speech synthesis Speech Communication  51
1110391064 2009
Zhang H Dauphin Y  N and Ma T Fixup initialization
Residual learning without normalization In International
Conference on Machine Learning  2019a
Zhang H Goodfellow I Metaxas D and Odena A
Selfattention generative adversarial networks In Inter
national Conference on Machine Learning  2019bJukebox A Generative Model for Music
A Scalable Transformer
We make the Sparse Transformer Child et al 2019 more
scalable and easier to implement by a few small changes
We implement a simpler attention pattern that has the same
performance without needing custom kernels to implement
We simplify the initialization by using the same initalization
scale in the whole model without rescaling the weights
based on fanin and depth and we optimize the memory
footprint with fully halfprecision training ie storing the
model weights gradients and the optimizer states in half
precision and performing computations in half precision as
well To cope with the narrower dynamic range of the fp16
format we use dynamic scaling of the gradient and Adam
optimizer states
Axisaligned attention patterns The Sparse Transformer
Child et al 2019 sparsiﬁes the attention pattern by
reshaping the input sequence into a 2D sequence of
shape blocks block length to use factorized attention
They observe that the strided attention pattern works
best for images and audio because it does not have the
state bottleneck of the ﬁxed attention However their
implementation require specialized CUDA kernels We
can obtain a similar pattern by doing masked row masked
column and unmasked previousrow attention While
the masked row captures the local context the masked
column and unmasked previousrow attention captures
the context of all previous rows We observe the same
computational speed as well as training loss with this
pattern Each of these can be implemented directly as a
dense attention by transposing or slicing the input sequence
along appropriate axes and thus do not require special
CUDA kernels to implement This can be easily extended
to video too Complementary to our work a similar
pattern was introduced in Ho et al 2019 where they also
used axisaligned attention but instead used a twostream
architecture
Halfprecision parameters and optimizer state with dy
namic scaling To allow training large models Child et al
2019 uses recompute with gradient checkpointing per
forms computations using half precision activations and
gradients and uses dynamic loss scaling While this speeds
up training on V olta cores one still has a high memory us
age from storing the parameters and Adam state in full ﬂoat
precision To scale our models further we store our matmul
parameters and their Adam state in half precision thus halv
ing our memory usage We use a single parameter sto set the
scale of all weights and initialize all matmul and inputout
put embeddings3toN0s and position embeddings to
N02s The initialization ensures all parameters are in a
similar dynamic range and allows us to train in half preci
3We share the input and output embedding
Masked
Row AttentionMasked
Column AttentionUnmasked
PreviousRow Attentiona Three axisaligned attention patterns are sparse attention pat
terns that allow autoregressive generative modeling while only
using simple Pythonlevel array manipulation Masked row and
column attention patterns use autoregressive masks whereas un
masked previousrow attention is fully visible
        
b Combining two of the attention patterns each position can
attend to any of the previous positions while not causing a state
bottleneck as in ﬁxed sparse attention Child et al 2019
Figure 6 Axisaligned attention patterns
sion completely without loss in training performance For
the Adam state tensors m_t v_t we do dynamic scal
ing For each iteration and for every parameter we rescale
its state tensors before casting so that their maximum corre
sponds to the maximum value of the ﬂoat16 range thus max
imizing the use of the ﬂoat16 range Thus we store the state
m_t as the tuple scale m_tscalehalf 
where scale  m_tmaxfloat16max  and
similarly for v_t The above lets us ﬁt models of size 1B
parameters into memory for our large context of 8192 to
kens To train even larger models we use GPipe Huang
et al 2019bJukebox A Generative Model for Music
B Experimental details
B1 Music VQV AE
We have three separate raw audio VQV AEs to produce dis
crete codes at varying hop sizes for the bottom middle and
top priors All autoencoders comprise noncausal dilated
1D convolutions and are trained independently using non
autoregressive reconstruction losses Basic building blocks
in these networks share the same architecture as shown in
Figure 7 Each encoder block consists of a downsampling
convolution a residual network and a 1D convolution with
a kernel size of 3 Dilation is grown by a factor of 3 in
these residual networks to increase the receptive ﬁeld The
decoder block mirrors this exactly with a 1D convolution
with the kernel size of 3 a residual network with dilation
contracting across depth and an upsampling transposed con
volution Here all resampling convolutions use a kernel size
of 4 and stride 2 so that each building block changes the
hop length by a factor of 2 To get higher compression in
time we simply stack more of these blocks For example
using seven blocks yields a hop length of 128 for the top
level autoencoder
Each residual network has four residual blocks in the mid
dle and top VQV AEs resulting in a receptive ﬁeld of 120
ms and 480 ms for the respective discrete tokens Because
increasing the residual depth helped improve reconstruction
quality slightly we doubled the number of residual blocks
for the bottom level This dramatically increases the recep
tive ﬁeld to about 2 seconds per code but the actual receptive
ﬁeld is mostly local
We also experimented with having a single decoder and
modeling the residuals to separate out learned representa
tions as in Razavi et al 2019 hoping upsampling priors
would simply ﬁll in local musical structure However push
ing information to the top level was quite challenging as the
bottommost level reconstructs almost perfectly early on in
training When we add auxiliary objectives to encourage
the top to be used more the toplevel codes add serious
distortions to the ﬁnal output A similar challenge is shown
in Dieleman et al 2018
B2 Music Priors and Upsamplers
Architectural details of our music prior and upsampler mod
els are depicted in Figure 8 They perform autoregressive
modeling of tokens at each level conditioned on informa
tion such as artist and genre as well as the tokens from the
upper level in the case of the upsamplers Figure 8a Each
artist and genre are learned as embedding vectors whose
sum is provided as the very ﬁrst token in each sequence
In addition positional embedding is learned as a function
of each positions absolute and relative timing in the dura
tion of the song In upsampler models upperlevel tokens
L
Conv1DDDilated
Conv1DConv1D
 x t h ta The encoder compresses the raw audio input into a sequence
of embeddings The length of this latent representation relative
to the raw audio duration determines the amount of compression
and is an important factor for the tradeoff between ﬁdelity and
coherence
Gradient PassthroughNearestNeighbor
Searchz tCodebook
h t e z tCodebook
Lookup
b The bottleneck takes the sequence of embeddings from the
encoder and maps it into a sequence of code vectors from the
codebook This sequence of code indices is used as a discrete
representation to be modeled by the priors Larger codebooks
improve ﬁdelity but may be more difﬁcult to compress
Conv1DL
DDilated
Conv1DConv1D
T ransposed
Conv1De z t x t
c The decoder reconstructs the raw audio from latent represen
tations It is a mirror of the encoder where dilations constracts
by a factor of 3 down to 1 at the last block The ﬁnal Conv1D
projects to the desired number of audio channels and also acts as a
smoothing operation after a sequence of transposed convolutions
Figure 7 Components of the VQV AE model
are upsampled by the conditioner network using WaveNet
style dilated convolutions followed by a transposed 1D
convolutional layer Figure 8b
When the model is trained on lyrics the toplevel prior takes
lyrics data corresponding to each audio segment and uses
them to train an encoderdecoder Transformer as shown in
Figure 8c All transformer stacks use sparse selfattention
layers with the three factorized attention types row column
and previousrow repeating and encoderdecoder attention
layers when present are interleaved with the other attention
types Each layer consists of residual connections of an
attention and an MLP feedforward network each prepended
by layer normalization see Figure 8dJukebox A Generative Model for Music
Artist  GenreConditioner
z 1 TScalable T ransformerUpperLevel T okens
Time Embedding 
LyricsTiming DataNot Used in the T op Level
							z 1 T 1
a The structure of our prior models performing nexttoken prediction at each
level The Transformer takes the embeddings of the tokens z1T1prepended by
the sum of the artist and genre embeddings in addition to the time embedding
that encodes relative and absolute timing of the segments in the duration of the
song The upsampler priors additionally take the tokens from the upper level
which are fed to the conditioner network and added to the input sequence The
toplevel prior takes lyrics as conditioning information as well see Figure 8c
D
Dilated Conv1D
Conv1D

Transposed Conv1DToken Embeddingb The conditioner network takes the tokens from
the upper level and their embedding vectors go
through noncausal WaveNetlike layers with in
creasingly dilated convolutions The transposed 1D
convolution upsamples the sequence to the higher
temporal resolution of the current level
Lyrics
Row Attention Layer
Column Attention Layer
PreviousRow Attention Layer
Row Attention Layer
Column Attention Layer
PreviousRow Attention Layer

Row Attention Layer
Column Attention Layer
PreviousRow Attention LayerLyrics T oken Embedding
NextT oken PredictionLyrics T oken EmbeddingRow Attention Layer
Column Attention Layer
PreviousRow Attention LayerVQ Code Embedding
NextT oken PredictionVQ Code EmbeddingVQ Codes
6
Row Attention Layer
Column Attention Layer
PreviousRow Attention Layer3
EncoderDecoder Attention Layer
EncoderDecoder Attention Layer
Row Attention Layer
Column Attention Layer
PreviousRow Attention LayerEncoderDecoder Attention Layer
3Only in the T op Level
EncoderDecoder Attention Layer
c The Scalable Transformer architecture shown with the lyrics Transformer used in the
toplevel prior The Transformer layers use the three factorized attention types alternatingly
ie repeating row column and previousrow attentions In the toplevel prior the VQ
Transformer additionally includes interleaved encoderdecoder attention layers that apply
lyrics conditioning by attending on the activation of the last encoder layer
Layer Norm
Attention
Layer Norm
MLP
Encoder
Featuresd Each Transformer layer is a resid
ual attention block which performs
two residual operations attention and
MLP each prepended with layer nor
malization Depending on the layers
type it uses either one of the three fac
torized attentions or encoderdecoder
attention taking the lyrics features
from the encoder
Figure 8 Detailed architecture of the music prior and upsampler modelsJukebox A Generative Model for Music
B3 Hyperparameters
For all Transformers residual blocks we use MLP blocks
with the same width as the model width and attention blocks
with queries keys and values with width 025 times the
model width For all convolutional residual blocks we use
convolutions with same channels as the model width
Sample rate 44100
Sample length 393216
Hop lengths 8 32 128
Embedding width 64
Residual block width 64 32 32
Residual blocks per 2x downsample 8 4 4
Conv ﬁlter size 3
Conv channels 32
Dilation growth rate 3
Commit weight  002
Codebook EMA  099
Codebook size 2048
Spectral loss STFT bins 2048 1024 512
Spectral loss STFT hop length 240 120 50
Spectral loss STFT window size 1200 600 240
Initialization scale 002
Batch size 256
Training steps 384618
Learning rate 00003
Table 4 VQV AE hyperparameters
1B upsamplers
Sample length 262144 65536
Context length 8192
Transformer width 1920
Transformer layers 72
Attention heads 1
Factorized attention shape 128 64
Conditioner residual block width 1024
Conditioner residual blocks 16
Conditioner conv ﬁlter size 3
Conditioner conv channels 1024
Conditioner dilation growth rate 3
Conditioner dilation cycle 8
Initialization scale 0004 0008
Batch size 192 184
Training steps 265000 279000
Learning rate 00003
Adam2 095
Weight decay 001
Table 5 Middle and bottomlevel upsampler hyperparameters5B prior
Sample length 1048576
Context length 8192
Transformer width 4800
Transformer selfattention layers 72
Attention heads 8
Factorized attention shape 128 64
Lyrics encoder tokens 512
Lyrics encoder width 1280
Lyrics encoder layers 18
Lyrics encoder attention heads 4
Lyrics encoder factored attention shape 32 16
EncoderDecoder attention layers 7
Initialization scale 0002
Encoder initialization scale 0014
Batch size 512
Training steps 310500
Learning rate 000015
Adam2 0925
Weight decay 0002
Table 6 Toplevel prior hyperparametersJukebox A Generative Model for Music
B4tSNE Plot of Artists
The Beatles
George Harrison
Paul McCartney
John Lennon
Ringo Starr
Cheap Trick
Aerosmith
QueenStatus QuoELORod StewartFleetwood Mac
Neil Young
Eagles
Pearl JamThe Beatles
George Harrison
Paul McCartney
John Lennon
Ringo Starr
Cheap Trick
Aerosmith
QueenStatus QuoELORod StewartFleetwood Mac
Neil Young
Eagles
Pearl JamArianaGrande
PnkEdSheeranTheWeeknd
TheBeatles
PinkFloyd
LinkinParkTheBeachBoysBuckOwens
EddyArnold
BillyRayCyrusShaggySeanPaulJimmyCliffBarringtonLevy
DeanMartin
MartyRobbinsHowardShoreTonyBennett
NeilDiamondCabCalloway
HenryMancini
VanMorrisonDinahWashingtonFatsDomino
HankSnow
TheSmashingPumpkinsRaminDjawadiNinaSimoneDonnaSummer
TheCureLouRawls
MigosVangelisNatalieCole
TPain
KanyeWestBessieSmithRayNoble
BobbyBlandTheMillsBrothers
AkonLouisPrima
OSTRLonnieJohnson
AndreaBocelliThePlattersBarryWhite
LutherVandross
YannTiersen
FranzSchubert
JohannSebastianBachGlennGould
YoYoMaGarrickOhlssonWalterGieseking
PopRock
ReggaeRBRB Soul
Hip Hop
 CountryBlues
ClassicalJazz
Soundtrack
Figure 9 tSNE of artist genre embedding The overall clustering shows very clearly how genres are related to one another The
broadest of all pop is situated in the middle of rock country blues hip hop and many more Soundtrack and classical form their own
island Within a genre we see a similar trend among artists John Lennon Paul McCartney George Harrison and Ringo Starr are clustered
around The Beatles Cheap Trick which has a number of Beatles covers is also found near Because we are showing only about 400 artists
here not all neighboring artists may be related For an interactive version we point to our blog post
  Preprint Under review
Leave No Context Behind
Efficient Infinite Context Transformers with Infiniattention
Tsendsuren Munkhdalai Manaal Faruqui and Siddharth Gopal
Google
tsendsurengooglecom
Abstract
This work introduces an efficient method to scale Transformerbased Large
Language Models LLMs to infinitely long inputs with bounded memory
and computation A key component in our proposed approach is a new at
tention technique dubbed Infiniattention The Infiniattention incorporates
a compressive memory into the vanilla attention mechanism and builds
in both masked local attention and longterm linear attention mechanisms
in a single Transformer block We demonstrate the effectiveness of our
approach on longcontext language modeling benchmarks 1M sequence
length passkey context block retrieval and 500K length book summarization
tasks with 1B and 8B LLMs Our approach introduces minimal bounded
memory parameters and enables fast streaming inference for LLMs
1 Introduction
Memory serves as a cornerstone of intelligence as it enables efficient computations tailored
to specific contexts However Transformers Vaswani et al 2017 and Transformerbased
LLMs Brown et al 2020 Touvron et al 2023 Anil et al 2023 Groeneveld et al 2024 have
a constrained contextdependent memory due to the nature of the attention mechanism
Update 
VVConcat Concat 
Q V
Q V
QsKVsCompressive memory  
Linear attention Causal scaled dotproduct 
attention  PE Linear 
projection 
KVs1Retrieve 
Figure 1 Infiniattention has an addi
tional compressive memory with linear
attention for processing infinitely long
contexts KVs1andKVsare atten
tion key and values for current and previ
ous input segments respectively and Qs
the attention queries PE denotes position
embeddingsThe attention mechanism in Transformers ex
hibits quadratic complexity in both memory
footprint and computation time For example
the attention KeyValue KV states have 3TB
memory footprint for a 500B model with batch
size 512 and context length 2048 Pope et al
2023 Indeed scaling LLMs to longer sequences
ie 1M tokens is challenging with the standard
Transformer architectures and serving longer
and longer context models becomes costly finan
cially
Compressive memory systems promise to be
more scalable and efficient than the attention
mechanism for extremely long sequences Kan
erva 1988 Munkhdalai et al 2019 Instead
of using an array that grows with the input se
quence length a compressive memory primarily
maintains a fixed number of parameters to store
and recall information with a bounded storage
and computation costs In the compressive mem
ory new information is added to the memory
by changing its parameters with an objective
that this information can be recovered back later
on However the LLMs in their current state
have yet to see an effective practical compres
sive memory technique that balances simplicity along with quality
1arXiv240407143v1  csCL  10 Apr 2024Preprint Under review
In this work we introduce a novel approach that enables Transformer LLMs to effectively
process infinitely long inputs with bounded memory footprint and computation A key
component in our proposed approach is a new attention technique dubbed Infiniattention
Figure 1 The Infiniattention incorporates a compressive memory into the vanilla attention
mechanism Bahdanau et al 2014 Vaswani et al 2017 and builds in both masked local
attention and longterm linear attention mechanisms in a single Transformer block
Such a subtle but critical modification to the Transformer attention layer enables a natural
extension of existing LLMs to infinitely long contexts via continual pretraining and fine
tuning
Our Infiniattention reuses all the key value and query states of the standard attention
computation for longterm memory consolidation and retrieval We store old KV states of
the attention in the compressive memory instead of discarding them like in the standard
attention mechanism We then retrieve the values from the memory by using the attention
query states when processing subsequent sequences To compute the final contextual
output the Infiniattention aggregates the longterm memoryretrieved values and the local
attention contexts
In our experiments we show that our approach outperforms baseline models on long
context language modeling benchmarks while having 114x comprehension ratio in terms of
memory size The model achieves even better perplexity when trained with 100K sequence
length A 1B LLM naturally scales to 1M sequence length and solves the passkey retrieval
task when injected with Infiniattention Finally we show that a 8B model with Infini
attention reaches a new SOTA result on a 500K length book summarization task after
continual pretraining and task finetuning
In summary our work makes the following contributions
1We introduce a practical and yet powerful attention mechanism  Infiniattention
with longterm compressive memory and local causal attention for efficiently mod
eling both long and shortrange contextual dependencies
2Infiniattention introduces minimal change to the standard scaled dotproduct atten
tion and supports plugandplay continual pretraining and longcontext adaptation
by design
3Our approach enables Transformer LLMs to scale to infinitely long context with a
bounded memory and compute resource by processing extremely long inputs in a
streaming fashion
2 Method
Figure 2 compares our model InfiniTransformer and TransformerXL Dai et al 2019
Similar to TransformerXL InfiniTransformer operates on a sequence of segments We
compute the standard causal dotproduct attention context within each segment So the
dotproduct attention computation is local in a sense that it covers a total Nnumber of
tokens of the current segment with index SNis the segment length
The local attention Dai et al 2019 however discards the attention states of the previous
segment when processing the next one In InfiniTransformers instead of leaving out the
old KV attention states we propose to reuse them to maintain the entire context history
with a compressive memory So each attention layer of InfiniTransformers has both global
compressive and local finegrained states We call such an efficient attention mechanism
Infiniattention which is illustrated in Figure 1 and described formally in the following
sections
21 Infiniattention
As shown Figure 1 our Infiniattention computes both local and global context states and
combine them for its output Similar to multihead attention MHA it maintains Hnumber
2Preprint Under review
Segment 1 Segment 2 Segment 3 
Segment 1 Segment 2 Segment 3 Transformer block InfiniT ransformer 
T ransformerXL Compressive memory 
Memory update 
Memory retrieval 
Effective context 
Input segment Segment 1 
Figure 2 InfiniTransformer top has an entire context history whereas TransformerXL
bottom discards old contexts since it caches the KV states for the last segment only
of parallel compressive memory per attention layer  His the number of attention heads in
addition to the dotproduct attention
211 Scaled Dotproduct Attention
The multihead scaled dotproduct attention Vaswani et al 2017 specially its selfattention
variant Munkhdalai et al 2016 Cheng et al 2016 has been the main building block in
LLMs The MHAs strong capability to model contextdependent dynamic computation and
its conveniences of temporal masking have been leveraged extensively in the autoregressive
generative models
A single head in the vanilla MHA computes its attention context AdotI RNdvaluefrom
sequence of input segments XI RNdmodel as follows First it computes attention query
key and value states
KXW KVXW Vand QXW Q 1
Here WKI RdmodeldkeyWVI Rdmodeldvalueand WQI Rdmodeldkeyare trainable projection
matrices Then the attention context is calculated as a weighted average of all other values
as
AdotsoftmaxQKT
dmodel
V 2
For MHA we compute Hnumber of attention context vectors for each sequence element
in parallel concatenate them along the second dimension and then finally project the
concatenated vector to the model space to obtain attention the output
212 Compressive Memory
In Infiniattention instead of computing new memory entries for compressive memory we
reuse the query key and value states  QKand V from the dotproduct attention compu
tation The state sharing and reusing between the dotproduct attention and compressive
memory not only enables efficient pluginplay longcontext adaptation but also speeds up
training and inference Similar to the prior work Munkhdalai et al 2019 our goal is to
store bindings of key and value states in the compressive memory and retrieve by using the
query vectors
3Preprint Under review
While there are different forms of compressive memory proposed in the literature Hop
field 1982 Kanerva 1988 Schlag et al 2019 Munkhdalai et al 2019 for simplicity and
computational efficiency in this work we parameterize the memory with an associative
matrix Schlag et al 2020 This approach further allows us to cast the memory update
and retrieval process as linear attention mechanism Shen et al 2018 and to leverage
stable training techniques from the related methods Specially we adopt the update rule
and retrieval mechanism by Katharopoulos et al 2020 mainly due to its simplicity and
competitive performance
Memory retrieval In Infiniattention we retrieve new content AmemI RNdvaluefrom the
memory Ms1I Rdkeydvalueby using the query QI RNdkeyas
AmemσQMs1
σQzs1 3
Here σand zs1I Rdkeyare a nonlinear activation function and a normalization term
respectively As the choice of the nonlinearity and the norm method is crucial for training
stability following Katharopoulos et al 2020 we record a sum over all keys as the normal
ization term zs1and use elementwise ELU  1 as the activation function Clevert et al
2015
Memory update Once the retrieval is done we update the memory and the normalization
term with the new KV entries and obtain the next states as
MsMs1σKTVand zszs1N

t1σKt 4
The new memory states Msand zsare then passed to the next segment S1 building in
a recurrence in each attention layer The right side term σKTVin Eq 4is known as an
associative binding operator Smolensky 1990 Hebb 2005 Schlag et al 2020
Inspired by the success of delta rule Munkhdalai et al 2019 Schlag et al 2020 2021
we have also incorporated it into our Infiniattention The delta rule attempts a slightly
improved memory update by first retrieving existing value entries and subtracting them
from the new values before applying the associative bindings as new update
MsMs1σKTVσKMs1
σKzs1 5
This update rule  Linear Delta  leaves the associative matrix unmodified if the KV binding
already exists in the memory while still tracking the same normalization term as the former
one  Linear  for numerical stability
Longterm context injection We aggregate the local attention state Adotand memory
retrieved content Amemvia a learned gating scalar β
Asigmoid βAmem 1sigmoid βAdot 6
This adds only a single scalar value as training parameter per head while allowing a
learnable tradeoff between the longterm and local information flows in the model Wu
et al 2022
Similar to the standard MHA for the multihead Infiniattention we compute Hnumber of
context states in parallel and concatenate and project them for the final attention output
OI RNdmodel
O A1   AHWO 7
where WOI RHdvaluedmodelis trainable weights
22 Memory and Effective Context Window
Our InfiniTransformer enables an unbounded context window with a bounded memory
footprint To illustrate this Table 1 lists the previous segmentlevel memory models with
4Preprint Under review
Model Memory cache footprint Context length Memory update Memory retrieval
TransformerXL dkeydvalueHNl N l Discarded Dotproduct attention
Compressive Transformer dmodelcNl crNl Discarded Dotproduct attention
Memorizing Transformers dkeydvalueHNS N S None kNN  dotproduct attention
RMT dmodelpl2 NS Discarded Softprompt input
AutoCompressors dmodelpm1l N S Discarded Softprompt input
InfiniTransformers dkeydvalue1Hl N S Incremental Linear attention
Table 1 Transformer models with segmentlevel memory are compared For each model the
memory size and effective context length are defined in terms of their model parameters  N
input segment length S the number of segments l the number of layers H the number
of attention heads c Compressive Transformer memory size r compression ratio p the
number of softprompt summary vectors and m summary vector accumulation steps
their contextmemory footprint and effective context length defined in terms of model
parameters and input segment length InfiniTransformer has a constant memory complexity
ofdkeydvaluedkeyfor storing compressed context in Msand zsfor each head in single
layer while for the other models the complexity grows along with the sequence dimension
 the memory complexity depends either on the cache size for TransformerXL Dai et al
2019 Compressive Transformer Rae et al 2019 and Memorizing Transformers Wu et al
2022 or on the softprompt size for RTM Bulatov et al 2022 and AutoCompressors Ge
et al 2023
Early 
layers 
Attention heads 
Figure 3 There are two types of heads
emerged in Infiniattention after training spe
cialized heads with gating score near 0 or
1 and mixer heads with score close to 05
The specialized heads either process contex
tual information via the local attention mech
anism or retrieve from the compressive mem
ory whereas the mixer heads aggregate both
current contextual information and longterm
memory content together into single outputTransformerXL computes attention over KV
states cached from the last segment in addition
to the current states Since this is done for each
layer TransformerXL extends the context win
dow from NtoNltokens with an additional
memory footprint of dkeydvalueHNl
Compressive Transformer adds a second cache
to TransformerXL and stores compressed rep
resentations of past segment activations So it
extends the TransformerXLs context window
bycrlbut still has a large contextmemory
complexity Taking the idea further Memoriz
ing Transformers opt to store the entire KV states
as context for input sequences Since the stor
age becomes prohibitively expensive in this case
they restrict the contextual computation to a sin
gle layer only By utilizing a fast kNN retriever
Memorizing Transformers then build a context
window covering the entire sequence history of
length NSat an increased cost of storage Our
experiments show that InfiniTransformer LM
can achieve more than 100x compression rate on
top of Memorizing Transformers while further
improving the perplexity score
RMT and AutoCompressors allow for a poten
tially infinite context length since they compress
the input into summary vectors and then pass
them as extra softprompt inputs for the subsequent segments However in practice the
success of those techniques highly depends on the size of softprompt vectors Namely it
is necessary to increase the number of softprompt summary vectors to achieve a better
performance with AutoCompressors Chevalier et al 2023 and with that the memory and
compute complexity grow quickly resulting in diminished efficiency It was also observed in
AutoCompressors Chevalier et al 2023 that an efficient compression objective is needed
for training such prompt compression techniques Ge et al 2023
5Preprint Under review
Model Memory size comp XL cache Segment length PG19 Arxivmath
TransformerXL 50M 37x 2048 2048 1188 242
Memorizing Transformers 183M 1x 2048 2048 1137 226
RMT 25M 73x None 2048 1327 255
InfiniTransformer Linear 16M 114x None 2048 965 224
InfiniTransformer Linear  Delta 16M 114x None 2048 967 223
Table 2 Longcontext language modeling results are compared in terms of average token
level perplexity Comp denotes compression ratio InfiniTransformer outperforms memo
rizing transformers with memory length of 65K and achieves 114x compression ratio
3 Experiments
We evaluated our InfiniTransformer models on benchmarks involving extremely long input
sequences longcontext language modeling 1M length passkey context block retrieval
and 500K length book summarization tasks For the language modeling benchmark we
train our models from scratch while for the passkey and book summarization tasks we
continually pretrain existing LLMs in order to highlight a plugandplay longcontext
adaptation capability of our approach
31 Longcontext Language Modeling
We trained and evaluated small InfiniTransformer models on PG19 Rae et al 2019 and
Arxivmath Wu et al 2022 benchmarks Our setup closely resembles that of Memorizing
Transformers Wu et al 2022 Namely all our models have 12 layers and 8 attention heads
of dimension 128 each and FFNs with hidden layer 4096
We set the Infiniattention segment length Nto 2048 for all attention layers and the input
sequence length to 32768 for training This allows the Infiniattention to unroll over 16 steps
wrt its compressive memory states For the RMT baseline we performed several runs with
summary prompt lengths 50 100 and 150 and sequence lengths 4096 8196 and 32768 RMT
with 100 summary vectors gave the best result when trained on 8196 length sequences
The main results from the language modeling experiments are summarized in Table 2 Our
InfiniTransformer outperforms both TransformerXL Dai et al 2019 and Memorizing
Transformers Wu et al 2022 baselines while maintaining 114x less memory parameters
than the Memorizing Transformer model with a vector retrievalbased KV memory with
length of 65K at its 9thlayer
100K length training We further increased the training sequence length to 100K from
32K and trained the models on Arxivmath dataset 100K training further decreased the
perplexity score to 221 and 220 forLinear and Linear Delta models
Gating score visualization Figure 3 visualizes the gating score sigmoid βfor the compres
sive memory for all attention heads in each layer There are two types of heads emerged in
Infiniattention after training specialized heads with a gating score near 0 or 1 and mixer
heads with a score close to 05 The specialized heads either process contextual information
via the local attention computation or retrieve from the compressive memory whereas the
Zeroshot
32K 128K 256K 512K 1M
InfiniTransformer Linear 141398 1114100 63100 6799 8698
InfiniTransformer Linear  Delta 131199 6999 7599 6897 7697
FT 400 steps
InfiniTransformer Linear 100100100 100100100 100100100 9799100 9694100
InfiniTransformer Linear  Delta 100100100 10010099 10010099 100100100 100100100
Table 3 InfiniTransformers solved the passkey task with up to 1M context length when
finetuned on 5K length inputs We report tokenlevel retrieval accuracy for passkeys hidden
in a different part  startmiddleend  of long inputs with lengths 32K to 1M
6Preprint Under review
Model Rouge1 Rouge2 RougeL Overall
BART 364 76 153 162
BART  Unlimiformer 368 83 157 169
PRIMERA 386 72 156 163
PRIMERA  Unlimiformer 379 82 163 172
InfiniTransformers Linear 379 87 176 180
InfiniTransformers Linear  Delta 400 88 179 185
Table 4 500K length book summarization BookSum results The BART PRIMERA and
Unlimiformer results are from Bertsch et al 2024
mixer heads aggregate both current contextual information and longterm memory content
together into a single output Interestingly each layer has at least a single shortrange
head allowing a forwardpropagation of input signal up until the output layer We also
observed an interleaving of long and shortterm content retrievals throughout the forward
computation
32 LLM Continual Pretraining
We performed a lightweight continual pretraining for longcontext adaptation of existing
LLMs The pretraining data includes the PG19 and Arxivmath corpus as well as C4
text Raffel et al 2020 with length more than 4K tokens The segment length Nwas set to
2K throughout our experiments
1M passkey retrieval benchmark We replaced the vanilla MHA in a 1B LLM with Infini
attention and continued to pretrain on inputs with length of 4K The model was trained for
30K steps with batch size of 64 before finetuning on the passkey retrieval task Mohtashami
 Jaggi 2024
The passkey task hides a random number into a long text and asks it back at the model
output The length of the distraction text is varied by repeating a text chunk multiple times
The previous work Chen et al 2023a showed that a 8B LLaMA model can solve the task up
to 32K length when finetuned with the same 32K length inputs with Position Interpolation
We take this challenge further and finetune on only 5K length inputs to test on 1M length
regime
Input lengthRouge overall score
17181920
16K 32K 64K 128K 256K 500K
Figure 4 InfiniTransformers obtain better
Rouge overall scores with more book text pro
vided as inputTable 3 reports the tokenlevel accuracy for
test subsets with input lengths ranging from
32K to 1M For each test subset we con
trolled the position of the passkey so that it
is either located around the beginning mid
dle or the end of the input sequence We
reported both zeroshot accuracy and fine
tuning accuracy InfiniTransformers solved
the task with up to 1M context length af
ter finetuning on 5K length inputs for 400
steps
500K length book summarization Book
Sum We further scaled our approach by
continuously pretraining a 8B LLM model
with 8K input length for 30K steps We then
finetuned on a book summarization task
BookSum Kry scinski et al 2021 where the
goal is to generate a summary of an entire
book text
We set the input length to 32K for finetuning and increase to 500K for evaluating We use a
generation temperature of 05 and top p095 and set the number of decoding steps to 1024
to generate a summary of each book
7Preprint Under review
Table 4 compares our model against the encoderdecoder models that were built particularly
for the summarization task Lewis et al 2019 Xiao et al 2021 and their retrievalbased
longcontext extension Bertsch et al 2024 Our model outperforms the previous best
results and achieves a new SOTA on BookSum by processing the entire text from book We
have also plotted the overall Rouge score on validation split of BookSum data in Figure 4
There is a clear trend showing that with more text provided as input from books Our
InfiniTransformers improves its summarization performance metric
4 Related Work
Compressive memory Inspired by the plasticity in biological neurons Munkhdalai  Yu
2017a Miconi et al 2018 compressive memory approaches cast parameterized functions
as memory to store and retrieve information Hinton  Plaut 1987 Schmidhuber 1992 Ba
et al 2016 Munkhdalai et al 2019 Unlike the Transformer KV memory array Vaswani
et al 2017 Wu et al 2022 which grows with input sequence length compressive memory
systems maintain a constant number of memory parameters for computational efficiency
The parameters are modified with an update rule to store information which is then
retrieved via a memory reading mechanism Graves et al 2014 Sukhbaatar et al 2015
Munkhdalai  Yu 2017b
Compressed input representations can be viewed as a summary of past sequence seg
ments Rae et al 2019 Chevalier et al 2023 Along this direction more recent works
have been utilizing a Transformer LLM itself to compress input sequence for efficient long
context modeling Bulatov et al 2022 Chevalier et al 2023 Ge et al 2023 Mu et al
2024 However the previous segmentlevel compression methods including Compressive
Transformers Rae et al 2019 still discard the memory entries of old segments in order to
free up space for the new ones limiting their context window to the most recent segments
This is in contrast to our Infiniattention that computes incremental memory updates to a
fixed amount of memory parameters in a recurrent fashion
Longcontext continual pretraining There is a line of work that extends the doproduct
attention layers and continues to train LLMs for longcontext Xiong et al 2023 Fu et al
2024 The attention extensions include incorporating sparsity into the attention layer Chen
et al 2023b Ratner et al 2022 Mohtashami  Jaggi 2024 as well as manipulating the
position encodings Chen et al 2023a Peng et al 2023 Although the position encoding
based methods such as position interpolation techniques Chen et al 2023a can be data
efficient as they only adjust the positional bias in the attention layer they are still costly for
inference
The attention mechanism is also prone to the issues of attention sink Xiao et al 2023 and
lostinthemiddle Liu et al 2024 Consequently they struggle in a regime where context
length is longer than what was observed during training Press et al 2021 Kazemnejad
et al 2024 The proposed Infiniattention addresses those issues by enabling a segment
level streaming computation over long sequences with a fixed local attention window Our
InfiniTransformers successfully extrapolate to 1M input length regimes when trained on
32K and even 5K length sequences
Efficient attention The efficient attention techniques attempt to improve the efficiency of
the dotproduct attention with an approximation or a systemlevel optimization Multiple
directions have been explored for different forms of efficient attention approximation
including sparsitybased Child et al 2019 Beltagy et al 2020 Sukhbaatar et al 2021
Ding et al 2023 and linear attention approximation Shen et al 2018 Katharopoulos et al
2020 Schlag et al 2021 Among those the linear attention variants are closely related
to the associative memory matrix Schlag et al 2020 2021 and the metalearned neural
memory Munkhdalai et al 2019 where KV bindings Smolensky 1990 are stored in
FastWeights Hinton  Plaut 1987 Schmidhuber 1992 Ba et al 2016 that are modified
in with respect to new contextual information More recently systemlevel optimization
techniques have been proposed by leveraging specific hardware architecture to make the
exact attention computation more efficient Dao et al 2022 Liu et al 2023
8Preprint Under review
5 Conclusion
An effective memory system is crucial not just for comprehending long contexts with LLMs
but also for reasoning planning continual adaptation for fresh knowledge and even for
learning how to learn This work introduces a close integration of compressive memory mod
ule into the vanilla dotproduct attention layer This subtle but critical modification to the
attention layer enables LLMs to process infinitely long contexts with bounded memory and
computation resources We show that our approach can naturally scale to a million length
regime of input sequences while outperforming the baselines on longcontext language
modeling benchmark and book summarization tasks We also demonstrate a promising
length generalization capability of our approach 1B model that was finetuned on up to 5K
sequence length passkey instances solved the 1M length problem
References
Rohan Anil Andrew M Dai Orhan Firat Melvin Johnson Dmitry Lepikhin Alexandre
Passos Siamak Shakeri Emanuel Taropa Paige Bailey Zhifeng Chen et al Palm 2
technical report arXiv preprint arXiv230510403  2023
Jimmy Ba Geoffrey E Hinton Volodymyr Mnih Joel Z Leibo and Catalin Ionescu Using
fast weights to attend to the recent past Advances in neural information processing systems 
29 2016
Dzmitry Bahdanau Kyunghyun Cho and Yoshua Bengio Neural machine translation by
jointly learning to align and translate arXiv preprint arXiv14090473  2014
Iz Beltagy Matthew E Peters and Arman Cohan Longformer The longdocument trans
former arXiv preprint arXiv200405150  2020
Amanda Bertsch Uri Alon Graham Neubig and Matthew Gormley Unlimiformer Long
range transformers with unlimited length input Advances in Neural Information Processing
Systems  36 2024
Tom Brown Benjamin Mann Nick Ryder Melanie Subbiah Jared D Kaplan Prafulla
Dhariwal Arvind Neelakantan Pranav Shyam Girish Sastry Amanda Askell et al
Language models are fewshot learners Advances in neural information processing systems 
3318771901 2020
Aydar Bulatov Yury Kuratov and Mikhail Burtsev Recurrent memory transformer Advances
in Neural Information Processing Systems  351107911091 2022
Shouyuan Chen Sherman Wong Liangjian Chen and Yuandong Tian Extending con
text window of large language models via positional interpolation arXiv preprint
arXiv230615595  2023a
Yukang Chen Shengju Qian Haotian Tang Xin Lai Zhijian Liu Song Han and Jiaya Jia
Longlora Efficient finetuning of longcontext large language models arXiv preprint
arXiv230912307  2023b
Jianpeng Cheng Li Dong and Mirella Lapata Long shortterm memorynetworks for
machine reading arXiv preprint arXiv160106733  2016
Alexis Chevalier Alexander Wettig Anirudh Ajith and Danqi Chen Adapting language
models to compress contexts arXiv preprint arXiv230514788  2023
Rewon Child Scott Gray Alec Radford and Ilya Sutskever Generating long sequences with
sparse transformers arXiv preprint arXiv190410509  2019
DjorkArn e Clevert Thomas Unterthiner and Sepp Hochreiter Fast and accurate deep
network learning by exponential linear units elus arXiv preprint arXiv151107289  2015
9Preprint Under review
Zihang Dai Zhilin Yang Yiming Yang Jaime Carbonell Quoc V Le and Ruslan Salakhut
dinov Transformerxl Attentive language models beyond a fixedlength context arXiv
preprint arXiv190102860  2019
Tri Dao Dan Fu Stefano Ermon Atri Rudra and Christopher R e Flashattention Fast
and memoryefficient exact attention with ioawareness Advances in Neural Information
Processing Systems  351634416359 2022
Jiayu Ding Shuming Ma Li Dong Xingxing Zhang Shaohan Huang Wenhui Wang
Nanning Zheng and Furu Wei Longnet Scaling transformers to 1000000000 tokens
arXiv preprint arXiv230702486  2023
Yao Fu Rameswar Panda Xinyao Niu Xiang Yue Hannaneh Hajishirzi Yoon Kim and
Hao Peng Data engineering for scaling language models to 128k context arXiv preprint
arXiv240210171  2024
Tao Ge Jing Hu Xun Wang SiQing Chen and Furu Wei Incontext autoencoder for context
compression in a large language model arXiv preprint arXiv230706945  2023
Alex Graves Greg Wayne and Ivo Danihelka Neural turing machines arXiv preprint
arXiv14105401  2014
Dirk Groeneveld Iz Beltagy Pete Walsh Akshita Bhagia Rodney Kinney Oyvind Tafjord
Ananya Harsh Jha Hamish Ivison Ian Magnusson Yizhong Wang et al Olmo Acceler
ating the science of language models arXiv preprint arXiv240200838  2024
Donald Olding Hebb The organization of behavior A neuropsychological theory  Psychology
press 2005
Geoffrey E Hinton and David C Plaut Using fast weights to deblur old memories In
Proceedings of the ninth annual conference of the Cognitive Science Society  pp 177186 1987
John J Hopfield Neural networks and physical systems with emergent collective computa
tional abilities Proceedings of the national academy of sciences  79825542558 1982
Pentti Kanerva Sparse distributed memory  MIT press 1988
Angelos Katharopoulos Apoorv Vyas Nikolaos Pappas and Fran c ois Fleuret Transformers
are rnns Fast autoregressive transformers with linear attention In International conference
on machine learning  pp 51565165 PMLR 2020
Amirhossein Kazemnejad Inkit Padhi Karthikeyan Natesan Ramamurthy Payel Das and
Siva Reddy The impact of positional encoding on length generalization in transformers
Advances in Neural Information Processing Systems  36 2024
Wojciech Kry scinski Nazneen Rajani Divyansh Agarwal Caiming Xiong and Dragomir
Radev Booksum A collection of datasets for longform narrative summarization arXiv
preprint arXiv210508209  2021
Mike Lewis Yinhan Liu Naman Goyal Marjan Ghazvininejad Abdelrahman Mohamed
Omer Levy Ves Stoyanov and Luke Zettlemoyer Bart Denoising sequencetosequence
pretraining for natural language generation translation and comprehension arXiv
preprint arXiv191013461  2019
Hao Liu Matei Zaharia and Pieter Abbeel Ring attention with blockwise transformers for
nearinfinite context arXiv preprint arXiv231001889  2023
Nelson F Liu Kevin Lin John Hewitt Ashwin Paranjape Michele Bevilacqua Fabio Petroni
and Percy Liang Lost in the middle How language models use long contexts Transactions
of the Association for Computational Linguistics  12157173 2024
Thomas Miconi Kenneth Stanley and Jeff Clune Differentiable plasticity training plastic
neural networks with backpropagation In International Conference on Machine Learning 
pp 35593568 PMLR 2018
10Preprint Under review
Amirkeivan Mohtashami and Martin Jaggi Randomaccess infinite context length for
transformers Advances in Neural Information Processing Systems  36 2024
Jesse Mu Xiang Li and Noah Goodman Learning to compress prompts with gist tokens
Advances in Neural Information Processing Systems  36 2024
Tsendsuren Munkhdalai and Hong Yu Meta networks In International conference on machine
learning  pp 25542563 PMLR 2017a
Tsendsuren Munkhdalai and Hong Yu Neural semantic encoders In Proceedings of the
conference Association for Computational Linguistics Meeting  volume 1 pp 397 NIH Public
Access 2017b
Tsendsuren Munkhdalai John P Lalor and Hong Yu Citation analysis with neural attention
models In Proceedings of the Seventh International Workshop on Health Text Mining and
Information Analysis  pp 6977 2016
Tsendsuren Munkhdalai Alessandro Sordoni Tong Wang and Adam Trischler Metalearned
neural memory Advances in Neural Information Processing Systems  32 2019
Bowen Peng Jeffrey Quesnelle Honglu Fan and Enrico Shippole Yarn Efficient context
window extension of large language models arXiv preprint arXiv230900071  2023
Reiner Pope Sholto Douglas Aakanksha Chowdhery Jacob Devlin James Bradbury
Jonathan Heek Kefan Xiao Shivani Agrawal and Jeff Dean Efficiently scaling trans
former inference Proceedings of Machine Learning and Systems  5 2023
Ofir Press Noah A Smith and Mike Lewis Train short test long Attention with linear
biases enables input length extrapolation arXiv preprint arXiv210812409  2021
Jack W Rae Anna Potapenko Siddhant M Jayakumar and Timothy P Lillicrap Compressive
transformers for longrange sequence modelling arXiv preprint arXiv191105507  2019
Colin Raffel Noam Shazeer Adam Roberts Katherine Lee Sharan Narang Michael Matena
Yanqi Zhou Wei Li and Peter J Liu Exploring the limits of transfer learning with a
unified texttotext transformer The Journal of Machine Learning Research  21154855551
2020
Nir Ratner Yoav Levine Yonatan Belinkov Ori Ram Omri Abend Ehud Karpas Amnon
Shashua Kevin LeytonBrown and Yoav Shoham Parallel context windows improve
incontext learning of large language models arXiv preprint arXiv221210947  2022
Imanol Schlag Paul Smolensky Roland Fernandez Nebojsa Jojic J urgen Schmidhuber
and Jianfeng Gao Enhancing the transformer with explicit relational encoding for math
problem solving arXiv preprint arXiv191006611  2019
Imanol Schlag Tsendsuren Munkhdalai and J urgen Schmidhuber Learning associative
inference using fast weight memory arXiv preprint arXiv201107831  2020
Imanol Schlag Kazuki Irie and J urgen Schmidhuber Linear transformers are secretly
fast weight programmers In International Conference on Machine Learning  pp 93559366
PMLR 2021
Jurgen Schmidhuber Learning to control fastweight memories An alternative to dynamic
recurrent networks Neural Computation  41131139 1992
Noam Shazeer and Mitchell Stern Adafactor Adaptive learning rates with sublinear
memory cost In International Conference on Machine Learning  pp 45964604 PMLR 2018
Zhuoran Shen Mingyuan Zhang Haiyu Zhao Shuai Yi and Hongsheng Li Efficient
attention Attention with linear complexities arXiv preprint arXiv181201243  2018
Paul Smolensky Tensor product variable binding and the representation of symbolic
structures in connectionist systems Artificial intelligence  4612159216 1990
11Preprint Under review
Sainbayar Sukhbaatar Jason Weston Rob Fergus et al Endtoend memory networks
Advances in neural information processing systems  28 2015
Sainbayar Sukhbaatar Da Ju Spencer Poff Stephen Roller Arthur Szlam Jason Weston
and Angela Fan Not all memories are created equal Learning to forget by expiring In
International Conference on Machine Learning  pp 99029912 PMLR 2021
Hugo Touvron Louis Martin Kevin Stone Peter Albert Amjad Almahairi Yasmine Babaei
Nikolay Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale et al Llama 2
Open foundation and finetuned chat models arXiv preprint arXiv230709288  2023
Ashish Vaswani Noam Shazeer Niki Parmar Jakob Uszkoreit Llion Jones Aidan N Gomez
Łukasz Kaiser and Illia Polosukhin Attention is all you need Advances in neural informa
tion processing systems  30 2017
Yuhuai Wu Markus N Rabe DeLesley Hutchins and Christian Szegedy Memorizing
transformers arXiv preprint arXiv220308913  2022
Guangxuan Xiao Yuandong Tian Beidi Chen Song Han and Mike Lewis Efficient stream
ing language models with attention sinks arXiv preprint arXiv230917453  2023
Wen Xiao Iz Beltagy Giuseppe Carenini and Arman Cohan Primera Pyramid
based masked sentence pretraining for multidocument summarization arXiv preprint
arXiv211008499  2021
Wenhan Xiong Jingyu Liu Igor Molybog Hejia Zhang Prajjwal Bhargava Rui Hou Louis
Martin Rashi Rungta Karthik Abinav Sankararaman Barlas Oguz et al Effective
longcontext scaling of foundation models arXiv preprint arXiv230916039  2023
A Additional Training Details
For the longcontext language modeling task we set the learning rate to 001 by perform
ing small search over values of 0003 0005 001 and 003 We used the Adafactor opti
mizer Shazeer  Stern 2018 with linear warmup with 1000 steps followed by cosine
decay We applied gradient checkpointing after each segment to save to save memory The
batch size was set to 64 For the LLM experiments we set the learning rate to 00001 during
continual pretraining and task finetuning
B Passkey Retrieval Task
Below we showed the input format of the passkey task
There is an important info hidden inside a lot of irrelevant text Find it and memorize them I
will quiz you about the important information there The grass is green The sky is blue The sun
is yellow Here we go There and back again repeat x times The pass key is 9054  Remember
it9054 is the pass key The grass is green The sky is blue The sun is yellow Here we go
There and ack again repeat y times What is the pass key The pass key is
12
  NEURAL AUDIO FINGERPRINT FOR HIGHSPECIFIC AUDIO RETRIEV AL
BASED ON CONTRASTIVE LEARNING
Sungkyun Chang1 Donmoon Lee12 Jeongsoo Park1 Hyungui Lim1
Kyogu Lee2 Karam Ko3 and Yoonchang Han1
1Cochlearai2Seoul National University3SK Telecom
ABSTRACT
Most of existing audio ﬁngerprinting systems have limitations to be
used for highspeciﬁc audio retrieval at scale In this work we gen
erate a lowdimensional representation from a short unit segment of
audio and couple this ﬁngerprint with a fast maximum innerproduct
search To this end we present a contrastive learning framework
that derives from the segmentlevel search objective Each update in
training uses a batch consisting of a set of pseudo labels randomly
selected original samples and their augmented replicas These repli
cas can simulate the degrading effects on original audio signals by
applying small time offsets and various types of distortions such
as background noise and roommicrophone impulse responses In
the segmentlevel search task where the conventional audio ﬁnger
printing systems used to fail our system using 10x smaller storage
has shown promising results Our code and dataset are available at
httpsmimbresgithubioneuralaudiofp 
Index Terms acoustic ﬁngerprint selfsupervised learning
data augmentation music information retrieval
1 INTRODUCTION
Audio ﬁngerprinting is a content summarization technique that links
short snippets of unlabeled audio contents to the same contents in the
database 1 The most wellknown application is the music ﬁnger
printing system 17 that enables users to identify unknown songs
from the microphone or streaming audio input Other applications
include detecting copyrights 3 deleting duplicated contents 8
monitoring broadcasts 1 9 and tracking advertisements 10
General requirements for audio ﬁngerprinting system are dis
criminability over a huge number of other ﬁngerprints robustness
against various types of acoustic distortions and computational efﬁ
ciency for processing largescale database To achieve these require
ments most of conventional approaches 16 11 employed a nov
elty function to extract sparse representations of spectrotemporal
features from a predeﬁned audio window These sparse represen
tations or acoustic landmarks 5 used to be coupled with binary
hashing algorithms 1 2 12 for scalable search in hamming space
Still the representation learning approach to audio ﬁngerprint
ing has not been discovered well Nowplaying 7 has been pio
neering work in the direction They trained a neural network using
semihard triplet loss which derived from face recognition 13 In
their setup 7 Nowplaying could identify songs within 44 h long
audio database In our benchmark we replicate this semihard triplet
approach and compare it with our work in a new setup highspeciﬁc
audio retrieval in a 180 times larger database
We present a neural audio ﬁngerprinter for robust highspeciﬁc
audio retrieval based on contrastive learning Our ﬁngerprinting
model in Figure 1 differs from the prior works in three key aspects
Fig 1  Overview of the neural audio ﬁngerprinter We generate
segmentwise embeddings zt2Z that can represent a unit segment
of audio from the acoustic features Sat time step t In our frame
work each segment can be searched by maximum innerproduct
 Prior works 17 11 have focused on songlevel audio retrieval
from a music excerpt we challenge a highspeciﬁc audio search
by allowing missmatch less than 250 ms from a few seconds in
put
 We introduce the contrastive learning framework for simulating
maximum innerproduct search MIPS in minibatch
 We employ various types of data augmentation methods for gen
erating acoustic distractors and show their beneﬁts to training a
robust neural audio ﬁngerprinter
2 NEURAL AUDIO FINGERPRINTER
Our neural audio ﬁngerprinter in Figure 1 transforms and maps the
segmentlevel acoustic features into L2normalized space where the
innerproduct can measure similarities between segments It consists
of a preprocessor and neural networks
As a ﬁrst step input audio Xis converted to timefrequency
representationS It is then fed into convolutional encoder f
which is based on the previous study 7 Finally L2normalization
is applied to its output through a linear projection layer g Thus
we employgfS7Zdas a segmentwise encoder that trans
formsSinto ddimensional ﬁngerprint embedding space Zd The
ddimensional output space Zdalways belongs to Hilbert space
L2Rd the cosine similarity of a pair unit such as cos zazbbearXiv201011910v4  csSD  10 Feb 2021Fig 2  Illustration of the contrastive prediction task in Section 21
left Batch size N 6 We prepare N2pairs of originalreplica
The same shapes with soliddashed lines represent the positive pair
of originalreplica respectively right Each element in the matrix
represents pairwise similarity In each row a prediction task can
be deﬁned as classifying a positive pair one of the orange squares
against the negative pairs green or purple squares in the same row
comes innerproduct zT
azb and due to its simplicity L2projection
has been widely adopted in metric learning studies 7 13 14
Thegfdescribed here can be interpreted as a reorganization
of the previous audio ﬁngerprinting networks 7 into the common
form employed in selfsupervised learning SSL 1417 However
our approach differs from the typical SSL that throws gaway be
fore ﬁnetuning for the target task we maintain the selfsupervised
gup to the ﬁnal target task
21 Contrastive learning framework
As mentioned earlier we can use the innerproduct as a measure of
similarity between zt2 Zdfor any time step t Without losing
generality searching the most similar point  of database V  fvig
for a given query qinZdspace can be formulated as maximum inner
product search MIPS v
iarg maxiqvi
We simulate MIPS in a minibatch setup that takes into account
various acoustic distortions and input frame mismatches occurring
in the ﬁngerprint task A minibatch with the size of Nconsists of
N2pairs offsorgsrepgsorgis the timefrequency representation
of sampled audio and srepis the augmented replica of sorg where
srepMsorgMis an ordered augmentation chain that con
sists of multiple augmentors with the random parameter set for
each replica In this conﬁguration the indices of original examples
are always odd and that of replicas are even Therefore the batch
wise output of fgscan befzorg
2k1zrep
2kg2N
k1
We give each kth example a chance to be an anchor or a query
in MIPS to be compared with all other examples excluding itself in
the batch We calculate the pairwise innerproduct matrix between
all elements in the batch fzigN
i1asaij zT
izjfor8ij2
f12Ngas Figure 2 Then we deﬁne the contrastive prediction
task for a positive pair of examples ijas
ij logexpaijPN
k11k6iexpaij 1
12f01gis an indicator function that returns 1iffis true
and 0denotes the temperature 18 parameter for softmax We
employ Equation 1 to replace MIPS from the property computing
the topkk1 in our setup predictions in the softmax function isAlgorithm 1 Training of neural audio ﬁngerprinter
Conﬁg even number of batch size N temperature 
Variables inputs representation z2Rd
AugmentorMwith parameters 
Nets encoderfL2projection layer g
1foreach sampled minibatch fskgN2
k1do
2 for8k2f1N 2gdo
3zorg
kgfsk
4zrep
kgfMsk
5zfzorg
1zrep
1zorg
N2zrep
N2g
6 for8i2f1Ngand8j2f1Ngdo
7aijz
izj Pairwise similarity 
8ij NTxent aij Eq1
9 Updatefgto minimizeL1
NNX
i1Eq2
10return ﬁngerprinter gf
equivalent to the MIPS A similar approach is found in 19 The
total lossLaverageslacross all positive pairs both ijandji
L1
NNX
k12k12k2k2k1 2
Updating rules are summarized in Algorithm 1
It is worth comparing our approach to SimCLR 14 for visual
representation Our approach differs from SimCLR on how to con
struct positive pairs We use foriginal replicag whereas SimCLR
usesfreplica replicagfrom the same original source In our case
the anchor is already given because the database will always store
the clean source so it can be more important to learn the consistent
relation between the original and its replica over all other negatives
22 Sequence search
Our model trained by simulating MIPS is optimized for segment
level search In the case of searching for a query sequence fQL
i0g
consisting of Lconsecutive segments We ﬁrst gather the top k
segmentlevel search results indices Iqifor eachqifrom the DB
The offset is then compensated by I0
qiIqii The set of candi
date indices c2Cis determined by taking unique elements of I0
qi
The sequencelevel similarity score is the sum of all segmentlevel
similarities from the segment index range ccL and the index
with the highest score is the output of the system
3 EXPERIMENTAL SETUP
31 Dataset
The main experiment in Table 3 is reproduceable with the following
three data sets which are isolated from each other
 Train 10K30s A subset of the fmamedium 20 consisting of
30 s audio clips from a total of 10K songs
 TestDummyDB 100Kfulldb a subset of the fmafull 20
consisting of about 278 s audio clips from a total of 100K songs
We scale the search experiment with this
 TestQueryDB 50030s TestDB is another subset of the
fmamedium  which is 500 audio clips of 30 s each TestQuery
was synthesized using TestDB as directed in Section 35Table 1  Fingerprinter FP network structure in Section 33
SCo i
ks ReLUCLNCCo i
k0s0CReLUCLNCCo i
ks
f SCh h
32CSCh 4d
32CSC4d 4d
32CSC4d 2d
32C
SC2d 2d
32CSC2d d
32CSCd d
32CSCd 1
32
g L2CConcat CC1 u
11CELUCCu v
11CSplithd
FPgCfinput st
32 Data pipeline with augmentation chain
A batch consists of fxorgxrepgpairs Eachxrepis generated from its
corresponding xorgthrough augmentation steps as following order
 Time offset modulation To simulate possible discrepancies in real
world search scenarios we deﬁne positive examples as 1 s audio
clips with an offset of up to 200 ms We ﬁrst sample 12 s of
audio and thenfxorgxrepgare chosen by random start positions
 Background mixing A randomly selected noise in the SNR range
of 0 10 dB is added to the audio to reﬂect the actual noise
The noise dataset consists of 43 h of a subset of AudioSet 21
and 23 h of pub and cafe noise recorded by us The AudioSet
was crawled within subway metro  and underground tags with no
musicrelated tags Each dataset is split into 82 for traintest
 IR ﬁlters To simulate the effect of diverse spatial and microphone
environments microphone and room impulse response IR are
sequentially applied by convolution operation Public microphone
22 and spacial 23 IR dataset are split into 82 for traintest
 Cutout 24 and Specaugment 25 are applied after extracting
logpower Melspectrogram features such that fsorgsrepg Un
like other augmentations we uniformly apply a batchwise ran
dom mask to all examples in a batch including sorg The size and
position of each rectangleverticalhorizontal mask is random in
the range 110 12 the length of each timefrequency axis
33 Network structure
In Table 1 a spacesaving notation Co i
ksdenotes Conv2d with input
channeli output channel o kernel size 1k and stride 1s The
k0ands0denote rotation as k1ands1Splithdsplits input
dimensionhintodparts of each output dimension vhdgCf
isgf The network parameters fdhuvgare in Table 2
Convolutional encoder fftakes as input a logpower
Melspectrogram stwith a time step trepresnting 1s audio cap
tured by 50 overlapping window fconsists of several blocks
containing spatially separable convolution SC 26 followed by
a layer normalization LN 27 and a ReLU activation
L2projection layer g We take the splithead from the input
embeddings and pass it through the separate LinearELULinear
layers for each split as in previous studies 7 28 After concate
nating the multihead outputs we apply L2normalization
34 Implementation details
The replication of Nowplaying and our work shared the shorttime
Fourier transform STFT settings listed in in Table 2 Note that due
to ambiguity in the previous study 7 the STFT parameters were
set by us We trained Nowplaying using online semihard triplet
loss 13 with the margin m 04and batch size N 320 Table 2  Shared conﬁgurations for experiments
Parameter Value
Sampling rate 8000 Hz
STFT window function Hann
STFT window length and hop 1024 256
STFT spectrogram size FT 512TT 32
logpower Melspectrogram size F0T 256TT 32
Dynamic range 80 dB
Frequencyfmin maxg f 300 4000gHz
Fingerprintfwindow length hopg f 1s05sgorf2s1sg
Fingerprint dimension d 64 or 128
Network parameters fh u vg f 102432 hdg
Batch size N 120 or 320 or 640
We trained our model using LAMB 29 optimizer which per
formed 2 pp better than Adam 30 with the 3 s query sequence for
batch sizeN320 In practice Adam worked better only for
N240 The learning rate had an initial value of 1e4 N640with
cosine decay without warmup 31 or restarts 32 then it reached a
minimum value of 1e7 in 100 epochs The temperature in Eq1 was
 005 and we did not observe a meaningful performance change
in the range 00101 The training ﬁnished in about 30 h with a
single NVIDIA RTX 6000 GPU or v38 Cloud TPUs
The search algorithm in Section 22 was implemented using an
open library 33 We used the inverted ﬁle IVF index structure
with product quantizer PQ as a nonexhaustive MIPS The IVFPQ
had 200 centroids with the code size of 26 and 8bits per index In
this setting the loss of recall remained below 01 compared to the
exhaustive search of 100K songs  56M segments database
35 Evaluation protocol
 Evaluation metric To measure the performance in segmentsong
level search in Section 4 we use Top1 hit rate 
100n of hits Top1
n of hits Top1  n of miss Top1  3
which is equivalent to recall  In Table 3 exact match is the case
when the system ﬁnds the correct index in database We further
deﬁne the tolerance range for near match as500 ms
 TestQuery generation 2K querysources for each f1 2 3 5 6
10gs length are randomly cropped from TestDB containing 500
clips of 30s each Each query is synthesized through the random
augmentation pipeline as described in Section 32 Note that we
exclude Cutout and Specaugment The default SNR range is 0
10 dB We make sure that the data used for background mixing
and IR as unseen to our model by isolating them from training set
4 RESULTS AND DISCUSSION
41 Experimental results
The main results are listed in Table 3 Using the same augmentation
method Nowplaying 7 based on semihard triplet 13 took 2 s as
a unit audio segment The modiﬁed Nowplaying with 1 s unit audio
segment could be more fairly compared with our works
VSNowplaying semihard triplet Modiﬁed Nowplaying con
sistently performed better than the replicated Nowplaying  While
cutting the dimension in half this trend was maintained Consider
ing that the DB size was the same when the number of ﬁngerprint
dimensions was half it could be seen that constructing DB with 1Table 3  Top1 hit rate  of largescale total of 100K songs
segmentlevel search ddenotes the dimension of ﬁngerprint em
bedding exact match means that our system ﬁnds the exact index
near match means a mismatch within 1 index or500 ms
Method d matchQuery length in seconds
1 s 2 s 3 s 5 s 6 s 10 s
Nowplaying
replicated128exact  443 601 736 810 861
near  468 635 752 816 863
Nowplaying
modiﬁed
for 1 s unit64exact 258 585 693 785 814 877
near 309 613 712 795 822 883
128exact 263 582 695 784 814 878
near 309 611 718 798 830 892
This work
N64064exact 546 789 854 904 920 949
near 613 817 867 909 927 951
128exact 622 832 874 920 933 956
near 683 849 887 927 941 958
This work
N320128exact 610 822 871 918 931 952
near 671 841 881 925 939 955
This work
N120128exact 559 788 849 909 922 953
near 623 809 863 915 928 955
This work
no aug128exact 00 00 00 00 00 00
near 00 00 00 00 00 00
Table 4  Effect of ﬁngerprint dimension din 1 s segment search
Embedding dimension d16 d32 d64 d128
Top1 hit rate1 s  116 402 546 622
s was more advantageous to segment search The proposed model
with a 128dimensional ﬁngerprint using batch size of 640 always
showed the best performance highlighted in Table 3 for any query
length This conﬁrmed that the proposed contrastive learning ap
proach outperformed over the semihard triplet approach
Embedding dimension In Table 2 increasing the embedding di
mensiond 64128 for the modiﬁed Nowplaying did not affect the
results signiﬁcantly In contrast increasing the embedding dimen
siond 64128 for our best model gave us a larger improvement of
exact match performance as 76 546622 pp for the 1 s query
This reafﬁrmed the training beneﬁts of our contrastive learning over
the semihard triplet fairly compared using the same network struc
ture In Table 4 we further investigated the effect of reducing dto
our model with 1 s query length We could observed a rapid drop in
exact match performance while decreasing d 643216
Performance of sequence search The longer the query sequence
the better the performance in all experiments In Table 3 segment
level hit rate of our best model highlighted was increasing as
622834920956 while increasing the query length by
almost double Thus the longer query length was useful In Table 3
the performance difference between near and exact match result of
our best model at 1 s query was 61 622 and 683 pp This inter
val decreased immediately as the query length became larger than 1
These results showed that our sequence search method introduced
in Section 22 was quite effective
Effect of batch size The larger the batch size the better the per
formance in all experiments In Table 3 reducing the batch size
N 640120 from our best model degraded the exact match per
formance by63 622559 pp at 1 s query length Recent
works 14 16 17 on contrastive learning has been consistently re
porting similar trends Our result implicated that the diversity of
negative examples existing by large batch could play an important
role in the contrastive learning frameworkVSDejavu We compared our work with the opensource project
Dejavu 34 based on the conventional method 1 5 in the song
level search task of smaller 10K30s scale 696 of Top1 hit rate
was achieved with Dejavu  a songlevel search engine using a 6 s
query Our best model achieved 995 for songlevel hit rate and
exactnear match was 989991 at the 6 s query respectively Our
model also achieved f836 954 974g exact match atf123gs
query The capacity of ﬁngerprints from Dejavu was about 400
MB while ours quantized with 14 compression rate was less than
40 MB ford64 These results suggest that our method has advan
tages over conventional methods in both performance and scalability
42 Size of training set search time and scalability
The models in Table 3 were trained with about 70 h dataset This
size was less than 1 of the total 8K h DB for test We assumed that
using the entire DB for training would be impracticala huge num
ber of new songs are produced every day In additional experiment
we used 10 of the TestdummyDB for training a d64 model It
achievedf583 811 865 924 934 960 g of Top1 hit rate for
the query sequence of f1 2 3 5 6 10gs This improved37
546583 pp at the 1 s query over the best model with d64 in
Table 3 still lower than the result of d128 Thus both dand the
amount of training data were the factors affecting performance
In our best model with d128 the ﬁnal DB size was about 58
GB for 56M segments from total of 100K songs We report about
15 s search time with i910980XE CPU inmemorysearch and
002 s with GPU for parallel search of 19 segments  10 s query
In case of using CPUs we could observe ondisksearch using the
latest SSD with CPU was only twice as slow as inmemorysearch
We reserve the industrylevel scalability issues for future work
43 Transfer to downstream task
We further investigated the generality of the learned embeddings by
performing a downstream task as in the typical SSL 1417 set
tings By ﬁxing fand ﬁnetuning a linear classiﬁer we tried audio
genre classiﬁcation in GTZAN dataset with stratiﬁed 10fold cross
validation Finetuning on the pretrained embeddings for ﬁngerprint
achieved 592 accuracy while training from scratch achieved only
320 This showed that the features encoded by fwere linearly
interpretable consistent with other SSL reports 1417 However
our result was slightly lower than the baseline of 610 accuracy us
ing MFCCsGMM 35 This might be due to the limitation of the
lightweight networks with the relatively shorttime analysis window
5 CONCLUSIONS AND FUTURE WORK
This study presented a neural audio ﬁngerprinter for highspeciﬁc
audio retrieval Our model was trained to maximize the inner
product between positive pairs of ﬁngerprints through a contrastive
prediction task To this end we explicitly sampled positive pairs to
have originalreplica relations by applying various augmentations to
clean signals We evaluated our model in the segmentlevel search
task with a public database of 100K songs In the experiment our
model performed better than the model with triplet embeddings It
was also shown that our work using 10 times less memory than an
existing work outperformed in songlevel search task So far these
results have implied that the audio ﬁngerprinting task would inher
ently have selfsupervised learning potentials The future direction
of this study is to test neural audio ﬁngerprints in industryscale
database and queries from a variety of user devices6 ACKNOWLEDGEMENT
We would like to thank the TensorFlow Research Cloud TFRC pro
gram that gave us access to Google Cloud TPUs
7 REFERENCES
1 J Haitsma and T Kalker A highly robust audio ﬁngerprinting
system in Proc of the Int Society for Music Information
Retrieval ISMIR  2002 vol 2002 pp 107115
2 A Wang et al An industrial strength audio search algo
rithm in Proc of the Int Society for Music Information Re
trieval ISMIR  2003 vol 2003 pp 713
3 P Cano E Batlle T Kalker et al A review of audio ﬁnger
printing Journal of VLSI signal processing systems for signal
image and video technology  vol 41 no 3 pp 271284 2005
4 S Baluja and M Covell Waveprint Efﬁcient waveletbased
audio ﬁngerprinting Pattern recognition  vol 41 no 11 pp
34673480 2008
5 C V  Cotton and D P Ellis Audio ﬁngerprinting to identify
multiple videos of an event in Proc of the IEEE Int Conf
on Acoustics Speech and Signal Processing ICASSP  IEEE
2010 pp 23862389
6 TK Hon L Wang J D Reiss and A Cavallaro Audio
ﬁngerprinting for multidevice selflocalization IEEEACM
Transactions on Audio Speech and language processing  vol
23 no 10 pp 16231636 2015
7 B Gfeller et al Now playing Continuous lowpower music
recognition in NeurIPS 2017 Workshop on Machine Learning
on the Phone and other Consumer Devices  2017
8 C J Burges D Plastina J C Platt E Renshaw and H S
Malvar Using audio ﬁngerprinting for duplicate detection
and thumbnail generation in Proc of the IEEE Int Conf
on Acoustics Speech and Signal Processing ICASSP  IEEE
2005 vol 3 pp iii9
9 E Allamanche Audioid Towards contentbased identiﬁca
tion of audio material in Proc of the 100th AES Conv  2001
10 Y  Jiang C Wu K Deng and Y  Wu An audio ﬁngerprint
ing extraction algorithm based on lifting wavelet packet and
improved optimalbasis selection Multimedia Tools and Ap
plications  vol 78 no 21 pp 3001130025 2019
11 J Six and M Leman Panako  A Scalable Acoustic Finger
printing System Handling TimeScale and Pitch Modiﬁcation
inProc of the Int Society for Music Information Retrieval IS
MIR  2014 pp 259264
12 A Gionis P Indyk and R Motwani Similarity search in
high dimensions via hashing in Proc of the Int Conf on Very
Large Data Bases VLDB  1999 VLDB 99 pp 518529
13 F Schroff D Kalenichenko and J Philbin Facenet A uni
ﬁed embedding for face recognition and clustering in Proc
of the IEEE Conf on Computer Vision and Pattern Recognition
CVPR  2015 pp 815823
14 T Chen S Kornblith M Norouzi and G Hinton A simple
framework for contrastive learning of visual representations
arXiv preprint arXiv200205709  2020
15 A v d Oord Y  Li and O Vinyals Representation
learning with contrastive predictive coding arXiv preprint
arXiv180703748  201816 T Chen S Kornblith K Swersky M Norouzi and G Hin
ton Big selfsupervised models are strong semisupervised
learners arXiv preprint arXiv200610029  2020
17 A Baevski H Zhou A Mohamed and M Auli wav2vec
20 A framework for selfsupervised learning of speech repre
sentations arXiv preprint arXiv200611477  2020
18 G Hinton O Vinyals and J Dean Distilling the knowledge
in a neural network arXiv preprint arXiv150302531  2015
19 P H Chen S Si S Kumar Y  Li and CJ Hsieh Learning
to screen for fast softmax inference on large vocabulary neural
networks arXiv preprint arXiv181012406  2018
20 M Defferrard K Benzi P Vandergheynst and X Bresson
Fma A dataset for music analysis in Proc of the Int Society
for Music Information Retrieval ISMIR  2017
21 J F Gemmeke D P Ellis and et al Audio set An ontology
and humanlabeled dataset for audio events in Proc of the
IEEE Int Conf on Acoustics Speech and Signal Processing
ICASSP  IEEE 2017 pp 776780
22 Xaudia Microphone impulse response project 2017 On
line httpmicirpblogspotcom
23 M Jeub M Schafer and P Vary A binaural room impulse
response database for the evaluation of dereverberation algo
rithms in Proc of the Int Conf on Digital Signal Processing
ICDSP  IEEE 2009 pp 15
24 T DeVries and G W Taylor Improved regularization of
convolutional neural networks with cutout arXiv preprint
arXiv170804552  2017
25 D S Park W Chan et al Specaugment A simple data
augmentation method for automatic speech recognition in
Proc of the Interspeech  2019 pp 26132617
26 F Mamalet and C Garcia Simplifying convnets for fast
learning in Proc of the Int Conf on Artiﬁcial Neural Net
works ICANN  Springer 2012 pp 5865
27 J L Ba J R Kiros and G E Hinton Layer normalization
arXiv preprint arXiv160706450  2016
28 H Lai Y  Pan Y  Liu and S Yan Simultaneous feature learn
ing and hash coding with deep neural networks in Proc of
the IEEE Conf on Computer Vision and Pattern Recognition
CVPR  2015 pp 32703278
29 Y  You J Li et al Large batch optimization for deep learn
ing Training bert in 76 minutes in Proc of the Int Conf on
Learning Representations ICLR  2019
30 D P Kingma and J Ba Adam A method for stochastic opti
mization arXiv preprint arXiv14126980  2014
31 P Goyal P Doll ar R Girshick et al Accurate large mini
batch sgd Training imagenet in 1 hour arXiv preprint
arXiv170602677  2017
32 I Loshchilov and F Hutter Sgdr Stochastic gradient descent
with warm restarts arXiv preprint arXiv160803983  2016
33 J Johnson M Douze and H J egou Billionscale similarity
search with gpus IEEE Transactions on Big Data  2019
34 W Drevo Dejavu opensource audio ﬁngerprinting project
2014 Online httpspypiorgprojectPyDejavu
35 G Tzanetakis and P Cook Musical genre classiﬁcation of
audio signals IEEE Transactions on speech and audio pro
cessing  vol 10 no 5 pp 293302 2002
  Semantic Segmentation using Vision Transformers
A survey
Hans Thisankea Chamli Deshana Kavindu Chamitha Sachith
Seneviratnebc Rajith Vidanaarachchibc Damayanthi Heratha
aDepartment of Computer Engineering University of
Peradeniya Peradeniya 20400 Sri Lanka
bMelbourne School of Design University of Melbourne Parkville VIC 3010 Australia
cFaculty of Engineering and IT University of Melbourne Parkville VIC 3010 Australia
Abstract
Semantic segmentation has a broad range of applications in a variety of do
mains including land coverage analysis autonomous driving and medical
image analysis Convolutional neural networks CNN and Vision Trans
formers ViTs provide the architecture models for semantic segmentation
Even though ViTs have proven success in image classication they cannot
be directly applied to dense prediction tasks such as image segmentation
and object detection since ViT is not a general purpose backbone due to
its patch partitioning scheme In this survey we discuss some of the dier
ent ViT architectures that can be used for semantic segmentation and how
their evolution managed the abovestated challenge The rise of ViT and
its performance with a high success rate motivated the community to slowly
replace the traditional convolutional neural networks in various computer
vision tasks This survey aims to review and compare the performances of
ViT architectures designed for semantic segmentation using benchmarking
datasets This will be worthwhile for the community to yield knowledge re
garding the implementations carried out in semantic segmentation and to
discover more ecient methodologies using ViTs
Keywords vision transformer semantic segmentation review survey
convolution neural networks selfsupervised learning deep learning
Corresponding author
Email addresses e16368engpdnaclk Hans Thisanke e16076engpdnaclk
Chamli Deshan e16057engpdnaclk Kavindu Chamith
sachithseneviratneunimelbeduau Sachith Seneviratne
rajithvidanaarachchiunimelbeduau Rajith Vidanaarachchi
damayanthiherathengpdnaclk Damayanthi Herath
Preprint submitted to Engineering Applications of Articial Intelligence May 8 2023arXiv230503273v1  csCV  5 May 20231 Introduction
Transformers became the new stateoftheart in natural language pro
cessing NLP 1 after the tremendous success it achieved This led to the
development of ViT 2 which was later adapted into the computer vision
tasks such as image classication 2 3 semantic segmentation 4 5 and
object detection 6 7 A typical Transformer encoder consists of a multi
head selfattention MSA layer a multilayer perceptron MLP and a layer
norm LN The main driving force behind the ViT is the multihead self
attention mechanism It helps ViT to capture longrange dependencies with
less inductive bias 8 When trained on a sucient amount of data ViT
shows remarkable performance beating the performance of stateoftheart
CNNs 2 However ViTs still have some drawbacks compared to CNNs such
as the need for very large datasets Strategies such as selfsupervised based
approaches can be used to alleviate some of these drawbacks and further
enhance ViTs 9
Semantic segmentation is the process of assigning a class label to each
and every pixel of an image This requires accurate predictions at the pixel
level For segmentation there exist both CNNbased models and Trans
former based models However plain ViT models cannot be directly used
for segmentation tasks because they do not consist of segmentation heads
10 Instead SETR 5 and Swin Transformer 4 based architectures can be
utilized for segmentation tasks Unlike image classication dense prediction
tasks such as semantic segmentation and object detection come with a few
diculties due to the rich intraclass variation context variation occlusion
ambiguities and low image resolution 11 There have been many improve
ments in the ViT domain in the last few years to overcome these challenges
while further developments are still in progress to make them ecient
The review focuses specically on semantic segmentation using Vision
Transformers The comparison of the ViT models specialized for semantic
segmentation is discussed with architecturewise and tabulated specic sets
of model variants that can be compared with the same set of benchmark
datasets The current surveys performed on ViTs have been structured with
a detailed historical evolution from NLP to the Vision Transformer domain
12 focuses on selfattention and its varieties with advantages and limitations
with existing methods for segmentation object detection classication and
action recognition The comparison follows between CNN and ViT back
bones on the ImageNet dataset The survey done by 13 is also considering
various vision tasks and surpasses CNNbased models with experimental re
2sults on benchmark datasets Even though several surveys have been done
12 13 14 a comparison between segmentation models with several bench
mark datasets to identify the bestperforming model has not been performed
In our survey we provide a set of segmentation models for each of which we
dene the best variant in each benchmark dataset category This is useful in
the sense of identifying the most optimal parameters such as patch size iter
ations count for each variant of the model By providing mIoU  of model
performance results over several semantic segmentationrelated benchmark
datasets overall evaluation and highestperforming model variants for each
dataset can be identied
In Section 2 we discuss the applications of semantic segmentation ViTs
their challenges and loss functions Section 3 describes benchmark datasets
used in semantic segmentation Section 4 describes the existing work done
in semantic segmentation using ViTs and presents a quantitative analysis
Finally Section 5 provides the discussions and Section 6 concludes the paper
with future directions
2 Semantic Segmentation using Vision Transformers
This section aims to provide an indepth analysis of the applications in
semantic segmentation with a focus on recent advancements in ViTs We
begin by exploring the principles and architecture of ViTs and their potential
for improving semantic segmentation performance We then delve into vari
ous application domains of semantic segmentation We also devote a section
to practical approaches for overcoming the data limitations that often arise
in ViT models Finally we discuss various loss functions used in semantic
segmentation and their eectiveness in dierent scenarios
21 Vision Transformers
Automatic segmentation techniques have been evolving and improving
throughout the years with the advancements of deep learning approaches
and the application of semantic segmentation in practical usage For seman
tic segmentation the requirement is to locally identify the dierent classes
in the image with spatial location For that the fully connected layers in
the conventional CNN architecture were replaced with fully convolutional
layers combined with feature extraction This was introduced as Fully Con
volutional Networks FCN 15 to identify highlevel semantic features from
images These networks have shown to be faster compared to previous CNN
based techniques and are also capable of generating segmentation maps for
images of any resolution Some of the commonly known architectures are
3UNet stateoftheart FCN and more improved architectures with higher
accuracy and eciency are developed by 16 17 18
One of the limitations identied with the FCN architecture is the low
resolution of the nal output segmentation image of the feature map due
to going through several convolutional and pooling layers Furthermore the
locality property of the FCNbased methods caused limitations to the capture
of longrange dependencies of the feature maps To solve this researchers also
looked into attention mechanisms to merge or replace these models This has
led to trying out Transformer architectures in the computer vision domain
which were successful in NLP
Selfattentionbased architectures have taken priority in NLP by avoiding
the drawbacks such as vanishing gradients in sequence modeling and trans
duction tasks Specially designed for sequence modeling and transduction
tasks Transformers with attention were able to model longrange sequences
of data When training a NLP model one of the best ways is to pretrain
on a large text corpus and then netune on a small set of data which is for
the related task But with deep neural networks this was a challenging task
As Transformers have high computational eciency and scalability it was
easier to train on a large set of data 19
With the success of using selfattention to enhance the inputoutput in
teraction in NLP works have been proposed to combine convolutional ar
chitectures with selfattention especially in object detection and semantic
segmentation where inputoutput interaction is highly needed 20 But ap
plying attention to convolutional architectures demands high computation
power even though they are theoretically ecient 1
Considering images calculating selfattention is quadratic to the image
size as each pixel attends to every other pixel therefore it is a quadratic cost of
the pixel count 2 Thus 2 proposed to divide the image into a sequence of
patches and treat them as tokens as it was done in NLP Instead of pixelwise
attention patchwise attention was used in the architecture which helped to
reduce the computational complexity compared to applying selfattention to
convolutional architecture
This architecture showed promising results by surpassing all the state
oftheart convolutionbased methods by reaching an accuracy of 8855 on
ImageNet 9072 on ImageNetReaL and 9455 on CIFAR100 datasets
2 A major characteristic of the ViT is that it needs more data for model
training Experiments carried out by 2 ensure that with increasing data
size ViT performs well
4Figure 1 Architecture of the Vision Transformer The model splits an image into a
number of xedsize patches and linearly embeds them with position embeddings left
Then the result is fed into a standard transformer encoder right Adapted from 2
22 Applications of Semantic Segmentation
In this section we discuss various application domains of semantic seg
mentation including remote sensing medical imaging and video processing
For each of these domains we highlight the unique challenges and opportuni
ties that arise as well as the current stateoftheart methods and techniques
221 Semantic Segmentation of Remote Sensing Images
Remote sensing is the process of getting information and monitoring the
characteristics of an area without having any physical contact The two
main types of remote sensing techniques are the use of active sensors such
as RADAR LiDAR and the use of passive sensors such as satellite imagery
21 These highresolution earth surface images provide a wide range of use
cases such as world mapping updates 22 forest degradation analysis 23
monitoring changes to the surface 24 etc
Remote sensing imagery is widely used in combination with computer vi
sion and Articial Intelligence AI for analyzing and processing the earths
surface over large areas with complex feature distributions The images col
lected by satellites or unmanned aerial vehicles UAV provide a wide range
of information for applications such as urban planning disaster management
trac management climate change wildlife conservation crop monitoring
etc The use of datasets containing these highresolution images and their
respective segmented masks 25 have provided a base for remote sensing
5image analysis using computer vision and AI The use of neural networks
provides the ability to process large amounts of image data for object de
tection semantic segmentation and change detection tasks The evolution
in the remote sensing domain has further improved satellite sensors and the
introduction of drone technology for aerial imagery has been vital to getting
ner details on the earths surface This has resulted in precise and accurate
data for processing using AI techniques 26
Remote sensing images of the earths surface provide land cover areas that
can be categorized into dierent segmented classes Each of these classes is
assigned a label for each pixel while preserving the spatial resolution of the
image Many datasets containing these remote sensing images and their seg
mented masks are available 25 27 28 to use for dierent applications such
as change detection land cover segmentation and classication Examples
of common land cover classes covered by the pixellevel classication are
forests crops buildings water resources grasslands roads etc Research
has been conducted using ViT architecture models by adding layers and at
tention mechanisms eciently and improvements in performance to process
highresolution remote sensing images for semantic segmentation such as Ef
cient Transformer 10 and WideContext Transformer 29
Manual segmentation of these dierent environmental areas from a com
plex satellite or aerial images is a dicult task which is timeconsuming
errorprone and requires expertise in the remote sensing domain
222 Semantic Segmentation of Medical Images
Medical image analysis has developed and incorporated scanning and vi
sualization techniques Segmentation techniques have been vital as it has the
ability to identify and segment medical imagery to assist in further diagnosis
and interventions By identifying each region of interest ROI highlighted
various important diagnoses are happening such as brain tumor boundary
detection from MRI images pneumonia aections in Xrays cancer detec
tion from biopsy sample images etc The demand for this type of analy
sis through image segmentation has emerged in the recent past with much
research being done in the scope to develop more precise ecient models
and algorithms These medical images that are used in image segmentation
tasks can be grouped based on modalities such as MRI CT scan Xray
ultrasound microscopy dermoscopy etc Each of these categories contains
datasets that were collected under medical supervision and some are made
publicly available
Since there exist several modalities as mentioned above the technological
systems that are used for medical imagery dier Medical imagery system
development vendors built them as per the doctors requirements There
6fore the images generated are bound to the limitations of the technology
available and require medical personal intervention to examine them 30
Therefore the segmentation of these images in dierent biological domains
requires experts in each eld to cope with these systems and spend a vast
amount of time examining them To overcome these diculties the capabil
ity of automatic feature extraction has been introduced with deep learning
based techniques which have been valuable in the sense of medical imagery
With the advancements in segmentation analysis betterperforming models
have been introduced with the use of medical images by many researchers
One such famous architecture is the UNet 31 which was initially intro
duced for medical image analysis Based on this several improved versions
have been followed up using medical imagery datasets from heart lesion and
liver segmentation 32 33 18 This proves how benecial the improvement
of segmentation has been in the medical environment In recent years the
emerging new architectures of ViTs have also been applied to the medical
domain with TransUNet 34 and SwinUnet 35 They are hybrid Trans
former architectures with the advantages of the UNet They performed with
better accuracy in cardiac and multiorgan segmentation applications
Some limitations of medical images are the relatively less number of im
ages available compared to natural image datasets landscapes people an
imals and automobiles with millions of images In the medical domain
there are several image modalities For annotating medical images expertise
in each medical eld is a must Among them MRI and microscopy images
are quite dicult to annotate 36 Typically these datasets contain fewer
images compared to ultrasound Xray and lesion datasets which are ob
tained with the existing scanning systems and are easier to annotate with
less complex structures and ne boundaries But still limitations exist due
to restrictions on privacy and other medical policies to obtain these images in
large quantities To overcome these limitations with some datasets several
image segmentation challenge competitions are taking place every year which
provide publicly available wellannotated medical image datasets Most of
the improvements made through research in semantic segmentation models
have been based on these challenge datasets and most are taken as bench
mark datasets for segmentation 37 38 39
223 Video Semantic Segmentation
HumanMachine interaction 40 augmented reality 41 autonomous ve
hicles 42 image search engines 43 are some applications in complete scene
understanding and for these type of applications semantic segmentation con
tributes more on complete scene understanding on videos Usually the idea
is to apply semantic segmentation on frames of a highresolution video where
7the video is considered as a set of uncorrelated xed images 44 The com
mon challenge with this type of semantic segmentation is the computational
complexity of scaling the spatial dimension of the video using the temporal
frame rate Removal of temporal features and only focusing on spatial frame
byframe features doesnt make sense in video segmentation Since there is
a combined ow among frames of a video considering the temporal context
of a video is an essential factor in video semantic segmentation even though
it is computationally expensive
Research has been conducted to reduce this high computation cost on
videos Feature reuse and feature warping 45 have been proposed as a
solution Cityscapes 46 and CamVid 47 are some largest video segmen
tation datasets available for framebyframe approach of video segmentation
48 Recent papers have proposed segmentation methods such as selective
reexecution of feature extraction layers 49 optical owbased feature warp
ing 50 and LSTMbased xedbudget keyframe selection policies 51 The
main key problem in these approaches is that they have less attention to the
temporal context of a video Researchers have shown that to satisfy both
spatial and temporal contexts using an optical ow of video as temporal in
formation to speed up uncertainty estimation makes good sense 52 VisTR
53 TeViT 54 and SeqFormer 55 are some of the Transformer models that
are used for video segmentation tasks
23 Practical approaches to overcome the data limitation
Deep neural networks have performed well with supervised learning in
computer vision and NLP But when it comes to the real world supervised
learning faces a bottleneck in training a neural network as it needs lots of
labeled data Collecting labeled data or manual labeling is dicult in every
aspect Training a network from scratch is a somewhat costly task as a
remedy for this transfer learning comes into play But when considering
specied downstream tasks such as satellite imagery semantic segmentation
using pretrained datasets is dicult as most of the architectures have been
trained on benchmark datasets where the data domain is dierent Therefore
getting good accuracy has been tricky
Specially when considering Transformer architectures selfsupervised learn
ing plays a great role as a remedy for datahungry problems in deep learning
In human vision humans are fed with dierent things in the environment and
then are able to distinguish those things from other objects in the environ
ment There are no labeling mechanisms for these scenarios Therefore this
is the technique used in SSL which actually trains a neural network using
an unlabeled dataset where the labels are automatically provided through
the dataset itself As the rst step the network is set to solve a pretext
8task as described in Figure 2 A pretext task is a predesigned task from
which the network can learn features and then using those trained weights
for dierent features the network can be applied to solve some downstream
tasks A downstream task is a specied task Common downstream tasks in
computer vision are semantic segmentation object detection etc
Figure 2 The general pipeline of selfsupervised learning The trained weights from solving
a pretext task are applied to solve some downstream tasks
Rotating an image by a given angle and predicting the rotation solving
jigsaw puzzles lling a cut patch on an image predicting the relative position
of a patch of an image and separating images belonging to dierent clusters
can be considered as some of the pretext tasks in SSL 56 By using these
methods the network can learn dierent features in the dataset under the
given scope No labels are used here and automatic labeling is achieved via
the image itself
SSL has three general categories based on how the training happens
Generative Train the encoder to encode the given input and using the
decoder get the input back
Contrastive Train the encoder to encode the given input and nd the
similarities
9GenerativeContrastive Adversarial Train encoder to encode the given
input and create fake outputs and compare the features of the input
and output 57
Semantic segmentation is one of the major downstream tasks that can
be performed using SSL Pixelwise labeling is essential in semantic segmen
tation If there are no properly annotated datasets SSL is the best way to
train semantic segmentation architectures
24 Loss functions in semantic segmentation
For segmentation classication and object detection models accuracy
improvement not only depends on the model architectures but also on the loss
functions used The loss function calculates the overall error while training
batches and adjust the weights through back propagation Numerous loss
functions have been created to cope with various domains and some of them
are derived from existing loss functions Additionally these loss functions
take into account the imbalances in the dataset too
In the case of semantic segmentation the default choice and most com
monly used is the crossentropy loss which is applied pixelwise The loss
function independently evaluates the class predictions for each pixel and av
erages over all the pixels
CElosspq nX
i1pilogqi 1
The equation 1 above computes the average loss for each pixel in an
image Here in the equation piis the true probability of the ithclass and
qiis the predicted probability of the same class This supports the model
to generate probability maps that closely resemble the actual segmentation
masks while penalizing inaccurate predictions more heavily By minimizing
the crossentropy loss function during training the model becomes better at
precise image segmentation
Even though the above method is widely used it can be biased with
dataset imbalance as the majority class will be dominant To overcome this
when the dataset is skewed a weighted cross entropy loss is introduced in
31
WCElosspq nX
i1piwilogqi 2
Here as in equation 2 a weight factor as wifor theithclass is inserted to
the typical equation 1 But the issue was not signicantly solved as the cross
10entropy calculates the average perpixel loss without considering the adjacent
pixels which can be boundaries
As a further improvement for the crossentropy loss the focal loss tech
nique 58 was introduced This is implemented by altering the structure
of crossentropy loss When focal loss is applied to samples with accurate
classications the scaling factor value is downweighted This ensures the
more harder samples are emphasized therefore high class imbalance wont
bias toward the overall calculations
Flosspt t1ptlogpt 3
In the equation 3 ptis the predicted probability of the true class tis a
scaling factor that gives higher weight to the positive class and is a focusing
parameter that controls how much the loss is focused on hard examples
The crossentropy loss is scaled in this loss function with the scaling
factors decreasing to zero as the condence in the wellclassied classes rises
Therefore more attention is given to the pixel classes which are dicult to
predict
Another set of loss calculation techniques is the overlapping between pre
diction and actual segmentations The models are trained to minimize the
loss such that the model outputs segmentations with higher overlaps
Dice loss is one such widely used popular measure in computer vision
tasks to calculate the similarity between two images It is based on the
dice coecient which was later developed as the dice loss function in the
segmentation domain This loss was rst used in the computer vision domain
by 59 in medical image segmentation tasks
Dlossgp  12Pn
i1gipiPn
i1giPn
i1pi4
Here in equation 4 gandpdescribes the ground truth and prediction
segmentations The sum is calculated over the nnumber of pixels with 
small constant added to avoid division by zero The dice coecient measures
the overlap between the samples ground truth and prediction and provides
a score ranging from 0 to 1 1 means perfect overlap Since this method
considered pixels in both global and local contexts the accuracy is higher
than crossentropy loss calculations
Another similar method used to evaluate the metric of models is the
IoU Intersection over Union loss also known as the Jaccard index It is
quite similar to the dice metric and measures the overlapping of the positive
instances between the considered samples This method as shown in equation
115 diers from the dice loss with correctly classied segments relative to total
pixels in either the ground truth or predicted segments
IoUlossgp  1Pn
i1gipiPn
i1giPn
i1piPn
i1gipi5
For multiclass segmentation the mean IoU is considered by taking the
average of each individual class IoU This is widely used for performance
comparison and evaluation of dense prediction models 60
3 Datasets
In this section the common datasets used for the training and testing
of semantic segmentation models are considered Factors aecting the cre
ation of real datasets are lighting conditions weather and season Based
on these factors datasets can be classied into dierent groups When data
is collected under normal daytime environmental conditions those data are
categorized under no crossdomain datasets If data is collected under some
deviated environmental conditions including rainy cloudy nighttime snowy
etc then such data are categorized under crossdomain datasets Another
category is synthetic data where the data is articially created and col
lected for training purposes These synthetic datasets are mostly created as
a costeective supplement for training purposes Following are some of the
benchmark datasets specially made for semantic segmentation tasks with a
summary presented in Table 1
PASCALContext 61 This dataset was created by manually labeling
every pixel of PASCALVOC 2010 62 dataset with semantic categories The
domain of this dataset is not limited and its data contains dierent objects
The semantic categories of this dataset can be divided into three main classes
i objects ii stu and iii hybrids Objects have dened categories such
as cups keyboards etc Stu has classes without a specic shape and has
regions such as sky water etc Hybrid contains intermediate objects such
as roads where roads have a clear boundary but shape cannot be predicted
correctly
ADE20K 63 Annotations of this dataset are done on scenes objects
parts of objects Many of the objects in the dataset are annotated with their
parts Annotations in this dataset are made continuously Therefore this is
a growing dataset
KITTI 64 This dataset contains both 2D and 3D images which have
been collected from urban and rural expressway incidents and trac sce
narios It is useful for robotics and autonomous driving This dataset has
12dierent variants namely KITTI2012 KITTI2015 and they have some dif
ferences in the ground truth
Cityscapes 46 This contains largescale pixellevel and instancelevel
semantic segmentation annotations recorded from a set of stereo video se
quences Compared to other datasets quality data size and annotations in
this dataset have a good rank and data have been collected from 50 dierent
cities in Germany and neighboring countries
IDD 65 This is specially designed for road scene understanding and data
have been collected from 182 Indian road scenes As these are taken from
Indian roads there are some variations in the weather and lighting conditions
because of dust and air quality on roads One key feature of this dataset is
this contains some special classes such as autorickshaws and animals on the
roads
Virtual KITTI 66 Except for dierent weather and imaging conditions
most of the virtual vision datasets such as Virtual KITTI are similar to the
real vision datasets Therefore virtual datasets are useful for pretraining
purposes This dataset is created from 5 dierent urban scene videos from
the realworld KITTI dataset Data have been automatically labeled and can
be used for object detection semantic segmentation instance segmentation
etc
IDDA 67 This contains 1 million frames generated from simulator
CARLA oriented on dierent 7 city models This dataset can be used to
do semantic segmentation for more than 100 dierent visual domains and is
specially designed for autonomous driving models
Dataset Classes Size Train Validation Test Resolution pixels Category
PASCALContext 540 19740 4998 5105 9637 387 470 No crossdomain
ADE20K 150 25210 20210 2000 3000  No crossdomain
KITTI 5 252 140  112 1392 512 No crossdomain
Cityscapes 30 5K ne 20K coarse 2975 500 1525 1024 2048 Crossdomain
IDD 34 10004 7003 1000 2001 1678 968 Crossdomain
Virtual KITTI 14 21260    1242 375 Synthetic
IDDA 24 1M    1920 1080 Synthetic
Table 1 Summary of the datasets
Note Both crossdomain and nocross domain falls into the nonsynthetic
category
4 Meta  analysis
In this section we discuss some of the ViT models specialized for the
task of semantic segmentation The models are selected upon considering the
datasets that they benchmarked ADE20K Cityscapes PASCALContext
13The intuition behind that is to compare all the models on a common basis
The benchmark results are summarized in Table 2
41SEgmentation TRansformer SETR
SETR 5 proposes semantic segmentation as a sequencetosequence pre
diction task They adopt a pure Transformer as the encoder part of their
segmentation model without utilizing any convolution layers In this model
they replace the prevalent stacked convolution layer based encoder with a
pure Transformer which gradually reduces the spatial resolution
Figure 3 SETR architecture and its variants adapted from 5 a SETR consists of a
standard Transformer b SETRPUP with a progressive upsampling design c SETR
MLA with a multilevel feature aggregation
The SETR encoder Figure 3a which is a standard Transformer treats
an image as a sequence of patches followed by a linear projection Then it
embeds these projections with patch embedding  position embedding to
feed them into a set of Transformer layers SETR has no downsampling
in spatial resolution at each layer of the encoder transformer while it only
provides global context modeling They classify SETR into a few variants
depending on the decoder part of the model SETRPUP Figure 3b which
has a progressive upsampling design and the SETRMLA Figure 3which
has a multilevel feature aggregation
SETR achieved stateoftheart semantic segmentation results on ADE20K
Pascal Context by the time of submission 5 It has also been tested on the
Cityscapes dataset and has shown impressive results
42 Swin Transformer
To address the issue of not having a general purpose Transformer back
bone for computer vision tasks 4 proposed Swin Transformer Hierarchical
14Vision Transformer using Shifted Win dows which can be served as a gen
eral purpose backbone for computer vision tasks such as image classication
and dense prediction
Figure 4 An overview of the Swin Transformer adapted from 4 a Hierarchical feature
maps for reducing computational complexity b Shifted window approach which was
used when calculating selfattention c Two successive Swin Transformer Blocks which
presented at each stage d Core architecture of the Swin
Swin Transformer was able to bring down the quadratic computational
complexity of calculating selfattention in Transformers to linear complex
ity by constructing hierarchical feature maps Figure 4a Also the shifted
window approach illustrated in Figure 4b has much lower latency than the
earlier sliding window based approaches which were used to calculate the self
attention Swin Transformer showed great success over the previous state
oftheart in image classication 873 top1 accuracy on ImageNet1K
semantic segmentation 535 mIoU on ADE20Kval and object detection
587 box AP and 511 mask AP on COCO testdev 4
According to the architecture of a Swin Transformer in the beginning
it splits the given image into a sequence of nonoverlapping patches tokens
by using the patch partitioning module Figure 4d Then a linear embed
ding is applied to this sequence of patches to project them into an arbitrary
dimension It is followed by several Swin Transformer blocks to apply self
attention The main responsibility of the patch merging module is to reduce
the number of tokens in deeper layers It is noteworthy that the feature map
resolutions in the hierarchical stages are similar to those in typical convo
15lution architectures such as ResNet 68 Therefore Swin Transformer can
eciently replace ResNet backbone networks in computer vision tasks
43 Segmenter
Segmenter 11 is a purely transformerbased approach for semantic seg
mentation which consist of a ViT backbone pretrained on ImageNet and
introduces a mask transformer as the decoder Figure 5 Even though the
model was built for segmentation tasks they take advantage of the mod
els made for image classication to pretrain and then netune them on
moderatesized segmentation datasets
Figure 5 Segmenter architecture adapted from 11 It basically has a ViT backbone with
a mask transformer as the decoder
CNNbased models are generally inecient when processing global image
context and ultimately result in a suboptimal segmentation The reason for
the suboptimal segmentation of the convolutionbased approaches is that
convolution is a local operation which poorly accesses the global information
of the image But the global information is crucial where the global image
context usually inuences the local patch labeling But modeling of global
interaction has a quadratic complexity to the image size because it needs
to model the interaction between each and every raw pixel of the image
The architecture of the Segmenter especially captures the global context of
images unlike the traditional CNNbased approaches
Other than the semantic segmentation tasks this Segmenter model also
can be applied to panoptic segmentation semantic segmentation  instance
segmentation tasks by altering the model architecture The class embed
dings of the model need to be replaced by object embeddings in such a case
1644 SegFormer
SegFormer 69 is an architecture for semantic segmentation which consist
of a hierarchical Transformer encoder with a lightweight multilayer percep
tron MLP decoder Figure 6 The MLP decoder is used for predicting the
nal mask To obtain a precise segmentation it uses a patch size of 4 4
in contrast to ViT which uses a patch size of 16 16 It has an overlapped
patch merging process to maintain the local continuity around the patches
Figure 6 SegFormer architecture adapted from 69 It has a hierarchical Transformer
encoder for feature extraction and a lightweight MLP decoder for predicting the nal
mask
Generally ViT has a xed resolution for positional encoding 70 This
leads to a drop in accuracy since it needs to interpolate the positional en
coding of testing images when they have a dierent resolution than training
images Thus SegFormer introduces a PositionalEncodingFree design as a
key feature
Moreover the authors claim their architecture is more robust against
common corruptions and perturbations than current methods which make
SegFormer appropriate for safetycritical applications SegFormer achieved
competitive results on ADE20K Cityscapes and COCOStu datasets as
shown in Table 2 SegFormer comes in several variants from SegFormerB0
to SegFormerB5 where the largest model is SegFormerB5 This largest
model surpasses the SETR 5 on the ADE20K dataset achieving the highest
mIoU while being 4 faster than SETR All of these SegFormer models have
tradeos between model size accuracy and runtime
1745 Pyramid Vision Transformer PVT
ViT couldnt be directly applicable to dense prediction tasks because its
output feature map is single scaled and it generally has a low resolution which
comes at a higher computational cost PVT 71 overcomes the aforemen
tioned concerns by introducing a progressive shrinking pyramid backbone
network to reduce the computational costs and simultaneously output more
negrained segmentation PVT comes in two variants PVT v1 71 is the
rst work by the authors and PVT v2 72 comes with some additional im
provements to the previous version
451 PVT v1
This initial version has some noteworthy changes compared to the ViT
It takes 44 input patches in contrast to the 16 16 patches in ViT This
improves the models ability to learn highresolution representations It also
reduces the computational demand of traditional ViT by using a progressive
shrinking pyramid This pyramid structure progressively shrinks the output
resolution from high to low in the stages which are responsible for generat
ing the scaled feature maps Figure 7 Another major dierence is that it
replaces the multihead attention layer MHA in ViT with a novel spatial
reduction attention SRA layer which reduces the spatial scales before the
attention operation This further reduces the computational and memory
demand because SRA has a low computational complexity than MHA
Figure 7 PVT v1 architecture adapted from 71 The pyramid structure of the stages
progressively shrinks the output resolution from high to low
18452 PVT v2
The former version has a few drawbacks The computational demand
of the PVT v1 is relatively large when processing highresolution images
It loses the local continuity of the images when processing the image as a
sequence of nonoverlapping patches It cannot process variablesized inputs
because of the xedsize position encoding This new version has three major
improvements which circumvent the previous design issues First one is linear
spatial reduction attention LSRA which reduces the spatial dimension of
the image to a xed size using average pooling Figure 8 Unlike SRA
in the PVT v1 LSRA benets from linear complexity Second one is the
overlapping patch embedding Figure 9a This is done by zeropadding the
border of the image and taking more enlarged patch windows which overlap
with the adjacent windows It helps to capture more local continuity of the
images The third one is the convolutional feedforward network Figure 9b
which helps to process dierent sizes of input resolutions With these major
improvements PVT v2 was able to bring down the complexity of PVT v1
to linear complexity
Figure 8 Comparison of spatial reduction attention SRA layers in PVT versions 72
We can clearly see how the improvements of the PVT v2 contribute to
higher gains in the benchmark comparison in Table 2
46 Twins
Twins 73 propose two modern Transformer designs for computer vision
named TwinsPCPVT and TwinsSVT by revisiting the work on the PVT
v1 71 and Swin Transformer 4
TwinsSVT uses a spatially separable selfattention SSSA mechanism
based on the depthwise separable convolutions in neural networks This
19Figure 9 Improved patch embedding and feedforward networks in PVT v2 72
SSSA has two underlying attention mechanisms which are capable of cap
turing local information as well as global information Locally grouped self
attention LSA and global subsampled attention GSA are the above
mentioned attention mechanisms respectively Those techniques greatly re
duce the heavy computational demand in highresolution image inputs while
keeping a negrained segmentation
Figure 10 TwinsPCPVT architecture adapted from 73 It uses conditional position
encoding with a positional encoding generator PEG to overcome some of the drawbacks
of xedpositional encoding
As we discussed in the Pyramid Vision Transformer section PVT v1
can only process xedsize image inputs due to its absolute positional en
20coding This hinders the performance of PVT To alleviate this challenge
TwinsPCPVT uses a conditional position encoding CPE rst introduced
in Conditional Position encoding Vision Transformer CPVT 70 This is
illustrated as the positional encoding generator PEG in Figure 10 It is
capable of alleviating some of the issues encountered in xedposition encod
ing
Twins architectures have shown outstanding performance on computer
vision tasks including image classication and semantic segmentation The
semantic segmentation results achieved by the two Twins architectures are
highly competitive compared to the Swin Transformer 4 and PVT 71
47 Dense Prediction Transformer DPT
DPT 74 architecture is introduced with a transformer backbone inside
the encoderdecoder design for negrained output segmentation predictions
compared to the fully convolutional networks The transformer encoder
based on ViT 2 is capable of maintaining spatial resolution over all the
stages of the Transformer architecture which is important for dense predic
tions
Figure 11 DPT architecture adapted from 74 a Nonoverlapping image patches are fed
into the Transformer block b Reassemble operation for assembling tokens into feature
maps c Fusion blocks for combining feature maps
In the paper the authors have introduced several models based on the
used image embedding technique The DPTBase and DPTLarge models
use patchbased embedding where the input image is separated into non
overlapping image patches Then these are fed into the Transformer block
with a learnable position embedding to locate the spatial position of each in
dividual token Figure 11a DPTBase has 12 transformer layers compared
to the DPTLarge which has 24 layers with wide feature sizes The other
model is the DPTHybrid which uses the convolutional backbone ResNet50
as a feature extractor and uses the pixelbased feature maps as token inputs
21to the 12layer transformer block The Transformer blocks reassemble the
tokens with multihead selfattention MSA 1 sequential blocks for global
interaction between tokens The tokens are reassembled into imagelike fea
ture representations in various resolutions Figure 11b Finally these rep
resentations are combined using residual convolutional units in the decoder
and fused together for the nal dense prediction Figure 11c
The experimental results of the dense prediction transformer have pro
vided improved accuracy results over several benchmark dataset compar
isons The results show that for a large training dataset the model has the
best performance The comparisons were done for depth estimations and
semantic segmentation ADE20K dataset is used for segmentation and the
DPTHybrid model has outperformed all the fullyconvolutional models 74
The DPT has the ability to identify precise boundaries of objects with less
distortion The DPT model was also compared with the PASCALContext
dataset after netuning
48 HighResolution Transformer HRFormer
HRFormer 75 is an architecture model that is built using a depthwise
convolutional design with a Feed Forward Network FFN and a local win
dow selfattention mechanism with a multiresolution parallel transformer
module This model is developed for dense prediction tasks focusing on pose
estimation and semantic segmentation The model outperforms the conven
tional ViT model which produces lowresolution outputs The HRFormer is
designed to maintain the highresolution using multiresolution streams and
is more ecient in computational complexity and memory usage
Figure 12 HRFormer architecture adapted from 75 a Selfattention blocks b FFN
with depthwise convolutions
HRFormer has been incorporated by using the HRNet 76 which is a
convolutional network consisting of a multiscale parallel design This ar
chitecture helps to capture feature maps in variant resolutions while main
taining high resolution At each of these resolution blocks partitioning is
22done by creating nonoverlapping windows and selfattention is performed
on each image window separately This improved the eciency signicantly
compared to overlapping local window mechanisms introduced earlier in dif
ferent studies 77 The selfattention blocks Figure 12a are followed by
an FFN with depthwise convolutions Figure 12b to increase the receptive
eld size by information exchange between local windows which is vital in
dense prediction By incorporating a multiresolution parallel transformer
architecture with convolutional multiscale fusions for the overall HRFormer
architecture the information between dierent resolutions is exchanged re
peatedly This process creates a highresolution output with both local and
global context information
49 Maskedattention Mask Transformer Mask2Former
Mask2Former 78 is a new transformer architecture that can be leveraged
to do segmentation tasks including panoptic instance and semantic segmen
tation It is a successful attempt to introduce a universal architecture for the
segmentation tasks which outperforms the current specialized SOTA archi
tectures for each of the segmentation tasks by the time of submission Its key
components consist of a transformer decoder with masked attention Gener
ally a standard Transformer attends to the full feature map In contrast the
masked attention operator in Mask2Former restricts the crossattention to
the foreground region of the predicted mask and then extracts the localized
features This makes the attention mechanism more ecient in this model
Figure 13 Mask2Former architecture adapted from 78 The model consists of a backbone
feature extractor a pixel decoder and a Transformer decoder
23The architecture of Mask2Former is similar in design to the previous
MaskFormer 79 architecture The main components are the backbone fea
ture extractor pixel decoder and the Transformer decoder Figure 13 The
backbone could be either a CNNbased model or a Transformer based model
As the pixel decoder they have used a more advanced multiscale deformable
attention Transformer MSDeformAttn 6 in contrast to the feature pyra
mid network 80 used in MaskFormer 79 Masked attention has been used
to enhance the eectiveness of the Transformer decoder
Despite being a universal architecture for segmentation Mask2Former
still needs to be trained separately for each of the specic tasks This is
a common limitation of the universal architectures for segmentation tasks
Mask2Former has achieved new SOTA performance on all three segmentation
tasks panoptic instance semantic in popular datasets such as COCO and
ADE20K and Cityscapes The semantic segmentation results are compared
for ADE20K and Cityscapes datasets in Table 2
24Datasets
Model Variant Backbone Params M ADE20K Cityscapes PASCALContext
SETR 5SETR Na ve 16160kViTLz2 30567 4806  4880  
SETR PUP 16160k ViTLz31831 4858  5009  
SETR MLA 16160k ViTLz31057 4864  5028  
SETR PUP 1640k ViTLz31831  7839  8157 
SETR PUP 1680k ViTLz31831  7934  8215 
SETR Na ve 1680k ViTLz30567   5289  5361
SETR PUP 1680k ViTLz31831   5440  5527
SETR MLA 1680k ViTLz31057   5487  5583
Swin4SwinT 60 461  
SwinS 81 493  
SwinBz121 516  
SwinLz234 535  
Segmenterx11SegB DeiTBy81 86 4805 805 539
SegBMask DeiTBy86 5008 806 550
SegL ViTLz307 5225 807 565
SegLMask ViTLz307 5363 813 590
SegFormer 69MiTB0y34 374  380 762  781 
MiTB1y131 422  431 785  800 
MiTB2y242 465  475 810  822 
MiTB3y440 494  500 817  833 
MiTB4y608 503  511 823  839 
MiTB5y814 510  518 824 840 
PVTPVTTinyz170 357  
PVTSmallz282 398  
PVT v1 71 PVTMediumz480 416  
PVTLargez651 421  
PVTLargez 651 448  
PVT v2B0z76 372  
PVT v2B1z178 425  
PVT v2 72 PVT v2B2z291 452  
PVT v2B3z490 473  
PVT v2B4z663 479  
PVT v2B5z857 487  
Twins 73TwinsPCPVTSy546 462  475  
TwinsPCPVT TwinsPCPVTBy743 471  484  
TwinsPCPVTLy915 486  498  
TwinsSVTSy544 462  471  
TwinsSVT TwinsSVTBy885 477  489  
TwinsSVTLy133 488  502  
DPTx74DPTHybrid ViTHybridz123 4902  6046
DPTLarge ViTLz343 4763  
HRFormer 75OCRNet7150kHRFormerS 135 440  451  
OCRNet7150k HRFormerB 503 463  476  
OCRNet780k HRFormerS 135  800  810 
OCRNet780k HRFormerB 503  814  820 
OCRNet1580k HRFormerB 503  819  826 576  585
OCRNet760k HRFormerB 503   563  571
OCRNet760k HRFormerS 135   538  546
Mask2Former 78SwinT  477  496  
SwinLz216 561  573  
SwinLFaPNz 564  577  
SwinLz216  833  843 
SwinBz  833  845 
Table 2 Comparison of the ViT models specialized for the task
of semantic segmentation according to mIoU  using dierent
benchmark datasets The bestperforming variant of each model
for a given dataset is highlighted Overall top performing model
variant for each dataset is shaded in gray SS  MS contains both
singlescale and multiscale inferences    Singlescale inference only  x
 Multiscale inference only    patch size iterations    pretrained
on ImageNet1K    pretrained on ImageNet21K    320K training
iterations and multiscale ip testing
255 Discussion
In this survey we discussed how ViTs became a powerful alternative to
classical CNNs in various computer vision applications their strengths as
well as limitations and how ViT contributed to the semantic segmentation
of images with their usage across dierent domains such as remote sensing
medical and video processing Even though we included some of the CNN ar
chitectures widely used in prior mentioned domains to provide a comparison
between the ViT and CNNs an indepth discussion about CNN architectures
is beyond the scope of this paper We have summarized the dierent statis
tics regarding popular datasets used for semantic segmentation tasks and the
results of dierent ViT architectures used for semantic segmentation to give
a clear and highlevel overview for the reader around the region of semantic
segmentation
6 Conclusions and Future Directions
Unlike mature convolutional neural networks ViTs are still in the early
stage of development Nevertheless we observed how powerful and compet
itive they are with their CNN counterparts ViTs are progressing towards
excellence and it is expected that they will replace traditional CNNbased
methods widely used in the deep learning domain in the near future Dier
ent variants of ViTs can be used for experiments with domains such as big
data analytics that require a vast amount of data for processing Exploring
research areas with less adaptation to ViT usage can create more ecient
performanceincreased outcomes for current implementation methods
Even though ViTs have proven successful they can be challenging to ex
periment with due to their high computational demand Thus improvements
to the ViT architecture are needed to make it lightweight and more ecient
This will inspire the community to open new pathways using ViTs
We believe there is a plethora of new research areas that ViT along with
semantic segmentation can be applied to solve realworld problems
References
1 A Vaswani G Brain N Shazeer N Parmar J Uszkoreit L Jones
A N Gomez  Lukasz Kaiser I Polosukhin Attention is all you need
Advances in Neural Information Processing Systems 30 2017
2 A Dosovitskiy L Beyer A Kolesnikov D Weissenborn X Zhai
T Unterthiner M Dehghani M Minderer G Heigold S Gelly et al
26An image is worth 16x16 words Transformers for image recognition at
scale arXiv preprint arXiv201011929 2020
3 CF R Chen Q Fan R Panda Crossvit Crossattention multi
scale vision transformer for image classication in Proceedings of the
IEEECVF international conference on computer vision 2021 pp 357
366
4 Z Liu Y Lin Y Cao H Hu Y Wei Z Zhang S Lin B Guo Swin
transformer Hierarchical vision transformer using shifted windows in
Proceedings of the IEEECVF International Conference on Computer
Vision 2021 pp 1001210022
5 S Zheng J Lu H Zhao X Zhu Z Luo Y Wang Y Fu J Feng
T Xiang P H Torr et al Rethinking semantic segmentation from a
sequencetosequence perspective with transformers in Proceedings of
the IEEECVF conference on computer vision and pattern recognition
2021 pp 68816890
6 X Zhu W Su L Lu B Li X Wang J Dai Deformable detr De
formable transformers for endtoend object detection arXiv preprint
arXiv201004159 2020
7 X Dai Y Chen J Yang P Zhang L Yuan L Zhang Dynamic detr
Endtoend object detection with dynamic attention in Proceedings
of the IEEECVF International Conference on Computer Vision 2021
pp 29882997
8 N Park S Kim How do vision transformers work arXiv preprint
arXiv220206709 2022
9 M Caron H Touvron I Misra H J egou J Mairal P Bojanowski
A Joulin Emerging properties in selfsupervised vision transformers in
Proceedings of the IEEECVF International Conference on Computer
Vision 2021 pp 96509660
10 Z Xu W Zhang T Zhang Z Yang J Li Ecient transformer for re
mote sensing image segmentation Remote Sensing 13 18 2021 3585
11 R Strudel R Garcia I Laptev C Schmid Segmenter Transformer for
semantic segmentation in Proceedings of the IEEECVF International
Conference on Computer Vision 2021 pp 72627272
2712 S Khan M Naseer M Hayat S W Zamir F S Khan M Shah
Transformers in vision A survey ACM computing surveys CSUR
54 10s 2022 141
13 Y Liu Y Zhang Y Wang F Hou J Yuan J Tian Y Zhang Z Shi
J Fan Z He A survey of visual transformers IEEE Transactions on
Neural Networks and Learning Systems 2023
14 K Han Y Wang H Chen X Chen J Guo Z Liu Y Tang A Xiao
C Xu Y Xu et al A survey on vision transformer IEEE transactions
on pattern analysis and machine intelligence 2022
15 J Long E Shelhamer T Darrell Fully convolutional networks for se
mantic segmentation in Proceedings of the IEEE conference on com
puter vision and pattern recognition 2015 pp 34313440
16 O Oktay J Schlemper L L Folgoc M Lee M Heinrich K Mis
awa K Mori S McDonagh N Y Hammerla B Kainz et al At
tention unet Learning where to look for the pancreas arXiv preprint
arXiv180403999 2018
17 F I Diakogiannis F Waldner P Caccetta C Wu Resuneta A deep
learning framework for semantic segmentation of remotely sensed data
ISPRS Journal of Photogrammetry and Remote Sensing 162 2020 94
114
18 Z Zhou M M Rahman Siddiquee N Tajbakhsh J Liang Unet
A nested unet architecture for medical image segmentation in Deep
learning in medical image analysis and multimodal learning for clinical
decision support Springer 2018 pp 311
19 S Hochreiter The vanishing gradient problem during learning recurrent
neural nets and problem solutions International Journal of Uncertainty
Fuzziness and KnowledgeBased Systems 6 02 1998 107116
20 P Ramachandran N Parmar A Vaswani I Bello A Levskaya
J Shlens Standalone selfattention in vision models Advances in Neu
ral Information Processing Systems 32 2019
21 L Zhu J Suomalainen J Liu J Hyypp a H Kaartinen H Haggren
et al A review Remote sensing sensors Multipurposeful application
of geospatial data 2018 1942
2822 M Schmitt J Prexl P Ebel L Liebel X X Zhu Weakly super
vised semantic segmentation of satellite images for land cover mapping
challenges and opportunities arXiv preprint arXiv200208254 2020
23 L P Olander H K Gibbs M Steininger J J Swenson B C Murray
Reference scenarios for deforestation and forest degradation in support
of redd a review of data and methods Environmental Research Letters
3 2 2008 025011
24 F Pacici F Del Frate C Solimini W J Emery An innovative
neuralnet method to detect temporal changes in highresolution op
tical satellite imagery IEEE Transactions on Geoscience and Remote
Sensing 45 9 2007 29402952
25 A Boguszewski D Batorski N ZiembaJankowska T Dziedzic
A Zambrzycka Landcover ai Dataset for automatic mapping of build
ings woodlands water and roads from aerial imagery in Proceedings
of the IEEECVF Conference on Computer Vision and Pattern Recog
nition 2021 pp 11021110
26 L P Osco J M Junior A P M Ramos L A de Castro Jorge S N
Fatholahi J de Andrade Silva E T Matsubara H Pistori W N
Gon calves J Li A review on deep learning in uav remote sensing
International Journal of Applied Earth Observation and Geoinformation
102 2021 102456
27 J Wang Z Zheng A Ma X Lu Y Zhong Loveda A remote sensing
landcover dataset for domain adaptive semantic segmentation arXiv
preprint arXiv211008733 2021
28 I Demir K Koperski D Lindenbaum G Pang J Huang S Basu
F Hughes D Tuia R Raskar Deepglobe 2018 A challenge to parse the
earth through satellite images in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition Workshops 2018 pp 172
181
29 L Ding D Lin S Lin J Zhang X Cui Y Wang H Tang L Bruz
zone Looking outside the window Widecontext transformer for the
semantic segmentation of highresolution remote sensing images IEEE
Transactions on Geoscience and Remote Sensing 60 2022 113
30 S D Olabarriaga A W Smeulders Interaction in the segmentation of
medical images A survey Medical image analysis 5 2 2001 127142
2931 O Ronneberger P Fischer T Brox Unet Convolutional networks for
biomedical image segmentation in International Conference on Medical
image computing and computerassisted intervention Springer 2015
pp 234241
32 Z Gu J Cheng H Fu K Zhou H Hao Y Zhao T Zhang S Gao
J Liu Cenet Context encoder network for 2d medical image segmen
tation IEEE transactions on medical imaging 38 10 2019 22812292
33 H Huang L Lin R Tong H Hu Q Zhang Y Iwamoto X Han Y
W Chen J Wu Unet 3 A fullscale connected unet for medical image
segmentation in ICASSP 20202020 IEEE International Conference on
Acoustics Speech and Signal Processing ICASSP IEEE 2020 pp
10551059
34 J Chen Y Lu Q Yu X Luo E Adeli Y Wang L Lu A L Yuille
Y Zhou Transunet Transformers make strong encoders for medical
image segmentation arXiv preprint arXiv210204306 2021
35 H Cao Y Wang J Chen D Jiang X Zhang Q Tian M Wang
Swinunet Unetlike pure transformer for medical image segmentation
in Computer VisionECCV 2022 Workshops Tel Aviv Israel October
2327 2022 Proceedings Part III Springer 2023 pp 205218
36 A I sn C Direko glu M S ah Review of mribased brain tumor image
segmentation using deep learning methods Procedia Computer Science
102 2016 317324
37 N C Codella D Gutman M E Celebi B Helba M A Marchetti
S W Dusza A Kalloo K Liopyris N Mishra H Kittler et al Skin
lesion analysis toward melanoma detection A challenge at the 2017
international symposium on biomedical imaging isbi hosted by the
international skin imaging collaboration isic in 2018 IEEE 15th in
ternational symposium on biomedical imaging ISBI 2018 IEEE 2018
pp 168172
38 P Bilic P F Christ E Vorontsov G Chlebus H Chen Q Dou CW
Fu X Han PA Heng J Hesser et al The liver tumor segmentation
benchmark lits arXiv preprint arXiv190104056 2019
39 B H Menze A Jakab S Bauer J KalpathyCramer K Farahani
J Kirby Y Burren N Porz J Slotboom R Wiest et al The multi
modal brain tumor image segmentation benchmark brats IEEE trans
actions on medical imaging 34 10 2014 19932024
3040 D Gorecky M Schmitt M Loskyll D Z uhlke Humanmachine
interaction in the industry 40 era in 2014 12th IEEE international
conference on industrial informatics INDIN Ieee 2014 pp 289294
41 R T Azuma A survey of augmented reality Presence teleoperators 
virtual environments 6 4 1997 355385
42 J Janai F G uney A Behl A Geiger et al Computer vision for au
tonomous vehicles Problems datasets and state of the art Foundations
and Trends in Computer Graphics and Vision 12 13 2020 1308
43 T Gevers A Smeulders Image search engines An overview Emerging
Topics in Computer Vision 2004 154
44 S Jain X Wang J E Gonzalez Accel A corrective fusion network
for ecient semantic segmentation on video in Proceedings of the
IEEECVF Conference on Computer Vision and Pattern Recognition
2019 pp 88668875
45 M Ding Z Wang B Zhou J Shi Z Lu P Luo Every frame counts
Joint learning of video segmentation and optical ow in Proceedings
of the AAAI Conference on Articial Intelligence Vol 34 2020 pp
1071310720
46 M Cordts M Omran S Ramos T Rehfeld M Enzweiler R Benen
son U Franke S Roth B Schiele The cityscapes dataset for semantic
urban scene understanding in Proceedings of the IEEE conference on
computer vision and pattern recognition 2016 pp 32133223
47 G J Brostow J Fauqueur R Cipolla Semantic object classes in video
A highdenition ground truth database Pattern Recognition Letters
30 2 2009 8897
48 S R Richter V Vineet S Roth V Koltun Playing for data Ground
truth from computer games in European conference on computer vi
sion Springer 2016 pp 102118
49 E Shelhamer K Rakelly J Homan T Darrell Clockwork convnets
for video semantic segmentation in European Conference on Computer
Vision Springer 2016 pp 852868
50 X Zhu Y Xiong J Dai L Yuan Y Wei Deep feature ow for video
recognition in Proceedings of the IEEE conference on computer vision
and pattern recognition 2017 pp 23492358
3151 B Mahasseni S Todorovic A Fern Budgetaware deep semantic video
segmentation in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition 2017 pp 10291038
52 A GarciaGarcia S OrtsEscolano S Oprea V VillenaMartinez
P MartinezGonzalez J GarciaRodriguez A survey on deep learn
ing techniques for image and video semantic segmentation Applied Soft
Computing 70 2018 4165
53 Y Wang Z Xu X Wang C Shen B Cheng H Shen H Xia Endto
end video instance segmentation with transformers in Proceedings of
the IEEECVF conference on computer vision and pattern recognition
2021 pp 87418750
54 S Yang X Wang Y Li Y Fang J Fang W Liu X Zhao Y Shan
Temporally ecient vision transformer for video instance segmentation
in Proceedings of the IEEECVF Conference on Computer Vision and
Pattern Recognition 2022 pp 28852895
55 J Wu Y Jiang S Bai W Zhang X Bai Seqformer Sequential trans
former for video instance segmentation in Computer VisionECCV
2022 17th European Conference Tel Aviv Israel October 2327 2022
Proceedings Part XXVIII Springer 2022 pp 553569
56 S Gustavsson Object detection and semantic segmentation using self
supervised learning 2021
57 X Liu F Zhang Z Hou L Mian Z Wang J Zhang J Tang Self
supervised learning Generative or contrastive IEEE Transactions on
Knowledge and Data Engineering 2021
58 TY Lin P Goyal R Girshick K He P Doll ar Focal loss for dense
object detection in Proceedings of the IEEE international conference
on computer vision 2017 pp 29802988
59 F Milletari N Navab SA Ahmadi Vnet Fully convolutional neural
networks for volumetric medical image segmentation in 2016 fourth
international conference on 3D vision 3DV IEEE 2016 pp 565571
60 S Jadon A survey of loss functions for semantic segmentation in 2020
IEEE Conference on Computational Intelligence in Bioinformatics and
Computational Biology CIBCB IEEE 2020 pp 17
3261 R Mottaghi X Chen X Liu NG Cho SW Lee S Fidler R Ur
tasun A Yuille The role of context for object detection and semantic
segmentation in the wild in Proceedings of the IEEE conference on
computer vision and pattern recognition 2014 pp 891898
62 M Everingham J Winn The pascal visual object classes challenge 2012
voc2012 development kit Pattern Anal Stat Model Comput Learn
Tech Rep 2007 2012 145
63 B Zhou H Zhao X Puig S Fidler A Barriuso A Torralba Scene
parsing through ade20k dataset in Proceedings of the IEEE conference
on computer vision and pattern recognition 2017 pp 633641
64 S Kuutti R Bowden Y Jin P Barber S Fallah A survey of deep
learning applications to autonomous vehicle control IEEE Transactions
on Intelligent Transportation Systems 22 2 2020 712733
65 G Varma A Subramanian A Namboodiri M Chandraker C Jawa
har Idd A dataset for exploring problems of autonomous navigation
in unconstrained environments in 2019 IEEE Winter Conference on
Applications of Computer Vision WACV IEEE 2019 pp 17431751
66 A Gaidon Q Wang Y Cabon E Vig Virtual worlds as proxy for
multiobject tracking analysis in Proceedings of the IEEE conference
on computer vision and pattern recognition 2016 pp 43404349
67 E Alberti A Tavera C Masone B Caputo Idda a largescale multi
domain dataset for autonomous driving IEEE Robotics and Automation
Letters 5 4 2020 55265533
68 K He X Zhang S Ren J Sun Deep residual learning for image
recognition in Proceedings of the IEEE conference on computer vision
and pattern recognition 2016 pp 770778
69 E Xie W Wang Z Yu A Anandkumar J M Alvarez P Luo
Segformer Simple and ecient design for semantic segmentation with
transformers Advances in Neural Information Processing Systems 34
2021 1207712090
70 X Chu Z Tian B Zhang X Wang X Wei H Xia C Shen Con
ditional positional encodings for vision transformers arXiv preprint
arXiv210210882 2021
3371 W Wang E Xie X Li DP Fan K Song D Liang T Lu P Luo
L Shao Pyramid vision transformer A versatile backbone for dense
prediction without convolutions in Proceedings of the IEEECVF In
ternational Conference on Computer Vision 2021 pp 568578
72 W Wang E Xie X Li DP Fan K Song D Liang T Lu P Luo
L Shao Pvt v2 Improved baselines with pyramid vision transformer
Computational Visual Media 8 3 2022 415424
73 X Chu Z Tian Y Wang B Zhang H Ren X Wei H Xia C Shen
Twins Revisiting the design of spatial attention in vision transformers
Advances in Neural Information Processing Systems 34 2021 9355
9366
74 R Ranftl A Bochkovskiy V Koltun Vision transformers for dense
prediction in Proceedings of the IEEECVF International Conference
on Computer Vision 2021 pp 1217912188
75 Y Yuan R Fu L Huang W Lin C Zhang X Chen J Wang
Hrformer Highresolution vision transformer for dense predict Ad
vances in Neural Information Processing Systems 34 2021 72817293
76 J Wang K Sun T Cheng B Jiang C Deng Y Zhao D Liu Y Mu
M Tan X Wang et al Deep highresolution representation learning for
visual recognition IEEE transactions on pattern analysis and machine
intelligence 43 10 2020 33493364
77 H Hu Z Zhang Z Xie S Lin Local relation networks for image
recognition in Proceedings of the IEEECVF International Conference
on Computer Vision 2019 pp 34643473
78 B Cheng I Misra A G Schwing A Kirillov R Girdhar Masked
attention mask transformer for universal image segmentation in Pro
ceedings of the IEEECVF Conference on Computer Vision and Pattern
Recognition 2022 pp 12901299
79 B Cheng A Schwing A Kirillov Perpixel classication is not all
you need for semantic segmentation Advances in Neural Information
Processing Systems 34 2021 1786417875
80 TY Lin P Doll ar R Girshick K He B Hariharan S Belongie
Feature pyramid networks for object detection in Proceedings of the
IEEE conference on computer vision and pattern recognition 2017 pp
21172125
3481 H Touvron M Cord M Douze F Massa A Sablayrolles H J egou
Training dataecient image transformers  distillation through atten
tion in International Conference on Machine Learning PMLR 2021
pp 1034710357
35
  Relational inductive biases deep learning and graph networks
Peter W Battaglia1Jessica B Hamrick1Victor Bapst1
Alvaro SanchezGonzalez1Vinicius Zambaldi1Mateusz Malinowski1
Andrea Tacchetti1David Raposo1Adam Santoro1Ryan Faulkner1
Caglar Gulcehre1Francis Song1Andrew Ballard1Justin Gilmer2
George Dahl2Ashish Vaswani2Kelsey Allen3Charles Nash4
Victoria Langston1Chris Dyer1Nicolas Heess1
Daan Wierstra1Pushmeet Kohli1Matt Botvinick1
Oriol Vinyals1Yujia Li1Razvan Pascanu1
1DeepMind2Google Brain3MIT4University of Edinburgh
Abstract
Articial intelligence AI has undergone a renaissance recently making major progress in
key domains such as vision language control and decisionmaking This has been due in
part to cheap data and cheap compute resources which have t the natural strengths of deep
learning However many dening characteristics of human intelligence which developed under
much dierent pressures remain out of reach for current approaches In particular generalizing
beyond ones experiencesa hallmark of human intelligence from infancyremains a formidable
challenge for modern AI
The following is part position paper part review and part unication We argue that
combinatorial generalization must be a top priority for AI to achieve humanlike abilities and that
structured representations and computations are key to realizing this objective Just as biology
uses nature and nurture cooperatively we reject the false choice between handengineering
and endtoend learning and instead advocate for an approach which benets from their
complementary strengths We explore how using relational inductive biases within deep learning
architectures can facilitate learning about entities relations and rules for composing them We
present a new building block for the AI toolkit with a strong relational inductive biasthe graph
network which generalizes and extends various approaches for neural networks that operate
on graphs and provides a straightforward interface for manipulating structured knowledge and
producing structured behaviors We discuss how graph networks can support relational reasoning
and combinatorial generalization laying the foundation for more sophisticated interpretable
and exible patterns of reasoning As a companion to this paper we have also released an
opensource software library for building graph networks with demonstrations of how to use
them in practice
1 Introduction
A key signature of human intelligence is the ability to make innite use of nite means Humboldt
1836 Chomsky 1965 in which a small set of elements such as words can be productively
composed in limitless ways such as into new sentences This reects the principle of combinatorial
generalization  that is constructing new inferences predictions and behaviors from known building
blocks Here we explore how to improve modern AIs capacity for combinatorial generalization by
Corresponding author peterbattagliagooglecom
1arXiv180601261v3  csLG  17 Oct 2018biasing learning towards structured representations and computations and in particular systems
that operate on graphs
Humans capacity for combinatorial generalization depends critically on our cognitive mecha
nisms for representing structure and reasoning about relations We represent complex systems as
compositions of entities and their interactions1Navon 1977 McClelland and Rumelhart 1981
Plaut et al 1996 Marcus 2001 Goodwin and JohnsonLaird 2005 Kemp and Tenenbaum 2008
such as judging whether a haphazard stack of objects is stable Battaglia et al 2013 We use
hierarchies to abstract away from negrained dierences and capture more general commonalities
between representations and behaviors Botvinick 2008 Tenenbaum et al 2011 such as parts of
an object objects in a scene neighborhoods in a town and towns in a country We solve novel
problems by composing familiar skills and routines Anderson 1982 for example traveling to a
new location by composing familiar procedures and objectives such as travel by airplane to
San Diego eat at and an Indian restaurant We draw analogies by aligning the relational
structure between two domains and drawing inferences about one based on corresponding knowledge
about the other Gentner and Markman 1997 Hummel and Holyoak 2003
Kenneth Craiks The Nature of Explanation 1943 connects the compositional structure of
the world to how our internal mental models are organized
a human mental model has a similar relationstructure to that of the process it
imitates By relationstructure I do not mean some obscure nonphysical entity which
attends the model but the fact that it is a working physical model which works in the
same way as the process it parallels physical reality is built up apparently from a few
fundamental types of units whose properties determine many of the properties of the
most complicated phenomena and this seems to aord a sucient explanation of the
emergence of analogies between mechanisms and similarities of relationstructure among
these combinations without the necessity of any theory of objective universals Craik
1943 page 5155
That is the world is compositional or at least we understand it in compositional terms When
learning we either t new knowledge into our existing structured representations or adjust the
structure itself to better accommodate and make use of the new and the old Tenenbaum et al
2006 Griths et al 2010 Ullman et al 2017
The question of how to build articial systems which exhibit combinatorial generalization has
been at the heart of AI since its origins and was central to many structured approaches including
logic grammars classic planning graphical models causal reasoning Bayesian nonparametrics and
probabilistic programming Chomsky 1957 Nilsson and Fikes 1970 Pearl 1986 2009 Russell and
Norvig 2009 Hjort et al 2010 Goodman et al 2012 Ghahramani 2015 Entire subelds have
focused on explicit entity and relationcentric learning such as relational reinforcement learning
D zeroski et al 2001 and statistical relational learning Getoor and Taskar 2007 A key reason
why structured approaches were so vital to machine learning in previous eras was in part because
data and computing resources were expensive and the improved sample complexity aorded by
structured approaches strong inductive biases was very valuable
In contrast with past approaches in AI modern deep learning methods LeCun et al 2015
Schmidhuber 2015 Goodfellow et al 2016 often follow an endtoend design philosophy which
emphasizes minimal a priori representational and computational assumptions and seeks to avoid
explicit structure and handengineering This emphasis has t well withand has perhaps been
armed bythe current abundance of cheap data and cheap computing resources which make
1Whether this entails a language of thought Fodor 1975 is beyond the scope of this work
2trading o sample eciency for more exible learning a rational choice The remarkable and rapid
advances across many challenging domains from image classication Krizhevsky et al 2012
Szegedy et al 2017 to natural language processing Sutskever et al 2014 Bahdanau et al 2015
to game play Mnih et al 2015 Silver et al 2016 Morav c k et al 2017 are a testament to this
minimalist principle A prominent example is from language translation where sequencetosequence
approaches Sutskever et al 2014 Bahdanau et al 2015 have proven very eective without using
explicit parse trees or complex relationships between linguistic entities
Despite deep learnings successes however important critiques Marcus 2001 ShalevShwartz
et al 2017 Lake et al 2017 Lake and Baroni 2018 Marcus 2018ab Pearl 2018 Yuille and
Liu 2018 have highlighted key challenges it faces in complex language and scene understanding
reasoning about structured data transferring learning beyond the training conditions and learning
from small amounts of experience These challenges demand combinatorial generalization and so it
is perhaps not surprising that an approach which eschews compositionality and explicit structure
struggles to meet them
When deep learnings connectionist Rumelhart et al 1987 forebears were faced with analogous
critiques from structured symbolic positions Fodor and Pylyshyn 1988 Pinker and Prince 1988
there was a constructive eort Bobrow and Hinton 1990 Marcus 2001 to address the challenges
directly and carefully A variety of innovative subsymbolic approaches for representing and reasoning
about structured objects were developed in domains such as analogymaking linguistic analysis
symbol manipulation and other forms of relational reasoning Smolensky 1990 Hinton 1990
Pollack 1990 Elman 1991 Plate 1995 Eliasmith 2013 as well as more integrative theories for
how the mind works Marcus 2001 Such work also helped cultivate more recent deep learning
advances which use distributed vector representations to capture rich semantic content in text
Mikolov et al 2013 Pennington et al 2014 graphs Narayanan et al 2016 2017 algebraic and
logical expressions Allamanis et al 2017 Evans et al 2018 and programs Devlin et al 2017
Chen et al 2018b
We suggest that a key path forward for modern AI is to commit to combinatorial generalization
as a top priority and we advocate for integrative approaches to realize this goal Just as biology does
not choose between nature versus nurtureit uses nature and nurture jointly  to build wholes which
are greater than the sums of their partswe too reject the notion that structure and exibility are
somehow at odds or incompatible and embrace both with the aim of reaping their complementary
strengths In the spirit of numerous recent examples of principled hybrids of structurebased methods
and deep learning eg Reed and De Freitas 2016 Garnelo et al 2016 Ritchie et al 2016 Wu
et al 2017 Denil et al 2017 Hudson and Manning 2018 we see great promise in synthesizing
new techniques by drawing on the full AI toolkit and marrying the best approaches from today
with those which were essential during times when data and computation were at a premium
Recently a class of models has arisen at the intersection of deep learning and structured
approaches which focuses on approaches for reasoning about explicitly structured data in particular
graphs eg Scarselli et al 2009b Bronstein et al 2017 Gilmer et al 2017 Wang et al 2018c Li
et al 2018 Kipf et al 2018 Gulcehre et al 2018 What these approaches all have in common
is a capacity for performing computation over discrete entities and the relations between them
What sets them apart from classical approaches is how the representations and structure of the
entities and relationsand the corresponding computationscan be learned relieving the burden
of needing to specify them in advance Crucially these methods carry strong relational inductive
biases  in the form of specic architectural assumptions which guide these approaches towards
learning about entities and relations Mitchell 1980 which we joining many others Spelke et al
1992 Spelke and Kinzler 2007 Marcus 2001 Tenenbaum et al 2011 Lake et al 2017 Lake and
Baroni 2018 Marcus 2018b suggest are an essential ingredient for humanlike intelligence
3Box 1 Relational reasoning
We dene structure as the product of composing a set of known building blocks Structured
representations capture this composition ie the arrangement of the elements and structured
computations operate over the elements and their composition as a whole Relational reasoning
then involves manipulating structured representations of entities and relations  using rules
for how they can be composed We use these terms to capture notions from cognitive science
theoretical computer science and AI as follows
Anentity is an element with attributes such as a physical object with a size and mass
Arelation is a property between entities Relations between two objects might include
same size as heavier than  and distance from  Relations can have attributes as
well The relation more than Xtimes heavier than takes an attribute X which
determines the relative weight threshold for the relation to be true vsfalse  Relations
can also be sensitive to the global context For a stone and a feather the relation falls
with greater acceleration than depends on whether the context is in air vsin a
vacuum  Here we focus on pairwise relations between entities
Aruleis a function like a nonbinary logical predicate that maps entities and relations
to other entities and relations such as a scale comparison like is entity X large and
is entity X heavier than entity Y  Here we consider rules which take one or two
arguments unary and binary and return a unary property value
As an illustrative example of relational reasoning in machine learning graphical models Pearl
1988 Koller and Friedman 2009 can represent complex joint distributions by making explicit
random conditional independences among random variables Such models have been very
successful because they capture the sparse structure which underlies many realworld generative
processes and because they support ecient algorithms for learning and reasoning For example
hidden Markov models constrain latent states to be conditionally independent of others given
the state at the previous time step and observations to be conditionally independent given the
latent state at the current time step which are wellmatched to the relational structure of many
realworld causal processes Explicitly expressing the sparse dependencies among variables
provides for various ecient inference and reasoning algorithms such as messagepassing which
apply a common information propagation procedure across localities within a graphical model
resulting in a composable and partially parallelizable reasoning procedure which can be applied
to graphical models of dierent sizes and shape
In the remainder of the paper we examine various deep learning methods through the lens of
their relational inductive biases showing that existing methods often carry relational assumptions
which are not always explicit or immediately evident We then present a general framework for
entity and relationbased reasoningwhich we term graph networks for unifying and extending
existing methods which operate on graphs and describe key design principles for building powerful
architectures using graph networks as building blocks We have also released an opensource library
for building graph networks which can be found here githubcomdeepmindgraph nets 
2 Relational inductive biases
Many approaches in machine learning and AI which have a capacity for relational reasoning
4Box 2 Inductive biases
Learning is the process of apprehending useful knowledge by observing and interacting with the
world It involves searching a space of solutions for one expected to provide a better explanation
of the data or to achieve higher rewards But in many cases there are multiple solutions which
are equally good Goodman 1955 An inductive bias allows a learning algorithm to prioritize
one solution or interpretation over another independent of the observed data Mitchell
1980 In a Bayesian model inductive biases are typically expressed through the choice and
parameterization of the prior distribution Griths et al 2010 In other contexts an inductive
bias might be a regularization term McClelland 1994 added to avoid overtting or it might
be encoded in the architecture of the algorithm itself Inductive biases often trade exibility
for improved sample complexity and can be understood in terms of the biasvariance tradeo
Geman et al 1992 Ideally inductive biases both improve the search for solutions without
substantially diminishing performance as well as help nd solutions which generalize in a
desirable way however mismatched inductive biases can also lead to suboptimal performance
by introducing constraints that are too strong
Inductive biases can express assumptions about either the datagenerating process or the space
of solutions For example when tting a 1D function to data linear least squares follows
the constraint that the approximating function be a linear model and approximation errors
should be minimal under a quadratic penalty This reects an assumption that the data
generating process can be explained simply as a line process corrupted by additive Gaussian
noise Similarly L2 regularization prioritizes solutions whose parameters have small values
and can induce unique solutions and global structure to otherwise illposed problems This can
be interpreted as an assumption about the learning process that searching for good solutions
is easier when there is less ambiguity among solutions Note these assumptions need not be
explicitthey reect interpretations of how a model or algorithm interfaces with the world
Box 1 use a relational inductive bias  While not a precise formal denition we use this term to
refer generally to inductive biases Box 2 which impose constraints on relationships and interactions
among entities in a learning process
Creative new machine learning architectures have rapidly proliferated in recent years with
perhaps not surprisingly given the thesis of this paper practitioners often following a design pattern
of composing elementary building blocks to form more complex deep2computational hierarchies and
graphs3 Building blocks such as fully connected layers are stacked into multilayer perceptrons
MLPs convolutional layers are stacked into convolutional neural networks CNNs and a
standard recipe for an image processing network is generally some variety of CNN composed with
a MLP This composition of layers provides a particular type of relational inductive biasthat
of hierarchical processingin which computations are performed in stages typically resulting in
increasingly long range interactions among information in the input signal As we explore below the
building blocks themselves also carry various relational inductive biases Table 1 Though beyond
the scope of this paper various nonrelational inductive biases are used in deep learning as well for
example activation nonlinearities weight decay dropout Srivastava et al 2014 batch and layer
normalization Ioe and Szegedy 2015 Ba et al 2016 data augmentation training curricula and
optimization algorithms all impose constraints on the trajectory and outcome of learning
2This pattern of composition in depth is ubiquitous in deep learning and is where the deep comes from
3Recent methods Liu et al 2018 even automate architecture construction via learned graph editing procedures
5Component Entities Relations Rel inductive bias Invariance
Fully connected Units Alltoall Weak 
Convolutional Grid elements Local Locality Spatial translation
Recurrent Timesteps Sequential Sequentiality Time translation
Graph network Nodes Edges Arbitrary Node edge permutations
Table 1 Various relational inductive biases in standard deep learning components See also Section 2
To explore the relational inductive biases expressed within various deep learning methods we
must identify several key ingredients analogous to those in Box 1 what are the entities  what are
therelations  and what are the rules for composing entities and relations and computing their
implications In deep learning the entities and relations are typically expressed as distributed
representations and the rules as neural network function approximators however the precise forms
of the entities relations and rules vary between architectures To understand these dierences
between architectures we can further ask how each supports relational reasoning by probing
The arguments to the rule functions eg which entities and relations are provided as input
How the rule function is reused  orshared  across the computational graph eg across dierent
entities and relations across dierent time or processing steps etc
How the architecture denes interactions versus isolation among representations eg by
applying rules to draw conclusions about related entities versus processing them separately
21 Relational inductive biases in standard deep learning building blocks
211 Fully connected layers
Perhaps the most common building block is a fully connected layer Rosenblatt 1961 Typically
implemented as a nonlinear vectorvalued function of vector inputs each element or unit of
the output vector is the dot product between a weight vector followed by an added bias term and
nally a nonlinearity such as a rectied linear unit ReLU As such the entities are the units in
the network the relations are alltoall all units in layer iare connected to all units in layer j
and the rules are specied by the weights and biases The argument to the rule is the full input
signal there is no reuse and there is no isolation of information Figure 1a The implicit relational
inductive bias in a fully connected layer is thus very weak all input units can interact to determine
any output units value independently across outputs Table 1
212 Convolutional layers
Another common building block is a convolutional layer Fukushima 1980 LeCun et al 1989 It is
implemented by convolving an input vector or tensor with a kernel of the same rank adding a bias
term and applying a pointwise nonlinearity The entities here are still individual units or grid
elements eg pixels but the relations are sparser The dierences between a fully connected layer
and a convolutional layer impose some important relational inductive biases locality and translation
invariance Figure 1b Locality reects that the arguments to the relational rule are those entities in
close proximity with one another in the input signals coordinate space isolated from distal entities
Translation invariance reects reuse of the same rule across localities in the input These biases
are very eective for processing natural image data because there is high covariance within local
6a Fully connected
Sharing in space b Convolutional
Sharing in time c Recurrent
Figure 1 Reuse and sharing in common deep learning building blocks a Fully connected layer
in which all weights are independent and there is no sharing b Convolutional layer in which
a local kernel function is reused multiple times across the input Shared weights are indicated by
arrows with the same color c Recurrent layer in which the same function is reused across dierent
processing steps
neighborhoods which diminishes with distance and because the statistics are mostly stationary
across an image Table 1
213 Recurrent layers
A third common building block is a recurrent layer Elman 1990 which is implemented over a
sequence of steps Here we can view the inputs and hidden states at each processing step as the
entities and the Markov dependence of one steps hidden state on the previous hidden state and
the current input as the relations The rule for combining the entities takes a steps inputs and
hidden state as arguments to update the hidden state The rule is reused over each step Figure 1c
which reects the relational inductive bias of temporal invariance similar to a CNNs translational
invariance in space For example the outcome of some physical sequence of events should not
depend on the time of day RNNs also carry a bias for locality in the sequence via their Markovian
structure Table 1
22 Computations over sets and graphs
While the standard deep learning toolkit contains methods with various forms of relational inductive
biases there is no default deep learning component which operates on arbitrary relational structure
We need models with explicit representations of entities and relations and learning algorithms which
nd rules for computing their interactions as well as ways of grounding them in data Importantly
entities in the world such as objects and agents do not have a natural order rather orderings
can be dened by the properties of their relations For example the relations between the sizes of
a set of objects can potentially be used to order them as can their masses ages toxicities and
prices Invariance to orderingexcept in the face of relationsis a property that should ideally be
reected by a deep learning component for relational reasoning
Sets are a natural representation for systems which are described by entities whose order is
undened or irrelevant in particular their relational inductive bias does not come from the presence
of something but rather from the absence  For illustration consider the task of predicting the center
7H H
O
The brown 
dog jumped The 
brown dog jumped a b 
c d 
e f 
Molecule MassSpring System 
nbody System Rigid Body System 
Sentence and Parse Tree Image and FullyConnected Scene Graph Figure 2 Dierent graph representations a A molecule in which each atom is represented as a
node and edges correspond to bonds eg Duvenaud et al 2015 b A massspring system in
which the rope is dened by a sequence of masses which are represented as nodes in the graph eg
Battaglia et al 2016 Chang et al 2017 c A nbody system in which the bodies are nodes and
the underlying graph is fully connected eg Battaglia et al 2016 Chang et al 2017 d A rigid
body system in which the balls and walls are nodes and the underlying graph denes interactions
between the balls and between the balls and the walls eg Battaglia et al 2016 Chang et al 2017
e A sentence in which the words correspond to leaves in a tree and the other nodes and edges
could be provided by a parser eg Socher et al 2013 Alternately a fully connected graph could
be used eg Vaswani et al 2017 f An image which can be decomposed into image patches
corresponding to nodes in a fully connected graph eg Santoro et al 2017 Wang et al 2018c
of mass of a solar system comprised of nplanets whose attributes eg mass position velocity
etc are denoted by fx1x2xng For such a computation the order in which we consider the
planets does not matter because the state can be described solely in terms of aggregated averaged
quantities However if we were to use a MLP for this task having learned the prediction for a
particular input  x1x2xn would not necessarily transfer to making a prediction for the same
inputs under a dierent ordering  xnx1x2 Since there are n such possible permutations in
the worst case the MLP could consider each ordering as fundamentally dierent and thus require
an exponential number of inputoutput training examples to learn an approximating function
A natural way to handle such combinatorial explosion is to only allow the prediction to depend
on symmetric functions of the inputs attributes This might mean computing shared perobject
8featuresffx1f xngwhich are then aggregated in a symmetric way for example by taking
their mean Such an approach is the essence of the Deep Sets and related models Zaheer et al
2017 Edwards and Storkey 2016 Pevn y and Somol 2017 which we explore further in Section 423
Of course permutation invariance is not the only important form of underlying structure in
many problems For example each object in a set may be aected by pairwise interactions with
the other objects in the set Hartford et al 2018 In our planets scenario consider now the
task of predicting each individual planets position after a time interval  t In this case using
aggregated averaged information is not enough because the movement of each planet depends on
the forces the other planets are exerting on it Instead we could compute the state of each object
asx0
ifxiP
jgxixj wheregcould compute the force induced by the jth planet on the
ith planet and fcould compute the future state of the ith planet which results from the forces
and dynamics The fact that we use the same geverywhere is again a consequence of the global
permutation invariance of the system however it also supports a dierent relational structure
becausegnow takes two arguments rather than one4
The above solar system examples illustrate two relational structures one in which there are
no relations and one which consists of all pairwise relations Many realworld systems such as
in Figure 2 have a relational structure somewhere in between these two extremes however with
some pairs of entities possessing a relation and others lacking one In our solar system example if
the system instead consists of the planets and their moons one may be tempted to approximate
it by neglecting the interactions between moons of dierent planets In practice this means
computing interactions only between some pairs of objects ie x0
ifxiP
j2igxixj where
if1ngis a neighborhood around node i This corresponds to a graph in that the ith
object only interacts with a subset of the other objects described by its neighborhood Note the
updated states still do not depend in the order in which we describe the neighborhood5
Graphs generally are a representation which supports arbitrary pairwise relational struc
ture and computations over graphs aord a strong relational inductive bias beyond that which
convolutional and recurrent layers can provide
3 Graph networks
Neural networks that operate on graphs and structure their computations accordingly have been
developed and explored extensively for more than a decade under the umbrella of graph neural
networks Gori et al 2005 Scarselli et al 2005 2009a Li et al 2016 but have grown rapidly
in scope and popularity in recent years We survey the literature on these methods in the next
subsection 31 Then in the remaining subsections we present our graph networks framework
which generalizes and extends several lines of work in this area
31 Background
Models in the graph neural network family Gori et al 2005 Scarselli et al 2005 2009a Li et al
2016 have been explored in a diverse range of problem domains across supervised semisupervised
unsupervised and reinforcement learning settings They have been eective at tasks thought to
have rich relational structure such as visual scene understanding tasks Raposo et al 2017 Santoro
4We could extend this same analysis to increasingly entangled structures that depend on relations among triplets
iegxixjxk quartets and so on We note that if we restrict these functions to only operate on subsets of xi
which are spatially close then we end back up with something resembling CNNs In the most entangled sense where
there is a single relation function gx1    xn we end back up with a construction similar to a fully connected layer
5The invariance which this model enforces is the invariance under isomorphism of the graph
9et al 2017 and fewshot learning Garcia and Bruna 2018 They have also been used to learn
the dynamics of physical systems Battaglia et al 2016 Chang et al 2017 Watters et al 2017
van Steenkiste et al 2018 SanchezGonzalez et al 2018 and multiagent systems Sukhbaatar
et al 2016 Hoshen 2017 Kipf et al 2018 to reason about knowledge graphs Bordes et al 2013
O noroRubio et al 2017 Hamaguchi et al 2017 to predict the chemical properties of molecules
Duvenaud et al 2015 Gilmer et al 2017 to predict trac on roads Li et al 2017 Cui et al
2018 to classify and segment images and videos Wang et al 2018c Hu et al 2017 and 3D
meshes and point clouds Wang et al 2018d to classify regions in images Chen et al 2018a to
perform semisupervised text classication Kipf and Welling 2017 and in machine translation
Vaswani et al 2017 Shaw et al 2018 Gulcehre et al 2018 They have been used within both
modelfree Wang et al 2018b and modelbased Hamrick et al 2017 Pascanu et al 2017
SanchezGonzalez et al 2018 continuous control for modelfree reinforcement learning Hamrick
et al 2018 Zambaldi et al 2018 and for more classical approaches to planning Toyer et al
2017
Many traditional computer science problems which involve reasoning about discrete entities and
structure have also been explored with graph neural networks such as combinatorial optimization
Bello et al 2016 Nowak et al 2017 Dai et al 2017 boolean satisability Selsam et al 2018
program representation and verication Allamanis et al 2018 Li et al 2016 modeling cellular
automata and Turing machines Johnson 2017 and performing inference in graphical models
Yoon et al 2018 Recent work has also focused on building generative models of graphs Li et al
2018 De Cao and Kipf 2018 You et al 2018 Bojchevski et al 2018 and unsupervised learning of
graph embeddings Perozzi et al 2014 Tang et al 2015 Grover and Leskovec 2016 Garc aDur an
and Niepert 2017
The works cited above are by no means an exhaustive list but provide a representative cross
section of the breadth of domains for which graph neural networks have proven useful We point
interested readers to a number of existing reviews which examine the body of work on graph neural
networks in more depth In particular Scarselli et al 2009a provides an authoritative overview of
early graph neural network approaches Bronstein et al 2017 provides an excellent survey of deep
learning on nonEuclidean data and explores graph neural nets graph convolution networks and
related spectral approaches Recently Gilmer et al 2017 introduced the messagepassing neural
network MPNN which unied various graph neural network and graph convolutional network
approaches Monti et al 2017 Bruna et al 2014 Hena et al 2015 Deerrard et al 2016
Niepert et al 2016 Kipf and Welling 2017 Bronstein et al 2017 by analogy to messagepassing
in graphical models In a similar vein Wang et al 2018c introduced the nonlocal neural network
NLNN which unied various selfattentionstyle methods Vaswani et al 2017 Hoshen 2017
Veli ckovi c et al 2018 by analogy to methods from computer vision and graphical models for
capturing long range dependencies in signals
32 Graph network GN block
We now present our graph networks GN framework which denes a class of functions for
relational reasoning over graphstructured representations Our GN framework generalizes and
extends various graph neural network MPNN and NLNN approaches Scarselli et al 2009a Gilmer
et al 2017 Wang et al 2018c and supports constructing complex architectures from simple
building blocks Note we avoided using the term neural in the graph network label to reect
that they can be implemented with functions other than neural networks though here our focus is
on neural network implementations
The main unit of computation in the GN framework is the GN block  a graphtograph module
10Box 3 Our denition of graph
Attributesviek
latexit sha1_base644ton1cC0WpHTbJYOP5RCFkcwwAAAB83icbVDLSsNAFL3xWeur6tLNYBG6KokIuiy4cVnBPqAJZTK9aYdOJmFmIpTQ33DjQhG3ow78ZJm4W2Hhg4nHMv98wJU8G1cd1vZ2Nza3tnt7JX3T84PDqunZx2dZIphh2WiET1Q6pRcIkdw43AfqqQxqHAXji9KzeEyrNEloZikGMR1LHnFGjZV8P6ZmEkY5zofTYa3uNt0FyDrxSlKHEu1h7csfJSyLURomqNYDz01NkFNlOBM4rqZxpSyKR3jwFJJY9RBvsg8J5dWGZEoUfZJQxbq742cxlrP4tBOFhn1qleI3mDzES3Qc5lmhmUbHkoygQxCSkKICOukBkxs4QyxW1WwiZUUWZsTVVbgrf65XXSvWp6btN7uK63GmUdFTiHC2iABzfQgntoQwcYpPAMrDmZM6L858LEc3nHLnDP7AfwBXXGRzQlatexitlatexit sha1_base644ton1cC0WpHTbJYOP5RCFkcwwAAAB83icbVDLSsNAFL3xWeur6tLNYBG6KokIuiy4cVnBPqAJZTK9aYdOJmFmIpTQ33DjQhG3ow78ZJm4W2Hhg4nHMv98wJU8G1cd1vZ2Nza3tnt7JX3T84PDqunZx2dZIphh2WiET1Q6pRcIkdw43AfqqQxqHAXji9KzeEyrNEloZikGMR1LHnFGjZV8P6ZmEkY5zofTYa3uNt0FyDrxSlKHEu1h7csfJSyLURomqNYDz01NkFNlOBM4rqZxpSyKR3jwFJJY9RBvsg8J5dWGZEoUfZJQxbq742cxlrP4tBOFhn1qleI3mDzES3Qc5lmhmUbHkoygQxCSkKICOukBkxs4QyxW1WwiZUUWZsTVVbgrf65XXSvWp6btN7uK63GmUdFTiHC2iABzfQgntoQwcYpPAMrDmZM6L858LEc3nHLnDP7AfwBXXGRzQlatexitlatexit sha1_base644ton1cC0WpHTbJYOP5RCFkcwwAAAB83icbVDLSsNAFL3xWeur6tLNYBG6KokIuiy4cVnBPqAJZTK9aYdOJmFmIpTQ33DjQhG3ow78ZJm4W2Hhg4nHMv98wJU8G1cd1vZ2Nza3tnt7JX3T84PDqunZx2dZIphh2WiET1Q6pRcIkdw43AfqqQxqHAXji9KzeEyrNEloZikGMR1LHnFGjZV8P6ZmEkY5zofTYa3uNt0FyDrxSlKHEu1h7csfJSyLURomqNYDz01NkFNlOBM4rqZxpSyKR3jwFJJY9RBvsg8J5dWGZEoUfZJQxbq742cxlrP4tBOFhn1qleI3mDzES3Qc5lmhmUbHkoygQxCSkKICOukBkxs4QyxW1WwiZUUWZsTVVbgrf65XXSvWp6btN7uK63GmUdFTiHC2iABzfQgntoQwcYpPAMrDmZM6L858LEc3nHLnDP7AfwBXXGRzQlatexitlatexit sha1_base644ton1cC0WpHTbJYOP5RCFkcwwAAAB83icbVDLSsNAFL3xWeur6tLNYBG6KokIuiy4cVnBPqAJZTK9aYdOJmFmIpTQ33DjQhG3ow78ZJm4W2Hhg4nHMv98wJU8G1cd1vZ2Nza3tnt7JX3T84PDqunZx2dZIphh2WiET1Q6pRcIkdw43AfqqQxqHAXji9KzeEyrNEloZikGMR1LHnFGjZV8P6ZmEkY5zofTYa3uNt0FyDrxSlKHEu1h7csfJSyLURomqNYDz01NkFNlOBM4rqZxpSyKR3jwFJJY9RBvsg8J5dWGZEoUfZJQxbq742cxlrP4tBOFhn1qleI3mDzES3Qc5lmhmUbHkoygQxCSkKICOukBkxs4QyxW1WwiZUUWZsTVVbgrf65XXSvWp6btN7uK63GmUdFTiHC2iABzfQgntoQwcYpPAMrDmZM6L858LEc3nHLnDP7AfwBXXGRzQlatexituvskvrkuviek
Here we use graph to mean a directed attributed multigraph with a global attribute In our
terminology a node is denoted as vi an edge as ek and the global attributes as u We also use
skandrkto indicate the indices of the sender and receiver nodes see below respectively for
edgek To be more precise we dene these terms as
Directed  oneway edges from a sender node to a receiver node
Attribute  properties that can be encoded as a vector set or even another graph
Attributed  edges and vertices have attributes associated with them
Global attribute  a graphlevel attribute
Multigraph  there can be more than one edge between vertices including selfedges
Figure 2 shows a variety of dierent types of graphs corresponding to real data that we may be
interested in modeling including physical systems molecules images and text
which takes a graph as input performs computations over the structure and returns a graph as
output As described in Box 3 entities are represented by the graphs nodes  relations by the edges 
and systemlevel properties by global attributes The GN frameworks block organization emphasizes
customizability and synthesizing new architectures which express desired relational inductive biases
The key design principles are Flexible representations see Section 41 Congurable withinblock
structure see Section 42 and Composable multiblock architectures see Section 43
We introduce a motivating example to help make the GN formalism more concrete Consider
predicting the movements a set of rubber balls in an arbitrary gravitational eld which instead of
bouncing against one another each have one or more springs which connect them to some or all of
the others We will refer to this running example throughout the denitions below to motivate the
graph representation and the computations operating over it Figure 2 depicts some other common
scenarios that can be represented by graphs and reasoned over using graph networks
321 Denition of graph
Within our GN framework a graph is dened as a 3tuple GuVE see Box 3 for details of
graph representations The uis a global attribute for example umight represent the gravitational
eld TheVfvigi1Nvis the set of nodes of cardinality Nv where each viis a nodes attribute
For example Vmight represent each ball with attributes for position velocity and mass The
Efekrkskgk1Neis the set of edges of cardinality Ne where each ekis the edges attribute
rkis the index of the receiver node and skis the index of the sender node For example Emight
represent the presence of springs between dierent balls and their corresponding spring constants
11Algorithm 1 Steps of computation in a full GN block
function GraphNetwork EVu
fork2f1Negdo
e0
k eekvrkvsku 1 Compute updated edge attributes
end for
fori2f1Nngdo
letE0
ife0
krkskgrkik1Ne
 e0
i evE0
i 2 Aggregate edge attributes per node
v0
i v e0
iviu 3 Compute updated node attributes
end for
letV0fv0gi1Nv
letE0fe0
krkskgk1Ne
 e0 euE0 4 Aggregate edge attributes globally
 v0 vuV0 5 Aggregate node attributes globally
u0 u e0 v0u 6 Compute updated global attribute
return E0V0u0
end function
322 Internal structure of a GN block
A GN block contains three update functions  and three aggregation functions 
e0
keekvrkvsku
v0
iv
 e0
iviu
u0u
 e0 v0u e0
iev
E0
i
 e0eu
E0
 v0vu
V01
whereE0
ife0
krkskgrkik1NeV0fv0
igi1Nv andE0S
iE0
ife0
krkskgk1Ne
Theeis mapped across all edges to compute peredge updates the vis mapped across all
nodes to compute pernode updates and the uis applied once as the global update The 
functions each take a set as input and reduce it to a single element which represents the aggregated
information Crucially the functions must be invariant to permutations of their inputs and should
take variable numbers of arguments eg elementwise summation mean maximum etc
323 Computational steps within a GN block
When a graph G is provided as input to a GN block the computations proceed from the edge to
the node to the global level Figure 3 shows a depiction of which graph elements are involved in
each of these computations and Figure 4a shows a full GN block with its update and aggregation
functions Algorithm 1 shows the following steps of computation
1eis applied per edge with arguments  ekvrkvsku and returns e0
k In our springs example
this might correspond to the forces or potential energies between two connected balls The
set of resulting peredge outputs for each node i isE0
ife0
krkskgrkik1Ne And
E0S
iE0
ife0
krkskgk1Neis the set of all peredge outputs
2evis applied to E0
i and aggregates the edge updates for edges that project to vertex i into
 e0
i which will be used in the next steps node update In our running example this might
correspond to summing all the forces or potential energies acting on the ithball
12vi
latexit sha1_base64UuhsKP3lpHlYK0A8uvGImQtNkIAAAB83icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFNKJPppB06mYSZm0IJQ03LhRx68482ctllo64GBwzn3cscMJXCoOtO6Wt7Z3dvfJ5eDw6PikenrWMUmmGWzRCa6F1LDpVC8jQIl76Wa0ziUvBtO7hdd8q1EYl6wlnKg5iOlIgEo2gl348pjsMon84HYlCtuQ13CbJJvILUoEBrUP3yhwnLYq6QSWpM33NTDHKqUTDJ5xUMzylbEJHvGpojE3Qb7MPCdXVhmSKNH2KSRL9fdGTmNjZnFoJxcZzbq3EPzhlGd0EuVJohV2x1KMokwYQsCiBDoTlDObOEMi1sVsLGVFOGtqaKLcFbIm6Vw3PLfhPd7UmvWijjJcwCXUwYNbaMIDtKANDFJ4hld4czLnxXl3PlajJafYOYccD5AHRgkdwlatexitlatexit sha1_base64UuhsKP3lpHlYK0A8uvGImQtNkIAAAB83icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFNKJPppB06mYSZm0IJQ03LhRx68482ctllo64GBwzn3cscMJXCoOtO6Wt7Z3dvfJ5eDw6PikenrWMUmmGWzRCa6F1LDpVC8jQIl76Wa0ziUvBtO7hdd8q1EYl6wlnKg5iOlIgEo2gl348pjsMon84HYlCtuQ13CbJJvILUoEBrUP3yhwnLYq6QSWpM33NTDHKqUTDJ5xUMzylbEJHvGpojE3Qb7MPCdXVhmSKNH2KSRL9fdGTmNjZnFoJxcZzbq3EPzhlGd0EuVJohV2x1KMokwYQsCiBDoTlDObOEMi1sVsLGVFOGtqaKLcFbIm6Vw3PLfhPd7UmvWijjJcwCXUwYNbaMIDtKANDFJ4hld4czLnxXl3PlajJafYOYccD5AHRgkdwlatexitlatexit sha1_base64UuhsKP3lpHlYK0A8uvGImQtNkIAAAB83icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFNKJPppB06mYSZm0IJQ03LhRx68482ctllo64GBwzn3cscMJXCoOtO6Wt7Z3dvfJ5eDw6PikenrWMUmmGWzRCa6F1LDpVC8jQIl76Wa0ziUvBtO7hdd8q1EYl6wlnKg5iOlIgEo2gl348pjsMon84HYlCtuQ13CbJJvILUoEBrUP3yhwnLYq6QSWpM33NTDHKqUTDJ5xUMzylbEJHvGpojE3Qb7MPCdXVhmSKNH2KSRL9fdGTmNjZnFoJxcZzbq3EPzhlGd0EuVJohV2x1KMokwYQsCiBDoTlDObOEMi1sVsLGVFOGtqaKLcFbIm6Vw3PLfhPd7UmvWijjJcwCXUwYNbaMIDtKANDFJ4hld4czLnxXl3PlajJafYOYccD5AHRgkdwlatexitlatexit sha1_base64UuhsKP3lpHlYK0A8uvGImQtNkIAAAB83icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFNKJPppB06mYSZm0IJQ03LhRx68482ctllo64GBwzn3cscMJXCoOtO6Wt7Z3dvfJ5eDw6PikenrWMUmmGWzRCa6F1LDpVC8jQIl76Wa0ziUvBtO7hdd8q1EYl6wlnKg5iOlIgEo2gl348pjsMon84HYlCtuQ13CbJJvILUoEBrUP3yhwnLYq6QSWpM33NTDHKqUTDJ5xUMzylbEJHvGpojE3Qb7MPCdXVhmSKNH2KSRL9fdGTmNjZnFoJxcZzbq3EPzhlGd0EuVJohV2x1KMokwYQsCiBDoTlDObOEMi1sVsLGVFOGtqaKLcFbIm6Vw3PLfhPd7UmvWijjJcwCXUwYNbaMIDtKANDFJ4hld4czLnxXl3PlajJafYOYccD5AHRgkdwlatexitu
latexit sha1_base64WlNKcf4FQq41kPZqpr8GSpKP8AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GEoUbF4q49WcTdO2iy09cDA4Zx7mXNPkAiujetO6WNza3tnfJuZW4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0jWDmSXoR3QsecgZNVZ6HETUTIIwSfDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzFRpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslBb2Q00noWBXYyT6hXvVz8zunJrzxMy6T1KBky4CVBATkx8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZLoowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucUsD5AHw2JDlatexitlatexit sha1_base64WlNKcf4FQq41kPZqpr8GSpKP8AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GEoUbF4q49WcTdO2iy09cDA4Zx7mXNPkAiujetO6WNza3tnfJuZW4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0jWDmSXoR3QsecgZNVZ6HETUTIIwSfDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzFRpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslBb2Q00noWBXYyT6hXvVz8zunJrzxMy6T1KBky4CVBATkx8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZLoowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucUsD5AHw2JDlatexitlatexit sha1_base64WlNKcf4FQq41kPZqpr8GSpKP8AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GEoUbF4q49WcTdO2iy09cDA4Zx7mXNPkAiujetO6WNza3tnfJuZW4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0jWDmSXoR3QsecgZNVZ6HETUTIIwSfDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzFRpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslBb2Q00noWBXYyT6hXvVz8zunJrzxMy6T1KBky4CVBATkx8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZLoowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucUsD5AHw2JDlatexitlatexit sha1_base64WlNKcf4FQq41kPZqpr8GSpKP8AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GEoUbF4q49WcTdO2iy09cDA4Zx7mXNPkAiujetO6WNza3tnfJuZW4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0jWDmSXoR3QsecgZNVZ6HETUTIIwSfDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzFRpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslBb2Q00noWBXYyT6hXvVz8zunJrzxMy6T1KBky4CVBATkx8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZLoowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucUsD5AHw2JDlatexite0k
latexit sha1_base64a1hco1MShws4KpmpFnenOcfEqycAAAB9HicdVDLSgMxFM34rPVVdekmWMSuhkxtsd0V3LisYBQDiWT3mlDMwTTKEMQ43LhRx68e482MtBVU9EDgcM693JPjxYIrTciHtbasbm1ndvJ77tHxwWjo7bKkokgxaLRCS7HlUgeAgtzbWAbiyBBp6Ajje5zvzOFKTiUXinZzG4AR2F3OeMaiO5YDqseenML8YTAaFIrGdCilXy5jY1XqdVOqG1KqXpEywY5MFimiF5qDw3h9GLAkg1ExQpXoOibWbUqk5EzDP9xMFMWUTOoKeoSENQLnpIvQcnxtliP1ImhdqvFCb6Q0UGoWeGYyC6le5n4l9dLtF9zUx7GiYaQLQ5icA6wlkDeMglMC1mhlAmucmK2ZhKyrTpKW9KPopp0y7ZjurqtFBulVR05dIrOUAk56Ao10A1qohZi6B49oCf0bE2tRvFel2OrlmrnRP0A9bbJyRskkIlatexitlatexit sha1_base64a1hco1MShws4KpmpFnenOcfEqycAAAB9HicdVDLSgMxFM34rPVVdekmWMSuhkxtsd0V3LisYBQDiWT3mlDMwTTKEMQ43LhRx68e482MtBVU9EDgcM693JPjxYIrTciHtbasbm1ndvJ77tHxwWjo7bKkokgxaLRCS7HlUgeAgtzbWAbiyBBp6Ajje5zvzOFKTiUXinZzG4AR2F3OeMaiO5YDqseenML8YTAaFIrGdCilXy5jY1XqdVOqG1KqXpEywY5MFimiF5qDw3h9GLAkg1ExQpXoOibWbUqk5EzDP9xMFMWUTOoKeoSENQLnpIvQcnxtliP1ImhdqvFCb6Q0UGoWeGYyC6le5n4l9dLtF9zUx7GiYaQLQ5icA6wlkDeMglMC1mhlAmucmK2ZhKyrTpKW9KPopp0y7ZjurqtFBulVR05dIrOUAk56Ao10A1qohZi6B49oCf0bE2tRvFel2OrlmrnRP0A9bbJyRskkIlatexitlatexit sha1_base64a1hco1MShws4KpmpFnenOcfEqycAAAB9HicdVDLSgMxFM34rPVVdekmWMSuhkxtsd0V3LisYBQDiWT3mlDMwTTKEMQ43LhRx68e482MtBVU9EDgcM693JPjxYIrTciHtbasbm1ndvJ77tHxwWjo7bKkokgxaLRCS7HlUgeAgtzbWAbiyBBp6Ajje5zvzOFKTiUXinZzG4AR2F3OeMaiO5YDqseenML8YTAaFIrGdCilXy5jY1XqdVOqG1KqXpEywY5MFimiF5qDw3h9GLAkg1ExQpXoOibWbUqk5EzDP9xMFMWUTOoKeoSENQLnpIvQcnxtliP1ImhdqvFCb6Q0UGoWeGYyC6le5n4l9dLtF9zUx7GiYaQLQ5icA6wlkDeMglMC1mhlAmucmK2ZhKyrTpKW9KPopp0y7ZjurqtFBulVR05dIrOUAk56Ao10A1qohZi6B49oCf0bE2tRvFel2OrlmrnRP0A9bbJyRskkIlatexitlatexit sha1_base64a1hco1MShws4KpmpFnenOcfEqycAAAB9HicdVDLSgMxFM34rPVVdekmWMSuhkxtsd0V3LisYBQDiWT3mlDMwTTKEMQ43LhRx68e482MtBVU9EDgcM693JPjxYIrTciHtbasbm1ndvJ77tHxwWjo7bKkokgxaLRCS7HlUgeAgtzbWAbiyBBp6Ajje5zvzOFKTiUXinZzG4AR2F3OeMaiO5YDqseenML8YTAaFIrGdCilXy5jY1XqdVOqG1KqXpEywY5MFimiF5qDw3h9GLAkg1ExQpXoOibWbUqk5EzDP9xMFMWUTOoKeoSENQLnpIvQcnxtliP1ImhdqvFCb6Q0UGoWeGYyC6le5n4l9dLtF9zUx7GiYaQLQ5icA6wlkDeMglMC1mhlAmucmK2ZhKyrTpKW9KPopp0y7ZjurqtFBulVR05dIrOUAk56Ao10A1qohZi6B49oCf0bE2tRvFel2OrlmrnRP0A9bbJyRskkIlatexita Edge update
u
latexit sha1_base64WlNKcf4FQq41kPZqpr8GSpKP8AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GEoUbF4q49WcTdO2iy09cDA4Zx7mXNPkAiujetO6WNza3tnfJuZW4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0jWDmSXoR3QsecgZNVZ6HETUTIIwSfDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzFRpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslBb2Q00noWBXYyT6hXvVz8zunJrzxMy6T1KBky4CVBATkx8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZLoowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucUsD5AHw2JDlatexitlatexit sha1_base64WlNKcf4FQq41kPZqpr8GSpKP8AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GEoUbF4q49WcTdO2iy09cDA4Zx7mXNPkAiujetO6WNza3tnfJuZW4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0jWDmSXoR3QsecgZNVZ6HETUTIIwSfDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzFRpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslBb2Q00noWBXYyT6hXvVz8zunJrzxMy6T1KBky4CVBATkx8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZLoowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucUsD5AHw2JDlatexitlatexit sha1_base64WlNKcf4FQq41kPZqpr8GSpKP8AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GEoUbF4q49WcTdO2iy09cDA4Zx7mXNPkAiujetO6WNza3tnfJuZW4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0jWDmSXoR3QsecgZNVZ6HETUTIIwSfDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzFRpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslBb2Q00noWBXYyT6hXvVz8zunJrzxMy6T1KBky4CVBATkx8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZLoowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucUsD5AHw2JDlatexitlatexit sha1_base64WlNKcf4FQq41kPZqpr8GSpKP8AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GEoUbF4q49WcTdO2iy09cDA4Zx7mXNPkAiujetO6WNza3tnfJuZW4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0jWDmSXoR3QsecgZNVZ6HETUTIIwSfDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzFRpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslBb2Q00noWBXYyT6hXvVz8zunJrzxMy6T1KBky4CVBATkx8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZLoowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucUsD5AHw2JDlatexite0k
latexit sha1_base64TmBm7ikN3ChoJpDcsfwhm1T5rLkAAAB9HicbVDLSgMxFL1TX7Wqi7dBIvYVZkRQZcFNy4r2Ae0Q8mkd9rQTGZMMoUy9DvcuFDErRjzr8x03ahrQcCh3Pu5Z6cIBFcG9f9dgobm1vbO8Xd0t7weFRfikpeNUMWyyWMSqE1CNgktsGm4EdhKFNAoEtoPxXe63J6g0jWjmSboR3QoecgZNVbyexE1oyDMcHbZHfLFbfmzkHWibckFVii0S99QYxSyOUhgmqdddzENnVBnOBM5KvVRjQtmYDrFrqaQRajbh56RC6sMSBgr6QhcX3RkYjradRYCfzkHrVy8XvG5qwls4zJJDUq2OBSmgpiY5A2QAVfIjJhaQpniNithI6ooM7anki3BW3yOmld1Ty35j1cVrVZR1FOINzqIIHN1CHe2hAExg8wTO8wpszcV6cddjMVpwljun8AfO5wCAJHlatexitlatexit sha1_base64TmBm7ikN3ChoJpDcsfwhm1T5rLkAAAB9HicbVDLSgMxFL1TX7Wqi7dBIvYVZkRQZcFNy4r2Ae0Q8mkd9rQTGZMMoUy9DvcuFDErRjzr8x03ahrQcCh3Pu5Z6cIBFcG9f9dgobm1vbO8Xd0t7weFRfikpeNUMWyyWMSqE1CNgktsGm4EdhKFNAoEtoPxXe63J6g0jWjmSboR3QoecgZNVbyexE1oyDMcHbZHfLFbfmzkHWibckFVii0S99QYxSyOUhgmqdddzENnVBnOBM5KvVRjQtmYDrFrqaQRajbh56RC6sMSBgr6QhcX3RkYjradRYCfzkHrVy8XvG5qwls4zJJDUq2OBSmgpiY5A2QAVfIjJhaQpniNithI6ooM7anki3BW3yOmld1Ty35j1cVrVZR1FOINzqIIHN1CHe2hAExg8wTO8wpszcV6cddjMVpwljun8AfO5wCAJHlatexitlatexit sha1_base64TmBm7ikN3ChoJpDcsfwhm1T5rLkAAAB9HicbVDLSgMxFL1TX7Wqi7dBIvYVZkRQZcFNy4r2Ae0Q8mkd9rQTGZMMoUy9DvcuFDErRjzr8x03ahrQcCh3Pu5Z6cIBFcG9f9dgobm1vbO8Xd0t7weFRfikpeNUMWyyWMSqE1CNgktsGm4EdhKFNAoEtoPxXe63J6g0jWjmSboR3QoecgZNVbyexE1oyDMcHbZHfLFbfmzkHWibckFVii0S99QYxSyOUhgmqdddzENnVBnOBM5KvVRjQtmYDrFrqaQRajbh56RC6sMSBgr6QhcX3RkYjradRYCfzkHrVy8XvG5qwls4zJJDUq2OBSmgpiY5A2QAVfIjJhaQpniNithI6ooM7anki3BW3yOmld1Ty35j1cVrVZR1FOINzqIIHN1CHe2hAExg8wTO8wpszcV6cddjMVpwljun8AfO5wCAJHlatexitlatexit sha1_base64TmBm7ikN3ChoJpDcsfwhm1T5rLkAAAB9HicbVDLSgMxFL1TX7Wqi7dBIvYVZkRQZcFNy4r2Ae0Q8mkd9rQTGZMMoUy9DvcuFDErRjzr8x03ahrQcCh3Pu5Z6cIBFcG9f9dgobm1vbO8Xd0t7weFRfikpeNUMWyyWMSqE1CNgktsGm4EdhKFNAoEtoPxXe63J6g0jWjmSboR3QoecgZNVbyexE1oyDMcHbZHfLFbfmzkHWibckFVii0S99QYxSyOUhgmqdddzENnVBnOBM5KvVRjQtmYDrFrqaQRajbh56RC6sMSBgr6QhcX3RkYjradRYCfzkHrVy8XvG5qwls4zJJDUq2OBSmgpiY5A2QAVfIjJhaQpniNithI6ooM7anki3BW3yOmld1Ty35j1cVrVZR1FOINzqIIHN1CHe2hAExg8wTO8wpszcV6cddjMVpwljun8AfO5wCAJHlatexitv0i
latexit sha1_base64eeLXdOBZMDToGpT2JKCAlGanLL8AAAB9HicdVDLSgMxFL3js9ZX1aWbYBG7GjK1xXZXcOOygn1AO5RMmmlDMwTTKEMQ43LhRx68e482MtBVU9EDgcM693JPjxYIrjfGHtbasbm1ndvJ77tHxwWjo7bKkokZS0aiUh2PaKY4CFraa4F68aSkcATrONNrjOM2VS8Si807OYuQEZhdznlGgjuf2A6LHnp9P5xYAPCkVsOxVcrpYRtqv1Oq7UDalVL3EZI8fGCxRhheag8N4fRjQJWKipIEr1HBxrNyVScyrYPN9PFIsJnZAR6xkakoApN12EnqNzowyRH0nzQo0W6veNlARKzQLPTGYh1W8vEyeon2a27KwzjRLKTLQ34ikI5Q1gAacsmoFjNDCJXcZEV0TCSh2vSUNyV8RT9T9pl2zFd3VaKjdKqjhycwhmUwIEraMANNKEFFO7hAZ7g2Zpaj9aL9bocXbNWOyfwA9bbJztsklElatexitlatexit sha1_base64eeLXdOBZMDToGpT2JKCAlGanLL8AAAB9HicdVDLSgMxFL3js9ZX1aWbYBG7GjK1xXZXcOOygn1AO5RMmmlDMwTTKEMQ43LhRx68e482MtBVU9EDgcM693JPjxYIrjfGHtbasbm1ndvJ77tHxwWjo7bKkokZS0aiUh2PaKY4CFraa4F68aSkcATrONNrjOM2VS8Si807OYuQEZhdznlGgjuf2A6LHnp9P5xYAPCkVsOxVcrpYRtqv1Oq7UDalVL3EZI8fGCxRhheag8N4fRjQJWKipIEr1HBxrNyVScyrYPN9PFIsJnZAR6xkakoApN12EnqNzowyRH0nzQo0W6veNlARKzQLPTGYh1W8vEyeon2a27KwzjRLKTLQ34ikI5Q1gAacsmoFjNDCJXcZEV0TCSh2vSUNyV8RT9T9pl2zFd3VaKjdKqjhycwhmUwIEraMANNKEFFO7hAZ7g2Zpaj9aL9bocXbNWOyfwA9bbJztsklElatexitlatexit sha1_base64eeLXdOBZMDToGpT2JKCAlGanLL8AAAB9HicdVDLSgMxFL3js9ZX1aWbYBG7GjK1xXZXcOOygn1AO5RMmmlDMwTTKEMQ43LhRx68e482MtBVU9EDgcM693JPjxYIrjfGHtbasbm1ndvJ77tHxwWjo7bKkokZS0aiUh2PaKY4CFraa4F68aSkcATrONNrjOM2VS8Si807OYuQEZhdznlGgjuf2A6LHnp9P5xYAPCkVsOxVcrpYRtqv1Oq7UDalVL3EZI8fGCxRhheag8N4fRjQJWKipIEr1HBxrNyVScyrYPN9PFIsJnZAR6xkakoApN12EnqNzowyRH0nzQo0W6veNlARKzQLPTGYh1W8vEyeon2a27KwzjRLKTLQ34ikI5Q1gAacsmoFjNDCJXcZEV0TCSh2vSUNyV8RT9T9pl2zFd3VaKjdKqjhycwhmUwIEraMANNKEFFO7hAZ7g2Zpaj9aL9bocXbNWOyfwA9bbJztsklElatexitlatexit sha1_base64eeLXdOBZMDToGpT2JKCAlGanLL8AAAB9HicdVDLSgMxFL3js9ZX1aWbYBG7GjK1xXZXcOOygn1AO5RMmmlDMwTTKEMQ43LhRx68e482MtBVU9EDgcM693JPjxYIrjfGHtbasbm1ndvJ77tHxwWjo7bKkokZS0aiUh2PaKY4CFraa4F68aSkcATrONNrjOM2VS8Si807OYuQEZhdznlGgjuf2A6LHnp9P5xYAPCkVsOxVcrpYRtqv1Oq7UDalVL3EZI8fGCxRhheag8N4fRjQJWKipIEr1HBxrNyVScyrYPN9PFIsJnZAR6xkakoApN12EnqNzowyRH0nzQo0W6veNlARKzQLPTGYh1W8vEyeon2a27KwzjRLKTLQ34ikI5Q1gAacsmoFjNDCJXcZEV0TCSh2vSUNyV8RT9T9pl2zFd3VaKjdKqjhycwhmUwIEraMANNKEFFO7hAZ7g2Zpaj9aL9bocXbNWOyfwA9bbJztsklElatexit b Node update
u0
latexit sha1_base64RuJWOWmv0qWsx0aAsZGj4qvbr4AAAB8nicdVDJSgNBEO2JW4xb1KOXxiDmNMxk0cwt4MVjBLPAZAg9nZ6kSc9Cd40YhnyGFwKePVrvPk3dhZBRR8UPN6roqqenwiuwLIjNzasbmVn67sLO7t39QPDzqqDiVlLVpLGLZ84ligkesDRwE6yWSkdAXrOtPruZ945JxePoFqYJ80IyinjAKQEtuX1g9AHWTo7HxRLllm5aFTqFrbMulOrNmqaNCpO1XGwbVoLlNAKrUHxvTMaRqyCKggSrm2lYCXEQmcCjYr9FPFEkInZMRcTSMSMuVli5NnEwrQxzEUlcEeKFn8hIqNQ09HVnSGCsfntz8SPTSFoeBmPkhRYRJeLglRgiPH8fzzkklEQU00IlVzfiumYSEJBp1TQIXx9iv8nnYppW6Z9Uys1y6s48ugEnaIystElaqJr1EJtRFGMHtATejbAeDRejNdla85YzRyjHzDePgEJTZGrlatexitlatexit sha1_base64RuJWOWmv0qWsx0aAsZGj4qvbr4AAAB8nicdVDJSgNBEO2JW4xb1KOXxiDmNMxk0cwt4MVjBLPAZAg9nZ6kSc9Cd40YhnyGFwKePVrvPk3dhZBRR8UPN6roqqenwiuwLIjNzasbmVn67sLO7t39QPDzqqDiVlLVpLGLZ84ligkesDRwE6yWSkdAXrOtPruZ945JxePoFqYJ80IyinjAKQEtuX1g9AHWTo7HxRLllm5aFTqFrbMulOrNmqaNCpO1XGwbVoLlNAKrUHxvTMaRqyCKggSrm2lYCXEQmcCjYr9FPFEkInZMRcTSMSMuVli5NnEwrQxzEUlcEeKFn8hIqNQ09HVnSGCsfntz8SPTSFoeBmPkhRYRJeLglRgiPH8fzzkklEQU00IlVzfiumYSEJBp1TQIXx9iv8nnYppW6Z9Uys1y6s48ugEnaIystElaqJr1EJtRFGMHtATejbAeDRejNdla85YzRyjHzDePgEJTZGrlatexitlatexit sha1_base64RuJWOWmv0qWsx0aAsZGj4qvbr4AAAB8nicdVDJSgNBEO2JW4xb1KOXxiDmNMxk0cwt4MVjBLPAZAg9nZ6kSc9Cd40YhnyGFwKePVrvPk3dhZBRR8UPN6roqqenwiuwLIjNzasbmVn67sLO7t39QPDzqqDiVlLVpLGLZ84ligkesDRwE6yWSkdAXrOtPruZ945JxePoFqYJ80IyinjAKQEtuX1g9AHWTo7HxRLllm5aFTqFrbMulOrNmqaNCpO1XGwbVoLlNAKrUHxvTMaRqyCKggSrm2lYCXEQmcCjYr9FPFEkInZMRcTSMSMuVli5NnEwrQxzEUlcEeKFn8hIqNQ09HVnSGCsfntz8SPTSFoeBmPkhRYRJeLglRgiPH8fzzkklEQU00IlVzfiumYSEJBp1TQIXx9iv8nnYppW6Z9Uys1y6s48ugEnaIystElaqJr1EJtRFGMHtATejbAeDRejNdla85YzRyjHzDePgEJTZGrlatexitlatexit sha1_base64RuJWOWmv0qWsx0aAsZGj4qvbr4AAAB8nicdVDJSgNBEO2JW4xb1KOXxiDmNMxk0cwt4MVjBLPAZAg9nZ6kSc9Cd40YhnyGFwKePVrvPk3dhZBRR8UPN6roqqenwiuwLIjNzasbmVn67sLO7t39QPDzqqDiVlLVpLGLZ84ligkesDRwE6yWSkdAXrOtPruZ945JxePoFqYJ80IyinjAKQEtuX1g9AHWTo7HxRLllm5aFTqFrbMulOrNmqaNCpO1XGwbVoLlNAKrUHxvTMaRqyCKggSrm2lYCXEQmcCjYr9FPFEkInZMRcTSMSMuVli5NnEwrQxzEUlcEeKFn8hIqNQ09HVnSGCsfntz8SPTSFoeBmPkhRYRJeLglRgiPH8fzzkklEQU00IlVzfiumYSEJBp1TQIXx9iv8nnYppW6Z9Uys1y6s48ugEnaIystElaqJr1EJtRFGMHtATejbAeDRejNdla85YzRyjHzDePgEJTZGrlatexite0k
latexit sha1_base64Iztn6Umi7rLG5lNF0JpW0x6Js0AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGCYA2lM120i7dbOLupFhCf4cXD4p49cd4894bXPQ1gcDjdmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAEYgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3SldfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23SpGsbalkM7V3xMZi4yZRIHtjBgOzbI3EzOimGN34mVJIiKL5YFKaSYkxnCdC0MBRTixhXAt7KVDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IXgfP4A6WSGQlatexitlatexit sha1_base64Iztn6Umi7rLG5lNF0JpW0x6Js0AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGCYA2lM120i7dbOLupFhCf4cXD4p49cd4894bXPQ1gcDjdmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAEYgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3SldfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23SpGsbalkM7V3xMZi4yZRIHtjBgOzbI3EzOimGN34mVJIiKL5YFKaSYkxnCdC0MBRTixhXAt7KVDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IXgfP4A6WSGQlatexitlatexit sha1_base64Iztn6Umi7rLG5lNF0JpW0x6Js0AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGCYA2lM120i7dbOLupFhCf4cXD4p49cd4894bXPQ1gcDjdmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAEYgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3SldfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23SpGsbalkM7V3xMZi4yZRIHtjBgOzbI3EzOimGN34mVJIiKL5YFKaSYkxnCdC0MBRTixhXAt7KVDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IXgfP4A6WSGQlatexitlatexit sha1_base64hP6LrUf2d3tZaldqaQQvEKMXywAAAB2XicbZDNSgMxFIXv1L86Vq1rN8EiuCozbnQpuHFZwbZCO5RM5k4bmskMyR2hDH0BF25EfC93vo3pz0JbDwQzknIvSculLQUBN9ebWd3bgfugfNfzjk9Nmo2fz0gjsilzl5jnmFpXU2CVJCp8LgzyLFfbj6f0i77gsTLXTzQrMMr4WMtUCk7O6oyaraAdLMW2IVxDC9YaNbGSS7KDDUJxa0dhEFBUcUNSaFw7g9LiwUXUz7GgUPNM7RRtRxzzi6dk7A0N5oYkv394uKZ9bOstjdzDhN7Ga2MPLBiWlt1EldVESarH6KC0Vo5wtdmaJNChIzRxwYaSblYkJN1yQa8Z3HYSbG29D77odBu3wMYA6nMMFXEEIN3AHD9CBLghI4BXevYn35n2suqp569LO4I8zx84xIo4latexitlatexit sha1_base64ywz7v1q7Yrl4nBXQcnkaM0kGoAAAB6XicbZBLSwMxFIXvKz1Vd26CRbRVZlxo0vBjcsK9gHtUDLpnRqayYzJnWIZjvcuFDEPTOf2P6WGjrgcDHOQn35kSZkpZ89tbW9Y3Nou7ZR39YPDitHe02b5kZgQ6QqNe2IW1RSY4MkKWxnBnkSKWxFw9tp3hqhsTLVDzTOMEz4QMtYCk7OCruEzxTFBU7Oe8NeperXJnYKgQLqMJC9V7lq9tPRZ6gJqG4tZ3AzygsuCEpFE7K3dxixsWQD7DjUPMEbVjMlp6wMf0WZwadzSxmfv7RcETa8dJ5G4mnB7tcjY18s6OcXXYSF1lhNqMR8U54pRyqYNsL40KEiNHXBhpNuViUduuCDXU9mVECxeRWal7XArwX3PpTgBE7hAgK4ghu4gzo0QMATvMAbvHsj79X7mNe15i16O4Y8j5AKfckNQlatexitlatexit sha1_base64ywz7v1q7Yrl4nBXQcnkaM0kGoAAAB6XicbZBLSwMxFIXvKz1Vd26CRbRVZlxo0vBjcsK9gHtUDLpnRqayYzJnWIZjvcuFDEPTOf2P6WGjrgcDHOQn35kSZkpZ89tbW9Y3Nou7ZR39YPDitHe02b5kZgQ6QqNe2IW1RSY4MkKWxnBnkSKWxFw9tp3hqhsTLVDzTOMEz4QMtYCk7OCruEzxTFBU7Oe8NeperXJnYKgQLqMJC9V7lq9tPRZ6gJqG4tZ3AzygsuCEpFE7K3dxixsWQD7DjUPMEbVjMlp6wMf0WZwadzSxmfv7RcETa8dJ5G4mnB7tcjY18s6OcXXYSF1lhNqMR8U54pRyqYNsL40KEiNHXBhpNuViUduuCDXU9mVECxeRWal7XArwX3PpTgBE7hAgK4ghu4gzo0QMATvMAbvHsj79X7mNe15i16O4Y8j5AKfckNQlatexitlatexit sha1_base64Wxt2EGaSkqmyg6rX9KQvpR9rldEAAAB9HicbVA9TwJBEN3DL8Qv1NJmIzFSkTsbLUlsLDGRjwQuZGZgw17efuHJFcB02Fhpj64x894wBUKvmSSldmMjMvSKQw6LrfTmFjc2t7p7hb2tsODwqH50TJxqDk0ey1h3AmZACgVNFCihk2hgUSChHYxv5357AtqIWD3gNAEYkMlQsEZWsnvITxhEGYwuyPWKW3MXoOvEy0mF5Gj0y19QczTCBRyyYzpem6CfsY0Ci5hVuqlBhLGx2wIXUsVi8D42eLoGb2wyoCGsbalkC7U3xMZi4yZRoHtjBiOzKo3FzuimGN34mVJIiKL5cFKaSYkznCdCB0MBRTi1hXAt7KUjphlHm1PJhuCtvrxOWlc1z615926lXs3jKJIzck6qxCPXpE7uSIM0CSeP5Jm8kjdn4rw4787HsrXg5DOn5AczxqpZIVlatexitlatexit sha1_base64Iztn6Umi7rLG5lNF0JpW0x6Js0AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGCYA2lM120i7dbOLupFhCf4cXD4p49cd4894bXPQ1gcDjdmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAEYgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3SldfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23SpGsbalkM7V3xMZi4yZRIHtjBgOzbI3EzOimGN34mVJIiKL5YFKaSYkxnCdC0MBRTixhXAt7KVDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IXgfP4A6WSGQlatexitlatexit sha1_base64Iztn6Umi7rLG5lNF0JpW0x6Js0AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGCYA2lM120i7dbOLupFhCf4cXD4p49cd4894bXPQ1gcDjdmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAEYgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3SldfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23SpGsbalkM7V3xMZi4yZRIHtjBgOzbI3EzOimGN34mVJIiKL5YFKaSYkxnCdC0MBRTixhXAt7KVDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IXgfP4A6WSGQlatexitlatexit sha1_base64Iztn6Umi7rLG5lNF0JpW0x6Js0AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGCYA2lM120i7dbOLupFhCf4cXD4p49cd4894bXPQ1gcDjdmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAEYgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3SldfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23SpGsbalkM7V3xMZi4yZRIHtjBgOzbI3EzOimGN34mVJIiKL5YFKaSYkxnCdC0MBRTixhXAt7KVDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IXgfP4A6WSGQlatexitlatexit sha1_base64Iztn6Umi7rLG5lNF0JpW0x6Js0AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGCYA2lM120i7dbOLupFhCf4cXD4p49cd4894bXPQ1gcDjdmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAEYgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3SldfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23SpGsbalkM7V3xMZi4yZRIHtjBgOzbI3EzOimGN34mVJIiKL5YFKaSYkxnCdC0MBRTixhXAt7KVDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IXgfP4A6WSGQlatexitlatexit sha1_base64Iztn6Umi7rLG5lNF0JpW0x6Js0AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGCYA2lM120i7dbOLupFhCf4cXD4p49cd4894bXPQ1gcDjdmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAEYgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3SldfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23SpGsbalkM7V3xMZi4yZRIHtjBgOzbI3EzOimGN34mVJIiKL5YFKaSYkxnCdC0MBRTixhXAt7KVDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IXgfP4A6WSGQlatexitlatexit sha1_base64Iztn6Umi7rLG5lNF0JpW0x6Js0AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGCYA2lM120i7dbOLupFhCf4cXD4p49cd4894bXPQ1gcDjdmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAEYgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3SldfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23SpGsbalkM7V3xMZi4yZRIHtjBgOzbI3EzOimGN34mVJIiKL5YFKaSYkxnCdC0MBRTixhXAt7KVDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IXgfP4A6WSGQlatexitv0i
latexit sha1_base64PT7VlVtIO1b4RdkSG9z8jpkhSqkAAAB9HicbVBNS8NAEJ34WetX1aOXxSL2VBIR9Fjw4rGCYA2lM120y7dbOLupFhCf4cXD4p49cd4894bXPQ1gcDjdmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqxhsslrFuB9RwKRRvoEDJ24nmNAokbwWj25nfGnNtRKwecJJwP6IDJULBKFrJ7yJwiDMxtOLnuiVym7VnYOsEi8nZchR75Wuv2YpRFXyCQ1puO5CfoZ1SiY5NNiNzU8oWxEB7xjqaIRN342P3pKzq3SJ2GsbSkkcX3REYjYyZRYDsjikOz7M3E7xOiuGNnwmVpMgVWywKU0kwJrMESF9ozlBOLKFMC3srYUOqKUObU9GG4C2vEqal1XPrXr3VVaJYjAKdwBhXw4BpqcAd1aACDR3iGV3hzxs6L858LFrXnHzmBP7AfwBAvSSKAlatexitlatexit sha1_base64PT7VlVtIO1b4RdkSG9z8jpkhSqkAAAB9HicbVBNS8NAEJ34WetX1aOXxSL2VBIR9Fjw4rGCYA2lM120y7dbOLupFhCf4cXD4p49cd4894bXPQ1gcDjdmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqxhsslrFuB9RwKRRvoEDJ24nmNAokbwWj25nfGnNtRKwecJJwP6IDJULBKFrJ7yJwiDMxtOLnuiVym7VnYOsEi8nZchR75Wuv2YpRFXyCQ1puO5CfoZ1SiY5NNiNzU8oWxEB7xjqaIRN342P3pKzq3SJ2GsbSkkcX3REYjYyZRYDsjikOz7M3E7xOiuGNnwmVpMgVWywKU0kwJrMESF9ozlBOLKFMC3srYUOqKUObU9GG4C2vEqal1XPrXr3VVaJYjAKdwBhXw4BpqcAd1aACDR3iGV3hzxs6L858LFrXnHzmBP7AfwBAvSSKAlatexitlatexit sha1_base646WjtAQy1eEki3DeLmUkkI9SkOsAAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0XckvuBunaRbaemDgcM65zL0nSAXXxnWncrG5tb2TnW3trdcHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOBWB6uD7T6g0TSDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzVTpXhTOC8Nso0ppRN6RiHlkoaobzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQOGmYluJzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3vqrRbpZ1VOEMzqEJHlxDG6gA11gEMIzvMKbw50X5935WEYrTjlzCngfP4AA6WMYAlatexitlatexit sha1_base646WjtAQy1eEki3DeLmUkkI9SkOsAAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0XckvuBunaRbaemDgcM65zL0nSAXXxnWncrG5tb2TnW3trdcHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOBWB6uD7T6g0TSDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzVTpXhTOC8Nso0ppRN6RiHlkoaobzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQOGmYluJzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3vqrRbpZ1VOEMzqEJHlxDG6gA11gEMIzvMKbw50X5935WEYrTjlzCngfP4AA6WMYAlatexitlatexit sha1_base64hP6LrUf2d3tZaldqaQQvEKMXywAAAB2XicbZDNSgMxFIXv1L86Vq1rN8EiuCozbnQpuHFZwbZCO5RM5k4bmskMyR2hDH0BF25EfC93vo3pz0JbDwQzknIvSculLQUBN9ebWd3bgfugfNfzjk9Nmo2fz0gjsilzl5jnmFpXU2CVJCp8LgzyLFfbj6f0i77gsTLXTzQrMMr4WMtUCk7O6oyaraAdLMW2IVxDC9YaNbGSS7KDDUJxa0dhEFBUcUNSaFw7g9LiwUXUz7GgUPNM7RRtRxzzi6dk7A0N5oYkv394uKZ9bOstjdzDhN7Ga2MPLBiWlt1EldVESarH6KC0Vo5wtdmaJNChIzRxwYaSblYkJN1yQa8Z3HYSbG29D77odBu3wMYA6nMMFXEEIN3AHD9CBLghI4BXevYn35n2suqp569LO4I8zx84xIo4latexitlatexit sha1_base64OfCCjkcIiyDbxmNKxve032U7QH4AAAB3HicbVBNS8NAEJ3Ur1qrVq9eFovgqSRe9Ch48VjBfkAbZLOZtEs3m7A7EUroHDiQRFlzfjduPg7YGHi8N8PMvChX0pLvf3uVre2d3b3qfu2gfnh03Dipd21WGIEdkanM9CNuUUmNHZKksJ8b5GmksBdN7uZ7xmNlZlpGmOYcpHWiZScHJS6nR9FvAmyTBCvShBVc9cwzkSRoiahuLWDwM8pLLkhKRTOasPCYs7FhI9w4KjmKdqwXNw5YxdOiVmSGVea2EL9PVHy1NppGrnOlNPYrntz8T9vUFByE5ZS5wWhFstFSaEYZWzNIulQUFq6ggXRrpbmRhzwwW5aGouhGD95U3SvWoFfit48KEKZ3AOlxDANdzCPbShAwJieIE3ePek9p9LOOqeKvcTuEPvM8fCqLKAlatexitlatexit sha1_base64OfCCjkcIiyDbxmNKxve032U7QH4AAAB3HicbVBNS8NAEJ3Ur1qrVq9eFovgqSRe9Ch48VjBfkAbZLOZtEs3m7A7EUroHDiQRFlzfjduPg7YGHi8N8PMvChX0pLvf3uVre2d3b3qfu2gfnh03Dipd21WGIEdkanM9CNuUUmNHZKksJ8b5GmksBdN7uZ7xmNlZlpGmOYcpHWiZScHJS6nR9FvAmyTBCvShBVc9cwzkSRoiahuLWDwM8pLLkhKRTOasPCYs7FhI9w4KjmKdqwXNw5YxdOiVmSGVea2EL9PVHy1NppGrnOlNPYrntz8T9vUFByE5ZS5wWhFstFSaEYZWzNIulQUFq6ggXRrpbmRhzwwW5aGouhGD95U3SvWoFfit48KEKZ3AOlxDANdzCPbShAwJieIE3ePek9p9LOOqeKvcTuEPvM8fCqLKAlatexitlatexit sha1_base64PwZ8GjhNs4EFPNOrNCQnWexiUCQAAAB53icbVDLSgMxFL1TX7Wqi7dBIvQVZlxo8uCG5cV7APaQTKZTBuaZIbkjlCGoAbF4q49ZfcTem7Sy0eiBwOOdccuJMiksv6XV9nY3Nreqe7W9vYPDoqxyc9maG8S5LZWoGEbVcCs27KFDyQWY4VZHkWh6sD7j9xYkep7nGU8VHSsRSIYRSd1HuoNvUvQf6SoCQNKOHyn6M4ZbniGpmk1g4DP8OwoAYFk3xeGWWZ5RN6ZgPHdVUcRsWyz3n5MIpMUlS455GslRThRUWTtTkUsqihO77i3E7xhjsl1WAid5cg1W32U5JJgShZHk1gYzlDOHKHMCLcrYRNqKENXTc2VEKyfJf0LluB3wruEa7WdZRhTM4hyYEcAVtuIUOdIFBDEwAqe8J69N99Fa145cwpIL38Q0CZYxclatexitlatexit sha1_base646WjtAQy1eEki3DeLmUkkI9SkOsAAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0XckvuBunaRbaemDgcM65zL0nSAXXxnWncrG5tb2TnW3trdcHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOBWB6uD7T6g0TSDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzVTpXhTOC8Nso0ppRN6RiHlkoaobzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQOGmYluJzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3vqrRbpZ1VOEMzqEJHlxDG6gA11gEMIzvMKbw50X5935WEYrTjlzCngfP4AA6WMYAlatexitlatexit sha1_base646WjtAQy1eEki3DeLmUkkI9SkOsAAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0XckvuBunaRbaemDgcM65zL0nSAXXxnWncrG5tb2TnW3trdcHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOBWB6uD7T6g0TSDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzVTpXhTOC8Nso0ppRN6RiHlkoaobzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQOGmYluJzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3vqrRbpZ1VOEMzqEJHlxDG6gA11gEMIzvMKbw50X5935WEYrTjlzCngfP4AA6WMYAlatexitlatexit sha1_base646WjtAQy1eEki3DeLmUkkI9SkOsAAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0XckvuBunaRbaemDgcM65zL0nSAXXxnWncrG5tb2TnW3trdcHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOBWB6uD7T6g0TSDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzVTpXhTOC8Nso0ppRN6RiHlkoaobzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQOGmYluJzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3vqrRbpZ1VOEMzqEJHlxDG6gA11gEMIzvMKbw50X5935WEYrTjlzCngfP4AA6WMYAlatexitlatexit sha1_base646WjtAQy1eEki3DeLmUkkI9SkOsAAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0XckvuBunaRbaemDgcM65zL0nSAXXxnWncrG5tb2TnW3trdcHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOBWB6uD7T6g0TSDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzVTpXhTOC8Nso0ppRN6RiHlkoaobzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQOGmYluJzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3vqrRbpZ1VOEMzqEJHlxDG6gA11gEMIzvMKbw50X5935WEYrTjlzCngfP4AA6WMYAlatexitlatexit sha1_base646WjtAQy1eEki3DeLmUkkI9SkOsAAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0XckvuBunaRbaemDgcM65zL0nSAXXxnWncrG5tb2TnW3trdcHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOBWB6uD7T6g0TSDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzVTpXhTOC8Nso0ppRN6RiHlkoaobzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQOGmYluJzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3vqrRbpZ1VOEMzqEJHlxDG6gA11gEMIzvMKbw50X5935WEYrTjlzCngfP4AA6WMYAlatexitlatexit sha1_base646WjtAQy1eEki3DeLmUkkI9SkOsAAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0XckvuBunaRbaemDgcM65zL0nSAXXxnWncrG5tb2TnW3trdcHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOBWB6uD7T6g0TSDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzVTpXhTOC8Nso0ppRN6RiHlkoaobzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQOGmYluJzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3vqrRbpZ1VOEMzqEJHlxDG6gA11gEMIzvMKbw50X5935WEYrTjlzCngfP4AA6WMYAlatexit c Global update
Figure 3 Updates in a GN block Blue indicates the element that is being updated and black
indicates other elements which are involved in the update note that the preupdate value of the
blue element is also used in the update See Equation 1 for details on the notation
3vis applied to each node i to compute an updated node attribute v0
i In our running
examplevmay compute something analogous to the updated position velocity and kinetic
energy of each ball The set of resulting pernode outputs is V0fv0
igi1Nv
4euis applied to E0 and aggregates all edge updates into  e0 which will then be used in the
next steps global update In our running example eumay compute the summed forces
which should be zero in this case due to Newtons third law and the springs potential
energies
5vuis applied to V0 and aggregates all node updates into  v0 which will then be used in
the next steps global update In our running example vumight compute the total kinetic
energy of the system
6uis applied once per graph and computes an update for the global attribute u0 In our
running example umight compute something analogous to the net forces and total energy
of the physical system
Note though we assume this sequence of steps here the order is not strictly enforced it is possible
to reverse the update functions to proceed from global to pernode to peredge updates for example
Kearnes et al 2016 computes edge updates from nodes in a similar manner
324 Relational inductive biases in graph networks
Our GN framework imposes several strong relational inductive biases when used as components in
a learning process First graphs can express arbitrary relationships among entities which means
the GNs input determines how representations interact and are isolated rather than those choices
being determined by the xed architecture For example the assumption that two entities have a
relationshipand thus should interactis expressed by an edge between the entities corresponding
nodes Similarly the absence of an edge expresses the assumption that the nodes have no relationship
and should not inuence each other directly
Second graphs represent entities and their relations as sets which are invariant to permutations
This means GNs are invariant to the order of these elements6 which is often desirable For example
the objects in a scene do not have a natural ordering see Sec 22
Third a GNs peredge and pernode functions are reused across all edges and nodes respectively
This means GNs automatically support a form of combinatorial generalization see Section 51
because graphs are composed of edges nodes and global features a single GN can operate on
graphs of dierent sizes numbers of edges and nodes and shapes edge connectivity
6Note an ordering can be imposed by encoding the indices in the node or edge attributes or via the edges
themselves eg by encoding a chain or partial ordering
134 Design principles for graph network architectures
The GN framework can be used to implement a wide variety of architectures in accordance with
the design principles listed above in Section 32 which also correspond to the subsections 41
42 and 43 below In general the framework is agnostic to specic attribute representations and
functional forms Here however we focus mainly on deep learning architectures which allow GNs
to act as learnable graphtograph function approximators
41 Flexible representations
Graph networks support highly exible graph representations in two ways rst in terms of the
representation of the attributes and second in terms of the structure of the graph itself
411 Attributes
The global node and edge attributes of a GN block can use arbitrary representational formats In
deep learning implementations realvalued vectors and tensors are most common However other
data structures such as sequences sets or even graphs could also be used
The requirements of the problem will often determine what representations should be used for
the attributes For example when the input data is an image the attributes might be represented
as tensors of image patches however when the input data is a text document the attributes might
be sequences of words corresponding to sentences
For each GN block within a broader architecture the edge and node outputs typically correspond
to lists of vectors or tensors one per edge or node and the global outputs correspond to a single
vector or tensor This allows a GNs output to be passed to other deep learning building blocks
such as MLPs CNNs and RNNs
The output of a GN block can also be tailored to the demands of the task In particular
Anedgefocused GN uses the edges as output for example to make decisions about interactions
among entities Kipf et al 2018 Hamrick et al 2018
Anodefocused GN uses the nodes as output for example to reason about physical systems
Battaglia et al 2016 Chang et al 2017 Wang et al 2018b SanchezGonzalez et al 2018
Agraphfocused GN uses the globals as output for example to predict the potential energy of
a physical system Battaglia et al 2016 the properties of a molecule Gilmer et al 2017
or answers to questions about a visual scene Santoro et al 2017
The nodes edges and global outputs can also be mixedandmatched depending on the task For
example Hamrick et al 2018 used both the output edge and global attributes to compute a policy
over actions
412 Graph structure
When dening how the input data will be represented as a graph there are generally two scenarios
rst the input explicitly species the relational structure and second the relational structure must
be inferred or assumed These are not hard distinctions but extremes along a continuum
Examples of data with more explicitly specied entities and relations include knowledge graphs
social networks parse trees optimization problems chemical graphs road networks and physical
systems with known interactions Figures 2ad illustrate how such data can be expressed as graphs
Examples of data where the relational structure is not made explicit and must be inferred or
assumed include visual scenes text corpora programming language source code and multiagent
14systems In these types of settings the data may be formatted as a set of entities without relations
or even just a vector or tensor eg an image If the entities are not specied explicitly they might
be assumed for instance by treating each word in a sentence Vaswani et al 2017 or each local
feature vector in a CNNs output feature map as a node Watters et al 2017 Santoro et al 2017
Wang et al 2018c Figures 2ef Or it might be possible to use a separate learned mechanism to
infer entities from an unstructured signal Luong et al 2015 Mnih et al 2014 Eslami et al 2016
van Steenkiste et al 2018 If relations are not available the simplest approach is to instantiate all
possible directed edges between entities Figure 2f This can be prohibitive for large numbers of
entities however because the number of possible edges grows quadratically with the number of
nodes Thus developing more sophisticated ways of inferring sparse structure from unstructured
data Kipf et al 2018 is an important future direction
42 Congurable withinblock structure
The structure and functions within a GN block can be congured in dierent ways which oers
exibility in what information is made available as inputs to its functions as well as how output edge
node and global updates are produced In particular each in Equation 1 must be implemented
with some function f wherefs argument signature determines what information it requires as
input in Figure 4 the incoming arrows to each depict whether uV andEare taken as inputs
Hamrick et al 2018 and SanchezGonzalez et al 2018 used the full GN block shown in Figure 4a
Theirimplementations used neural networks denoted NNeNNv and NNubelow to indicate that
they are dierent functions with dierent parameters Their implementations used elementwise
summation but averages and maxmin could also be used
eekvrkvskufeekvrkvsku  NNeekvrkvsku 2
v
 e0
iviufv
 e0
iviu
 NNv
 e0
iviu
u
 e0 v0ufu
 e0 v0u
 NNu
 e0 v0u
ev
E0
i X
fkrkige0
k
vu
V0 X
iv0
i
eu
E0 X
ke0
k
where  xyz indicates vectortensor concatenation For vector attributes a MLP is often used for
 while for tensors such as image feature maps CNNs may be more suitable
Thefunctions can also use RNNs which requires an additional hidden state as input and
output Figure 4b shows a very simple version of a GN block with RNNs as functions there is no
messagepassing in this formulation and this type of block might be used for recurrent smoothing of
some dynamic graph states Of course RNNs as functions could also be used in a full GN block
Figure 4a
A variety of other architectures can be expressed in the GN framework often as dierent
function choices and withinblock congurations The remaining subsections explore how a GNs
withinblock structure can be congured in dierent ways with examples of published work which
uses such congurations See the Appendix for details
15a Full GN block
u0u0hid
latexit sha1_base64W2hu4ghYojV0SmOcBbcdBDITOSoAAACDnicbVC7TsMwFHV4lvIKMLJYVBUMqEoQEgwMlVgYi0QfUhtFjuu0Vm0nsh2kKsoXsPArLAwgxMrMxtgpBlKy5EsHZ9zr69J4gZVdpxfqyV1bX1jc3KVnV7Z3dv3z447KgokZi0ccQi2QuQIowK0tZUM9KLJUE8YKQbTG5zvtIpKKReNDTmHgcjQQNKUbaSL5dH3Ckx0GYJtnpOZz7AWXPB3TYebbNafhFIDLxC1JDZRofb3YBjhhBOhMUNK9V0n1l6KpKaYkaw6SBSJEZ6gEekbKhAnykuLczJYN8oQhpE0T2hYqPMdKeJKTXlgKvMd1aKXi95USH115KRZxoIvBsUJgwqCOYZwOHVBKs2dQQhCU1u0I8RhJhbRKsmhDcxZOXSeei4ToN96y1rwp46iAY3ACzoALrkAT3IEWaAMMnsALeAPv1rP1an1Yn7PSFavsOQJYH39AtnQnJglatexitlatexit sha1_base64W2hu4ghYojV0SmOcBbcdBDITOSoAAACDnicbVC7TsMwFHV4lvIKMLJYVBUMqEoQEgwMlVgYi0QfUhtFjuu0Vm0nsh2kKsoXsPArLAwgxMrMxtgpBlKy5EsHZ9zr69J4gZVdpxfqyV1bX1jc3KVnV7Z3dv3z447KgokZi0ccQi2QuQIowK0tZUM9KLJUE8YKQbTG5zvtIpKKReNDTmHgcjQQNKUbaSL5dH3Ckx0GYJtnpOZz7AWXPB3TYebbNafhFIDLxC1JDZRofb3YBjhhBOhMUNK9V0n1l6KpKaYkaw6SBSJEZ6gEekbKhAnykuLczJYN8oQhpE0T2hYqPMdKeJKTXlgKvMd1aKXi95USH115KRZxoIvBsUJgwqCOYZwOHVBKs2dQQhCU1u0I8RhJhbRKsmhDcxZOXSeei4ToN96y1rwp46iAY3ACzoALrkAT3IEWaAMMnsALeAPv1rP1an1Yn7PSFavsOQJYH39AtnQnJglatexitlatexit sha1_base64W2hu4ghYojV0SmOcBbcdBDITOSoAAACDnicbVC7TsMwFHV4lvIKMLJYVBUMqEoQEgwMlVgYi0QfUhtFjuu0Vm0nsh2kKsoXsPArLAwgxMrMxtgpBlKy5EsHZ9zr69J4gZVdpxfqyV1bX1jc3KVnV7Z3dv3z447KgokZi0ccQi2QuQIowK0tZUM9KLJUE8YKQbTG5zvtIpKKReNDTmHgcjQQNKUbaSL5dH3Ckx0GYJtnpOZz7AWXPB3TYebbNafhFIDLxC1JDZRofb3YBjhhBOhMUNK9V0n1l6KpKaYkaw6SBSJEZ6gEekbKhAnykuLczJYN8oQhpE0T2hYqPMdKeJKTXlgKvMd1aKXi95USH115KRZxoIvBsUJgwqCOYZwOHVBKs2dQQhCU1u0I8RhJhbRKsmhDcxZOXSeei4ToN96y1rwp46iAY3ACzoALrkAT3IEWaAMMnsALeAPv1rP1an1Yn7PSFavsOQJYH39AtnQnJglatexitlatexit sha1_base64W2hu4ghYojV0SmOcBbcdBDITOSoAAACDnicbVC7TsMwFHV4lvIKMLJYVBUMqEoQEgwMlVgYi0QfUhtFjuu0Vm0nsh2kKsoXsPArLAwgxMrMxtgpBlKy5EsHZ9zr69J4gZVdpxfqyV1bX1jc3KVnV7Z3dv3z447KgokZi0ccQi2QuQIowK0tZUM9KLJUE8YKQbTG5zvtIpKKReNDTmHgcjQQNKUbaSL5dH3Ckx0GYJtnpOZz7AWXPB3TYebbNafhFIDLxC1JDZRofb3YBjhhBOhMUNK9V0n1l6KpKaYkaw6SBSJEZ6gEekbKhAnykuLczJYN8oQhpE0T2hYqPMdKeJKTXlgKvMd1aKXi95USH115KRZxoIvBsUJgwqCOYZwOHVBKs2dQQhCU1u0I8RhJhbRKsmhDcxZOXSeei4ToN96y1rwp46iAY3ACzoALrkAT3IEWaAMMnsALeAPv1rP1an1Yn7PSFavsOQJYH39AtnQnJglatexitE0E0hid
latexit sha1_base64ZYHbDTDbKJNm66GNrtdUMK6558AAABHicbVDLSgMxFM3UV62v0S7dBIvUhZQZEXThoiAFlxXsA9phyGQybWiSGZKMUIb6K25cKOLWD3Hn35hpZ6GtBwKHc7lnpwgYVRpxm2SmvrG5tb5e3Kzu7eoF9eNRVcSox6eCYxbIfIEUYFaSjqWakn0iCeMBIL5jc5n7vkUhFYGgpwnxOBoJGlGMtJF8u9qqn8NW3R9ypMeSZ2Mazny75jScOeAqcQtSAwXavv01DGOcciI0Zkipgesk2suQ1BQzMqsMU0UShCdoRAaGCsSJ8rJ5Bk8NUoIo1iaJzScq783MsSVmvLATOYZ1bKXi95g1RH115GRZJqIvDiUJQyqGOYNwFDKgnWbGoIwpKarBCPkURYm74qpgR3curpHvRcJ2Ge39Za94UdZTBMTgBZ8AFV6AJ7kAbdAAGUAMXsGb9WS9WOWx2K0ZBU7VfAH1ucPbBGTAlatexitlatexit sha1_base64ZYHbDTDbKJNm66GNrtdUMK6558AAABHicbVDLSgMxFM3UV62v0S7dBIvUhZQZEXThoiAFlxXsA9phyGQybWiSGZKMUIb6K25cKOLWD3Hn35hpZ6GtBwKHc7lnpwgYVRpxm2SmvrG5tb5e3Kzu7eoF9eNRVcSox6eCYxbIfIEUYFaSjqWakn0iCeMBIL5jc5n7vkUhFYGgpwnxOBoJGlGMtJF8u9qqn8NW3R9ypMeSZ2Mazny75jScOeAqcQtSAwXavv01DGOcciI0Zkipgesk2suQ1BQzMqsMU0UShCdoRAaGCsSJ8rJ5Bk8NUoIo1iaJzScq783MsSVmvLATOYZ1bKXi95g1RH115GRZJqIvDiUJQyqGOYNwFDKgnWbGoIwpKarBCPkURYm74qpgR3curpHvRcJ2Ge39Za94UdZTBMTgBZ8AFV6AJ7kAbdAAGUAMXsGb9WS9WOWx2K0ZBU7VfAH1ucPbBGTAlatexitlatexit sha1_base64ZYHbDTDbKJNm66GNrtdUMK6558AAABHicbVDLSgMxFM3UV62v0S7dBIvUhZQZEXThoiAFlxXsA9phyGQybWiSGZKMUIb6K25cKOLWD3Hn35hpZ6GtBwKHc7lnpwgYVRpxm2SmvrG5tb5e3Kzu7eoF9eNRVcSox6eCYxbIfIEUYFaSjqWakn0iCeMBIL5jc5n7vkUhFYGgpwnxOBoJGlGMtJF8u9qqn8NW3R9ypMeSZ2Mazny75jScOeAqcQtSAwXavv01DGOcciI0Zkipgesk2suQ1BQzMqsMU0UShCdoRAaGCsSJ8rJ5Bk8NUoIo1iaJzScq783MsSVmvLATOYZ1bKXi95g1RH115GRZJqIvDiUJQyqGOYNwFDKgnWbGoIwpKarBCPkURYm74qpgR3curpHvRcJ2Ge39Za94UdZTBMTgBZ8AFV6AJ7kAbdAAGUAMXsGb9WS9WOWx2K0ZBU7VfAH1ucPbBGTAlatexitlatexit sha1_base64ZYHbDTDbKJNm66GNrtdUMK6558AAABHicbVDLSgMxFM3UV62v0S7dBIvUhZQZEXThoiAFlxXsA9phyGQybWiSGZKMUIb6K25cKOLWD3Hn35hpZ6GtBwKHc7lnpwgYVRpxm2SmvrG5tb5e3Kzu7eoF9eNRVcSox6eCYxbIfIEUYFaSjqWakn0iCeMBIL5jc5n7vkUhFYGgpwnxOBoJGlGMtJF8u9qqn8NW3R9ypMeSZ2Mazny75jScOeAqcQtSAwXavv01DGOcciI0Zkipgesk2suQ1BQzMqsMU0UShCdoRAaGCsSJ8rJ5Bk8NUoIo1iaJzScq783MsSVmvLATOYZ1bKXi95g1RH115GRZJqIvDiUJQyqGOYNwFDKgnWbGoIwpKarBCPkURYm74qpgR3curpHvRcJ2Ge39Za94UdZTBMTgBZ8AFV6AJ7kAbdAAGUAMXsGb9WS9WOWx2K0ZBU7VfAH1ucPbBGTAlatexitV0V0hid
latexit sha1_base64WhBBytL7AOLCOPFKqkdbfTZ2xq4AAABHicbVBPS8MwHE3nvznVXf0EhwyDzJaEfTgYeDF4wTXDbZS0jTdwpK0JKkwyvwqXjwo4tUP4s1vY7r1oJsPAo3fj9Ly9MGVXacb6tytr6xuZWdbu2s7u3f2AfHnkqySQmXZywRPZDpAijgnQ11Yz0U0kQDxnphZPbwu89EqloIh70NCURyNBY4qRNlJg173mOfSawZAjPZY8H9NoFtgNpXMAVeJW5IGKNEJ7K9hlOCME6ExQ0oNXCfVfo6kppiRWW2YKZIiPEEjMjBUIE6Un8DzCpUSIYJ9I8oeFcb2RI67UlIdmssiolr1CM8bZDq9nMq0kwTgReH4oxBncCiCRhRSbBmU0MQltRkhXiMJMLa9FUzJbjLX14l3kXLdVruWWjfVPWUQXH4AScARdcgTa4Ax3QBRhMwTN4BWWkVivVsfi9GKVe7UwR9YnzhuZQalatexitlatexit sha1_base64WhBBytL7AOLCOPFKqkdbfTZ2xq4AAABHicbVBPS8MwHE3nvznVXf0EhwyDzJaEfTgYeDF4wTXDbZS0jTdwpK0JKkwyvwqXjwo4tUP4s1vY7r1oJsPAo3fj9Ly9MGVXacb6tytr6xuZWdbu2s7u3f2AfHnkqySQmXZywRPZDpAijgnQ11Yz0U0kQDxnphZPbwu89EqloIh70NCURyNBY4qRNlJg173mOfSawZAjPZY8H9NoFtgNpXMAVeJW5IGKNEJ7K9hlOCME6ExQ0oNXCfVfo6kppiRWW2YKZIiPEEjMjBUIE6Un8DzCpUSIYJ9I8oeFcb2RI67UlIdmssiolr1CM8bZDq9nMq0kwTgReH4oxBncCiCRhRSbBmU0MQltRkhXiMJMLa9FUzJbjLX14l3kXLdVruWWjfVPWUQXH4AScARdcgTa4Ax3QBRhMwTN4BWWkVivVsfi9GKVe7UwR9YnzhuZQalatexitlatexit sha1_base64WhBBytL7AOLCOPFKqkdbfTZ2xq4AAABHicbVBPS8MwHE3nvznVXf0EhwyDzJaEfTgYeDF4wTXDbZS0jTdwpK0JKkwyvwqXjwo4tUP4s1vY7r1oJsPAo3fj9Ly9MGVXacb6tytr6xuZWdbu2s7u3f2AfHnkqySQmXZywRPZDpAijgnQ11Yz0U0kQDxnphZPbwu89EqloIh70NCURyNBY4qRNlJg173mOfSawZAjPZY8H9NoFtgNpXMAVeJW5IGKNEJ7K9hlOCME6ExQ0oNXCfVfo6kppiRWW2YKZIiPEEjMjBUIE6Un8DzCpUSIYJ9I8oeFcb2RI67UlIdmssiolr1CM8bZDq9nMq0kwTgReH4oxBncCiCRhRSbBmU0MQltRkhXiMJMLa9FUzJbjLX14l3kXLdVruWWjfVPWUQXH4AScARdcgTa4Ax3QBRhMwTN4BWWkVivVsfi9GKVe7UwR9YnzhuZQalatexitlatexit sha1_base64WhBBytL7AOLCOPFKqkdbfTZ2xq4AAABHicbVBPS8MwHE3nvznVXf0EhwyDzJaEfTgYeDF4wTXDbZS0jTdwpK0JKkwyvwqXjwo4tUP4s1vY7r1oJsPAo3fj9Ly9MGVXacb6tytr6xuZWdbu2s7u3f2AfHnkqySQmXZywRPZDpAijgnQ11Yz0U0kQDxnphZPbwu89EqloIh70NCURyNBY4qRNlJg173mOfSawZAjPZY8H9NoFtgNpXMAVeJW5IGKNEJ7K9hlOCME6ExQ0oNXCfVfo6kppiRWW2YKZIiPEEjMjBUIE6Un8DzCpUSIYJ9I8oeFcb2RI67UlIdmssiolr1CM8bZDq9nMq0kwTgReH4oxBncCiCRhRSbBmU0MQltRkhXiMJMLa9FUzJbjLX14l3kXLdVruWWjfVPWUQXH4AScARdcgTa4Ax3QBRhMwTN4BWWkVivVsfi9GKVe7UwR9YnzhuZQalatexitEEhid
latexit sha1_base64DYZek5SmevKS8py25dx0aIqUbBYAAABnicbVDLSsNAFL3xWesr1aWbwSK4kJKIoAsXBSm4rGAf0IYwmUzboTNJmJkoJfZT3LhQxK1f4s6cdJmoa0HBg7n3Ms9c4KEM6Ud59taWV1b39gsbZW3d3b39u3KQVvFqSS0RWIey26AFeUsoi3NNKfdRFIsAk47wfgm9zsPVCoWRd6klBP4GHEBoxgbSTfrjTOUMPvC6xHUmQjFk59urUnBnQMnELUoUCTd6ocxSQWNNOFYqZ7rJNrLsNSMcDot91NFE0zGeEh7hkZYUOVlshTdGKUEA1iaV6k0Uz9vZFhodREBGYyz6gWvVz8zulenDlZSxKUk0jMj80SDnSMcp7QCGTlGgMQQTyUxWREZYYqJNW2VTgrv45WXSPq5Ts29u6jWr4s6SnAEx3AKLlxCHW6hCS0g8AjP8Apv1pP1Yr1bHPRFavYOYQsD5AKXVk5Ylatexitlatexit sha1_base64DYZek5SmevKS8py25dx0aIqUbBYAAABnicbVDLSsNAFL3xWesr1aWbwSK4kJKIoAsXBSm4rGAf0IYwmUzboTNJmJkoJfZT3LhQxK1f4s6cdJmoa0HBg7n3Ms9c4KEM6Ud59taWV1b39gsbZW3d3b39u3KQVvFqSS0RWIey26AFeUsoi3NNKfdRFIsAk47wfgm9zsPVCoWRd6klBP4GHEBoxgbSTfrjTOUMPvC6xHUmQjFk59urUnBnQMnELUoUCTd6ocxSQWNNOFYqZ7rJNrLsNSMcDot91NFE0zGeEh7hkZYUOVlshTdGKUEA1iaV6k0Uz9vZFhodREBGYyz6gWvVz8zulenDlZSxKUk0jMj80SDnSMcp7QCGTlGgMQQTyUxWREZYYqJNW2VTgrv45WXSPq5Ts29u6jWr4s6SnAEx3AKLlxCHW6hCS0g8AjP8Apv1pP1Yr1bHPRFavYOYQsD5AKXVk5Ylatexitlatexit sha1_base64DYZek5SmevKS8py25dx0aIqUbBYAAABnicbVDLSsNAFL3xWesr1aWbwSK4kJKIoAsXBSm4rGAf0IYwmUzboTNJmJkoJfZT3LhQxK1f4s6cdJmoa0HBg7n3Ms9c4KEM6Ud59taWV1b39gsbZW3d3b39u3KQVvFqSS0RWIey26AFeUsoi3NNKfdRFIsAk47wfgm9zsPVCoWRd6klBP4GHEBoxgbSTfrjTOUMPvC6xHUmQjFk59urUnBnQMnELUoUCTd6ocxSQWNNOFYqZ7rJNrLsNSMcDot91NFE0zGeEh7hkZYUOVlshTdGKUEA1iaV6k0Uz9vZFhodREBGYyz6gWvVz8zulenDlZSxKUk0jMj80SDnSMcp7QCGTlGgMQQTyUxWREZYYqJNW2VTgrv45WXSPq5Ts29u6jWr4s6SnAEx3AKLlxCHW6hCS0g8AjP8Apv1pP1Yr1bHPRFavYOYQsD5AKXVk5Ylatexitlatexit sha1_base64DYZek5SmevKS8py25dx0aIqUbBYAAABnicbVDLSsNAFL3xWesr1aWbwSK4kJKIoAsXBSm4rGAf0IYwmUzboTNJmJkoJfZT3LhQxK1f4s6cdJmoa0HBg7n3Ms9c4KEM6Ud59taWV1b39gsbZW3d3b39u3KQVvFqSS0RWIey26AFeUsoi3NNKfdRFIsAk47wfgm9zsPVCoWRd6klBP4GHEBoxgbSTfrjTOUMPvC6xHUmQjFk59urUnBnQMnELUoUCTd6ocxSQWNNOFYqZ7rJNrLsNSMcDot91NFE0zGeEh7hkZYUOVlshTdGKUEA1iaV6k0Uz9vZFhodREBGYyz6gWvVz8zulenDlZSxKUk0jMj80SDnSMcp7QCGTlGgMQQTyUxWREZYYqJNW2VTgrv45WXSPq5Ts29u6jWr4s6SnAEx3AKLlxCHW6hCS0g8AjP8Apv1pP1Yr1bHPRFavYOYQsD5AKXVk5YlatexitV Vhid
latexit sha1_base64brwuxF74R6OEOykh378asRBfzEAAABnicbVBNS8NAFHzxs9avVI9eFovgQUoigh48FLx4rGDTQhvCZrNtl4mYXejlNif4sWDIl79Jd78N27aHLR1YGGYeY83O2HKmdKO822trK6tb2xWtqrbO7t73btwFNJJgltk4QnshtiRTmLaVszzWk3lRSLkNNOOL4pM4DlYol8b2epNQXeBizASNYGymwa94Z8oKwHokRT5i0TSw607DmQEtE7ckdSjRCuyvfpSQTNBYE46V6rlOqv0cS80Ip9NqP1M0xWSMh7RnaIwFVX4iz5FJ0aJ0CCR5sUazdTfGzkWSk1EaCaLjGrRK8TvF6mB1dzuI00zQm80ODjCOdoKIHFDFJieYTQzCRzGRFZIQlJtq0VTUluItfXibeecN1Gu7dRb15XdZRgSM4hlNw4RKacAstaAOBR3iGV3iznqwX6936mIuWOXOIfyB9fkD20qTuAlatexitlatexit sha1_base64brwuxF74R6OEOykh378asRBfzEAAABnicbVBNS8NAFHzxs9avVI9eFovgQUoigh48FLx4rGDTQhvCZrNtl4mYXejlNif4sWDIl79Jd78N27aHLR1YGGYeY83O2HKmdKO822trK6tb2xWtqrbO7t73btwFNJJgltk4QnshtiRTmLaVszzWk3lRSLkNNOOL4pM4DlYol8b2epNQXeBizASNYGymwa94Z8oKwHokRT5i0TSw607DmQEtE7ckdSjRCuyvfpSQTNBYE46V6rlOqv0cS80Ip9NqP1M0xWSMh7RnaIwFVX4iz5FJ0aJ0CCR5sUazdTfGzkWSk1EaCaLjGrRK8TvF6mB1dzuI00zQm80ODjCOdoKIHFDFJieYTQzCRzGRFZIQlJtq0VTUluItfXibeecN1Gu7dRb15XdZRgSM4hlNw4RKacAstaAOBR3iGV3iznqwX6936mIuWOXOIfyB9fkD20qTuAlatexitlatexit sha1_base64brwuxF74R6OEOykh378asRBfzEAAABnicbVBNS8NAFHzxs9avVI9eFovgQUoigh48FLx4rGDTQhvCZrNtl4mYXejlNif4sWDIl79Jd78N27aHLR1YGGYeY83O2HKmdKO822trK6tb2xWtqrbO7t73btwFNJJgltk4QnshtiRTmLaVszzWk3lRSLkNNOOL4pM4DlYol8b2epNQXeBizASNYGymwa94Z8oKwHokRT5i0TSw607DmQEtE7ckdSjRCuyvfpSQTNBYE46V6rlOqv0cS80Ip9NqP1M0xWSMh7RnaIwFVX4iz5FJ0aJ0CCR5sUazdTfGzkWSk1EaCaLjGrRK8TvF6mB1dzuI00zQm80ODjCOdoKIHFDFJieYTQzCRzGRFZIQlJtq0VTUluItfXibeecN1Gu7dRb15XdZRgSM4hlNw4RKacAstaAOBR3iGV3iznqwX6936mIuWOXOIfyB9fkD20qTuAlatexitlatexit sha1_base64brwuxF74R6OEOykh378asRBfzEAAABnicbVBNS8NAFHzxs9avVI9eFovgQUoigh48FLx4rGDTQhvCZrNtl4mYXejlNif4sWDIl79Jd78N27aHLR1YGGYeY83O2HKmdKO822trK6tb2xWtqrbO7t73btwFNJJgltk4QnshtiRTmLaVszzWk3lRSLkNNOOL4pM4DlYol8b2epNQXeBizASNYGymwa94Z8oKwHokRT5i0TSw607DmQEtE7ckdSjRCuyvfpSQTNBYE46V6rlOqv0cS80Ip9NqP1M0xWSMh7RnaIwFVX4iz5FJ0aJ0CCR5sUazdTfGzkWSk1EaCaLjGrRK8TvF6mB1dzuI00zQm80ODjCOdoKIHFDFJieYTQzCRzGRFZIQlJtq0VTUluItfXibeecN1Gu7dRb15XdZRgSM4hlNw4RKacAstaAOBR3iGV3iznqwX6936mIuWOXOIfyB9fkD20qTuAlatexituuhid
latexit sha1_base64UO6spgXZarocGO1zBKohmEwCjcAAACDHicbZDNSsNAFIVv6ltf1WXbgaL4EJKIoIuXBTcuKxgW6ENZTKZtENnkjAzEUrIA7jxVdy4UMStDDOt3GSBtTWAwMf597L3Hu8mDOlbfvLqiwtr6yuVddrG5tb2zv13b2uihJJaIdEPJJ3HlaUs5B2NNOc3sWSYuFx2vMmV3m9d0lYlF4q6cxdQUehSxgBGtjDeuNgcB67AVpkp2gHx4WKEU6Zn5muuymXQgtglNCA0q1hXPgRRRNBQE46V6jt2rN0US80Ip1ltkCgaYzLBI9o3GGJBlZsWx2ToyDgCiJpXqhR4f6eSLFQaio805nvqOZruflfrZo4MJNWRgnmoZk9lGQcKQjlCeDfCYp0XxqABPJzK6IjLHERJv8aiYEZ7kReieNh276dycNVqXZRxVOIBDOAYHzqEF19CGDhB4gCd4gVfr0Xq23qz3WWvFKmf24Ysj28GGpw2latexitlatexit sha1_base64UO6spgXZarocGO1zBKohmEwCjcAAACDHicbZDNSsNAFIVv6ltf1WXbgaL4EJKIoIuXBTcuKxgW6ENZTKZtENnkjAzEUrIA7jxVdy4UMStDDOt3GSBtTWAwMf597L3Hu8mDOlbfvLqiwtr6yuVddrG5tb2zv13b2uihJJaIdEPJJ3HlaUs5B2NNOc3sWSYuFx2vMmV3m9d0lYlF4q6cxdQUehSxgBGtjDeuNgcB67AVpkp2gHx4WKEU6Zn5muuymXQgtglNCA0q1hXPgRRRNBQE46V6jt2rN0US80Ip1ltkCgaYzLBI9o3GGJBlZsWx2ToyDgCiJpXqhR4f6eSLFQaio805nvqOZruflfrZo4MJNWRgnmoZk9lGQcKQjlCeDfCYp0XxqABPJzK6IjLHERJv8aiYEZ7kReieNh276dycNVqXZRxVOIBDOAYHzqEF19CGDhB4gCd4gVfr0Xq23qz3WWvFKmf24Ysj28GGpw2latexitlatexit sha1_base64UO6spgXZarocGO1zBKohmEwCjcAAACDHicbZDNSsNAFIVv6ltf1WXbgaL4EJKIoIuXBTcuKxgW6ENZTKZtENnkjAzEUrIA7jxVdy4UMStDDOt3GSBtTWAwMf597L3Hu8mDOlbfvLqiwtr6yuVddrG5tb2zv13b2uihJJaIdEPJJ3HlaUs5B2NNOc3sWSYuFx2vMmV3m9d0lYlF4q6cxdQUehSxgBGtjDeuNgcB67AVpkp2gHx4WKEU6Zn5muuymXQgtglNCA0q1hXPgRRRNBQE46V6jt2rN0US80Ip1ltkCgaYzLBI9o3GGJBlZsWx2ToyDgCiJpXqhR4f6eSLFQaio805nvqOZruflfrZo4MJNWRgnmoZk9lGQcKQjlCeDfCYp0XxqABPJzK6IjLHERJv8aiYEZ7kReieNh276dycNVqXZRxVOIBDOAYHzqEF19CGDhB4gCd4gVfr0Xq23qz3WWvFKmf24Ysj28GGpw2latexitlatexit sha1_base64UO6spgXZarocGO1zBKohmEwCjcAAACDHicbZDNSsNAFIVv6ltf1WXbgaL4EJKIoIuXBTcuKxgW6ENZTKZtENnkjAzEUrIA7jxVdy4UMStDDOt3GSBtTWAwMf597L3Hu8mDOlbfvLqiwtr6yuVddrG5tb2zv13b2uihJJaIdEPJJ3HlaUs5B2NNOc3sWSYuFx2vMmV3m9d0lYlF4q6cxdQUehSxgBGtjDeuNgcB67AVpkp2gHx4WKEU6Zn5muuymXQgtglNCA0q1hXPgRRRNBQE46V6jt2rN0US80Ip1ltkCgaYzLBI9o3GGJBlZsWx2ToyDgCiJpXqhR4f6eSLFQaio805nvqOZruflfrZo4MJNWRgnmoZk9lGQcKQjlCeDfCYp0XxqABPJzK6IjLHERJv8aiYEZ7kReieNh276dycNVqXZRxVOIBDOAYHzqEF19CGDhB4gCd4gVfr0Xq23qz3WWvFKmf24Ysj28GGpw2latexit
Edge blockNode blockGlobal blocku
latexit sha1_base64znt8hwWv6wryqwCugrweUajkM8AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubfaSNXu7y6eEo78BxsLRWz9P3bGzcfhSYGHi8N8PMvFhxZqzvf3tLyyurauFjeLm1vbObmlvPzQy04TWieRSN2NsKGeC1i2znDaVpjiNOW3Eg6ux33ik2jApbu1Q0SjFPcESRrB1UthWfXaXdUplvJPgBZJMCPl6mH4dA8AtU7pq92VJEupsIRjY1qBr2yUY20Z4XRUbGeGKkwGuEdbjgqcUhPlk2tH6MQpXZRI7UpYNFFTQ4NWaYxq4zxbZv5r2xJXymxyEeVMqMxSQaaLkowjK9H4ddRlmhLLh45gopm7FZE1phYF1DRhRDMv7xIwrNK4FeCG5fGJUxRgCM4hlMI4ByqcA01qAOBB3iGV3jzpPfivXsf09YlbzZzAHgff4AqE6Qnwlatexitlatexit sha1_base64Nc0DXje6uYB0fHlXL99yCq0noAAAB7XicbVC7SgNBFL0bXzGooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS7HzAwPJ49CEw9cOJxzLfeE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYyeIAbtNuWKW3UnQIvEm5FK7cBkNn3e71buz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQHXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2YccD5AGXTkqwlatexitlatexit sha1_base64Nc0DXje6uYB0fHlXL99yCq0noAAAB7XicbVC7SgNBFL0bXzGooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS7HzAwPJ49CEw9cOJxzLfeE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYyeIAbtNuWKW3UnQIvEm5FK7cBkNn3e71buz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQHXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2YccD5AGXTkqwlatexitlatexit sha1_base64S5XnA5iYIAgqxiIi0ptSwAiKP4AAAB7XicbVA9SwNBEJ2LXzFRS1tFoNgFe5stLAI2FhGMBQnGFvM5es2ds9dveEEPIfbCwUsfX2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6wtr6xuVXcLu3s7u0flAPmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2HCHrlSt1ZDrJIgJxXIUeVv7p9xbIEpWWCGtMJNSGE6otZwKnpW5mMKVsRAfYcVTSBE04mV87JWdO6ZNYaVfSkrn6e2JCE2PGSeQ6E2qHZtmbif95nczGVGEyzSzKNliUZwJYhWZvU76XCOzYuwIZZq7WwkbUk2ZdQGVXAjB8surpHlRDfxqcOdXatd5HEU4gVM4hwAuoQa3UIcGMHiEZ3iFN095L96797FoLXj5zDH8gff5A53Djxwlatexit
latexit sha1_base64m8MJ1M94ujO0d0COo5n2Dsol6rcAAAB53icbVC7SgNBFL0bXzGopY2g0GwCrs2phAM2FhGMA9IFpmdvZsMmZ1dZmaFsKS0sbFQxNZP8RfsAZwsmj0MQDFw7nnMt9BKng2rjul1NYWV1b3yhulra2d3b3yvsHLZ1kimGTJSJRnYBqFFxi03AjsJMqpHEgsB0MryZx6V5om8NaMUZj2JY84o8ZKjbtyxa26U5Bl4s1J5fLj4EAgM19sKEZTFKwwTVuuu5qfFzqgxnAselXqYxpWxI9i1VNIYtZ9P9xyTE6uEJEqULWnIVP3dkdNY61Ec2GRMzUAvehPxP6bmajm51ymmUHJZoOiTBCTkMnRJOQKmREjSyhT3O5K2IAqyox9TckwVs8eZm0zqqeWVu3Er9AmYowhEcwyl4cA51uIYGNIFBCIwDC8Od56cVdtFi04855DAPnQdsI8Alatexitlatexit sha1_base64q1zM5ZsCNMJAtNUZI97B98ATlaAAAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo1Si4xJrhRmAzUUgjX2DD71OMYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL967g7fqt81tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhYV3R0YjrQeRb5MRNT09643E7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1TtE7zZkdJaTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvkiCM3Zgz9wXn8AGkeQ8wlatexitlatexit sha1_base64q1zM5ZsCNMJAtNUZI97B98ATlaAAAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo1Si4xJrhRmAzUUgjX2DD71OMYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL967g7fqt81tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhYV3R0YjrQeRb5MRNT09643E7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1TtE7zZkdJaTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvkiCM3Zgz9wXn8AGkeQ8wlatexitlatexit sha1_base64ioxb3woZF1oAlTScqds23PrgiMYAAAB53icbVC7SgNBFL3rM8ZX1NJmMAhWYdZGC4uAjWUE84BkkdnZ2WTI7Owyc1cIS37AxkIRW3Jzr9xkmyhiQcGDuecy9x7wkxJi5Re2vrG5tb25Wd6u7esFh7ei4Y9PccNHmqUpNL2RWKKlFGyUq0cuMYEmoRDcc38787pMwVqb6ASeZCBI21DKWnKGTWo1Om3QOcgq8UtShxIuzWIUp4nQiNXzNqTzMMCmZQciWm1UFuRcb4mA1F31HNEmGDYr7nlJw7JSJxatzTSObq74mCJdZOktAlE4YjuzNxP8fo7xdVBIneUoNF98FOeKYEpmR5NIGsFRTRxh3Ei3KEjZhhHV03VleAvn7xKOpcNnzb8e1pv3pR1VOAUzuACfLiCJtxBC9rAIYJneIU3T3ov3rv3sYiueeXMCfyB9kDCGmMcAlatexitv
latexit sha1_base64HCiXjOq04H3f4Ed7vqyiRfd2dIAAAB7XicbVA9SwNBEJ2LXzFnQo2NotBsAp3NlpYBGwsI5hLIDnj3mYvWbO3ezuRcKR2BjoYit8fOfPmo9DEBwOP92aYmRelnGnjed9OYWV1bX2juFna2t7Z3XP3DwItM0VonUguVTPCmnImaN0ww2kzVRQnEaeNaHA98RtDqjST4s6MUhomuCdYzAg2VgraaZdDztu2at4U6Bl4s9JuXoUPD0AQK3jfrW7kmQJFYZwrHXL91IT5lgZRjgdl9qZpikmA9yjLUsFTqgO8m1Y3RqlS6KpbIlDJqqvydynGg9SiLbmWDT14veRPzPa2UmvgxzJtLMUEFmiKMIyPR5HXUZYoSw0eWYKKYvRWRPlaYGBtQyYbgL768TILziu9VFubxhXMUIRjOIEz8OECqnADNagDgUd4hld4c6Tz4rw7H7PWgjOfOYQcD5AKnSkKAlatexitlatexit sha1_base64voSHBGyFE5xYPVKPYMGTIarLQAAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0fz8APDyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQWNZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX21zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcqo1N4xKmKMARHMMpHAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYAcD5AGdXkq0latexitlatexit sha1_base64voSHBGyFE5xYPVKPYMGTIarLQAAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0fz8APDyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQWNZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX21zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcqo1N4xKmKMARHMMpHAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYAcD5AGdXkq0latexitlatexit sha1_base64Fc8T4ygtia14k1zCDji4ezWDqYAAAB7XicbVA9SwNBEJ3zM8avqKXNYhCswp2NFhYBG8sI5gOSMxtNsmavd1jdy4QjvwHGwtFbP0dv4bN8kVmvhg4PHeDDPzokQKi777a2tb2xubRd2irt7weHpaPjhtWpYbzOtNSmFVHLpVC8jgIlbyWG0ziSvBmNbmdc8yNFVo94CThYUwHSvQFoikRicZisdxt1T2K4cZJUEOSlDjlq39NXpaZbGXCGT1Np24CcYZtSgYJJPi53U8oSyER3wtqOKxtyG2fzaKTl3So0tXGlkMzV3xMZja2dxJHrjCkO7bI3Ez2in2r8NMqCRFrthiUTVBDWZvU56wnCGcuIIZUa4WwkbUkMZuoCKLoRgeVV0risBH4luPfL1Zs8jgKcwhlcQABXUIU7qEEdGDzBM7zCm6e9Fd1i0rnn5zAn8gff5A59Hjx0latexite
latexit sha1_base64gRKFyQFytmwqWy0cvo5FmmPz8IAAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubeaSNXu3x6eEo78BxsLRWz9P3bGzcfhSYGHi8N8PMvDAVXBvXXaWlldW19YLG8XNre2d3dLevq9lphjWmRRSNUOqUfAE64Ybgc1UIY1DgY1wcDX2G4oNJfJrRmmGMS0lCIM2qs5LfTPrDTqnsVtwJyCLxZqRcPfSf7gGg1il9tbuSZTEmhgmqdctzUxPkVBnOBI6K7UxjStmA9rBlaUJj1EEuXZETqzSJZFUthJDJurviZzGWgj0HbG1PT1vDcWNamYkugpwnaWYwYdNFUSaIkWT8OulyhcyIoSWUKW5vJaxPFWXGBlS0IXjzLy8S6ziuRXvxqZxCVMU4AiO4RQ8OIcqXEMN6sDgAZ7hFd4c6bw4787HtHXJmc0cwB84nzQDpCPlatexitlatexit sha1_base6474MJShuZzxGyM2nLY3EY87InuhIAAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS7HzAwPJ49CEw9cOJxzLfeE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2OeQ9Ks0TcmExCEJOYBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75YO72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZmFdEAUocYGVLIhePMvLxLtOq5VeapnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8OaO27S14Mxm9tEfOB8TZOSnAlatexitlatexit sha1_base6474MJShuZzxGyM2nLY3EY87InuhIAAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS7HzAwPJ49CEw9cOJxzLfeE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2OeQ9Ks0TcmExCEJOYBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75YO72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZmFdEAUocYGVLIhePMvLxLtOq5VeapnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8OaO27S14Mxm9tEfOB8TZOSnAlatexitlatexit sha1_base64pLq6KB1S9uyUeWpG4byg43mK0AAAB7XicbVA9SwNBEJ2LXzFRS1tFoNgFe5stLAI2FhGMBQnGFvM5es2ds9dveEEPIfbCwUsfX2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6wtr6xuVXcLu3s7u0flAPmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2HAF75Ypf9ecgqyTISQVy1Hvlr25fsSxBaZmgxnQCP7XhhGrLmcBpqZsZTCkb0QF2HJU0QRNO5tdOyZlTiRW2pW0ZK7npjQxJhxErnOhNqhWfZm4n9eJ7PxVTjhMs0sSrZYFGeCWEVmr5M18isGDtCmebuVsKGVFNmXUAlF0KwPIqaV5UA78a3PmV2nUeRxFO4BTOIYBLqMEt1KEBDB7hGV7hzVPeifufSxaC14cwx4H3AIWDjwwlatexit b Independent recurrent block
Edge blockNode blockGlobal blockV0
latexit sha1_base64gAQ7qdt3IKvK5oBqK3uN1PHYi1kAAAB6XicbVA9SwNBEJ2LXzFRS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCPwMZCEVvkZ3xk1yhSYGHi8N8PMvCARXBvXXYKasbm1vF7dLO7t7QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbzh0jyWj2aSoBRoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL31BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4ned3UhNdxmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cddj0Vpw8pljAPn8wcRSI0Flatexitlatexit sha1_base64gAQ7qdt3IKvK5oBqK3uN1PHYi1kAAAB6XicbVA9SwNBEJ2LXzFRS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCPwMZCEVvkZ3xk1yhSYGHi8N8PMvCARXBvXXYKasbm1vF7dLO7t7QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbzh0jyWj2aSoBRoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL31BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4ned3UhNdxmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cddj0Vpw8pljAPn8wcRSI0Flatexitlatexit sha1_base64gAQ7qdt3IKvK5oBqK3uN1PHYi1kAAAB6XicbVA9SwNBEJ2LXzFRS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCPwMZCEVvkZ3xk1yhSYGHi8N8PMvCARXBvXXYKasbm1vF7dLO7t7QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbzh0jyWj2aSoBRoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL31BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4ned3UhNdxmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cddj0Vpw8pljAPn8wcRSI0Flatexitlatexit sha1_base64gAQ7qdt3IKvK5oBqK3uN1PHYi1kAAAB6XicbVA9SwNBEJ2LXzFRS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCPwMZCEVvkZ3xk1yhSYGHi8N8PMvCARXBvXXYKasbm1vF7dLO7t7QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbzh0jyWj2aSoBRoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL31BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4ned3UhNdxmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cddj0Vpw8pljAPn8wcRSI0Flatexitu0
latexit sha1_base64Zn1gIms2ONBt0R58c8NGdBbqUAAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7dUD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9WvjBmacQVMkmN6Xlugn5GNQomazSTw1PKJvQEe9ZqmjEjZNI8ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvEr5dieONnQiUpcsUWH4WpJBiTH4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMzAKbw46L86787EYLTnFzjH8gfP5A1s4kUQlatexitlatexit sha1_base64Zn1gIms2ONBt0R58c8NGdBbqUAAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7dUD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9WvjBmacQVMkmN6Xlugn5GNQomazSTw1PKJvQEe9ZqmjEjZNI8ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvEr5dieONnQiUpcsUWH4WpJBiTH4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMzAKbw46L86787EYLTnFzjH8gfP5A1s4kUQlatexitlatexit sha1_base64Zn1gIms2ONBt0R58c8NGdBbqUAAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7dUD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9WvjBmacQVMkmN6Xlugn5GNQomazSTw1PKJvQEe9ZqmjEjZNI8ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvEr5dieONnQiUpcsUWH4WpJBiTH4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMzAKbw46L86787EYLTnFzjH8gfP5A1s4kUQlatexitlatexit sha1_base64Zn1gIms2ONBt0R58c8NGdBbqUAAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7dUD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9WvjBmacQVMkmN6Xlugn5GNQomazSTw1PKJvQEe9ZqmjEjZNI8ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvEr5dieONnQiUpcsUWH4WpJBiTH4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMzAKbw46L86787EYLTnFzjH8gfP5A1s4kUQlatexitu
latexit sha1_base64znt8hwWv6wryqwCugrweUajkM8AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubfaSNXu7y6eEo78BxsLRWz9P3bGzcfhSYGHi8N8PMvFhxZqzvf3tLyyurauFjeLm1vbObmlvPzQy04TWieRSN2NsKGeC1i2znDaVpjiNOW3Eg6ux33ik2jApbu1Q0SjFPcESRrB1UthWfXaXdUplvJPgBZJMCPl6mH4dA8AtU7pq92VJEupsIRjY1qBr2yUY20Z4XRUbGeGKkwGuEdbjgqcUhPlk2tH6MQpXZRI7UpYNFFTQ4NWaYxq4zxbZv5r2xJXymxyEeVMqMxSQaaLkowjK9H4ddRlmhLLh45gopm7FZE1phYF1DRhRDMv7xIwrNK4FeCG5fGJUxRgCM4hlMI4ByqcA01qAOBB3iGV3jzpPfivXsf09YlbzZzAHgff4AqE6Qnwlatexitlatexit sha1_base64Nc0DXje6uYB0fHlXL99yCq0noAAAB7XicbVC7SgNBFL0bXzGooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS7HzAwPJ49CEw9cOJxzLfeE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYyeIAbtNuWKW3UnQIvEm5FK7cBkNn3e71buz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQHXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2YccD5AGXTkqwlatexitlatexit sha1_base64Nc0DXje6uYB0fHlXL99yCq0noAAAB7XicbVC7SgNBFL0bXzGooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS7HzAwPJ49CEw9cOJxzLfeE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYyeIAbtNuWKW3UnQIvEm5FK7cBkNn3e71buz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQHXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2YccD5AGXTkqwlatexitlatexit sha1_base64S5XnA5iYIAgqxiIi0ptSwAiKP4AAAB7XicbVA9SwNBEJ2LXzFRS1tFoNgFe5stLAI2FhGMBQnGFvM5es2ds9dveEEPIfbCwUsfX2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6wtr6xuVXcLu3s7u0flAPmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2HCHrlSt1ZDrJIgJxXIUeVv7p9xbIEpWWCGtMJNSGE6otZwKnpW5mMKVsRAfYcVTSBE04mV87JWdO6ZNYaVfSkrn6e2JCE2PGSeQ6E2qHZtmbif95nczGVGEyzSzKNliUZwJYhWZvU76XCOzYuwIZZq7WwkbUk2ZdQGVXAjB8surpHlRDfxqcOdXatd5HEU4gVM4hwAuoQa3UIcGMHiEZ3iFN095L96797FoLXj5zDH8gff5A53Djxwlatexitvu
latexit sha1_base648QVocR3pGD0iQLG1OhPZDl9fEAAABnicbVBNS8NAEN3Ur1qouLJy2IRPJVEBD0WvXisYDgiWWz3TRLN9mwO6mUUPCvePGgiFdhzfjds2B219MPB4b4aZeUEquAbHbZKK6tr6xvlzcrW9s7unr10NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs88lQkHKRpggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zopDlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJXjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bHPWklXMHKIsD5AAdVliglatexitlatexit sha1_base648QVocR3pGD0iQLG1OhPZDl9fEAAABnicbVBNS8NAEN3Ur1qouLJy2IRPJVEBD0WvXisYDgiWWz3TRLN9mwO6mUUPCvePGgiFdhzfjds2B219MPB4b4aZeUEquAbHbZKK6tr6xvlzcrW9s7unr10NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs88lQkHKRpggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zopDlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJXjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bHPWklXMHKIsD5AAdVliglatexitlatexit sha1_base648QVocR3pGD0iQLG1OhPZDl9fEAAABnicbVBNS8NAEN3Ur1qouLJy2IRPJVEBD0WvXisYDgiWWz3TRLN9mwO6mUUPCvePGgiFdhzfjds2B219MPB4b4aZeUEquAbHbZKK6tr6xvlzcrW9s7unr10NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs88lQkHKRpggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zopDlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJXjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bHPWklXMHKIsD5AAdVliglatexitlatexit sha1_base648QVocR3pGD0iQLG1OhPZDl9fEAAABnicbVBNS8NAEN3Ur1qouLJy2IRPJVEBD0WvXisYDgiWWz3TRLN9mwO6mUUPCvePGgiFdhzfjds2B219MPB4b4aZeUEquAbHbZKK6tr6xvlzcrW9s7unr10NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs88lQkHKRpggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zopDlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJXjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bHPWklXMHKIsD5AAdVliglatexit
latexit sha1_base64m8MJ1M94ujO0d0COo5n2Dsol6rcAAAB53icbVC7SgNBFL0bXzGopY2g0GwCrs2phAM2FhGMA9IFpmdvZsMmZ1dZmaFsKS0sbFQxNZP8RfsAZwsmj0MQDFw7nnMt9BKng2rjul1NYWV1b3yhulra2d3b3yvsHLZ1kimGTJSJRnYBqFFxi03AjsJMqpHEgsB0MryZx6V5om8NaMUZj2JY84o8ZKjbtyxa26U5Bl4s1J5fLj4EAgM19sKEZTFKwwTVuuu5qfFzqgxnAselXqYxpWxI9i1VNIYtZ9P9xyTE6uEJEqULWnIVP3dkdNY61Ec2GRMzUAvehPxP6bmajm51ymmUHJZoOiTBCTkMnRJOQKmREjSyhT3O5K2IAqyox9TckwVs8eZm0zqqeWVu3Er9AmYowhEcwyl4cA51uIYGNIFBCIwDC8Od56cVdtFi04855DAPnQdsI8Alatexitlatexit sha1_base64q1zM5ZsCNMJAtNUZI97B98ATlaAAAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo1Si4xJrhRmAzUUgjX2DD71OMYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL967g7fqt81tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhYV3R0YjrQeRb5MRNT09643E7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1TtE7zZkdJaTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvkiCM3Zgz9wXn8AGkeQ8wlatexitlatexit sha1_base64q1zM5ZsCNMJAtNUZI97B98ATlaAAAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo1Si4xJrhRmAzUUgjX2DD71OMYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL967g7fqt81tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhYV3R0YjrQeRb5MRNT09643E7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1TtE7zZkdJaTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvkiCM3Zgz9wXn8AGkeQ8wlatexitlatexit sha1_base64ioxb3woZF1oAlTScqds23PrgiMYAAAB53icbVC7SgNBFL3rM8ZX1NJmMAhWYdZGC4uAjWUE84BkkdnZ2WTI7Owyc1cIS37AxkIRW3Jzr9xkmyhiQcGDuecy9x7wkxJi5Re2vrG5tb25Wd6u7esFh7ei4Y9PccNHmqUpNL2RWKKlFGyUq0cuMYEmoRDcc38787pMwVqb6ASeZCBI21DKWnKGTWo1Om3QOcgq8UtShxIuzWIUp4nQiNXzNqTzMMCmZQciWm1UFuRcb4mA1F31HNEmGDYr7nlJw7JSJxatzTSObq74mCJdZOktAlE4YjuzNxP8fo7xdVBIneUoNF98FOeKYEpmR5NIGsFRTRxh3Ei3KEjZhhHV03VleAvn7xKOpcNnzb8e1pv3pR1VOAUzuACfLiCJtxBC9rAIYJneIU3T3ov3rv3sYiueeXMCfyB9kDCGmMcAlatexitv
latexit sha1_base64HCiXjOq04H3f4Ed7vqyiRfd2dIAAAB7XicbVA9SwNBEJ2LXzFnQo2NotBsAp3NlpYBGwsI5hLIDnj3mYvWbO3ezuRcKR2BjoYit8fOfPmo9DEBwOP92aYmRelnGnjed9OYWV1bX2juFna2t7Z3XP3DwItM0VonUguVTPCmnImaN0ww2kzVRQnEaeNaHA98RtDqjST4s6MUhomuCdYzAg2VgraaZdDztu2at4U6Bl4s9JuXoUPD0AQK3jfrW7kmQJFYZwrHXL91IT5lgZRjgdl9qZpikmA9yjLUsFTqgO8m1Y3RqlS6KpbIlDJqqvydynGg9SiLbmWDT14veRPzPa2UmvgxzJtLMUEFmiKMIyPR5HXUZYoSw0eWYKKYvRWRPlaYGBtQyYbgL768TILziu9VFubxhXMUIRjOIEz8OECqnADNagDgUd4hld4c6Tz4rw7H7PWgjOfOYQcD5AKnSkKAlatexitlatexit sha1_base64voSHBGyFE5xYPVKPYMGTIarLQAAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0fz8APDyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQWNZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX21zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcqo1N4xKmKMARHMMpHAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYAcD5AGdXkq0latexitlatexit sha1_base64voSHBGyFE5xYPVKPYMGTIarLQAAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0fz8APDyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQWNZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX21zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcqo1N4xKmKMARHMMpHAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYAcD5AGdXkq0latexitlatexit sha1_base64Fc8T4ygtia14k1zCDji4ezWDqYAAAB7XicbVA9SwNBEJ3zM8avqKXNYhCswp2NFhYBG8sI5gOSMxtNsmavd1jdy4QjvwHGwtFbP0dv4bN8kVmvhg4PHeDDPzokQKi777a2tb2xubRd2irt7weHpaPjhtWpYbzOtNSmFVHLpVC8jgIlbyWG0ziSvBmNbmdc8yNFVo94CThYUwHSvQFoikRicZisdxt1T2K4cZJUEOSlDjlq39NXpaZbGXCGT1Np24CcYZtSgYJJPi53U8oSyER3wtqOKxtyG2fzaKTl3So0tXGlkMzV3xMZja2dxJHrjCkO7bI3Ez2in2r8NMqCRFrthiUTVBDWZvU56wnCGcuIIZUa4WwkbUkMZuoCKLoRgeVV0risBH4luPfL1Zs8jgKcwhlcQABXUIU7qEEdGDzBM7zCm6e9Fd1i0rnn5zAn8gff5A59Hjx0latexitev
latexit sha1_base64s3CwiDIc9TAit26LWmPV1hK0AAABnicbVBNS8NAEN3Ur1qouLJy2IRPJVEBD0WvXisYDgiWWznTRLN9mwu6mUUPCvePGgiFdhzfjds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXvoKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5ez8CT41ShHQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGxgEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ87hmWGAlatexitlatexit sha1_base64s3CwiDIc9TAit26LWmPV1hK0AAABnicbVBNS8NAEN3Ur1qouLJy2IRPJVEBD0WvXisYDgiWWznTRLN9mwu6mUUPCvePGgiFdhzfjds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXvoKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5ez8CT41ShHQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGxgEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ87hmWGAlatexitlatexit sha1_base64s3CwiDIc9TAit26LWmPV1hK0AAABnicbVBNS8NAEN3Ur1qouLJy2IRPJVEBD0WvXisYDgiWWznTRLN9mwu6mUUPCvePGgiFdhzfjds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXvoKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5ez8CT41ShHQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGxgEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ87hmWGAlatexitlatexit sha1_base64s3CwiDIc9TAit26LWmPV1hK0AAABnicbVBNS8NAEN3Ur1qouLJy2IRPJVEBD0WvXisYDgiWWznTRLN9mwu6mUUPCvePGgiFdhzfjds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXvoKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5ez8CT41ShHQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGxgEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ87hmWGAlatexite
latexit sha1_base64gRKFyQFytmwqWy0cvo5FmmPz8IAAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubeaSNXu3x6eEo78BxsLRWz9P3bGzcfhSYGHi8N8PMvDAVXBvXXaWlldW19YLG8XNre2d3dLevq9lphjWmRRSNUOqUfAE64Ybgc1UIY1DgY1wcDX2G4oNJfJrRmmGMS0lCIM2qs5LfTPrDTqnsVtwJyCLxZqRcPfSf7gGg1il9tbuSZTEmhgmqdctzUxPkVBnOBI6K7UxjStmA9rBlaUJj1EEuXZETqzSJZFUthJDJurviZzGWgj0HbG1PT1vDcWNamYkugpwnaWYwYdNFUSaIkWT8OulyhcyIoSWUKW5vJaxPFWXGBlS0IXjzLy8S6ziuRXvxqZxCVMU4AiO4RQ8OIcqXEMN6sDgAZ7hFd4c6bw4787HtHXJmc0cwB84nzQDpCPlatexitlatexit sha1_base6474MJShuZzxGyM2nLY3EY87InuhIAAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS7HzAwPJ49CEw9cOJxzLfeE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2OeQ9Ks0TcmExCEJOYBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75YO72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZmFdEAUocYGVLIhePMvLxLtOq5VeapnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8OaO27S14Mxm9tEfOB8TZOSnAlatexitlatexit sha1_base6474MJShuZzxGyM2nLY3EY87InuhIAAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS7HzAwPJ49CEw9cOJxzLfeE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2OeQ9Ks0TcmExCEJOYBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75YO72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZmFdEAUocYGVLIhePMvLxLtOq5VeapnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8OaO27S14Mxm9tEfOB8TZOSnAlatexitlatexit sha1_base64pLq6KB1S9uyUeWpG4byg43mK0AAAB7XicbVA9SwNBEJ2LXzFRS1tFoNgFe5stLAI2FhGMBQnGFvM5es2ds9dveEEPIfbCwUsfX2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6wtr6xuVXcLu3s7u0flAPmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2HAF75Ypf9ecgqyTISQVy1Hvlr25fsSxBaZmgxnQCP7XhhGrLmcBpqZsZTCkb0QF2HJU0QRNO5tdOyZlTiRW2pW0ZK7npjQxJhxErnOhNqhWfZm4n9eJ7PxVTjhMs0sSrZYFGeCWEVmr5M18isGDtCmebuVsKGVFNmXUAlF0KwPIqaV5UA78a3PmV2nUeRxFO4BTOIYBLqMEt1KEBDB7hGV7hzVPeifufSxaC14cwx4H3AIWDjwwlatexitElatexit sha1_base64iJx8cSgmmYNbMN8WtCvsNrlHUAAAB6HicbVBNS8NAEJ3Ur1qqh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFdkjfjds2B219MPB4b4aZeUEiuDau0U1tY3NreK26Wd3b39gLhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQTiQ8lDzqixUuOuX664VXcOskq8nFQgR71fuoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9RNj90Ss6sMiBhrGxJQbq74mMRlpPosB2RtSM9LI3EzuqkJr2MyyQ1KNliUZgKYmIy5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4AfzB5cfjMMlatexitlatexit sha1_base64iJx8cSgmmYNbMN8WtCvsNrlHUAAAB6HicbVBNS8NAEJ3Ur1qqh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFdkjfjds2B219MPB4b4aZeUEiuDau0U1tY3NreK26Wd3b39gLhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQTiQ8lDzqixUuOuX664VXcOskq8nFQgR71fuoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9RNj90Ss6sMiBhrGxJQbq74mMRlpPosB2RtSM9LI3EzuqkJr2MyyQ1KNliUZgKYmIy5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4AfzB5cfjMMlatexitlatexit sha1_base64iJx8cSgmmYNbMN8WtCvsNrlHUAAAB6HicbVBNS8NAEJ3Ur1qqh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFdkjfjds2B219MPB4b4aZeUEiuDau0U1tY3NreK26Wd3b39gLhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQTiQ8lDzqixUuOuX664VXcOskq8nFQgR71fuoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9RNj90Ss6sMiBhrGxJQbq74mMRlpPosB2RtSM9LI3EzuqkJr2MyyQ1KNliUZgKYmIy5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4AfzB5cfjMMlatexitlatexit sha1_base64iJx8cSgmmYNbMN8WtCvsNrlHUAAAB6HicbVBNS8NAEJ3Ur1qqh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFdkjfjds2B219MPB4b4aZeUEiuDau0U1tY3NreK26Wd3b39gLhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQTiQ8lDzqixUuOuX664VXcOskq8nFQgR71fuoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9RNj90Ss6sMiBhrGxJQbq74mMRlpPosB2RtSM9LI3EzuqkJr2MyyQ1KNliUZgKYmIy5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4AfzB5cfjMMlatexitVlatexit sha1_base64xc4uzoZiBSxZUZkArgltxczS6nMAAAB6HicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEdgY2FIrbJDvjZvkCk18MPB4b4aZeUEiuDau0UNja3tneKu6W9YPDoLxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QTiI8lDzqixUrM9KFfcqrsAWSdeTiqQozEofWHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWoWxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1GzOIpwBudwCR7UoA730IAWMEB4hld4cx6dFfdVi2Fpx85hTwPn8AbDjjNQlatexitlatexit sha1_base64xc4uzoZiBSxZUZkArgltxczS6nMAAAB6HicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEdgY2FIrbJDvjZvkCk18MPB4b4aZeUEiuDau0UNja3tneKu6W9YPDoLxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QTiI8lDzqixUrM9KFfcqrsAWSdeTiqQozEofWHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWoWxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1GzOIpwBudwCR7UoA730IAWMEB4hld4cx6dFfdVi2Fpx85hTwPn8AbDjjNQlatexitlatexit sha1_base64xc4uzoZiBSxZUZkArgltxczS6nMAAAB6HicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEdgY2FIrbJDvjZvkCk18MPB4b4aZeUEiuDau0UNja3tneKu6W9YPDoLxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QTiI8lDzqixUrM9KFfcqrsAWSdeTiqQozEofWHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWoWxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1GzOIpwBudwCR7UoA730IAWMEB4hld4cx6dFfdVi2Fpx85hTwPn8AbDjjNQlatexitlatexit sha1_base64xc4uzoZiBSxZUZkArgltxczS6nMAAAB6HicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEdgY2FIrbJDvjZvkCk18MPB4b4aZeUEiuDau0UNja3tneKu6W9YPDoLxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QTiI8lDzqixUrM9KFfcqrsAWSdeTiqQozEofWHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWoWxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1GzOIpwBudwCR7UoA730IAWMEB4hld4cx6dFfdVi2Fpx85hTwPn8AbDjjNQlatexit
c Messagepassing neural network
Edge blockNode blockGlobal blockV0
latexit sha1_base64gAQ7qdt3IKvK5oBqK3uN1PHYi1kAAAB6XicbVA9SwNBEJ2LXzFRS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCPwMZCEVvkZ3xk1yhSYGHi8N8PMvCARXBvXXYKasbm1vF7dLO7t7QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbzh0jyWj2aSoBRoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL31BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4ned3UhNdxmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cddj0Vpw8pljAPn8wcRSI0Flatexitlatexit sha1_base64gAQ7qdt3IKvK5oBqK3uN1PHYi1kAAAB6XicbVA9SwNBEJ2LXzFRS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCPwMZCEVvkZ3xk1yhSYGHi8N8PMvCARXBvXXYKasbm1vF7dLO7t7QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbzh0jyWj2aSoBRoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL31BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4ned3UhNdxmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cddj0Vpw8pljAPn8wcRSI0Flatexitlatexit sha1_base64gAQ7qdt3IKvK5oBqK3uN1PHYi1kAAAB6XicbVA9SwNBEJ2LXzFRS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCPwMZCEVvkZ3xk1yhSYGHi8N8PMvCARXBvXXYKasbm1vF7dLO7t7QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbzh0jyWj2aSoBRoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL31BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4ned3UhNdxmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cddj0Vpw8pljAPn8wcRSI0Flatexitlatexit sha1_base64gAQ7qdt3IKvK5oBqK3uN1PHYi1kAAAB6XicbVA9SwNBEJ2LXzFRS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCPwMZCEVvkZ3xk1yhSYGHi8N8PMvCARXBvXXYKasbm1vF7dLO7t7QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbzh0jyWj2aSoBRoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL31BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4ned3UhNdxmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cddj0Vpw8pljAPn8wcRSI0Flatexit
latexit sha1_base64m8MJ1M94ujO0d0COo5n2Dsol6rcAAAB53icbVC7SgNBFL0bXzGopY2g0GwCrs2phAM2FhGMA9IFpmdvZsMmZ1dZmaFsKS0sbFQxNZP8RfsAZwsmj0MQDFw7nnMt9BKng2rjul1NYWV1b3yhulra2d3b3yvsHLZ1kimGTJSJRnYBqFFxi03AjsJMqpHEgsB0MryZx6V5om8NaMUZj2JY84o8ZKjbtyxa26U5Bl4s1J5fLj4EAgM19sKEZTFKwwTVuuu5qfFzqgxnAselXqYxpWxI9i1VNIYtZ9P9xyTE6uEJEqULWnIVP3dkdNY61Ec2GRMzUAvehPxP6bmajm51ymmUHJZoOiTBCTkMnRJOQKmREjSyhT3O5K2IAqyox9TckwVs8eZm0zqqeWVu3Er9AmYowhEcwyl4cA51uIYGNIFBCIwDC8Od56cVdtFi04855DAPnQdsI8Alatexitlatexit sha1_base64q1zM5ZsCNMJAtNUZI97B98ATlaAAAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo1Si4xJrhRmAzUUgjX2DD71OMYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL967g7fqt81tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhYV3R0YjrQeRb5MRNT09643E7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1TtE7zZkdJaTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvkiCM3Zgz9wXn8AGkeQ8wlatexitlatexit sha1_base64q1zM5ZsCNMJAtNUZI97B98ATlaAAAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo1Si4xJrhRmAzUUgjX2DD71OMYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL967g7fqt81tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhYV3R0YjrQeRb5MRNT09643E7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1TtE7zZkdJaTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvkiCM3Zgz9wXn8AGkeQ8wlatexitlatexit sha1_base64ioxb3woZF1oAlTScqds23PrgiMYAAAB53icbVC7SgNBFL3rM8ZX1NJmMAhWYdZGC4uAjWUE84BkkdnZ2WTI7Owyc1cIS37AxkIRW3Jzr9xkmyhiQcGDuecy9x7wkxJi5Re2vrG5tb25Wd6u7esFh7ei4Y9PccNHmqUpNL2RWKKlFGyUq0cuMYEmoRDcc38787pMwVqb6ASeZCBI21DKWnKGTWo1Om3QOcgq8UtShxIuzWIUp4nQiNXzNqTzMMCmZQciWm1UFuRcb4mA1F31HNEmGDYr7nlJw7JSJxatzTSObq74mCJdZOktAlE4YjuzNxP8fo7xdVBIneUoNF98FOeKYEpmR5NIGsFRTRxh3Ei3KEjZhhHV03VleAvn7xKOpcNnzb8e1pv3pR1VOAUzuACfLiCJtxBC9rAIYJneIU3T3ov3rv3sYiueeXMCfyB9kDCGmMcAlatexitv
latexit sha1_base64HCiXjOq04H3f4Ed7vqyiRfd2dIAAAB7XicbVA9SwNBEJ2LXzFnQo2NotBsAp3NlpYBGwsI5hLIDnj3mYvWbO3ezuRcKR2BjoYit8fOfPmo9DEBwOP92aYmRelnGnjed9OYWV1bX2juFna2t7Z3XP3DwItM0VonUguVTPCmnImaN0ww2kzVRQnEaeNaHA98RtDqjST4s6MUhomuCdYzAg2VgraaZdDztu2at4U6Bl4s9JuXoUPD0AQK3jfrW7kmQJFYZwrHXL91IT5lgZRjgdl9qZpikmA9yjLUsFTqgO8m1Y3RqlS6KpbIlDJqqvydynGg9SiLbmWDT14veRPzPa2UmvgxzJtLMUEFmiKMIyPR5HXUZYoSw0eWYKKYvRWRPlaYGBtQyYbgL768TILziu9VFubxhXMUIRjOIEz8OECqnADNagDgUd4hld4c6Tz4rw7H7PWgjOfOYQcD5AKnSkKAlatexitlatexit sha1_base64voSHBGyFE5xYPVKPYMGTIarLQAAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0fz8APDyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQWNZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX21zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcqo1N4xKmKMARHMMpHAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYAcD5AGdXkq0latexitlatexit sha1_base64voSHBGyFE5xYPVKPYMGTIarLQAAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0fz8APDyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQWNZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX21zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcqo1N4xKmKMARHMMpHAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYAcD5AGdXkq0latexitlatexit sha1_base64Fc8T4ygtia14k1zCDji4ezWDqYAAAB7XicbVA9SwNBEJ3zM8avqKXNYhCswp2NFhYBG8sI5gOSMxtNsmavd1jdy4QjvwHGwtFbP0dv4bN8kVmvhg4PHeDDPzokQKi777a2tb2xubRd2irt7weHpaPjhtWpYbzOtNSmFVHLpVC8jgIlbyWG0ziSvBmNbmdc8yNFVo94CThYUwHSvQFoikRicZisdxt1T2K4cZJUEOSlDjlq39NXpaZbGXCGT1Np24CcYZtSgYJJPi53U8oSyER3wtqOKxtyG2fzaKTl3So0tXGlkMzV3xMZja2dxJHrjCkO7bI3Ez2in2r8NMqCRFrthiUTVBDWZvU56wnCGcuIIZUa4WwkbUkMZuoCKLoRgeVV0risBH4luPfL1Zs8jgKcwhlcQABXUIU7qEEdGDzBM7zCm6e9Fd1i0rnn5zAn8gff5A59Hjx0latexitev
latexit sha1_base64s3CwiDIc9TAit26LWmPV1hK0AAABnicbVBNS8NAEN3Ur1qouLJy2IRPJVEBD0WvXisYDgiWWznTRLN9mwu6mUUPCvePGgiFdhzfjds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXvoKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5ez8CT41ShHQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGxgEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ87hmWGAlatexitlatexit sha1_base64s3CwiDIc9TAit26LWmPV1hK0AAABnicbVBNS8NAEN3Ur1qouLJy2IRPJVEBD0WvXisYDgiWWznTRLN9mwu6mUUPCvePGgiFdhzfjds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXvoKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5ez8CT41ShHQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGxgEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ87hmWGAlatexitlatexit sha1_base64s3CwiDIc9TAit26LWmPV1hK0AAABnicbVBNS8NAEN3Ur1qouLJy2IRPJVEBD0WvXisYDgiWWznTRLN9mwu6mUUPCvePGgiFdhzfjds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXvoKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5ez8CT41ShHQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGxgEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ87hmWGAlatexitlatexit sha1_base64s3CwiDIc9TAit26LWmPV1hK0AAABnicbVBNS8NAEN3Ur1qouLJy2IRPJVEBD0WvXisYDgiWWznTRLN9mwu6mUUPCvePGgiFdhzfjds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXvoKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5ez8CT41ShHQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGxgEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ87hmWGAlatexite
latexit sha1_base64gRKFyQFytmwqWy0cvo5FmmPz8IAAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubeaSNXu3x6eEo78BxsLRWz9P3bGzcfhSYGHi8N8PMvDAVXBvXXaWlldW19YLG8XNre2d3dLevq9lphjWmRRSNUOqUfAE64Ybgc1UIY1DgY1wcDX2G4oNJfJrRmmGMS0lCIM2qs5LfTPrDTqnsVtwJyCLxZqRcPfSf7gGg1il9tbuSZTEmhgmqdctzUxPkVBnOBI6K7UxjStmA9rBlaUJj1EEuXZETqzSJZFUthJDJurviZzGWgj0HbG1PT1vDcWNamYkugpwnaWYwYdNFUSaIkWT8OulyhcyIoSWUKW5vJaxPFWXGBlS0IXjzLy8S6ziuRXvxqZxCVMU4AiO4RQ8OIcqXEMN6sDgAZ7hFd4c6bw4787HtHXJmc0cwB84nzQDpCPlatexitlatexit sha1_base6474MJShuZzxGyM2nLY3EY87InuhIAAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS7HzAwPJ49CEw9cOJxzLfeE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2OeQ9Ks0TcmExCEJOYBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75YO72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZmFdEAUocYGVLIhePMvLxLtOq5VeapnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8OaO27S14Mxm9tEfOB8TZOSnAlatexitlatexit sha1_base6474MJShuZzxGyM2nLY3EY87InuhIAAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS7HzAwPJ49CEw9cOJxzLfeE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2OeQ9Ks0TcmExCEJOYBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75YO72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZmFdEAUocYGVLIhePMvLxLtOq5VeapnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8OaO27S14Mxm9tEfOB8TZOSnAlatexitlatexit sha1_base64pLq6KB1S9uyUeWpG4byg43mK0AAAB7XicbVA9SwNBEJ2LXzFRS1tFoNgFe5stLAI2FhGMBQnGFvM5es2ds9dveEEPIfbCwUsfX2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6wtr6xuVXcLu3s7u0flAPmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2HAF75Ypf9ecgqyTISQVy1Hvlr25fsSxBaZmgxnQCP7XhhGrLmcBpqZsZTCkb0QF2HJU0QRNO5tdOyZlTiRW2pW0ZK7npjQxJhxErnOhNqhWfZm4n9eJ7PxVTjhMs0sSrZYFGeCWEVmr5M18isGDtCmebuVsKGVFNmXUAlF0KwPIqaV5UA78a3PmV2nUeRxFO4BTOIYBLqMEt1KEBDB7hGV7hzVPeifufSxaC14cwx4H3AIWDjwwlatexitElatexit sha1_base64iJx8cSgmmYNbMN8WtCvsNrlHUAAAB6HicbVBNS8NAEJ3Ur1qqh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFdkjfjds2B219MPB4b4aZeUEiuDau0U1tY3NreK26Wd3b39gLhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQTiQ8lDzqixUuOuX664VXcOskq8nFQgR71fuoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9RNj90Ss6sMiBhrGxJQbq74mMRlpPosB2RtSM9LI3EzuqkJr2MyyQ1KNliUZgKYmIy5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4AfzB5cfjMMlatexitlatexit sha1_base64iJx8cSgmmYNbMN8WtCvsNrlHUAAAB6HicbVBNS8NAEJ3Ur1qqh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFdkjfjds2B219MPB4b4aZeUEiuDau0U1tY3NreK26Wd3b39gLhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQTiQ8lDzqixUuOuX664VXcOskq8nFQgR71fuoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9RNj90Ss6sMiBhrGxJQbq74mMRlpPosB2RtSM9LI3EzuqkJr2MyyQ1KNliUZgKYmIy5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4AfzB5cfjMMlatexitlatexit sha1_base64iJx8cSgmmYNbMN8WtCvsNrlHUAAAB6HicbVBNS8NAEJ3Ur1qqh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFdkjfjds2B219MPB4b4aZeUEiuDau0U1tY3NreK26Wd3b39gLhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQTiQ8lDzqixUuOuX664VXcOskq8nFQgR71fuoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9RNj90Ss6sMiBhrGxJQbq74mMRlpPosB2RtSM9LI3EzuqkJr2MyyQ1KNliUZgKYmIy5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4AfzB5cfjMMlatexitlatexit sha1_base64iJx8cSgmmYNbMN8WtCvsNrlHUAAAB6HicbVBNS8NAEJ3Ur1qqh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFdkjfjds2B219MPB4b4aZeUEiuDau0U1tY3NreK26Wd3b39gLhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQTiQ8lDzqixUuOuX664VXcOskq8nFQgR71fuoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9RNj90Ss6sMiBhrGxJQbq74mMRlpPosB2RtSM9LI3EzuqkJr2MyyQ1KNliUZgKYmIy5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4AfzB5cfjMMlatexitVlatexit sha1_base64xc4uzoZiBSxZUZkArgltxczS6nMAAAB6HicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEdgY2FIrbJDvjZvkCk18MPB4b4aZeUEiuDau0UNja3tneKu6W9YPDoLxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QTiI8lDzqixUrM9KFfcqrsAWSdeTiqQozEofWHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWoWxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1GzOIpwBudwCR7UoA730IAWMEB4hld4cx6dFfdVi2Fpx85hTwPn8AbDjjNQlatexitlatexit sha1_base64xc4uzoZiBSxZUZkArgltxczS6nMAAAB6HicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEdgY2FIrbJDvjZvkCk18MPB4b4aZeUEiuDau0UNja3tneKu6W9YPDoLxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QTiI8lDzqixUrM9KFfcqrsAWSdeTiqQozEofWHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWoWxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1GzOIpwBudwCR7UoA730IAWMEB4hld4cx6dFfdVi2Fpx85hTwPn8AbDjjNQlatexitlatexit sha1_base64xc4uzoZiBSxZUZkArgltxczS6nMAAAB6HicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEdgY2FIrbJDvjZvkCk18MPB4b4aZeUEiuDau0UNja3tneKu6W9YPDoLxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QTiI8lDzqixUrM9KFfcqrsAWSdeTiqQozEofWHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWoWxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1GzOIpwBudwCR7UoA730IAWMEB4hld4cx6dFfdVi2Fpx85hTwPn8AbDjjNQlatexitlatexit sha1_base64xc4uzoZiBSxZUZkArgltxczS6nMAAAB6HicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEdgY2FIrbJDvjZvkCk18MPB4b4aZeUEiuDau0UNja3tneKu6W9YPDoLxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QTiI8lDzqixUrM9KFfcqrsAWSdeTiqQozEofWHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWoWxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1GzOIpwBudwCR7UoA730IAWMEB4hld4cx6dFfdVi2Fpx85hTwPn8AbDjjNQlatexit d Nonlocal neural network
Edge blockNode blockGlobal blocku0
latexit sha1_base64Zn1gIms2ONBt0R58c8NGdBbqUAAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7dUD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9WvjBmacQVMkmN6Xlugn5GNQomazSTw1PKJvQEe9ZqmjEjZNI8ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvEr5dieONnQiUpcsUWH4WpJBiTH4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMzAKbw46L86787EYLTnFzjH8gfP5A1s4kUQlatexitlatexit sha1_base64Zn1gIms2ONBt0R58c8NGdBbqUAAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7dUD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9WvjBmacQVMkmN6Xlugn5GNQomazSTw1PKJvQEe9ZqmjEjZNI8ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvEr5dieONnQiUpcsUWH4WpJBiTH4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMzAKbw46L86787EYLTnFzjH8gfP5A1s4kUQlatexitlatexit sha1_base64Zn1gIms2ONBt0R58c8NGdBbqUAAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7dUD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9WvjBmacQVMkmN6Xlugn5GNQomazSTw1PKJvQEe9ZqmjEjZNI8ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvEr5dieONnQiUpcsUWH4WpJBiTH4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMzAKbw46L86787EYLTnFzjH8gfP5A1s4kUQlatexitlatexit sha1_base64Zn1gIms2ONBt0R58c8NGdBbqUAAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7dUD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9WvjBmacQVMkmN6Xlugn5GNQomazSTw1PKJvQEe9ZqmjEjZNI8ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvEr5dieONnQiUpcsUWH4WpJBiTH4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMzAKbw46L86787EYLTnFzjH8gfP5A1s4kUQlatexitu
latexit sha1_base64znt8hwWv6wryqwCugrweUajkM8AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubfaSNXu7y6eEo78BxsLRWz9P3bGzcfhSYGHi8N8PMvFhxZqzvf3tLyyurauFjeLm1vbObmlvPzQy04TWieRSN2NsKGeC1i2znDaVpjiNOW3Eg6ux33ik2jApbu1Q0SjFPcESRrB1UthWfXaXdUplvJPgBZJMCPl6mH4dA8AtU7pq92VJEupsIRjY1qBr2yUY20Z4XRUbGeGKkwGuEdbjgqcUhPlk2tH6MQpXZRI7UpYNFFTQ4NWaYxq4zxbZv5r2xJXymxyEeVMqMxSQaaLkowjK9H4ddRlmhLLh45gopm7FZE1phYF1DRhRDMv7xIwrNK4FeCG5fGJUxRgCM4hlMI4ByqcA01qAOBB3iGV3jzpPfivXsf09YlbzZzAHgff4AqE6Qnwlatexitlatexit sha1_base64Nc0DXje6uYB0fHlXL99yCq0noAAAB7XicbVC7SgNBFL0bXzGooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS7HzAwPJ49CEw9cOJxzLfeE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYyeIAbtNuWKW3UnQIvEm5FK7cBkNn3e71buz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQHXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2YccD5AGXTkqwlatexitlatexit sha1_base64Nc0DXje6uYB0fHlXL99yCq0noAAAB7XicbVC7SgNBFL0bXzGooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS7HzAwPJ49CEw9cOJxzLfeE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYyeIAbtNuWKW3UnQIvEm5FK7cBkNn3e71buz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQHXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2YccD5AGXTkqwlatexitlatexit sha1_base64S5XnA5iYIAgqxiIi0ptSwAiKP4AAAB7XicbVA9SwNBEJ2LXzFRS1tFoNgFe5stLAI2FhGMBQnGFvM5es2ds9dveEEPIfbCwUsfX2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6wtr6xuVXcLu3s7u0flAPmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2HCHrlSt1ZDrJIgJxXIUeVv7p9xbIEpWWCGtMJNSGE6otZwKnpW5mMKVsRAfYcVTSBE04mV87JWdO6ZNYaVfSkrn6e2JCE2PGSeQ6E2qHZtmbif95nczGVGEyzSzKNliUZwJYhWZvU76XCOzYuwIZZq7WwkbUk2ZdQGVXAjB8surpHlRDfxqcOdXatd5HEU4gVM4hwAuoQa3UIcGMHiEZ3iFN095L96797FoLXj5zDH8gff5A53Djxwlatexiteu
latexit sha1_base642suSYs2KtjHJeb1CIts1JrhYPIIAAABnicbVBNS8NAEN34WetXVDx5WSyCp5KIoMeiF48V7Ac0sWy2m3bpZjfsTpQSCv4VLx4U8erv8OacdvmoK0PBh7vzTAzL0oFNB5387S8srq2nppo7y5tb2z67tN43KNGUNqoTS7YgYJrhkDeAgWDvVjCSRYK1oeD3xWw9MG67kHYxSFiakL3nMKQErdd3DQAUfc4CzfsDIFqrR5yNu27Fq3pT4EXiF6SCCtS77lfQUzRLmAQqiDEd30shzIkGTgUbl4PMsJTQIemzjqWSJMyEfT8MT6xSgHStuSgKfq74mcJMaMksh2JgQGZt6biP95nQziyzDnMs2ASTpbFGcCg8KTLHCPa0ZBjCwhVHN7K6YDogkFm1jZhuDPv7xImmdV36v6teV2lURRwkdoWN0inx0gWroBtVRA1GUo2f0it6cJfFeXcZq1LTjFzgP7AfwB7JSWFwlatexitlatexit sha1_base642suSYs2KtjHJeb1CIts1JrhYPIIAAABnicbVBNS8NAEN34WetXVDx5WSyCp5KIoMeiF48V7Ac0sWy2m3bpZjfsTpQSCv4VLx4U8erv8OacdvmoK0PBh7vzTAzL0oFNB5387S8srq2nppo7y5tb2z67tN43KNGUNqoTS7YgYJrhkDeAgWDvVjCSRYK1oeD3xWw9MG67kHYxSFiakL3nMKQErdd3DQAUfc4CzfsDIFqrR5yNu27Fq3pT4EXiF6SCCtS77lfQUzRLmAQqiDEd30shzIkGTgUbl4PMsJTQIemzjqWSJMyEfT8MT6xSgHStuSgKfq74mcJMaMksh2JgQGZt6biP95nQziyzDnMs2ASTpbFGcCg8KTLHCPa0ZBjCwhVHN7K6YDogkFm1jZhuDPv7xImmdV36v6teV2lURRwkdoWN0inx0gWroBtVRA1GUo2f0it6cJfFeXcZq1LTjFzgP7AfwB7JSWFwlatexitlatexit sha1_base642suSYs2KtjHJeb1CIts1JrhYPIIAAABnicbVBNS8NAEN34WetXVDx5WSyCp5KIoMeiF48V7Ac0sWy2m3bpZjfsTpQSCv4VLx4U8erv8OacdvmoK0PBh7vzTAzL0oFNB5387S8srq2nppo7y5tb2z67tN43KNGUNqoTS7YgYJrhkDeAgWDvVjCSRYK1oeD3xWw9MG67kHYxSFiakL3nMKQErdd3DQAUfc4CzfsDIFqrR5yNu27Fq3pT4EXiF6SCCtS77lfQUzRLmAQqiDEd30shzIkGTgUbl4PMsJTQIemzjqWSJMyEfT8MT6xSgHStuSgKfq74mcJMaMksh2JgQGZt6biP95nQziyzDnMs2ASTpbFGcCg8KTLHCPa0ZBjCwhVHN7K6YDogkFm1jZhuDPv7xImmdV36v6teV2lURRwkdoWN0inx0gWroBtVRA1GUo2f0it6cJfFeXcZq1LTjFzgP7AfwB7JSWFwlatexitlatexit sha1_base642suSYs2KtjHJeb1CIts1JrhYPIIAAABnicbVBNS8NAEN34WetXVDx5WSyCp5KIoMeiF48V7Ac0sWy2m3bpZjfsTpQSCv4VLx4U8erv8OacdvmoK0PBh7vzTAzL0oFNB5387S8srq2nppo7y5tb2z67tN43KNGUNqoTS7YgYJrhkDeAgWDvVjCSRYK1oeD3xWw9MG67kHYxSFiakL3nMKQErdd3DQAUfc4CzfsDIFqrR5yNu27Fq3pT4EXiF6SCCtS77lfQUzRLmAQqiDEd30shzIkGTgUbl4PMsJTQIemzjqWSJMyEfT8MT6xSgHStuSgKfq74mcJMaMksh2JgQGZt6biP95nQziyzDnMs2ASTpbFGcCg8KTLHCPa0ZBjCwhVHN7K6YDogkFm1jZhuDPv7xImmdV36v6teV2lURRwkdoWN0inx0gWroBtVRA1GUo2f0it6cJfFeXcZq1LTjFzgP7AfwB7JSWFwlatexite
latexit sha1_base64gRKFyQFytmwqWy0cvo5FmmPz8IAAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubeaSNXu3x6eEo78BxsLRWz9P3bGzcfhSYGHi8N8PMvDAVXBvXXaWlldW19YLG8XNre2d3dLevq9lphjWmRRSNUOqUfAE64Ybgc1UIY1DgY1wcDX2G4oNJfJrRmmGMS0lCIM2qs5LfTPrDTqnsVtwJyCLxZqRcPfSf7gGg1il9tbuSZTEmhgmqdctzUxPkVBnOBI6K7UxjStmA9rBlaUJj1EEuXZETqzSJZFUthJDJurviZzGWgj0HbG1PT1vDcWNamYkugpwnaWYwYdNFUSaIkWT8OulyhcyIoSWUKW5vJaxPFWXGBlS0IXjzLy8S6ziuRXvxqZxCVMU4AiO4RQ8OIcqXEMN6sDgAZ7hFd4c6bw4787HtHXJmc0cwB84nzQDpCPlatexitlatexit sha1_base6474MJShuZzxGyM2nLY3EY87InuhIAAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS7HzAwPJ49CEw9cOJxzLfeE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2OeQ9Ks0TcmExCEJOYBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75YO72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZmFdEAUocYGVLIhePMvLxLtOq5VeapnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8OaO27S14Mxm9tEfOB8TZOSnAlatexitlatexit sha1_base6474MJShuZzxGyM2nLY3EY87InuhIAAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS7HzAwPJ49CEw9cOJxzLfeE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2OeQ9Ks0TcmExCEJOYBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75YO72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZmFdEAUocYGVLIhePMvLxLtOq5VeapnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8OaO27S14Mxm9tEfOB8TZOSnAlatexitlatexit sha1_base64pLq6KB1S9uyUeWpG4byg43mK0AAAB7XicbVA9SwNBEJ2LXzFRS1tFoNgFe5stLAI2FhGMBQnGFvM5es2ds9dveEEPIfbCwUsfX2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6wtr6xuVXcLu3s7u0flAPmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2HAF75Ypf9ecgqyTISQVy1Hvlr25fsSxBaZmgxnQCP7XhhGrLmcBpqZsZTCkb0QF2HJU0QRNO5tdOyZlTiRW2pW0ZK7npjQxJhxErnOhNqhWfZm4n9eJ7PxVTjhMs0sSrZYFGeCWEVmr5M18isGDtCmebuVsKGVFNmXUAlF0KwPIqaV5UA78a3PmV2nUeRxFO4BTOIYBLqMEt1KEBDB7hGV7hzVPeifufSxaC14cwx4H3AIWDjwwlatexitElatexit sha1_base64iJx8cSgmmYNbMN8WtCvsNrlHUAAAB6HicbVBNS8NAEJ3Ur1qqh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFdkjfjds2B219MPB4b4aZeUEiuDau0U1tY3NreK26Wd3b39gLhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQTiQ8lDzqixUuOuX664VXcOskq8nFQgR71fuoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9RNj90Ss6sMiBhrGxJQbq74mMRlpPosB2RtSM9LI3EzuqkJr2MyyQ1KNliUZgKYmIy5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4AfzB5cfjMMlatexitlatexit sha1_base64iJx8cSgmmYNbMN8WtCvsNrlHUAAAB6HicbVBNS8NAEJ3Ur1qqh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFdkjfjds2B219MPB4b4aZeUEiuDau0U1tY3NreK26Wd3b39gLhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQTiQ8lDzqixUuOuX664VXcOskq8nFQgR71fuoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9RNj90Ss6sMiBhrGxJQbq74mMRlpPosB2RtSM9LI3EzuqkJr2MyyQ1KNliUZgKYmIy5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4AfzB5cfjMMlatexitlatexit sha1_base64iJx8cSgmmYNbMN8WtCvsNrlHUAAAB6HicbVBNS8NAEJ3Ur1qqh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFdkjfjds2B219MPB4b4aZeUEiuDau0U1tY3NreK26Wd3b39gLhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQTiQ8lDzqixUuOuX664VXcOskq8nFQgR71fuoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9RNj90Ss6sMiBhrGxJQbq74mMRlpPosB2RtSM9LI3EzuqkJr2MyyQ1KNliUZgKYmIy5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4AfzB5cfjMMlatexitlatexit sha1_base64iJx8cSgmmYNbMN8WtCvsNrlHUAAAB6HicbVBNS8NAEJ3Ur1qqh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFdkjfjds2B219MPB4b4aZeUEiuDau0U1tY3NreK26Wd3b39gLhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQTiQ8lDzqixUuOuX664VXcOskq8nFQgR71fuoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9RNj90Ss6sMiBhrGxJQbq74mMRlpPosB2RtSM9LI3EzuqkJr2MyyQ1KNliUZgKYmIy5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4AfzB5cfjMMlatexitVlatexit sha1_base64xc4uzoZiBSxZUZkArgltxczS6nMAAAB6HicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEdgY2FIrbJDvjZvkCk18MPB4b4aZeUEiuDau0UNja3tneKu6W9YPDoLxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QTiI8lDzqixUrM9KFfcqrsAWSdeTiqQozEofWHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWoWxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1GzOIpwBudwCR7UoA730IAWMEB4hld4cx6dFfdVi2Fpx85hTwPn8AbDjjNQlatexitlatexit sha1_base64xc4uzoZiBSxZUZkArgltxczS6nMAAAB6HicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEdgY2FIrbJDvjZvkCk18MPB4b4aZeUEiuDau0UNja3tneKu6W9YPDoLxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QTiI8lDzqixUrM9KFfcqrsAWSdeTiqQozEofWHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWoWxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1GzOIpwBudwCR7UoA730IAWMEB4hld4cx6dFfdVi2Fpx85hTwPn8AbDjjNQlatexitlatexit sha1_base64xc4uzoZiBSxZUZkArgltxczS6nMAAAB6HicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEdgY2FIrbJDvjZvkCk18MPB4b4aZeUEiuDau0UNja3tneKu6W9YPDoLxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QTiI8lDzqixUrM9KFfcqrsAWSdeTiqQozEofWHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWoWxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1GzOIpwBudwCR7UoA730IAWMEB4hld4cx6dFfdVi2Fpx85hTwPn8AbDjjNQlatexitlatexit sha1_base64xc4uzoZiBSxZUZkArgltxczS6nMAAAB6HicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEdgY2FIrbJDvjZvkCk18MPB4b4aZeUEiuDau0UNja3tneKu6W9YPDoLxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QTiI8lDzqixUrM9KFfcqrsAWSdeTiqQozEofWHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWoWxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1GzOIpwBudwCR7UoA730IAWMEB4hld4cx6dFfdVi2Fpx85hTwPn8AbDjjNQlatexit
e Relation network
Edge blockNode blockGlobal blocku0
latexit sha1_base64Zn1gIms2ONBt0R58c8NGdBbqUAAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7dUD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9WvjBmacQVMkmN6Xlugn5GNQomazSTw1PKJvQEe9ZqmjEjZNI8ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvEr5dieONnQiUpcsUWH4WpJBiTH4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMzAKbw46L86787EYLTnFzjH8gfP5A1s4kUQlatexitlatexit sha1_base64Zn1gIms2ONBt0R58c8NGdBbqUAAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7dUD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9WvjBmacQVMkmN6Xlugn5GNQomazSTw1PKJvQEe9ZqmjEjZNI8ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvEr5dieONnQiUpcsUWH4WpJBiTH4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMzAKbw46L86787EYLTnFzjH8gfP5A1s4kUQlatexitlatexit sha1_base64Zn1gIms2ONBt0R58c8NGdBbqUAAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7dUD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9WvjBmacQVMkmN6Xlugn5GNQomazSTw1PKJvQEe9ZqmjEjZNI8ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvEr5dieONnQiUpcsUWH4WpJBiTH4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMzAKbw46L86787EYLTnFzjH8gfP5A1s4kUQlatexitlatexit sha1_base64Zn1gIms2ONBt0R58c8NGdBbqUAAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7dUD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9WvjBmacQVMkmN6Xlugn5GNQomazSTw1PKJvQEe9ZqmjEjZNI8ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvEr5dieONnQiUpcsUWH4WpJBiTH4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMzAKbw46L86787EYLTnFzjH8gfP5A1s4kUQlatexitu
latexit sha1_base64znt8hwWv6wryqwCugrweUajkM8AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubfaSNXu7y6eEo78BxsLRWz9P3bGzcfhSYGHi8N8PMvFhxZqzvf3tLyyurauFjeLm1vbObmlvPzQy04TWieRSN2NsKGeC1i2znDaVpjiNOW3Eg6ux33ik2jApbu1Q0SjFPcESRrB1UthWfXaXdUplvJPgBZJMCPl6mH4dA8AtU7pq92VJEupsIRjY1qBr2yUY20Z4XRUbGeGKkwGuEdbjgqcUhPlk2tH6MQpXZRI7UpYNFFTQ4NWaYxq4zxbZv5r2xJXymxyEeVMqMxSQaaLkowjK9H4ddRlmhLLh45gopm7FZE1phYF1DRhRDMv7xIwrNK4FeCG5fGJUxRgCM4hlMI4ByqcA01qAOBB3iGV3jzpPfivXsf09YlbzZzAHgff4AqE6Qnwlatexitlatexit sha1_base64Nc0DXje6uYB0fHlXL99yCq0noAAAB7XicbVC7SgNBFL0bXzGooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS7HzAwPJ49CEw9cOJxzLfeE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYyeIAbtNuWKW3UnQIvEm5FK7cBkNn3e71buz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQHXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2YccD5AGXTkqwlatexitlatexit sha1_base64Nc0DXje6uYB0fHlXL99yCq0noAAAB7XicbVC7SgNBFL0bXzGooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS7HzAwPJ49CEw9cOJxzLfeE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYyeIAbtNuWKW3UnQIvEm5FK7cBkNn3e71buz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQHXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2YccD5AGXTkqwlatexitlatexit sha1_base64S5XnA5iYIAgqxiIi0ptSwAiKP4AAAB7XicbVA9SwNBEJ2LXzFRS1tFoNgFe5stLAI2FhGMBQnGFvM5es2ds9dveEEPIfbCwUsfX2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6wtr6xuVXcLu3s7u0flAPmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2HCHrlSt1ZDrJIgJxXIUeVv7p9xbIEpWWCGtMJNSGE6otZwKnpW5mMKVsRAfYcVTSBE04mV87JWdO6ZNYaVfSkrn6e2JCE2PGSeQ6E2qHZtmbif95nczGVGEyzSzKNliUZwJYhWZvU76XCOzYuwIZZq7WwkbUk2ZdQGVXAjB8surpHlRDfxqcOdXatd5HEU4gVM4hwAuoQa3UIcGMHiEZ3iFN095L96797FoLXj5zDH8gff5A53Djxwlatexitvu
latexit sha1_base648QVocR3pGD0iQLG1OhPZDl9fEAAABnicbVBNS8NAEN3Ur1qouLJy2IRPJVEBD0WvXisYDgiWWz3TRLN9mwO6mUUPCvePGgiFdhzfjds2B219MPB4b4aZeUEquAbHbZKK6tr6xvlzcrW9s7unr10NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs88lQkHKRpggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zopDlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJXjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bHPWklXMHKIsD5AAdVliglatexitlatexit sha1_base648QVocR3pGD0iQLG1OhPZDl9fEAAABnicbVBNS8NAEN3Ur1qouLJy2IRPJVEBD0WvXisYDgiWWz3TRLN9mwO6mUUPCvePGgiFdhzfjds2B219MPB4b4aZeUEquAbHbZKK6tr6xvlzcrW9s7unr10NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs88lQkHKRpggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zopDlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJXjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bHPWklXMHKIsD5AAdVliglatexitlatexit sha1_base648QVocR3pGD0iQLG1OhPZDl9fEAAABnicbVBNS8NAEN3Ur1qouLJy2IRPJVEBD0WvXisYDgiWWz3TRLN9mwO6mUUPCvePGgiFdhzfjds2B219MPB4b4aZeUEquAbHbZKK6tr6xvlzcrW9s7unr10NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs88lQkHKRpggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zopDlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJXjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bHPWklXMHKIsD5AAdVliglatexitlatexit sha1_base648QVocR3pGD0iQLG1OhPZDl9fEAAABnicbVBNS8NAEN3Ur1qouLJy2IRPJVEBD0WvXisYDgiWWz3TRLN9mwO6mUUPCvePGgiFdhzfjds2B219MPB4b4aZeUEquAbHbZKK6tr6xvlzcrW9s7unr10NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs88lQkHKRpggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zopDlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJXjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bHPWklXMHKIsD5AAdVliglatexit
latexit sha1_base64m8MJ1M94ujO0d0COo5n2Dsol6rcAAAB53icbVC7SgNBFL0bXzGopY2g0GwCrs2phAM2FhGMA9IFpmdvZsMmZ1dZmaFsKS0sbFQxNZP8RfsAZwsmj0MQDFw7nnMt9BKng2rjul1NYWV1b3yhulra2d3b3yvsHLZ1kimGTJSJRnYBqFFxi03AjsJMqpHEgsB0MryZx6V5om8NaMUZj2JY84o8ZKjbtyxa26U5Bl4s1J5fLj4EAgM19sKEZTFKwwTVuuu5qfFzqgxnAselXqYxpWxI9i1VNIYtZ9P9xyTE6uEJEqULWnIVP3dkdNY61Ec2GRMzUAvehPxP6bmajm51ymmUHJZoOiTBCTkMnRJOQKmREjSyhT3O5K2IAqyox9TckwVs8eZm0zqqeWVu3Er9AmYowhEcwyl4cA51uIYGNIFBCIwDC8Od56cVdtFi04855DAPnQdsI8Alatexitlatexit sha1_base64q1zM5ZsCNMJAtNUZI97B98ATlaAAAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo1Si4xJrhRmAzUUgjX2DD71OMYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL967g7fqt81tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhYV3R0YjrQeRb5MRNT09643E7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1TtE7zZkdJaTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvkiCM3Zgz9wXn8AGkeQ8wlatexitlatexit sha1_base64q1zM5ZsCNMJAtNUZI97B98ATlaAAAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo1Si4xJrhRmAzUUgjX2DD71OMYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL967g7fqt81tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhYV3R0YjrQeRb5MRNT09643E7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1TtE7zZkdJaTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvkiCM3Zgz9wXn8AGkeQ8wlatexitlatexit sha1_base64ioxb3woZF1oAlTScqds23PrgiMYAAAB53icbVC7SgNBFL3rM8ZX1NJmMAhWYdZGC4uAjWUE84BkkdnZ2WTI7Owyc1cIS37AxkIRW3Jzr9xkmyhiQcGDuecy9x7wkxJi5Re2vrG5tb25Wd6u7esFh7ei4Y9PccNHmqUpNL2RWKKlFGyUq0cuMYEmoRDcc38787pMwVqb6ASeZCBI21DKWnKGTWo1Om3QOcgq8UtShxIuzWIUp4nQiNXzNqTzMMCmZQciWm1UFuRcb4mA1F31HNEmGDYr7nlJw7JSJxatzTSObq74mCJdZOktAlE4YjuzNxP8fo7xdVBIneUoNF98FOeKYEpmR5NIGsFRTRxh3Ei3KEjZhhHV03VleAvn7xKOpcNnzb8e1pv3pR1VOAUzuACfLiCJtxBC9rAIYJneIU3T3ov3rv3sYiueeXMCfyB9kDCGmMcAlatexitv
latexit sha1_base64HCiXjOq04H3f4Ed7vqyiRfd2dIAAAB7XicbVA9SwNBEJ2LXzFnQo2NotBsAp3NlpYBGwsI5hLIDnj3mYvWbO3ezuRcKR2BjoYit8fOfPmo9DEBwOP92aYmRelnGnjed9OYWV1bX2juFna2t7Z3XP3DwItM0VonUguVTPCmnImaN0ww2kzVRQnEaeNaHA98RtDqjST4s6MUhomuCdYzAg2VgraaZdDztu2at4U6Bl4s9JuXoUPD0AQK3jfrW7kmQJFYZwrHXL91IT5lgZRjgdl9qZpikmA9yjLUsFTqgO8m1Y3RqlS6KpbIlDJqqvydynGg9SiLbmWDT14veRPzPa2UmvgxzJtLMUEFmiKMIyPR5HXUZYoSw0eWYKKYvRWRPlaYGBtQyYbgL768TILziu9VFubxhXMUIRjOIEz8OECqnADNagDgUd4hld4c6Tz4rw7H7PWgjOfOYQcD5AKnSkKAlatexitlatexit sha1_base64voSHBGyFE5xYPVKPYMGTIarLQAAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0fz8APDyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQWNZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX21zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcqo1N4xKmKMARHMMpHAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYAcD5AGdXkq0latexitlatexit sha1_base64voSHBGyFE5xYPVKPYMGTIarLQAAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0fz8APDyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQWNZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX21zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcqo1N4xKmKMARHMMpHAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYAcD5AGdXkq0latexitlatexit sha1_base64Fc8T4ygtia14k1zCDji4ezWDqYAAAB7XicbVA9SwNBEJ3zM8avqKXNYhCswp2NFhYBG8sI5gOSMxtNsmavd1jdy4QjvwHGwtFbP0dv4bN8kVmvhg4PHeDDPzokQKi777a2tb2xubRd2irt7weHpaPjhtWpYbzOtNSmFVHLpVC8jgIlbyWG0ziSvBmNbmdc8yNFVo94CThYUwHSvQFoikRicZisdxt1T2K4cZJUEOSlDjlq39NXpaZbGXCGT1Np24CcYZtSgYJJPi53U8oSyER3wtqOKxtyG2fzaKTl3So0tXGlkMzV3xMZja2dxJHrjCkO7bI3Ez2in2r8NMqCRFrthiUTVBDWZvU56wnCGcuIIZUa4WwkbUkMZuoCKLoRgeVV0risBH4luPfL1Zs8jgKcwhlcQABXUIU7qEEdGDzBM7zCm6e9Fd1i0rnn5zAn8gff5A59Hjx0latexitVlatexit sha1_base64xc4uzoZiBSxZUZkArgltxczS6nMAAAB6HicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEdgY2FIrbJDvjZvkCk18MPB4b4aZeUEiuDau0UNja3tneKu6W9YPDoLxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QTiI8lDzqixUrM9KFfcqrsAWSdeTiqQozEofWHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWoWxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1GzOIpwBudwCR7UoA730IAWMEB4hld4cx6dFfdVi2Fpx85hTwPn8AbDjjNQlatexitlatexit sha1_base64xc4uzoZiBSxZUZkArgltxczS6nMAAAB6HicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEdgY2FIrbJDvjZvkCk18MPB4b4aZeUEiuDau0UNja3tneKu6W9YPDoLxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QTiI8lDzqixUrM9KFfcqrsAWSdeTiqQozEofWHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWoWxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1GzOIpwBudwCR7UoA730IAWMEB4hld4cx6dFfdVi2Fpx85hTwPn8AbDjjNQlatexitlatexit sha1_base64xc4uzoZiBSxZUZkArgltxczS6nMAAAB6HicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEdgY2FIrbJDvjZvkCk18MPB4b4aZeUEiuDau0UNja3tneKu6W9YPDoLxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QTiI8lDzqixUrM9KFfcqrsAWSdeTiqQozEofWHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWoWxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1GzOIpwBudwCR7UoA730IAWMEB4hld4cx6dFfdVi2Fpx85hTwPn8AbDjjNQlatexitlatexit sha1_base64xc4uzoZiBSxZUZkArgltxczS6nMAAAB6HicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEdgY2FIrbJDvjZvkCk18MPB4b4aZeUEiuDau0UNja3tneKu6W9YPDoLxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QTiI8lDzqixUrM9KFfcqrsAWSdeTiqQozEofWHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWoWxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1GzOIpwBudwCR7UoA730IAWMEB4hld4cx6dFfdVi2Fpx85hTwPn8AbDjjNQlatexitulatexit sha1_base640coyYP26hzTYQyod27M3N3DnUAAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6MJFwY3LCvaBbSiT6aQdOpmEmRuhhP6FGxeKuPVv3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7dUD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJre533ni2ohYPeA04X5ER0qEglG00mMojgOwiydDao1t7OQVaJV5AaFGgOql9YczSiCtkkhrT89wEYxqFEzyWaWfGp5QNqEj3rNU0YgbP5snnpEzqwxJGGv7FJK5nsjo5Ex0yiwk3lCszl4n9eL8Xw2sESlLkii0ClNJMCb5WQoNGcop5ZQpoXNStiYasrQllSxJXjLJ6S9kXdcveWWtcVPUUYYTOIVz8OAKGnAHTWgBAwXP8ApvjnFenHfnYzFacoqdYgD5MH9tyREwlatexitlatexit sha1_base640coyYP26hzTYQyod27M3N3DnUAAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6MJFwY3LCvaBbSiT6aQdOpmEmRuhhP6FGxeKuPVv3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7dUD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJre533ni2ohYPeA04X5ER0qEglG00mMojgOwiydDao1t7OQVaJV5AaFGgOql9YczSiCtkkhrT89wEYxqFEzyWaWfGp5QNqEj3rNU0YgbP5snnpEzqwxJGGv7FJK5nsjo5Ex0yiwk3lCszl4n9eL8Xw2sESlLkii0ClNJMCb5WQoNGcop5ZQpoXNStiYasrQllSxJXjLJ6S9kXdcveWWtcVPUUYYTOIVz8OAKGnAHTWgBAwXP8ApvjnFenHfnYzFacoqdYgD5MH9tyREwlatexitlatexit sha1_base640coyYP26hzTYQyod27M3N3DnUAAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6MJFwY3LCvaBbSiT6aQdOpmEmRuhhP6FGxeKuPVv3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7dUD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJre533ni2ohYPeA04X5ER0qEglG00mMojgOwiydDao1t7OQVaJV5AaFGgOql9YczSiCtkkhrT89wEYxqFEzyWaWfGp5QNqEj3rNU0YgbP5snnpEzqwxJGGv7FJK5nsjo5Ex0yiwk3lCszl4n9eL8Xw2sESlLkii0ClNJMCb5WQoNGcop5ZQpoXNStiYasrQllSxJXjLJ6S9kXdcveWWtcVPUUYYTOIVz8OAKGnAHTWgBAwXP8ApvjnFenHfnYzFacoqdYgD5MH9tyREwlatexitlatexit sha1_base640coyYP26hzTYQyod27M3N3DnUAAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6MJFwY3LCvaBbSiT6aQdOpmEmRuhhP6FGxeKuPVv3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7dUD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJre533ni2ohYPeA04X5ER0qEglG00mMojgOwiydDao1t7OQVaJV5AaFGgOql9YczSiCtkkhrT89wEYxqFEzyWaWfGp5QNqEj3rNU0YgbP5snnpEzqwxJGGv7FJK5nsjo5Ex0yiwk3lCszl4n9eL8Xw2sESlLkii0ClNJMCb5WQoNGcop5ZQpoXNStiYasrQllSxJXjLJ6S9kXdcveWWtcVPUUYYTOIVz8OAKGnAHTWgBAwXP8ApvjnFenHfnYzFacoqdYgD5MH9tyREwlatexit f Deep set
Figure 4 Dierent internal GN block congurations See Section 32 for details on the notation
and Section 4 for details about each variant a A full GN predicts node edge and global output
attributes based on incoming node edge and global attributes b An independent recurrent
update block takes input and hidden graphs and the functions are RNNs SanchezGonzalez
et al 2018 c An MPNN Gilmer et al 2017 predicts node edge and global output attributes
based on incoming node edge and global attributes Note that the global prediction does not
include aggregated edges d A NLNN Wang et al 2018c only predicts node output attributes
e A relation network Raposo et al 2017 Santoro et al 2017 only uses the edge predictions
to predict global attributes f A Deep Set Zaheer et al 2017 bypasses the edge update and
predicts updated global attributes
421 Messagepassing neural network MPNN
Gilmer et al 2017s MPNN generalizes a number of previous architectures and can be translated
naturally into the GN formalism Following the MPNN papers terminology see Gilmer et al
2017 pages 24
the message function Mt plays the role of the GNs e but does not take uas input
elementwise summation is used for the GNs ev
the update function Ut plays the role of the GNs v
16Vlatexit sha1_base64xc4uzoZiBSxZUZkArgltxczS6nMAAAB6HicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEdgY2FIrbJDvjZvkCk18MPB4b4aZeUEiuDau0UNja3tneKu6W9YPDoLxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QTiI8lDzqixUrM9KFfcqrsAWSdeTiqQozEofWHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWoWxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1GzOIpwBudwCR7UoA730IAWMEB4hld4cx6dFfdVi2Fpx85hTwPn8AbDjjNQlatexitlatexit sha1_base64xc4uzoZiBSxZUZkArgltxczS6nMAAAB6HicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEdgY2FIrbJDvjZvkCk18MPB4b4aZeUEiuDau0UNja3tneKu6W9YPDoLxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QTiI8lDzqixUrM9KFfcqrsAWSdeTiqQozEofWHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWoWxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1GzOIpwBudwCR7UoA730IAWMEB4hld4cx6dFfdVi2Fpx85hTwPn8AbDjjNQlatexitlatexit sha1_base64xc4uzoZiBSxZUZkArgltxczS6nMAAAB6HicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEdgY2FIrbJDvjZvkCk18MPB4b4aZeUEiuDau0UNja3tneKu6W9YPDoLxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QTiI8lDzqixUrM9KFfcqrsAWSdeTiqQozEofWHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWoWxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1GzOIpwBudwCR7UoA730IAWMEB4hld4cx6dFfdVi2Fpx85hTwPn8AbDjjNQlatexitlatexit sha1_base64xc4uzoZiBSxZUZkArgltxczS6nMAAAB6HicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEdgY2FIrbJDvjZvkCk18MPB4b4aZeUEiuDau0UNja3tneKu6W9YPDoLxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QTiI8lDzqixUrM9KFfcqrsAWSdeTiqQozEofWHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWoWxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1GzOIpwBudwCR7UoA730IAWMEB4hld4cx6dFfdVi2Fpx85hTwPn8AbDjjNQlatexite
latexit sha1_base64mjs48M94BmT64MLe6tKpQUPkNJMAAAB73icbVA9SwNBEJ3zM8avqKXNYRCswp0IWlgEbCwjmA9IzjC3mUuW7O2du3tCCPkTNhaK2Pp37Pw3bpIrNPHBwOO9GWbmhang2njet7OyurasVnYKm7v7O7tlw4OGzrJFKM6S0SiWiFqElxS3XAjqJUqwjgU1AyHN1OURK80Tem1FKQYx9ySPO0Fip1UGRDvCBuqWyVFmcJeJn5My5Kh1S1dXsKymKRhArVu15qgjEqw5mgSbGTaUqRDbFPbUslxqSD8ezeiXtqlZ4bJcqWNO5MT0xxljrURzazhjNQC96UEr52Z6CoYc5lmhiSbL4oy4ZrEnT7v9rgiZsTIEmSK21tdNkCFzNiIijYEfHlZdI4rhexb7KFev8zgKcAwncAYXEIVbqEGdWAg4Ble4c15dF6cddj3rri5DNH8AfO5w8AXYplatexitlatexit sha1_base64mjs48M94BmT64MLe6tKpQUPkNJMAAAB73icbVA9SwNBEJ3zM8avqKXNYRCswp0IWlgEbCwjmA9IzjC3mUuW7O2du3tCCPkTNhaK2Pp37Pw3bpIrNPHBwOO9GWbmhang2njet7OyurasVnYKm7v7O7tlw4OGzrJFKM6S0SiWiFqElxS3XAjqJUqwjgU1AyHN1OURK80Tem1FKQYx9ySPO0Fip1UGRDvCBuqWyVFmcJeJn5My5Kh1S1dXsKymKRhArVu15qgjEqw5mgSbGTaUqRDbFPbUslxqSD8ezeiXtqlZ4bJcqWNO5MT0xxljrURzazhjNQC96UEr52Z6CoYc5lmhiSbL4oy4ZrEnT7v9rgiZsTIEmSK21tdNkCFzNiIijYEfHlZdI4rhexb7KFev8zgKcAwncAYXEIVbqEGdWAg4Ble4c15dF6cddj3rri5DNH8AfO5w8AXYplatexitlatexit sha1_base64mjs48M94BmT64MLe6tKpQUPkNJMAAAB73icbVA9SwNBEJ3zM8avqKXNYRCswp0IWlgEbCwjmA9IzjC3mUuW7O2du3tCCPkTNhaK2Pp37Pw3bpIrNPHBwOO9GWbmhang2njet7OyurasVnYKm7v7O7tlw4OGzrJFKM6S0SiWiFqElxS3XAjqJUqwjgU1AyHN1OURK80Tem1FKQYx9ySPO0Fip1UGRDvCBuqWyVFmcJeJn5My5Kh1S1dXsKymKRhArVu15qgjEqw5mgSbGTaUqRDbFPbUslxqSD8ezeiXtqlZ4bJcqWNO5MT0xxljrURzazhjNQC96UEr52Z6CoYc5lmhiSbL4oy4ZrEnT7v9rgiZsTIEmSK21tdNkCFzNiIijYEfHlZdI4rhexb7KFev8zgKcAwncAYXEIVbqEGdWAg4Ble4c15dF6cddj3rri5DNH8AfO5w8AXYplatexitlatexit sha1_base64mjs48M94BmT64MLe6tKpQUPkNJMAAAB73icbVA9SwNBEJ3zM8avqKXNYRCswp0IWlgEbCwjmA9IzjC3mUuW7O2du3tCCPkTNhaK2Pp37Pw3bpIrNPHBwOO9GWbmhang2njet7OyurasVnYKm7v7O7tlw4OGzrJFKM6S0SiWiFqElxS3XAjqJUqwjgU1AyHN1OURK80Tem1FKQYx9ySPO0Fip1UGRDvCBuqWyVFmcJeJn5My5Kh1S1dXsKymKRhArVu15qgjEqw5mgSbGTaUqRDbFPbUslxqSD8ezeiXtqlZ4bJcqWNO5MT0xxljrURzazhjNQC96UEr52Z6CoYc5lmhiSbL4oy4ZrEnT7v9rgiZsTIEmSK21tdNkCFzNiIijYEfHlZdI4rhexb7KFev8zgKcAwncAYXEIVbqEGdWAg4Ble4c15dF6cddj3rri5DNH8AfO5w8AXYplatexite
latexit sha1_base64EsSika7GeuHZg021QVL5D37TgAAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoAcPAS8eI5gHJGuYnfQmQ2YfzPQKIeQjvHhQxKvf482cZLsQRMLGoqqbrq7glRJQ6777RTW1jc2t4rbpZ3dvf2D8uFR0ySZFtgQiUp0OAGlYyxQZIUtlONPAoUtoLR7cxvPaE2MokfaJyiHFBLEMpOFmp1Q2QCP2yhW36s7BVomXkwrkqPfKX91IrIIYxKKG9Px3JT8CdckhcJpqZsZTLkY8QF2LI15hMafzMdsjOr9FmYaFsxsbn6e2LCI2PGUWA7I05DszNxP8TkbhtTRcZoRxmKxKMwUo4TNfmd9qVGQGlvChZb2ViaGXHNBNqGSDcFbfnmVNCqnlv17i8rtZs8jiKcwCmcgwdXUIM7qEMDBIzgGV7hzUmdFfdVi0Fpx85hjwPn8ATfaj3Ulatexitlatexit sha1_base64EsSika7GeuHZg021QVL5D37TgAAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoAcPAS8eI5gHJGuYnfQmQ2YfzPQKIeQjvHhQxKvf482cZLsQRMLGoqqbrq7glRJQ6777RTW1jc2t4rbpZ3dvf2D8uFR0ySZFtgQiUp0OAGlYyxQZIUtlONPAoUtoLR7cxvPaE2MokfaJyiHFBLEMpOFmp1Q2QCP2yhW36s7BVomXkwrkqPfKX91IrIIYxKKG9Px3JT8CdckhcJpqZsZTLkY8QF2LI15hMafzMdsjOr9FmYaFsxsbn6e2LCI2PGUWA7I05DszNxP8TkbhtTRcZoRxmKxKMwUo4TNfmd9qVGQGlvChZb2ViaGXHNBNqGSDcFbfnmVNCqnlv17i8rtZs8jiKcwCmcgwdXUIM7qEMDBIzgGV7hzUmdFfdVi0Fpx85hjwPn8ATfaj3Ulatexitlatexit sha1_base64EsSika7GeuHZg021QVL5D37TgAAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoAcPAS8eI5gHJGuYnfQmQ2YfzPQKIeQjvHhQxKvf482cZLsQRMLGoqqbrq7glRJQ6777RTW1jc2t4rbpZ3dvf2D8uFR0ySZFtgQiUp0OAGlYyxQZIUtlONPAoUtoLR7cxvPaE2MokfaJyiHFBLEMpOFmp1Q2QCP2yhW36s7BVomXkwrkqPfKX91IrIIYxKKG9Px3JT8CdckhcJpqZsZTLkY8QF2LI15hMafzMdsjOr9FmYaFsxsbn6e2LCI2PGUWA7I05DszNxP8TkbhtTRcZoRxmKxKMwUo4TNfmd9qVGQGlvChZb2ViaGXHNBNqGSDcFbfnmVNCqnlv17i8rtZs8jiKcwCmcgwdXUIM7qEMDBIzgGV7hzUmdFfdVi0Fpx85hjwPn8ATfaj3Ulatexitlatexit sha1_base64EsSika7GeuHZg021QVL5D37TgAAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoAcPAS8eI5gHJGuYnfQmQ2YfzPQKIeQjvHhQxKvf482cZLsQRMLGoqqbrq7glRJQ6777RTW1jc2t4rbpZ3dvf2D8uFR0ySZFtgQiUp0OAGlYyxQZIUtlONPAoUtoLR7cxvPaE2MokfaJyiHFBLEMpOFmp1Q2QCP2yhW36s7BVomXkwrkqPfKX91IrIIYxKKG9Px3JT8CdckhcJpqZsZTLkY8QF2LI15hMafzMdsjOr9FmYaFsxsbn6e2LCI2PGUWA7I05DszNxP8TkbhtTRcZoRxmKxKMwUo4TNfmd9qVGQGlvChZb2ViaGXHNBNqGSDcFbfnmVNCqnlv17i8rtZs8jiKcwCmcgwdXUIM7qEMDBIzgGV7hzUmdFfdVi0Fpx85hjwPn8ATfaj3Ulatexitev
latexit sha1_base64s3CwiDIc9TAit26LWmPV1hK0AAABnicbVBNS8NAEN3Ur1qouLJy2IRPJVEBD0WvXisYDgiWWznTRLN9mwu6mUUPCvePGgiFdhzfjds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXvoKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5ez8CT41ShHQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGxgEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ87hmWGAlatexitlatexit sha1_base64s3CwiDIc9TAit26LWmPV1hK0AAABnicbVBNS8NAEN3Ur1qouLJy2IRPJVEBD0WvXisYDgiWWznTRLN9mwu6mUUPCvePGgiFdhzfjds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXvoKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5ez8CT41ShHQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGxgEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ87hmWGAlatexitlatexit sha1_base64s3CwiDIc9TAit26LWmPV1hK0AAABnicbVBNS8NAEN3Ur1qouLJy2IRPJVEBD0WvXisYDgiWWznTRLN9mwu6mUUPCvePGgiFdhzfjds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXvoKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5ez8CT41ShHQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGxgEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ87hmWGAlatexitlatexit sha1_base64s3CwiDIc9TAit26LWmPV1hK0AAABnicbVBNS8NAEN3Ur1qouLJy2IRPJVEBD0WvXisYDgiWWznTRLN9mwu6mUUPCvePGgiFdhzfjds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXvoKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5ez8CT41ShHQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGxgEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ87hmWGAlatexite
latexit sha1_base64gRKFyQFytmwqWy0cvo5FmmPz8IAAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubeaSNXu3x6eEo78BxsLRWz9P3bGzcfhSYGHi8N8PMvDAVXBvXXaWlldW19YLG8XNre2d3dLevq9lphjWmRRSNUOqUfAE64Ybgc1UIY1DgY1wcDX2G4oNJfJrRmmGMS0lCIM2qs5LfTPrDTqnsVtwJyCLxZqRcPfSf7gGg1il9tbuSZTEmhgmqdctzUxPkVBnOBI6K7UxjStmA9rBlaUJj1EEuXZETqzSJZFUthJDJurviZzGWgj0HbG1PT1vDcWNamYkugpwnaWYwYdNFUSaIkWT8OulyhcyIoSWUKW5vJaxPFWXGBlS0IXjzLy8S6ziuRXvxqZxCVMU4AiO4RQ8OIcqXEMN6sDgAZ7hFd4c6bw4787HtHXJmc0cwB84nzQDpCPlatexitlatexit sha1_base6474MJShuZzxGyM2nLY3EY87InuhIAAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS7HzAwPJ49CEw9cOJxzLfeE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2OeQ9Ks0TcmExCEJOYBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75YO72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZmFdEAUocYGVLIhePMvLxLtOq5VeapnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8OaO27S14Mxm9tEfOB8TZOSnAlatexitlatexit sha1_base6474MJShuZzxGyM2nLY3EY87InuhIAAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS7HzAwPJ49CEw9cOJxzLfeE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2OeQ9Ks0TcmExCEJOYBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75YO72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZmFdEAUocYGVLIhePMvLxLtOq5VeapnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8OaO27S14Mxm9tEfOB8TZOSnAlatexitlatexit sha1_base64pLq6KB1S9uyUeWpG4byg43mK0AAAB7XicbVA9SwNBEJ2LXzFRS1tFoNgFe5stLAI2FhGMBQnGFvM5es2ds9dveEEPIfbCwUsfX2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6wtr6xuVXcLu3s7u0flAPmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2HAF75Ypf9ecgqyTISQVy1Hvlr25fsSxBaZmgxnQCP7XhhGrLmcBpqZsZTCkb0QF2HJU0QRNO5tdOyZlTiRW2pW0ZK7npjQxJhxErnOhNqhWfZm4n9eJ7PxVTjhMs0sSrZYFGeCWEVmr5M18isGDtCmebuVsKGVFNmXUAlF0KwPIqaV5UA78a3PmV2nUeRxFO4BTOIYBLqMEt1KEBDB7hGV7hzVPeifufSxaC14cwx4H3AIWDjwwlatexitlatexit sha1_base64m8MJ1M94ujO0d0COo5n2Dsol6rcAAAB53icbVC7SgNBFL0bXzGopY2g0GwCrs2phAM2FhGMA9IFpmdvZsMmZ1dZmaFsKS0sbFQxNZP8RfsAZwsmj0MQDFw7nnMt9BKng2rjul1NYWV1b3yhulra2d3b3yvsHLZ1kimGTJSJRnYBqFFxi03AjsJMqpHEgsB0MryZx6V5om8NaMUZj2JY84o8ZKjbtyxa26U5Bl4s1J5fLj4EAgM19sKEZTFKwwTVuuu5qfFzqgxnAselXqYxpWxI9i1VNIYtZ9P9xyTE6uEJEqULWnIVP3dkdNY61Ec2GRMzUAvehPxP6bmajm51ymmUHJZoOiTBCTkMnRJOQKmREjSyhT3O5K2IAqyox9TckwVs8eZm0zqqeWVu3Er9AmYowhEcwyl4cA51uIYGNIFBCIwDC8Od56cVdtFi04855DAPnQdsI8Alatexitlatexit sha1_base64q1zM5ZsCNMJAtNUZI97B98ATlaAAAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo1Si4xJrhRmAzUUgjX2DD71OMYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL967g7fqt81tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhYV3R0YjrQeRb5MRNT09643E7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1TtE7zZkdJaTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvkiCM3Zgz9wXn8AGkeQ8wlatexitlatexit sha1_base64q1zM5ZsCNMJAtNUZI97B98ATlaAAAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo1Si4xJrhRmAzUUgjX2DD71OMYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL967g7fqt81tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhYV3R0YjrQeRb5MRNT09643E7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1TtE7zZkdJaTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvkiCM3Zgz9wXn8AGkeQ8wlatexitlatexit sha1_base64ioxb3woZF1oAlTScqds23PrgiMYAAAB53icbVC7SgNBFL3rM8ZX1NJmMAhWYdZGC4uAjWUE84BkkdnZ2WTI7Owyc1cIS37AxkIRW3Jzr9xkmyhiQcGDuecy9x7wkxJi5Re2vrG5tb25Wd6u7esFh7ei4Y9PccNHmqUpNL2RWKKlFGyUq0cuMYEmoRDcc38787pMwVqb6ASeZCBI21DKWnKGTWo1Om3QOcgq8UtShxIuzWIUp4nQiNXzNqTzMMCmZQciWm1UFuRcb4mA1F31HNEmGDYr7nlJw7JSJxatzTSObq74mCJdZOktAlE4YjuzNxP8fo7xdVBIneUoNF98FOeKYEpmR5NIGsFRTRxh3Ei3KEjZhhHV03VleAvn7xKOpcNnzb8e1pv3pR1VOAUzuACfLiCJtxBC9rAIYJneIU3T3ov3rv3sYiueeXMCfyB9kDCGmMcAlatexitv
latexit sha1_base64HCiXjOq04H3f4Ed7vqyiRfd2dIAAAB7XicbVA9SwNBEJ2LXzFnQo2NotBsAp3NlpYBGwsI5hLIDnj3mYvWbO3ezuRcKR2BjoYit8fOfPmo9DEBwOP92aYmRelnGnjed9OYWV1bX2juFna2t7Z3XP3DwItM0VonUguVTPCmnImaN0ww2kzVRQnEaeNaHA98RtDqjST4s6MUhomuCdYzAg2VgraaZdDztu2at4U6Bl4s9JuXoUPD0AQK3jfrW7kmQJFYZwrHXL91IT5lgZRjgdl9qZpikmA9yjLUsFTqgO8m1Y3RqlS6KpbIlDJqqvydynGg9SiLbmWDT14veRPzPa2UmvgxzJtLMUEFmiKMIyPR5HXUZYoSw0eWYKKYvRWRPlaYGBtQyYbgL768TILziu9VFubxhXMUIRjOIEz8OECqnADNagDgUd4hld4c6Tz4rw7H7PWgjOfOYQcD5AKnSkKAlatexitlatexit sha1_base64voSHBGyFE5xYPVKPYMGTIarLQAAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0fz8APDyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQWNZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX21zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcqo1N4xKmKMARHMMpHAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYAcD5AGdXkq0latexitlatexit sha1_base64voSHBGyFE5xYPVKPYMGTIarLQAAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0fz8APDyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQWNZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX21zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcqo1N4xKmKMARHMMpHAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYAcD5AGdXkq0latexitlatexit sha1_base64Fc8T4ygtia14k1zCDji4ezWDqYAAAB7XicbVA9SwNBEJ3zM8avqKXNYhCswp2NFhYBG8sI5gOSMxtNsmavd1jdy4QjvwHGwtFbP0dv4bN8kVmvhg4PHeDDPzokQKi777a2tb2xubRd2irt7weHpaPjhtWpYbzOtNSmFVHLpVC8jgIlbyWG0ziSvBmNbmdc8yNFVo94CThYUwHSvQFoikRicZisdxt1T2K4cZJUEOSlDjlq39NXpaZbGXCGT1Np24CcYZtSgYJJPi53U8oSyER3wtqOKxtyG2fzaKTl3So0tXGlkMzV3xMZja2dxJHrjCkO7bI3Ez2in2r8NMqCRFrthiUTVBDWZvU56wnCGcuIIZUa4WwkbUkMZuoCKLoRgeVV0risBH4luPfL1Zs8jgKcwhlcQABXUIU7qEEdGDzBM7zCm6e9Fd1i0rnn5zAn8gff5A59Hjx0latexitV0
latexit sha1_base64gAQ7qdt3IKvK5oBqK3uN1PHYi1kAAAB6XicbVA9SwNBEJ2LXzFRS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCPwMZCEVvkZ3xk1yhSYGHi8N8PMvCARXBvXXYKasbm1vF7dLO7t7QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbzh0jyWj2aSoBRoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL31BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4ned3UhNdxmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cddj0Vpw8pljAPn8wcRSI0Flatexitlatexit sha1_base64gAQ7qdt3IKvK5oBqK3uN1PHYi1kAAAB6XicbVA9SwNBEJ2LXzFRS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCPwMZCEVvkZ3xk1yhSYGHi8N8PMvCARXBvXXYKasbm1vF7dLO7t7QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbzh0jyWj2aSoBRoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL31BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4ned3UhNdxmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cddj0Vpw8pljAPn8wcRSI0Flatexitlatexit sha1_base64gAQ7qdt3IKvK5oBqK3uN1PHYi1kAAAB6XicbVA9SwNBEJ2LXzFRS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCPwMZCEVvkZ3xk1yhSYGHi8N8PMvCARXBvXXYKasbm1vF7dLO7t7QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbzh0jyWj2aSoBRoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL31BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4ned3UhNdxmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cddj0Vpw8pljAPn8wcRSI0Flatexitlatexit sha1_base64gAQ7qdt3IKvK5oBqK3uN1PHYi1kAAAB6XicbVA9SwNBEJ2LXzFRS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCPwMZCEVvkZ3xk1yhSYGHi8N8PMvCARXBvXXYKasbm1vF7dLO7t7QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbzh0jyWj2aSoBRoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL31BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4ned3UhNdxmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cddj0Vpw8pljAPn8wcRSI0FlatexitFigure 5 NLNNs as GNs A schematic showing how NLNNs Wang et al 2018c are implemented
by theeandevunder the GN framework Typically NLNNs assume that dierent regions of
an image or words in a sentence correspond to nodes in a fully connected graph and the attention
mechanism denes a weighted sum over nodes during the aggregation step
the readout function R plays the role of the GNs u but does not take uorE0as input
and thus an analog to the GNs euis not required
dmaster serves a roughly similar purpose to the GNs u but is dened as an extra node
connected to all others and thus does not inuence the edge and global updates directly It
can then be represented in the GNs V
Figure 4c shows how an MPNN is structured according to the GN framework For details and
various MPNN architectures see the Appendix
422 Nonlocal neural networks NLNN
Wang et al 2018cs NLNN which unies various intraselfvertexgraphattention approaches
Lin et al 2017 Vaswani et al 2017 Hoshen 2017 Veli ckovi c et al 2018 Shaw et al 2018
can also be translated into the GN formalism The label attention refers to how the nodes are
updated each node update is based on a weighted sum of some function of the node attributes of
its neighbors where the weight between a node and one of its neighbors is computed by a scalar
pairwise function between their attributes and then normalized across neighbors The published
NLNN formalism does not explicitly include edges and instead computes pairwise attention weights
between all nodes But various NLNNcompliant models such as the vertex attention interaction
network Hoshen 2017 and graph attention network Veli ckovi c et al 2018 are able to handle
explicit edges by eectively setting to zero the weights between nodes which do not share an edge
As Figures 4d and 5 illustrate the eis factored into the scalar pairwiseinteraction function
which returns the unnormalized attention term denoted evrkvska0
k and a vectorvalued
nonpairwise term denoted evskb0
k In theevaggregation the a0
kterms are normalized
across each receivers edges b0
k and elementwise summed
eekvrkvskufevrkvsk   evrkvsk evsk  a0
kb0
k e0
k
ve0
iviufve0
i
ev
E0
i1P
fkrkiga0
kX
fkrkiga0
kb0
k
In the NLNN papers terminology see Wang et al 2018c pages 24
theirfplays the role of the above 
17theirgplays the role of the above 
This formulation may be helpful for focusing only on those interactions which are most relevant for
the downstream task especially when the input entities were a set from which a graph was formed
by adding all possible edges between them
Vaswani et al 2017s multiheaded selfattention mechanism adds an interesting feature where
theeandevare implemented by a parallel set of functions whose results are concatenated
together as the nal step of ev This can be interpreted as using typed edges where the dierent
types index into dierent ecomponent functions analogous to Li et al 2016
For details and various NLNN architectures see the Appendix
423 Other graph network variants
The full GN Equation 2 can be used to predict a full graph or any subset of u0V0E0 as
outlined in Section 411 For example to predict a global property of a graph V0andE0can just
be ignored Similarly if global node or edge attributes are unspecied in the inputs those vectors
can be zerolength ie not taken as explicit input arguments The same idea applies for other GN
variants which do not use the full set of mapping   and reduction   functions For instance
Interaction Networks Battaglia et al 2016 Watters et al 2017 and the Neural Physics Engine
Chang et al 2017 use a full GN but for the absence of the global to update the edge properties
see Appendix for details
Various models including CommNet Sukhbaatar et al 2016 structure2vec Dai et al 2016
in the version of Dai et al 2017 and Gated Graph Sequence Neural Networks Li et al 2016
have used a ewhich does not directly compute pairwise interactions but instead ignore the receiver
node operating only on the sender node and in some cases an edge attribute This can be expressed
by implementations of ewith the following signatures such as
eekvrkvskufevsk
oreekvrkvskuvskfeek
oreekvrkvskufeekvsk
See the Appendix for further details
Relation Networks Raposo et al 2017 Santoro et al 2017 bypass the node update entirely
and predict the global output from pooled edge information directly see also Figure 4e
eekvrkvskufevrkvsk  NNevrkvsk
u
 e0 v0ufu
 e0
 NNu
 e0
eu
E0 X
ke0
k
Deep Sets Zaheer et al 2017 bypass the edges update completely and predict the global output
from pooled nodes information directly Figure 4f
veiviufvviu  NNvviu
u
 e0 v0ufu
 v0
 NNu
 v0
vu
V0 X
iv0
i
PointNet Qi et al 2017 use similar update rule with a maxaggregation for vuand a twostep
node update
18GM
latexit sha1_base64vjTCpRgsPEJfhljVzwQb7AFhV5cAAAB6nicbVA9SwNBEJ2LXzFRS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOfMmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd68f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUeVv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf8TmrCSzjMkkNSjZfFKaCmJhMyZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuzv1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5AmdjYUlatexitlatexit sha1_base64vjTCpRgsPEJfhljVzwQb7AFhV5cAAAB6nicbVA9SwNBEJ2LXzFRS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOfMmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd68f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUeVv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf8TmrCSzjMkkNSjZfFKaCmJhMyZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuzv1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5AmdjYUlatexitlatexit sha1_base64vjTCpRgsPEJfhljVzwQb7AFhV5cAAAB6nicbVA9SwNBEJ2LXzFRS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOfMmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd68f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUeVv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf8TmrCSzjMkkNSjZfFKaCmJhMyZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuzv1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5AmdjYUlatexitlatexit sha1_base64vjTCpRgsPEJfhljVzwQb7AFhV5cAAAB6nicbVA9SwNBEJ2LXzFRS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOfMmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd68f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUeVv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf8TmrCSzjMkkNSjZfFKaCmJhMyZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuzv1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5AmdjYUlatexitGN1
latexit sha1_base64oAmr7S238q10w2wEvXkfEGmAr8AAAB9HicbVDLSgMxFL1TX7Wqi7dBIvgqsyIUBcuCi50JRXsA9qhZNJMG5pkxiRTKEOw40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3mdeUKVZJBNNKawEPJQkawsZLfE9iMlEhv72d9r1uuFV3DrRKvJxUIEejX7qDSKSCCoN4VjrrufGxkxMoxwOiv1Ek1jTMZ4SLuWSiyo9tN56Bk6s8oAhZGyTxo0V39vpFhoPRWBncxC6mUvEzuokJryUyTgxVJLFoTDhyEQoawANmKLE8KklmChmsyIywgoTY3sq2RK85SvktZF1XOr3sNlpX6d11GEEziFcCgBnW4gwY0gcATPMMrvDkT58V5dz4WowUn3zmGP3AfwCg3ZH4latexitlatexit sha1_base64oAmr7S238q10w2wEvXkfEGmAr8AAAB9HicbVDLSgMxFL1TX7Wqi7dBIvgqsyIUBcuCi50JRXsA9qhZNJMG5pkxiRTKEOw40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3mdeUKVZJBNNKawEPJQkawsZLfE9iMlEhv72d9r1uuFV3DrRKvJxUIEejX7qDSKSCCoN4VjrrufGxkxMoxwOiv1Ek1jTMZ4SLuWSiyo9tN56Bk6s8oAhZGyTxo0V39vpFhoPRWBncxC6mUvEzuokJryUyTgxVJLFoTDhyEQoawANmKLE8KklmChmsyIywgoTY3sq2RK85SvktZF1XOr3sNlpX6d11GEEziFcCgBnW4gwY0gcATPMMrvDkT58V5dz4WowUn3zmGP3AfwCg3ZH4latexitlatexit sha1_base64oAmr7S238q10w2wEvXkfEGmAr8AAAB9HicbVDLSgMxFL1TX7Wqi7dBIvgqsyIUBcuCi50JRXsA9qhZNJMG5pkxiRTKEOw40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3mdeUKVZJBNNKawEPJQkawsZLfE9iMlEhv72d9r1uuFV3DrRKvJxUIEejX7qDSKSCCoN4VjrrufGxkxMoxwOiv1Ek1jTMZ4SLuWSiyo9tN56Bk6s8oAhZGyTxo0V39vpFhoPRWBncxC6mUvEzuokJryUyTgxVJLFoTDhyEQoawANmKLE8KklmChmsyIywgoTY3sq2RK85SvktZF1XOr3sNlpX6d11GEEziFcCgBnW4gwY0gcATPMMrvDkT58V5dz4WowUn3zmGP3AfwCg3ZH4latexitlatexit sha1_base64oAmr7S238q10w2wEvXkfEGmAr8AAAB9HicbVDLSgMxFL1TX7Wqi7dBIvgqsyIUBcuCi50JRXsA9qhZNJMG5pkxiRTKEOw40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3mdeUKVZJBNNKawEPJQkawsZLfE9iMlEhv72d9r1uuFV3DrRKvJxUIEejX7qDSKSCCoN4VjrrufGxkxMoxwOiv1Ek1jTMZ4SLuWSiyo9tN56Bk6s8oAhZGyTxo0V39vpFhoPRWBncxC6mUvEzuokJryUyTgxVJLFoTDhyEQoawANmKLE8KklmChmsyIywgoTY3sq2RK85SvktZF1XOr3sNlpX6d11GEEziFcCgBnW4gwY0gcATPMMrvDkT58V5dz4WowUn3zmGP3AfwCg3ZH4latexitGN2
latexit sha1_base64pet508CCa1uIZM8cv8xqNGylB9wAAAB9HicbVDLSgMxFL1TX7Wqi7dBIvgqswUQRcuCi50JRXsA9qhZNJMG5pkxiRTKEOw40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3mdeUKVZJBNNKawEPJQkawsZLfE9iMlEhv72f9Wr9ccavuHGiVeDmpQI5GvzVG0QkEVQawrHWXcNjZ9iZRjhdFbqJZrGmIzxkHYtlVhQ7afz0DN0ZpUBCiNlnzRorv7eSLHQeioCO5mF1MteJv7ndRMTXvkpk3FiqCSLQ2HCkYlQ1gAaMEWJ4VNLMFHMZkVkhBUmxvZUsiV4y19eJa1a1XOr3sNFpX6d11GEEziFcDgEupwBw1oAoEneIZXeHMmzovz7nwsRgtOvnMMfB8gCiYZH5latexitlatexit sha1_base64pet508CCa1uIZM8cv8xqNGylB9wAAAB9HicbVDLSgMxFL1TX7Wqi7dBIvgqswUQRcuCi50JRXsA9qhZNJMG5pkxiRTKEOw40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3mdeUKVZJBNNKawEPJQkawsZLfE9iMlEhv72f9Wr9ccavuHGiVeDmpQI5GvzVG0QkEVQawrHWXcNjZ9iZRjhdFbqJZrGmIzxkHYtlVhQ7afz0DN0ZpUBCiNlnzRorv7eSLHQeioCO5mF1MteJv7ndRMTXvkpk3FiqCSLQ2HCkYlQ1gAaMEWJ4VNLMFHMZkVkhBUmxvZUsiV4y19eJa1a1XOr3sNFpX6d11GEEziFcDgEupwBw1oAoEneIZXeHMmzovz7nwsRgtOvnMMfB8gCiYZH5latexitlatexit sha1_base64pet508CCa1uIZM8cv8xqNGylB9wAAAB9HicbVDLSgMxFL1TX7Wqi7dBIvgqswUQRcuCi50JRXsA9qhZNJMG5pkxiRTKEOw40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3mdeUKVZJBNNKawEPJQkawsZLfE9iMlEhv72f9Wr9ccavuHGiVeDmpQI5GvzVG0QkEVQawrHWXcNjZ9iZRjhdFbqJZrGmIzxkHYtlVhQ7afz0DN0ZpUBCiNlnzRorv7eSLHQeioCO5mF1MteJv7ndRMTXvkpk3FiqCSLQ2HCkYlQ1gAaMEWJ4VNLMFHMZkVkhBUmxvZUsiV4y19eJa1a1XOr3sNFpX6d11GEEziFcDgEupwBw1oAoEneIZXeHMmzovz7nwsRgtOvnMMfB8gCiYZH5latexitlatexit sha1_base64pet508CCa1uIZM8cv8xqNGylB9wAAAB9HicbVDLSgMxFL1TX7Wqi7dBIvgqswUQRcuCi50JRXsA9qhZNJMG5pkxiRTKEOw40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3mdeUKVZJBNNKawEPJQkawsZLfE9iMlEhv72f9Wr9ccavuHGiVeDmpQI5GvzVG0QkEVQawrHWXcNjZ9iZRjhdFbqJZrGmIzxkHYtlVhQ7afz0DN0ZpUBCiNlnzRorv7eSLHQeioCO5mF1MteJv7ndRMTXvkpk3FiqCSLQ2HCkYlQ1gAaMEWJ4VNLMFHMZkVkhBUmxvZUsiV4y19eJa1a1XOr3sNFpX6d11GEEziFcDgEupwBw1oAoEneIZXeHMmzovz7nwsRgtOvnMMfB8gCiYZH5latexitGNM
latexit sha1_base641uUQuLnXmq2FrQq5fvsHBbzm7v8AAAB9HicbVDLSgMxFL1TX7Wqi7dBIvgqsyIoAsXBRe6USrYB7RDyaSZNjTJjEmmUIZhxsXirj1Y9z5N2baWWjrgcDhnHu5JyeIOdPGdbdwsrq2vpGcbO0tb2zu1feP2jqKFGENkjEI9UOsKacSdowzHDajhXFIuC0FYyuM781pkqzSD6aSUx9gQeShYxgYyWK7AZKpHe3E97d71yxa26M6Bl4uWkAjnqvfJXtxRRFBpCMdadzw3Nn6KlWGE02mpm2gaYzLCA9qxVGJBtZOQkRiVX6KIyUfdKgmfp7I8VC64kI7GQWUi96mfif10lMeOmnTMaJoZLMD4UJRyZCWQOozxQlhk8swUQxmxWRIVaYGNtTyZbgLX55mTTPqp5b9R7OK7WrvI4iHMExnIIHF1CDW6hDAwg8wTO8wpszdl6cddjPlpw8p1DAPn8wfLTZIUlatexitlatexit sha1_base641uUQuLnXmq2FrQq5fvsHBbzm7v8AAAB9HicbVDLSgMxFL1TX7Wqi7dBIvgqsyIoAsXBRe6USrYB7RDyaSZNjTJjEmmUIZhxsXirj1Y9z5N2baWWjrgcDhnHu5JyeIOdPGdbdwsrq2vpGcbO0tb2zu1feP2jqKFGENkjEI9UOsKacSdowzHDajhXFIuC0FYyuM781pkqzSD6aSUx9gQeShYxgYyWK7AZKpHe3E97d71yxa26M6Bl4uWkAjnqvfJXtxRRFBpCMdadzw3Nn6KlWGE02mpm2gaYzLCA9qxVGJBtZOQkRiVX6KIyUfdKgmfp7I8VC64kI7GQWUi96mfif10lMeOmnTMaJoZLMD4UJRyZCWQOozxQlhk8swUQxmxWRIVaYGNtTyZbgLX55mTTPqp5b9R7OK7WrvI4iHMExnIIHF1CDW6hDAwg8wTO8wpszdl6cddjPlpw8p1DAPn8wfLTZIUlatexitlatexit sha1_base641uUQuLnXmq2FrQq5fvsHBbzm7v8AAAB9HicbVDLSgMxFL1TX7Wqi7dBIvgqsyIoAsXBRe6USrYB7RDyaSZNjTJjEmmUIZhxsXirj1Y9z5N2baWWjrgcDhnHu5JyeIOdPGdbdwsrq2vpGcbO0tb2zu1feP2jqKFGENkjEI9UOsKacSdowzHDajhXFIuC0FYyuM781pkqzSD6aSUx9gQeShYxgYyWK7AZKpHe3E97d71yxa26M6Bl4uWkAjnqvfJXtxRRFBpCMdadzw3Nn6KlWGE02mpm2gaYzLCA9qxVGJBtZOQkRiVX6KIyUfdKgmfp7I8VC64kI7GQWUi96mfif10lMeOmnTMaJoZLMD4UJRyZCWQOozxQlhk8swUQxmxWRIVaYGNtTyZbgLX55mTTPqp5b9R7OK7WrvI4iHMExnIIHF1CDW6hDAwg8wTO8wpszdl6cddjPlpw8p1DAPn8wfLTZIUlatexitlatexit sha1_base641uUQuLnXmq2FrQq5fvsHBbzm7v8AAAB9HicbVDLSgMxFL1TX7Wqi7dBIvgqsyIoAsXBRe6USrYB7RDyaSZNjTJjEmmUIZhxsXirj1Y9z5N2baWWjrgcDhnHu5JyeIOdPGdbdwsrq2vpGcbO0tb2zu1feP2jqKFGENkjEI9UOsKacSdowzHDajhXFIuC0FYyuM781pkqzSD6aSUx9gQeShYxgYyWK7AZKpHe3E97d71yxa26M6Bl4uWkAjnqvfJXtxRRFBpCMdadzw3Nn6KlWGE02mpm2gaYzLCA9qxVGJBtZOQkRiVX6KIyUfdKgmfp7I8VC64kI7GQWUi96mfif10lMeOmnTMaJoZLMD4UJRyZCWQOozxQlhk8swUQxmxWRIVaYGNtTyZbgLX55mTTPqp5b9R7OK7WrvI4iHMExnIIHF1CDW6hDAwg8wTO8wpszdl6cddjPlpw8p1DAPn8wfLTZIUlatexitlatexit sha1_base64Gj7yv98SlyD93GhofpNnyXvd2cAAAB7HicbVBNS8NAFHypX7VVT16WSyCp5KIUA8eCl48VjBtoQ1ls920SzebsPsilNDf4MWDIl79Qd78N27bHLR1YGGYecON2EqhUHXXZKG5tb2zvl3cresHhUfX4pG2STDPus0QmuhtSw6VQ3EeBkndTzWkcSt4JJ3dzvPEtRGJesRpyoOYjpSIBKNoJb8TNAMqjW37i5A1olXkBoUaA2qXzbHspgrZJIa0PcFIOcahRM8lmlnxmeUjahI96zVNGYmyBfLDsjF1YZkijR9ikkCV3IqexMdM4tJMxxbFZ9ebif14vwgmyIVKMSKLTKMkkwIfPLyVBozlBOLaFMC7srYWOqKUPbT8WW4K2evE7aV3XPrXsP17XmbVFHGc7gHC7BgwY04R5a4AMDAcwCmOcl6cddjOVpyiswpIHzQPvmo68latexitlatexit sha1_base64Gj7yv98SlyD93GhofpNnyXvd2cAAAB7HicbVBNS8NAFHypX7VVT16WSyCp5KIUA8eCl48VjBtoQ1ls920SzebsPsilNDf4MWDIl79Qd78N27bHLR1YGGYecON2EqhUHXXZKG5tb2zvl3cresHhUfX4pG2STDPus0QmuhtSw6VQ3EeBkndTzWkcSt4JJ3dzvPEtRGJesRpyoOYjpSIBKNoJb8TNAMqjW37i5A1olXkBoUaA2qXzbHspgrZJIa0PcFIOcahRM8lmlnxmeUjahI96zVNGYmyBfLDsjF1YZkijR9ikkCV3IqexMdM4tJMxxbFZ9ebif14vwgmyIVKMSKLTKMkkwIfPLyVBozlBOLaFMC7srYWOqKUPbT8WW4K2evE7aV3XPrXsP17XmbVFHGc7gHC7BgwY04R5a4AMDAcwCmOcl6cddjOVpyiswpIHzQPvmo68latexitlatexit sha1_base64Gj7yv98SlyD93GhofpNnyXvd2cAAAB7HicbVBNS8NAFHypX7VVT16WSyCp5KIUA8eCl48VjBtoQ1ls920SzebsPsilNDf4MWDIl79Qd78N27bHLR1YGGYecON2EqhUHXXZKG5tb2zvl3cresHhUfX4pG2STDPus0QmuhtSw6VQ3EeBkndTzWkcSt4JJ3dzvPEtRGJesRpyoOYjpSIBKNoJb8TNAMqjW37i5A1olXkBoUaA2qXzbHspgrZJIa0PcFIOcahRM8lmlnxmeUjahI96zVNGYmyBfLDsjF1YZkijR9ikkCV3IqexMdM4tJMxxbFZ9ebif14vwgmyIVKMSKLTKMkkwIfPLyVBozlBOLaFMC7srYWOqKUPbT8WW4K2evE7aV3XPrXsP17XmbVFHGc7gHC7BgwY04R5a4AMDAcwCmOcl6cddjOVpyiswpIHzQPvmo68latexitlatexit sha1_base64Gj7yv98SlyD93GhofpNnyXvd2cAAAB7HicbVBNS8NAFHypX7VVT16WSyCp5KIUA8eCl48VjBtoQ1ls920SzebsPsilNDf4MWDIl79Qd78N27bHLR1YGGYecON2EqhUHXXZKG5tb2zvl3cresHhUfX4pG2STDPus0QmuhtSw6VQ3EeBkndTzWkcSt4JJ3dzvPEtRGJesRpyoOYjpSIBKNoJb8TNAMqjW37i5A1olXkBoUaA2qXzbHspgrZJIa0PcFIOcahRM8lmlnxmeUjahI96zVNGYmyBfLDsjF1YZkijR9ikkCV3IqexMdM4tJMxxbFZ9ebif14vwgmyIVKMSKLTKMkkwIfPLyVBozlBOLaFMC7srYWOqKUPbT8WW4K2evE7aV3XPrXsP17XmbVFHGc7gHC7BgwY04R5a4AMDAcwCmOcl6cddjOVpyiswpIHzQPvmo68latexitG1
latexit sha1_base64YNShseMoKm2HdChKvcjMRmoBu5oAAAB6nicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x6cEI78BBsLRWz9RXbGzfJFZr4YODx3gwz84JECoOu0U1tY3NreK26Wd3b39gLhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Ze0nro2I1SNOEu5HdKhEKBhFKz3c9r1ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMldT2Q0MmYSBbYzojgyy95MMrphheZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpXdxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8kDvy2NaQlatexitlatexit sha1_base64YNShseMoKm2HdChKvcjMRmoBu5oAAAB6nicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x6cEI78BBsLRWz9RXbGzfJFZr4YODx3gwz84JECoOu0U1tY3NreK26Wd3b39gLhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Ze0nro2I1SNOEu5HdKhEKBhFKz3c9r1ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMldT2Q0MmYSBbYzojgyy95MMrphheZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpXdxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8kDvy2NaQlatexitlatexit sha1_base64YNShseMoKm2HdChKvcjMRmoBu5oAAAB6nicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x6cEI78BBsLRWz9RXbGzfJFZr4YODx3gwz84JECoOu0U1tY3NreK26Wd3b39gLhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Ze0nro2I1SNOEu5HdKhEKBhFKz3c9r1ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMldT2Q0MmYSBbYzojgyy95MMrphheZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpXdxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8kDvy2NaQlatexitlatexit sha1_base64YNShseMoKm2HdChKvcjMRmoBu5oAAAB6nicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x6cEI78BBsLRWz9RXbGzfJFZr4YODx3gwz84JECoOu0U1tY3NreK26Wd3b39gLhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Ze0nro2I1SNOEu5HdKhEKBhFKz3c9r1ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMldT2Q0MmYSBbYzojgyy95MMrphheZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpXdxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8kDvy2NaQlatexitG0
latexit sha1_base64vj48jMMQe2f55rU6zb6RZpK9y4AAAB6nicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x6cEI78BBsLRWz9RXbGzfJFZr4YODx3gwz84JECoOu0U1tY3NreK26Wd3b39gLhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Ze0nro2I1SNOEu5HdKhEKBhFKz3c9t1ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMldT2Q0MmYSBbYzojgyy95MMrphheZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpXdxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8kDvamNaAlatexitlatexit sha1_base64vj48jMMQe2f55rU6zb6RZpK9y4AAAB6nicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x6cEI78BBsLRWz9RXbGzfJFZr4YODx3gwz84JECoOu0U1tY3NreK26Wd3b39gLhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Ze0nro2I1SNOEu5HdKhEKBhFKz3c9t1ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMldT2Q0MmYSBbYzojgyy95MMrphheZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpXdxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8kDvamNaAlatexitlatexit sha1_base64vj48jMMQe2f55rU6zb6RZpK9y4AAAB6nicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x6cEI78BBsLRWz9RXbGzfJFZr4YODx3gwz84JECoOu0U1tY3NreK26Wd3b39gLhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Ze0nro2I1SNOEu5HdKhEKBhFKz3c9t1ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMldT2Q0MmYSBbYzojgyy95MMrphheZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpXdxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8kDvamNaAlatexitlatexit sha1_base64vj48jMMQe2f55rU6zb6RZpK9y4AAAB6nicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x6cEI78BBsLRWz9RXbGzfJFZr4YODx3gwz84JECoOu0U1tY3NreK26Wd3b39gLhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Ze0nro2I1SNOEu5HdKhEKBhFKz3c9t1ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMldT2Q0MmYSBbYzojgyy95MMrphheZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpXdxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8kDvamNaAlatexitGM
latexit sha1_base64vjTCpRgsPEJfhljVzwQb7AFhV5cAAAB6nicbVA9SwNBEJ2LXzFRS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOfMmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd68f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUeVv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf8TmrCSzjMkkNSjZfFKaCmJhMyZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuzv1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5AmdjYUlatexitlatexit sha1_base64vjTCpRgsPEJfhljVzwQb7AFhV5cAAAB6nicbVA9SwNBEJ2LXzFRS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOfMmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd68f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUeVv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf8TmrCSzjMkkNSjZfFKaCmJhMyZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuzv1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5AmdjYUlatexitlatexit sha1_base64vjTCpRgsPEJfhljVzwQb7AFhV5cAAAB6nicbVA9SwNBEJ2LXzFRS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOfMmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd68f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUeVv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf8TmrCSzjMkkNSjZfFKaCmJhMyZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuzv1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5AmdjYUlatexitlatexit sha1_base64vjTCpRgsPEJfhljVzwQb7AFhV5cAAAB6nicbVA9SwNBEJ2LXzFRS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOfMmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd68f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUeVv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf8TmrCSzjMkkNSjZfFKaCmJhMyZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuzv1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5AmdjYUlatexitG0
latexit sha1_base64vj48jMMQe2f55rU6zb6RZpK9y4AAAB6nicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x6cEI78BBsLRWz9RXbGzfJFZr4YODx3gwz84JECoOu0U1tY3NreK26Wd3b39gLhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Ze0nro2I1SNOEu5HdKhEKBhFKz3c9t1ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMldT2Q0MmYSBbYzojgyy95MMrphheZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpXdxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8kDvamNaAlatexitlatexit sha1_base64vj48jMMQe2f55rU6zb6RZpK9y4AAAB6nicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x6cEI78BBsLRWz9RXbGzfJFZr4YODx3gwz84JECoOu0U1tY3NreK26Wd3b39gLhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Ze0nro2I1SNOEu5HdKhEKBhFKz3c9t1ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMldT2Q0MmYSBbYzojgyy95MMrphheZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpXdxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8kDvamNaAlatexitlatexit sha1_base64vj48jMMQe2f55rU6zb6RZpK9y4AAAB6nicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x6cEI78BBsLRWz9RXbGzfJFZr4YODx3gwz84JECoOu0U1tY3NreK26Wd3b39gLhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Ze0nro2I1SNOEu5HdKhEKBhFKz3c9t1ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMldT2Q0MmYSBbYzojgyy95MMrphheZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpXdxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8kDvamNaAlatexitlatexit sha1_base64vj48jMMQe2f55rU6zb6RZpK9y4AAAB6nicbVA9SwNBEJ2LXzFRS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x6cEI78BBsLRWz9RXbGzfJFZr4YODx3gwz84JECoOu0U1tY3NreK26Wd3b39gLhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Ze0nro2I1SNOEu5HdKhEKBhFKz3c9t1ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMldT2Q0MmYSBbYzojgyy95MMrphheZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpXdxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8kDvamNaAlatexitGNcore
latexit sha1_base64sfcetjjriA53KVhP8LRkSGs9KNAAAAB3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhMJLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZnsjQ1zrKYsZJFUL3qFJXT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqSAXv7xMOmcN32v49f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc5qMrTrlzCP7AfwBopiUywlatexitlatexit sha1_base64sfcetjjriA53KVhP8LRkSGs9KNAAAAB3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhMJLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZnsjQ1zrKYsZJFUL3qFJXT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqSAXv7xMOmcN32v49f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc5qMrTrlzCP7AfwBopiUywlatexitlatexit sha1_base64sfcetjjriA53KVhP8LRkSGs9KNAAAAB3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhMJLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZnsjQ1zrKYsZJFUL3qFJXT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqSAXv7xMOmcN32v49f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc5qMrTrlzCP7AfwBopiUywlatexitlatexit sha1_base64sfcetjjriA53KVhP8LRkSGs9KNAAAAB3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhMJLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZnsjQ1zrKYsZJFUL3qFJXT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqSAXv7xMOmcN32v49f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc5qMrTrlzCP7AfwBopiUywlatexitM
latexit sha1_base64xCEPSgjeJaAOppNxwTZXrwRukIgAAAB73icbVA9SwNBEJ2LXzFRS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbuzOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFGgQMnbieY0CiRvBaPbmd964tqIWD3iJOFRAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlViSMtS2FZK7nshoZMwkCmxnRHFolr2ZJXSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Zy8OOOx6K14OQzxAHzucPqJqPrwlatexitlatexit sha1_base64xCEPSgjeJaAOppNxwTZXrwRukIgAAAB73icbVA9SwNBEJ2LXzFRS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbuzOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFGgQMnbieY0CiRvBaPbmd964tqIWD3iJOFRAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlViSMtS2FZK7nshoZMwkCmxnRHFolr2ZJXSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Zy8OOOx6K14OQzxAHzucPqJqPrwlatexitlatexit sha1_base64xCEPSgjeJaAOppNxwTZXrwRukIgAAAB73icbVA9SwNBEJ2LXzFRS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbuzOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFGgQMnbieY0CiRvBaPbmd964tqIWD3iJOFRAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlViSMtS2FZK7nshoZMwkCmxnRHFolr2ZJXSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Zy8OOOx6K14OQzxAHzucPqJqPrwlatexitlatexit sha1_base64xCEPSgjeJaAOppNxwTZXrwRukIgAAAB73icbVA9SwNBEJ2LXzFRS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbuzOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFGgQMnbieY0CiRvBaPbmd964tqIWD3iJOFRAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlViSMtS2FZK7nshoZMwkCmxnRHFolr2ZJXSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Zy8OOOx6K14OQzxAHzucPqJqPrwlatexita Composition of GN blocks
GNenc
latexit sha1_base64KZY5NxgXVECq8QikcRvAbEAxBsAAABnicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6cRKz0NYDA4dz7mXOPWHMqNKu2UtLasrq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3Me49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfcNtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa94wuWeXOPvyB9fENyBqUTglatexitlatexit sha1_base64KZY5NxgXVECq8QikcRvAbEAxBsAAABnicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6cRKz0NYDA4dz7mXOPWHMqNKu2UtLasrq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3Me49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfcNtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa94wuWeXOPvyB9fENyBqUTglatexitlatexit sha1_base64KZY5NxgXVECq8QikcRvAbEAxBsAAABnicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6cRKz0NYDA4dz7mXOPWHMqNKu2UtLasrq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3Me49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfcNtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa94wuWeXOPvyB9fENyBqUTglatexitlatexit sha1_base64KZY5NxgXVECq8QikcRvAbEAxBsAAABnicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6cRKz0NYDA4dz7mXOPWHMqNKu2UtLasrq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3Me49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfcNtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa94wuWeXOPvyB9fENyBqUTglatexitGNdec
latexit sha1_base6479QHnx4t4kSfeuqQfRizzDMfIAAABnicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79EnfjZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk5p03Wa7t1Zo3VZ1lEFhAInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQBYH3ALjdlEQlatexitlatexit sha1_base6479QHnx4t4kSfeuqQfRizzDMfIAAABnicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79EnfjZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk5p03Wa7t1Zo3VZ1lEFhAInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQBYH3ALjdlEQlatexitlatexit sha1_base6479QHnx4t4kSfeuqQfRizzDMfIAAABnicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79EnfjZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk5p03Wa7t1Zo3VZ1lEFhAInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQBYH3ALjdlEQlatexitlatexit sha1_base6479QHnx4t4kSfeuqQfRizzDMfIAAABnicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79EnfjZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk5p03Wa7t1Zo3VZ1lEFhAInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQBYH3ALjdlEQlatexitGNcore
latexit sha1_base64sfcetjjriA53KVhP8LRkSGs9KNAAAAB3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhMJLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZnsjQ1zrKYsZJFUL3qFJXT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqSAXv7xMOmcN32v49f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc5qMrTrlzCP7AfwBopiUywlatexitlatexit sha1_base64sfcetjjriA53KVhP8LRkSGs9KNAAAAB3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhMJLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZnsjQ1zrKYsZJFUL3qFJXT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqSAXv7xMOmcN32v49f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc5qMrTrlzCP7AfwBopiUywlatexitlatexit sha1_base64sfcetjjriA53KVhP8LRkSGs9KNAAAAB3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhMJLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZnsjQ1zrKYsZJFUL3qFJXT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqSAXv7xMOmcN32v49f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc5qMrTrlzCP7AfwBopiUywlatexitlatexit sha1_base64sfcetjjriA53KVhP8LRkSGs9KNAAAAB3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhMJLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZnsjQ1zrKYsZJFUL3qFJXT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqSAXv7xMOmcN32v49f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc5qMrTrlzCP7AfwBopiUywlatexitM
latexit sha1_base64xCEPSgjeJaAOppNxwTZXrwRukIgAAAB73icbVA9SwNBEJ2LXzFRS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbuzOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFGgQMnbieY0CiRvBaPbmd964tqIWD3iJOFRAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlViSMtS2FZK7nshoZMwkCmxnRHFolr2ZJXSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Zy8OOOx6K14OQzxAHzucPqJqPrwlatexitlatexit sha1_base64xCEPSgjeJaAOppNxwTZXrwRukIgAAAB73icbVA9SwNBEJ2LXzFRS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbuzOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFGgQMnbieY0CiRvBaPbmd964tqIWD3iJOFRAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlViSMtS2FZK7nshoZMwkCmxnRHFolr2ZJXSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Zy8OOOx6K14OQzxAHzucPqJqPrwlatexitlatexit sha1_base64xCEPSgjeJaAOppNxwTZXrwRukIgAAAB73icbVA9SwNBEJ2LXzFRS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbuzOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFGgQMnbieY0CiRvBaPbmd964tqIWD3iJOFRAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlViSMtS2FZK7nshoZMwkCmxnRHFolr2ZJXSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Zy8OOOx6K14OQzxAHzucPqJqPrwlatexitlatexit sha1_base64xCEPSgjeJaAOppNxwTZXrwRukIgAAAB73icbVA9SwNBEJ2LXzFRS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbuzOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFGgQMnbieY0CiRvBaPbmd964tqIWD3iJOFRAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlViSMtS2FZK7nshoZMwkCmxnRHFolr2ZJXSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Zy8OOOx6K14OQzxAHzucPqJqPrwlatexitGout
latexit sha1_base64TKn8tu9S9KYM9INqkcELhgYcuAAAABXicbVDLSsNAFJ3UV62vqEs3wSK4KokIunBRcKHLCvYBbQiT6aQdOo8wc1MooXixoUibv0Td6NkzYLrR4YOJxzLfMiVPODPjl1NZW9Y3Kpu13Z29YP3MOjjlGZJrRNFFe6F2NDOZO0DQw47aWaYhFz2o0nt4XfnVJtmJKPMEtpKPBIsoQRDFaKXPcuygcCw1iLXGUwn0du3W4C3hSVCSOirRitzPwVCRTFAJhGNjoGfQphjDYxwOq8NMkNTTCZ4RPuWSiyoCfNF8rl3ZpWhlyhtnwRvof7cyLEwZiZiO1mENKteIf7n9TNIrsOcyTQDKsnyUJJxD5RX1OANmaYEMwSTDSzWT0yxhoTsGXVbAnB6pfks5FIAbwcNlvXlT1lFFJgUnaMAXaEmukct1EYETdETekGvTu48O2O3K04pQ7xgXnI9vWWaUGAlatexitlatexit sha1_base64TKn8tu9S9KYM9INqkcELhgYcuAAAABXicbVDLSsNAFJ3UV62vqEs3wSK4KokIunBRcKHLCvYBbQiT6aQdOo8wc1MooXixoUibv0Td6NkzYLrR4YOJxzLfMiVPODPjl1NZW9Y3Kpu13Z29YP3MOjjlGZJrRNFFe6F2NDOZO0DQw47aWaYhFz2o0nt4XfnVJtmJKPMEtpKPBIsoQRDFaKXPcuygcCw1iLXGUwn0du3W4C3hSVCSOirRitzPwVCRTFAJhGNjoGfQphjDYxwOq8NMkNTTCZ4RPuWSiyoCfNF8rl3ZpWhlyhtnwRvof7cyLEwZiZiO1mENKteIf7n9TNIrsOcyTQDKsnyUJJxD5RX1OANmaYEMwSTDSzWT0yxhoTsGXVbAnB6pfks5FIAbwcNlvXlT1lFFJgUnaMAXaEmukct1EYETdETekGvTu48O2O3K04pQ7xgXnI9vWWaUGAlatexitlatexit sha1_base64TKn8tu9S9KYM9INqkcELhgYcuAAAABXicbVDLSsNAFJ3UV62vqEs3wSK4KokIunBRcKHLCvYBbQiT6aQdOo8wc1MooXixoUibv0Td6NkzYLrR4YOJxzLfMiVPODPjl1NZW9Y3Kpu13Z29YP3MOjjlGZJrRNFFe6F2NDOZO0DQw47aWaYhFz2o0nt4XfnVJtmJKPMEtpKPBIsoQRDFaKXPcuygcCw1iLXGUwn0du3W4C3hSVCSOirRitzPwVCRTFAJhGNjoGfQphjDYxwOq8NMkNTTCZ4RPuWSiyoCfNF8rl3ZpWhlyhtnwRvof7cyLEwZiZiO1mENKteIf7n9TNIrsOcyTQDKsnyUJJxD5RX1OANmaYEMwSTDSzWT0yxhoTsGXVbAnB6pfks5FIAbwcNlvXlT1lFFJgUnaMAXaEmukct1EYETdETekGvTu48O2O3K04pQ7xgXnI9vWWaUGAlatexitlatexit sha1_base64TKn8tu9S9KYM9INqkcELhgYcuAAAABXicbVDLSsNAFJ3UV62vqEs3wSK4KokIunBRcKHLCvYBbQiT6aQdOo8wc1MooXixoUibv0Td6NkzYLrR4YOJxzLfMiVPODPjl1NZW9Y3Kpu13Z29YP3MOjjlGZJrRNFFe6F2NDOZO0DQw47aWaYhFz2o0nt4XfnVJtmJKPMEtpKPBIsoQRDFaKXPcuygcCw1iLXGUwn0du3W4C3hSVCSOirRitzPwVCRTFAJhGNjoGfQphjDYxwOq8NMkNTTCZ4RPuWSiyoCfNF8rl3ZpWhlyhtnwRvof7cyLEwZiZiO1mENKteIf7n9TNIrsOcyTQDKsnyUJJxD5RX1OANmaYEMwSTDSzWT0yxhoTsGXVbAnB6pfks5FIAbwcNlvXlT1lFFJgUnaMAXaEmukct1EYETdETekGvTu48O2O3K04pQ7xgXnI9vWWaUGAlatexitGinp
latexit sha1_base64mvYjY6mgtt6w2Efm7YMP3XaOQ7IAAABXicbVDLSgMxFL1TX7WRl26CRbBVZkRQRcuCi50WcEoB2GTJppQ5PMkGQKZeifuHGhiFvxJ1Y6adhVYPBA7n3EvOPVHKmTae9VU1tY3Nreq27Wd3b39AfwqKOTTBHaJglPVCCmnImadsww2kvVRSLiNNuNLktO6UKs0SWhmKQ0EHkkWM4KNlULXvQvzgcBmrETOZDqfh27da3gLoLEL0kdSrRC93MwTEgmqDSEY637vpeaIMfKMMLpvDbINE0xmeAR7VsqsaA6yBfJ5jMKkMUJ8oadBCbmRY6H1TER2sgipV71CMrZyaDoqLMkMlWX4UZxyZBBU1oCFTlBgswQTxWxWRMZYYWJsWTVbgr968lSuWj4XsNuKw3b8o6qnACp3AOPlxBE6hBW0gMIUneIFXJ3eenTfnfTlaccqdYgF5MbP22UBwlatexitlatexit sha1_base64mvYjY6mgtt6w2Efm7YMP3XaOQ7IAAABXicbVDLSgMxFL1TX7WRl26CRbBVZkRQRcuCi50WcEoB2GTJppQ5PMkGQKZeifuHGhiFvxJ1Y6adhVYPBA7n3EvOPVHKmTae9VU1tY3Nreq27Wd3b39AfwqKOTTBHaJglPVCCmnImadsww2kvVRSLiNNuNLktO6UKs0SWhmKQ0EHkkWM4KNlULXvQvzgcBmrETOZDqfh27da3gLoLEL0kdSrRC93MwTEgmqDSEY637vpeaIMfKMMLpvDbINE0xmeAR7VsqsaA6yBfJ5jMKkMUJ8oadBCbmRY6H1TER2sgipV71CMrZyaDoqLMkMlWX4UZxyZBBU1oCFTlBgswQTxWxWRMZYYWJsWTVbgr968lSuWj4XsNuKw3b8o6qnACp3AOPlxBE6hBW0gMIUneIFXJ3eenTfnfTlaccqdYgF5MbP22UBwlatexitlatexit sha1_base64mvYjY6mgtt6w2Efm7YMP3XaOQ7IAAABXicbVDLSgMxFL1TX7WRl26CRbBVZkRQRcuCi50WcEoB2GTJppQ5PMkGQKZeifuHGhiFvxJ1Y6adhVYPBA7n3EvOPVHKmTae9VU1tY3Nreq27Wd3b39AfwqKOTTBHaJglPVCCmnImadsww2kvVRSLiNNuNLktO6UKs0SWhmKQ0EHkkWM4KNlULXvQvzgcBmrETOZDqfh27da3gLoLEL0kdSrRC93MwTEgmqDSEY637vpeaIMfKMMLpvDbINE0xmeAR7VsqsaA6yBfJ5jMKkMUJ8oadBCbmRY6H1TER2sgipV71CMrZyaDoqLMkMlWX4UZxyZBBU1oCFTlBgswQTxWxWRMZYYWJsWTVbgr968lSuWj4XsNuKw3b8o6qnACp3AOPlxBE6hBW0gMIUneIFXJ3eenTfnfTlaccqdYgF5MbP22UBwlatexitlatexit sha1_base64mvYjY6mgtt6w2Efm7YMP3XaOQ7IAAABXicbVDLSgMxFL1TX7WRl26CRbBVZkRQRcuCi50WcEoB2GTJppQ5PMkGQKZeifuHGhiFvxJ1Y6adhVYPBA7n3EvOPVHKmTae9VU1tY3Nreq27Wd3b39AfwqKOTTBHaJglPVCCmnImadsww2kvVRSLiNNuNLktO6UKs0SWhmKQ0EHkkWM4KNlULXvQvzgcBmrETOZDqfh27da3gLoLEL0kdSrRC93MwTEgmqDSEY637vpeaIMfKMMLpvDbINE0xmeAR7VsqsaA6yBfJ5jMKkMUJ8oadBCbmRY6H1TER2sgipV71CMrZyaDoqLMkMlWX4UZxyZBBU1oCFTlBgswQTxWxWRMZYYWJsWTVbgr968lSuWj4XsNuKw3b8o6qnACp3AOPlxBE6hBW0gMIUneIFXJ3eenTfnfTlaccqdYgF5MbP22UBwlatexit b Encodeprocessdecode
GNenc
latexit sha1_base64KZY5NxgXVECq8QikcRvAbEAxBsAAABnicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6cRKz0NYDA4dz7mXOPWHMqNKu2UtLasrq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3Me49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfcNtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa94wuWeXOPvyB9fENyBqUTglatexitlatexit sha1_base64KZY5NxgXVECq8QikcRvAbEAxBsAAABnicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6cRKz0NYDA4dz7mXOPWHMqNKu2UtLasrq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3Me49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfcNtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa94wuWeXOPvyB9fENyBqUTglatexitlatexit sha1_base64KZY5NxgXVECq8QikcRvAbEAxBsAAABnicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6cRKz0NYDA4dz7mXOPWHMqNKu2UtLasrq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3Me49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfcNtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa94wuWeXOPvyB9fENyBqUTglatexitlatexit sha1_base64KZY5NxgXVECq8QikcRvAbEAxBsAAABnicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6cRKz0NYDA4dz7mXOPWHMqNKu2UtLasrq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3Me49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfcNtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa94wuWeXOPvyB9fENyBqUTglatexitGNdec
latexit sha1_base6479QHnx4t4kSfeuqQfRizzDMfIAAABnicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79EnfjZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk5p03Wa7t1Zo3VZ1lEFhAInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQBYH3ALjdlEQlatexitlatexit sha1_base6479QHnx4t4kSfeuqQfRizzDMfIAAABnicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79EnfjZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk5p03Wa7t1Zo3VZ1lEFhAInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQBYH3ALjdlEQlatexitlatexit sha1_base6479QHnx4t4kSfeuqQfRizzDMfIAAABnicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79EnfjZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk5p03Wa7t1Zo3VZ1lEFhAInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQBYH3ALjdlEQlatexitlatexit sha1_base6479QHnx4t4kSfeuqQfRizzDMfIAAABnicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79EnfjZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk5p03Wa7t1Zo3VZ1lEFhAInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQBYH3ALjdlEQlatexitGNcore
latexit sha1_base64sfcetjjriA53KVhP8LRkSGs9KNAAAAB3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhMJLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZnsjQ1zrKYsZJFUL3qFJXT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqSAXv7xMOmcN32v49f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc5qMrTrlzCP7AfwBopiUywlatexitlatexit sha1_base64sfcetjjriA53KVhP8LRkSGs9KNAAAAB3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhMJLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZnsjQ1zrKYsZJFUL3qFJXT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqSAXv7xMOmcN32v49f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc5qMrTrlzCP7AfwBopiUywlatexitlatexit sha1_base64sfcetjjriA53KVhP8LRkSGs9KNAAAAB3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhMJLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZnsjQ1zrKYsZJFUL3qFJXT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqSAXv7xMOmcN32v49f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc5qMrTrlzCP7AfwBopiUywlatexitlatexit sha1_base64sfcetjjriA53KVhP8LRkSGs9KNAAAAB3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhMJLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZnsjQ1zrKYsZJFUL3qFJXT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqSAXv7xMOmcN32v49f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc5qMrTrlzCP7AfwBopiUywlatexitM
latexit sha1_base64xCEPSgjeJaAOppNxwTZXrwRukIgAAAB73icbVA9SwNBEJ2LXzFRS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbuzOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFGgQMnbieY0CiRvBaPbmd964tqIWD3iJOFRAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlViSMtS2FZK7nshoZMwkCmxnRHFolr2ZJXSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Zy8OOOx6K14OQzxAHzucPqJqPrwlatexitlatexit sha1_base64xCEPSgjeJaAOppNxwTZXrwRukIgAAAB73icbVA9SwNBEJ2LXzFRS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbuzOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFGgQMnbieY0CiRvBaPbmd964tqIWD3iJOFRAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlViSMtS2FZK7nshoZMwkCmxnRHFolr2ZJXSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Zy8OOOx6K14OQzxAHzucPqJqPrwlatexitlatexit sha1_base64xCEPSgjeJaAOppNxwTZXrwRukIgAAAB73icbVA9SwNBEJ2LXzFRS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbuzOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFGgQMnbieY0CiRvBaPbmd964tqIWD3iJOFRAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlViSMtS2FZK7nshoZMwkCmxnRHFolr2ZJXSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Zy8OOOx6K14OQzxAHzucPqJqPrwlatexitlatexit sha1_base64xCEPSgjeJaAOppNxwTZXrwRukIgAAAB73icbVA9SwNBEJ2LXzFRS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbuzOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFGgQMnbieY0CiRvBaPbmd964tqIWD3iJOFRAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlViSMtS2FZK7nshoZMwkCmxnRHFolr2ZJXSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Zy8OOOx6K14OQzxAHzucPqJqPrwlatexitGthid
latexit sha1_base64Vr1kZJh7jApVU4cGmtPimK4PPicAAABXicbVDLSsNAFJ3UV62vNi5GSyCq5KIoAsXBRe6rGAf0MYwmUzaoTOTMDMRagjihsXirj1P9z5N07aLLT1wMDhnHu5Z06QMKq043xblaXlldW16nptY3Nre8fe3euoOJWYtHHMYtkLkCKMCtLWVDPSSyRBPGCkG4yvCr7QKSisbjTk4R4HA0FjShG2kifXB9nnczwYc6ZHk2YiGee7bdafhTAEXiVuSOijR8u2vQRjjlBOhMUNK9V0n0V6GpKaYkbw2SBVJEB6jIekbKhAnysum6XN4bJQQRrE0T2g4VX9vZIgrNeGBmSxCqnmvEPzqmOLryMiiTVRODZoShlUMewqAKGVBKs2cQQhCU1WSEeIYmwNoXVTAnuJcXSee04ToN9as3rws66iCQ3AEToALzkET3IAWaAMMHsEzeAVv1pP1Yr1bH7PRilXu7IMsD5AI1Vleclatexitlatexit sha1_base64Vr1kZJh7jApVU4cGmtPimK4PPicAAABXicbVDLSsNAFJ3UV62vNi5GSyCq5KIoAsXBRe6rGAf0MYwmUzaoTOTMDMRagjihsXirj1P9z5N07aLLT1wMDhnHu5Z06QMKq043xblaXlldW16nptY3Nre8fe3euoOJWYtHHMYtkLkCKMCtLWVDPSSyRBPGCkG4yvCr7QKSisbjTk4R4HA0FjShG2kifXB9nnczwYc6ZHk2YiGee7bdafhTAEXiVuSOijR8u2vQRjjlBOhMUNK9V0n0V6GpKaYkbw2SBVJEB6jIekbKhAnysum6XN4bJQQRrE0T2g4VX9vZIgrNeGBmSxCqnmvEPzqmOLryMiiTVRODZoShlUMewqAKGVBKs2cQQhCU1WSEeIYmwNoXVTAnuJcXSee04ToN9as3rws66iCQ3AEToALzkET3IAWaAMMHsEzeAVv1pP1Yr1bH7PRilXu7IMsD5AI1Vleclatexitlatexit sha1_base64Vr1kZJh7jApVU4cGmtPimK4PPicAAABXicbVDLSsNAFJ3UV62vNi5GSyCq5KIoAsXBRe6rGAf0MYwmUzaoTOTMDMRagjihsXirj1P9z5N07aLLT1wMDhnHu5Z06QMKq043xblaXlldW16nptY3Nre8fe3euoOJWYtHHMYtkLkCKMCtLWVDPSSyRBPGCkG4yvCr7QKSisbjTk4R4HA0FjShG2kifXB9nnczwYc6ZHk2YiGee7bdafhTAEXiVuSOijR8u2vQRjjlBOhMUNK9V0n0V6GpKaYkbw2SBVJEB6jIekbKhAnysum6XN4bJQQRrE0T2g4VX9vZIgrNeGBmSxCqnmvEPzqmOLryMiiTVRODZoShlUMewqAKGVBKs2cQQhCU1WSEeIYmwNoXVTAnuJcXSee04ToN9as3rws66iCQ3AEToALzkET3IAWaAMMHsEzeAVv1pP1Yr1bH7PRilXu7IMsD5AI1Vleclatexitlatexit sha1_base64Vr1kZJh7jApVU4cGmtPimK4PPicAAABXicbVDLSsNAFJ3UV62vNi5GSyCq5KIoAsXBRe6rGAf0MYwmUzaoTOTMDMRagjihsXirj1P9z5N07aLLT1wMDhnHu5Z06QMKq043xblaXlldW16nptY3Nre8fe3euoOJWYtHHMYtkLkCKMCtLWVDPSSyRBPGCkG4yvCr7QKSisbjTk4R4HA0FjShG2kifXB9nnczwYc6ZHk2YiGee7bdafhTAEXiVuSOijR8u2vQRjjlBOhMUNK9V0n0V6GpKaYkbw2SBVJEB6jIekbKhAnysum6XN4bJQQRrE0T2g4VX9vZIgrNeGBmSxCqnmvEPzqmOLryMiiTVRODZoShlUMewqAKGVBKs2cQQhCU1WSEeIYmwNoXVTAnuJcXSee04ToN9as3rws66iCQ3AEToALzkET3IAWaAMMHsEzeAVv1pP1Yr1bH7PRilXu7IMsD5AI1VleclatexitGt1hid
latexit sha1_base64TTYaLKTJF7FnWkmBKBRepTsjYHcAAAB3icbVDLSsNAFJ34rPUVFdy4GSyCG0sigi5cFFzosoJ9QBvDZDJph85MwsxEKDELf8WNC0XchvuBsnbRbaemDgcM693DMnSBhV2nGrYXFpeWV1cpadX1jc2vb3tltqziVmLRwzGLZDZAijArS0lQz0k0kQTxgpBOMrgq80CkorG40OEeBwNBI0oRtpIvr1fZpEzf3sz5Heih5NqRhnvt2zak7E8B54pakBko0ffurH8Y45URozJBSPddJtJchqSlmJK2U0UShEdoQHqGCsSJ8rJJhweGSWEUSzNExpO1N8bGeJKjXlgJouQatYrxP8XqqjCyjIkk1EXh6KEoZ1DEsyoAhlQRrNjYEYUlNVoiHSCKsTWVVU4I7V50j6tu07dvT2rNS7LOirgAByCYCCc9AAN6AJWgCDRAMXsGb9WS9WOWx3R0wSp39sAfWJ8b2WWWQlatexitlatexit sha1_base64TTYaLKTJF7FnWkmBKBRepTsjYHcAAAB3icbVDLSsNAFJ34rPUVFdy4GSyCG0sigi5cFFzosoJ9QBvDZDJph85MwsxEKDELf8WNC0XchvuBsnbRbaemDgcM693DMnSBhV2nGrYXFpeWV1cpadX1jc2vb3tltqziVmLRwzGLZDZAijArS0lQz0k0kQTxgpBOMrgq80CkorG40OEeBwNBI0oRtpIvr1fZpEzf3sz5Heih5NqRhnvt2zak7E8B54pakBko0ffurH8Y45URozJBSPddJtJchqSlmJK2U0UShEdoQHqGCsSJ8rJJhweGSWEUSzNExpO1N8bGeJKjXlgJouQatYrxP8XqqjCyjIkk1EXh6KEoZ1DEsyoAhlQRrNjYEYUlNVoiHSCKsTWVVU4I7V50j6tu07dvT2rNS7LOirgAByCYCCc9AAN6AJWgCDRAMXsGb9WS9WOWx3R0wSp39sAfWJ8b2WWWQlatexitlatexit sha1_base64TTYaLKTJF7FnWkmBKBRepTsjYHcAAAB3icbVDLSsNAFJ34rPUVFdy4GSyCG0sigi5cFFzosoJ9QBvDZDJph85MwsxEKDELf8WNC0XchvuBsnbRbaemDgcM693DMnSBhV2nGrYXFpeWV1cpadX1jc2vb3tltqziVmLRwzGLZDZAijArS0lQz0k0kQTxgpBOMrgq80CkorG40OEeBwNBI0oRtpIvr1fZpEzf3sz5Heih5NqRhnvt2zak7E8B54pakBko0ffurH8Y45URozJBSPddJtJchqSlmJK2U0UShEdoQHqGCsSJ8rJJhweGSWEUSzNExpO1N8bGeJKjXlgJouQatYrxP8XqqjCyjIkk1EXh6KEoZ1DEsyoAhlQRrNjYEYUlNVoiHSCKsTWVVU4I7V50j6tu07dvT2rNS7LOirgAByCYCCc9AAN6AJWgCDRAMXsGb9WS9WOWx3R0wSp39sAfWJ8b2WWWQlatexitlatexit sha1_base64TTYaLKTJF7FnWkmBKBRepTsjYHcAAAB3icbVDLSsNAFJ34rPUVFdy4GSyCG0sigi5cFFzosoJ9QBvDZDJph85MwsxEKDELf8WNC0XchvuBsnbRbaemDgcM693DMnSBhV2nGrYXFpeWV1cpadX1jc2vb3tltqziVmLRwzGLZDZAijArS0lQz0k0kQTxgpBOMrgq80CkorG40OEeBwNBI0oRtpIvr1fZpEzf3sz5Heih5NqRhnvt2zak7E8B54pakBko0ffurH8Y45URozJBSPddJtJchqSlmJK2U0UShEdoQHqGCsSJ8rJJhweGSWEUSzNExpO1N8bGeJKjXlgJouQatYrxP8XqqjCyjIkk1EXh6KEoZ1DEsyoAhlQRrNjYEYUlNVoiHSCKsTWVVU4I7V50j6tu07dvT2rNS7LOirgAByCYCCc9AAN6AJWgCDRAMXsGb9WS9WOWx3R0wSp39sAfWJ8b2WWWQlatexitGtout
latexit sha1_base64Fm4Iz64LvWn082kf2Dq6sd1g9IAAAB3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNBlBfuANobJdNIOnWTCzI1YQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0gE1A439bK6tr6xmZlq7q9s7u3bxUOlqmirI2lUKqXkA0EzxmbeAgWC9RjESBYN1gcl343UemNJfxPUwT5kVkFPOQUwJG8u3azQP42SAiMFZRJlPIc9uOw1nBrxM3JLUUYmWb38NhpKmEYuBCqJ133US8DKigFPB8uog1SwhdEJGrG9oTCKmvWyWPccnRhniUCrzYsAz9fdGRiKtp1FgJouQetErxP8fgrhpZfxOEmBxXRKEwFBomLIvCQK0ZBTA0hVHGTFdMxUYSCqatqSnAXv7xMOmcN12m4df15lVZRwUdoWN0ilx0gZroFrVQG1H0hJ7RK3qzcuvFerc5qMrVrlziP7AvwB7zuUglatexitlatexit sha1_base64Fm4Iz64LvWn082kf2Dq6sd1g9IAAAB3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNBlBfuANobJdNIOnWTCzI1YQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0gE1A439bK6tr6xmZlq7q9s7u3bxUOlqmirI2lUKqXkA0EzxmbeAgWC9RjESBYN1gcl343UemNJfxPUwT5kVkFPOQUwJG8u3azQP42SAiMFZRJlPIc9uOw1nBrxM3JLUUYmWb38NhpKmEYuBCqJ133US8DKigFPB8uog1SwhdEJGrG9oTCKmvWyWPccnRhniUCrzYsAz9fdGRiKtp1FgJouQetErxP8fgrhpZfxOEmBxXRKEwFBomLIvCQK0ZBTA0hVHGTFdMxUYSCqatqSnAXv7xMOmcN12m4df15lVZRwUdoWN0ilx0gZroFrVQG1H0hJ7RK3qzcuvFerc5qMrVrlziP7AvwB7zuUglatexitlatexit sha1_base64Fm4Iz64LvWn082kf2Dq6sd1g9IAAAB3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNBlBfuANobJdNIOnWTCzI1YQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0gE1A439bK6tr6xmZlq7q9s7u3bxUOlqmirI2lUKqXkA0EzxmbeAgWC9RjESBYN1gcl343UemNJfxPUwT5kVkFPOQUwJG8u3azQP42SAiMFZRJlPIc9uOw1nBrxM3JLUUYmWb38NhpKmEYuBCqJ133US8DKigFPB8uog1SwhdEJGrG9oTCKmvWyWPccnRhniUCrzYsAz9fdGRiKtp1FgJouQetErxP8fgrhpZfxOEmBxXRKEwFBomLIvCQK0ZBTA0hVHGTFdMxUYSCqatqSnAXv7xMOmcN12m4df15lVZRwUdoWN0ilx0gZroFrVQG1H0hJ7RK3qzcuvFerc5qMrVrlziP7AvwB7zuUglatexitlatexit sha1_base64Fm4Iz64LvWn082kf2Dq6sd1g9IAAAB3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNBlBfuANobJdNIOnWTCzI1YQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0gE1A439bK6tr6xmZlq7q9s7u3bxUOlqmirI2lUKqXkA0EzxmbeAgWC9RjESBYN1gcl343UemNJfxPUwT5kVkFPOQUwJG8u3azQP42SAiMFZRJlPIc9uOw1nBrxM3JLUUYmWb38NhpKmEYuBCqJ133US8DKigFPB8uog1SwhdEJGrG9oTCKmvWyWPccnRhniUCrzYsAz9fdGRiKtp1FgJouQetErxP8fgrhpZfxOEmBxXRKEwFBomLIvCQK0ZBTA0hVHGTFdMxUYSCqatqSnAXv7xMOmcN12m4df15lVZRwUdoWN0ilx0gZroFrVQG1H0hJ7RK3qzcuvFerc5qMrVrlziP7AvwB7zuUglatexitGtinp
latexit sha1_base64KNSYXpeJERsrbPQxnbJ2F1T0AAAB3icbVDLSsNAFL3xWesr1qWbYBFclUQEXbgouNBlBfuANobJdNIOnZmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvcy5J0wYVdp1v62V1bX1jc3KVnV7Z3dv3z6odVScSkzaOGax7IVIEUYFaWuqGeklkiAeMtINJ9eF330kUtFY3OtpQnyORoJGFCNtpMCu3QTZgCM9ljyjIsnzBx3YdbfhzuAsE68kdSjRCuyvwTDGKSdCY4aU6ntuov0MSU0xI3l1kCqSIDxBI9I3VCBOlJNsufOiVGGThRL84R2ZurvjQxxpaY8NJNFTLXoFeJXjV0aVf3JRqIvD8oyhljo6doghnSCXBmk0NQVhSk9XBYyQR1qauqinBWzx5mXTOGp7b8O7O682rso4KHMExnIIHF9CEW2hBGzA8wTO8wpuVWyWuUxH12xyp1DAPr8wfV6pTtlatexitlatexit sha1_base64KNSYXpeJERsrbPQxnbJ2F1T0AAAB3icbVDLSsNAFL3xWesr1qWbYBFclUQEXbgouNBlBfuANobJdNIOnZmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvcy5J0wYVdp1v62V1bX1jc3KVnV7Z3dv3z6odVScSkzaOGax7IVIEUYFaWuqGeklkiAeMtINJ9eF330kUtFY3OtpQnyORoJGFCNtpMCu3QTZgCM9ljyjIsnzBx3YdbfhzuAsE68kdSjRCuyvwTDGKSdCY4aU6ntuov0MSU0xI3l1kCqSIDxBI9I3VCBOlJNsufOiVGGThRL84R2ZurvjQxxpaY8NJNFTLXoFeJXjV0aVf3JRqIvD8oyhljo6doghnSCXBmk0NQVhSk9XBYyQR1qauqinBWzx5mXTOGp7b8O7O682rso4KHMExnIIHF9CEW2hBGzA8wTO8wpuVWyWuUxH12xyp1DAPr8wfV6pTtlatexitlatexit sha1_base64KNSYXpeJERsrbPQxnbJ2F1T0AAAB3icbVDLSsNAFL3xWesr1qWbYBFclUQEXbgouNBlBfuANobJdNIOnZmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvcy5J0wYVdp1v62V1bX1jc3KVnV7Z3dv3z6odVScSkzaOGax7IVIEUYFaWuqGeklkiAeMtINJ9eF330kUtFY3OtpQnyORoJGFCNtpMCu3QTZgCM9ljyjIsnzBx3YdbfhzuAsE68kdSjRCuyvwTDGKSdCY4aU6ntuov0MSU0xI3l1kCqSIDxBI9I3VCBOlJNsufOiVGGThRL84R2ZurvjQxxpaY8NJNFTLXoFeJXjV0aVf3JRqIvD8oyhljo6doghnSCXBmk0NQVhSk9XBYyQR1qauqinBWzx5mXTOGp7b8O7O682rso4KHMExnIIHF9CEW2hBGzA8wTO8wpuVWyWuUxH12xyp1DAPr8wfV6pTtlatexitlatexit sha1_base64KNSYXpeJERsrbPQxnbJ2F1T0AAAB3icbVDLSsNAFL3xWesr1qWbYBFclUQEXbgouNBlBfuANobJdNIOnZmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvcy5J0wYVdp1v62V1bX1jc3KVnV7Z3dv3z6odVScSkzaOGax7IVIEUYFaWuqGeklkiAeMtINJ9eF330kUtFY3OtpQnyORoJGFCNtpMCu3QTZgCM9ljyjIsnzBx3YdbfhzuAsE68kdSjRCuyvwTDGKSdCY4aU6ntuov0MSU0xI3l1kCqSIDxBI9I3VCBOlJNsufOiVGGThRL84R2ZurvjQxxpaY8NJNFTLXoFeJXjV0aVf3JRqIvD8oyhljo6doghnSCXBmk0NQVhSk9XBYyQR1qauqinBWzx5mXTOGp7b8O7O682rso4KHMExnIIHF9CEW2hBGzA8wTO8wpuVWyWuUxH12xyp1DAPr8wfV6pTtlatexit c Recurrent GN architecture
Figure 6 a An example composing multiple GN blocks in sequence to form a GN core Here
the GN blocks can use shared weights or they could be independent b The encodeprocessdecode
architecture which is a common choice for composing GN blocks see Section 43 Here a GN
encodes an input graph which is then processed by a GN core The output of the core is decoded
by a third GN block into an output graph whose nodes edges andor global attributes would be
used for taskspecic purposes c The encodeprocessdecode architecture applied in a sequential
setting in which the core is also unrolled over time potentially using a GRU or LSTM architecture
in addition to being repeated within each time step Here merged lines indicate concatenation and
split lines indicate copying
43 Composable multiblock architectures
A key design principle of graph networks is constructing complex architectures by composing GN
blocks We dened a GN block as always taking a graph comprised of edge node and global
elements as input and returning a graph with the same constituent elements as output simply
passing through the input elements to the output when those elements are not explicitly updated
This graphtograph inputoutput interface ensures that the output of one GN block can be passed
as input to another even if their internal congurations are dierent similar to the tensortotensor
interface of the standard deep learning toolkit In the most basic form two GN blocks GN1and
GN2 can be composed as GN1GN2by passing the output of the rst as input to the second
G0 GN 2GN 1G
Arbitrary numbers of GN blocks can be composed as show in Figure 6a The blocks can
be unshared dierent functions andor parameters analogous to layers of a CNN GN16
GN266GNM or shared reused functions and parameters analogous to an unrolled RNN
GN1GN2GNM The white box around the GNcorein Figure 6a represents Mrepeated
internal processing substeps with either shared or unshared GN blocks Shared congurations
are analogous to messagepassing Gilmer et al 2017 where the same local update procedure is
applied iteratively to propagate information across the structure Figure 7 If we exclude the global
uwhich aggregates information from across the nodes and edges the information that a node
has access to after msteps of propagation is determined by the set of nodes and edges that are at
mostmhops away This can be interpreted as breaking down a complex computation into smaller
elementary steps The steps can also be used to capture sequentiality in time In our ballspring
example if each propagation step predicts the physical dynamics over one time step of duration  t
then theMpropagation steps result in a total simulation time of Mt
A common architecture design is what we call the encodeprocessdecode conguration Hamrick
et al 2018 also see Figure 6ba an input graph Ginpis transformed into a latent representation
G0 by an encoder GNenc a shared core block GNcore is applied Mtimes to return GM and
nally an output graph Gout is decoded by GNdec For example in our running example the
encoder might compute the initial forces and interaction energies between the balls the core might
19m0
latexit sha1_base64ic6WezPV7TWar890N1QnpVOPjNAAAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79ImjZNkDxotaCiquunuilIpLPrl1dYW9Y3Cpul3Z29YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324cWJHoB5ykPFR0qEUsGEUn3atrv1uDVAfKXBDmpQI5GvzZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4ved0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMvB9fAO9341Ylatexitlatexit sha1_base64ic6WezPV7TWar890N1QnpVOPjNAAAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79ImjZNkDxotaCiquunuilIpLPrl1dYW9Y3Cpul3Z29YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324cWJHoB5ykPFR0qEUsGEUn3atrv1uDVAfKXBDmpQI5GvzZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4ved0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMvB9fAO9341Ylatexitlatexit sha1_base64ic6WezPV7TWar890N1QnpVOPjNAAAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79ImjZNkDxotaCiquunuilIpLPrl1dYW9Y3Cpul3Z29YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324cWJHoB5ykPFR0qEUsGEUn3atrv1uDVAfKXBDmpQI5GvzZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4ved0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMvB9fAO9341Ylatexitlatexit sha1_base64ic6WezPV7TWar890N1QnpVOPjNAAAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79ImjZNkDxotaCiquunuilIpLPrl1dYW9Y3Cpul3Z29YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324cWJHoB5ykPFR0qEUsGEUn3atrv1uDVAfKXBDmpQI5GvzZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4ved0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMvB9fAO9341Ylatexitm1
latexit sha1_base64Zv5ybfKoH8QvXQaOWqQVyOFtg4AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79ImjZNkDxotaCiquunuilLBjfX9L6wtr6xuVXcLu3s7u0flAPWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2NbZxG14Yl6sJMUQ0mHisecUeuke3kd9MsVvYvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKFnNiSqUxExm5TkntyKx6cEr5vZCqccpVmFhVbLoozQWxC5nTAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMvB9fAOY41Zlatexitlatexit sha1_base64Zv5ybfKoH8QvXQaOWqQVyOFtg4AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79ImjZNkDxotaCiquunuilLBjfX9L6wtr6xuVXcLu3s7u0flAPWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2NbZxG14Yl6sJMUQ0mHisecUeuke3kd9MsVvYvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKFnNiSqUxExm5TkntyKx6cEr5vZCqccpVmFhVbLoozQWxC5nTAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMvB9fAOY41Zlatexitlatexit sha1_base64Zv5ybfKoH8QvXQaOWqQVyOFtg4AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79ImjZNkDxotaCiquunuilLBjfX9L6wtr6xuVXcLu3s7u0flAPWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2NbZxG14Yl6sJMUQ0mHisecUeuke3kd9MsVvYvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKFnNiSqUxExm5TkntyKx6cEr5vZCqccpVmFhVbLoozQWxC5nTAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMvB9fAOY41Zlatexitlatexit sha1_base64hP6LrUf2d3tZaldqaQQvEKMXywAAAB2XicbZDNSgMxFIXv1L86Vq1rN8EiuCozbnQpuHFZwbZCO5RM5k4bmskMyR2hDH0BF25EfC93vo3pz0JbDwQzknIvSculLQUBN9ebWd3bgfugfNfzjk9Nmo2fz0gjsilzl5jnmFpXU2CVJCp8LgzyLFfbj6f0i77gsTLXTzQrMMr4WMtUCk7O6oyaraAdLMW2IVxDC9YaNbGSS7KDDUJxa0dhEFBUcUNSaFw7g9LiwUXUz7GgUPNM7RRtRxzzi6dk7A0N5oYkv394uKZ9bOstjdzDhN7Ga2MPLBiWlt1EldVESarH6KC0Vo5wtdmaJNChIzRxwYaSblYkJN1yQa8Z3HYSbG29D77odBu3wMYA6nMMFXEEIN3AHD9CBLghI4BXevYn35n2suqp569LO4I8zx84xIo4latexitlatexit sha1_base6441wucjmacSzA8ipzMzk3JQ30CsAAAB33icbZDNSgMxFIXv1L9aq1a3boJFcFUybnQjCG5cVrS10A4lk95pQ5PMkGSEMvQR3LhQxLdy59uYiy09UDg45yE3HviTArrKP0OShubW9s75d3KXnX4LB2VG3bNDccWzyVqenEzKIUGltOOImdzCBTscSneHw7y5e0ViR6kc3yTBSbKhFIjhz3npQ12GVqcNOhdZh3AJdViq2a999QYpzxVqxyWzthvSzEUFM05widNKL7eYMT5mQx61EyhjYr5qFNy5p0BSVLjj3Zk7v5UTBl7UTFqZibmRXs5n5X9bNXXIVFUJnuUPNFx8luSQuJbO9yUAY5E5OPDBuhJV8BEzjDvfTsWXEK6uvA7ti0ZIGE9hTKcwCmcQwiXcAN30IQWcBjCC7zBeyCD1BjUVcpWPZ2DH8UfP4Arc6MHglatexitlatexit sha1_base6441wucjmacSzA8ipzMzk3JQ30CsAAAB33icbZDNSgMxFIXv1L9aq1a3boJFcFUybnQjCG5cVrS10A4lk95pQ5PMkGSEMvQR3LhQxLdy59uYiy09UDg45yE3HviTArrKP0OShubW9s75d3KXnX4LB2VG3bNDccWzyVqenEzKIUGltOOImdzCBTscSneHw7y5e0ViR6kc3yTBSbKhFIjhz3npQ12GVqcNOhdZh3AJdViq2a999QYpzxVqxyWzthvSzEUFM05widNKL7eYMT5mQx61EyhjYr5qFNy5p0BSVLjj3Zk7v5UTBl7UTFqZibmRXs5n5X9bNXXIVFUJnuUPNFx8luSQuJbO9yUAY5E5OPDBuhJV8BEzjDvfTsWXEK6uvA7ti0ZIGE9hTKcwCmcQwiXcAN30IQWcBjCC7zBeyCD1BjUVcpWPZ2DH8UfP4Arc6MHglatexitlatexit sha1_base64c5O5JuZLnHr2YYnYcAra2hwAn0AAAB6nicbVA9SwNBEJ2LXzFRS1tFoOQKtzZaCMEbCwjmg9IjrC3mSRLdveO3T0hHPkJNhaK2PqL7Pw3bpIrNPHBwOO9GWbmRYngxvrt1fY2Nza3inulvb2Dw6PyscnLROnmmGTxSLWnYgaFFxh03IrsJNopDIS2I4mt3OYTa8Fg92mmCoaQjxYecUeukB3kT9MsVvYvQNZJkJMK5Gj0y19QcxSicoyQY3pBn5iw4xqy5nAWamXGkwom9ARdh1VVKIJs8WpM3LhlAEZxtqVsmShp7IqDRmKiPXKakdm1VvLv7ndVM7vA4zrpLUomLLRcNUEBuTd9kwDUyK6aOUKa5u5WwMdWUWZdOyYUQrL68TlqXtcCvBfdpV7N4yjCGZxDFQK4gjrcQQOawGAEzAKb57wXrx372PZWvDymVP4AzB74jjVUlatexitlatexit sha1_base64Zv5ybfKoH8QvXQaOWqQVyOFtg4AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79ImjZNkDxotaCiquunuilLBjfX9L6wtr6xuVXcLu3s7u0flAPWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2NbZxG14Yl6sJMUQ0mHisecUeuke3kd9MsVvYvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKFnNiSqUxExm5TkntyKx6cEr5vZCqccpVmFhVbLoozQWxC5nTAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMvB9fAOY41Zlatexitlatexit sha1_base64Zv5ybfKoH8QvXQaOWqQVyOFtg4AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79ImjZNkDxotaCiquunuilLBjfX9L6wtr6xuVXcLu3s7u0flAPWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2NbZxG14Yl6sJMUQ0mHisecUeuke3kd9MsVvYvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKFnNiSqUxExm5TkntyKx6cEr5vZCqccpVmFhVbLoozQWxC5nTAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMvB9fAOY41Zlatexitlatexit sha1_base64Zv5ybfKoH8QvXQaOWqQVyOFtg4AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79ImjZNkDxotaCiquunuilLBjfX9L6wtr6xuVXcLu3s7u0flAPWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2NbZxG14Yl6sJMUQ0mHisecUeuke3kd9MsVvYvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKFnNiSqUxExm5TkntyKx6cEr5vZCqccpVmFhVbLoozQWxC5nTAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMvB9fAOY41Zlatexitlatexit sha1_base64Zv5ybfKoH8QvXQaOWqQVyOFtg4AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79ImjZNkDxotaCiquunuilLBjfX9L6wtr6xuVXcLu3s7u0flAPWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2NbZxG14Yl6sJMUQ0mHisecUeuke3kd9MsVvYvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKFnNiSqUxExm5TkntyKx6cEr5vZCqccpVmFhVbLoozQWxC5nTAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMvB9fAOY41Zlatexitlatexit sha1_base64Zv5ybfKoH8QvXQaOWqQVyOFtg4AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79ImjZNkDxotaCiquunuilLBjfX9L6wtr6xuVXcLu3s7u0flAPWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2NbZxG14Yl6sJMUQ0mHisecUeuke3kd9MsVvYvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKFnNiSqUxExm5TkntyKx6cEr5vZCqccpVmFhVbLoozQWxC5nTAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMvB9fAOY41Zlatexitlatexit sha1_base64Zv5ybfKoH8QvXQaOWqQVyOFtg4AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79ImjZNkDxotaCiquunuilLBjfX9L6wtr6xuVXcLu3s7u0flAPWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2NbZxG14Yl6sJMUQ0mHisecUeuke3kd9MsVvYvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKFnNiSqUxExm5TkntyKx6cEr5vZCqccpVmFhVbLoozQWxC5nTAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMvB9fAOY41Zlatexitm2
latexit sha1_base64oVNcNejQAmVb9Nn6VIeeV7f50lsAAAB6nicbVBNSwMxEJ3Ur1qqh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3CFwKePUXefPfmLZ70NYHA43ZpiZFyaCGt536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5UxUK17NWwCvEz8nFcjRHJSsOYppIpSwUxpud7iQ0yoi2ngs1KdSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F7xeaqPrIOMqSS1TdLkoSgW2MZ7jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZpf1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541alatexitlatexit sha1_base64oVNcNejQAmVb9Nn6VIeeV7f50lsAAAB6nicbVBNSwMxEJ3Ur1qqh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3CFwKePUXefPfmLZ70NYHA43ZpiZFyaCGt536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5UxUK17NWwCvEz8nFcjRHJSsOYppIpSwUxpud7iQ0yoi2ngs1KdSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F7xeaqPrIOMqSS1TdLkoSgW2MZ7jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZpf1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541alatexitlatexit sha1_base64oVNcNejQAmVb9Nn6VIeeV7f50lsAAAB6nicbVBNSwMxEJ3Ur1qqh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3CFwKePUXefPfmLZ70NYHA43ZpiZFyaCGt536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5UxUK17NWwCvEz8nFcjRHJSsOYppIpSwUxpud7iQ0yoi2ngs1KdSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F7xeaqPrIOMqSS1TdLkoSgW2MZ7jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZpf1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541alatexitlatexit sha1_base64oVNcNejQAmVb9Nn6VIeeV7f50lsAAAB6nicbVBNSwMxEJ3Ur1qqh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3CFwKePUXefPfmLZ70NYHA43ZpiZFyaCGt536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5UxUK17NWwCvEz8nFcjRHJSsOYppIpSwUxpud7iQ0yoi2ngs1KdSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F7xeaqPrIOMqSS1TdLkoSgW2MZ7jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZpf1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541alatexitm3
latexit sha1_base64jOsYdJ9SNJymWDVPaXJffNRSPMAAAB6nicbVBNSwMxEJ3Ur1qqh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpTBiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9Y3Cpul3Z29YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6JeeoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezXrJSrZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4QQ5wCa41blatexitlatexit sha1_base64jOsYdJ9SNJymWDVPaXJffNRSPMAAAB6nicbVBNSwMxEJ3Ur1qqh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpTBiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9Y3Cpul3Z29YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6JeeoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezXrJSrZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4QQ5wCa41blatexitlatexit sha1_base64jOsYdJ9SNJymWDVPaXJffNRSPMAAAB6nicbVBNSwMxEJ3Ur1qqh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpTBiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9Y3Cpul3Z29YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6JeeoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezXrJSrZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4QQ5wCa41blatexitlatexit sha1_base64jOsYdJ9SNJymWDVPaXJffNRSPMAAAB6nicbVBNSwMxEJ3Ur1qqh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpTBiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9Y3Cpul3Z29YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6JeeoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezXrJSrZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4QQ5wCa41blatexit
m0
latexit sha1_base64ic6WezPV7TWar890N1QnpVOPjNAAAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79ImjZNkDxotaCiquunuilIpLPrl1dYW9Y3Cpul3Z29YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324cWJHoB5ykPFR0qEUsGEUn3atrv1uDVAfKXBDmpQI5GvzZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4ved0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMvB9fAO9341Ylatexitlatexit sha1_base64ic6WezPV7TWar890N1QnpVOPjNAAAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79ImjZNkDxotaCiquunuilIpLPrl1dYW9Y3Cpul3Z29YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324cWJHoB5ykPFR0qEUsGEUn3atrv1uDVAfKXBDmpQI5GvzZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4ved0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMvB9fAO9341Ylatexitlatexit sha1_base64ic6WezPV7TWar890N1QnpVOPjNAAAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79ImjZNkDxotaCiquunuilIpLPrl1dYW9Y3Cpul3Z29YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324cWJHoB5ykPFR0qEUsGEUn3atrv1uDVAfKXBDmpQI5GvzZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4ved0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMvB9fAO9341Ylatexitlatexit sha1_base64ic6WezPV7TWar890N1QnpVOPjNAAAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79ImjZNkDxotaCiquunuilIpLPrl1dYW9Y3Cpul3Z29YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324cWJHoB5ykPFR0qEUsGEUn3atrv1uDVAfKXBDmpQI5GvzZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4ved0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMvB9fAO9341Ylatexitm1
latexit sha1_base64Zv5ybfKoH8QvXQaOWqQVyOFtg4AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79ImjZNkDxotaCiquunuilLBjfX9L6wtr6xuVXcLu3s7u0flAPWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2NbZxG14Yl6sJMUQ0mHisecUeuke3kd9MsVvYvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKFnNiSqUxExm5TkntyKx6cEr5vZCqccpVmFhVbLoozQWxC5nTAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMvB9fAOY41Zlatexitlatexit sha1_base64Zv5ybfKoH8QvXQaOWqQVyOFtg4AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79ImjZNkDxotaCiquunuilLBjfX9L6wtr6xuVXcLu3s7u0flAPWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2NbZxG14Yl6sJMUQ0mHisecUeuke3kd9MsVvYvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKFnNiSqUxExm5TkntyKx6cEr5vZCqccpVmFhVbLoozQWxC5nTAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMvB9fAOY41Zlatexitlatexit sha1_base64Zv5ybfKoH8QvXQaOWqQVyOFtg4AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79ImjZNkDxotaCiquunuilLBjfX9L6wtr6xuVXcLu3s7u0flAPWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2NbZxG14Yl6sJMUQ0mHisecUeuke3kd9MsVvYvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKFnNiSqUxExm5TkntyKx6cEr5vZCqccpVmFhVbLoozQWxC5nTAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMvB9fAOY41Zlatexitlatexit sha1_base64hP6LrUf2d3tZaldqaQQvEKMXywAAAB2XicbZDNSgMxFIXv1L86Vq1rN8EiuCozbnQpuHFZwbZCO5RM5k4bmskMyR2hDH0BF25EfC93vo3pz0JbDwQzknIvSculLQUBN9ebWd3bgfugfNfzjk9Nmo2fz0gjsilzl5jnmFpXU2CVJCp8LgzyLFfbj6f0i77gsTLXTzQrMMr4WMtUCk7O6oyaraAdLMW2IVxDC9YaNbGSS7KDDUJxa0dhEFBUcUNSaFw7g9LiwUXUz7GgUPNM7RRtRxzzi6dk7A0N5oYkv394uKZ9bOstjdzDhN7Ga2MPLBiWlt1EldVESarH6KC0Vo5wtdmaJNChIzRxwYaSblYkJN1yQa8Z3HYSbG29D77odBu3wMYA6nMMFXEEIN3AHD9CBLghI4BXevYn35n2suqp569LO4I8zx84xIo4latexitlatexit sha1_base6441wucjmacSzA8ipzMzk3JQ30CsAAAB33icbZDNSgMxFIXv1L9aq1a3boJFcFUybnQjCG5cVrS10A4lk95pQ5PMkGSEMvQR3LhQxLdy59uYiy09UDg45yE3HviTArrKP0OShubW9s75d3KXnX4LB2VG3bNDccWzyVqenEzKIUGltOOImdzCBTscSneHw7y5e0ViR6kc3yTBSbKhFIjhz3npQ12GVqcNOhdZh3AJdViq2a999QYpzxVqxyWzthvSzEUFM05widNKL7eYMT5mQx61EyhjYr5qFNy5p0BSVLjj3Zk7v5UTBl7UTFqZibmRXs5n5X9bNXXIVFUJnuUPNFx8luSQuJbO9yUAY5E5OPDBuhJV8BEzjDvfTsWXEK6uvA7ti0ZIGE9hTKcwCmcQwiXcAN30IQWcBjCC7zBeyCD1BjUVcpWPZ2DH8UfP4Arc6MHglatexitlatexit sha1_base6441wucjmacSzA8ipzMzk3JQ30CsAAAB33icbZDNSgMxFIXv1L9aq1a3boJFcFUybnQjCG5cVrS10A4lk95pQ5PMkGSEMvQR3LhQxLdy59uYiy09UDg45yE3HviTArrKP0OShubW9s75d3KXnX4LB2VG3bNDccWzyVqenEzKIUGltOOImdzCBTscSneHw7y5e0ViR6kc3yTBSbKhFIjhz3npQ12GVqcNOhdZh3AJdViq2a999QYpzxVqxyWzthvSzEUFM05widNKL7eYMT5mQx61EyhjYr5qFNy5p0BSVLjj3Zk7v5UTBl7UTFqZibmRXs5n5X9bNXXIVFUJnuUPNFx8luSQuJbO9yUAY5E5OPDBuhJV8BEzjDvfTsWXEK6uvA7ti0ZIGE9hTKcwCmcQwiXcAN30IQWcBjCC7zBeyCD1BjUVcpWPZ2DH8UfP4Arc6MHglatexitlatexit sha1_base64c5O5JuZLnHr2YYnYcAra2hwAn0AAAB6nicbVA9SwNBEJ2LXzFRS1tFoOQKtzZaCMEbCwjmg9IjrC3mSRLdveO3T0hHPkJNhaK2PqL7Pw3bpIrNPHBwOO9GWbmRYngxvrt1fY2Nza3inulvb2Dw6PyscnLROnmmGTxSLWnYgaFFxh03IrsJNopDIS2I4mt3OYTa8Fg92mmCoaQjxYecUeukB3kT9MsVvYvQNZJkJMK5Gj0y19QcxSicoyQY3pBn5iw4xqy5nAWamXGkwom9ARdh1VVKIJs8WpM3LhlAEZxtqVsmShp7IqDRmKiPXKakdm1VvLv7ndVM7vA4zrpLUomLLRcNUEBuTd9kwDUyK6aOUKa5u5WwMdWUWZdOyYUQrL68TlqXtcCvBfdpV7N4yjCGZxDFQK4gjrcQQOawGAEzAKb57wXrx372PZWvDymVP4AzB74jjVUlatexitlatexit sha1_base64Zv5ybfKoH8QvXQaOWqQVyOFtg4AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79ImjZNkDxotaCiquunuilLBjfX9L6wtr6xuVXcLu3s7u0flAPWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2NbZxG14Yl6sJMUQ0mHisecUeuke3kd9MsVvYvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKFnNiSqUxExm5TkntyKx6cEr5vZCqccpVmFhVbLoozQWxC5nTAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMvB9fAOY41Zlatexitlatexit sha1_base64Zv5ybfKoH8QvXQaOWqQVyOFtg4AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79ImjZNkDxotaCiquunuilLBjfX9L6wtr6xuVXcLu3s7u0flAPWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2NbZxG14Yl6sJMUQ0mHisecUeuke3kd9MsVvYvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKFnNiSqUxExm5TkntyKx6cEr5vZCqccpVmFhVbLoozQWxC5nTAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMvB9fAOY41Zlatexitlatexit sha1_base64Zv5ybfKoH8QvXQaOWqQVyOFtg4AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79ImjZNkDxotaCiquunuilLBjfX9L6wtr6xuVXcLu3s7u0flAPWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2NbZxG14Yl6sJMUQ0mHisecUeuke3kd9MsVvYvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKFnNiSqUxExm5TkntyKx6cEr5vZCqccpVmFhVbLoozQWxC5nTAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMvB9fAOY41Zlatexitlatexit sha1_base64Zv5ybfKoH8QvXQaOWqQVyOFtg4AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79ImjZNkDxotaCiquunuilLBjfX9L6wtr6xuVXcLu3s7u0flAPWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2NbZxG14Yl6sJMUQ0mHisecUeuke3kd9MsVvYvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKFnNiSqUxExm5TkntyKx6cEr5vZCqccpVmFhVbLoozQWxC5nTAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMvB9fAOY41Zlatexitlatexit sha1_base64Zv5ybfKoH8QvXQaOWqQVyOFtg4AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79ImjZNkDxotaCiquunuilLBjfX9L6wtr6xuVXcLu3s7u0flAPWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2NbZxG14Yl6sJMUQ0mHisecUeuke3kd9MsVvYvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKFnNiSqUxExm5TkntyKx6cEr5vZCqccpVmFhVbLoozQWxC5nTAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMvB9fAOY41Zlatexitlatexit sha1_base64Zv5ybfKoH8QvXQaOWqQVyOFtg4AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79ImjZNkDxotaCiquunuilLBjfX9L6wtr6xuVXcLu3s7u0flAPWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2NbZxG14Yl6sJMUQ0mHisecUeuke3kd9MsVvYvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKFnNiSqUxExm5TkntyKx6cEr5vZCqccpVmFhVbLoozQWxC5nTAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwVLbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMvB9fAOY41Zlatexitm2
latexit sha1_base64oVNcNejQAmVb9Nn6VIeeV7f50lsAAAB6nicbVBNSwMxEJ3Ur1qqh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3CFwKePUXefPfmLZ70NYHA43ZpiZFyaCGt536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5UxUK17NWwCvEz8nFcjRHJSsOYppIpSwUxpud7iQ0yoi2ngs1KdSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F7xeaqPrIOMqSS1TdLkoSgW2MZ7jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZpf1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541alatexitlatexit sha1_base64oVNcNejQAmVb9Nn6VIeeV7f50lsAAAB6nicbVBNSwMxEJ3Ur1qqh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3CFwKePUXefPfmLZ70NYHA43ZpiZFyaCGt536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5UxUK17NWwCvEz8nFcjRHJSsOYppIpSwUxpud7iQ0yoi2ngs1KdSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F7xeaqPrIOMqSS1TdLkoSgW2MZ7jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZpf1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541alatexitlatexit sha1_base64oVNcNejQAmVb9Nn6VIeeV7f50lsAAAB6nicbVBNSwMxEJ3Ur1qqh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3CFwKePUXefPfmLZ70NYHA43ZpiZFyaCGt536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5UxUK17NWwCvEz8nFcjRHJSsOYppIpSwUxpud7iQ0yoi2ngs1KdSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F7xeaqPrIOMqSS1TdLkoSgW2MZ7jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZpf1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541alatexitlatexit sha1_base64oVNcNejQAmVb9Nn6VIeeV7f50lsAAAB6nicbVBNSwMxEJ3Ur1qqh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3CFwKePUXefPfmLZ70NYHA43ZpiZFyaCGt536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5UxUK17NWwCvEz8nFcjRHJSsOYppIpSwUxpud7iQ0yoi2ngs1KdSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F7xeaqPrIOMqSS1TdLkoSgW2MZ7jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZpf1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541alatexitm3
latexit sha1_base64jOsYdJ9SNJymWDVPaXJffNRSPMAAAB6nicbVBNSwMxEJ3Ur1qqh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpTBiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9Y3Cpul3Z29YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6JeeoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezXrJSrZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4QQ5wCa41blatexitlatexit sha1_base64jOsYdJ9SNJymWDVPaXJffNRSPMAAAB6nicbVBNSwMxEJ3Ur1qqh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpTBiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9Y3Cpul3Z29YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6JeeoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezXrJSrZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4QQ5wCa41blatexitlatexit sha1_base64jOsYdJ9SNJymWDVPaXJffNRSPMAAAB6nicbVBNSwMxEJ3Ur1qqh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpTBiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9Y3Cpul3Z29YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6JeeoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezXrJSrZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4QQ5wCa41blatexitlatexit sha1_base64jOsYdJ9SNJymWDVPaXJffNRSPMAAAB6nicbVBNSwMxEJ3Ur1qqh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpTBiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9Y3Cpul3Z29YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6JeeoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezXrJSrZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4QQ5wCa41blatexitFigure 7 Example of message passing Each row highlights the information that diuses through
the graph starting from a particular node In the top row the node of interest is in the upper
right in the bottom row the node of interest is in the bottom right Shaded nodes indicate how far
information from the original node can travel in msteps of message passing bolded edges indicate
which edges that information has the potential to travel across Note that during the full message
passing procedure this propagation of information happens simultaneously for all nodes and edges
in the graph not just the two shown here
apply an elementary dynamics update and the decoder might read out the nal positions from the
updated graph state
Similar to the encodeprocessdecode design recurrent GNbased architectures can be built by
maintaining a hidden graph Gt
hid taking as input an observed graph Gt
inp and returning an output
graphGt
out on each step see Figure 6c This type of architecture can be particularly useful for
predicting sequences of graphs such as predicting the trajectory of a dynamical system over time
eg SanchezGonzalez et al 2018 The encoded graph output by GNenc must have the same
structure as Gt
hid and they can be easily combined by concatenating their corresponding ekvi
anduvectors where the upward arrow merges into the lefthand horizontal arrow in Figure 6c
before being passed to GNcore For the output the Gt
hidis copied where the righthand horizontal
arrow splits into the downward arrow in Figure 6c and decoded by GNdec This design reuses GN
blocks in several ways GNencGNdec and GNcoreare shared across each step t and within each
step GN coremay perform multiple shared substeps
Various other techniques for designing GNbased architectures can be useful Graph skip
connections for example would concatenate a GN blocks input graph Gm with its output graph
Gm1 before proceeding to further computations Merging and smoothing input and hidden graph
information as in Figure 6c can use LSTM or GRUstyle gating schemes instead of simple
concatenation Li et al 2016 Or distinct recurrent GN blocks eg Figure 4b can be composed
before andor after other GN blocks to improve stability in the representations over multiple
propagation steps SanchezGonzalez et al 2018
44 Implementing graph networks in code
Similar to CNNs see Figure 1 which are naturally parallelizable eg on GPUs GNs have a
natural parallel structure since the eandvfunctions in Equation 1 are shared over the edges
and nodes respectively they can be computed in parallel In practice this means that with respect
20Box 4 Graph Nets opensource software library githubcomdeepmindgraph nets
We have released an opensource library for building GNs in TensorowSonnet It includes
demos of how to create manipulate and train GNs to reason about graphstructured data on
a shortest pathnding task a sorting task and a physical prediction task Each demo uses the
same GN architecture which highlights the exibility of the approach
Shortest path demo tinyurlcomgnshortestpathdemo
This demo creates random graphs and trains a GN to label the nodes and edges on the shortest
path between any two nodes Over a sequence of messagepassing steps as depicted by each
steps plot the model renes its prediction of the shortest path
Sort demo tinyurlcomgnsortdemo
This demo creates lists of random numbers and trains a GN to sort the list After a sequence
of messagepassing steps the model makes an accurate prediction of which elements columns
in the gure come next after each other rows
Physics demo tinyurlcomgnphysicsdemo
This demo creates random massspring physical systems and trains a GN to predict the state of
the system on the next timestep The models nextstep predictions can be fed back in as input
to create a rollout of a future trajectory Each subplot below shows the true and predicted
massspring system states over 50 timesteps This is similar to the model and experiments in
Battaglia et al 2016s interaction networks
21toeandv the nodes and edges can be treated like the batch dimension in typical minibatch
training regimes Moreover several graphs can be naturally batched together by treating them as
disjoint components of a larger graph With some additional bookkeeping this allows batching
together the computations made on several independent graphs
Reusingeandvalso improves GNs sample eciency Again analogous to a convolutional
kernel the number of samples which are used to optimize a GNs eandvfunctions is the number
of edges and nodes respectively across all training graphs For example in the balls example
from Sec 32 a scene with four balls which are all connected by springs will provide twelve 4 3
examples of the contact interaction between them
We have released an opensource software library for building GNs which can be found here
githubcomdeepmindgraph nets  See Box 4 for an overview
45 Summary
In this section we have discussed the design principles behind graph networks exible representa
tions congurable withinblock structure and composable multiblock architectures These three
design principles combine in our framework which is extremely exible and applicable to a wide range
of domains ranging from perception language and symbolic reasoning And as we will see in the
remainder of this paper the strong relational inductive bias possessed by graph networks supports
combinatorial generalization thus making it a powerful tool both in terms of implementation and
theory
5 Discussion
In this paper we analyzed the extent to which relational inductive bias exists in deep learning
architectures like MLPs CNNs and RNNs and concluded that while CNNs and RNNs do contain
relational inductive biases they cannot naturally handle more structured representations such as
sets or graphs We advocated for building stronger relational inductive biases into deep learning
architectures by highlighting an underused deep learning building block called a graph network 
which performs computations over graphstructured data Our graph network framework unies
existing approaches that also operate over graphs and provides a straightforward interface for
assembling graph networks into complex sophisticated architectures
51 Combinatorial generalization in graph networks
The structure of GNs naturally supports combinatorial generalization because they do not perform
computations strictly at the system level but also apply shared computations across the entities and
across the relations as well This allows neverbeforeseen systems to be reasoned about because
they are built from familiar components in a way that reects von Humboldts innite use of nite
means Humboldt 1836 Chomsky 1965
A number of studies have explored GNs capacity for combinatorial generalization Battaglia
et al 2016 found that GNs trained to make onestep physical state predictions could simulate
thousands of future time steps and also exhibit accurate zeroshot transfer to physical systems
with double or half the number of entities experienced during training SanchezGonzalez et al
2018 found similar results in more complex physical control settings including that GNs trained as
forward models on simulated multijoint agents could generalize to agents with new numbers of joints
Hamrick et al 2018 and Wang et al 2018b each found that GNbased decisionmaking policies
could transfer to novel numbers of entities as well In combinatorial optimization problems Bello
22et al 2016 Nowak et al 2017 Dai et al 2017 Kool and Welling 2018 showed that GNs could
generalize well to problems of much dierent sizes than they had been trained on Similarly Toyer
et al 2017 showed generalization to dierent sizes of planning problems and Hamilton et al 2017
showed generalization to producing useful node embeddings for previously unseen data On boolean
SAT problems Selsam et al 2018 demonstrated generalization both to dierent problem sizes and
across problem distributions their model retained good performance upon strongly modifying the
distribution of the input graphs and its typical local structure
These striking examples of combinatorial generalization are not entirely surprising given GNs
entity and relationcentric organization but nonetheless provide important support for the view
that embracing explicit structure and exible learning is a viable approach toward realizing better
sample eciency and generalization in modern AI
52 Limitations of graph networks
One limitation of GNs and MPNNs form of learned messagepassing Shervashidze et al 2011
is that it cannot be guaranteed to solve some classes of problems such as discriminating between
certain nonisomorphic graphs Kondor et al 2018 suggested that covariance7Cohen and Welling
2016 Kondor and Trivedi 2018 rather than invariance to permutations of the nodes and edges
is preferable and proposed covariant compositional networks which can preserve structural
information and allow it to be ignored only if desired
More generally while graphs are a powerful way of representing structure information they
have limits For example notions like recursion control ow and conditional iteration are not
straightforward to represent with graphs and minimally require additional assumptions eg in
interpreting abstract syntax trees Programs and more computerlike processing can oer greater
representational and computational expressivity with respect to these notions and some have argued
they are an important component of human cognition Tenenbaum et al 2011 Lake et al 2015
Goodman et al 2015
53 Open questions
Although we are excited about the potential impacts that graph networks can have we caution that
these models are only one step forward Realizing the full potential of graph networks will likely be
far more challenging than organizing their behavior under one framework and indeed there are a
number of unanswered questions regarding the best ways to use graph networks
One pressing question is where do the graphs come from that graph networks operate over
One of the hallmarks of deep learning has been its ability to perform complex computations over
raw sensory data such as images and text yet it is unclear the best ways to convert sensory data
into more structured representations like graphs One approach which we have already discussed
assumes a fully connected graph structure between spatial or linguistic entities such as in the
literature on selfattention Vaswani et al 2017 Wang et al 2018c However such representations
may not correspond exactly to the true entities eg convolutional features do not directly
correspond to objects in a scene Moreover many underlying graph structures are much more
sparse than a fully connected graph and it is an open question how to induce this sparsity Several
lines of active research are exploring these issues Watters et al 2017 van Steenkiste et al 2018
Li et al 2018 Kipf et al 2018 but as of yet there is no single method which can reliably extract
discrete entities from sensory data Developing such a method is an exciting challenge for future
7Covariance means roughly that the activations vary in a predictable way as a function of the ordering of the
incoming edges
23research and once solved will likely open the door for much more powerful and exible reasoning
algorithms
A related question is how to adaptively modify graph structures during the course of computation
For example if an object fractures into multiple pieces a node representing that object also ought
to split into multiple nodes Similarly it might be useful to only represent edges between objects
that are in contact thus requiring the ability to add or remove edges depending on context The
question of how to support this type of adaptivity is also actively being researched and in particular
some of the methods used for identifying the underlying structure of a graph may be applicable eg
Li et al 2018 Kipf et al 2018
Human cognition makes the strong assumption that the world is composed of objects and
relations Spelke and Kinzler 2007 and because GNs make a similar assumption their behavior
tends to be more interpretable The entities and relations that GNs operate over often correspond
to things that humans understand such as physical objects thus supporting more interpretable
analysis and visualization eg as in Selsam et al 2018 An interesting direction for future work
is to further explore the interpretability of the behavior of graph networks
54 Integrative approaches for learning and structure
While our focus here has been on graphs one takeaway from this paper is less about graphs
themselves and more about the approach of blending powerful deep learning approaches with
structured representations We are excited by related approaches which have explored this idea for
other types of structured representations and computations such as linguistic trees Socher et al
2011ab 2012 2013 Tai et al 2015 Andreas et al 2016 partial tree traversals in a stateaction
graph Guez et al 2018 Farquhar et al 2018 hierarchical action policies Andreas et al 2017
multiagent communication channels Foerster et al 2016 capsules Sabour et al 2017 and
programs Parisotto et al 2017 Other methods have attempted to capture dierent types of
structure by mimicking key hardware and software components in computers and how they transfer
information between each other such as persistent slotted storage registers memory IO controllers
stacks and queues eg Dyer et al 2015 Grefenstette et al 2015 Joulin and Mikolov 2015
Sukhbaatar et al 2015 Kurach et al 2016 Graves et al 2016
55 Conclusion
Recent advances in AI propelled by deep learning have been transformative across many important
domains Despite this a vast gap between human and machine intelligence remains especially with
respect to ecient generalizable learning We argue for making combinatorial generalization a top
priority for AI and advocate for embracing integrative approaches which draw on ideas from human
cognition traditional computer science standard engineering practice and modern deep learning
Here we explored exible learningbased approaches which implement strong relational inductive
biases to capitalize on explicitly structured representations and computations and presented a
framework called graph networks  which generalize and extend various recent approaches for neural
networks applied to graphs Graph networks are designed to promote building complex architectures
using customizable graphtograph building blocks and their relational inductive biases promote
combinatorial generalization and improved sample eciency over other standard machine learning
building blocks
Despite their benets and potential however learnable models which operate on graphs are
only a stepping stone on the path toward humanlike intelligence We are optimistic about a
number of other relevant and perhaps underappreciated research directions including marrying
24learningbased approaches with programs Ritchie et al 2016 Andreas et al 2016 Gaunt et al
2016 Evans and Grefenstette 2018 Evans et al 2018 developing modelbased approaches with an
emphasis on abstraction Kansky et al 2017 Konidaris et al 2018 Zhang et al 2018 Hay et al
2018 investing more heavily in metalearning Wang et al 2016 2018a Finn et al 2017 and
exploring multiagent learning and interaction as a key catalyst for advanced intelligence Nowak
2006 Ohtsuki et al 2006 These directions each involve rich notions of entities relations and
combinatorial generalization and can potentially benet and benet from greater interaction with
approaches for learning relational reasoning over explicitly structured representations
Acknowledgements
We thank Tobias Pfa Danilo Rezende Nando de Freitas Murray Shanahan Thore Graepel John
Jumper Demis Hassabis and the broader DeepMind and Google communities for valuable feedback
and support
References
Allamanis M Brockschmidt M and Khademi M 2018 Learning to represent programs with
graphs In Proceedings of the International Conference on Learning Representations ICLR 
Allamanis M Chanthirasegaran P Kohli P and Sutton C 2017 Learning continuous
semantic representations of symbolic expressions In Proceedings of the International Conference
on Machine Learning ICML 
Anderson J R 1982 Acquisition of cognitive skill Psychological Review  894369
Andreas J Klein D and Levine S 2017 Modular multitask reinforcement learning with policy
sketches In Proceedings of the International Conference on Machine Learning ICML 
Andreas J Rohrbach M Darrell T and Klein D 2016 Neural module networks In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition CVPR  pages 3948
Ba J L Kiros J R and Hinton G E 2016 Layer normalization arXiv preprint
arXiv160706450 
Bahdanau D Cho K and Bengio Y 2015 Neural machine translation by jointly learning to
align and translate In Proceedings of the International Conference on Learning Representations
ICLR 
Battaglia P Pascanu R Lai M Rezende D J et al 2016 Interaction networks for learning
about objects relations and physics In Advances in Neural Information Processing Systems 
pages 45024510
Battaglia P W Hamrick J B and Tenenbaum J B 2013 Simulation as an engine of physical
scene understanding Proceedings of the National Academy of Sciences  110451832718332
Bello I Pham H Le Q V Norouzi M and Bengio S 2016 Neural combinatorial optimization
with reinforcement learning arXiv preprint arXiv161109940 
Bobrow D G and Hinton G E editors 1990 Articial Intelligence  volume 46 Elsevier Science
Publishers Ltd Essex UK Special Issue 12 On Connectionist Symbol Processing
25Bojchevski A Shchur O Z ugner D and G unnemann S 2018 Netgan Generating graphs via
random walks arXiv preprint arXiv180300816 
Bordes A Usunier N GarciaDuran A Weston J and Yakhnenko O 2013 Translating
embeddings for modeling multirelational data In Advances in Neural Information Processing
Systems  pages 27872795
Botvinick M M 2008 Hierarchical models of behavior and prefrontal function Trends in
Cognitive Sciences  125201208
Bronstein M M Bruna J LeCun Y Szlam A and Vandergheynst P 2017 Geometric deep
learning going beyond euclidean data IEEE Signal Processing Magazine  3441842
Bruna J Zaremba W Szlam A and LeCun Y 2014 Spectral networks and locally connected
networks on graphs In Proceedings of the International Conference on Learning Representations
ICLR 
Chang M B Ullman T Torralba A and Tenenbaum J B 2017 A compositional object
based approach to learning physical dynamics In Proceedings of the International Conference on
Learning Representations ICLR 
Chen X Li L FeiFei L and Gupta A 2018a Iterative visual reasoning beyond convolutions
InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition CVPR 
Chen X Liu C and Song D 2018b Treetotree neural networks for program translation In
Workshops of the International Conference on Learning Representations ICLR 
Cho K Van Merri enboer B Gulcehre C Bahdanau D Bougares F Schwenk H and Bengio
Y 2014 Learning phrase representations using rnn encoderdecoder for statistical machine
translation In Proceeding of the Conference on Empirical Methods in Natural Language Processing
EMNLP 
Chomsky N 1957 Syntactic Structures  Mouton  Co
Chomsky N 1965 Aspects of the Theory of Syntax  MIT Press
Cohen T and Welling M 2016 Group equivariant convolutional networks In International
Conference on Machine Learning  pages 29902999
Craik K J W 1943 The Nature of Explanation  Cambridge University Press
Cui Z Henrickson K Ke R and Wang Y 2018 Highorder graph convolutional recurrent
neural network A deep learning framework for networkscale trac learning and forecasting
arXiv preprint arXiv180207007 
Dai H Dai B and Song L 2016 Discriminative embeddings of latent variable models for
structured data In Proceedings of the International Conference on Machine Learning ICML 
Dai H Khalil E B Zhang Y Dilkina B and Song L 2017 Learning combinatorial
optimization algorithms over graphs In Advances in Neural Information Processing Systems 
De Cao N and Kipf T 2018 MolGAN An implicit generative model for small molecular graphs
arXiv preprint arXiv180511973 
26Deerrard M Bresson X and Vandergheynst P 2016 Convolutional neural networks on graphs
with fast localized spectral ltering In Advances in Neural Information Processing Systems  pages
38443852
Denil M Colmenarejo S G Cabi S Saxton D and de Freitas N 2017 Programmable
agents arXiv preprint arXiv170606383 
Devlin J Uesato J Singh R and Kohli P 2017 Semantic code repair using neurosymbolic
transformation networks arXiv preprint arXiv171011054 
Duvenaud D K Maclaurin D Iparraguirre J Bombarell R Hirzel T AspuruGuzik A and
Adams R P 2015 Convolutional networks on graphs for learning molecular ngerprints In
Advances in Neural Information Processing Systems  pages 22242232
Dyer C Ballesteros M Ling W Matthews A and Smith N A 2015 Transitionbased
dependency parsing with stack long shortterm memory In Proceedings of the Annual Meeting of
the Association for Computational Linguistics ACL 
D zeroski S De Raedt L and Driessens K 2001 Relational reinforcement learning Machine
Learning  4312752
Edwards H and Storkey A 2016 Towards a neural statistician arXiv preprint arXiv160602185 
Eliasmith C 2013 How to build a brain A neural architecture for biological cognition  Oxford
University Press
Elman J L 1990 Finding structure in time Cognitive Science  142179211
Elman J L 1991 Distributed representations simple recurrent networks and grammatical
structure Machine Learning  723195225
Eslami S A Heess N Weber T Tassa Y Szepesvari D Hinton G E et al 2016 Attend
infer repeat Fast scene understanding with generative models In Advances in Neural Information
Processing Systems  pages 32253233
Evans R and Grefenstette E 2018 Learning explanatory rules from noisy data Journal of
Articial Intelligence Research  61164
Evans R Saxton D Amos D Kohli P and Grefenstette E 2018 Can neural networks
understand logical entailment In Proceedings of the International Conference on Learning
Representations ICLR 
Farquhar G Rockt aschel T Igl M and Whiteson S 2018 TreeQN and ATreeC Dierentiable
tree planning for deep reinforcement learning In Proceedings of the International Conference on
Learning Representations ICLR 
Finn C Abbeel P and Levine S 2017 Modelagnostic metalearning for fast adaptation of
deep networks arXiv preprint arXiv170303400 
Fodor J A 1975 The Language of Thought  Harvard University Press
Fodor J A and Pylyshyn Z W 1988 Connectionism and cognitive architecture A critical
analysis Cognition  2812371
27Foerster J Assael I A de Freitas N and Whiteson S 2016 Learning to communicate with
deep multiagent reinforcement learning In Advances in Neural Information Processing Systems 
pages 21372145
Fukushima K 1980 Neocognitron A selforganizing neural network model for a mechanism of
pattern recognition unaected by shift in position Biological Cybernetics  36193202
Garcia V and Bruna J 2018 Fewshot learning with graph neural networks In Proceedings of
the International Conference on Learning Representations ICLR 
Garc aDur an A and Niepert M 2017 Learning graph representations with embedding propaga
tion arXiv preprint arXiv171003059 
Garnelo M Arulkumaran K and Shanahan M 2016 Towards deep symbolic reinforcement
learning arXiv preprint arXiv160905518 
Gaunt A L Brockschmidt M Kushman N and Tarlow D 2016 Dierentiable programs
with neural libraries arXiv preprint arXiv161102109 
Geman S Bienenstock E and Doursat R 1992 Neural networks and the biasvariance dilemma
Neural Computation  41158
Gentner D and Markman A B 1997 Structure mapping in analogy and similarity American
Psychologist  52145
Getoor L and Taskar B 2007 Introduction to Statistical Relational Learning  MIT press
Ghahramani Z 2015 Probabilistic machine learning and articial intelligence Nature 
5217553452
Gilmer J Schoenholz S S Riley P F Vinyals O and Dahl G E 2017 Neural message
passing for quantum chemistry arXiv preprint arXiv170401212 
Goodfellow I Bengio Y Courville A and Bengio Y 2016 Deep Learning  MIT Press
Goodman N 1955 The new riddle of induction In Fact Fiction and Forecast  pages 5983
Harvard University Press
Goodman N Mansinghka V Roy D M Bonawitz K and Tenenbaum J B 2012 Church a
language for generative models arXiv preprint arXiv12063255 
Goodman N D Tenenbaum J B and Gerstenberg T 2015 Concepts in a probabilistic
language of thought In Margolis E and Laurence S editors The Conceptual Mind New
Directions in the Study of Concepts  MIT Press
Goodwin G P and JohnsonLaird P 2005 Reasoning about relations Psychological Review 
1122468
Gori M Monfardini G and Scarselli F 2005 A new model for learning in graph domains In
Proceedings of the International Joint Conference on Neural Networks IJCNN  volume 2 pages
729734 IEEE
Graves A Wayne G Reynolds M Harley T Danihelka I GrabskaBarwi nska A Colmenarejo
S G Grefenstette E Ramalho T Agapiou J et al 2016 Hybrid computing using a neural
network with dynamic external memory Nature  5387626471
28Grefenstette E Hermann K M Suleyman M and Blunsom P 2015 Learning to transduce
with unbounded memory In Advances in Neural Information Processing Systems  pages 18281836
Griths T L Chater N Kemp C Perfors A and Tenenbaum J B 2010 Probabilistic
models of cognition Exploring representations and inductive biases Trends in Cognitive Sciences 
148357364
Grover A and Leskovec J 2016 node2vec Scalable feature learning for networks In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining 
pages 855864 ACM
Guez A Weber T Antonoglou I Simonyan K Vinyals O Wierstra D Munos R and
Silver D 2018 Learning to search with MCTSnets arXiv preprint arXiv180204697 
Gulcehre C Denil M Malinowski M Razavi A Pascanu R Hermann K M Battaglia P
Bapst V Raposo D Santoro A and de Freitas N 2018 Hyperbolic attention networks
arXiv preprint arXiv180509786 
Hamaguchi T Oiwa H Shimbo M and Matsumoto Y 2017 Knowledge transfer for outof
knowledgebase entities A graph neural network approach In Proceedings of the International
Joint Conference on Articial Intelligence IJCAI 
Hamilton W Ying Z and Leskovec J 2017 Inductive representation learning on large graphs
InAdvances in Neural Information Processing Systems  pages 10251035
Hamrick J Allen K Bapst V Zhu T McKee K Tenenbaum J and Battaglia P 2018
Relational inductive bias for physical construction in humans and machines In Proceedings of the
40th Annual Conference of the Cognitive Science Society 
Hamrick J B Ballard A J Pascanu R Vinyals O Heess N and Battaglia P W 2017
Metacontrol for adaptive imaginationbased optimization In Proceedings of the International
Conference on Learning Representations ICLR 
Hartford J Graham D R LeytonBrown K and Ravanbakhsh S 2018 Deep models of
interactions across sets arXiv preprint arXiv180302879 
Hay N Stark M Schlegel A Wendelken C Park D Purdy E Silver T Phoenix D S
and George D 2018 Behavior is everythingtowards representing concepts with sensorimotor
contingencies In Proceedings of the AAAI Conference on Articial Intelligence AAAI 
Hena M Bruna J and LeCun Y 2015 Deep convolutional networks on graphstructured
data arXiv preprint arXiv150605163 
Hinton G E 1990 Mapping partwhole hierarchies into connectionist networks Articial
Intelligence  46124775
Hjort N L Holmes C M uller P and Walker S G 2010 Bayesian Nonparametrics  Cambridge
University Press
Hoshen Y 2017 Vain Attentional multiagent predictive modeling In Advances in Neural
Information Processing Systems  pages 26982708
Hu H Gu J Zhang Z Dai J and Wei Y 2017 Relation networks for object detection
arXiv preprint arXiv171111575 
29Hudson D A and Manning C D 2018 Compositional attention networks for machine reasoning
InProceedings of the International Conference on Learning Representations ICLR 
Humboldt W 19991836 On Language On the diversity of human language construction and its
inuence on the mental development of the human species  Cambridge University Press
Hummel J E and Holyoak K J 2003 A symbolicconnectionist theory of relational inference
and generalization Psychological Review  1102220
Ioe S and Szegedy C 2015 Batch normalization Accelerating deep network training by reducing
internal covariate shift In Proceedings of the 32nd International Conference on International
Conference on Machine Learning ICML 
Johnson D D 2017 Learning graphical state transitions Proceedings of the International
Conference on Learning Representations ICLR 
Joulin A and Mikolov T 2015 Inferring algorithmic patterns with stackaugmented recurrent
nets In Advances in Neural Information Processing Systems  pages 190198
Kansky K Silver T M ely D A Eldawy M L azaroGredilla M Lou X Dorfman N Sidor
S Phoenix S and George D 2017 Schema networks Zeroshot transfer with a generative
causal model of intuitive physics In Proceedings of the International Conference on Machine
Learning ICML 
Kearnes S McCloskey K Berndl M Pande V and Riley P 2016 Molecular graph
convolutions moving beyond ngerprints Journal of computeraided molecular design  308595
608
Kemp C and Tenenbaum J B 2008 The discovery of structural form Proceedings of the
National Academy of Sciences  105311068710692
Kipf T Fetaya E Wang KC Welling M and Zemel R 2018 Neural relational inference
for interacting systems In Proceedings of the International Conference on Machine Learning
ICML 
Kipf T N and Welling M 2017 Semisupervised classication with graph convolutional networks
InProceedings of the International Conference on Learning Representations ICLR 
Koller D and Friedman N 2009 Probabilistic Graphical Models Principles and Techniques 
MIT press
Kondor R Son H T Pan H Anderson B and Trivedi S 2018 Covariant compositional
networks for learning graphs arXiv preprint arXiv180102144 
Kondor R and Trivedi S 2018 On the generalization of equivariance and convolution in neural
networks to the action of compact groups arXiv preprint arXiv180203690 
Konidaris G Kaelbling L P and LozanoPerez T 2018 From skills to symbols Learning
symbolic representations for abstract highlevel planning Journal of Articial Intelligence
Research  61215289
Kool W and Welling M 2018 Attention solves your TSP arXiv preprint arXiv180308475 
30Krizhevsky A Sutskever I and Hinton G E 2012 ImageNet classication with deep convolu
tional neural networks In Advances in Neural Information Processing Systems  pages 10971105
Kurach K Andrychowicz M and Sutskever I 2016 Neural randomaccess machines In
Proceedings of the International Conference on Learning Representations ICLR 
Lake B M and Baroni M 2018 Still not systematic after all these years On the compositional
skills of sequencetosequence recurrent networks In Proceedings of the International Conference
on Machine Learning ICML 
Lake B M Salakhutdinov R and Tenenbaum J B 2015 Humanlevel concept learning
through probabilistic program induction Science  350626613321338
Lake B M Ullman T D Tenenbaum J B and Gershman S J 2017 Building machines that
learn and think like people Behavioral and Brain Sciences  40
LeCun Y Bengio Y and Hinton G 2015 Deep learning Nature  5217553436
LeCun Y Boser B Denker J S Henderson D Howard R E Hubbard W and Jackel
L D 1989 Backpropagation applied to handwritten zip code recognition Neural computation 
14541551
Li Y Tarlow D Brockschmidt M and Zemel R 2016 Gated graph sequence neural networks
InProceedings of the International Conference on Learning Representations ICLR 
Li Y Vinyals O Dyer C Pascanu R and Battaglia P 2018 Learning deep generative
models of graphs In Workshops at the International Conference on Learning Representations
ICLR 
Li Y Yu R Shahabi C and Liu Y 2017 Diusion convolutional recurrent neural network
Datadriven trac forecasting arXiv preprint arXiv170701926 
Lin Z Feng M Santos C N d Yu M Xiang B Zhou B and Bengio Y 2017 A structured
selfattentive sentence embedding In Proceedings of the International Conference on Learning
Representations ICLR 
Liu H Simonyan K Vinyals O Fernando C and Kavukcuoglu K 2018 Hierarchical
representations for ecient architecture search In Proceedings of the International Conference on
Learning Representations ICLR 
Luong MT Pham H and Manning C D 2015 Eective approaches to attentionbased neural
machine translation arXiv preprint arXiv150804025 
Marcus G 2001 The algebraic mind
Marcus G 2018a Deep learning A critical appraisal arXiv preprint arXiv180100631 
Marcus G 2018b Innateness alphazero and articial intelligence arXiv preprint
arXiv180105667 
McClelland J L 1994 The interaction of nature and nurture in development A parallel
distributed processing perspective International perspectives on psychological science  15788
31McClelland J L and Rumelhart D E 1981 An interactive activation model of context eects
in letter perception I an account of basic ndings Psychological Review  885375
Mikolov T Yih Wt and Zweig G 2013 Linguistic regularities in continuous space word
representations In Proceedings of the 2013 Conference of the North American Chapter of the
Association for Computational Linguistics Human Language Technologies  pages 746751
Mitchell T M 1980 The need for biases in learning generalizations  Department of Computer
Science Laboratory for Computer Science Research Rutgers Univ New Jersey
Mnih V Heess N Graves A et al 2014 Recurrent models of visual attention In Advances in
neural information processing systems  pages 22042212
Mnih V Kavukcuoglu K Silver D Rusu A A Veness J Bellemare M G Graves A
Riedmiller M Fidjeland A K Ostrovski G et al 2015 Humanlevel control through deep
reinforcement learning Nature  5187540529
Monti F Boscaini D Masci J Rodola E Svoboda J and Bronstein M M 2017 Geometric
deep learning on graphs and manifolds using mixture model cnns In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition CVPR 
Morav c k M Schmid M Burch N Lis y V Morrill D Bard N Davis T Waugh K
Johanson M and Bowling M 2017 Deepstack Expertlevel articial intelligence in headsup
nolimit poker Science  3566337508513
Narayanan A Chandramohan M Chen L Liu Y and Saminathan S 2016 subgraph2vec
Learning distributed representations of rooted subgraphs from large graphs In Workshops at the
20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 
Narayanan A Chandramohan M Venkatesan R Chen L Liu Y and Jaiswal S 2017
graph2vec Learning distributed representations of graphs arXiv preprint arXiv170705005 
Navon D 1977 Forest before trees The precedence of global features in visual perception
Cognitive Psychology  93353383
Niepert M Ahmed M and Kutzkov K 2016 Learning convolutional neural networks for
graphs In Proceedings of the International Conference on Machine Learning ICML  pages
20142023
Nilsson N J and Fikes R E 1970 Strips A new approach to the application of theorem proving
to problem solving Technical report SRI International Menlo Park CA Articial Intelligence
Center
Nowak A Villar S Bandeira A S and Bruna J 2017 A note on learning algorithms for
quadratic assignment with graph neural networks In Proceedings of the Principled Approaches to
Deep Learning Workshop PADL at the International Conference of Machine Learning ICML 
Nowak M A 2006 Five rules for the evolution of cooperation science  314580515601563
Ohtsuki H Hauert C Lieberman E and Nowak M A 2006 A simple rule for the evolution
of cooperation on graphs and social networks Nature  4417092502
32O noroRubio D Niepert M Garc aDur an A Gonz alezS anchez R and L opezSastre
R J 2017 Representation learning for visualrelational knowledge graphs arXiv preprint
arXiv170902314 
Parisotto E Mohamed Ar Singh R Li L Zhou D and Kohli P 2017 Neurosymbolic
program synthesis In Proceedings of the International Conference on Learning Representations
ICLR 
Pascanu R Li Y Vinyals O Heess N Buesing L Racani ere S Reichert D Weber T
Wierstra D and Battaglia P 2017 Learning modelbased planning from scratch arXiv
preprint arXiv170706170 
Pearl J 1986 Fusion propagation and structuring in belief networks Articial intelligence 
293241288
Pearl J 1988 Probabilistic reasoning in intelligent systems Networks of plausible inference
Morgan Kaufmann
Pearl J 2009 Causality Models Reasoning and Inference  Cambridge University Press New
York NY USA 2nd edition
Pearl J 2018 Theoretical impediments to machine learning with seven sparks from the causal
revolution arXiv preprint arXiv180104016 
Pennington J Socher R and Manning C 2014 Glove Global vectors for word representation
InProceedings of the Conference on Empirical Methods in Natural Language Processing EMNLP 
pages 15321543
Perozzi B AlRfou R and Skiena S 2014 Deepwalk Online learning of social representations
InProceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and
data mining  pages 701710 ACM
Pevn y T and Somol P 2017 Using neural network formalism to solve multipleinstance problems
InInternational Symposium on Neural Networks  pages 135142 Springer
Pinker S and Prince A 1988 On language and connectionism Analysis of a parallel distributed
processing model of language acquisition Cognition  281273193
Plate T A 1995 Holographic reduced representations IEEE Transactions on Neural Networks 
63623641
Plaut D C McClelland J L Seidenberg M S and Patterson K 1996 Understanding normal
and impaired word reading computational principles in quasiregular domains Psychological
Review  103156
Pollack J B 1990 Recursive distributed representations Articial Intelligence  461277105
Qi C R Su H Mo K and Guibas L J 2017 Pointnet Deep learning on point sets for 3d
classication and segmentation In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition CVPR 
Raposo D Santoro A Barrett D Pascanu R Lillicrap T and Battaglia P 2017 Discovering
objects and their relations from entangled scene representations In Workshops at the International
Conference on Learning Representations ICLR 
33Reed S and De Freitas N 2016 Neural programmerinterpreters In Proceedings of the
International Conference on Learning Representations ICLR 
Ritchie D Horsfall P and Goodman N D 2016 Deep amortized inference for probabilistic
programs arXiv preprint arXiv161005735 
Rosenblatt F 1961 Principles of neurodynamics perceptrons and the theory of brain mechanisms
Technical report Cornell Aeronautical Lab Inc Bualo NY
Rumelhart D E McClelland J L Group P R et al 1987 Parallel Distributed Processing 
volume 1 MIT Press
Russell S J and Norvig P 2009 Articial Intelligence A Modern Approach 3rd Edition 
Pearson
Sabour S Frosst N and Hinton G E 2017 Dynamic routing between capsules In Advances
in Neural Information Processing Systems  pages 38593869
SanchezGonzalez A Heess N Springenberg J T Merel J Riedmiller M Hadsell R and
Battaglia P 2018 Graph networks as learnable physics engines for inference and control In
Proceedings of the 35th International Conference on Machine Learning ICLR 
Santoro A Raposo D Barrett D G Malinowski M Pascanu R Battaglia P and Lillicrap
T 2017 A simple neural network module for relational reasoning In Advances in Neural
Information Processing Systems 
Scarselli F Gori M Tsoi A C Hagenbuchner M and Monfardini G 2009a Computational
capabilities of graph neural networks IEEE Transactions on Neural Networks  20181102
Scarselli F Gori M Tsoi A C Hagenbuchner M and Monfardini G 2009b The graph
neural network model IEEE Transactions on Neural Networks  2016180
Scarselli F Yong S L Gori M Hagenbuchner M Tsoi A C and Maggini M 2005 Graph
neural networks for ranking web pages In Proceedings of the 2005 IEEEWICACM International
Conference on Web Intelligence  pages 666672 IEEE
Schmidhuber J 2015 Deep learning in neural networks An overview Neural Networks  6185117
Selsam D Lamm M Bunz B Liang P de Moura L and Dill D L 2018 Learning a sat
solver from singlebit supervision arXiv preprint arXiv180203685 
ShalevShwartz S Shamir O and Shammah S 2017 Failures of gradientbased deep learning
arXiv preprint arXiv170307950 
Shaw P Uszkoreit J and Vaswani A 2018 Selfattention with relative position representations
InProceedings of the 16th Annual Conference of the North American Chapter of the Association
for Computational Linguistics Human Language Technologies 
Shervashidze N Schweitzer P Leeuwen E J v Mehlhorn K and Borgwardt K M 2011
Weisfeilerlehman graph kernels Journal of Machine Learning Research  12Sep25392561
Silver D Huang A Maddison C J Guez A Sifre L Van Den Driessche G Schrittwieser J
Antonoglou I Panneershelvam V Lanctot M et al 2016 Mastering the game of go with
deep neural networks and tree search Nature  5297587484489
34Smolensky P 1990 Tensor product variable binding and the representation of symbolic structures
in connectionist systems Articial Intelligence  4612159216
Socher R Huval B Manning C D and Ng A Y 2012 Semantic compositionality through
recursive matrixvector spaces In Proceedings of the Joint Conference on Empirical Methods in
Natural Language Processing EMNLP and Computational Natural Language Learning CNLL 
pages 12011211 Association for Computational Linguistics
Socher R Lin C C Manning C and Ng A Y 2011a Parsing natural scenes and natural
language with recursive neural networks In Proceedings of the 28th International Conference on
Machine Learning ICML  pages 129136
Socher R Pennington J Huang E H Ng A Y and Manning C D 2011b Semisupervised
recursive autoencoders for predicting sentiment distributions In Proceedings of the Conference
on Empirical Methods in Natural Language Processing EMNLP  pages 151161 Association for
Computational Linguistics
Socher R Perelygin A Wu J Chuang J Manning C D Ng A and Potts C 2013
Recursive deep models for semantic compositionality over a sentiment treebank In Proceedings of
the Conference on Empirical Methods in Natural Language Processing EMNLP  pages 16311642
Spelke E S Breinlinger K Macomber J and Jacobson K 1992 Origins of knowledge
Psychological review  994605
Spelke E S and Kinzler K D 2007 Core knowledge Developmental Science  1018996
Srivastava N Hinton G Krizhevsky A Sutskever I and Salakhutdinov R 2014 Dropout
A simple way to prevent neural networks from overtting The Journal of Machine Learning
Research  15119291958
Sukhbaatar S Fergus R et al 2016 Learning multiagent communication with backpropagation
InAdvances in Neural Information Processing Systems  pages 22442252
Sukhbaatar S Weston J Fergus R et al 2015 Endtoend memory networks In Advances in
Neural Information Processing Systems  pages 24402448
Sutskever I Vinyals O and Le Q V 2014 Sequence to sequence learning with neural networks
InAdvances in Neural Information Processing Systems  pages 31043112
Szegedy C Ioe S Vanhoucke V and Alemi A A 2017 Inceptionv4 inceptionresnet
and the impact of residual connections on learning In Proceedings of the AAAI Conference on
Articial Intelligence AAAI  volume 4 page 12
Tai K S Socher R and Manning C D 2015 Improved semantic representations from
treestructured long shortterm memory networks In Proceedings of the Annual Meeting of the
Association for Computational Linguistics ACL 
Tang J Qu M Wang M Zhang M Yan J and Mei Q 2015 Line Largescale information
network embedding In Proceedings of the 24th International Conference on World Wide Web 
pages 10671077 International World Wide Web Conferences Steering Committee
Tenenbaum J B Griths T L and Kemp C 2006 Theorybased bayesian models of inductive
learning and reasoning Trends in Cognitive Sciences  107309318
35Tenenbaum J B Kemp C Griths T L and Goodman N D 2011 How to grow a mind
Statistics structure and abstraction Science  331602212791285
Toyer S Trevizan F Thiebaux S and Xie L 2017 Action schema networks Generalised
policies with deep learning In Proceedings of the AAAI Conference on Articial Intelligence
AAAI 
Ullman T D Spelke E Battaglia P and Tenenbaum J B 2017 Mind games Game engines
as an architecture for intuitive physics Trends in Cognitive Sciences  219649665
van Steenkiste S Chang M Gre K and Schmidhuber J 2018 Relational neural expectation
maximization Unsupervised discovery of objects and their interactions Proceedings of the
International Conference on Learning Representations ICLR 
Vaswani A Shazeer N Parmar N Uszkoreit J Jones L Gomez A N Kaiser L and
Polosukhin I 2017 Attention is all you need In Advances in Neural Information Processing
Systems 
Veli ckovi c P Cucurull G Casanova A Romero A Li o P and Bengio Y 2018 Graph
attention networks In Proceedings of the International Conference on Learning Representations
ICLR 
Wang J X KurthNelson Z Kumaran D Tirumala D Soyer H Leibo J Z Hassabis D
and Botvinick M 2018a Prefrontal cortex as a metareinforcement learning system Nature
neuroscience  page 1
Wang J X KurthNelson Z Tirumala D Soyer H Leibo J Z Munos R Blundell C
Kumaran D and Botvinick M 2016 Learning to reinforcement learn arXiv preprint
arXiv161105763 
Wang T Liao R Ba J and Fidler S 2018b Nervenet Learning structured policy with graph
neural networks In Proceedings of the International Conference on Learning Representations
ICLR 
Wang X Girshick R Gupta A and He K 2018c Nonlocal neural networks In Proceedings
of the Conference on Computer Vision and Pattern Recognition CVPR 
Wang Y Sun Y Liu Z Sarma S E Bronstein M M and Solomon J M 2018d Dynamic
graph cnn for learning on point clouds arXiv preprint arXiv180107829 
Watters N Zoran D Weber T Battaglia P Pascanu R and Tacchetti A 2017 Visual
interaction networks Learning a physics simulator from video In Advances in Neural Information
Processing Systems  pages 45424550
Wu J Lu E Kohli P Freeman B and Tenenbaum J 2017 Learning to see physics via
visual deanimation In Advances in Neural Information Processing Systems  pages 152163
Yoon K Liao R Xiong Y Zhang L Fetaya E Urtasun R Zemel R and Pitkow X
2018 Inference in probabilistic graphical models by graph neural networks In Workshops at
the International Conference on Learning Representations ICLR 
You J Ying R Ren X Hamilton W L and Leskovec J 2018 GraphRNN A deep generative
model for graphs arXiv preprint arXiv180208773 
36Yuille A L and Liu C 2018 Deep nets What have they ever done for vision arXiv preprint
arXiv180504025 
Zaheer M Kottur S Ravanbakhsh S Poczos B Salakhutdinov R R and Smola A J 2017
Deep sets In Advances in Neural Information Processing Systems  pages 33943404
Zambaldi V Raposo D Santoro A Bapst V Li Y Babuschkin I Tuyls K Reichert D
Lillicrap T Lockhart E Shanahan M Langston V Pascanu R Botvinick M Vinyals O
and Battaglia P 2018 Relational deep reinforcement learning arXiv preprint arXiv 
Zhang A Lerer A Sukhbaatar S Fergus R and Szlam A 2018 Composable planning with
attributes arXiv preprint arXiv180300512 
Z ugner D Akbarnejad A and G unnemann S 2018 Adversarial Attacks on Neural Networks
for Graph Data arXiv preprint arXiv180507984 
37Appendix Formulations of additional models
In this appendix we give more examples of how published networks can t in the frame dened by
Equation 1
Interaction networks
Interaction Networks Battaglia et al 2016 Watters et al 2017 and the Neural Physics Engine
Chang et al 2017 use a full GN but for the absence of the global to update the edge properties
eekvrkvskufeekvrkvsk  NNeekvrkvsk
v
 e0
iviufv
 e0
iviu
 NNv
 e0
iviu
ev
E0
i X
fkrkige0
k
That work also included an extension to the above formulation which output global rather than
pernode predictions
eekvrkvskufeekvrkvsk  NNeekvrkvsk
v
 e0
iviufv
 e0
iviu
 NNv
 e0
iviu
u
 e0 v0ufu
 v0u
 NNu
 v0u
vg
V0 X
iv0
i
Nonpairwise interactions
Gated Graph Sequence Neural Networks GGSNN Li et al 2016 use a slightly generalized
formulation where each edge has an attached type tk2f1Tg and the updates are
eektkvrkvskufeekvsk  NNetkvsk
v
 e0
iviufv
 e0
ivi
 NNv
 e0
ivi
ev
E0
i X
fkrkige0
k
These updates are applied recurrently the NNvis a GRU Cho et al 2014 followed by a global
decoder which computes a weighted sum of embedded nal node states Here each NNetkis a neural
network with specic parameters
CommNet Sukhbaatar et al 2016 in the slightly more general form described by Hoshen
2017 uses
eekvrkvskufevsk  NN evsk
v
 e0
iviufv
 e0
ivi
 NNv
 e0
iNNv0vi
ev
E0
i 1
jE0
ijX
fkrkige0
k
38Attentionbased approaches
The various attentionbased approaches use a ewhich is factored into a scalar pairwiseinteraction
function which returns the unnormalized attention term denoted evrkvska0
k and a vector
valued nonpairwise term denoted evsk b0
k
eekvrkvskufevrkvsk  evrkvsk evsk  a0
kb0
k e0
k
The singleheaded selfattention SA in the Transformer architecture Vaswani et al 2017
implements the nonlocal formulation as
evrkvsk  exp NN queryvrkNNkeyvsk
evsk  NN vsk
v
 e0
iviufv
 e0
i
 NNv
 e0
i
where NNqueryNNkey and NNare again neural network functions with dierent parameters and
possibly dierent architectures They also use a multiheaded version which computes Nhparallel
 e0h
iusing dierent NNquery
hNNkey
hNNh wherehindexes the dierent parameters These are
passed tofvand concatenated
fv
f e0h
igh1Nh
 NNv
 e01
i e0Nh
i
Vertex Attention Interaction Networks Hoshen 2017 are very similar to singleheaded SA
but use Euclidean distance for the attentional similarity metric with shared parameters across the
attention inputs embeddings and also use the input node feature in the node update function
evrkvsk  exp
kNNvrkNNvskk2
evsk  NN vsk
v
 e0
iviufv
 e0
i
 NNv
 e0
ivi
Graph Attention Networks Veli ckovi c et al 2018 are also similar to multiheaded SA but use
a neural network as the attentional similarity metric with shared parameters across the attention
inputs embeddings
evrkvsk  exp NN 0NNvrkNNvsk
evsk  NN vsk
v
 e0
iviufv
f e0h
igh1Nh
 NNv
 e01
i e0Nh
i
Stretching beyond the specic nonlocal formulation Shaw et al 2018 extended multiheaded
SA with relative position encodings Relative refers to an encoding of the spatial distance between
nodes in a sequence or other signal in a metric space This can be expressed in GN language as an
edge attribute ek and replacing the evsk from multiheaded SA above with
eekvsk  NNevsk ek
39Belief Propagation embeddings
Finally we briey summarize how the general structure2vec algorithm of Dai et al 2016 can t
into our framework In order to do so we need to slightly modify our main Equation 1 ie
k
felgslrk
rl6sk
X
rlsk
sl6rkel
e0
kek fk  NN k
 e0
i
fe0
kgrkiX
fkrkigek
v0
iv
 e0
if e0
i  NN  e0
i
Edges features now takes the meaning of message between their receiver and sender note that
there is only one set of parameters to learn for both the edges and nodes updates
40
  Diffusion Models Beat GANs on Image Synthesis
Prafulla Dhariwal
OpenAI
prafullaopenaicomAlex Nichol
OpenAI
alexopenaicom
Abstract
We show that diffusion models can achieve image sample quality superior to the
current stateoftheart generative models We achieve this on unconditional im
age synthesis by ﬁnding a better architecture through a series of ablations For
conditional image synthesis we further improve sample quality with classiﬁer guid
ance a simple computeefﬁcient method for trading off diversity for ﬁdelity using
gradients from a classiﬁer We achieve an FID of 297 on ImageNet 128 128
459 on ImageNet 256 256 and 772 on ImageNet 512 512 and we match
BigGANdeep even with as few as 25 forward passes per sample all while main
taining better coverage of the distribution Finally we ﬁnd that classiﬁer guidance
combines well with upsampling diffusion models further improving FID to 394
on ImageNet 256256 and 385 on ImageNet 512 512 We release our code at
httpsgithubcomopenaiguideddiffusion 
1 Introduction
Figure 1 Selected samples from our best ImageNet 512 512 model FID 385
Over the past few years generative models have gained the ability to generate humanlike natural
language  6 inﬁnite highquality synthetic images  52851 and highly diverse human speech and
music  6413 These models can be used in a variety of ways such as generating images from text
prompts  7250 or learning useful feature representations  147 While these models are already
Equal contributionarXiv210505233v4  csLG  1 Jun 2021capable of producing realistic images and sound there is still much room for improvement beyond
the current stateoftheart and better generative models could have wideranging impacts on graphic
design games music production and countless other ﬁelds
GANs  19 currently hold the stateoftheart on most image generation tasks  56828 as measured
by sample quality metrics such as FID  23 Inception Score  54 and Precision  32 However some
of these metrics do not fully capture diversity and it has been shown that GANs capture less diversity
than stateoftheart likelihoodbased models  514342 Furthermore GANs are often difﬁcult to
train collapsing without carefully selected hyperparameters and regularizers 5 41 4
While GANs hold the stateoftheart their drawbacks make them difﬁcult to scale and apply to
new domains As a result much work has been done to achieve GANlike sample quality with
likelihoodbased models  5125429 While these models capture more diversity and are typically
easier to scale and train than GANs they still fall short in terms of visual sample quality Furthermore
except for V AEs sampling from these models is slower than GANs in terms of wallclock time
Diffusion models are a class of likelihoodbased models which have recently been shown to produce
highquality images  565925 while offering desirable properties such as distribution coverage
a stationary training objective and easy scalability These models generate samples by gradually
removing noise from a signal and their training objective can be expressed as a reweighted variational
lowerbound  25 This class of models already holds the stateoftheart  60 on CIFAR10  31 but
still lags behind GANs on difﬁcult generation datasets like LSUN and ImageNet Nichol and Dhariwal
43 found that these models improve reliably with increased compute and can produce highquality
samples even on the difﬁcult ImageNet 256 256 dataset using an upsampling stack However the
FID of this model is still not competitive with BigGANdeep 5 the current stateoftheart on this
dataset
We hypothesize that the gap between diffusion models and GANs stems from at least two factors
ﬁrst that the model architectures used by recent GAN literature have been heavily explored and
reﬁned second that GANs are able to trade off diversity for ﬁdelity producing high quality samples
but not covering the whole distribution We aim to bring these beneﬁts to diffusion models ﬁrst by
improving model architecture and then by devising a scheme for trading off diversity for ﬁdelity
With these improvements we achieve a new stateoftheart surpassing GANs on several different
metrics and datasets
The rest of the paper is organized as follows In Section 2 we give a brief background of diffusion
models based on Ho et al  25 and the improvements from Nichol and Dhariwal  43 and Song
et al  57 and we describe our evaluation setup In Section 3 we introduce simple architecture
improvements that give a substantial boost to FID In Section 4 we describe a method for using
gradients from a classiﬁer to guide a diffusion model during sampling We ﬁnd that a single
hyperparameter the scale of the classiﬁer gradients can be tuned to trade off diversity for ﬁdelity
and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial
examples  61 Finally in Section 5 we show that models with our improved architecture achieve
stateoftheart on unconditional image synthesis tasks and with classiﬁer guidance achieve stateof
theart on conditional image synthesis When using classiﬁer guidance we ﬁnd that we can sample
with as few as 25 forward passes while maintaining FIDs comparable to BigGAN We also compare
our improved models to upsampling stacks ﬁnding that the two approaches give complementary
improvements and that combining them gives the best results on ImageNet 256 256 and 512512
2 Background
In this section we provide a brief overview of diffusion models For a more detailed mathematical
description we refer the reader to Appendix B
On a high level diffusion models sample from a distribution by reversing a gradual noising process In
particular sampling starts with noise xTand produces gradually lessnoisy samples xT1xT2
until reaching a ﬁnal sample x0 Each timestep tcorresponds to a certain noise level and xtcan be
thought of as a mixture of a signal x0with some noise where the signal to noise ratio is determined
by the timestep t For the remainder of this paper we assume that the noise is drawn from a diagonal
Gaussian distribution which works well for natural images and simpliﬁes various derivations
2A diffusion model learns to produce a slightly more denoised xt1fromxt Ho et al  25
parameterize this model as a function xttwhich predicts the noise component of a noisy sample
xt To train these models each sample in a minibatch is produced by randomly drawing a data sample
x0 a timestep t and noise which together give rise to a noised sample xtEquation 17 The
training objective is then jjxttjj2 ie a simple meansquared error loss between the true
noise and the predicted noise Equation 26
It is not immediately obvious how to sample from a noise predictor xtt Recall that diffusion
sampling proceeds by repeatedly predicting xt1fromxt starting from xT Ho et al  25 show
that under reasonable assumptions we can model the distribution pxt1jxtofxt1givenxtas
a diagonal Gaussian Nxt1xttxtt where the mean xttcan be calculated as a
function ofxttEquation 27 The variance xttof this Gaussian distribution can be ﬁxed
to a known constant  25 or learned with a separate neural network head  43 and both approaches
yield highquality samples when the total number of diffusion steps Tis large enough
Ho et al  25 observe that the simple meansqaured error objective Lsimple  works better in practice
than the actual variational lower bound Lvlbthat can be derived from interpreting the denoising diffu
sion model as a V AE They also note that training with this objective and using their corresponding
sampling procedure is equivalent to the denoising score matching model from Song and Ermon  58
who use Langevin dynamics to sample from a denoising model trained with multiple noise levels to
produce high quality image samples We often use diffusion models as shorthand to refer to both
classes of models
21 Improvements
Following the breakthrough work of Song and Ermon  58 and Ho et al  25 several recent papers
have proposed improvements to diffusion models Here we describe a few of these improvements
which we employ for our models
Nichol and Dhariwal  43 ﬁnd that ﬁxing the variance xttto a constant as done in Ho et al
25 is suboptimal for sampling with fewer diffusion steps and propose to parameterize xttas
a neural network whose output vis interpolated as
xtt  expvlogt 1v log t 1
HeretandtEquation 19 are the variances in Ho et al  25 corresponding to upper and lower
bounds for the reverse process variances Additionally Nichol and Dhariwal  43 propose a hybrid
objective for training both xttandxttusing the weighted sum Lsimple Lvlb Learning
the reverse process variances with their hybrid objective allows sampling with fewer steps without
much drop in sample quality We adopt this objective and parameterization and use it throughout our
experiments
Song et al  57 propose DDIM which formulates an alternative nonMarkovian noising process
that has the same forward marginals as DDPM but allows producing different reverse samplers by
changing the variance of the reverse noise By setting this noise to 0 they provide a way to turn any
modelxttinto a deterministic mapping from latents to images and ﬁnd that this provides an
alternative way to sample with fewer steps We adopt this sampling approach when using fewer than
50 sampling steps since Nichol and Dhariwal 43 found it to be beneﬁcial in this regime
22 Sample Quality Metrics
For comparing sample quality across models we perform quantitative evaluations using the following
metrics While these metrics are often used in practice and correspond well with human judgement
they are not a perfect proxy and ﬁnding better metrics for sample quality evaluation is still an open
problem
Inception Score IS was proposed by Salimans et al  54 and it measures how well a model captures
the full ImageNet class distribution while still producing individual samples that are convincing
examples of a single class One drawback of this metric is that it does not reward covering the
whole distribution or capturing diversity within a class and models which memorize a small subset
of the full dataset will still have high IS  3 To better capture diversity than IS Fréchet Inception
Distance FID was proposed by Heusel et al  23 who argued that it is more consistent with human
3Channels Depth HeadsAttention BigGAN Rescale FID FID
resolutions updownsample resblock 700K 1200K
160 2 1 16 7 7 1533 1321
128 4 021 048
4 054 082
32168 072 066
3 120 121
3 016 025
160 2 4 32168 3 7 314 300
Table 1 Ablation of various architecture changes evaluated at 700K and 1200K iterations
judgement than Inception Score FID provides a symmetric measure of the distance between two
image distributions in the InceptionV3  62 latent space Recently sFID was proposed by Nash
et al  42 as a version of FID that uses spatial features rather than the standard pooled features
They ﬁnd that this metric better captures spatial relationships rewarding image distributions with
coherent highlevel structure Finally Kynkäänniemi et al  32 proposed Improved Precision and
Recall metrics to separately measure sample ﬁdelity as the fraction of model samples which fall into
the data manifold precision and diversity as the fraction of data samples which fall into the sample
manifold recall
We use FID as our default metric for overall sample quality comparisons as it captures both diversity
and ﬁdelity and has been the de facto standard metric for stateoftheart generative modeling work
2728525 We use Precision or IS to measure ﬁdelity and Recall to measure diversity or
distribution coverage When comparing against other methods we recompute these metrics using
public samples or models whenever possible This is for two reasons ﬁrst some papers  2728
25 compare against arbitrary subsets of the training set which are not readily available and second
subtle implementation differences can affect the resulting FID values  45 To ensure consistent
comparisons we use the entire training set as the reference batch  235 and evaluate metrics for all
models using the same codebase
3 Architecture Improvements
In this section we conduct several architecture ablations to ﬁnd the model architecture that provides
the best sample quality for diffusion models
Ho et al  25 introduced the UNet architecture for diffusion models which JolicoeurMartineau
et al  26 found to substantially improve sample quality over the previous architectures  5833 used
for denoising score matching The UNet model uses a stack of residual layers and downsampling
convolutions followed by a stack of residual layers with upsampling colvolutions with skip con
nections connecting the layers with the same spatial size In addition they use a global attention
layer at the 1616 resolution with a single head and add a projection of the timestep embedding into
each residual block Song et al  60 found that further changes to the UNet architecture improved
performance on the CIFAR10  31 and CelebA64  34 datasets We show the same result on
ImageNet 128128 ﬁnding that architecture can indeed give a substantial boost to sample quality on
much larger and more diverse datasets at a higher resolution
We explore the following architectural changes
 Increasing depth versus width holding model size relatively constant
 Increasing the number of attention heads
 Using attention at 32 32 1616 and 88 resolutions rather than only at 16 16
Using the BigGAN  5 residual block for upsampling and downsampling the activations
following 60
 Rescaling residual connections with1p
2 following 60 27 28
For all comparisons in this section we train models on ImageNet 128 128 with batch size 256 and
sample using 250 sampling steps We train models with the above architecture changes and compare
4Number of heads Channels per head FID
1 1408
2 050
4 097
8 117
32 136
64 103
128 108
Table 2 Ablation of various attention conﬁgurations More heads or lower channels per heads both
lead to improved FID
40 60 80 100 120 140 160 180
time hrs14161820222426FIDch128 res4
ch160 res2
ch160 res2 heads4
ch160 res2 multires attn
ch160 res2 biggan updown
ch160 res2 skip rescale
ch160 res2 heads4 multires attn biggan updown
20 40 60 80 100
time hrs1416182022242628FID1 head
2 heads
4 heads
8 heads
32 head channels
64 head channels
128 head channels
Figure 2 Ablation of various architecture changes showing FID as a function of wallclock time
FID evaluated over 10k samples instead of 50k for efﬁciency
Operation FID
AdaGN 1306
Addition  GroupNorm 1508
Table 3 Ablating the elementwise operation used when projecting timestep and class embeddings
into each residual block Replacing AdaGN with the Addition  GroupNorm layer from Ho et al
25 makes FID worse
them on FID evaluated at two different points of training in Table 1 Aside from rescaling residual
connections all of the other modiﬁcations improve performance and have a positive compounding
effect We observe in Figure 2 that while increased depth helps performance it increases training
time and takes longer to reach the same performance as a wider model so we opt not to use this
change in further experiments
We also study other attention conﬁgurations that better match the Transformer architecture  66 To
this end we experimented with either ﬁxing attention heads to a constant or ﬁxing the number of
channels per head For the rest of the architecture we use 128 base channels 2 residual blocks
per resolution multiresolution attention and BigGAN updownsampling and we train the models
for 700K iterations Table 2 shows our results indicating that more heads or fewer channels per
head improves FID In Figure 2 we see 64 channels is best for wallclock time so we opt to use 64
channels per head as our default We note that this choice also better matches modern transformer
architectures and is on par with our other conﬁgurations in terms of ﬁnal FID
531 Adaptive Group Normalization
We also experiment with a layer  43 that we refer to as adaptive group normalization AdaGN which
incorporates the timestep and class embedding into each residual block after a group normalization
operation  69 similar to adaptive instance norm  27 and FiLM  48 We deﬁne this layer as
AdaGN hy ysGroupNorm hyb wherehis the intermediate activations of the residual block
following the ﬁrst convolution and y ysybis obtained from a linear projection of the timestep
and class embedding
We had already seen AdaGN improve our earliest diffusion models and so had it included by
default in all our runs In Table 3 we explicitly ablate this choice and ﬁnd that the adaptive group
normalization layer indeed improved FID Both models use 128 base channels and 2 residual blocks
per resolution multiresolution attention with 64 channels per head and BigGAN updownsampling
and were trained for 700K iterations
In the rest of the paper we use this ﬁnal improved model architecture as our default variable width
with 2 residual blocks per resolution multiple heads with 64 channels per head attention at 32 16 and
8 resolutions BigGAN residual blocks for up and downsampling and adaptive group normalization
for injecting timestep and class embeddings into residual blocks
4 Classiﬁer Guidance
In addition to employing well designed architectures GANs for conditional image synthesis  395
make heavy use of class labels This often takes the form of classconditional normalization statistics
1611 as well as discriminators with heads that are explicitly designed to behave like classiﬁers
pyjx40 As further evidence that class information is crucial to the success of these models
Lucic et al  36 ﬁnd that it is helpful to generate synthetic labels when working in a labellimited
regime
Given this observation for GANs it makes sense to explore different ways to condition diffusion
models on class labels We already incorporate class information into normalization layers Section
31 Here we explore a different approach exploiting a classiﬁer pyjxto improve a diffusion
generator SohlDickstein et al  56 and Song et al  60 show one way to achieve this wherein a
pretrained diffusion model can be conditioned using the gradients of a classiﬁer In particular we
can train a classiﬁer pyjxtton noisy images xt and then use gradients rxtlogpyjxttto
guide the diffusion sampling process towards an arbitrary class label y
In this section we ﬁrst review two ways of deriving conditional sampling processes using classiﬁers
We then describe how we use such classiﬁers in practice to improve sample quality We choose the
notationpyjxtt pyjxtandxtt xtfor brevity noting that they refer to separate
functions for each timestep tand at training time the models must be conditioned on the input t
41 Conditional Reverse Noising Process
We start with a diffusion model with an unconditional reverse noising process pxtjxt1 To
condition this on a label y it sufﬁces to sample each transition2according to
pxtjxt1y Zpxtjxt1pyjxt 2
whereZis a normalizing constant proof in Appendix H It is typically intractable to sample from
this distribution exactly but SohlDickstein et al  56 show that it can be approximated as a perturbed
Gaussian distribution Here we review this derivation
Recall that our diffusion model predicts the previous timestep xtfrom timestep xt1using a Gaussian
distribution
pxtjxt1 N 3
logpxtjxt1 1
2xtT1xt C 4
2We must also sample xTconditioned on y but a noisy enough diffusion process causes xTto be nearly
Gaussian even in the conditional case
6Algorithm 1 Classiﬁer guided diffusion sampling given a diffusion model xtxt classi
ﬁerpyjxt and gradient scale s
Input class label y gradient scale s
xT sample fromN0I
for alltfromTto 1do
 xtxt
xt1 sample fromNsrxtlogpyjxt
end for
returnx0
Algorithm 2 Classiﬁer guided DDIM sampling given a diffusion model xt classiﬁerpyjxt
and gradient scale s
Input class label y gradient scale s
xT sample fromN0I
for alltfromTto 1do
 xtp1trxtlogpyjxt
xt1 pt1
xtp1tpt
p1t1
end for
returnx0
We can assume that logpyjxthas low curvature compared to 1 This assumption is reasonable
in the limit of inﬁnite diffusion steps where jjjj 0 In this case we can approximate logpyjxt
using a Taylor expansion around xtas
logpyjxtlogpyjxtjxt xtrxtlogpyjxtjxt 5
 xtgC1 6
Heregrxtlogpyjxtjxt andC1is a constant This gives
logpxtjxt1pyjxt1
2xtT1xt  xtgC2 7
1
2xtgT1xtg 1
2gTgC2 8
1
2xtgT1xtg C3 9
 logpz C4zN g 10
We can safely ignore the constant term C4 since it corresponds to the normalizing coefﬁcient Zin
Equation 2 We have thus found that the conditional transition operator can be approximated by a
Gaussian similar to the unconditional transition operator but with its mean shifted by g Algorithm
1 summaries the corresponding sampling algorithm We include an optional scale factor sfor the
gradients which we describe in more detail in Section 43
42 Conditional Sampling for DDIM
The above derivation for conditional sampling is only valid for the stochastic diffusion sampling
process and cannot be applied to deterministic sampling methods like DDIM  57 To this end we
use a scorebased conditioning trick adapted from Song et al  60 which leverages the connection
between diffusion models and score matching  59 In particular if we have a model xtthat
predicts the noise added to a sample then this can be used to derive a score function
rxtlogpxt 1p1txt 11
7Figure 3 Samples from an unconditional diffusion model with classiﬁer guidance to condition
on the class Pembroke Welsh corgi Using classiﬁer scale 10 left FID 330 does not produce
convincing samples in this class whereas classiﬁer scale 100 right FID 120 produces much more
classconsistent images
We can now substitute this into the score function for pxtpyjxt
rxtlogpxtpyjxt rxtlogpxt rxtlogpyjxt 12
1p1txt rxtlogpyjxt 13
Finally we can deﬁne a new epsilon prediction xtwhich corresponds to the score of the joint
distribution
xtxtp
1trxtlogpyjxt 14
We can then use the exact same sampling procedure as used for regular DDIM but with the modiﬁed
noise predictions xtinstead ofxt Algorithm 2 summaries the corresponding sampling
algorithm
43 Scaling Classiﬁer Gradients
To apply classiﬁer guidance to a large scale generative task we train classiﬁcation models on
ImageNet Our classiﬁer architecture is simply the downsampling trunk of the UNet model with
an attention pool  49 at the 8x8 layer to produce the ﬁnal output We train these classiﬁers on the
same noising distribution as the corresponding diffusion model and also add random crops to reduce
overﬁtting After training we incorporate the classiﬁer into the sampling process of the diffusion
model using Equation 10 as outlined by Algorithm 1
In initial experiments with unconditional ImageNet models we found it necessary to scale the
classiﬁer gradients by a constant factor larger than 1 When using a scale of 1 we observed that the
classiﬁer assigned reasonable probabilities around 50 to the desired classes for the ﬁnal samples
but these samples did not match the intended classes upon visual inspection Scaling up the classiﬁer
gradients remedied this problem and the class probabilities from the classiﬁer increased to nearly
100 Figure 3 shows an example of this effect
To understand the effect of scaling classiﬁer gradients note that srxlogpyjx rxlog1
Zpyjxs
whereZis an arbitrary constant As a result the conditioning process is still theoretically grounded
in a renormalized classiﬁer distribution proportional to pyjxs Whens  1 this distribution
becomes sharper than pyjx since larger values are ampliﬁed by the exponent In other words using
a larger gradient scale focuses more on the modes of the classiﬁer which is potentially desirable for
producing higher ﬁdelity but less diverse samples
In the above derivations we assumed that the underlying diffusion model was unconditional modeling
px It is also possible to train conditional diffusion models pxjy and use classiﬁer guidance in
the exact same way Table 4 shows that the sample quality of both unconditional and conditional
models can be greatly improved by classiﬁer guidance We see that with a high enough scale the
guided unconditional model can get quite close to the FID of an unguided conditional model although
training directly with the class labels still helps Guiding a conditional model further improves FID
Table 4 also shows that classiﬁer guidance improves precision at the cost of recall thus introducing
a tradeoff in sample ﬁdelity versus diversity We explicitly evaluate how this tradeoff varies with
8Conditional Guidance Scale FID sFID IS Precision Recall
7 7 2621 635 3970 061 063
7 3 10 3303 699 3292 056 065
7 3 100 1200 1040 9541 076 044
3 7 1094 602 10098 069 063
3 3 10 459 525 18670 082 052
3 3 100 911 1093 28392 088 032
Table 4 Effect of classiﬁer guidance on sample quality Both conditional and unconditional models
were trained for 2M iterations on ImageNet 256 256 with batch size 256
0 2 4 6 8 10
gradient scale46810121416FID sFID
0 2 4 6 8 10
gradient scale100150200250300IS
0 2 4 6 8 10
gradient scale03040506070809precision recall
Figure 4 Change in sample quality as we vary scale of the classiﬁer gradients for a classconditional
ImageNet 128128 model
070 075 080 085 090 095
Precision00010203040506RecallBigGANdeep
Classifier  guidance ours
100125150175200225250275
IS51015202530FIDBigGANdeep
Classifier guidance ours
Figure 5 Tradeoffs when varying truncation for BigGANdeep and gradient scale for classiﬁer
guidance Models are evaluated on ImageNet 128 128 The BigGANdeep results were produced
using the TFHub model 12 at truncation levels 01020310
the gradient scale in Figure 4 We see that scaling the gradients beyond 10 smoothly trades off
recall a measure of diversity for higher precision and IS measures of ﬁdelity Since FID and sFID
depend on both diversity and ﬁdelity their best values are obtained at an intermediate point We also
compare our guidance with the truncation trick from BigGAN in Figure 5 We ﬁnd that classiﬁer
guidance is strictly better than BigGANdeep when trading off FID for Inception Score Less clear
cut is the precisionrecall tradeoff which shows that classiﬁer guidance is only a better choice up
until a certain precision threshold after which point it cannot achieve better precision
5 Results
To evaluate our improved model architecture on unconditional image generation we train separate
diffusion models on three LSUN  71 classes bedroom horse and cat To evaluate classiﬁer
guidance we train conditional diffusion models on the ImageNet  52 dataset at 128128 256256
and 512512 resolution
9Model FID sFID Prec Rec
LSUN Bedrooms 256 256
DCTransformery42 640 666 044 056
DDPM 25 489 907 060 045
IDDPM 43 424 821 062 046
StyleGAN 27 235 662 059 048
ADM dropout 190 559 066 051
LSUN Horses 256256
StyleGAN2 28 384 646 063 048
ADM 295 594 069 055
ADM dropout 257 681 071 055
LSUN Cats 256256
DDPM 25 171 124 053 048
StyleGAN2 28 725 633 058 043
ADM dropout 557 669 063 052
ImageNet 6464
BigGANdeep 5 406 396 079 048
IDDPM 43 292 379 074 062
ADM 261 377 073 063
ADM dropout 207 429 074 063Model FID sFID Prec Rec
ImageNet 128128
BigGANdeep 5 602 718 086 035
LOGANy68 336
ADM 591 509 070 065
ADMG 25 steps 598 704 078 051
ADMG 297 509 078 059
ImageNet 256256
DCTransformery42 3651 824 036 067
VQV AE2yz51 3111 1738 036 057
IDDPMz43 1226 542 070 062
SR3yz53 1130
BigGANdeep 5 695 736 087 028
ADM 1094 602 069 063
ADMG 25 steps 544 532 081 049
ADMG 459 525 082 052
ImageNet 512512
BigGANdeep 5 843 813 088 029
ADM 2324 1019 073 060
ADMG 25 steps 841 967 083 047
ADMG 772 657 087 042
Table 5 Sample quality comparison with stateoftheart generative models for each task ADM
refers to our ablated diffusion model and ADMG additionally uses classiﬁer guidance LSUN
diffusion models are sampled using 1000 steps see Appendix J ImageNet diffusion models are
sampled using 250 steps except when we use the DDIM sampler with 25 steps No BigGANdeep
model was available at this resolution so we trained our ownyValues are taken from a previous
paper due to lack of public models or sampleszResults use tworesolution stacks
51 Stateoftheart Image Synthesis
Table 5 summarizes our results Our diffusion models can obtain the best FID on each task and
the best sFID on all but one task With the improved architecture we already obtain stateoftheart
image generation on LSUN and ImageNet 64 64 For higher resolution ImageNet we observe that
classiﬁer guidance allows our models to substantially outperform the best GANs These models
obtain perceptual quality similar to GANs while maintaining a higher coverage of the distribution as
measured by recall and can even do so using only 25 diffusion steps
Figure 6 compares random samples from the best BigGANdeep model to our best diffusion model
While the samples are of similar perceptual quality the diffusion model contains more modes than the
GAN such as zoomed ostrich heads single ﬂamingos different orientations of cheeseburgers and a
tinca ﬁsh with no human holding it We also check our generated samples for nearest neighbors in
the InceptionV3 feature space in Appendix C and we show additional samples in Appendices KM
52 Comparison to Upsampling
We also compare guidance to using a twostage upsampling stack Nichol and Dhariwal  43 and
Saharia et al  53 train twostage diffusion models by combining a lowresolution diffusion model
with a corresponding upsampling diffusion model In this approach the upsampling model is
trained to upsample images from the training set and conditions on lowresolution images that are
concatenated channelwise to the model input using a simple interpolation eg bilinear During
sampling the lowresolution model produces a sample and then the upsampling model is conditioned
on this sample This greatly improves FID on ImageNet 256 256 but does not reach the same
performance as stateoftheart models like BigGANdeep 43 53 as seen in Table 5
In Table 6 we show that guidance and upsampling improve sample quality along different axes
While upsampling improves precision while keeping a high recall guidance provides a knob to trade
10Figure 6 Samples from BigGANdeep with truncation 10 FID 695 left vs samples from our
diffusion model with guidance FID 459 middle and samples from the training set right
Model Sbase Supsample FID sFID IS Precision Recall
ImageNet 256256
ADM 250 1094 602 10098 069 063
ADMU 250 250 749 513 12749 072 063
ADMG 250 459 525 18670 082 052
ADMG ADMU 250 250 394 614 21584 083 053
ImageNet 512512
ADM 250 2324 1019 5806 073 060
ADMU 250 250 996 562 12178 075 064
ADMG 250 772 657 17271 087 042
ADMG ADMU 25 25 596 1210 18787 081 054
ADMG ADMU 250 25 411 957 21929 083 055
ADMG ADMU 250 250 385 586 22172 084 053
Table 6 Comparing our single upsampling and classiﬁer guided models For upsampling we use
theupsampling stack from Nichol and Dhariwal  43 combined with our architecture improvements
which we refer to as ADMU The base resolution for the twostage upsampling models is 64and
128for the 256and512models respectively When combining classiﬁer guidance with upsampling
we only guide the lower resolution model
off diversity for much higher precision We achieve the best FIDs by using guidance at a lower
resolution before upsampling to a higher resolution indicating that these approaches complement
one another
6 Related Work
Score based generative models were introduced by Song and Ermon  59 as a way of modeling a
data distribution using its gradients and then sampling using Langevin dynamics  67 Ho et al  25
found a connection between this method and diffusion models  56 and achieved excellent sample
quality by leveraging this connection After this breakthrough work many works followed up with
more promising results Kong et al  30 and Chen et al  8 demonstrated that diffusion models
11work well for audio JolicoeurMartineau et al  26 found that a GANlike setup could improve
samples from these models Song et al  60 explored ways to leverage techniques from stochastic
differential equations to improve the sample quality obtained by scorebased models Song et al  57
and Nichol and Dhariwal  43 proposed methods to improve sampling speed Nichol and Dhariwal
43 and Saharia et al  53 demonstrated promising results on the difﬁcult ImageNet generation task
using upsampling diffusion models Also related to diffusion models and following the work of
SohlDickstein et al  56 Goyal et al  21 described a technique for learning a model with learned
iterative generation steps and found that it could achieve good image samples when trained with a
likelihood objective
One missing element from previous work on diffusion models is a way to trade off diversity for ﬁdelity
Other generative techniques provide natural levers for this tradeoff Brock et al  5 introduced the
truncation trick for GANs wherein the latent vector is sampled from a truncated normal distribution
They found that increasing truncation naturally led to a decrease in diversity but an increase in ﬁdelity
More recently Razavi et al  51 proposed to use classiﬁer rejection sampling to ﬁlter out bad samples
from an autoregressive likelihoodbased model and found that this technique improved FID Most
likelihoodbased models also allow for lowtemperature sampling  1 which provides a natural way
to emphasize modes of the data distribution see Appendix G
Other likelihoodbased models have been shown to produce highﬁdelity image samples VQV AE
65 and VQV AE2  51 are autoregressive models trained on top of quantized latent codes greatly
reducing the computational resources required to train these models on large images These models
produce diverse and high quality images but still fall short of GANs without expensive rejection
sampling and special metrics to compensate for blurriness DCTransformer  42 is a related method
which relies on a more intelligent compression scheme V AEs are another promising class of
likelihoodbased models and recent methods such as NV AE  63 and VDV AE  9 have successfully
been applied to difﬁcult image generation domains Energybased models are another class of
likelihoodbased models with a rich history  11024 Sampling from the EBM distribution is
challenging and Xie et al  70 demonstrate that Langevin dynamics can be used to sample coherent
images from these models Du and Mordatch  15 further improve upon this approach obtaining
high quality images More recently Gao et al  18 incorporate diffusion steps into an energybased
model and ﬁnd that doing so improves image samples from these models
Other works have controlled generative models with a pretrained classiﬁer For example an emerging
body of work  17472 aims to optimize GAN latent spaces for text prompts using pretrained CLIP
49 models More similar to our work Song et al  60 uses a classiﬁer to generate classconditional
CIFAR10 images with a diffusion model In some cases classiﬁers can act as standalone generative
models For example Santurkar et al  55 demonstrate that a robust image classiﬁer can be used as a
standalone generative model and Grathwohl et al  22 train a model which is jointly a classiﬁer and
an energybased model
7 Limitations and Future Work
While we believe diffusion models are an extremely promising direction for generative modeling
they are still slower than GANs at sampling time due to the use of multiple denoising steps and
therefore forward passes One promising work in this direction is from Luhman and Luhman  37
who explore a way to distill the DDIM sampling process into a single step model The samples
from the single step model are not yet competitive with GANs but are much better than previous
singlestep likelihoodbased models Future work in this direction might be able to completely close
the sampling speed gap between diffusion models and GANs without sacriﬁcing image quality
Our proposed classiﬁer guidance technique is currently limited to labeled datasets and we have
provided no effective strategy for trading off diversity for ﬁdelity on unlabeled datasets In the future
our method could be extended to unlabeled data by clustering samples to produce synthetic labels
36 or by training discriminative models to predict when samples are in the true data distribution or
from the sampling distribution
The effectiveness of classiﬁer guidance demonstrates that we can obtain powerful generative models
from the gradients of a classiﬁcation function This could be used to condition pretrained models
in a plethora of ways for example by conditioning an image generator with a text caption using a
noisy version of CLIP  49 similar to recent methods that guide GANs using text prompts  1747
122 It also suggests that large unlabeled datasets could be leveraged in the future to pretrain powerful
diffusion models that can later be improved by using a classiﬁer with desirable properties
8 Conclusion
We have shown that diffusion models a class of likelihoodbased models with a stationary training
objective can obtain better sample quality than stateoftheart GANs Our improved architecture
is sufﬁcient to achieve this on unconditional image generation tasks and our classiﬁer guidance
technique allows us to do so on classconditional tasks In the latter case we ﬁnd that the scale
of the classiﬁer gradients can be adjusted to trade off diversity for ﬁdelity These guided diffusion
models can reduce the sampling time gap between GANs and diffusion models although diffusion
models still require multiple forward passes during sampling Finally by combining guidance with
upsampling we can further improve sample quality on highresolution conditional image synthesis
9 Acknowledgements
We thank Alec Radford Mark Chen Pranav Shyam and Raul Puri for providing feedback on this
work
References
1David Ackley Geoffrey Hinton and Terrence Sejnowski A learning algorithm for boltzmann
machines Cognitive science 91147169  1985
2Adverb The big sleep httpstwittercomadvadnounstatus
1351038053033406468  2021
3 Shane Barratt and Rishi Sharma A note on the inception score arXiv180101973  2018
4Andrew Brock Theodore Lim J M Ritchie and Nick Weston Neural photo editing with
introspective adversarial networks arXiv160907093  2016
5Andrew Brock Jeff Donahue and Karen Simonyan Large scale gan training for high ﬁdelity
natural image synthesis arXiv180911096  2018
6Tom B Brown Benjamin Mann Nick Ryder Melanie Subbiah Jared Kaplan Prafulla Dhari
wal Arvind Neelakantan Pranav Shyam Girish Sastry Amanda Askell Sandhini Agarwal
Ariel HerbertV oss Gretchen Krueger Tom Henighan Rewon Child Aditya Ramesh Daniel M
Ziegler Jeffrey Wu Clemens Winter Christopher Hesse Mark Chen Eric Sigler Mateusz
Litwin Scott Gray Benjamin Chess Jack Clark Christopher Berner Sam McCandlish
Alec Radford Ilya Sutskever and Dario Amodei Language models are fewshot learners
arXiv200514165  2020
7Mark Chen Alec Radford Rewon Child Jeffrey Wu Heewoo Jun David Luan and Ilya
Sutskever Generative pretraining from pixels In International Conference on Machine
Learning  pages 16911703 PMLR 2020
8Nanxin Chen Yu Zhang Heiga Zen Ron J Weiss Mohammad Norouzi and William Chan
Wavegrad Estimating gradients for waveform generation arXiv200900713  2020
9Rewon Child Very deep vaes generalize autoregressive models and can outperform them on
images arXiv201110650  2021
10 Peter Dayan Geoffrey E Hinton Radford M Neal and Richard S Zemel The helmholtz
machine Neural computation  75889904 1995
11 Harm de Vries Florian Strub Jérémie Mary Hugo Larochelle Olivier Pietquin and Aaron
Courville Modulating early visual processing by language arXiv170700683  2017
12 DeepMind Biggandeep 128x128 on tensorﬂow hub httpstfhubdevdeepmind
biggandeep1281  2018
1313 Prafulla Dhariwal Heewoo Jun Christine Payne Jong Wook Kim Alec Radford and Ilya
Sutskever Jukebox A generative model for music arXiv200500341  2020
14 Jeff Donahue and Karen Simonyan Large scale adversarial representation learning
arXiv190702544  2019
15 Yilun Du and Igor Mordatch Implicit generation and generalization in energybased models
arXiv190308689  2019
16 Vincent Dumoulin Jonathon Shlens and Manjunath Kudlur A learned representation for
artistic style arXiv161007629  2017
17 Federico A Galatolo Mario G C A Cimino and Gigliola Vaglini Generating images from
caption and vice versa via clipguided generative latent space search arXiv210201645  2021
18 Ruiqi Gao Yang Song Ben Poole Ying Nian Wu and Diederik P Kingma Learning energy
based models by diffusion recovery likelihood arXiv201208125  2020
19 Ian J Goodfellow Jean PougetAbadie Mehdi Mirza Bing Xu David WardeFarley Sherjil
Ozair Aaron Courville and Yoshua Bengio Generative adversarial networks arXiv14062661 
2014
20 Google Cloud tpus httpscloudgooglecomtpu  2018
21 Anirudh Goyal Nan Rosemary Ke Surya Ganguli and Yoshua Bengio Variational walkback
Learning a transition operator as a stochastic recurrent net arXiv171102282  2017
22 Will Grathwohl KuanChieh Wang JörnHenrik Jacobsen David Duvenaud Mohammad
Norouzi and Kevin Swersky Your classiﬁer is secretly an energy based model and you should
treat it like one arXiv191203263  2019
23 Martin Heusel Hubert Ramsauer Thomas Unterthiner Bernhard Nessler and Sepp Hochreiter
Gans trained by a two timescale update rule converge to a local nash equilibrium Advances in
Neural Information Processing Systems 30 NIPS 2017  2017
24 Geoffrey E Hinton Training products of experts by minimizing contrastive divergence Neural
computation  14817711800 2002
25 Jonathan Ho Ajay Jain and Pieter Abbeel Denoising diffusion probabilistic models
arXiv200611239  2020
26 Alexia JolicoeurMartineau Rémi PichéTaillefer Rémi Tachet des Combes and Ioan
nis Mitliagkas Adversarial score matching and improved sampling for image generation
arXiv200905475  2020
27 Tero Karras Samuli Laine and Timo Aila A stylebased generator architecture for generative
adversarial networks arXivarXiv181204948  2019
28 Tero Karras Samuli Laine Miika Aittala Janne Hellsten Jaakko Lehtinen and Timo Aila
Analyzing and improving the image quality of stylegan arXiv191204958  2019
29 Diederik P Kingma and Jimmy Ba Adam A method for stochastic optimization
arXiv14126980  2014
30 Zhifeng Kong Wei Ping Jiaji Huang Kexin Zhao and Bryan Catanzaro Diffwave A versatile
diffusion model for audio synthesis arXiv200909761  2020
31 Alex Krizhevsky Vinod Nair and Geoffrey Hinton CIFAR10 Canadian Institute for Advanced
Research 2009 URL httpwwwcstorontoedukrizcifarhtml 
32 Tuomas Kynkäänniemi Tero Karras Samuli Laine Jaakko Lehtinen and Timo Aila Improved
precision and recall metric for assessing generative models arXiv190406991  2019
33 Guosheng Lin Anton Milan Chunhua Shen and Ian Reid Reﬁnenet Multipath reﬁnement
networks for highresolution semantic segmentation arXiv161106612  2016
1434 Ziwei Liu Ping Luo Xiaogang Wang and Xiaoou Tang Deep learning face attributes in the
wild In Proceedings of International Conference on Computer Vision ICCV  December 2015
35 Ilya Loshchilov and Frank Hutter Decoupled weight decay regularization arXiv171105101 
2017
36 Mario Lucic Michael Tschannen Marvin Ritter Xiaohua Zhai Olivier Bachem and Sylvain
Gelly Highﬁdelity image generation with fewer labels arXiv190302271  2019
37 Eric Luhman and Troy Luhman Knowledge distillation in iterative generative models for
improved sampling speed arXiv210102388  2021
38 Paulius Micikevicius Sharan Narang Jonah Alben Gregory Diamos Erich Elsen David Garcia
Boris Ginsburg Michael Houston Oleksii Kuchaiev Ganesh Venkatesh and Hao Wu Mixed
precision training arXiv171003740  2017
39 Mehdi Mirza and Simon Osindero Conditional generative adversarial nets arXiv14111784 
2014
40 Takeru Miyato and Masanori Koyama cgans with projection discriminator arXiv180205637 
2018
41 Takeru Miyato Toshiki Kataoka Masanori Koyama and Yuichi Yoshida Spectral normalization
for generative adversarial networks arXiv180205957  2018
42 Charlie Nash Jacob Menick Sander Dieleman and Peter W Battaglia Generating images with
sparse representations arXiv210303841  2021
43 Alex Nichol and Prafulla Dhariwal Improved denoising diffusion probabilistic models
arXiv210209672  2021
44 NVIDIA Stylegan2 httpsgithubcomNVlabsstylegan2  2019
45 Gaurav Parmar Richard Zhang and JunYan Zhu On buggy resizing libraries and surprising
subtleties in ﬁd calculation arXiv210411222  2021
46 Adam Paszke Sam Gross Francisco Massa Adam Lerer James Bradbury Gregory Chanan
Trevor Killeen Zeming Lin Natalia Gimelshein Luca Antiga et al Pytorch An imperative
style highperformance deep learning library arXiv191201703  2019
47 Or Patashnik Zongze Wu Eli Shechtman Daniel CohenOr and Dani Lischinski Styleclip
Textdriven manipulation of stylegan imagery arXiv210317249  2021
48 Ethan Perez Florian Strub Harm de Vries Vincent Dumoulin and Aaron Courville Film
Visual reasoning with a general conditioning layer arXiv170907871  2017
49 Alec Radford Jong Wook Kim Chris Hallacy Aditya Ramesh Gabriel Goh Sandhini
Agarwal Girish Sastry Amanda Askell Pamela Mishkin Jack Clark Gretchen Krueger
and Ilya Sutskever Learning transferable visual models from natural language supervision
arXiv210300020  2021
50 Aditya Ramesh Mikhail Pavlov Gabriel Goh Scott Gray Chelsea V oss Alec Radford Mark
Chen and Ilya Sutskever Zeroshot texttoimage generation arXiv210212092  2021
51 Ali Razavi Aaron van den Oord and Oriol Vinyals Generating diverse highﬁdelity images
with VQV AE2 arXiv190600446  2019
52 Olga Russakovsky Jia Deng Hao Su Jonathan Krause Sanjeev Satheesh Sean Ma Zhiheng
Huang Andrej Karpathy Aditya Khosla Michael Bernstein Alexander C Berg and Li FeiFei
Imagenet large scale visual recognition challenge arXiv14090575  2014
53 Chitwan Saharia Jonathan Ho William Chan Tim Salimans David J Fleet and Mohammad
Norouzi Image superresolution via iterative reﬁnement arXivarXiv210407636  2021
1554 Tim Salimans Ian Goodfellow Wojciech Zaremba Vicki Cheung Alec Radford and Xi Chen
Improved techniques for training gans arXiv160603498  2016
55 Shibani Santurkar Dimitris Tsipras Brandon Tran Andrew Ilyas Logan Engstrom and
Aleksander Madry Image synthesis with a single robust classiﬁer arXiv190609453  2019
56 Jascha SohlDickstein Eric A Weiss Niru Maheswaranathan and Surya Ganguli Deep
unsupervised learning using nonequilibrium thermodynamics arXiv150303585  2015
57 Jiaming Song Chenlin Meng and Stefano Ermon Denoising diffusion implicit models
arXiv201002502  2020
58 Yang Song and Stefano Ermon Improved techniques for training scorebased generative models
arXiv200609011  2020
59 Yang Song and Stefano Ermon Generative modeling by estimating gradients of the data
distribution arXivarXiv190705600  2020
60 Yang Song Jascha SohlDickstein Diederik P Kingma Abhishek Kumar Stefano Ermon
and Ben Poole Scorebased generative modeling through stochastic differential equations
arXiv201113456  2020
61 Christian Szegedy Wojciech Zaremba Ilya Sutskever Joan Bruna Dumitru Erhan Ian Good
fellow and Rob Fergus Intriguing properties of neural networks arXiv13126199  2013
62 Christian Szegedy Vincent Vanhoucke Sergey Ioffe Jonathon Shlens and Zbigniew Wojna
Rethinking the inception architecture for computer vision arXiv151200567  2015
63 Arash Vahdat and Jan Kautz Nvae A deep hierarchical variational autoencoder
arXiv200703898  2020
64 Aaron van den Oord Sander Dieleman Heiga Zen Karen Simonyan Oriol Vinyals Alex
Graves Nal Kalchbrenner Andrew Senior and Koray Kavukcuoglu Wavenet A generative
model for raw audio arXiv160903499  2016
65 Aaron van den Oord Oriol Vinyals and Koray Kavukcuoglu Neural discrete representation
learning arXiv171100937  2017
66 Ashish Vaswani Noam Shazeer Niki Parmar Jakob Uszkoreit Llion Jones Aidan N Gomez
Lukasz Kaiser and Illia Polosukhin Attention is all you need arXiv170603762  2017
67 Max Welling and Yee W Teh Bayesian learning via stochastic gradient langevin dynamics
InProceedings of the 28th international conference on machine learning ICML11  pages
681688 Citeseer 2011
68 Yan Wu Jeff Donahue David Balduzzi Karen Simonyan and Timothy Lillicrap Logan Latent
optimisation for generative adversarial networks arXiv191200953  2019
69 Yuxin Wu and Kaiming He Group normalization arXiv180308494  2018
70 Jianwen Xie Yang Lu SongChun Zhu and Ying Nian Wu A theory of generative convnet
arXiv160203264  2016
71 Fisher Yu Ari Seff Yinda Zhang Shuran Song Thomas Funkhouser and Jianxiong Xiao
Lsun Construction of a largescale image dataset using deep learning with humans in the loop
arXiv150603365  2015
72 Han Zhang Tao Xu Hongsheng Li Shaoting Zhang Xiaogang Wang Xiaolei Huang and
Dimitris Metaxas Stackgan Text to photorealistic image synthesis with stacked generative
adversarial networks arXiv161203242  2016
73 Ligeng Zhu Thop httpsgithubcomLyken17pytorchOpCounter  2018
16A Computational Requirements
Compute is essential to modern machine learning applications and more compute typically yields
better results It is thus important to compare our methods compute requirements to competing
methods In this section we demonstrate that we can achieve results better than StyleGAN2 and
BigGANdeep with the same or lower compute budget
A1 Throughput
We ﬁrst benchmark the throughput of our models in Table 7 For the theoretical throughput we
measure the theoretical FLOPs for our model using THOP  73 and assume 100 utilization of an
NVIDIA Tesla V100 120 TFLOPs while for the actual throughput we use measured wallclock
time We include communication time across two machines whenever our training batch size doesnt
ﬁt on a single machine where each of our machines has 8 V100s
We ﬁnd that a naive implementation of our models in PyTorch 17 is very inefﬁcient utilizing only
2030 of the hardware We also benchmark our optimized version which use larger perGPU batch
sizes fused GroupNormSwish and fused Adam CUDA ops For our ImageNet 128 128 model in
particular we ﬁnd that we can increase the perGPU batch size from 4 to 32 while still ﬁtting in GPU
memory and this makes a large utilization difference Our implementation is still far from optimal
and further optimizations should allow us to reach higher levels of utilization
Model ImplementationBatch Size ThroughputUtilizationper GPU Imgs per V100sec
6464Theoretical  1823 100
Naive 32 370 20
Optimized 96 741 41
128128Theoretical  652 100
Naive 4 115 18
Optimized 32 248 38
256256Theoretical  179 100
Naive 4 44 25
Optimized 8 64 36
64256Theoretical  317 100
Naive 4 63 20
Optimized 12 95 30
128512Theoretical  80 100
Naive 2 19 24
Optimized 2 23 29
Table 7 Throughput of our ImageNet models measured in Images per V100sec
A2 Early stopping
In addition we can train for many fewer iterations while maintaining sample quality superior to
BigGANdeep Table 8 and 9 evaluate our ImageNet 128 128 and 256256 models throughout
training We can see that the ImageNet 128 128 model beats BigGANdeeps FID 602 after 500K
training iterations only one eighth of the way through training Similarly the ImageNet 256 256
model beats BigGANdeep after 750K iterations roughly a third of the way through training
Iterations FID sFID Precision Recall
250K 797 648 080 050
500K 531 597 083 049
1000K 410 580 081 051
2000K 342 569 083 053
4360K 309 559 082 054
Table 8 Evaluating an ImageNet 128 128 model throughout training classiﬁer scale 10
17Iterations FID sFID Precision Recall
250K 1221 615 078 050
500K 795 551 081 050
750K 649 539 081 050
1000K 574 529 081 052
1500K 501 520 082 052
1980K 459 525 082 052
Table 9 Evaluating an ImageNet 256 256 model throughout training classiﬁer scale 10
A3 Compute comparison
Finally in Table 10 we compare the compute of our models with StyleGAN2 and BigGANdeep and
show we can obtain better FIDs with a similar compute budget For BigGANdeep Brock et al  5 do
not explicitly describe the compute requirements for training their models but rather provide rough
estimates in terms of days on a Google TPUv3 pod  20 We convert their TPUv3 estimates to V100
days according to 2 TPUv3 day  1 V100 day For StyleGAN2 we use the reported throughput of
25M images over 32 days 13 hour on one V100 for conﬁgf  44 We note that our classiﬁer training
is relatively lightweight compared to training the generative model
Model Generator Classiﬁer Total FID sFID Precision Recall
Compute Compute Compute
LSUN Horse 256256
StyleGAN2 28 130 384 646 063 048
ADM 250K 116  116 295 594 069 055
ADM dropout 250K 116  116 257 681 071 055
LSUN Cat 256256
StyleGAN2 28 115 725 633 058 043
ADM dropout 200K 92  92 557 669 063 052
ImageNet 128128
BigGANdeep 5 64128 602 718 086 035
ADMG 4360K 521 9 530 309 559 082 054
ADMG 450K 54 9 63 567 619 082 049
ImageNet 256256
BigGANdeep 5 128256 695 736 087 028
ADMG 1980K 916 46 962 459 525 082 052
ADMG 750K 347 46 393 649 539 081 050
ADMG 750K 347 14y361 668 534 081 051
ADMG 540K ADMU 500K 329 30 359 385 586 084 053
ADMG 540K ADMU 150K 219 30 249 415 614 082 054
ADMG 200K ADMU 150K 110 10z126 493 582 082 052
ImageNet 512512
BigGANdeep 5 256512 843 813 088 029
ADMG 4360K ADMU 1050K 1878 36 1914 385 586 084 053
ADMG 500K ADMU 100K 189 9 198 759 684 084 053
Table 10 Training compute requirements for our diffusion models compared to StyleGAN2 and
BigGANdeep Training iterations for each diffusion model are mentioned in parenthesis Compute
is measured in V100daysyImageNet 256256 classiﬁer with 150K iterations instead of 500K
zImageNet 6464 classiﬁer with batch size 256 instead of 1024 ImageNet 128 128 classiﬁer
with batch size 256 instead of 1024
18B Detailed Formulation of DDPM
Here we provide a detailed review of the formulation of Gaussian diffusion models from Ho et al
25 We start by deﬁning our data distribution x0qx0and a Markovian noising process qwhich
gradually adds noise to the data to produce noised samples x1throughxT In particular each step of
the noising process adds Gaussian noise according to some variance schedule given by t
qxtjxt1Nxtp
1txt1tI 15
Ho et al  25 note that we need not apply qrepeatedly to sample from xtqxtjx0 Instead
qxtjx0can be expressed as a Gaussian distribution With t 1tandtQt
s0s
qxtjx0 Nxtptx01tI 16
ptx0p
1t N0I 17
Here 1ttells us the variance of the noise for an arbitrary timestep and we could equivalently
use this to deﬁne the noise schedule instead of t
Using Bayes theorem one ﬁnds that the posterior qxt1jxtx0is also a Gaussian with mean
txtx0and variance tdeﬁned as follows
txtx0pt1t
1tx0pt1t1
1txt 18
t1t1
1tt 19
qxt1jxtx0 Nxt1 xtx0tI 20
If we wish to sample from the data distribution qx0 we can ﬁrst sample from qxTand then sample
reverse steps qxt1jxtuntil we reach x0 Under reasonable settings for tandT the distribution
qxTis nearly an isotropic Gaussian distribution so sampling xTis trivial All that is left is to
approximate qxt1jxtusing a neural network since it cannot be computed exactly when the data
distribution is unknown To this end SohlDickstein et al  56 note thatqxt1jxtapproaches a
diagonal Gaussian distribution as T1 and correspondingly t0 so it is sufﬁcient to train a
neural network to predict a mean and a diagonal covariance matrix 
pxt1jxtNxt1xttxtt 21
To train this model such that px0learns the true data distribution qx0 we can optimize the
following variational lowerbound Lvlbforpx0
LvlbL0L1LT1LT 22
L0logpx0jx1 23
Lt1DKLqxt1jxtx0jjpxt1jxt 24
LTDKLqxTjx0jjpxT 25
While the above objective is welljustiﬁed Ho et al  25 found that a different objective produces
better samples in practice In particular they do not directly parameterize xttas a neural
network but instead train a model xttto predictfrom Equation 17 This simpliﬁed objective
is deﬁned as follows
Lsimple Et1Tx0qx0N0Ijjxttjj2 26
During sampling we can use substitution to derive xttfromxtt
xtt 1pt
xt1tp1txtt
27
Note thatLsimple does not provide any learning signal for xtt Ho et al  25 ﬁnd that instead of
learning xtt they can ﬁx it to a constant choosing either tIortI These values correspond
to upper and lower bounds for the true reverse step variance
19C Nearest Neighbors for Samples
Figure 7 Nearest neighbors for samples from a classiﬁer guided model on ImageNet 256 256 For
each image the top row is a sample and the remaining rows are the top 3 nearest neighbors from the
dataset The top samples were generated with classiﬁer scale 1 and 250 diffusion sampling steps FID
459 The bottom samples were generated with classiﬁer scale 25 and 25 DDIM steps FID 544
Our models achieve their best FID when using a classiﬁer to reduce the diversity of the generations
One might fear that such a process could cause the model to recall existing images from the training
dataset especially as the classiﬁer scale is increased To test this we looked at the nearest neighbors
in InceptionV3  62 feature space for a handful of samples Figure 7 shows our results revealing
that the samples are indeed unique and not stored in the training set
D Effect of Varying the Classiﬁer Scale
Figure 8 Samples when increasing the classiﬁer scale from 00 left to 55 right Each row
corresponds to a ﬁxed noise seed We observe that the classiﬁer drastically changes some images
while leaving others relatively unaffected
20E LSUN Diversity Comparison
Figure 9 Samples from StyleGAN2 or StyleGAN for bedrooms with truncation 10 left vs
samples from our diffusion models middle and samples from the training set right
21F Interpolating Between Dataset Images Using DDIM
The DDIM  57 sampling process is deterministic given the initial noise xT thus giving rise to an
implicit latent space It corresponds to integrating an ODE in the forward direction and we can run
the process in reverse to get the latents that produce a given real image Here we experiment with
encoding real images into this latent space and then interpolating between them
Equation 13 for the generative pass in DDIM looks like
xt1xtpt1hp
1tp
1t1
xtp
1t11p
1t1
xti
Thus in the limit of small steps we can expect the reversal of this ODE in the forward direction
looks like
xt1xtpt1hp
1tp
1t1
xtp
1t11p
1t1
xti
We found that this reverse ODE approximation gives latents with reasonable reconstructions even
with as few as 250 reverse steps However we noticed some noise artifacts when reversing all 250
steps and ﬁnd that reversing the ﬁrst 249 steps gives much better reconstructions To interpolate
the latents class embeddings and classiﬁer log probabilities we use cosx0sinx1where
sweeps linearly from 0 to
2
Figures 10athrough 10cshow DDIM latent space interpolations on a classconditional 256 256
model while varying the classiﬁer scale The left and rightmost images are ground truth dataset
examples and between them are reconstructed interpolations in DDIM latent space including both
endpoints We see that the model with no guidance has almost perfect reconstructions due to its high
recall whereas raising the guidance scale to 25 only ﬁnds approximately similar reconstructions
Figure 10a DDIM latent reconstructions and interpolations on real images with no classiﬁer guidance
22Figure 10b DDIM latent reconstructions and interpolations on real images with classiﬁer scale 10
Figure 10c DDIM latent reconstructions and interpolations on real images with classiﬁer scale 25
23G Reduced Temperature Sampling
We achieved our best ImageNet samples by reducing the diversity of our models using classiﬁer
guidance For many classes of generative models there is a much simpler way to reduce diversity
reducing the temperature  1 The temperature parameter is typically setup so that  10corre
sponds to standard sampling and  10focuses more on highdensity samples We experimented
with two ways of implementing this for diffusion models ﬁrst by scaling the Gaussian noise used
for each transition by  and second by dividing xtby The latter implementation makes
sense when thinking about as a rescaled score function see Section 42 and scaling up the score
function is similar to scaling up classiﬁer gradients
To measure how temperature scaling affects samples we experimented with our ImageNet 128 128
model evaluating FID Precision and Recall across different temperatures Figure 11 We ﬁnd
that two techniques behave similarly and neither technique provides any substantial improvement in
our evaluation metrics We also ﬁnd that low temperatures have both low precision and low recall
indicating that the model is not focusing on modes of the real data distribution Figure 12 highlights
this effect indicating that reducing temperature produces blurry smooth images
103102
1  temperature05101520FIDnoise temperature
epsilon temperature
103102
1  temperature04050607Precision
noise temperature
epsilon temperature
103102
1  temperature045050055060065Recall
noise temperature
epsilon temperature
Figure 11 The effect of changing temperature for an ImageNet 128 128 model
Figure 12 Samples at temperature 098 with epsilon scaling left and noise scaling right
24H Conditional Diffusion Process
In this section we show that conditional sampling can be achieved with a transition operator
proportional to pxtjxt1pyjxt wherepxtjxt1approximates qxtjxt1andpyjxt
approximates the label distribution for a noised sample xt
We start by deﬁning a conditional Markovian noising process qsimilar toq and assume that qyjx0
is a known and readily available label distribution for each sample
qx0qx0 28
qyjx0Known labels per sample 29
qxt1jxtyqxt1jxt 30
qx1Tjx0yTY
t1qxtjxt1y 31
While we deﬁned the noising process qconditioned on y we can prove that qbehaves exactly like
qwhen not conditioned on y Along these lines we ﬁrst derive the unconditional noising operator
qxt1jxt
qxt1jxt Z
yqxt1yjxtdy 32
Z
yqxt1jxtyqyjxtdy 33
Z
yqxt1jxtqyjxtdy 34
qxt1jxtZ
yqyjxtdy 35
qxt1jxt 36
 qxt1jxty 37
Following similar logic we ﬁnd the joint distribution qx1Tjx0
qx1Tjx0 Z
yqx1Tyjx0dy 38
Z
yqyjx0qx1Tjx0ydy 39
Z
yqyjx0TY
t1qxtjxt1ydy 40
Z
yqyjx0TY
t1qxtjxt1dy 41
TY
t1qxtjxt1Z
yqyjx0dy 42
TY
t1qxtjxt1 43
qx1Tjx0 44
25Using Equation 44 we can now derive qxt
qxt Z
x0t1qx0xtdx0t1 45
Z
x0t1qx0qx1xtjx0dx0t1 46
Z
x0t1qx0qx1xtjx0dx0t1 47
Z
x0t1qx0xtdx0t1 48
qxt 49
50
Using the identities qxt qxtandqxt1jxt qxt1jxt it is trivial to show via Bayes rule
that the unconditional reverse process qxtjxt1 qxtjxt1
One observation about qis that it gives rise to a noisy classiﬁcation function qyjxt We can show
that this classiﬁcation distribution does not depend on xt1a noisier version of xt a fact which we
will later use
qyjxtxt1  qxt1jxtyqyjxt
qxt1jxt51
 qxt1jxtqyjxt
qxt1jxt52
 qyjxt 53
54
We can now derive the conditional reverse process
qxtjxt1y qxtxt1y
qxt1y55
qxtxt1y
qyjxt1qxt156
qxtjxt1qyjxtxt1qxt1
qyjxt1qxt157
qxtjxt1qyjxtxt1
qyjxt158
qxtjxt1qyjxt
qyjxt159
qxtjxt1qyjxt
qyjxt160
61
Theqyjxt1term can be treated as a constant since it does not depend on xt We thus want to
sample from the distribution Zqxtjxt1qyjxtwhereZis a normalizing constant We already
have a neural network approximation of qxtjxt1 calledpxtjxt1 so all that is left is an
approximation of qyjxt This can be obtained by training a classiﬁer pyjxton noised images xt
derived by sampling from qxt
26I Hyperparameters
When choosing optimal classiﬁer scales for our sampler we swept over 0512for ImageNet
128128 and ImageNet 256 256 and 123354455for ImageNet 512512 For DDIM
we swept over values 05075101252for ImageNet 12812805115225335for
ImageNet 256256 and 34567911for ImageNet 512512
Hyperparameters for training the diffusion and classiﬁcation models are in Table 11 and Table 12
respectively Hyperparameters for guided sampling are in Table 14 Hyperparameters used to train
upsampling models are in Table 13 We train all of our models using Adam  29 or AdamW  35
with1 09and2 0999 We train in 16bit precision using lossscaling  38 but maintain
32bit weights EMA and optimizer state We use an EMA rate of 09999 for all experiments We
use PyTorch 46 and train on NVIDIA Tesla V100s
For all architecture ablations we train with batch size 256 and sample using 250 sampling steps
For our attention heads ablations we use 128 base channels 2 residual blocks per resolution multi
resolution attention and BigGAN updownsampling and we train the models for 700K iterations By
default all of our experiments use adaptive group normalization except when explicitly ablating for
it
When sampling with 1000 timesteps we use the same noise schedule as for training On ImageNet
we use the uniform stride from Nichol and Dhariwal  43 for 250 step samples and the slightly
different uniform stride from Song et al 57 for 25 step DDIM
LSUN ImageNet 64 ImageNet 128 ImageNet 256 ImageNet 512
Diffusion steps 1000 1000 1000 1000 1000
Noise Schedule linear cosine linear linear linear
Model size 552M 296M 422M 554M 559M
Channels 256 192 256 256 256
Depth 2 3 2 2 2
Channels multiple 112244 1234 11234 112244 05112244
Heads 4
Heads Channels 64 64 64 64
Attention resolution 32168 32168 32168 32168 32168
BigGAN updownsample 3 3 3 3 3
Dropout 01 01 00 00 00
Batch size 256 2048 256 256 256
Iterations varies 540K 4360K 1980K 1940K
Learning Rate 1e4 3e4 1e4 1e4 1e4
Table 11 Hyperparameters for diffusion models We used 200K iterations for LSUN cat 250K for
LSUN horse and 500K for LSUN bedroom
ImageNet 64 ImageNet 128 ImageNet 256 ImageNet 512
Diffusion steps 1000 1000 1000 1000
Noise Schedule cosine linear linear linear
Model size 65M 43M 54M 54M
Channels 128 128 128 128
Depth 4 2 2 2
Channels multiple 1234 11234 112244 05112244
Heads Channels 64 64 64 64
Attention resolution 32168 32168 32168 32168
BigGAN updownsample 3 3 3 3
Attention pooling 3 3 3 3
Weight decay 02 005 005 005
Batch size 1024 256 256 256
Iterations 300K 300K 500K 500K
Learning rate 6e4 3e4 3e4 3e4
Table 12 Hyperparameters for classiﬁcation models For our ImageNet 128 128512512
upsamples we used a different classiﬁer for the base model with batch size 1024 and learning rate
6e5
27ImageNet 64256 ImageNet 128512
Diffusion steps 1000 1000
Noise Schedule linear linear
Model size 312M 309M
Channels 192 192
Depth 2 2
Channels multiple 112244 112244
Heads 4
Heads Channels 64
Attention resolution 32168 32168
BigGAN updownsample 3 3
Dropout 00 00
Batch size 256 256
Iterations 500K 1050K
Learning Rate 1e4 1e4
Table 13 Hyperparameters for upsampling diffusion models We chose this as an optimization with
the intuition that a lowerresolution path should be unnecessary for upsampling 128x128 images
ImageNet 64 ImageNet 128 ImageNet 256 ImageNet 512
Gradient Scale 250 steps 10 05 10 40
Gradient Scale DDIM 25 steps  125 25 90
Table 14 Hyperparameters for classiﬁerguided sampling
28J Using Fewer Sampling Steps on LSUN
We initially found that our LSUN models achieved much better results when sampling with 1000
steps rather than 250 steps contrary to previous results from Nichol and Dhariwal  43 To address
this we conducted a sweep over samplingtime noise schedules ﬁnding that an improved schedule
can largely close the gap We swept over schedules on LSUN bedrooms and selected the schedule
with the best FID for use on the other two datasets Table 15 details the ﬁndings of this sweep and
Table 16 applies this schedule to three LSUN datasets
While sweeping over sampling schedules is not as expensive as retraining models from scratch it
does require a signiﬁcant amount of sampling compute As a result we did not conduct an exhaustive
sweep and superior schedules are likely to exist
Schedule FID
5050505050 231
7060504030 217
9050404030 210
9060503020 209
8060503030 209
9050503030 207
10050403030 203
9060602020 202
Table 15 Results of sweeping over 250 step sampling schedules on LSUN bedrooms The schedule
is expressed as a sequence of ﬁve integers where each integer is the number of steps allocated to
one ﬁfth of the diffusion process The ﬁrst integer corresponding to t20199 and the last to
t2T200T1 Thus 5050505050is a uniform schedule and 2500000is a schedule
where all timesteps are spent near t 0
Schedule FID sFID Prec Rec
LSUN Bedrooms 256 256
1000 steps 190 559 066 051
250 steps uniform 231 612 065 050
250 steps sweep 202 612 067 050
LSUN Horses 256256
1000 steps 257 681 071 055
250 steps uniform 345 755 068 056
250 steps sweep 283 708 069 056
LSUN Cat 256256
1000 steps 557 669 063 052
250 steps uniform 703 824 060 053
250 steps sweep 594 743 062 052
Table 16 Evaluations on LSUN bedrooms horses and cats using different sampling schedules We
ﬁnd that the sweep schedule produces better results than the uniform 250 step schedule on all three
datasets and mostly bridges the gap to the 1000 step schedule
29K Samples from ImageNet 512 512
Figure 13 Samples from our best 512 512 model FID 385 Classes are 1 goldﬁsh 279 arctic
fox 323 monarch butterﬂy 386 african elephant 130 ﬂamingo 852 tennis ball
30Figure 14 Samples from our best 512 512 model FID 385 Classes are 933 cheeseburger 562
fountain 417 balloon 281 tabby cat 90 lorikeet 992 agaric
31Figure 15 Difﬁcult class samples from our best 512 512 model FID 385 Classes are 432
bassoon 468 cab 424 barbershop 444 bicyclebuiltfortwo 981 ballplayer 550 espresso maker
32Figure 16 Samples from our guided 512 512 model using 250 steps with classiﬁer scale 40 FID
772 Classes are 1 goldﬁsh 279 arctic fox 323 monarch butterﬂy 386 african elephant 130
ﬂamingo 852 tennis ball
33Figure 17 Samples from our guided 512 512 model using 250 steps with classiﬁer scale 40 FID
772 Classes are 933 cheeseburger 562 fountain 417 balloon 281 tabby cat 90 lorikeet 992
agaric
34Figure 18 Random samples from our best ImageNet 512 512 model FID 385
35Figure 19 Random samples from our guided 512 512 model using 250 steps with classiﬁer scale
40 FID 772
36L Samples from ImageNet 256 256
Figure 20 Samples using our best 256 256 model FID 394 Classes are 1 goldﬁsh 279 arctic
fox 323 monarch butterﬂy 386 african elephant 130 ﬂamingo 852 tennis ball 933 cheeseburger
562 fountain 417 balloon 281 tabby cat 90 lorikeet 992 agaric
37Figure 21 Samples from our guided 256 256 model using 250 steps with classiﬁer scale 10 FID
459 Classes are 1 goldﬁsh 279 arctic fox 323 monarch butterﬂy 386 african elephant 130
ﬂamingo 852 tennis ball 933 cheeseburger 562 fountain 417 balloon 281 tabby cat 90
lorikeet 992 agaric
38Figure 22 Samples from our guided 256 256 model using 25 DDIM steps with classiﬁer scale 25
FID 544 Classes are 1 goldﬁsh 279 arctic fox 323 monarch butterﬂy 386 african elephant
130 ﬂamingo 852 tennis ball 933 cheeseburger 562 fountain 417 balloon 281 tabby cat 90
lorikeet 992 agaric
39Figure 23 Random samples from our best 256 256 model FID 394
40Figure 24 Random samples from our guided 256 256 model using 250 steps with classiﬁer scale
10 FID 459
41M Samples from LSUN
Figure 25 Random samples from our LSUN bedroom model using 1000 sampling steps FID 190
42Figure 26 Random samples from our LSUN horse model using 1000 sampling steps FID 257
43Figure 27 Random samples from our LSUN cat model using 1000 sampling steps FID 557
44
  Mixtral of Experts
Albert Q Jiang Alexandre Sablayrolles Antoine Roux Arthur Mensch
Blanche Savary Chris Bamford Devendra Singh Chaplot Diego de las Casas
Emma Bou Hanna Florian Bressand Gianna Lengyel Guillaume Bour
Guillaume Lample Lélio Renard Lavaud Lucile Saulnier MarieAnne Lachaux
Pierre Stock Sandeep Subramanian Sophia Yang Szymon Antoniak Teven Le Scao
Théophile Gervet Thibaut Lavril Thomas Wang Timothée Lacroix William El Sayed
Abstract
We introduce Mixtral 8x7B a Sparse Mixture of Experts SMoE language
model Mixtral has the same architecture as Mistral 7B with the difference
that each layer is composed of 8 feedforward blocks ie experts For every
token at each layer a router network selects two experts to process the current
state and combine their outputs Even though each token only sees two experts
the selected experts can be different at each timestep As a result each token
has access to 47B parameters but only uses 13B active parameters during
inference Mixtral was trained with a context size of 32k tokens and it outperforms
or matches Llama 2 70B and GPT35 across all evaluated benchmarks In
particular Mixtral vastly outperforms Llama 2 70B on mathematics code
generation and multilingual benchmarks We also provide a model fine
tuned to follow instructions Mixtral 8x7B  Instruct that surpasses GPT35
Turbo Claude21 Gemini Pro and Llama 2 70B  chat model on human bench
marks Both the base and instruct models are released under the Apache 20 license
Code httpsgithubcommistralaimistralsrc
Webpage httpsmistralainewsmixtralofexperts
1 Introduction
In this paper we present Mixtral 8x7B a sparse mixture of experts model SMoE with open weights
licensed under Apache 20 Mixtral outperforms Llama 2 70B and GPT35 on most benchmarks As
it only uses a subset of its parameters for every token Mixtral allows faster inference speed at low
batchsizes and higher throughput at large batchsizes
Mixtral is a sparse mixtureofexperts network It is a decoderonly model where the feedforward
block picks from a set of 8 distinct groups of parameters At every layer for every token a router
network chooses two of these groups the experts to process the token and combine their output
additively This technique increases the number of parameters of a model while controlling cost and
latency as the model only uses a fraction of the total set of parameters per token
Mixtral is pretrained with multilingual data using a context size of 32k tokens It either matches
or exceeds the performance of Llama 2 70B and GPT35 over several benchmarks In particulararXiv240104088v1  csLG  8 Jan 2024Figure 1 Mixture of Experts Layer Each input vector is assigned to 2 of the 8 experts by a router The
layers output is the weighted sum of the outputs of the two selected experts In Mixtral an expert is a standard
feedforward block as in a vanilla transformer architecture
Mixtral demonstrates superior capabilities in mathematics code generation and tasks that require
multilingual understanding significantly outperforming Llama 2 70B in these domains Experiments
show that Mixtral is able to successfully retrieve information from its context window of 32k tokens
regardless of the sequence length and the location of the information in the sequence
We also present Mixtral 8x7B  Instruct a chat model finetuned to follow instructions using
supervised finetuning and Direct Preference Optimization  25 Its performance notably surpasses
that of GPT35 Turbo Claude21 Gemini Pro and Llama 2 70B  chat model on human evaluation
benchmarks Mixtral  Instruct also demonstrates reduced biases and a more balanced sentiment
profile in benchmarks such as BBQ and BOLD
We release both Mixtral 8x7B and Mixtral 8x7B  Instruct under the Apache 20 license1 free for
academic and commercial usage ensuring broad accessibility and potential for diverse applications
To enable the community to run Mixtral with a fully opensource stack we submitted changes to
the vLLM project which integrates Megablocks CUDA kernels for efficient inference Skypilot also
allows the deployment of vLLM endpoints on any instance in the cloud
2 Architectural details
Parameter Value
dim 4096
n_layers 32
head_dim 128
hidden_dim 14336
n_heads 32
n_kv_heads 8
context_len 32768
vocab_size 32000
num_experts 8
top_k_experts 2
Table 1 Model architectureMixtral is based on a transformer architecture  31 and uses the same
modifications as described in  18 with the notable exceptions that Mix
tral supports a fully dense context length of 32k tokens and the feed
forward blocks are replaced by MixtureofExpert layers Section 21
The model architecture parameters are summarized in Table 1
21 Sparse Mixture of Experts
We present a brief overview of the Mixture of Experts layer Figure 1
For a more indepth overview see  12 The output of the MoE module
for a given input xis determined by the weighted sum of the outputs
of the expert networks where the weights are given by the gating
networks output ie given nexpert networks E0 Ei  E n1 the
output of the expert layer is given by
n1X
i0GxiEix
Here Gxidenotes the ndimensional output of the gating network for the ith expert and Eix
is the output of the ith expert network If the gating vector is sparse we can avoid computing
the outputs of experts whose gates are zero There are multiple alternative ways of implementing
Gx61535 but a simple and performant one is implemented by taking the softmax over the
TopK logits of a linear layer 28 We use
Gx  Softmax TopK xWg
where TopK ℓiℓiifℓiis among the topK coordinates of logits ℓRnandTopK ℓi
otherwise The value of K  the number of experts used per token  is a hyperparameter that modu
lates the amount of compute used to process each token If one increases nwhile keeping Kfixed one
1httpsmistralainewsmixtralofexperts
2can increase the models parameter count while keeping its computational cost effectively constant
This motivates a distinction between the models total parameter count commonly referenced as the
sparse parameter count which grows with n and the number of parameters used for processing an
individual token called the active parameter count which grows with Kup to n
MoE layers can be run efficiently on single GPUs with high performance specialized kernels For
example Megablocks  13 casts the feedforward network FFN operations of the MoE layer as large
sparse matrix multiplications significantly enhancing the execution speed and naturally handling
cases where different experts get a variable number of tokens assigned to them Moreover the
MoE layer can be distributed to multiple GPUs through standard Model Parallelism techniques and
through a particular kind of partitioning strategy called Expert Parallelism EP  28 During the MoE
layers execution tokens meant to be processed by a specific expert are routed to the corresponding
GPU for processing and the experts output is returned to the original token location Note that EP
introduces challenges in load balancing as it is essential to distribute the workload evenly across the
GPUs to prevent overloading individual GPUs or hitting computational bottlenecks
In a Transformer model the MoE layer is applied independently per token and replaces the
feedforward FFN subblock of the transformer block For Mixtral we use the same SwiGLU
architecture as the expert function Eixand set K 2 This means each token is routed to two
SwiGLU subblocks with different sets of weights Taking this all together the output yfor an input
token xis computed as
yn1X
i0Softmax Top2 xWgiSwiGLU ix
This formulation is similar to the GShard architecture  21 with the exceptions that we replace all
FFN subblocks by MoE layers while GShard replaces every other block and that GShard uses a
more elaborate gating strategy for the second expert assigned to each token
3 Results
We compare Mixtral to Llama and rerun all benchmarks with our own evaluation pipeline for fair
comparison We measure performance on a wide variety of tasks categorized as follow
Commonsense Reasoning 0shot Hellaswag  32 Winogrande  26 PIQA  3 SIQA  27
OpenbookQA 22 ARCEasy ARCChallenge 8 CommonsenseQA 30
World Knowledge 5shot NaturalQuestions 20 TriviaQA 19
Reading Comprehension 0shot BoolQ 7 QuAC 5
Math GSM8K 9 8shot with maj8 and MATH 17 4shot with maj4
Code Humaneval 4 0shot and MBPP 1 3shot
Popular aggregated results MMLU  16 5shot BBH  29 3shot and AGI Eval  34
35shot English multiplechoice questions only
Figure 2 Performance of Mixtral and different Llama models on a wide range of benchmarks  All models
were reevaluated on all metrics with our evaluation pipeline for accurate comparison Mixtral outperforms or
matches Llama 2 70B on all benchmarks In particular it is vastly superior in mathematics and code generation
3ModelActive
ParamsMMLU HellaS WinoG PIQA Arce Arcc NQ TriQA HumanE MBPP Math GSM8K
LLaMA 2 7B 7B 444 771 695 779 687 432 175 566 116 261 39 160
LLaMA 2 13B 13B 556 807 729 808 752 488 167 640 189 354 60 343
LLaMA 1 33B 33B 568 837 762 822 796 544 241 685 250 409 84 441
LLaMA 2 70B 70B 699 854 804 826 799 565 254 730 293 498 138 696
Mistral 7B 7B 625 810 742 822 805 549 232 625 262 502 127 500
Mixtral 8x7B 13B 706 844 772 836 831 597 306 715 402 607 284 744
Table 2 Comparison of Mixtral with Llama Mixtral outperforms or matches Llama 2 70B performance on
almost all popular benchmarks while using 5x fewer active parameters during inference
Figure 3 Results on MMLU commonsense reasoning world knowledge and reading comprehension
math and code for Mistral 7B8x7B vs Llama 2 7B13B70B  Mixtral largely outperforms Llama 2 70B
on all benchmarks except on reading comprehension benchmarks while using 5x lower active parameters It
is also vastly superior to Llama 2 70B on code and math
Detailed results for Mixtral Mistral 7B and Llama 2 7B13B70B and Llama 1 34B2are reported
in Table 2 Figure 2 compares the performance of Mixtral with the Llama models in different
categories Mixtral surpasses Llama 2 70B across most metrics In particular Mixtral displays a
superior performance in code and mathematics benchmarks
Size and Efficiency We compare our performance to the Llama 2 family aiming to understand
Mixtral models efficiency in the costperformance spectrum see Figure 3 As a sparse Mixture
ofExperts model Mixtral only uses 13B active parameters for each token With 5x lower active
parameters Mixtral is able to outperform Llama 2 70B across most categories
Note that this analysis focuses on the active parameter count see Section 21 which is directly
proportional to the inference compute cost but does not consider the memory costs and hardware
utilization The memory costs for serving Mixtral are proportional to its sparse parameter count
47B which is still smaller than Llama 2 70B As for device utilization we note that the SMoEs layer
introduces additional overhead due to the routing mechanism and due to the increased memory loads
when running more than one expert per device They are more suitable for batched workloads where
one can reach a good degree of arithmetic intensity
Comparison with Llama 2 70B and GPT35 In Table 3 we report the performance of Mixtral 8x7B
compared to Llama 2 70B and GPT35 We observe that Mixtral performs similarly or above the
two other models On MMLU Mixtral obtains a better performance despite its significantly smaller
capacity 47B tokens compared to 70B For MT Bench we report the performance of the latest
GPT35Turbo model available gpt35turbo1106 
2Since Llama 2 34B was not opensourced we report results for Llama 1 34B
4LLaMA 2 70B GPT35 Mixtral 8x7B
MMLU
MCQ in 57 subjects699 700 706
HellaSwag
10shot871 855 867
ARC Challenge
25shot851 852 858
WinoGrande
5shot832 816 812
MBPP
pass1498 522 607
GSM8K
5shot536 571 584
MT Bench
for Instruct Models686 832 830
Table 3 Comparison of Mixtral with Llama 2 70B and GPT35 Mixtral outperforms or matches Llama 2
70B and GPT35 performance on most metrics
Evaluation Differences On some benchmarks there are some differences between our evaluation
protocol and the one reported in the Llama 2 paper 1 on MBPP we use the handverified subset 2
on TriviaQA we do not provide Wikipedia contexts
31 Multilingual benchmarks
Compared to Mistral 7B we significantly upsample the proportion of multilingual data during
pretraining The extra capacity allows Mixtral to perform well on multilingual benchmarks while
maintaining a high accuracy in English In particular Mixtral significantly outperforms Llama 2 70B
in French German Spanish and Italian as shown in Table 4
Active
ParamsFrench German Spanish Italian
Model Arcc HellaS MMLU Arcc HellaS MMLU Arcc HellaS MMLU Arcc HellaS MMLU
LLaMA 1 33B 33B 393 681 499 411 633 487 457 698 523 429 654 490
LLaMA 2 70B 70B 499 725 643 473 687 642 505 745 660 494 709 651
Mixtral 8x7B 13B 582 774 709 543 730 715 554 776 725 528 751 709
Table 4 Comparison of Mixtral with Llama on Multilingual Benchmarks On ARC Challenge Hellaswag
and MMLU Mixtral outperforms Llama 2 70B on 4 languages French German Spanish and Italian
32 Long range performance
To assess the capabilities of Mixtral to tackle long context we evaluate it on the passkey retrieval
task introduced in  23 a synthetic task designed to measure the ability of the model to retrieve a
passkey inserted randomly in a long prompt Results in Figure 4 Left show that Mixtral achieves a
100 retrieval accuracy regardless of the context length or the position of passkey in the sequence
Figure 4 Right shows that the perplexity of Mixtral on a subset of the proofpile dataset  2 decreases
monotonically as the size of the context increases
Figure 4 Long range performance of Mixtral Left Mixtral has 100 retrieval accuracy of the Passkey task
regardless of the location of the passkey and length of the input sequence Right The perplexity of Mixtral on
the proofpile dataset decreases monotonically as the context length increases
533 Bias Benchmarks
Llama 2 70B Mixtral 8x7B
BBQ accuracy 515 560
BOLD sentiment score avg std
gender 0293 0073 0323 0045
profession 0218 0073 0243 0087
religious_ideology 0188 0133 0144 0089
political_ideology 0149 0140 0186 0146
race 0232 0049 0232 0052
Figure 5 Bias Benchmarks Compared Llama 2 70B
Mixtral presents less bias higher accuracy on BBQ lower
std on BOLD and displays more positive sentiment higher
avg on BOLDTo identify possible flaws to be corrected
by finetuning  preference modeling we
measure the base model performance on
Bias Benchmark for QA BBQ  24 and
Bias in OpenEnded Language Generation
Dataset BOLD  10 BBQ is a dataset
of handwritten question sets that target
attested social biases against nine differ
ent sociallyrelevant categories age dis
ability status gender identity nationality
physical appearance raceethnicity religion
socioeconomic status sexual orientation
BOLD is a largescale dataset that consists
of 23679 English text generation prompts
for bias benchmarking across five domains
We benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report
the results in Table 5 Compared to Llama 2 Mixtral presents less bias on the BBQ benchmark
560 vs 515 For each group in BOLD a higher average sentiment score means more positive
sentiments and a lower standard deviation indicates less bias within the group Overall Mixtral
displays more positive sentiments than Llama 2 with similar variances within each group
4 Instruction Finetuning
We train Mixtral  Instruct using supervised finetuning SFT on an instruction dataset followed by
Direct Preference Optimization DPO  25 on a paired feedback dataset Mixtral  Instruct reaches a
score of 830 on MTBench  33 see Table 2 making it the best openweights model as of December
2023 Independent human evaluation conducted by LMSys is reported in Figure 63and shows that
Mixtral  Instruct outperforms GPT35Turbo Gemini Pro Claude21 and Llama 2 70B chat
Figure 6 LMSys Leaderboard Screenshot from Dec 22 2023 Mixtral 8x7B Instruct v01 achieves an Arena
Elo rating of 1121 outperforming Claude21 1117 all versions of GPT35Turbo 1117 best Gemini Pro
1111 and Llama270bchat 1077 Mixtral is currently the best openweights model by a large margin
3httpshuggingfacecospaceslmsyschatbotarenaleaderboard
65 Routing analysis
In this section we perform a small analysis on the expert selection by the router In particular
we are interested to see if during training some experts specialized to some specific domains eg
mathematics biology philosophy etc
To investigate this we measure the distribution of selected experts on different subsets of The Pile
validation dataset  14 Results are presented in Figure 7 for layers 0 15 and 31 layers 0 and 31
respectively being the first and the last layers of the model Surprisingly we do not observe obvious
patterns in the assignment of experts based on the topic For instance at all layers the distribution of
expert assignment is very similar for ArXiv papers written in Latex for biology PubMed Abstracts
and for Philosophy PhilPapers documents
Only for DM Mathematics we note a marginally different distribution of experts This divergence is
likely a consequence of the datasets synthetic nature and its limited coverage of the natural language
spectrum and is particularly noticeable at the first and last layers where the hidden states are very
correlated to the input and output embeddings respectively
This suggests that the router does exhibit some structured syntactic behavior Figure 8 shows
examples of text from different domains Python code mathematics and English where each token
is highlighted with a background color corresponding to its selected expert The figure shows that
words such as self in Python and Question in English often get routed through the same expert
even though they involve multiple tokens Similarly in code the indentation tokens are always
assigned to the same experts particularly at the first and last layers where the hidden states are more
correlated to the input and output of the model
We also note from Figure 8 that consecutive tokens are often assigned the same experts In fact we
observe some degree of positional locality in The Pile datasets Table 5 shows the proportion of con
secutive tokens that get the same expert assignments per domain and layer The proportion of repeated
0005010015020layer 0
0005010015020layer 15
0 1 2 3 4 5 6 70005010015020layer 31
Expert IDSelection proportion
ArXiv
DM MathematicsGithub
GutenbergPhilPapers
PubMed AbstractsStackExchange
Wikipedia en
Figure 7 Proportion of tokens assigned to each expert on different domains from The Pile dataset for
layers 0 15 and 31 The gray dashed vertical line marks 18 ie the proportion expected with uniform
sampling Here we consider experts that are either selected as a first or second choice by the router A
breakdown of the proportion of assignments done in each case cane be seen in Figure 9 in the Appendix
7First choice First or second choice
Layer 0 Layer 15 Layer 31 Layer 0 Layer 15 Layer 31
ArXiv 140 279 227 465 623 529
DM Mathematics 141 284 197 449 670 445
Github 149 281 197 499 669 492
Gutenberg 139 261 263 495 631 522
PhilPapers 136 253 221 469 619 513
PubMed Abstracts 142 246 220 486 616 518
StackExchange 136 272 236 482 646 536
Wikipedia en 144 236 253 498 621 518
Table 5 Percentage of expert assignment repetitions We evaluate the proportion of times the same expert is
assigned to a token iand its following token i1 We report whether the first chosen expert is the same or whether
the same expert is observed as first or second choice in consecutive tokens For reference the expected proportion
of repetitions in the case of random assignments is1
8 125for First choice and 16
85
746 for First
and second choice Repetitions at the first layer are close to random but are significantly higher at layers 15
and 31 The high number of repetitions shows that expert choice exhibits high temporal locality at these layers
consecutive assignments is significantly higher than random for higher layers This has implications
in how one might optimize the model for fast training and inference For example cases with high
locality are more likely to cause oversubscription of certain experts when doing Expert Parallelism
Conversely this locality can be leveraged for caching as is done in  11 A more complete view of
these same expert frequency is provided for all layers and across datasets in Figure 10 in the Appendix
6 Conclusion
In this paper we introduced Mixtral 8x7B the first mixtureofexperts network to reach a stateofthe
art performance among opensource models Mixtral 8x7B Instruct outperforms Claude21 Gem
ini Pro and GPT35 Turbo on human evaluation benchmarks Because it only uses two experts at each
time step Mixtral only uses 13B active parameters per token while outperforming the previous best
model using 70B parameters per token Llama 2 70B We are making our trained and finetuned mod
els publicly available under the Apache 20 license By sharing our models we aim to facilitate the de
velopment of new techniques and applications that can benefit a wide range of industries and domains
Figure 8 Text samples where each token is colored with the first expert choice The selection of experts
appears to be more aligned with the syntax rather than the domain especially at the initial and final layers
8Acknowledgements
We thank the CoreWeave and Scaleway teams for technical support as we trained our models We
are grateful to NVIDIA for supporting us in integrating TensorRTLLM and Triton and working
alongside us to make a sparse mixture of experts compatible with TensorRTLLM
References
1Jacob Austin Augustus Odena Maxwell Nye Maarten Bosma Henryk Michalewski David
Dohan Ellen Jiang Carrie Cai Michael Terry Quoc Le et al Program synthesis with large
language models arXiv preprint arXiv210807732  2021
2Zhangir Azerbayev Hailey Schoelkopf Keiran Paster Marco Dos Santos Stephen McAleer
Albert Q Jiang Jia Deng Stella Biderman and Sean Welleck Llemma An open language
model for mathematics arXiv preprint arXiv231010631  2023
3Yonatan Bisk Rowan Zellers Jianfeng Gao Yejin Choi et al Piqa Reasoning about phys
ical commonsense in natural language In Proceedings of the AAAI conference on artificial
intelligence  pages 74327439 2020
4Mark Chen Jerry Tworek Heewoo Jun Qiming Yuan Henrique Ponde de Oliveira Pinto Jared
Kaplan Harri Edwards Yuri Burda Nicholas Joseph Greg Brockman et al Evaluating large
language models trained on code arXiv preprint arXiv210703374  2021
5Eunsol Choi He He Mohit Iyyer Mark Yatskar Wentau Yih Yejin Choi Percy Liang and
Luke Zettlemoyer Quac Question answering in context arXiv preprint arXiv180807036 
2018
6Aidan Clark Diego De Las Casas Aurelia Guy Arthur Mensch Michela Paganini Jordan
Hoffmann Bogdan Damoc Blake Hechtman Trevor Cai Sebastian Borgeaud et al Unified
scaling laws for routed language models In International Conference on Machine Learning 
pages 40574086 PMLR 2022
7Christopher Clark Kenton Lee MingWei Chang Tom Kwiatkowski Michael Collins and
Kristina Toutanova Boolq Exploring the surprising difficulty of natural yesno questions
arXiv preprint arXiv190510044  2019
8Peter Clark Isaac Cowhey Oren Etzioni Tushar Khot Ashish Sabharwal Carissa Schoenick
and Oyvind Tafjord Think you have solved question answering try arc the ai2 reasoning
challenge arXiv preprint arXiv180305457  2018
9Karl Cobbe Vineet Kosaraju Mohammad Bavarian Mark Chen Heewoo Jun Lukasz Kaiser
Matthias Plappert Jerry Tworek Jacob Hilton Reiichiro Nakano et al Training verifiers to
solve math word problems arXiv preprint arXiv211014168  2021
10 Jwala Dhamala Tony Sun Varun Kumar Satyapriya Krishna Yada Pruksachatkun KaiWei
Chang and Rahul Gupta Bold Dataset and metrics for measuring biases in openended
language generation In Proceedings of the 2021 ACM conference on fairness accountability
and transparency  pages 862872 2021
11 Artyom Eliseev and Denis Mazur Fast inference of mixtureofexperts language models with
offloading arXiv preprint arXiv231217238  2023
12 William Fedus Jeff Dean and Barret Zoph A review of sparse expert models in deep learning
arXiv preprint arXiv220901667  2022
13 Trevor Gale Deepak Narayanan Cliff Young and Matei Zaharia Megablocks Efficient sparse
training with mixtureofexperts arXiv preprint arXiv221115841  2022
14 Leo Gao Stella Biderman Sid Black Laurence Golding Travis Hoppe Charles Foster Jason
Phang Horace He Anish Thite Noa Nabeshima et al The pile An 800gb dataset of diverse
text for language modeling arXiv preprint arXiv210100027  2020
15 Hussein Hazimeh Zhe Zhao Aakanksha Chowdhery Maheswaran Sathiamoorthy Yihua Chen
Rahul Mazumder Lichan Hong and Ed Chi Dselectk Differentiable selection in the mixture
of experts with applications to multitask learning Advances in Neural Information Processing
Systems  342933529347 2021
916 Dan Hendrycks Collin Burns Steven Basart Andy Zou Mantas Mazeika Dawn Song and
Jacob Steinhardt Measuring massive multitask language understanding arXiv preprint
arXiv200903300  2020
17 Dan Hendrycks Collin Burns Saurav Kadavath Akul Arora Steven Basart Eric Tang Dawn
Song and Jacob Steinhardt Measuring mathematical problem solving with the math dataset
arXiv preprint arXiv210303874  2021
18 Albert Q Jiang Alexandre Sablayrolles Arthur Mensch Chris Bamford Devendra Singh
Chaplot Diego de las Casas Florian Bressand Gianna Lengyel Guillaume Lample Lucile
Saulnier et al Mistral 7b arXiv preprint arXiv231006825  2023
19 Mandar Joshi Eunsol Choi Daniel S Weld and Luke Zettlemoyer Triviaqa A large
scale distantly supervised challenge dataset for reading comprehension arXiv preprint
arXiv170503551  2017
20 Tom Kwiatkowski Jennimaria Palomaki Olivia Redfield Michael Collins Ankur Parikh Chris
Alberti Danielle Epstein Illia Polosukhin Jacob Devlin Kenton Lee et al Natural questions a
benchmark for question answering research Transactions of the Association for Computational
Linguistics  pages 453466 2019
21 Dmitry Lepikhin HyoukJoong Lee Yuanzhong Xu Dehao Chen Orhan Firat Yanping Huang
Maxim Krikun Noam Shazeer and Zhifeng Chen Gshard Scaling giant models with condi
tional computation and automatic sharding arXiv preprint arXiv200616668  2020
22 Todor Mihaylov Peter Clark Tushar Khot and Ashish Sabharwal Can a suit of armor conduct
electricity a new dataset for open book question answering arXiv preprint arXiv180902789 
2018
23 Amirkeivan Mohtashami and Martin Jaggi Landmark attention Randomaccess infinite context
length for transformers arXiv preprint arXiv230516300  2023
24 Alicia Parrish Angelica Chen Nikita Nangia Vishakh Padmakumar Jason Phang Jana Thomp
son Phu Mon Htut and Samuel R Bowman Bbq A handbuilt bias benchmark for question
answering arXiv preprint arXiv211008193  2021
25 Rafael Rafailov Archit Sharma Eric Mitchell Stefano Ermon Christopher D Manning and
Chelsea Finn Direct preference optimization Your language model is secretly a reward model
arXiv preprint arXiv230518290  2023
26 Keisuke Sakaguchi Ronan Le Bras Chandra Bhagavatula and Yejin Choi Winogrande An
adversarial winograd schema challenge at scale Communications of the ACM  pages 99106
2021
27 Maarten Sap Hannah Rashkin Derek Chen Ronan LeBras and Yejin Choi Socialiqa Com
monsense reasoning about social interactions arXiv preprint arXiv190409728  2019
28 Noam Shazeer Azalia Mirhoseini Krzysztof Maziarz Andy Davis Quoc Le Geoffrey Hinton
and Jeff Dean Outrageously large neural networks The sparselygated mixtureofexperts
layer arXiv preprint arXiv170106538  2017
29 Mirac Suzgun Nathan Scales Nathanael Schärli Sebastian Gehrmann Yi Tay Hyung Won
Chung Aakanksha Chowdhery Quoc V Le Ed H Chi Denny Zhou  and Jason Wei
Challenging bigbench tasks and whether chainofthought can solve them arXiv preprint
arXiv221009261  2022
30 Alon Talmor Jonathan Herzig Nicholas Lourie and Jonathan Berant Commonsenseqa A ques
tion answering challenge targeting commonsense knowledge arXiv preprint arXiv181100937 
2018
31 Ashish Vaswani Noam Shazeer Niki Parmar Jakob Uszkoreit Llion Jones Aidan N Gomez
Łukasz Kaiser and Illia Polosukhin Attention is all you need Advances in neural information
processing systems  30 2017
32 Rowan Zellers Ari Holtzman Yonatan Bisk Ali Farhadi and Yejin Choi Hellaswag Can a
machine really finish your sentence arXiv preprint arXiv190507830  2019
33 Lianmin Zheng WeiLin Chiang Ying Sheng Siyuan Zhuang Zhanghao Wu Yonghao Zhuang
Zi Lin Zhuohan Li Dacheng Li Eric Xing et al Judging llmasajudge with mtbench and
chatbot arena arXiv preprint arXiv230605685  2023
1034 Wanjun Zhong Ruixiang Cui Yiduo Guo Yaobo Liang Shuai Lu Yanlin Wang Amin Saied
Weizhu Chen and Nan Duan Agieval A humancentric benchmark for evaluating foundation
models arXiv preprint arXiv230406364  2023
35 Yanqi Zhou Tao Lei Hanxiao Liu Nan Du Yanping Huang Vincent Zhao Andrew M Dai
Quoc V Le James Laudon et al Mixtureofexperts with expert choice routing Advances in
Neural Information Processing Systems  3571037114 2022
110010203Layer 0  Either choice
0010203Layer 0  First choice
0010203Layer 0  Second choice
0010203Layer 15  Either choice
0010203Layer 15  First choice
0010203Layer 15  Second choice
0010203Layer 31  Either choice
0010203Layer 31  First choice
0 1 2 3 4 5 6 70010203Layer 31  Second choice
Expert IDSelection proportion
ArXiv
DM MathematicsGithub
GutenbergPhilPapers
PubMed AbstractsStackExchange
Wikipedia enFigure 9 Proportion of tokens assigned to each expert on different subsets from The Pile dataset separated
by whether the expert was selected as first or second choice or either The Either choice case is equivalent
to Figure 7 The gray dashed vertical line marks1
8 ie the proportion expected with uniform sampling
12015020025030035First choice
0 10 20 30050607First or second choice
LayerProportion of repeated assignmentssource
ArXiv
DM Mathematics
Github
Gutenberg
PhilPapers
PubMed Abstracts
StackExchange
Wikipedia enFigure 10 Repeated consecutive assignments per MoE layer Repeated assignments occur a lot more
often than they would with uniform assignments materialized by the dashed lines Patterns are similar across
datasets with less repetitions for DM Mathematics
13
  1
Longterm Recurrent Convolutional Networks for
Visual Recognition and Description
Jeff Donahue Lisa Anne Hendricks Marcus Rohrbach Subhashini Venugopalan Sergio Guadarrama
Kate Saenko Trevor Darrell
Abstract 
Models based on deep convolutional networks have dominated recent image interpretation tasks we investigate whether models which
are also recurrent are effective for tasks involving sequences visual and otherwise We describe a class of recurrent convolutional
architectures which is endtoend trainable and suitable for largescale visual understanding tasks and demonstrate the value of these
models for activity recognition image captioning and video description In contrast to previous models which assume a ﬁxed visual
representation or perform simple temporal averaging for sequential processing recurrent convolutional models are doubly deep in
that they learn compositional representations in space and time Learning longterm dependencies is possible when nonlinearities are
incorporated into the network state updates Differentiable recurrent models are appealing in that they can directly map variablelength
inputs eg videos to variablelength outputs eg natural language text and can model complex temporal dynamics yet they can be
optimized with backpropagation Our recurrent sequence models are directly connected to modern visual convolutional network
models and can be jointly trained to learn temporal dynamics and convolutional perceptual representations Our results show that such
models have distinct advantages over stateoftheart models for recognition or generation which are separately deﬁned or optimized
F
1 I NTRODUCTION
Recognition and description of images and videos is
a fundamental challenge of computer vision Dramatic
progress has been achieved by supervised convolutional
neural network CNN models on image recognition tasks
and a number of extensions to process video have been
recently proposed Ideally a video model should allow pro
cessing of variable length input sequences and also provide
for variable length outputs including generation of full
length sentence descriptions that go beyond conventional
oneversusall prediction tasks In this paper we propose
Longterm Recurrent Convolutional Networks LRCNs a class
of architectures for visual recognition and description which
combines convolutional layers and longrange temporal re
cursion and is endtoend trainable Figure 1 We instanti
ate our architecture for speciﬁc video activity recognition
image caption generation and video description tasks as
described below
Research on CNN models for video processing has
considered learning 3D spatiotemporal ﬁlters over raw
sequence data 1 2 and learning of frametoframe rep
resentations which incorporate instantaneous optic ﬂow or
trajectorybased models aggregated over ﬁxed windows
or video shot segments 3 4 Such models explore two
extrema of perceptual timeseries representation learning
either learn a fully general timevarying weighting or apply
J Donahue L A Hendricks M Rohrbach S Guadarrama and T Darrell
are with the Department of Electrical Engineering and Computer Science
UC Berkeley Berkeley CA
M Rohrbach and T Darrell are additionally afﬁliated with the Interna
tional Computer Science Institute Berkeley CA
S Venugopalan is with the Department of Computer Science UT Austin
Austin TX
K Saenko is with the Department of Computer Science UMass Lowell
Lowell MA
Manuscript received November 30 2015
y1
y2
yTOutput 
CNN 
CNN 
CNN Visual 
Features Input 
LSTM 
LSTM 
LSTM Sequence 
Learning Fig 1 We propose Longterm Recurrent Convolutional Networks LR
CNs a class of architectures leveraging the strengths of rapid progress
in CNNs for visual recognition problems and the growing desire to
apply such models to timevarying inputs and outputs LRCN processes
the possibly variablelength visual input left with a CNN middle
left whose outputs are fed into a stack of recurrent sequence models
LSTMs  middleright which ﬁnally produce a variablelength prediction
right Both the CNN and LSTM weights are shared across time result
ing in a representation that scales to arbitrarily long sequences
simple temporal pooling Following the same inspiration
that motivates current deep convolutional models we ad
vocate for video recognition and description models which
are also deep over temporal dimensions ie have temporal
recurrence of latent variables Recurrent Neural Network
RNN models are deep in time  explicitly so when
unrolled  and form implicit compositional representationsarXiv14114389v4  csCV  31 May 20162
in the time domain Such deep models predated deep
spatial convolution models in the literature 5 6
The use of RNNs in perceptual applications has been ex
plored for many decades with varying results A signiﬁcant
limitation of simple RNN models which strictly integrate
state information over time is known as the vanishing
gradient effect the ability to backpropagate an error signal
through a longrange temporal interval becomes increas
ingly difﬁcult in practice Long ShortTerm Memory LSTM
units ﬁrst proposed in 7 are recurrent modules which
enable longrange learning LSTM units have hidden state
augmented with nonlinear mechanisms to allow state to
propagate without modiﬁcation be updated or be reset
using simple learned gating functions LSTMs have recently
been demonstrated to be capable of largescale learning of
speech recognition 8 and language translation models 9
10
We show here that convolutional networks with re
current units are generally applicable to visual timeseries
modeling and argue that in visual tasks where static or ﬂat
temporal models have previously been employed LSTM
style RNNs can provide signiﬁcant improvement when
ample training data are available to learn or reﬁne the rep
resentation Speciﬁcally we show that LSTM type models
provide for improved recognition on conventional video
activity challenges and enable a novel endtoend optimiz
able mapping from image pixels to sentencelevel natural
language descriptions We also show that these models
improve generation of descriptions from intermediate visual
representations derived from conventional visual models
We instantiate our proposed architecture in three ex
perimental settings Figure 3 First we show that directly
connecting a visual convolutional model to deep LSTM
networks we are able to train video recognition models
that capture temporal state dependencies Figure 3 left
Section 4 While existing labeled video activity datasets
may not have actions or activities with particularly com
plex temporal dynamics we nonetheless observe signiﬁcant
improvements on conventional benchmarks
Second we explore endtoend trainable image to sen
tence mappings Strong results for machine translation
tasks have recently been reported 9 10 such models
are encoderdecoder pairs based on LSTM networks We
propose a multimodal analog of this model and describe an
architecture which uses a visual convnet to encode a deep
state vector and an LSTM to decode the vector into a natural
language string Figure 3 middle Section 5 The resulting
model can be trained endtoend on largescale image and
text datasets and even with modest training provides com
petitive generation results compared to existing methods
Finally we show that LSTM decoders can be driven
directly from conventional computer vision methods which
predict higherlevel discriminative labels such as the se
mantic video role tuple predictors in 11 Figure 3 right
Section 6 While not endtoend trainable such models offer
architectural and performance advantages over previous
statistical machine translationbased approaches
We have realized a generic framework for recurrent
models in the widely adopted deep learning framework
Caffe 12 including readytouse implementations of RNN
and LSTM units See httpjeffdonahuecomlrcn

σσ σ
xt
ht1
ht  ztOutput 
Gate Input 
Gate 
Forget Gate Input Modulation Gate LSTM Unit 
ϕxt
ht1ht
Output ztRNN Unit 
σσ
ϕ
ftit
gtot
ctct1Fig 2 A diagram of a basic RNN cell left and an LSTM memory
cell right used in this paper from 13 a slight simpliﬁcation of the
architecture described in 14 which was derived from the LSTM initially
proposed in 7
2 B ACKGROUND  RECURRENT NETWORKS
Traditional recurrent neural networks RNNs Figure 2 left
model temporal dynamics by mapping input sequences to
hidden states and hidden states to outputs via the following
recurrence equations Figure 2 left
htgWxhxtWhhht1bh
ztgWhzhtbz
wheregis an elementwise nonlinearity such as a sigmoid
or hyperbolic tangent xtis the input ht2RNis the hidden
state withNhidden units and ztis the output at time t
For a length Tinput sequencehx1x2x Ti the updates
above are computed sequentially as h1lettingh0 0z1
h2z2 hTzT
Though RNNs have proven successful on tasks such
as speech recognition 15 and text generation 16 it can
be difﬁcult to train them to learn longterm dynamics
likely due in part to the vanishing and exploding gradients
problem 7 that can result from propagating the gradients
down through the many layers of the recurrent network
each corresponding to a particular time step LSTMs provide
a solution by incorporating memory units that explicitly
allow the network to learn when to forget previous hid
den states and when to update hidden states given new
information As research on LSTMs has progressed hidden
units with varying connections within the memory unit
have been proposed We use the LSTM unit as described
in 13 Figure 2 right a slight simpliﬁcation of the one
described in 8 which was derived from the original LSTM
unit proposed in 7 Letting x  1 ex1be the
sigmoid nonlinearity which squashes realvalued inputs to
a01range and letting tanhx exex
exex 22x1
be the hyperbolic tangent nonlinearity similarly squashing
its inputs to a 11range the LSTM updates for time step
tgiven inputs xtht1 andct1are
itWxixtWhiht1bi
ftWxfxtWhfht1bf
otWxoxtWhoht1bo
gt tanhWxcxtWhcht1bc
ctftct1itgt
htottanhct3
xydenotes the elementwise product of vectors xandy
In addition to a hidden unit ht2RN the LSTM includes
an input gate it2RN forget gate ft2RN output gate
ot2RN input modulation gate gt2RN and memory cell
ct2RN The memory cell unit ctis a sum of two terms the
previous memory cell unit ct1which is modulated by ft
andgt a function of the current input and previous hidden
state modulated by the input gate it Becauseitandftare
sigmoidal their values lie within the range 01 andit
andftcan be thought of as knobs that the LSTM learns
to selectively forget its previous memory or consider its
current input Likewise the output gate otlearns how much
of the memory cell to transfer to the hidden state These
additional cells seem to enable the LSTM to learn complex
and longterm temporal dynamics for a wide variety of
sequence learning and prediction tasks Additional depth
can be added to LSTMs by stacking them on top of each
other using the hidden state h1
t of the LSTM in layer
1as the input to the LSTM in layer 
Recently LSTMs have achieved impressive results on
language tasks such as speech recognition 8 and ma
chine translation 9 10 Analogous to CNNs LSTMs are
attractive because they allow endtoend ﬁnetuning For
example 8 eliminates the need for complex multistep
pipelines in speech recognition by training a deep bidirec
tional LSTM which maps spectrogram inputs to text Even
with no language model or pronunciation dictionary the
model produces convincing text translations 9 and 10
translate sentences from English to French with a multi
layer LSTM encoder and decoder Sentences in the source
language are mapped to a hidden state using an encoding
LSTM and then a decoding LSTM maps the hidden state to
a sequence in the target language Such an encoderdecoder
scheme allows an input sequence of arbitrary length to
be mapped to an output sequence of different length The
sequencetosequence architecture for machine translation
circumvents the need for language models
The advantages of LSTMs for modeling sequential data
in vision problems are twofold First when integrated with
current vision systems LSTM models are straightforward
to ﬁnetune endtoend Second LSTMs are not conﬁned to
ﬁxed length inputs or outputs allowing simple modeling
for sequential data of varying lengths such as text or video
We next describe a uniﬁed framework to combine recurrent
models such as LSTMs with deep convolutional networks
to form endtoend trainable networks capable of complex
visual and sequence prediction tasks
3 L ONGTERM RECURRENT CONVOLUTIONAL
NETWORK LRCN MODEL
This work proposes a Longterm Recurrent Convolutional
Network LRCN model combining a deep hierarchical vi
sual feature extractor such as a CNN with a model that can
learn to recognize and synthesize temporal dynamics for
tasks involving sequential data inputs or outputs visual
linguistic or otherwise Figure 1 depicts the core of our
approach LRCN works by passing each visual input xt
an image in isolation or a frame from a video through
a feature transformation Vwith parameters V usually
a CNN to produce a ﬁxedlength vector representationVxt The outputs of Vare then passed into a recurrent
sequence learning module
In its most general form a recurrent model has param
etersW and maps an input xtand a previous time step
hidden state ht1to an output ztand updated hidden state
ht Therefore inference must be run sequentially ie from
top to bottom in the Sequence Learning box of Figure 1 by
computing in order h1fWx1h0 fWx10 then
h2fWx2h1 etc up to hT Some of our models stack
multiple LSTMs atop one another as described in Section 2
To predict a distribution Pytover outcomes yt2C
whereCis a discrete ﬁnite set of outcomes at time step
t the outputs zt2Rdzof the sequential model are passed
through a linear prediction layer ytWzztbz where
Wz2RjCjdzandbz2RjCjare learned parameters Finally
the predicted distribution Pytis computed by taking the
softmax of ytPytc  softmax yt expytcP
c02Cexpytc0
The success of recent deep models for object recogni
tion 17 18 19 suggests that strategically composing
many layers of nonlinear functions can result in powerful
models for perceptual problems For large T the above
recurrence indicates that the last few predictions from a
recurrent network with Ttime steps are computed by a very
deep Tlayer nonlinear function suggesting that the
resulting recurrent model may have similar representational
power to a Tlayer deep network Critically however the
sequence models weights Ware reused at every time step
forcing the model to learn generic time steptotime step
dynamics as opposed to dynamics conditioned on t the
sequence index and preventing the parameter size from
growing in proportion to the maximum sequence length
In most of our experiments the visual feature transfor
mationcorresponds to the activations in some layer of
a deep CNN Using a visual transformation Vwhich
is timeinvariant and independent at each time step has the
important advantage of making the expensive convolutional
inference and training parallelizable over all time steps of
the input facilitating the use of fast contemporary CNN
implementations whose efﬁciency relies on independent
batch processing and endtoend optimization of the visual
and sequential model parameters VandW
We consider three vision problems activity recognition
image description and video description each of which
instantiates one of the following broad classes of sequential
learning tasks
1 Sequential input static output Figure 3 left
hx1x2x Ti7y The visual activity recognition
problem can fall under this umbrella with videos
of arbitrary length Tas input but with the goal
of predicting a single label like running orjumping
drawn from a ﬁxed vocabulary
2 Static input sequential output Figure 3 middle
x7hy1y2y Ti The image captioning problem
ﬁts in this category with a static nontimevarying
image as input but a much larger and richer label
space consisting of sentences of any length
3 Sequential input and output Figure 3 right
hx1x2x Ti7hy1y2y T0i In tasks such as
video description both the visual input and output
are timevarying and in general the number of4
Activity Recognition 
Sequences in the Input 
Image Captioning 
Sequences in the Output Video Description 
Sequences in the Input and Output 
BOS 
 HighJump 
CNN 
CNN CNN 
LSTM LSTM LSTM LSTM LSTM LSTM LSTM 
LSTM LSTM LSTM LSTM 
A man runs EOS 
LSTM LSTM LSTM LSTM LSTM 
A man jumps high BOS EOS Average CNN CRF 
LSTM LSTM LSTM LSTM LSTM 
Fig 3 Taskspeciﬁc instantiations of our LRCN model for activity recognition image description and video description
input and output time steps may differ ie we may
haveT6T0 In video description for example the
number of frames in the video should not constrain
the length of number of words in the natural
language description
In the previously described generic formulation of re
current models each instance has Tinputshx1x2x Ti
andToutputshy1y2y Ti Note that this formulation
does not align cleanly with any of the three problem classes
described above  in the ﬁrst two classes either the input
or output is static and in the third class the input length
Tneed not match the output length T0 Hence we describe
how we adapt this formulation in our hybrid model to each
of the above three problem settings
With sequential inputs and static outputs class 1 we
take a latefusion approach to merging the pertime step
predictionshy1y2y Tiinto a single prediction yfor the
full sequence With static inputs xand sequential outputs
class 2 we simply duplicate the input xat allTtime
steps8t2f12Tgxtx Finally for a sequence
tosequence problem with in general different input and
output lengths class 3 we take an encoderdecoder
approach as proposed for machine translation by 9 20
In this approach one sequence model the encoder  maps
the input sequence to a ﬁxedlength vector and another se
quence model the decoder  unrolls this vector to a sequential
output of arbitrary length Under this type of model a run
of the full system on one instance occurs over TT01time
steps For the ﬁrst Ttime steps the encoder processes the
inputx1x2x T and the decoder is inactive until time
stepT when the encoders output is passed to the decoder
which in turn predicts the ﬁrst output y1 For the latter T01
time steps the decoder predicts the remainder of the out
puty2y3y T0with the encoder inactive This encoder
decoder approach as applied to the video description task
is depicted in Section 6 Figure 5 left
Under the proposed system the parameters VW 
of the models visual and sequential components can
be jointly optimized by maximizing the likelihood of
the ground truth outputs ytat each time step t con
ditioned on the input data and labels up to that point
x1ty1t1 In particular for a training set Dof labeled
sequences xtytT
t12D  we optimize parameters VW 
to minimize the expected negative log likelihood of asequence sampled from the training set LVWD 
1
jDjP
xtytT
t12DPT
t1logPytjx1ty1t1VW 
One of the most appealing aspects of the described sys
tem is the ability to learn the parameters endtoend such
that the parameters Vof the visual feature extractor learn
to pick out the aspects of the visual input that are relevant
to the sequential classiﬁcation problem We train our LRCN
models using stochastic gradient descent with backprop
agation used to compute the gradient rVWLVW Dof
the objectiveLwith respect to all parameters VW over
minibatches DD sampled from the training dataset D
We next demonstrate the power of endtoend trainable
hybrid convolutional and recurrent networks by exploring
three applications activity recognition image captioning
and video description
4 A CTIVITY RECOGNITION
Activity recognition is an instance of the ﬁrst class of se
quential learning tasks described above each frame in a
lengthTsequence is the input to a single convolutional
network ie the convnet weights are tied across time We
consider both RGB and ﬂow as inputs to our recognition
system Flow is computed with 21 and transformed into a
ﬂow image by scaling and shifting xandyﬂow values to
a range of 128128  A third channel for the ﬂow image
is created by calculating the ﬂow magnitude
During training videos are resized to 240320and we
augment our data by using 227227 crops and mirroring
Additionally we train the LRCN networks with video clips
of 16 frames even though the UCF101 videos are generally
much longer on the order of 100 frames when extracting
frames at 30 FPS Training on shorter video clips can be
seen as analogous to training on image crops and is a useful
method of data augmentation LRCN is trained to predict
the videos activity class at each time step To produce a
single label prediction for an entire video clip we average
the label probabilities  the outputs of the networks softmax
layer  across all frames and choose the most probable label
At test time we extract 16 frame clips with a stride of 8
frames from each video and average across all clips from a
single video
The CNN base of LRCN in our activity recognition
experiments is a hybrid of the CaffeNet 12 reference model
a minor variant of AlexNet 17 and the network used5
by Zeiler  Fergus 22 The network is pretrained on
the 12M image ILSVRC2012 23 classiﬁcation training
subset of the ImageNet 24 dataset giving the network a
strong initialization to facilitate faster training and avoid
overﬁtting to the relatively small video activity recognition
datasets When classifying center crops the top1 classiﬁca
tion accuracy is 602 and 574 for the hybrid and CaffeNet
reference models respectively
We compare LRCN to a single frame baseline model
In our baseline model Tvideo frames are individually
classiﬁed by a CNN As in the LSTM model whole video
classiﬁcation is done by averaging scores across all video
frames
41 Evaluation
We evaluate our architecture on the UCF101 dataset 25
which consists of over 12000 videos categorized into 101
human action classes The dataset is split into three splits
with just under 8000 videos in the training set for each split
We explore various hyperparameters for the LRCN activ
ity recognition architecture To explore different variants we
divide the ﬁrst training split of UCF101 into a smaller train
ing set 6000 videos and a validation set  3000 videos
We ﬁnd that the most inﬂuential hyperparameters include
the number of hidden units in the LSTM and whether fc6
orfc7features are used as input to the LSTM We compare
networks with 256512 and 1024 LSTM hidden units When
using ﬂow as an input more hidden units leads to better
peformance with 1024 hidden units yielding a 17 boost in
accuracy in comparison to a network with 256 hidden units
on our validation set In contrast for networks with RGB
input the number of hidden units has little impact on the
performance of the model We thus use 1024 hidden units
for ﬂow inputs and 256 for RGB inputs We ﬁnd that using
fc6as opposed to fc7features improves accuracy when
using ﬂow as input on our validation set by 1 When using
RGB images as input the difference between using fc6or
fc7features is quite small using fc6features only increases
accuracy by 02 Because both models perform better with
fc6features we train our ﬁnal models using fc6features
denoted by LRCN fc6 We also considered subsampling
the frames input to the LSTM but found that this hurts
performance compared with using all frames Additionally
when training the LRCN network endtoend we found that
aggressive dropout  09 was needed to avoid overﬁtting
Table 1 reports the average accuracy across the three
standard test splits of UCF101 Columns 23 compare video
classiﬁcation of LRCN against the baseline single frame
architecture for both RGB and ﬂow inputs LRCN yields the
best results for both RGB and ﬂow and improves upon the
baseline network by 083 and 291 respectively RGB and
ﬂow networks can be combined by computing a weighted
average of network scores as proposed in 4 Like 4
we report two weighted averages of the predictions from
the RGB and ﬂow networks in Table 1 right Since the
ﬂow network outperforms the RGB network weighting the
ﬂow network higher unsurprisingly leads to better accuracy
In this case LRCN outperforms the baseline singleframe
model by 340
Table 2 compares LRCNs accuracy with the single frame
baseline model for individual classes on Split 1 of UCF101Single Input Type Weighted Average
Model RGB Flow 1212 1323
Single frame 6737 7437 7546 7894
LRCNfc 6 6820 7728 8090 8234
TABLE 1
Activity recognition Comparing single frame models to LRCN networks
for activity recognition on the UCF101 25 dataset with RGB and ﬂow
inputs Average values across all three splits are shown LRCN
consistently and strongly outperforms a model based on predictions
from the underlying convolutional network architecture alone
Label  Label 
BoxingPunchingBag 4082 BoxingSpeedBag 1622
HighJump 2973 Mixing 1556
JumpRope 2895 Knitting 1471
CricketShot 2857 Typing 1395
Basketball 2857 Skiing 1250
WallPushups 2571 BaseballPitch 1163
Nunchucks 2286 BrushingTeeth 1111
ApplyEyeMakeup 2273 Skijet 1071
HeadMassage 2195 Haircut 910
Drumming 1778 TennisSwing 816
TABLE 2
Activity recognition comparison of improvement in LRCNs perclass
recognition accuracy versus the singleframe baseline Here we report
results on all three splits of UCF101 only results on the ﬁrst split were
presented in the paper is the difference between LRCNs accuracy
and the singleframe models accuracy
For the majority of classes LRCN improves performance
over the single frame model Though LRCN performs worse
on some classes including Knitting and Mixing  in general
when LRCN performs worse the loss in accuracy is not
as substantial as the gain in accuracy for classes like Box
ingPunchingBag and HighJump  Consequently accuracy is
higher overall
Table 3 compares accuracies for the LRCN ﬂow and
LRCN RGB models for individual classes on Split 1 of
UCF101 Note that for some classes the LRCN ﬂow model
outperforms the LRCN RGB model and vice versa One
explanation is that activities which are better classiﬁed by
the LRCN RGB model are best determined by which objects
are present in the scene while activities which are better
classiﬁed by the LRCN ﬂow model are best classiﬁed by the
kind of motion in the scene For example activity classes
likeTyping are highly correlated with the presence of certain
objects such as a keyboard and are thus best learned by the
LRCN RGB model Other activities such as SoccerJuggling
include more generic objects which are frequently seen
in other activities soccer balls people and are thus best
identiﬁed from classspeciﬁc motion cues Because RGB and
ﬂow signals are complementary the best models take both
into account
LRCN shows clear improvement over the baseline
singleframe system and is comparable to accuracy achieved
by other deep models 4 report the results on UCF101
by computing a weighted average between ﬂow and RGB
networks and achieve 876 3 reports 654 accuracy on
UCF101 which is substantially lower than LRCN
5 I MAGE CAPTIONING
In contrast to activity recognition the static image caption
ing task requires only a single invocation of a convolutional
network since the input consists of a single image At each
time step both the image features and the previous word6
Label  Label 
BoxingPunchingBag 5714 Typing 4419
PushUps 5333 TennisSwing 4286
JumpRope 5000 FieldHockeyPenalty 3250
SoccerJuggling 4872 BrushingTeeth 3056
HandstandWalking 4412 CuttingInKitchen 3030
Basketball 4000 Skijet 2857
BodyWeightSquats 3846 Mixing 2667
Lunges 3784 Skiing 2500
Nunchucks 3429 Knitting 2059
WallPushups 3429 FloorGymnastics 1944
TABLE 3
Activity recognition comparison of perclass recognition accuracy
between the ﬂow and RGB LRCN models is the difference between
LRCN ﬂow accuracy and LRCN RGB accuracy
are provided as inputs to the sequence model in this case a
stack of LSTMs each with 1000 hidden units which is used
to learn the dynamics of the timevarying output sequence
natural language
At time step t the input to the bottommost LSTM is the
embedded word from the previous time step yt1 Input
words are encoded as onehot vectors vectors y2RK
with a single nonzero component yi 1 denoting the ith
word in the vocabulary where Kis the number of words
in the vocabulary plus one additional entry for the BOS
beginning of sequence token which is always taken as y0
the previous word at the ﬁrst time step  t 1 These
onehot vectors are then projected into an embedding space
with dimension deby multiplication Weytwith a learned
parameter matrix We2RdeK The result of a matrix
vector multiplication with a onehot vector is the column
of the matrix corresponding to the index of the single non
zero component of the onehot vector Wecan therefore be
thought of as a lookup table mapping each of the K
words in the vocabulary to a dedimensional vector
The visual feature representation Vxof the image x
may be input to the sequence model  a stack of LLSTMs
 by concatenating it at each time step either with 1 the
embedded previous word Weyt1and fed into the ﬁrst
LSTM of the stack or 2 the hidden state h1
t output
from LSTM 1and fed into LSTM  for some22L 
These choices are depicted in Figure 4 We refer to the
latter choice as factored as it forces a sort of separation
of responsibilities by blinding the ﬁrst 1LSTMs and
forcing all of the capacity of their hidden states at time step
tto represent only the partial caption y1t1independent
of the visual input while the LSTMs starting from are
responsible for fusing the lower layers hidden state given
by the partial caption with the visual feature representation
Vxto produce a joint hidden state representation h
t
of the visual and language inputs up to time step tfrom
which the next word ytcan be predicted In the factored
case the hidden state htfor the lower layers is conditionally
independent of the image xgiven the partial caption y1t1
The outputs of the ﬁnal LSTM in the stack are the
inputs to a learned linear prediction layer with a softmax
producing a distribution Pytjy1t1Vxover words yt
in the models vocabulary including the EOS token de
noting the end of the caption allowing the model to predict
captions of varying length The visual model Vused for
our image captioning experiments is either the CaffeNet 12
reference model a variant of AlexNet 17 or the moreR1 R5 R10 Med r
Caption to Image Flickr30k
DeViSE 30 67 219 327 25
SDTRNN 29 89 298 411 16
DeFrag 28 103 314 445 13
mRNN 27 126 312 415 16
ConvNet 31 118 340 463 13
LRCN 2fours 175 403 508 9
Image to Caption Flickr30k
DeViSE 30 45 181 292 26
SDTRNN 29 96 298 411 16
DeFrag 28 164 402 547 8
mRNN 27 184 402 509 10
ConvNet 31 148 392 509 10
LRCN 2fours 236 466 583 7
TABLE 4
Image description retrieval results for the Flickr30k 32 datasets
RKis the average recall at rank Khigh is good Med ris the
median rank low is good
modern and computationally expensive VGGNet 18 model
pretrained for ILSVRC2012 23 classiﬁcation
Without any explicit language modeling or impositions
on the structure of the generated captions the described
LRCN system learns mappings from images input as pixel
intensity values to natural language descriptions that are
often semantically descriptive and grammatically correct
At training time the previous word inputs y1t1at time
steptare from the ground truth caption For inference of
captions on a novel image x the input is a sample yt
Pytjy1t1Vxfrom the models predicted distribution
at the previous time step and generation continues until an
EOS end of sequence token is generated
51 Evaluation
We evaluate our image description model for retrieval and
generation tasks We ﬁrst demonstrate the effectiveness of
our model by quantitatively evaluating it on the image and
caption retrieval tasks proposed by 26 and seen in 27
28 29 30 31 We report results on Flickr30k 32 and
COCO 2014 33 datasets both with ﬁve captions annotated
per image
511 Retrieval
Retrieval results on the Flickr30k 32 dataset are recorded in
Table 4 We report median rank Med r of the ﬁrst retrieved
ground truth image or caption and Recall K the number
of images or captions for which a correct caption or image is
retrieved within the top Kresults Our model consistently
outperforms the strong baselines from recent work 27
28 29 30 31 as can be seen in Table 4 Here we
note that the VGGNet model in 31 called OxfordNet in
their work outperforms our model on the retrieval task
However VGGNet is a stronger convolutional network 18
than that used for our results on this task The strength
of our sequence model and integration of the sequence
and visual models can be more directly measured against
the ConvNet 31 result which uses a very similar base
CNN architecture AlexNet 17 where we use CaffeNet
pretrained on the same data
We also ablate the models retrieval performance on a
randomly chosen subset of 1000 images and 5000 cap
tions from the COCO 2014 33 validation set Results are7
BOS 
LSTM LSTM 
A man 
CNN 
BOS 
LSTM LSTM 
A man 
CNN 
LSTM LSTM 
BOS 
LSTM LSTM 
A man 
CNN 
LSTM LSTM 
Single Layer  L 1 Two Layers  L 2 Unfactored Two Layers  L 2 Factored
LRCN 1u LRCN 2u LRCN 2f
Fig 4 Three variants of the LRCN image captioning architecture that we experimentally evaluate We explore the effect of depth in the LSTM
stack and the effect of the factorization of the modalities
recorded in Table 5 The ﬁrst group of results for each
task examines the effectiveness of an LSTM compared with
a vanilla RNN as described in Section 2 These results
demonstrate that the use of the LSTM unit compared to
the simpler RNN architecture is an important element of
our models performance on this task justifying the addi
tional complexity and suggesting that the LSTMs gating
mechanisms allowing for longterm memory may be quite
useful even for relatively simple sequences
Within the second and third result groups we compare
performance among the three sequence model architectural
variants depicted in Figure 4 For both tasks and under all
metrics the two layer unfactored variant LRCN 2u per
forms worse than the other two The fact that LRCN 1uout
performs LRCN 2uindicates that stacking additional LSTM
layers alone is not beneﬁcial for this task The other two
variants LRCN 2fand LRCN 1u perform similarly across
the board with LRCN 2fappearing to have a slight edge
in the image to caption task under most metrics but the
reverse for caption to image retrieval
Unsurprisingly ﬁnetuning the CNN indicated by the
FT column of Table 5 and using a more powerful CNN
VGGNet 18 rather than CaffeNet each improve results
substantially across the board Finetuning boosts the R k
metrics by 35 for CaffeNet and 58 for VGGNet Switch
ing from CaffeNet to VGGNet improves results by around
812 for the caption to image task and by roughly 1117
for the image to caption task
512 Generation
We evaluate LRCNs caption generation performance on
the COCO2014 33 dataset using the ofﬁcial metrics on
which COCO image captioning submissions are evaluated
The BLEU 34 and METEOR 36 metrics were designed
for automatic evaluation of machine translation methods
ROUGEL 37 was designed for evaluating summarization
performance CIDErD 35 was designed speciﬁcally to
evaluate the image captioning task
In Table 6 we evaluate variants of our model along the
same axes as done for the retrieval tasks in Table 5 In the
last of the three groups of results we additionally explore
and evaluate various caption generation strategies that canVision Model Sequence Model Retrieval Performance
CNN FT Unit L Factor R1 R5 R10 Med r
Caption to Image
CaffeNet  RNN 2 X 213 517 672 5
CaffeNet  LSTM 2 X 250 562 706 4
CaffeNet  LSTM 1  252 562 708 4
CaffeNet  LSTM 2  234 548 693 5
CaffeNet  LSTM 2 X 250 562 706 4
CaffeNet X LSTM 1  285 600 745 4
CaffeNet X LSTM 2  256 572 722 4
CaffeNet X LSTM 2 X 272 596 747 4
VGGNet  LSTM 2 X 335 681 808 3
VGGNet X LSTM 2 X 393 747 859 2
Image to Caption
CaffeNet  RNN 2 X 302 610 726 4
CaffeNet  LSTM 2 X 338 653 753 3
CaffeNet  LSTM 1  323 645 756 3
CaffeNet  LSTM 2  299 608 727 3
CaffeNet  LSTM 2 X 338 653 753 3
CaffeNet X LSTM 1  361 684 795 3
CaffeNet X LSTM 2  331 637 769 3
CaffeNet X LSTM 2 X 363 673 806 2
VGGNet  LSTM 2 X 460 774 883 2
VGGNet X LSTM 2 X 533 843 919 1
TABLE 5
Retrieval results image to caption and caption to image for a randomly
chosen subset 1000 images of the COCO 2014 33 validation set
RKis the average recall at rank Khigh is good Med ris the
median rank low is good
be employed for a given network The simplest strategy
and the one employed for most of our generation results
in our prior work 43 is to generate captions greedily
ie by simply choosing the most probable word at each
time step This is equivalent to and denoted in Table 6 by
beam search with beam width 1 In general beam search
with beam width Napproximates the most likely caption
by retaining and expanding only the Ncurrent most likely
partial captions according to the model We ﬁnd that of the
beam search strategies a beam width of 35 gives the best
generation numbers  performance saturates quickly and
even degrades for larger beam width eg 10
An alternative nondeterministic generation strategy is
to randomly sample Ncaptions from the models distri
bution and choose the most probable among these Under8
Generation Strategy Vision Model Sequence Model Generation Performance COCO 2014 33 Validation Set
Beam Sample
Width N T CNN FT Unit L Factor B1 B2 B3 B4 C M R
1   CaffeNet  RNN 2 X 0638 0454 0315 0220 0660 0209 0473
1   CaffeNet  LSTM 2 X 0646 0462 0321 0224 0674 0210 0477
1   CaffeNet  LSTM 1  0654 0475 0333 0231 0661 0209 0480
1   CaffeNet  LSTM 2  0653 0470 0328 0230 0682 0212 0480
1   CaffeNet  LSTM 2 X 0646 0462 0321 0224 0674 0210 0477
1   CaffeNet X LSTM 1  0661 0485 0344 0241 0702 0216 0489
1   CaffeNet X LSTM 2  0659 0478 0338 0238 0716 0217 0486
1   CaffeNet X LSTM 2 X 0659 0478 0336 0237 0717 0218 0486
1   VGGNet  LSTM 2 X 0674 0494 0351 0248 0773 0227 0497
1   VGGNet X LSTM 2 X 0695 0519 0374 0268 0839 0237 0512
 100 15 CaffeNet  RNN 2 X 0647 0466 0334 0244 0703 0212 0479
 100 15 CaffeNet  LSTM 2 X 0657 0478 0344 0251 0720 0215 0485
 100 15 CaffeNet  LSTM 1  0664 0490 0354 0254 0704 0211 0488
 100 15 CaffeNet  LSTM 2  0664 0486 0352 0257 0732 0216 0489
 100 15 CaffeNet  LSTM 2 X 0657 0478 0344 0251 0720 0215 0485
 100 15 CaffeNet X LSTM 1  0679 0507 0370 0268 0753 0219 0499
 100 15 CaffeNet X LSTM 2  0672 0495 0361 0265 0762 0222 0495
 100 15 CaffeNet X LSTM 2 X 0670 0493 0358 0264 0764 0222 0495
 100 15 VGGNet  LSTM 2 X 0690 0514 0377 0278 0828 0231 0508
 100 15 VGGNet X LSTM 2 X 0711 0541 0402 0300 0896 0242 0524
1   VGGNet X LSTM 2 X 0695 0519 0374 0268 0839 0237 0512
2   VGGNet X LSTM 2 X 0707 0533 0394 0291 0879 0242 0520
3   VGGNet X LSTM 2 X 0708 0536 0399 0298 0888 0243 0521
4   VGGNet X LSTM 2 X 0706 0534 0398 0299 0888 0243 0521
5   VGGNet X LSTM 2 X 0704 0533 0398 0300 0888 0242 0520
10   VGGNet X LSTM 2 X 0699 0528 0395 0298 0886 0241 0518
 1 20 VGGNet X LSTM 2 X 0658 0472 0327 0224 0733 0222 0483
 10 20 VGGNet X LSTM 2 X 0708 0534 0391 0286 0868 0239 0519
 25 20 VGGNet X LSTM 2 X 0712 0540 0398 0294 0885 0241 0523
 100 20 VGGNet X LSTM 2 X 0714 0543 0402 0297 0889 0242 0524
 100 10 VGGNet X LSTM 2 X 0674 0494 0357 0261 0805 0228 0494
 100 15 VGGNet X LSTM 2 X 0711 0541 0402 0300 0896 0242 0524
 100 20 VGGNet X LSTM 2 X 0714 0543 0402 0297 0889 0242 0524
TABLE 6
Image caption generation performance under the BLEU 14 34 B1B4 CIDErD 35 C METEOR 36 M and ROUGEL 37 R metrics
across various network architectures and generation strategies In the topmost set of results we show performance across various CNN and
recurrent architectures for a simple generation strategy  beam search with beam width 1 ie simply choosing the most probable word at each
time step In the middle set of results we show performance across the same set of architectures for a more sophisticated and computationally
intensive generation strategy found to be the best performing in terms of performance under the CIDErD metric among those explored in the
bottommost set of results which explores various generation strategies while ﬁxing the choice of network In the ﬁrst two sets of results we vary
the visual input CNN architecture either CaffeNet 12 an architecture similar to AlexNet 17 or the more modern VGGNet 18 and whether its
weights are ﬁnetuned FT Keeping the visual input CNN ﬁxed with CaffeNet we also vary the choice of recurrent architecture comparing a
stack of vanilla RNNs with LSTMs 7 as well as the number of layers in the stack L and for L 2 whether the layers are factored ie
whether the visual input is passed into the second layer In the last set of results we explore two generation strategies  beam search and
choosing the best highest loglikelihood among Nsamples from the models predicted distribution For beam search we vary the beam width
from 110 For the sampling strategy we explore the effect of sample size Nas well as the effect of applying various choices of scalar factor T
inverse of the temperature to the logits input to the softmax producing the distribution
this strategy we also examine the effect of applying various
choices of scalar factors inverse of the temperature Tto
the realvalued predictions input to the softmax producing
the distribution For larger values of Tthe samples are
greedier and less diverse with T1being equivalent to
beam search with beam width 1 Larger values of Nsuggest
using smaller values of T and vice versa  for example
with largeNand largeT most of theONcomputation is
wasted as many of the samples will be redundant We assess
saturation as the number of samples Ngrows and ﬁnd that
N 100 samples with T 2 improves little over N 25 
We also varied the temperature Tamong values 1 15 and
2 all withN 100  and found T 15to perform the best
We adopt the bestperforming generation strategy from
the bottommost set of results in Table 6 sampling with
T 15N 100  as the strategy for the middle setof results in the table which ablates LRCN architectures
We also record generation performance for all architectures
Table 6 top set of results with the simpler generation
strategy used in our earlier work 43 for ease of comparison
with this work and for future researchers For the remainder
of this discussion we will focus on the middle set of
results and particularly on the CIDErD 35 C metric
as it was designed speciﬁcally for automatic evaluation of
image captioning systems We see again that the LSTM unit
outperforms an RNN unit for generation though not as
signiﬁcantly as for retrieval Between the sequence model
architecture choices depicted in Figure 4 of the number
of layersLand whether to factor we see that in this
case the twolayer models LRCN 2fand LRCN 2u perform
similarly outperforming the single layer model LRCN 1u
Interestingly of the three variants LRCN 2fis the only one9
Generation Performance COCO 2014 33 Test Set
Method B1 B2 B3 B4 C M R
38 NIC 0895 0802 0694 0587 0946 0346 0682
39 MSR Captivator 0907 0819 0710 0601 0937 0339 0680
40 mRNN 2015 0890 0798 0687 0575 0935 0325 0666
Ours  LRCN this work sample 0895 0804 0695 0585 0934 0335 0678
41 MSR 0880 0789 0678 0567 0925 0331 0662
42 Nearest Neighbor 0872 0770 0655 0542 0916 0318 0648
33 Human 0880 0744 0603 0471 0910 0335 0626
27 mRNN 2014 0890 0801 0690 0578 0896 0320 0668
Ours 43 LRCN greedy 0871 0772 0653 0534 0891 0322 0656
44 Show Attend and Tell 0872 0768 0644 0523 0878 0323 0651
31 MLBL 0848 0747 0633 0517 0752 0294 0635
45 NeuralTalk 0828 0701 0566 0446 0692 0280 0603
TABLE 7
Image caption generation results from topperforming methods in the 2015 COCO caption challenge competition sorted by performance under the
CIDErD metric We omit submissions that did not provide a reference to a report describing their method see full results at
httpmscocoorgdatasetcaptionsleaderboard All results except for our updated result denoted by LRCN this work  were competition entries
submitted by May 2015 Our updated result differs from our original competition entry only by generation strategy sampling with N 100 
T 15 rather than beam search with width 1 ie greedy search the visual and recurrent architectures and trained weights are the same
to perform best for both retrieval and generation
We see again that ﬁnetuning FT the visual represen
tation and using a stronger vision model VGGNet 18
improves results signiﬁcantly Finetuning improves CIDEr
D by roughly 004 points for CaffeNet and by roughly 007
points for VGGNet Switching from ﬁnetuned CaffeNet to
VGGNet improves CIDErD by 013 points
In Table 7 we compare generation performance with
contemporaneous and recent work submitted to the 2015
COCO caption challenge using our bestperforming method
under the CIDErD metric from the results on the valida
tion set described above  generating a caption for a single
image by taking the best of N 100 samples with a scalar
factor ofT 15applied to the softmax inputs using an
LRCN model which pairs a ﬁnetuned VGGNet with our
LRCN 2ftwo layer factored sequence model architecture
Our results are competitive with the contemporary work
performing 4th best in CIDErD 0934 compared with the
best result of 0946 from 38 and 3rd best in METEOR
0335 compared with 0346 from 38
In addition to standard quantitative evaluations we also
employ Amazon Mechnical Turk workers Turkers to
evaluate the generated sentences Given an image and a
set of descriptions from different models we ask Turkers
to rank the sentences based on correctness grammar and
relevance We compared sentences from our model to the
ones made publicly available by 31 As seen in Table 8
our ﬁnetuned FT LRCN model performs on par with the
Nearest Neighbour NN on correctness and relevance and
better on grammar
We show sample captions in Figure 6 We additionally
note some properties of the captions our model generates
When using the VGG model to generate sentences in the
validation set we ﬁnd that 337 of our generated setences
exactly match a sentence in the training set Furthermore
we ﬁnd that when using a beam size of one our model
generates 42 of the vocabulary words used by human
annotators when describing images in the validation set
Some words such as lady and guy are not generated
by our model but are commonly used by human annotators
but synonyms such as woman and man are two of the
most common words generated by our modelCorrectness Grammar Relevance
TreeTalk 46 408 435 398
VGGNet 31 371 346 370
NN 31 344 320 349
LRCN fc 8ours 374 319 372
LRCN FT ours 347 301 350
Captions 255 372 259
TABLE 8
Image description Human evaluator rankings from 16 low is good
averaged for each method and criterion We evaluated on 785 Flickr
images selected by the authors of 31 for the purposes of comparison
against this similar contemporary approach
6 V IDEO DESCRIPTION
In video description the LSTM framework allows us to
model the video as a variable length input stream How
ever due to the limitations of available video description
datasets we rely on more traditional activity and video
recognition processing for the input and use LSTMs for
generating a sentence We ﬁrst distinguish the following
architectures for video description see Figure 5 For each
architecture we assume we have predictions of activity tool
object and locations present in the video from a CRF based
on the full video input In this way we observe the video as
whole at each time step not incrementally frame by frame
a LSTM encoder  decoder with CRF max Fig
ure 5a This architecture is motivated by the video de
scription approach presented in 11 They ﬁrst recognize
a semantic representation of the video using the maximum
a posteriori MAP estimate of a CRF with video features
as unaries This representation eg hknifecutcarrotcutting
boardi is concatenated into an input sequence  knife cut
carrot cutting board  which is translated to a natural language
sentence  a person cuts a carrot on the board  using statistical
machine translation SMT 47 We replace SMT with an
encoderdecoder LSTM which encodes the input sequence
as a ﬁxed length vector before decoding to a sentence
b LSTM decoder with CRF max Figure 5b In this
variant we provide the full visual input representation at
each time step to the LSTM analogous to how an image is
provided as an input to the LSTM in image captioning
c LSTM decoder with CRF probabilites Figure 5c
A beneﬁt of using LSTMs for machine translation compared10
LSTM 
LSTM CRFcutting 
board 
cut
knife board 
cutting 
cut
knife 0 1 0 0 
1 0 0 0 
0 0 1 0 
0 0 0 1 LSTM 
LSTM 
A
man 
cuts 
EOS LSTM 
LSTM 
LSTM 
LSTM 
LSTM 
LSTM 
LSTM 
LSTM LSTM 
LSTM 
LSTM 
LSTM Decoder 
Encoder CRFmax Input 
Sentence One Hot Visual 
Input 
LSTM 
LSTM 
LSTM 
LSTM LSTM 
LSTM 
LSTM 
LSTM 0 1 0 0 0 0 1 0 0 0 0 1 
0 1 0 0 0 0 1 0 0 0 0 1 
0 1 0 0 0 0 1 0 0 0 0 1 
0 1 0 0 0 0 1 0 0 0 0 1 
LSTM 
LSTM CRF
LSTM 
LSTM LSTM 
LSTM 
LSTM 
LSTM A
man 
cuts 
EOS 0 08 02 0 03 0 07 0 0 01 02 07 
0 08 02 0 03 0 07 0 0 01 02 07 
0 08 02 0 03 0 07 0 0 01 02 07 
0 08 02 0 03 0 07 0 0 01 02 07 CRFprob Visual 
Input knife cutcutting 
board 
a b c
LSTM EncoderDecoder LSTM Decoder CRFmax LSTM Decoder CRFprob
Fig 5 Our approaches to video description a LSTM encoder  decoder with CRF max b LSTM decoder with CRF max c LSTM decoder with
CRF probabilities
Architecture Input BLEU
SMT 11 CRF max 249
SMT 48 CRF prob 269
a LSTM EncoderDecoder ours CRF max 253
b LSTM Decoder ours CRF max 274
c LSTM Decoder ours CRF prob 288
TABLE 9
Video description Results on detailed description of TACoS multilevel
48 in  see Section 6 for details
to phrasebased SMT 47 is that it can naturally incorporate
probability vectors during training and test time which
allows the LSTM to learn uncertainties in visual generation
rather than relying on MAP estimates The architecture is
the the same as in b but we replace max predictions with
probability distributions
61 Evaluation
We evaluate our approach on the TACoS multilevel 48
dataset which has 44762 videosentence pairs about
40000 for trainingvalidation We compare to 11 who use
max prediction as well as a variant presented in 48 which
takes CRF probabilities at test time and uses a word lattice
to ﬁnd an optimal sentence prediction Since we use the
max prediction as well as the probability scores provided
by 48 we have an identical visual representation 48
uses dense trajectories 49 and SIFT features as well as
temporal context reasoning modeled in a CRF In this set
of experiments we use the twolayered unfactored version
of LRCN as described for image description
Table 9 shows the BLEU4 score The results show that
1 the LSTM outperforms an SMTbased approach to video
description 2 the simpler decoder architecture b and
c achieve better performance than a likely because the
input does not need to be memorized and 3 our approach
achieves 288 clearly outperforming the best reported
number of 269 on TACoS multilevel by 48
More broadly these results show that our architecture is
not restricted only to input from deep networks but can be
cleanly integrated with ﬁxed or variable length inputs from
other vision systems
7 R ELATED WORK
We present previous literature pertaining to the three tasks
discussed in this work Additionally we discuss subsequent
extensions which combine convolutional and recurrent net
works to achieve improved results on activity recognitionimage captioning and video description as well as related
new tasks such as visual question answering
71 Prior Work
Activity Recognition Stateoftheart shallow models com
bine spatiotemporal features along dense trajectories 50
and encode features as bags of words or Fisher vectors
for classiﬁcation Such shallow features track how low
level features change through time but cannot track higher
level features Furthermore by encoding features as bags of
words or Fisher vectors temporal relationships are lost
Many deep architectures proposed for activity recogni
tion stack a ﬁxed number of video frames for input to a
deep network 3 propose a fusion convolutional network
which fuses layers which correspond to different input
frames at various levels of a deep network 4 proposes
a two stream CNN which combines one CNN trained
on RGB frames and one CNN trained on a stack of 10
ﬂow frames When combining RGB and ﬂow by averaging
softmax scores results are comparable to stateoftheart
shallow models on UCF101 25 and HMDB51 51 Results
are further improved by using an SVM to fuse RGB and
ﬂow as opposed to simply averaging scores Alternatively
1 and 2 propose learning deep spatiotemporal features
with 3D convolutional neural networks 2 52 propose
extracting visual and motion features and modeling tempo
ral dependencies with recurrent networks This architecture
most closely resembles our proposed architecture for activ
ity classiﬁcation though it differs in two key ways First we
integrate 2D CNNs that can be pretrained on large image
datasets Second we combine the CNN and LSTM into a
single model to enable endtoend ﬁnetuning
Image Captioning Several early works 53 54 55
56 on image captioning combine object and scene recog
nition with template or tree based approaches to generate
captions Such sentences are typically simple and are easily
distinguished from more ﬂuent human generated descrip
tions 46 57 address this by composing new sentences
from existing caption fragments which though more human
like are not necessarily accurate or correct
More recently a variety of deep and multimodal models
27 29 30 58 have been proposed for image and cap
tion retrieval as well as caption generation Though some of
these models rely on deep convolutional nets for image fea
ture extraction 30 58 recently researchers have realized
the importance of also including temporally deep networks11
to model text 29 propose an RNN to map sentences into
a multimodal embedding space By mapping images and
language into the same embedding space they are able to
compare images and descriptions for image and annotation
retrieval tasks 27 propose a model for caption generation
that is more similar to the model proposed in this work
predictions for the next word are based on previous words
in a sentence and image features 58 propose an encoder
decoder model for image caption retrieval which relies on
both a CNN and LSTM encoder to learn an embedding of
imagecaption pairs Their model uses a neural language
decoder to enable sentence generation As evidenced by the
rapid growth of image captioning visual sequence models
like LRCN are increasingly important for describing the
visual world using natural language
Video Description Recent approaches to describing
video with natural language have made use of templates
retrieval or language models 11 59 60 60 61 62
63 64 To our knowledge we present the ﬁrst application
of deep models to the video description task Most similar
to our work is 11 which use phrasebased SMT 47 to
generate a sentence In Section 6 we show that phrasebased
SMT can be replaced with LSTMs for video description as
has been shown previously for language translation 9 65
72 Contemporaneous and Subsequent Work
Similar work in activity recognition and visual description
was conducted contemporaneously with our work and a
variety of subsequent work has combined convolutional and
recurrent networks to both improve upon our results and
achieve exciting results on other sequential visual tasks
Activity Recognition Contemporaneous with our work
66 train a network which combines CNNs and LSTMs for
activity recognition Because activity recognition datasets
like UCF101 are relatively small in comparison to image
recognition datasets 66 pretrain their network using the
Sports1M 3 dataset which includes over a million videos
mined from YouTube By training a much larger network
four stacked LSTMs and pretraining on a large video
dataset 66 achieve 886 on the UCF101 dataset
67 also combines a convolutional network with an
LSTM to predict multiple activities per frame Unlike LRCN
67 focuses on framelevel rather than videolevel predic
tions which allows their system to label multiple activities
that occur in different temporal locations of a video clip
Like we show for activity recognition 67 demonstrates
that including temporal information improves upon a sin
gle frame baseline Additionally 67 employ an attention
mechanism to further improve results
Image Captioning 45 and 38 also propose models
which combine a CNN with a recurrent network for image
captioning Though similar to LRCN the architectures pro
posed in 45 and 38 differ in how image features are input
into the sequence model In contrast to our system in which
image features are input at each time step 45 and 38
only input image features at the ﬁrst time step Furthermore
they do not explore a factored representation Figure 4
Subsequent work 44 has proposed attention to focus on
which portion of the image is observed during sequence
generation By including attention 44 aim to visuallyfocus on the current word generated by the model Other
works aim to address speciﬁc limitations of captioning
models based on combining convolutional and recurrent
architectures For example methods have been proposed
to integrate new vocabulary with limited 40 or no 68
examples of images and corresponding captions
Video Description In this work we rely on intermedi
ate features for video description but endtoend trainable
models for visual captioning have since been proposed 69
propose creating a video feature by pooling high level CNN
features across frames The video feature is then used to
generate descriptions in the same way an image is used to
generate a description in LRCN Though achieving good
results by pooling CNN features temporal information
from the video is lost Consequently 70 propose an LSTM
to encode video frames into a ﬁxed length vector before
sentence generation with an LSTM Using an endtoend
trainable sequencetosequence model which can exploit
temporal structure in video 70 improve upon results for
video description 71 propose a similar model adding a
temporal attention mechanism which weights video frames
differently when generating each word in a sentence
Visual Grounding 72 combine CNNs with LSTMs for
visual grounding The model ﬁrst encodes a phrase which
describes part of an image using an LSTM then learns to
attend to the appropriate location in the image to accurately
reconstruct the phrase In order to reconstruct the phrase
the model must learn to visually ground the input phrase to
the appropriate location in the image
Natural Language Object Retrieval In this work we
present methods for image retrieval based on a natural
language description In contrast 73 use a model based on
LRCN for object retrieval  which returns the bounding box
around a given object as opposed to an entire image In or
der to adapt LRCN to the task of object retrieval 73 include
local convolutional features which are extracted from object
proposals and the spatial conﬁguration of object proposals
in addition to a global image feature By including local
features 73 effectively adapt LRCN for object retrieval
8 C ONCLUSION
Weve presented LRCN a class of models that is both
spatially and temporally deep and ﬂexible enough to be
applied to a variety of vision tasks involving sequential
inputs and outputs Our results consistently demonstrate
that by learning sequential dynamics with a deep sequence
model we can improve upon previous methods which learn
a deep hierarchy of parameters only in the visual domain
and on methods which take a ﬁxed visual representation
of the input and only learn the dynamics of the output
sequence
As the ﬁeld of computer vision matures beyond tasks
with static input and predictions deep sequence modeling
tools like LRCN are increasingly central to vision systems
for problems with sequential structure The ease with which
these tools can be incorporated into existing visual recog
nition pipelines makes them a natural choice for percep
tual problems with timevarying visual input or sequential
outputs which these methods are able to handle with little
input preprocessing and no handdesigned features12
A female tennis player in action on
the courtA group of young men playing a
game of soccerA man riding a wave on top of a
surfboard
A baseball game in progress with the
batter up to plateA brown bear standing on top of a
lush green ﬁeldA person holding a cell phone in
their hand
A close up of a person brushing his
teethA woman laying on a bed in a bed
roomA black and white cat is sitting on a
chair
A large clock mounted to the side of
a buildingA bunch of fruit that are sitting on a
tableA toothbrush holder sitting on top of
a white sink
Fig 6 Image description images with corresponding captions generated by our ﬁnetuned LRCN model These are images 112 of our randomly
chosen validation set from COCO 2014 33 We used beam search with a beam size of 5 to generate the sentences and display the top highest
likelihood result above13
ACKNOWLEDGMENTS
The authors thank Oriol Vinyals for valuable advice and
helpful discussion throughout this work This work was
supported in part by DARPAs MSEE and SMISC programs
NSF awards IIS1427425 and IIS1212798 and the Berkeley
Vision and Learning Center The GPUs used for this research
were donated by NVIDIA Marcus Rohrbach was supported
by a fellowship within the FITweltweitProgram of the
German Academic Exchange Service DAAD Lisa Anne
Hendricks was supported by the NDSEG
REFERENCES
1 S Ji W Xu M Yang and K Yu 3D convolutional neural
networks for human action recognition in IEEE Trans Pattern
Anal Mach Intell  2013
2 M Baccouche F Mamalet C Wolf C Garcia and A Baskurt Se
quential deep learning for human action recognition in Human
Behavior Understanding  2011
3 A Karpathy G Toderici S Shetty T Leung R Sukthankar and
L FeiFei Largescale video classiﬁcation with convolutional
neural networks in CVPR  2014
4 K Simonyan and A Zisserman Twostream convolutional net
works for action recognition in videos in NIPS  2014
5 D E Rumelhart G E Hinton and R J Williams Learning
internal representations by error propagation DTIC Document
Tech Rep 1985
6 R J Williams and D Zipser A learning algorithm for continually
running fully recurrent neural networks in Neural Computation 
1989
7 S Hochreiter and J Schmidhuber Long shortterm memory in
Neural Computation  MIT Press 1997
8 A Graves and N Jaitly Towards endtoend speech recognition
with recurrent neural networks in ICML  2014
9 I Sutskever O Vinyals and Q V  Le Sequence to sequence
learning with neural networks in NIPS  2014
10 K Cho B van Merri enboer D Bahdanau and Y Bengio On
the properties of neural machine translation Encoderdecoder
approaches in SSST Workshop  2014
11 M Rohrbach W Qiu I Titov S Thater M Pinkal and B Schiele
Translating video content to natural language descriptions in
ICCV  2013
12 Y Jia E Shelhamer J Donahue S Karayev J Long R Girshick
S Guadarrama and T Darrell Caffe Convolutional architecture
for fast feature embedding in ACM MM  2014
13 W Zaremba and I Sutskever Learning to execute in arXiv
preprint arXiv14104615  2014
14 A Graves Generating sequences with recurrent neural net
works in arXiv preprint arXiv13080850  2013
15 O Vinyals S V  Ravuri and D Povey Revisiting recurrent neural
networks for robust ASR in ICASSP  2012
16 I Sutskever J Martens and G E Hinton Generating text with
recurrent neural networks in ICML  2011
17 A Krizhevsky I Sutskever and G E Hinton ImageNet classiﬁ
cation with deep convolutional neural networks in NIPS  2012
18 K Simonyan and A Zisserman Very deep convolutional net
works for largescale image recognition in ICLR  2015
19 C Szegedy W Liu Y Jia P  Sermanet S Reed D Anguelov
D Erhan V  Vanhoucke and A Rabinovich Going deeper with
convolutions in CVPR  2015
20 K Cho B van Merrienboer C Gulcehre F Bougares H Schwenk
and Y Bengio Learning phrase representations using rnn
encoderdecoder for statistical machine translation in EMNLP 
2014
21 T Brox A Bruhn N Papenberg and J Weickert High accuracy
optical ﬂow estimation based on a theory for warping in ECCV 
2004
22 M D Zeiler and R Fergus Visualizing and understanding con
volutional networks in ECCV  2014
23 O Russakovsky J Deng H Su J Krause S Satheesh S Ma
Z Huang A Karpathy A Khosla M Bernstein A C Berg and
L FeiFei ImageNet Large Scale Visual Recognition Challenge
inIJCV  vol 115 no 3 201524 J Deng W Dong R Socher LJ Li K Li and L FeiFei Ima
geNet A largescale hierarchical image database in CVPR  2009
25 K Soomro A R Zamir and M Shah UCF101 A dataset of 101
human actions classes from videos in the wild CRCVTR1201
Tech Rep 2012
26 P  Y Micah Hodosh and J Hockenmaier Framing image descrip
tion as a ranking task Data models and evaluation metrics in
JAIR  vol 47 2013
27 J Mao W Xu Y Yang J Wang and A Yuille Deep captioning
with multimodal recurrent neural networks mRNN in ICLR 
2015
28 A Karpathy A Joulin and L FeiFei Deep fragment embed
dings for bidirectional image sentence mapping in NIPS  2014
29 R Socher A Karpathy Q V  Le C D Manning and A Y Ng
Grounded compositional semantics for ﬁnding and describing
images with sentences in TACL  vol 2 2014
30 A Frome G S Corrado J Shlens S Bengio J Dean T Mikolov
et al  Devise A deep visualsemantic embedding model in
NIPS  2013
31 R Kiros R Salakhuditnov and R S Zemel Unifying visual
semantic embeddings with multimodal neural language models
inTACL  2015
32 M H Peter Young Alice Lai and J Hockenmaier From image
descriptions to visual denotations New similarity metrics for
semantic inference over event descriptions in TACL  vol 2 2014
33 TY Lin M Maire S Belongie J Hays P  Perona D Ramanan
P  Doll ar and C L Zitnick Microsoft COCO Common objects in
context arXiv preprint arXiv14050312 Tech Rep 2014
34 K Papineni S Roukos T Ward and WJ Zhu BLEU a method
for automatic evaluation of machine translation in ACL  2002
35 R Vedantam C L Zitnick and D Parikh CIDEr Consensus
based image description evaluation in CVPR  2015
36 S Banerjee and A Lavie METEOR An automatic metric for MT
evaluation with improved correlation with human judgments in
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation andor Summarization  2005
37 CY Lin Rouge A package for automatic evaluation of sum
maries in Text Summarization Branches Out Proceedings of the ACL
04 Workshop  2004
38 O Vinyals A Toshev S Bengio and D Erhan Show and tell A
neural image caption generator in CVPR  2015
39 J Devlin H Cheng H Fang S Gupta L Deng X He G Zweig
and M Mitchell Language models for image captioning The
quirks and what works in ACL  2015
40 J Mao W Xu Y Yang J Wang Z Huang and A Yuille Learning
like a child Fast novel visual concept learning from sentence
descriptions of images in ICCV  2015
41 H Fang S Gupta F Iandola R Srivastava L Deng P  Doll ar
J Gao X He M Mitchell J Platt et al  From captions to visual
concepts and back in CVPR  2015
42 J Devlin S Gupta R Girshick M Mitchell and C L Zitnick
Exploring nearest neighbor approaches for image captioning
arXiv preprint arXiv150504467 Tech Rep 2015
43 J Donahue L A Hendricks S Guadarrama M Rohrbach
S Venugopalan K Saenko and T Darrell Longterm recurrent
convolutional networks for visual recognition and description in
CVPR  2015
44 K Xu J Ba R Kiros A Courville R Salakhutdinov R Zemel and
Y Bengio Show attend and tell Neural image caption generation
with visual attention in ICML  2015
45 A Karpathy and L FeiFei Deep visualsemantic alignments for
generating image descriptions in CVPR  2015
46 P  Kuznetsova V  Ordonez T L Berg U C Hill and Y Choi
TreeTalk Composition and compression of trees for image de
scriptions in TACL  vol 2 no 10 2014
47 P  Koehn H Hoang A Birch C CallisonBurch M Federico
N Bertoldi B Cowan W Shen C Moran R Zens C Dyer
O Bojar A Constantin and E Herbst Moses Open source
toolkit for statistical machine translation in ACL  2007
48 A Rohrbach M Rohrbach W Qiu A Friedrich M Pinkal
and B Schiele Coherent multisentence video description with
variable level of detail in German Conference on Pattern Recognition
GCPR  Springer 2014
49 H Wang A Kl aser C Schmid and C Liu Dense trajectories
and motion boundary descriptors for action recognition in IJCV 
201314
50 H Wang and C Schmid Action recognition with improved
trajectories in ICCV  2013
51 H Kuehne H Jhuang E Garrote T Poggio and T Serre HMDB
a large video database for human motion recognition in ICCV 
2011
52 M Baccouche F Mamalet C Wolf C Garcia and A Baskurt Ac
tion classiﬁcation in soccer videos with long shortterm memory
recurrent neural networks in International Conference on Artiﬁcial
Neural Networks ICANN  2010
53 A Farhadi M Hejrati M Sadeghi P  Young C Rashtchian
J Hockenmaier and D Forsyth Every picture tells a story
Generating sentences from images in ECCV  2010
54 G Kulkarni V  Premraj S Dhar S Li Y Choi A C Berg and T L
Berg Baby talk Understanding and generating simple image
descriptions in CVPR  2011
55 Y Yang C L Teo H Daum e III and Y Aloimonos Corpus
guided sentence generation of natural images in EMNLP  2011
56 M Mitchell X Han J Dodge A Mensch A Goyal A Berg
K Yamaguchi T Berg K Stratos and H Daum e III Midge
Generating image descriptions from computer vision detections
inProceedings of the 13th Conference of the European Chapter of the
Association for Computational Linguistics  2012
57 P  Kuznetsova V  Ordonez A C Berg T L Berg and Y Choi
Collective generation of natural image descriptions in ACL 
2012
58 R Kiros R Salakhutdinov and R Zemel Multimodal neural
language models in ICML  2014
59 S Guadarrama N Krishnamoorthy G Malkarnenkar S Venu
gopalan R Mooney T Darrell and K Saenko YouTube2Text
Recognizing and describing arbitrary activities using semantic
hierarchies and zeroshoot recognition in ICCV  2013
60 M U G Khan L Zhang and Y Gotoh Human focused video
description in ICCV Workshops  2011
61 A Barbu A Bridge Z Burchill D Coroian S Dickinson S Fidler
A Michaux S Mussman S Narayanaswamy D Salvi L Schmidt
J Shangguan J M Siskind J Waggoner S Wang J Wei Y Yin
and Z Zhang Video in sentences out in The Conference on
Uncertainty in Artiﬁcial Intelligence UAI  2012
62 P  Das C Xu R Doell and J Corso Thousand frames in just
a few words Lingual description of videos through latent topics
and sparse object stitching in CVPR  2013
63 C C Tan YG Jiang and CW Ngo Towards textually describ
ing complex video contents with audiovisual concept classiﬁers
inACM MM  2011
64 J Thomason S Venugopalan S Guadarrama K Saenko and R J
Mooney Integrating language and vision to generate natural
language descriptions of videos in the wild in International
Conference on Computational Linguistics COLING  2014
65 H Sak O Vinyals G Heigold A Senior E McDermott R Monga
and M Mao Sequence discriminative distributed training of long
shortterm memory recurrent neural networks in Interspeech 
2014
66 J YH Ng M Hausknecht S Vijayanarasimhan O Vinyals
R Monga and G Toderici Beyond short snippets Deep net
works for video classiﬁcation in CVPR  2015
67 S Yeung O Russakovsky N Jin M Andriluka G Mori and
L FeiFei Every moment counts Dense detailed labeling of
actions in complex videos arXiv preprint arXiv150705738 Tech
Rep 2015
68 L A Hendricks S Venugopalan M Rohrbach R Mooney
K Saenko and T Darrell Deep compositional captioning De
scribing novel object categories without paired training data in
CVPR  2016
69 S Venugopalan H Xu J Donahue M Rohrbach R Mooney and
K Saenko Translating videos to natural language using deep
recurrent neural networks in NAACL  2015
70 S Venugopalan M Rohrbach J Donahue R Mooney T Darrell
and K Saenko Sequence to sequencevideo to text in ICCV 
2015
71 L Yao A Torabi K Cho N Ballas C Pal H Larochelle and
A Courville Describing videos by exploiting temporal struc
ture in CVPR  vol 1050 2015
72 A Rohrbach M Rohrbach R Hu T Darrell and B Schiele
Grounding of textual phrases in images by reconstruction arXiv
preprint arXiv151103745 Tech Rep 2015
73 R Hu H Xu M Rohrbach J Feng K Saenko and T Darrell
Natural language object retrieval in CVPR  2016Jeff Donahue is a PhD student at the University of California Berkeley
advised by Prof Trevor Darrell His research focuses on the use of deep
learning for computer vision applications He graduated with a BS in
computer science from the University of Texas at Austin where he was
advised by Prof Kristen Grauman
Lisa Anne Hendricks is a PhD student at the University of California
Berkeley Her research focuses on deep learning for sequential models
as well as applications at the intersection of language and vision She is
advised by Prof Trevor Darrell Lisa Anne holds a Bachelors of Science
in Electrical Engineering BSEE from Rice University
Marcus Rohrbach sresearch focuses on visual recognition language
understanding and machine learning He received his BSc and MSc
degree in Computer Science from the University of Technology Darm
stadt Germany in 2006 and 2009 respectively From 20062007 he
spent one year at the University of British Columbia as a graduate
visiting student During his PhD he worked at the Max Planck Institute
for Informatics Saarbr ucken Germany with Bernt Schiele and Manfred
Pinkal He completed it in 2014 with summa cum laude at Saarland
University and received the DAGM MVTec Dissertation Award 2015 for
it He currently works as a postdoc with Trevor Darrell at UC Berkeley
Subhashini Venugopalan is a PhD student at the University of Texas at
Austin Her research focuses on deep learning techniques to generate
descriptions for events in videos She is advised by Prof Raymond
Mooney Subhashini holds a masters degree in Computer Science from
IIT Madras and a bachelors degree from NIT Karnataka India
Sergio Guadarrama is a Software Engineer at Google Research where
he works in Machine Perception as a member of the Vale team He
received his PhD from the Technical University of Madrid followed by
postdoctoral work at the European Center for Soft Computing After
that he was ﬁrst a Visiting Scholar and then a Research Scientist at
UC Berkeley EECS His research spans the areas of computer vision
language and deep learning Dr Guadarramas current research focus
is on new network architectures for multitask dense predictions such
as object detection instance segmentation depth prediction and visual
questionanswering He has received research grants from the Govern
ment of Spain such as the Juan de la Cierva Award Early Career Award
in Computer Science and the Mobility Grant for Postdoctoral Research
Kate Saenko is an Assistant Professor of Computer Science at the
University of Massachusetts Lowell where she leads the Computer
Vision and Learning Group She received her PhD from MIT followed
by postdoctoral work at UC Berkeley EECS and Harvard SEAS Her
research spans the areas of computer vision machine learning and
humanrobot interfaces Dr Saenkos current research interests include
domain adaptation of machine learning models and joint modeling of
language and vision She is the recipient of research grant awards from
the National Science Foundation DARPA and other government and
industry agencies
Trevor Darrell is on the faculty of the CS Division of the EECS Depart
ment at UC Berkeley and is also appointed at the UCBafﬁliated Interna
tional Computer Science Institute ICSI He is the director of the Berke
ley Vision and Learning Center BVLC and is the faculty director of the
PATH center in the UCB Institute of Transportation Studies PATH His
interests include computer vision machine learning computer graphics
and perceptionbased human computer interfaces Prof Darrell received
the SM and PhD degrees from MIT in 1992 and 1996 respectively He
was previously on the faculty of the MIT EECS department from 1999
2008 where he directed the Vision Interface Group He was a member
of the research staff at Interval Research Corporation from 19961999
He obtained the BSE degree from the University of Pennsylvania in
1988 having started his career in computer vision as an undergraduate
researcher in Ruzena Bajcsys GRASP lab
  Show and Tell A Neural Image Caption Generator
Oriol Vinyals
Google
vinyalsgooglecomAlexander Toshev
Google
toshevgooglecomSamy Bengio
Google
bengiogooglecomDumitru Erhan
Google
dumitrugooglecom
Abstract
Automatically describing the content of an image is a
fundamental problem in artiﬁcial intelligence that connects
computer vision and natural language processing In this
paper we present a generative model based on a deep re
current architecture that combines recent advances in com
puter vision and machine translation and that can be used
to generate natural sentences describing an image The
model is trained to maximize the likelihood of the target de
scription sentence given the training image Experiments
on several datasets show the accuracy of the model and the
ﬂuency of the language it learns solely from image descrip
tions Our model is often quite accurate which we verify
both qualitatively and quantitatively For instance while
the current stateoftheart BLEU1 score the higher the
better on the Pascal dataset is 25 our approach yields 59
to be compared to human performance around 69 We also
show BLEU1 score improvements on Flickr30k from 56 to
66 and on SBU from 19 to 28 Lastly on the newly released
COCO dataset we achieve a BLEU4 of 277 which is the
current stateoftheart
1 Introduction
Being able to automatically describe the content of an
image using properly formed English sentences is a very
challenging task but it could have great impact for instance
by helping visually impaired people better understand the
content of images on the web This task is signiﬁcantly
harder for example than the wellstudied image classiﬁ
cation or object recognition tasks which have been a main
focus in the computer vision community 27 Indeed a
description must capture not only the objects contained in
an image but it also must express how these objects relate
to each other as well as their attributes and the activities
they are involved in Moreover the above semantic knowl
edge has to be expressed in a natural language like English
which means that a language model is needed in addition to
visual understanding
Most previous attempts have proposed to stitch together
A group of people shopping at an outdoor market There are many vegetables at the fruit standVisionDeep CNNLanguage GeneratingRNN
Figure 1 NIC our model is based endtoend on a neural net
work consisting of a vision CNN followed by a language gener
ating RNN It generates complete sentences in natural language
from an input image as shown on the example above
existing solutions of the above subproblems in order to go
from an image to its description 6 16 In contrast we
would like to present in this work a single joint model that
takes an image Ias input and is trained to maximize the
likelihoodpSjIof producing a target sequence of words
SfS1S2gwhere each word Stcomes from a given
dictionary that describes the image adequately
The main inspiration of our work comes from recent ad
vances in machine translation where the task is to transform
a sentenceSwritten in a source language into its transla
tionTin the target language by maximizing pTjS For
many years machine translation was also achieved by a se
ries of separate tasks translating words individually align
ing words reordering etc but recent work has shown that
translation can be done in a much simpler way using Re
current Neural Networks RNNs 3 2 30 and still reach
stateoftheart performance An encoder RNN reads the
source sentence and transforms it into a rich ﬁxedlength
vector representation which in turn in used as the initial
hidden state of a decoder RNN that generates the target
sentence
Here we propose to follow this elegant recipe replac
ing the encoder RNN by a deep convolution neural network
CNN Over the last few years it has been convincingly
shown that CNNs can produce a rich representation of the
input image by embedding it to a ﬁxedlength vector such
that this representation can be used for a variety of vision
1arXiv14114555v2  csCV  20 Apr 2015tasks 28 Hence it is natural to use a CNN as an image
encoder by ﬁrst pretraining it for an image classiﬁcation
task and using the last hidden layer as an input to the RNN
decoder that generates sentences see Fig 1 We call this
model the Neural Image Caption or NIC
Our contributions are as follows First we present an
endtoend system for the problem It is a neural net which
is fully trainable using stochastic gradient descent Second
our model combines stateofart subnetworks for vision
and language models These can be pretrained on larger
corpora and thus can take advantage of additional data Fi
nally it yields signiﬁcantly better performance compared
to stateoftheart approaches for instance on the Pascal
dataset NIC yielded a BLEU score of 59 to be compared to
the current stateoftheart of 25 while human performance
reaches 69 On Flickr30k we improve from 56 to 66 and
on SBU from 19 to 28
2 Related Work
The problem of generating natural language descriptions
from visual data has long been studied in computer vision
but mainly for video 7 32 This has led to complex sys
tems composed of visual primitive recognizers combined
with a structured formal language eg AndOr Graphs or
logic systems which are further converted to natural lan
guage via rulebased systems Such systems are heav
ily handdesigned relatively brittle and have been demon
strated only on limited domains eg trafﬁc scenes or sports
The problem of still image description with natural text
has gained interest more recently Leveraging recent ad
vances in recognition of objects their attributes and loca
tions allows us to drive natural language generation sys
tems though these are limited in their expressivity Farhadi
et al 6 use detections to infer a triplet of scene elements
which is converted to text using templates Similarly Li
et al 19 start off with detections and piece together a ﬁ
nal description using phrases containing detected objects
and relationships A more complex graph of detections
beyond triplets is used by Kulkani et al 16 but with
templatebased text generation More powerful language
models based on language parsing have been used as well
23 1 17 18 5 The above approaches have been able to
describe images in the wild but they are heavily hand
designed and rigid when it comes to text generation
A large body of work has addressed the problem of rank
ing descriptions for a given image 11 8 24 Such ap
proaches are based on the idea of coembedding of images
and text in the same vector space For an image query de
scriptions are retrieved which lie close to the image in the
embedding space Most closely neural networks are used to
coembed images and sentences together 29 or even image
crops and subsentences 13 but do not attempt to generate
novel descriptions In general the above approaches cannotdescribe previously unseen compositions of objects even
though the individual objects might have been observed in
the training data Moreover they avoid addressing the prob
lem of evaluating how good a generated description is
In this work we combine deep convolutional nets for im
age classiﬁcation 12 with recurrent networks for sequence
modeling 10 to create a single network that generates de
scriptions of images The RNN is trained in the context of
this single endtoend network The model is inspired by
recent successes of sequence generation in machine trans
lation 3 2 30 with the difference that instead of starting
with a sentence we provide an image processed by a con
volutional net The closest works are by Kiros et al 15
who use a neural net but a feedforward one to predict the
next word given the image and previous words A recent
work by Mao et al 21 uses a recurrent NN for the same
prediction task This is very similar to the present proposal
but there are a number of important differences we use a
more powerful RNN model and provide the visual input to
the RNN model directly which makes it possible for the
RNN to keep track of the objects that have been explained
by the text As a result of these seemingly insigniﬁcant dif
ferences our system achieves substantially better results on
the established benchmarks Lastly Kiros et al 14 pro
pose to construct a joint multimodal embedding space by
using a powerful computer vision model and an LSTM that
encodes text In contrast to our approach they use two sepa
rate pathways one for images one for text to deﬁne a joint
embedding and even though they can generate text their
approach is highly tuned for ranking
3 Model
In this paper we propose a neural and probabilistic
framework to generate descriptions from images Recent
advances in statistical machine translation have shown that
given a powerful sequence model it is possible to achieve
stateoftheart results by directly maximizing the proba
bility of the correct translation given an input sentence in
an endtoend fashion  both for training and inference
These models make use of a recurrent neural network which
encodes the variable length input into a ﬁxed dimensional
vector and uses this representation to decode it to the de
sired output sentence Thus it is natural to use the same ap
proach where given an image instead of an input sentence
in the source language one applies the same principle of
translating it into its description
Thus we propose to directly maximize the probability of
the correct description given the image by using the follow
ing formulation
 arg max
X
ISlogpSjI 1
whereare the parameters of our model Iis an image andSits correct transcription Since Srepresents any sentence
its length is unbounded Thus it is common to apply the
chain rule to model the joint probability over S0SN
whereNis the length of this particular example as
logpSjI NX
t0logpStjIS 0St1 2
where we dropped the dependency on for convenience
At training time SIis a training example pair and we
optimize the sum of the log probabilities as described in 2
over the whole training set using stochastic gradient descent
further training details are given in Section 4
It is natural to model pStjIS 0St1with a Re
current Neural Network RNN where the variable number
of words we condition upon up to t1is expressed by a
ﬁxed length hidden state or memory ht This memory is
updated after seeing a new input xtby using a nonlinear
functionf
ht1fhtxt 3
To make the above RNN more concrete two crucial design
choices are to be made what is the exact form of fand
how are the images and words fed as inputs xt Forfwe
use a LongShort Term Memory LSTM net which has
shown stateofthe art performance on sequence tasks such
as translation This model is outlined in the next section
For the representation of images we use a Convolutional
Neural Network CNN They have been widely used and
studied for image tasks and are currently stateofthe art
for object recognition and detection Our particular choice
of CNN uses a novel approach to batch normalization and
yields the current best performance on the ILSVRC 2014
classiﬁcation competition 12 Furthermore they have
been shown to generalize to other tasks such as scene clas
siﬁcation by means of transfer learning 4 The words are
represented with an embedding model
31 LSTMbased Sentence Generator
The choice of fin 3 is governed by its ability to deal
with vanishing and exploding gradients 10 the most com
mon challenge in designing and training RNNs To address
this challenge a particular form of recurrent nets called
LSTM was introduced 10 and applied with great success
to translation 3 30 and sequence generation 9
The core of the LSTM model is a memory cell cencod
ing knowledge at every time step of what inputs have been
observed up to this step see Figure 2  The behavior of the
cell is controlled by gates  layers which are applied mul
tiplicatively and thus can either keep a value from the gated
layer if the gate is 1or zero this value if the gate is 0 In
particular three gates are being used which control whether
to forget the current cell value forget gate f if it should
hσσσc
inputLSTMmemory blockword predictionsoftmax
inputgate ioutputgate fforgetgate f
updatingtermct1ctmt
xFigure 2 LSTM the memory block contains a cell cwhich is
controlled by three gates In blue we show the recurrent connec
tions  the output mat time t1is fed back to the memory at
time tvia the three gates the cell value is fed back via the forget
gate the predicted word at time t1is fed back in addition to the
memory output mat time tinto the Softmax for word prediction
read its input input gate i and whether to output the new
cell value output gate o The deﬁnition of the gates and
cell update and output are as follows
itWixxtWimmt1 4
ftWfxxtWfmmt1 5
otWoxxtWommt1 6
ctftct1ithWcxxtWcmmt17
mtotct 8
pt1 Softmax mt 9
whererepresents the product with a gate value and the
variousWmatrices are trained parameters Such multi
plicative gates make it possible to train the LSTM robustly
as these gates deal well with exploding and vanishing gra
dients 10 The nonlinearities are sigmoid and hyper
bolic tangent h The last equation mtis what is used to
feed to a Softmax which will produce a probability distri
butionptover all words
Training The LSTM model is trained to predict each
word of the sentence after it has seen the image as well
as all preceding words as deﬁned by pStjIS 0St1
For this purpose it is instructive to think of the LSTM in un
rolled form  a copy of the LSTM memory is created for theLSTMLSTMLSTMWeS1WeSN1p1pNp2log p1S1 log p2S2 log pNSN LSTMWeS0S1SN1S0
imageFigure 3 LSTM model combined with a CNN image embedder
as deﬁned in 12 and word embeddings The unrolled connec
tions between the LSTM memories are in blue and they corre
spond to the recurrent connections in Figure 2 All LSTMs share
the same parameters
image and each sentence word such that all LSTMs share
the same parameters and the output mt1of the LSTM at
timet1is fed to the LSTM at time tsee Figure 3 All
recurrent connections are transformed to feedforward con
nections in the unrolled version In more detail if we denote
byIthe input image and by S S0SNa true sen
tence describing this image the unrolling procedure reads
x1CNN I 10
xtWeSt t2f0N1g 11
pt1 LSTM xt t2f0N1g 12
where we represent each word as a onehot vector Stof
dimension equal to the size of the dictionary Note that we
denote byS0a special start word and by SNa special stop
word which designates the start and end of the sentence In
particular by emitting the stop word the LSTM signals that a
complete sentence has been generated Both the image and
the words are mapped to the same space the image by using
a vision CNN the words by using word embedding We
The imageIis only input once at t1 to inform the
LSTM about the image contents We empirically veriﬁed
that feeding the image at each time step as an extra input
yields inferior results as the network can explicitly exploit
noise in the image and overﬁts more easily
Our loss is the sum of the negative log likelihood of the
correct word at each step as follows
LIS NX
t1logptSt 13
The above loss is minimized wrt all the parameters of the
LSTM the top layer of the image embedder CNN and word
embeddings WeInference There are multiple approaches that can be used
to generate a sentence given an image with NIC The ﬁrst
one is Sampling where we just sample the ﬁrst word ac
cording top1 then provide the corresponding embedding
as input and sample p2 continuing like this until we sample
the special endofsentence token or some maximum length
The second one is BeamSearch  iteratively consider the set
of thekbest sentences up to time tas candidates to generate
sentences of size t 1 and keep only the resulting best k
of them This better approximates S arg max S0pS0jI
We used the BeamSearch approach in the following experi
ments with a beam of size 20 Using a beam size of 1 ie
greedy search did degrade our results by 2 BLEU points on
average
4 Experiments
We performed an extensive set of experiments to assess
the effectiveness of our model using several metrics data
sources and model architectures in order to compare to
prior art
41 Evaluation Metrics
Although it is sometimes not clear whether a description
should be deemed successful or not given an image prior
art has proposed several evaluation metrics The most re
liable but time consuming is to ask for raters to give a
subjective score on the usefulness of each description given
the image In this paper we used this to reinforce that some
of the automatic metrics indeed correlate with this subjec
tive score following the guidelines proposed in 11 which
asks the graders to evaluate each generated sentence with a
scale from 1 to 41
For this metric we set up an Amazon Mechanical Turk
experiment Each image was rated by 2 workers The typ
ical level of agreement between workers is 65 In case
of disagreement we simply average the scores and record
the average as the score For variance analysis we perform
bootstrapping resampling the results with replacement and
computing meansstandard deviation over the resampled re
sults Like 11 we report the fraction of scores which are
larger or equal than a set of predeﬁned thresholds
The rest of the metrics can be computed automatically
assuming one has access to groundtruth ie human gen
erated descriptions The most commonly used metric so
far in the image description literature has been the BLEU
score 25 which is a form of precision of word ngrams
between generated and reference sentences2 Even though
1The raters are asked whether the image is described without any er
rors described with minor errors with a somewhat related description or
with an unrelated description with a score of 4 being the best and 1 being
the worst
2In this literature most previous work report BLEU1 ie they only
compute precision at the unigram level whereas BLEUn is a geometric
average of precision over 1 to ngramsthis metric has some obvious drawbacks it has been shown
to correlate well with human evaluations In this work
we corroborate this as well as we show in Section 43
An extensive evaluation protocol as well as the generated
outputs of our system can be found at httpnic
droppagescom 
Besides BLEU one can use the perplexity of the model
for a given transcription which is closely related to our
objective function in 1 The perplexity is the geometric
mean of the inverse probability for each predicted word We
used this metric to perform choices regarding model selec
tion and hyperparameter tuning in our heldout set but we
do not report it since BLEU is always preferred3 A much
more detailed discussion regarding metrics can be found in
31 and research groups working on this topic have been
reporting other metrics which are deemed more appropriate
for evaluating caption We report two such metrics  ME
TEOR and Cider  hoping for much more discussion and
research to arise regarding the choice of metric
Lastly the current literature on image description has
also been using the proxy task of ranking a set of avail
able descriptions with respect to a given image see for in
stance 14 Doing so has the advantage that one can use
known ranking metrics like recallk On the other hand
transforming the description generation task into a ranking
task is unsatisfactory as the complexity of images to de
scribe grows together with its dictionary the number of
possible sentences grows exponentially with the size of the
dictionary and the likelihood that a predeﬁned sentence will
ﬁt a new image will go down unless the number of such
sentences also grows exponentially which is not realistic
not to mention the underlying computational complexity
of evaluating efﬁciently such a large corpus of stored sen
tences for each image The same argument has been used in
speech recognition where one has to produce the sentence
corresponding to a given acoustic sequence while early at
tempts concentrated on classiﬁcation of isolated phonemes
or words stateoftheart approaches for this task are now
generative and can produce sentences from a large dictio
nary
Now that our models can generate descriptions of rea
sonable quality and despite the ambiguities of evaluating
an image description where there could be multiple valid
descriptions not in the groundtruth we believe we should
concentrate on evaluation metrics for the generation task
rather than for ranking
42 Datasets
For evaluation we use a number of datasets which consist
of images and sentences in English describing these images
3Even though it would be more desirable optimizing for BLEU score
yields a discrete optimization problem In general perplexity and BLEU
scores are fairly correlatedThe statistics of the datasets are as follows
Dataset namesize
train valid test
Pascal VOC 2008 6   1000
Flickr8k 26 6000 1000 1000
Flickr30k 33 28000 1000 1000
MSCOCO 20 82783 40504 40775
SBU 24 1M  
With the exception of SBU each image has been annotated
by labelers with 5 sentences that are relatively visual and
unbiased SBU consists of descriptions given by image
owners when they uploaded them to Flickr As such they
are not guaranteed to be visual or unbiased and thus this
dataset has more noise
The Pascal dataset is customary used for testing only af
ter a system has been trained on different data such as any of
the other four dataset In the case of SBU we hold out 1000
images for testing and train on the rest as used by 18 Sim
ilarly we reserve 4K random images from the MSCOCO
validation set as test called COCO4k and use it to report
results in the following section
43 Results
Since our model is data driven and trained endtoend
and given the abundance of datasets we wanted to an
swer questions such as how dataset size affects general
ization what kinds of transfer learning it would be able
to achieve and how it would deal with weakly labeled
examples As a result we performed experiments on ﬁve
different datasets explained in Section 42 which enabled
us to understand our model in depth
431 Training Details
Many of the challenges that we faced when training our
models had to do with overﬁtting Indeed purely supervised
approaches require large amounts of data but the datasets
that are of high quality have less than 100000 images The
task of assigning a description is strictly harder than object
classiﬁcation and data driven approaches have only recently
become dominant thanks to datasets as large as ImageNet
with ten times more data than the datasets we described
in this paper with the exception of SBU As a result we
believe that even with the results we obtained which are
quite good the advantage of our method versus most cur
rent humanengineered approaches will only increase in the
next few years as training set sizes will grow
Nonetheless we explored several techniques to deal with
overﬁtting The most obvious way to not overﬁt is to ini
tialize the weights of the CNN component of our system
to a pretrained model eg on ImageNet We did this in
all the experiments similar to 8 and it did help quite alot in terms of generalization Another set of weights that
could be sensibly initialized are We the word embeddings
We tried initializing them from a large news corpus 22
but no signiﬁcant gains were observed and we decided to
just leave them uninitialized for simplicity Lastly we did
some model level overﬁttingavoiding techniques We tried
dropout 34 and ensembling models as well as exploring
the size ie capacity of the model by trading off number
of hidden units versus depth Dropout and ensembling gave
a few BLEU points improvement and that is what we report
throughout the paper
We trained all sets of weights using stochastic gradi
ent descent with ﬁxed learning rate and no momentum
All weights were randomly initialized except for the CNN
weights which we left unchanged because changing them
had a negative impact We used 512 dimensions for the em
beddings and the size of the LSTM memory
Descriptions were preprocessed with basic tokenization
keeping all words that appeared at least 5 times in the train
ing set
432 Generation Results
We report our main results on all the relevant datasets in Ta
bles 1 and 2 Since PASCAL does not have a training set
we used the system trained using MSCOCO arguably the
largest and highest quality dataset for this task The state
oftheart results for PASCAL and SBU did not use image
features based on deep learning so arguably a big improve
ment on those scores comes from that change alone The
Flickr datasets have been used recently 11 21 14 but
mostly evaluated in a retrieval framework A notable ex
ception is 21 where they did both retrieval and genera
tion and which yields the best performance on the Flickr
datasets up to now
Human scores in Table 2 were computed by comparing
one of the human captions against the other four We do this
for each of the ﬁve raters and average their BLEU scores
Since this gives a slight advantage to our system given the
BLEU score is computed against ﬁve reference sentences
and not four we add back to the human scores the average
difference of having ﬁve references instead of four
Given that the ﬁeld has seen signiﬁcant advances in the
last years we do think it is more meaningful to report
BLEU4 which is the standard in machine translation mov
ing forward Additionally we report metrics shown to cor
relate better with human evaluations in Table 14 Despite
recent efforts on better evaluation metrics 31 our model
fares strongly versus human raters However when evalu
ating our captions using human raters see Section 436
our model fares much more poorly suggesting more work
4We used the implementation of these metrics kindly provided in
httpwwwmscocoorg Metric BLEU4 METEOR CIDER
NIC 277 237 855
Random 46 90 51
Nearest Neighbor 99 157 365
Human 217 252 854
Table 1 Scores on the MSCOCO development set
Approach PASCAL Flickr Flickr SBU
xfer 30k 8k
Im2Text 24 11
TreeTalk 18 19
BabyTalk 16 25
Tri5Sem 11 48
mRNN 21 55 58
MNLM 14556 51
SOTA 25 56 58 19
NIC 59 66 63 28
Human 69 68 70
Table 2 BLEU1 scores We only report previous work results
when available SOTA stands for the current stateoftheart
is needed towards better metrics On the ofﬁcial test set for
which labels are only available through the ofﬁcial website
our model had a 272 BLEU4
433 Transfer Learning Data Size and Label Quality
Since we have trained many models and we have several
testing sets we wanted to study whether we could transfer
a model to a different dataset and how much the mismatch
in domain would be compensated with eg higher quality
labels or more training data
The most obvious case for transfer learning and data size
is between Flickr30k and Flickr8k The two datasets are
similarly labeled as they were created by the same group
Indeed when training on Flickr30k with about 4 times
more training data the results obtained are 4 BLEU points
better It is clear that in this case we see gains by adding
more training data since the whole process is datadriven
and overﬁtting prone MSCOCO is even bigger 5 times
more training data than Flickr30k but since the collection
process was done differently there are likely more differ
ences in vocabulary and a larger mismatch Indeed all the
BLEU scores degrade by 10 points Nonetheless the de
scriptions are still reasonable
Since PASCAL has no ofﬁcial training set and was
collected independently of Flickr and MSCOCO we re
port transfer learning from MSCOCO in Table 2 Doing
transfer learning from Flickr30k yielded worse results with
BLEU1 at 53 cf 59
Lastly even though SBU has weak labeling ie the la
bels were captions and not human generated descriptions
5We computed these BLEU scores with the outputs that the authors of
14 kindly provided for their OxfordNet systemthe task is much harder with a much larger and noisier vo
cabulary However much more data is available for train
ing When running the MSCOCO model on SBU our per
formance degrades from 28 down to 16
434 Generation Diversity Discussion
Having trained a generative model that gives pSjI an ob
vious question is whether the model generates novel cap
tions and whether the generated captions are both diverse
and high quality Table 3 shows some samples when re
turning the Nbest list from our beam search decoder in
stead of the best hypothesis Notice how the samples are di
verse and may show different aspects from the same image
The agreement in BLEU score between the top 15 generated
sentences is 58 which is similar to that of humans among
them This indicates the amount of diversity our model gen
erates In bold are the sentences that are not present in the
training set If we take the best candidate the sentence is
present in the training set 80 of the times This is not
too surprising given that the amount of training data is quite
small so it is relatively easy for the model to pick exem
plar sentences and use them to generate descriptions If
we instead analyze the top 15 generated sentences about
half of the times we see a completely novel description but
still with a similar BLEU score indicating that they are of
enough quality yet they provide a healthy diversity
A man throwing a frisbee in a park
A man holding a frisbee in his hand
A man standing in the grass with a frisbee
A close up of a sandwich on a plate
A close up of a plate of food with french fries
A white plate topped with a cut in half sandwich
A display case ﬁlled with lots of donuts
A display case ﬁlled with lots of cakes
A bakery display case ﬁlled with lots of donuts
Table 3 Nbest examples from the MSCOCO test set Bold lines
indicate a novel sentence not present in the training set
435 Ranking Results
While we think ranking is an unsatisfactory way to evalu
ate description generation from images many papers report
ranking scores using the set of testing captions as candi
dates to rank given a test image The approach that works
best on these metrics MNLM speciﬁcally implemented a
rankingaware loss Nevertheless NIC is doing surprisingly
well on both ranking tasks ranking descriptions given im
ages and ranking images given descriptions as can be seen
in Tables 4 and 5 Note that for the Image Annotation task
we normalized our scores similar to what 21 usedApproachImage Annotation Image Search
R1 R10 Med rR1 R10 Med r
DeFrag 13 13 44 14 10 43 15
mRNN 21 15 49 11 12 42 15
MNLM 14 18 55 8 13 52 10
NIC 20 61 6 19 64 5
Table 4 Recallk and median rank on Flickr8k
ApproachImage Annotation Image Search
R1 R10 Med rR1 R10 Med r
DeFrag 13 16 55 8 10 45 13
mRNN 21 18 51 10 13 42 16
MNLM 14 23 63 5 17 57 8
NIC 17 56 7 17 57 7
Table 5 Recallk and median rank on Flickr30k
Figure 4 Flickr8k NIC  predictions produced by NIC on the
Flickr8k test set average score 237 Pascal NIC  average
score 245 COCO1k NIC  A subset of 1000 images from the
MSCOCO test set with descriptions produced by NIC average
score 272 Flickr8k ref  these are results from 11 on Flickr8k
rated using the same protocol as a baseline average score 208
Flickr8k GT  we rated the groundtruth labels from Flickr8k us
ing the same protocol This provides us with a calibration of the
scores average score 389
436 Human Evaluation
Figure 4 shows the result of the human evaluations of the
descriptions provided by NIC as well as a reference system
and groundtruth on various datasets We can see that NIC
is better than the reference system but clearly worse than
the groundtruth as expected This shows that BLEU is not
a perfect metric as it does not capture well the difference
between NIC and human descriptions assessed by raters
Examples of rated images can be seen in Figure 5 It is
interesting to see for instance in the second image of the
ﬁrst column how the model was able to notice the frisbee
given its sizeFigure 5 A selection of evaluation results grouped by human rating
437 Analysis of Embeddings
In order to represent the previous word St1as input to
the decoding LSTM producing St we use word embedding
vectors 22 which have the advantage of being indepen
dent of the size of the dictionary contrary to a simpler one
hotencoding approach Furthermore these word embed
dings can be jointly trained with the rest of the model It
is remarkable to see how the learned representations have
captured some semantic from the statistics of the language
Table 437 shows for a few example words the nearest
other words found in the learned embedding space
Note how some of the relationships learned by the model
will help the vision component Indeed having horse
pony and donkey close to each other will encourage the
CNN to extract features that are relevant to horselooking
animals We hypothesize that in the extreme case where
we see very few examples of a class eg unicorn its
proximity to other word embeddings eg horse should
provide a lot more information that would be completely
lost with more traditional bagofwords based approaches
5 Conclusion
We have presented NIC an endtoend neural network
system that can automatically view an image and generateWord Neighbors
car van cab suv vehicule jeep
boy toddler gentleman daughter son
street road streets highway freeway
horse pony donkey pig goat mule
computer computers pc crt chip compute
Table 6 Nearest neighbors of a few example words
a reasonable description in plain English NIC is based on
a convolution neural network that encodes an image into a
compact representation followed by a recurrent neural net
work that generates a corresponding sentence The model is
trained to maximize the likelihood of the sentence given the
image Experiments on several datasets show the robust
ness of NIC in terms of qualitative results the generated
sentences are very reasonable and quantitative evaluations
using either ranking metrics or BLEU a metric used in ma
chine translation to evaluate the quality of generated sen
tences It is clear from these experiments that as the size
of the available datasets for image description increases so
will the performance of approaches like NIC Furthermore
it will be interesting to see how one can use unsupervised
data both from images alone and text alone to improve im
age description approachesAcknowledgement
We would like to thank Geoffrey Hinton Ilya Sutskever
Quoc Le Vincent Vanhoucke and Jeff Dean for useful dis
cussions on the ideas behind the paper and the write up
References
1 A Aker and R Gaizauskas Generating image descriptions
using dependency relational patterns In ACL 2010
2 D Bahdanau K Cho and Y  Bengio Neural ma
chine translation by jointly learning to align and translate
arXiv14090473  2014
3 K Cho B van Merrienboer C Gulcehre F Bougares
H Schwenk and Y  Bengio Learning phrase representations
using RNN encoderdecoder for statistical machine transla
tion In EMNLP  2014
4 J Donahue Y  Jia O Vinyals J Hoffman N Zhang
E Tzeng and T Darrell Decaf A deep convolutional acti
vation feature for generic visual recognition In ICML  2014
5 D Elliott and F Keller Image description using visual de
pendency representations In EMNLP  2013
6 A Farhadi M Hejrati M A Sadeghi P Young
C Rashtchian J Hockenmaier and D Forsyth Every pic
ture tells a story Generating sentences from images In
ECCV  2010
7 R Gerber and HH Nagel Knowledge representation for
the generation of quantiﬁed natural language descriptions of
vehicle trafﬁc in image sequences In ICIP  IEEE 1996
8 Y  Gong L Wang M Hodosh J Hockenmaier and
S Lazebnik Improving imagesentence embeddings using
large weakly annotated photo collections In ECCV  2014
9 A Graves Generating sequences with recurrent neural net
works arXiv13080850  2013
10 S Hochreiter and J Schmidhuber Long shortterm memory
Neural Computation  98 1997
11 M Hodosh P Young and J Hockenmaier Framing image
description as a ranking task Data models and evaluation
metrics JAIR  47 2013
12 S Ioffe and C Szegedy Batch normalization Accelerating
deep network training by reducing internal covariate shift In
arXiv150203167  2015
13 A Karpathy A Joulin and L FeiFei Deep fragment em
beddings for bidirectional image sentence mapping NIPS 
2014
14 R Kiros R Salakhutdinov and R S Zemel Unifying
visualsemantic embeddings with multimodal neural lan
guage models In arXiv14112539  2014
15 R Kiros and R Z R Salakhutdinov Multimodal neural lan
guage models In NIPS Deep Learning Workshop  2013
16 G Kulkarni V  Premraj S Dhar S Li Y  Choi A C Berg
and T L Berg Baby talk Understanding and generating
simple image descriptions In CVPR  2011
17 P Kuznetsova V  Ordonez A C Berg T L Berg and
Y  Choi Collective generation of natural image descriptions
InACL 201218 P Kuznetsova V  Ordonez T Berg and Y  Choi Treetalk
Composition and compression of trees for image descrip
tions ACL 210 2014
19 S Li G Kulkarni T L Berg A C Berg and Y  Choi Com
posing simple image descriptions using webscale ngrams
InConference on Computational Natural Language Learn
ing 2011
20 TY  Lin M Maire S Belongie J Hays P Perona D Ra
manan P Doll ar and C L Zitnick Microsoft coco Com
mon objects in context arXiv14050312  2014
21 J Mao W Xu Y  Yang J Wang and A Yuille Ex
plain images with multimodal recurrent neural networks In
arXiv14101090  2014
22 T Mikolov K Chen G Corrado and J Dean Efﬁcient
estimation of word representations in vector space In ICLR 
2013
23 M Mitchell X Han J Dodge A Mensch A Goyal A C
Berg K Yamaguchi T L Berg K Stratos and H D III
Midge Generating image descriptions from computer vision
detections In EACL  2012
24 V  Ordonez G Kulkarni and T L Berg Im2text Describ
ing images using 1 million captioned photographs In NIPS 
2011
25 K Papineni S Roukos T Ward and W J Zhu BLEU A
method for automatic evaluation of machine translation In
ACL 2002
26 C Rashtchian P Young M Hodosh and J Hockenmaier
Collecting image annotations using amazons mechanical
turk In NAACL HLT Workshop on Creating Speech and
Language Data with Amazons Mechanical Turk  pages 139
147 2010
27 O Russakovsky J Deng H Su J Krause S Satheesh
S Ma Z Huang A Karpathy A Khosla M Bernstein
A C Berg and L FeiFei ImageNet Large Scale Visual
Recognition Challenge 2014
28 P Sermanet D Eigen X Zhang M Mathieu R Fergus
and Y  LeCun Overfeat Integrated recognition localization
and detection using convolutional networks arXiv preprint
arXiv13126229  2013
29 R Socher A Karpathy Q V  Le C Manning and A Y  Ng
Grounded compositional semantics for ﬁnding and describ
ing images with sentences In ACL 2014
30 I Sutskever O Vinyals and Q V  Le Sequence to sequence
learning with neural networks In NIPS  2014
31 R Vedantam C L Zitnick and D Parikh CIDEr
Consensusbased image description evaluation In
arXiv14115726  2015
32 B Z Yao X Yang L Lin M W Lee and SC Zhu I2t
Image parsing to text description Proceedings of the IEEE 
988 2010
33 P Young A Lai M Hodosh and J Hockenmaier From im
age descriptions to visual denotations New similarity met
rics for semantic inference over event descriptions In ACL
2014
34 W Zaremba I Sutskever and O Vinyals Recurrent neural
network regularization In arXiv14092329  2014
  Credit Card Fraud Detection Using Autoencoder 
Neural Network  
 
Ping Jiang MEng  Jinliang Zhang MEng  Junyi Zou MEng  
Department of Electrical  Computer 
Engineer  Department of Electrical  Computer 
Engineer  Department of Electrical  Computer 
Engineer  
University of Western Ontario  University of Western Ontario  University of Western Ontario  
pjiang28uwoca  jzhan964uwoca  jzou44uwoca  
250985261  250919668  250833154  
 
AbstractImbalanced data classification problem 
has always been a popular topic  in the field of machine 
learning research  In order to balance the samples 
between majority and minority class  Oversampling  
algorithm is used to synthesiz e new minority class 
samples  but it could bring in noise  Pointing to the 
noise proble ms this paper  proposed a denoising 
autoencoder  neural network DAE algorithm which 
can not only oversample minority class sample  through 
misclassification cost but it can denoise and classify  the 
sampled dataset Through  experiment s compared with 
the denoising  autoencoder neural network  DAE with 
oversampling process  and traditional fully connected 
neural networks  the results showed  the proposed 
algorithm improves the  classification accuracy of 
minority class of imbalanced  datasets  
Keywords imbalanc ed data oversampling  denoising 
autoencoder neural network classificatio n 
 
I INTRODUCTION  
Credit card fraud is a growing threat  with far 
reaching consequences in the finance industry 
corpor ation s and government Fraud can be defined as 
criminal deception with intent  of acquiring financial 
gain As credit card became the most popular method 
of payment for both online and offline transaction the 
fraud rate also accelerates The main reasons for fraud 
is due to the lack of security which involves the use of 
stolen credit card to get cash from bank through 
legitimate access T his results in high difficulty of 
preventing credit card fraud  
So how to do fraud detection is very significant A 
lot of researches have been proposed to the detection of 
such credit card fraud which account for majority of 
credit card frauds Detecting using traditional method is infeasible because of the big data However 
financial institutions have focused their attention to 
recent computational methodologies to handle c redit 
card fraud problem  
Classification problem is one of the key research 
topics in the field of machine learning Currently 
available classification methods can only achieve 
preferable performance on balanced datasets 
However there are a large number of imbalanced 
datasets in pract ical application For the fraud problem 
the minority class which is the abnormal transaction 
is more important  1 For instance when minority 
class accounts for less than 1 percent of the total 
dataset the overall accur acy reaches more than 99 
even though all the minority class has been 
misclassified  
Minority class sampling is a common method to 
handle with the imbalanced data classification 
problem The main purpose of oversampling is to 
increase the number of minori ty class samples so that 
the original classification information can get better 
retention Therefore in the fields where there is higher 
demand for the classification accuracy oversampling 
algorithm is chosen in general  
This paper seeks to implement cre dit card fraud 
detection using denoising autoencoder and 
oversampling For imbalanced data we decided use 
above method to achieve proper model  
 
II RELATED  WORKS  
 Data mining technique is one notable methods used 
in solving fraud detection problem This is the process 
of identifying those transactions  that are belong to 
frauds or not which is based on the behaviors and 
habits of cardholder many techniques have been applied to this area artificial neural network  2 
genetic algorithm support vector ma chine frequent 
item set mining decision tree migrating birds 
optimization algorithm Naïve Bayes A comparative 
analy sis of logistic regression and Naïve Bayes is 
carried out in 3  The performance of  Bayesian and 
neural network 4  is evaluated on cre dit card fraud 
data Decision tree neural networks and logistic 
regression are tested for their applicability  in fraud 
detections 5   
 In a seminar work  6 propose s two advanced data 
mining approaches support vector machines and 
random forests toget her with logistic regression as 
part of an attempt to better detect credit card fraud 
while neural network and logistic regression is applied 
on credit card fraud d etection problem 7  A number 
of challenges are associated with credit card detection 
namely fraudulent behavior profile is dynamic that is 
fraudulent transactions tend to look like legitimate 
ones credit card transaction datasets are rarely 
available and highly imbalanced or skewed optimal 
feature variables selection for the models su itable 
metric to evaluate performance of techniques on 
skewed credit card fraud data Credit card fraud 
detection performance is greatly affected by type of 
sampling approach used selection of variables and 
detection techniques used  
 
III BACKGROUND  
31 Autoencoder  
A Traditional Autoencoder Neural Network AE   
 Autoencoder is an artificial neural network used for 
unsupervised learning The aim of autoencoder is to 
learn representations to reconstructs features for a set 
of data typically for the pur pose of dimensionality 
reduction The simplest form of an autoencoder is a 
feedforward non recurrent neural network which is 
similar to the  multilayer perceptron  8 As the figure 1 
shown i t has 2 parts one is encoder and the other is 
decoder which are  consist of by an input layer one or 
more hidden layers and an output layer The significant 
difference between autoencoder and multiplayer 
perceptron is that the output layer of autoencoder has 
the same number of neurons as the input layer The 
purpose i s to reconstruct  its own inputs instead of 
predicting the target value from the  given inputs   
Fig 1  architecture of autoencoder neural network  
 In autoencoder the network structure has 
connections between layers but has no connection 
inside each laye r 𝑥𝑖 is input sample 𝑥𝑖 is output 
feature  
 The training of autoencoder neural network is to 
optimize reconstruction error using the given samples 
The cost function of autoencoder neural network 
defined in the project is 1  
𝐽𝐴𝐸 1
𝑚  1
2  𝑥𝑖𝑥𝑖2𝑚
𝑖1               1 
where m represents number of input samples  
B Denoising Autoencoder Neural Network DAE  
 For human when people  see an object if there is a 
small part of the object is blocked they can still 
recognize it  But how the autoenc oder does for the 
contaminated data  There is a variation of traditional 
autoencoder named denoising autoencoder which 
could m ake autoencoder neural network learn  how to 
remove the noise and  reconstruct undisturbed input  as 
much as possible  9 
 As show n in figure  2 the original data is  x and 𝑥 
is the data corrupted with  noise Through the complete 
process of denoising autoencoder the output is 𝑥 The 
loss function tr ies to minimize the difference between 
the output and the original data so that the  autoencoder 
has the ability of eliminating the influence of noise and 
extracting features from the corrupted data Therefore 
the features generated from  the learning of input 
corrupted with noise are more robust  which improved 
the data generalization ab ility of  autoencoder neural 
network model to input data   
 
Fig 2  Denoising autoencoder neural network  
The commonly used noises are Gaussian noise 
and Salt  and pepper noise And the  cost function of 
denoising autoencoder  neural network is defined 
according to  2 
𝐽𝐷𝐴𝐸 1
𝑚  1
2  𝑥𝑖𝑥𝑖2𝑚
𝑖1            2 
where 𝑥  𝑓𝑤𝑥𝑏 w represents weights 
and b represents bias  
32 Oversampling  
Imbalanced data set is a common problem faced in 
machine learning s ince most traditional machine 
learning classification model  cant handle imbalanced 
dataset  High misclassification cost often happened on 
minority class because classification model will try to 
classify all the data sample to the majority class  
Oversamplin g is a technique used to deal with 
imbalanced dataset its subject to create specific class 
sample so the class distribution of the original dataset 
can be balanced  The benefit of using o versampling is 
shown in figure 3  
 
Fig 3  Benefit of using oversamp ling 
SMOTE Synthetic Minority Oversampling 
Technique is one of the most popular oversampling 
technique In order to create a synthetic data point first 
we need to find a k nearest neighbors cluster in the 
feature space then randomly find a point within  this 
cluster finally using weighted average to forge the 
new data point  
33 Classification f ully connected  model  
Deep fully connected neural network is often used 
in classification problem with SoftMax cross entropy 
as the loss function deep learnin g classification model 
can achieve very high accuracy  
The SoftMax function is often used in the final 
layer of a neural network based classifier it first 
calculate s the exponential value of each output then 
normalize all the output and let the sum of th e output 
equal to 1 SoftMax function is often used for 
probability distribution transformation since the 
output of SoftMax function is within  range 0 to 1 that 
add up to 1 shown in the formula 3  
      P𝑦𝑖𝑥𝑖W𝑒𝑓𝑦𝑖
𝑒𝑓𝑗𝑗                      3 Entropy is a measure for information contents and 
could be defined as the unpredictability of an event So 
the greater the probability is the smaller the 
unpredictability is which means the information 
contents is also very small If an event occurs inevitably 
with the probability of 100 then the unpredictability 
and information content are 0 cross entropy loss 
function takes advantages of feature of entropy 
equation cross entropy loss function can measure the 
good ness of a classification mo del w hich is shown in 
formula 4  
Jθ1
𝑚  1𝑦𝑖𝑗𝑙𝑜𝑔𝑒𝜃𝑗𝑇𝑥𝑖
 𝑒𝜃𝑗𝑇𝑥𝑖 𝑘
𝑖1𝑘
𝑗1𝑚
𝑖1    4 
Cross entropy can be used in multi classification 
problems with the combination of SoftMax do not 
consider regularization Compared with quadratic loss 
function cross entropy loss function gives better 
training performance on neural networks  
34 Model evaluation  metric  
Accuracy is not sufficient to evaluate a 
classification model especially for imbalanced dataset 
For example an imbalanced dataset with 999 of 
normal data and 01 of abnormal data if the 
classification labels all the sample as normal class the 
model can still achieve 999 accuracy However for 
anomaly detection the detection rate of anomaly class  
is very important Confusion m atrix is often used in this 
situation  
Table 1  Confusion matrix for two class problem  
 
Recall Detection rate is the ratio between the 
number of correctly detected anomalies and the total 
number of anomalies it evaluates how much of the 
anomalies can be detected in this classification model  
 
IV METHODOLOGY  
The credit card fraud transaction dataset we are 
using is downloaded from Kaggle with totally 28315 
transaction detail and 05 of them are labeled as 
fraud  the dataset is shown in the fig 4  The  subject is 
to build a classification model for anomaly detection 
Dataset  contains only numerical input after doing PCA 
transformation Features V1 V2  V28 are the 
principal components the only features which have not 
been transformed with PCA are  Time and Amount 
Feature Class is the response variable and it takes 
value 1 in case of fraud and 0 otherwise  
 
Fig 4 Relationship between two classes  
The idea is very straight forward First use 
oversampling to transform imbalanced dataset to 
balanced dataset Then use denoised autoencoder to get 
denoised dataset Finally using deep fully connected 
neural network model for final classification  
 
Fig 5 Flowchart of the porcess  
41 Data Preprocessing  
For dataset preprocessing drop TIME data and 
normalized the AMOUNT part Other features are 
obtained by PCA do not need to do normalization 
Then choose the test sample which account for 20 of 
the total sample  
42 Oversampling  
Our group only perform oversampling on the 
training dataset Before oversampling there are total 
22652 transaction records in training dataset with 
22538 samples in normal class and 114 samples in 
abnormal class After over sampling the training 
dataset contains 22538 samples in normal class and 
22538 samples in abnormal class  
43 Denoising  autoencoder  
Our group designed a 7 layers  autoencoder for 
dataset denoising  process  After we got balanced 
training dat aset from oversa mpling we add G aussian 
noise to the training dataset then feed the training 
dataset into this denoised autoencoder After training 
this denoised autoencoder model this autoencoder has 
the capability to denoise the testing dataset in the 
prediction proce ss Table 2  Model design for denoised autoencoder  
Dataset with noise 29  
Fully Connected Layer 22  
Fully Connected Layer  15 
Fully Connected Layer  10 
Fully Connected Layer  15 
Fully Connected Layer  22 
Fully Connected Layer  29 
Square Loss  Function  
44 Classifier  
Our group designed a 6 layers autoencoder for 
dataset denoise process  After we got denoised training 
dataset from denoised autoencoder we feed the 
training dataset into this deep fully connected neural 
network classifier In the  end we are using SoftMax 
with cross entropy as the loss function for final 
classification  
Table 3  Model design for classifier  
Denoised Dataset 29  
Fully Connected Layer 22  
Fully Connected Layer 15  
Fully Connected Layer 10  
Fully Connected Layer 5  
Fully Connected Layer 2  
SoftMax Cross Entropy Loss Function  
 
V EVALUATION  AND  RESULTS  
This section first discusses the implementation 
details then presents evaluation results comparing the 
oversampling model with model with out 
oversampling  
51 Implementation details  
Our group using built in function from sklearn 
package for dataset normalization and built in function 
SMOTE from imblearn package for oversampling 
In addition we implement the denoised autoencoder 
mode l and deep fully connected neural network 
classifier with TensorFlow  We choose 
TensorFlow because its capable of GPU 
acceleration All models are trained on GTX 1060 
discrete GPU w6GB GDDR5 graphics memory It 
took 10 minutes for each model to conver ge 
52 Results  
After the training process we perform evaluation 
process using another separated evaluation dataset the 
accuracy rate and recall rate are applied to evaluate the 
accuracy of each model Th e results are shown in the 
fig 6 and fig 7 
 
Fig 6 Result for model 1  
 
Fig 7 Result for model 2  
For model 1 without the usage of oversampling and 
autoencoder the recall rate is very low because the 
model classifies all the sample as normal which means 
most fraud transaction is not detected For model 2 with 
oversampling and autoencoder  the recall rate is 
acceptable which means most fraud transaction can be 
detected Some evaluation result  of model 2 is showed 
in Table 4  
Table 4  Model 2 Evaluation Result  
Threshold  Recall Rate  Accuracy  
02 9066  8356  
03 8933  9093  04 88 9458  
05 8666  9673  
06 84 9793  
 
VI CONCLUSION  
In machine learning area imbalance data 
classification receives increasing attention as big data 
become popular On account of the drawbacks of 
traditional method oversampling algorithm and 
autoencoder can be used  This study combined stacked 
denoising autoencoder neural network  with 
oversampling to build the model which can achieve 
minority class sampling on the basis of 
misclassification cost and denoise and classify the 
sampled datasets The proposed algorithm increases  
classification accuracy of minority class  compared to 
the former methods we can achieve different accuracy 
by controlling the threshold In this study when 
threshold equal to 06 we can achieve the best 
performance which is 9793 However t he 
dimensionality reduction of high dimensional data still 
need to be further researched  
 
REFERENCES  
1 Y Sahin S Bulkan and E Duman A cost sensitive decision 
tree approach for fraud detection Expert Systems with 
Applicationsvol 40 pp 5916 5923 2013  
2 Ogwueleka F N 2011 Data Mining Application in Credit  
Card Fraud Detection System Journal of Engineering Science  and 
Technology Vol 6 No 3 pp 311  322 
3 Ng A Y and Jordan M I 2002 On discriminative vs 
generative c lassifiers A comparison of logistic regression and naive 
bayes Advances in neural information processing systems 2 841 
848 
4 Maes S Tuyls K Vanschoenwinkel B  Manderick B 
2002 Credit card fraud detection using Bayesian and neural 
netw orks In Proceedings of the 1st international naiso congress on 
neuro fuzzy technologies pp 261 270  
5 Shen A Tong R  Deng Y 2007 Application of  
classification models on credit card fraud detection In Service 
Systems and Service Managemen t 2007 International Conference 
on pp 1 4 IEEE  
6 Bhattacharyya S Jha S Tharakunnel K  Westland J C 
2011 Data mining for credit card fraud A comparative study 
Decision Support Systems 503 602 613 
7 Sahin Y and Duman E 2011 Detecting credit card fraud by 
ANN and logistic regression In Innovations in Intelligent Systems 
and Applications INISTA 2011 International Symposium on pp 
315319 IEEE  
8 Autoencoder for Words Liou C Y Cheng C W Liou J W 
and Liou D R Neurocomputing Volume 139 84 96 2014 
doi101016jneucom201309055  
9 M Koziarski and M Wożniak CCR A combined cleaning and 
resampling algorithm for imbalanced data classification 
International Journal of Applied Mathematics a nd Computer 
Science vol 27 no 4 2017  
 

  A Neural Representation of Sketch Drawings
David Ha
Google Brain
hadavidgooglecomDouglas Eck
Google Brain
deckgooglecom
Abstract
We present sketchrnn  a recurrent neural network RNN able to construct
strokebased drawings of common objects The model is trained on a dataset of
humandrawn images representing many different classes We outline a framework
for conditional and unconditional sketch generation and describe new robust
training methods for generating coherent sketch drawings in a vector format
1 Introduction
Recently there have been major advancements in generative modelling of images using neural
networks as a generative tool Generative Adversarial Networks GANs  5 Variational Inference
VI  15 and Autoregressive AR  19 models have become popular tools in this fast growing area
Most of the work thus far has been targeted towards modelling low resolution pixel images Humans
however do not understand the world as a grid of pixels but rather develop abstract concepts to
represent what we see From a young age we develop the ability to communicate what we see
by drawing on paper with a pencil or crayon In this way we learn to express a sequential vector
representation of an image as a short sequence of strokes In this paper we investigate an alternative
to traditional pixel image modelling approaches and propose a generative model for vector images
Figure 1 Latent space interpolation of various vector images produced by our model
Our goal is to train machines to draw and generalize abstract concepts in a manner similar to humans
As a ﬁrst step towards this goal we train our model on a dataset of handdrawn sketches each
represented as a sequence of motor actions controlling a pen which direction to move when to lift
the pen up and when to stop drawing In doing so we created a model that potentially has many
applications from assisting the creative process of an artist to helping teach students how to draw
This paper makes the following contributions We outline a framework for both unconditional and
conditional generation of vector images composed of a sequence of lines Our recurrent neural
networkbased generative model is capable of producing sketches of common objects in a vector
format We develop a training procedure unique to vector images to make the training more robust In
arXiv170403477v4  csNE  19 May 2017the conditional generation model we explore the latent space developed by the model to represent a
vector image We also discuss potential creative applications of our methodology We make available
a large dataset of hand drawn vector images to encourage further development of generative modelling
for vector images and also release an implementation of our model as an open source project1
2 Related Work
There is a long history of work related to algorithms that mimic painters One such work is Portrait
Drawing by Paul the Robot  2325 where an underlying algorithm controlling a mechanical robot
arm sketches lines on a canvas with a programmable artistic style to mimic a given digitized portrait
of a person Reinforcement Learning basedapproaches  25 have been developed to discover a set of
paint brush strokes that can best represent a given input photograph These prior works generally
attempt to mimic digitized photographs rather than develop generative models of vector images
Neural Networkbased approaches have been developed for generative models of images although
the majority of neural networkrelated research on image generation deal with pixel images  510
12141924 There has been relatively little work done on vector image generation using neural
networks An earlier work  22 makes use of Hidden Markov Models to synthesize lines and curves
of a human sketch More recent work  6 on handwriting generation with Recurrent Neural Networks
laid the groundwork for utilizing Mixture Density Networks  1 to generate continuous data points
Recent works of this approach attempted to generate vectorized Kanji characters unconditionally  7
and conditionally 26 by modelling Chinese characters as a sequence of pen stroke actions
In addition to unconditionally generating sketches we also explore encoding existing sketches
into a latent space of embedding vectors Previous work  2 outlined a methodology to combine
SequencetoSequence models with a Variational Autoencoder to model natural English sentences in
latent vector space A related work 16 utilizes probabilistic program induction rather than neural
networks to perform oneshot modelling of the Omniglot dataset containing images of symbols
One of the factors limiting research development in the space of generative vector drawings is the
lack of publicly available datasets Previously the Sketch dataset  4 consisting of 20K vector
sketches was used to explore feature extraction techniques A subsequent work the Sketchy dataset
20 provided 70K vector sketches along with corresponding pixel images for various classes This
allowed for a largerscale exploration of human sketches ShadowDraw  17 is an interactive system
that predicts what a ﬁnished drawing looks like based on a set of incomplete brush strokes from the
user while the sketch is being drawn ShadowDraw used a dataset of 30K raster images combined
with extracted vectorized features In this work we use a much larger dataset of vector sketches that
is made publicly available
3 Methodology
31 Dataset
We constructed QuickDraw  a dataset of vector drawings obtained from Quick Draw 11 an online
game where the players are asked to draw objects belonging to a particular object class in less than 20
seconds QuickDraw consists of hundreds of classes of common objects Each class of QuickDraw
is a dataset of 70K training samples in addition to 25K validation and 25K test samples
We use a data format that represents a sketch as a set of pen stroke actions This representation is an
extension of the format used in  6 Our format extends the binary pen stroke event into a multistate
event In this data format the initial absolute coordinate of the drawing is located at the origin A
sketch is a list of points and each point is a vector consisting of 5 elements xyp1p2p3
The ﬁrst two elements are the offset distance in the x and y directions of the pen from the previous
point The last 3 elements represents a binary onehot vector of 3 possible states The ﬁrst pen state
p1 indicates that the pen is currently touching the paper and that a line will be drawn connecting the
next point with the current point The second pen state p2 indicates that the pen will be lifted from
the paper after the current point and that no line will be drawn next The ﬁnal pen state p3 indicates
that the drawing has ended and subsequent points including the current point will not be rendered
1The code and dataset is available at httpsmagentatensorfloworgsketch_rnn 
232 SketchRNN
Figure 2 Schematic diagram of sketchrnn 
Our model is a SequencetoSequence Variational Autoencoder V AE similar to the architecture
described in  215 Our encoder is a bidirectional RNN  21 that takes in a sketch as an input and
outputs a latent vector of size Nz Speciﬁcally we feed the sketch sequence S and also the same
sketch sequence in reverse order Sreverse  into two encoding RNNs that make up the bidirectional
RNN to obtain two ﬁnal hidden states
hencodeS hencodeSreverse  h hh 1
We take this ﬁnal concatenated hidden state h and project it into two vectors µandˆσ each of size
Nz using a fully connected layer We convert ˆσinto a nonnegative standard deviation parameter
σusing an exponential operation We use µandσ along withN0I a vector of IID Gaussian
variables of size Nz to construct a random vector zRNz as in the approach for a V AE 15
µWµhbµˆσWσhbσ σ expparenleftBigˆσ
2parenrightBig
 zµσN0I 2
Under this encoding scheme the latent vector zis not a deterministic output for a given input sketch
but a random vector conditioned on the input sketch
Our decoder is an autoregressive RNN that samples output sketches conditional on a given latent
vectorz The initial hidden states h0 and optional cell states c0if applicable of the decoder RNN is
the output of a single layer network h0c0  tanhWzzbz
At each step iof the decoder RNN we feed the previous point Si1and the latent vector zin as
a concatenated input xi whereS0is deﬁned as 00100 The output at each time step are the
parameters for a probability distribution of the next data point Si In Equation 3 we model xy
as a Gaussian mixture model GMM with Mnormal distributions as in  16 and q1q2q3as
a categorical distribution to model the ground truth data p1p2p3 where q1q2q3 1 as
done in  7 and  26 Unlike  6 our generated sequence is conditioned from a latent code zsampled
from our encoder which is trained endtoend alongside the decoder
pxy Msummationdisplay
j1ΠjNxyµxjµyjσxjσyjρxyjwhereMsummationdisplay
j1Πj 1 3
Nxyµxµyσxσyρxyis the probability distribution function for a bivariate normal distribution
Each of theMbivariate normal distributions consist of ﬁve parameters µxµyσxσyρxy where
µxandµyare the means σxandσyare the standard deviations and ρxyis the correlation parameter of
each bivariate normal distribution An additional vector Πof lengthM also a categorical distribution
are the mixture weights of the Gaussian mixture model Hence the size of the output vector yis
5MM 3 which includes the 3 logits needed to generate q1q2q3
The next hidden state of the RNN generated with its forward operation projects into the output
vectoryiusing a fullyconnected layer
xi Si1zhici forward xihi1ci1 yiWyhiby yiR6M34
The vectoryiis broken down into the parameters of the probability distribution of the next data point
 ˆΠ1µxµyˆσxˆσyˆρxy1ˆΠ1µxµyˆσxˆσyˆρxyMˆq1ˆq2ˆq3  yi 5
3As in  6 we apply expandtanh operations to ensure the standard deviation values are nonnegative
and that the correlation value is between 1 and 1
σx expˆσx σy expˆσy ρxy tanhˆρxy 6
The probabilities for the categorical distributions are calculated using the outputs as logit values
qkexpˆqksummationtext3
j1expˆqjk123ΠkexpˆΠksummationtextM
j1expˆΠjk1  M 7
A key challenge is to train our model to know when to stop drawing Because the probabilities of
the three pen stroke events are highly unbalanced the model becomes more difﬁcult to train The
probability of a p1event is much higher than p2 and thep3event will only happen once per drawing
The approach developed in  7 and later followed by  26 was to use different weightings for each
pen event when calculating the losses such as a handtuned weighting of 110100  We ﬁnd this
approach to be inelegant and inadequate for our dataset of diverse image classes
We develop a simpler more robust approach that works well for a broad class of sketch drawing data
In our approach all sequences are generated to a length of NmaxwhereNmaxis the length of the
longest sketch in our training dataset In principle Nmaxcan be considered a hyper parameter As the
length ofSis usually shorter than Nmax we setSito be 00001foriNs We discuss the
training in detail in the next section
After training we can sample sketches from our model During the sampling process we generate
the parameters for both GMM and categorical distributions at each time step and sample an outcome
Sprime
ifor that time step Unlike the training process we feed the sampled outcome Sprime
ias input for the
next time step We continue to sample until p3 1 or when we have reached iNmax Like the
encoder the sampled output is not deterministic but a random sequence conditioned on the input
latent vector z We can control the level of randomness we would like our samples to have during the
sampling process by introducing a temperature parameter τ
ˆqkˆqk
τˆΠkˆΠk
τ σ2
xσ2
xτ σ2
yσ2
yτ 8
We can scale the softmax parameters of the categorial distribution and also the σparameters of the
bivariate normal distribution by a temperature parameter τ to control the level of randomness in
our samples τis typically set between 0 and 1 In the limiting case as τ0 our model becomes
deterministic and samples will consist of the most likely point in the probability density function
Figure 3 illustrates of effect of sampling sketches with various temperature parameters
33 Unconditional Generation
Figure 3 Unconditional generation of ﬁretrucks yoga poses gardens and owls with varying τ
As a special case we can also train our model to generate sketches unconditionally where we only
train the decoder RNN module without any input or latent vectors By removing the encoder the
decoder RNN as a standalone model is an autoregressive model without latent variables In this use
case the initial hidden states and cell states of the decoder RNN are initialized to zero The inputs xi
of the decoder RNN at each time step is only Si1orSprime
i1 as we do not need to concatenate a latent
vectorz In Figure 3 we sample various sketch images generated unconditionally by varying the
temperature parameter from τ 02at the top in blue to τ 09at the bottom in red
434 Training
Our training procedure follows the approach of the Variational Autoencoder  15 where the loss
function is the sum of two terms the Reconstruction Loss LR and the KullbackLeibler Divergence
LossLKL We train our model to optimize this twopart loss function The Reconstruction loss
term described in Equation 9 maximizes the loglikehood of the generated probability distribution
to explain the training data S We can calculate this reconstruction loss LR using the generated
parameters of the pdf and the training data SLRis composed of the sum of the log loss of the offset
terms xyLs and the log loss of the pen state terms p1p2p3Lp
Ls1
NmaxNssummationdisplay
i1logparenleftBigMsummationdisplay
j1ΠjiNxiyiµxjiµyjiσxjiσyjiρxyjiparenrightBig
Lp1
NmaxNmaxsummationdisplay
i13summationdisplay
k1pkilogqki LRLsLp9
Note that we discard the pdf parameters modelling the xypoints beyond Nswhen calculating
Ls whileLpis calculated using all of the pdf parameters modelling the p1p2p3points until
Nmax Both terms are normalized by the total sequence length Nmax We found this methodology of
loss calculation to be more robust and allows the model to easily learn when it should stop drawing
unlike the earlier mentioned method of assigning importance weightings to p1p2 andp3
The KullbackLeibler KL divergence loss term measures the difference between the distribution of
our latent vector z to that of an IID Gaussian vector with zero mean and unit variance Optimizing
for this loss term allows us to minimize this difference We use the result in  15 and calculate the
KL loss term LKL normalized by number of dimensions Nzof the latent vector
LKL1
2NzparenleftBig
1  ˆσµ2expˆσparenrightBig
10
The loss function in Equation 11 is a weighted sum of both the LRandLKLloss terms
Loss LRwKLLKL 11
There is a tradeoff between optimizing for one term over the other As wKL0 our model
approaches a pure autoencoder sacriﬁcing the ability to enforce a prior over our latent space while
obtaining better reconstruction loss metrics Note that for unconditional generation where our model
is the standalone decoder there will be no LKLterm as we only optimize for LR
Figure 4 Tradeoff between LRandLKL for two models trained on single class datasets left
Validation Loss Graph for models trained on the Yoga dataset using various wKL right
Figure 4 illustrates the tradeoff between different settings of wKLand the resulting LRandLKL
metrics on the test set along with the LRmetric on a standalone decoder RNN for comparison
As the unconditional model does not receive any prior information about the entire sketch it needs
to generate the LRmetric for the standalone decoder model serves as an upper bound for various
conditional models using a latent vector
4 Experiments
We conduct several experiments with sketchrnn for both conditional and unconditional vector
image generation We train sketchrnn on various QuickDraw classes using various settings for
5wKLand record the breakdown of losses To experiment with a diverse set of classes with varying
complexities we select the cat pig face ﬁretruck garden owl mosquito and yoga class We also
experiment on multiclass datasets by concatenating different classes together to form cat pig and
crab face pig rabbit The results for test set evaluation on various datasets are displayed in Table 1
Thesketchrnn model treats the RNN cell as an abstract component In our experiments we
use Long ShortTerm Memory LSTM  9 as the encoder RNN For the decoder RNN we use
HyperLSTM as this type of RNN cell excels at sequence generation tasks  8 The ability for
HyperLSTM to spontaneously augment its own weights enables it to adapt to many different regimes
in a large diverse dataset For model conﬁguration details please see the Supplementary Material
Dataset wKL 100 wKL 050 wKL 025 Decoder Only
LR LKL LR LKL LR LKL LR
cat 098 029 133 070 146 101 057
pig 114 022 137 049 152 080 082
cat pig 102 022 124 049 150 098 075
crab face pig rabbit 091 022 104 040 147 117 067
face 113 027 155 071 190 144 073
ﬁretruck 124 022 126 024 178 110 090
garden 079 020 081 025 099 054 062
owl 093 020 103 034 129 077 066
mosquito 067 030 102 066 141 154 034
yoga 080 024 107 055 151 133 048
Table 1 Loss ﬁgures  LRandLKL for various wKLsettings
The relative loss numbers are consistent with our expectations We see that the reconstruction loss
termLRdecreases as we relax the wKLparameter controlling the weight for the KL loss term and
meanwhile the KL loss term LRincreases as a result The LRfor the conditional model is strictly
less than the unconditional standalone decoder model In Figure 4 right we plot validationset loss
graphs for on the yoga class for models with various wKLsettings As LRdecreases the LKLterm
tends to increase due to the tradeoff between LRandLKL
41 Conditional Reconstruction
We qualitatively assess the reconstructed sketch Sprimegiven an input sketch S In Figure 5 left we
sample several reconstructions at various levels of temperature τusing a model trained on the single
cat class starting at 001 on the left and linearly increasing to 10 on the right The reconstructed cat
sketches have similar properties as the input image and occasionally add or remove details such as a
whisker a mouth a nose or the orientation of the tail
Figure 5 Conditional generation of cats left and pigs right
When presented with a nonstandard image of a cat such as a cats face with three eyes the
reconstructed cat only has two eyes If we input a sketch from another image class such a toothbrush
the model seemingly generate sketches with similar orientation and properties as the toothbrush
input image but with some catlike features such as cat ears whiskers or feet We perform a similar
experiment with a model trained on the pig class as shown in Figure 5 right
642 Latent Space Interpolation
By interpolating between latent vectors we can visualize how one image morphs into another image
by visualizing the reconstructions of the interpolations As we enforce a Gaussian prior on the latent
space we expect fewer gaps in the space between two encoded latent vectors We expect a model
trained using a higher wKLsetting to produce images that are closer to the data manifold given a
spherically interpolated 24 latent vector z compared to another model trained with a lower wKL
Figure 6 Latent space interpolation between cat and pig using with various wKLsettings left
Sketch Drawing Analogies right
To demonstrate this we train several models using various wKL on a dataset consisting of both cat
and pigs and we encode two distinct images from the test set  a cat face and a full pig Figure 6
left shows the reconstructed images from the interpolated latent vectors between the two original
images As expected models trained with higher wKLproduce more coherent interpolated images
43 Sketch Drawing Analogies
The interpolation example in Figure 6 left suggests that the latent vector zencode conceptual
features of a sketch Can we use these features to augment other sketches without such features  for
example adding a body to a cats head Indeed we ﬁnd that sketch drawing analogies are possible
for models trained with low LKLnumbers Given the smoothness of the latent space where any
interpolated vector between two latent vectors results in a coherent sketch we can perform vector
arithmetic on the latent vectors encoded from different sketches and explore how the model organizes
the latent space to represent different concepts in the manifold of generated sketches
For example as shown in Figure 6 right we can subtract the latent vector of an encoded pig head
from the latent vector of a full pig to arrive at a vector that represents a body Adding this difference
to the latent vector of a cat head results in a full cat ie cat head  body  full cat We repeat the
experiment to remove the body of a full pig These drawing analogies allow us to explore how the
model organizes its latent space to represent different concepts in the manifold of generated sketches
44 Predicting Different Endings of Incomplete Sketches
Figure 7 sketchrnn predicting possible endings of various incomplete sketches the red lines
We can use sketchrnn to ﬁnish an incomplete sketch By using the decoder RNN as a standalone
model we can generate a sketch that is conditioned on the previous points We use the decoder RNN
to ﬁrst encode an incomplete sketch into a hidden state h Afterwards we generate the remaining
points of the sketch using has the initial hidden state We show results in Figure 7 using decoderonly
models trained on individual classes and sample completions by setting τ 08
75 Applications and Future Work
We believe sketchrnn will enable many creative applications Even the decoderonly model trained
on various classes can assist the creative process of an artist by suggesting many possible ways of
ﬁnishing a sketch helping artists expand their imagination In the conditional model exploring the
latent space between different objects can potentially enable artists to ﬁnd interesting intersections
and relationships between different drawings Even in the simplest use pattern designers can apply
sketchrnn to generate a large number of similar but unique designs for textile or wallpaper prints
As we saw earlier in Section 41 a model trained to draw pigs can be made to draw piglike trucks if
given an input sketch of a truck We can extend this result to applications that might help creative
designers come up with abstract designs that can resonate more with their target audience For
instance in Figure 8 right we feed sketches of four different chairs into our catdrawing model to
produce four chairlike cats We can even interpolate between the four images to explore the latent
space of chairlike cats and select from a large grid of generated designs
Figure 8 Generating similar but unique sketches based on a single human sketch in the box left
Latent space of generated cats conditioned on sketch drawings of chairs right
A model trained on higher quality sketches may ﬁnd its way into educational applications that can
help teach students how to draw Even with the simple sketches in QuickDraw  the authors of this
work have become much more proﬁcient at drawing animals insects and various sea creatures after
conducting these experiments A related application is to encode a crude poorly sketched drawing
and generate more aesthetically looking reproductions by using a model trained with a high wKL
setting and sampling with a low temperature τto produce a more coherent version of the drawing In
the future we can also investigate augmenting the latent vector in the direction that maximizes the
aesthetics of the drawing by incorporating userrating data into the training process
Combining hybrid variations of sequencegeneration models with unsupervised crossdomain pixel
image generation models such as ImagetoImage models  31318 is another exciting direction
that we can explore We can already combine this model with supervised crossdomain models such
as Pix2Pix  10 to occasionally generate photo realistic cat images from generated sketches of cats
The opposite direction of converting a photograph of a cat into an unrealistic but similar looking
sketch of a cat composed of a minimal number of lines seems to be a more interesting problem
6 Conclusion
In this work we develop a methodology to model sketch drawings using recurrent neural networks
sketchrnn is able to generate possible ways to ﬁnish an existing but unﬁnished sketch drawing
Our model can also encode existing sketches into a latent vector and generate similar looking sketches
conditioned on the latent space We demonstrate what it means to interpolate between two different
sketches by interpolating between its latent space and also show that we can manipulate attributes
of a sketch by augmenting the latent space We demonstrate the importance of enforcing a prior
distribution on the latent vector for coherent vector image generation during interpolation By making
available a large dataset of sketch drawings we hope to encourage further research and development
in the area of generative vector image modelling
8References
1 C M Bishop Mixture density networks Technical Report  1994
2S R Bowman L Vilnis O Vinyals A M Dai R Józefowicz and S Bengio Generating Sentences from
a Continuous Space CoRR  abs151106349 2015
3H Dong P Neekhara C Wu and Y  Guo Unsupervised ImagetoImage Translation with Generative
Adversarial Networks ArXiv eprints  Jan 2017
4M Eitz J Hays and M Alexa How Do Humans Sketch Objects ACM Trans Graph Proc SIGGRAPH 
3144414410 2012
5 I Goodfellow NIPS 2016 Tutorial Generative Adversarial Networks ArXiv eprints  Dec 2017
6 A Graves Generating sequences with recurrent neural networks arXiv13080850  2013
7 D Ha Recurrent Net Dreams Up Fake Chinese Characters in Vector Format with TensorFlow 2015
8 D Ha A M Dai and Q V  Le HyperNetworks In ICLR  2017
9 S Hochreiter and J Schmidhuber Long shortterm memory Neural Computation  1997
10 P Isola JY  Zhu T Zhou and A A Efros ImagetoImage Translation with Conditional Adversarial
Networks ArXiv eprints  Nov 2016
11 J Jongejan H Rowley T Kawashima J Kim and N FoxGieg The Quick Draw  AI Experiment
httpsquickdrawwithgooglecom  2016
12 C Kaae Sønderby T Raiko L Maaløe S Kaae Sønderby and O Winther Ladder Variational Autoen
coders ArXiv eprints  Feb 2016
13 T Kim M Cha H Kim J Lee and J Kim Learning to Discover CrossDomain Relations with Generative
Adversarial Networks ArXiv eprints  Mar 2017
14 D P Kingma T Salimans and M Welling Improving variational inference with inverse autoregressive
ﬂow CoRR  abs160604934 2016
15 D P Kingma and M Welling AutoEncoding Variational Bayes ArXiv eprints  Dec 2013
16 B M Lake R Salakhutdinov and J B Tenenbaum Humanlevel concept learning through probabilistic
program induction Science  350626613321338 Dec 2015
17 Y  J Lee C L Zitnick and M F Cohen Shadowdraw Realtime user guidance for freehand drawing In
ACM SIGGRAPH 2011 Papers  SIGGRAPH 11 pages 2712710 New York NY  USA 2011 ACM
18 MY  Liu T Breuel and J Kautz Unsupervised ImagetoImage Translation Networks ArXiv eprints 
Mar 2017
19 S Reed A van den Oord N Kalchbrenner S Gómez Colmenarejo Z Wang D Belov and N de Freitas
Parallel Multiscale Autoregressive Density Estimation ArXiv eprints  Mar 2017
20 P Sangkloy N Burnell C Ham and J Hays The Sketchy Database Learning to Retrieve Badly Drawn
Bunnies ACM Trans Graph  354119111912 July 2016
21 M Schuster K K Paliwal and A General Bidirectional recurrent neural networks IEEE Transactions
on Signal Processing  1997
22 S Simhon and G Dudek Sketch interpretation and reﬁnement using statistical models In Proceedings of
the Fifteenth Eurographics Conference on Rendering Techniques  EGSR04 pages 2332 AirelaVille
Switzerland Switzerland 2004 Eurographics Association
23 P Tresset and F Fol Leymarie Portrait drawing by paul the robot Comput Graph  375348363 Aug
2013
24 T White Sampling Generative Networks ArXiv eprints  Sept 2016
25 N Xie H Hachiya and M Sugiyama Artist agent A reinforcement learning approach to automatic
stroke generation in oriental ink painting In ICML  icmlcc  Omnipress 2012
26 X Zhang F Yin Y  Zhang C Liu and Y  Bengio Drawing and Recognizing Chinese Characters with
Recurrent Neural Network CoRR  abs160606539 2016
9Supplementary Material for A Neural Representation of Sketch Drawings
1 Dataset Details
Figure 1 Example sketch drawings from QuickDraw dataset
The data from Quick Draw 4 expands daily and every so often new classes are added to the game
As such the QuickDraw dataset now consists of hundreds of classes from 75 classes initially in
Table 1 Each class consists of 70K training samples and 25K validation and test samples Stroke
simpliﬁcation using the RamerDouglasPeucker algorithm  3 with a parameter of epsilon1 20has been
applied to simplify the lines The data was originally recorded in pixeldimensions so we normalized
the offsets xyusing a single scaling factor This scaling factor was calculated to adjust the
offsets in the training set to have a standard deviation of 1 For simplicity we do not normalize the
offsets xyto have zero mean since the means are already relatively small
alarm clock ambulance angel ant barn basket bee
bicycle book bridge bulldozer bus butterﬂy cactus
castle cat chair couch crab cruise ship dolphin
duck elephant eye face fan ﬁre hydrant ﬁretruck
ﬂamingo ﬂower garden hand hedgehog helicopter kangaroo
key lighthouse lion map mermaid octopus owl
paintbrush palm tree parrot passport peas penguin pig
pineapple postcard power outlet rabbit radio rain rhinoceros
roller coaster sandwich scorpion sea turtle sheep skull snail
snowﬂake speedboat spider strawberry swan swing set tennis racquet
the mona lisa toothbrush truck whale windmill
Table 1 Initial 75 QuickDraw classes used for this work
Figure 2 below shows a training example before normalization of xycolumns
Figure 2 A sample sketch as a sequence of xyp1p2p3points and in rendered form
In the rendered sketch the line color corresponds to the sequential stroke ordering
12 Training Details
As a recap from the main text we deﬁned the Reconstruction loss term LRas
Ls1
NmaxNssummationdisplay
i1logparenleftBigMsummationdisplay
j1ΠjiNxiyiµxjiµyjiσxjiσyjiρxyjiparenrightBig
Lp1
NmaxNmaxsummationdisplay
i13summationdisplay
k1pkilogqki
LRLsLp1
We also deﬁned the KL loss term LKLas
LKL1
2NzparenleftBig
1  ˆσµ2expˆσparenrightBig
2
The loss function in Equation 3 is a weighted sum of both the LRandLKLloss terms
Loss LRwKLLKL 3
While the loss function in Equation 3 can be used during training we ﬁnd that annealing the KL term
in the loss function Equation 4 produced better results This modiﬁcation is only used for model
training and the original loss function in Equation 3 is still used to evaluate validation and test sets
and for early stopping
ηstep 11ηminRstep
Loss train LRwKLηstepmaxLKLKL min4
We ﬁnd that annealing the KL loss term generally results in better losses Annealing the LKL
term in the loss function directs the optimizer to ﬁrst focus more on the reconstruction term in
Equation 1 which is the more difﬁcult loss term of the model to optimize for before having to deal
with optimizing for the KL loss term in Equation 2 a far simpler expression in comparison This
approach has been used in  257 Our annealing term ηstepstarts atηmintypically 0 or 001 at
training step 0 and converges to 1 for large training steps Ris a term close to but less than 1
If the distribution of zis close enough to N0I we can sample sketches from the decoder using
randomly sampled zfromN0Ias the input In practice we ﬁnd that going from a larger LKL
value LKL10 to a smaller LKLvalue of03generally results in a substantial increase in the
quality of sampled images using randomly sampled zN0I However going from LKL 03
toLKLvalues closer to zero does not lead to any further noticeable improvements Hence we ﬁnd it
useful to put a ﬂoor on LKLin the loss function by enforcing maxLKLKL minin Equation 4
TheKL minterm inside the max operator is typically set to a small value such as 010 to 050 This
term will encourage the optimizer to put less focus on optimizing for the KL loss term LKLonce it is
low enough so we can obtain better metrics for the reconstruction loss term LR This approach is
similar to the approach described in  7 asfree bits  where they apply the max operator separately
inside each dimension of the latent vector z
3 Model Conﬁguration
Our encoder and decoder RNNs consist of 512 and 2048 nodes respectively In our model we use
M 20 mixture components for the decoder RNN The latent vector zhasNz 128 dimensions
We apply Layer Normalization  1 to our model and during training apply recurrent dropout  9 with
a keep probability of 90 We train the model with batch sizes of 100 samples using Adam  6 with
a learning rate of 00001 and gradient clipping of 10 All models are trained with KL min 020
R 099999  During training we perform simple data augmentation by multiplying the offset
columns xyby two IID random factors chosen uniformly between 090 and 110 Unless
mentioned otherwise all experiments are conducted with wKL 100
24 Model Limitations
Although sketchrnn can model a large variety of sketch drawings there are several limitations in
the current approach we wish to highlight For most singleclass datasets sketchrnn is capable
of modelling sketches up to around 300 data points The model becomes increasingly difﬁcult to
train beyond this length For our dataset we applied the RamerDouglasPeucker algorithm  3 to
simplify the strokes of the sketch data to less than 200 data points while still keeping most of the
important visual information of each sketch
Figure 3 Unconditional generated sketches of frogs cats and crabs at τ 08
For more complicated classes of images such as mermaids or lobsters the reconstruction loss metrics
are not as good compared to simpler classes such as ants faces or ﬁretrucks The models trained on
these more challenging image classes tend to draw smoother more circular line segments that do not
resemble individual sketches but rather resemble an averaging of many sketches in the training set
We can see some of this artifact in the frog class in Figure 3 This smoothness may be analogous
to the blurriness effect produced by a Variational Autoencoder  8 that is trained on pixel images
Depending on the use case of the model smooth circular lines can be viewed as aesthetically pleasing
and a desirable property
Figure 4 Unconditional generations from model trained on 75 classes left
From model trained on crab face pig and rabbit classes right
While both conditional and unconditional models are capable of training on datasets consisting of
several classes such as cat pig and crab face pig rabbit sketchrnn is ineffective at modelling
a large number of classes simultaneously In Figure 4 we sample sketches using an unconditional
model trained on 75 classes and a model trained on 4 classes The samples generated from the
75class model are incoherent with individual sketches displaying features from multiple classes
The fourclass unconditional model usually generates samples of a single class but occasionally also
combines features from multiple classes In the future we will explore incorporating class information
outside of the latent space to handle the modelling of a large number of classes simultaneously
35 MultiSketch Drawing Interpolation
Figure 5 Example of conditional generated sketches with single class models
Latent space interpolation from left to right and then top to bottom
In addition to interpolating between two sketches like in Figure 5 we can also visualize the
interpolation between four sketches in latent space to gain further insight from the model In this
section we show more examples conditionally generated with sketchrnn  We take four generated
images place them on four corners of a grid and populate the rest of the grid using the interpolation
of the latent vectors at the corners Figure 6 shows two examples of this fourway interpolation using
models trained on both cat pig classes and face class All samples generated with τ 01
Figure 6 Example input sketches and sketchrnn generated reproductions Top
Latent space interpolation between the four reproduced sketches Bottom
The left side of Figure 7 visualizes the interpolation between a full pig a rabbits head a crab and a
face using a model trained on these four classes In certain parts of the space between a crab and a
face is a rabbits head and we see that the ears of the rabbit becomes the crabs claws Applying the
model on the yoga class it is interesting to see how one yoga position slowly transitions to another
via a set of interpolated yoga positions generated by the model For visual effect we also interpolate
between four distinct colors and color each sketch using a unique interpolated color
4Figure 7 Interpolation of pig rabbit crab and face yoga poses mosquitoes and mermaids
We also interpolate between four distinct colors for visual effect
We also construct latent space interpolation examples for the mosquito class and the mermaid class
in the last two grids Figure 7 We see that the model can interpolate between concepts such as style
of wings leg counts and orientation In Figure 8 below we show more interpolation examples of
other classes from the dataset
Figure 8 Latent space interpolation between four generated gardens owls cats and ﬁretrucks
6 Which Loss Controls Image Coherency
We would like to question the relative importance of the reconstruction loss term LR relative to the
KL loss term LKL when our goal is to produce higher quality image reconstructions While our
reconstruction loss term LRoptimizes for the loglikelihood of the set of strokes that make up a
sketch this metric alone does not give us any guarantee that a model with a lower LRnumber will
produce higher quality reconstructions compared to a model with a higher LRnumber
For example imagine a simple sketch of an face smiley where most of the data points of Sare be used to
represent the head and only a minority of points represent facial features such as the eyes and mouth
It is possible to reconstruct the face with incoherent facial features and yet still score a lower LR
number compared to another reconstruction with a coherent and similar face if the edges around the
incoherent face are generated more precisely
In Figure 9 we compare the reconstructed images generated using models trained with various wKL
settings In the ﬁrst three examples from the left we train our model on a dataset consisting of four
image classes crab face pig rabbit We deliberately sketch input drawings that contain features of
two classes such as a rabbit with a pig mouth and pig tail a person with animal ears and a rabbit with
crab claws We see that the model trained using higher wKLweights tend to generate sketches with
features of a single class that look more coherent despite having lower LKLnumbers For instance
the model with wKL 100omit pig features animal ears and crab claws from its reconstructions
In contrast the model with wKL 025 with higher LKL but lowerLRnumbers tries to keep both
inconsistent features while generating sketches that look less coherent
In the last three examples in Figure 9 we repeat the experiment on models trained on singleclass
images and see similar results even when we deliberately choose input samples from the test set with
noisier lines
If we look at the interpolations produced in the Latent Space Interpolation section in the main text
models with better KL loss terms also generate more meaningful reconstructions from the interpolated
5Figure 9 Reconstructions of sketch images using models with various wKLsettings
space between two latent vectors This suggests the latent vector for models with lower LKLcontrol
more meaningful parts of the drawings such as controlling whether the sketch is an animal head
only or a full animal with a body or whether to draw a cat head or a pig head Altering such latent
vectors can allow us to directly manipulate these animal features Conversely altering the latent
codes of models with higher LKLresults in scattered movement of individual line segments rather
than alterations of meaningful conceptual features of the animal
This result is consistent with incoherent reconstructions seen in Figure 9 With a lower LKL the
model is likely to generate coherent images given any random z Even with a nonstandard or noisy
input image the model will still encode a zthat produces coherent images For models with lower
LKLnumbers the encoded latent vectors contain conceptual features belonging to the input image
while for models with higher LKLnumbers the latent vectors merely encode information about
speciﬁc line segments This observation suggests that when using sketchrnn on a new dataset we
should ﬁrst try different wKLsettings to evaluate the tradeoff between LRandLKL and then choose
a setting for wKLandKL min that best suit our requirements
7 Acknowledgements
We thank Ian Johnson Jonas Jongejan Martin Wattenberg Mike Schuster Thomas Deselaers
Ben Poole Kyle Kastner Junyoung Chung and Kyle McDonald for their help with this project This
work was done as part of the Google Brain Residency program  gcobrainresidency 
References
1 J L Ba J R Kiros and G E Hinton Layer normalization NIPS  2016
2S R Bowman L Vilnis O Vinyals A M Dai R Józefowicz and S Bengio Generating Sentences from
a Continuous Space CoRR  abs151106349 2015
3D H Douglas and T K Peucker Algorithms for the reduction of the number of points required to represent
a digitized line or its caricature Cartographica The International Journal for Geographic Information and
Geovisualization  102112122 Oct 1973
4J Jongejan H Rowley T Kawashima J Kim and N FoxGieg The Quick Draw  AI Experiment
httpsquickdrawwithgooglecom  2016
5C Kaae Sønderby T Raiko L Maaløe S Kaae Sønderby and O Winther Ladder Variational Autoencoders
ArXiv eprints  Feb 2016
6 D Kingma and J Ba Adam A method for stochastic optimization In ICLR  2015
7D P Kingma T Salimans and M Welling Improving variational inference with inverse autoregressive
ﬂow CoRR  abs160604934 2016
8 D P Kingma and M Welling AutoEncoding Variational Bayes ArXiv eprints  Dec 2013
9S Semeniuta A Severyn and E Barth Recurrent dropout without memory loss arXiv160305118  2016
6
  CS294A Lecture notes
Andrew Ng
Sparse autoencoder
1 Introduction
Supervised learning is one of the most powerful tools of AI and has led to
automatic zip code recognition speech recognition selfdriving cars and a
continually improving understanding of the human genome Despite its sig
nicant successes supervised learning today is still severely limited Speci
cally most applications of it still require that we manually specify the input
featuresxgiven to the algorithm Once a good feature representation is
given a supervised learning algorithm can do well But in such domains as
computer vision audio processing and natural language processing therere
now hundreds or perhaps thousands of researchers whove spent years of their
lives slowly and laboriously handengineering vision audio or text features
While much of this featureengineering work is extremely clever one has to
wonder if we can do better Certainly this laborintensive handengineering
approach does not scale well to new problems further ideally wed like to
have algorithms that can automatically learn even better feature representa
tions than the handengineered ones
These notes describe the sparse autoencoder learning algorithm which
is one approach to automatically learn features from unlabeled data In some
domains such as computer vision this approach is not by itself competitive
with the best handengineered features but the features it can learn do turn
out to be useful for a range of problems including ones in audio text etc
Further therere more sophisticated versions of the sparse autoencoder not
described in these notes but that youll hear more about later in the class
that do surprisingly well and in many cases are competitive with or superior
to even the best handengineered representations
1These notes are organized as follows We will rst describe feedforward
neural networks and the backpropagation algorithm for supervised learning
Then we show how this is used to construct an autoencoder which is an
unsupervised learning algorithm Finally we build on this to derive a sparse
autoencoder Because these notes are fairly notationheavy the last page
also contains a summary of the symbols used
2 Neural networks
Consider a supervised learning problem where we have access to labeled train
ing examples  xiyi Neural networks give a way of dening a complex
nonlinear form of hypotheses hWbx with parameters Wb that we can t
to our data
To describe neural networks we will begin by describing the simplest
possible neural network one which comprises a single neuron We will use
the following diagram to denote a single neuron
This neuron is a computational unit that takes as input x1x2x3and
a 1 intercept term and outputs hWbx fWTx fP3
i1Wixib
wherefR7Ris called the activation function  In these notes we will
choosef to be the sigmoid function
fz 1
1  expz
Thus our single neuron corresponds exactly to the inputoutput mapping
dened by logistic regression
Although these notes will use the sigmoid function it is worth noting that
another common choice for fis the hyperbolic tangent or tanh function
fz  tanhz ezez
ezez 1
Here are plots of the sigmoid and tanh functions
2The tanhz function is a rescaled version of the sigmoid and its output
range is 11 instead of 0 1
Note that unlike CS221 and parts of CS229 we are not using the con
vention here of x0 1 Instead the intercept term is handled separately by
the parameter b
Finally one identity thatll be useful later If fz  11  expz is
the sigmoid function then its derivative is given by f0z fz1fz
Iffis the tanh function then its derivative is given by f0z  1fz2
You can derive this yourself using the denition of the sigmoid or tanh
function
21 Neural network formulation
A neural network is put together by hooking together many of our simple
neurons so that the output of a neuron can be the input of another For
example here is a small neural network
3In this gure we have used circles to also denote the inputs to the net
work The circles labeled 1 are called bias units  and correspond to the
intercept term The leftmost layer of the network is called the input layer 
and the rightmost layer the output layer which in this example has only
one node The middle layer of nodes is called the hidden layer  because
its values are not observed in the training set We also say that our example
neural network has 3 input units not counting the bias unit 3 hidden
units  and 1 output unit 
We will let nldenote the number of layers in our network thus nl 3
in our example We label layer lasLl so layerL1is the input layer and
layerLnlthe output layer Our neural network has parameters  Wb 
W1b1W2b2 where we write Wl
ijto denote the parameter or weight
associated with the connection between unit jin layerl and unitiin layer
l1 Note the order of the indices Also bl
iis the bias associated with unit
iin layerl1 Thus in our example we have W12R33 andW22R13
Note that bias units dont have inputs or connections going into them since
they always output the value 1 We also let sldenote the number of nodes
in layerlnot counting the bias unit
We will write al
ito denote the activation meaning output value of
unitiin layerl Forl 1 we also use a1
ixito denote the ith input
Given a xed setting of the parameters Wb our neural network denes a
hypothesishWbx that outputs a real number Specically the computation
that this neural network represents is given by
a2
1fW1
11x1W1
12x2W1
13x3b1
1 2
a2
2fW1
21x1W1
22x2W1
23x3b1
2 3
a2
3fW1
31x1W1
32x2W1
33x3b1
3 4
hWbx a3
1fW2
11a2
1W2
12a2
2W2
13a2
3b2
1 5
In the sequel we also let zl
idenote the total weighted sum of inputs to unit
iin layerl including the bias term eg z2
iPn
j1W1
ijxjb1
i so that
al
ifzl
i
Note that this easily lends itself to a more compact notation Specically
if we extend the activation function f to apply to vectors in an element
wise fashion ie fz1z2z3  fz1fz2fz3 then we can write
4Equations 25 more compactly as
z2W1xb1
a2fz2
z3W2a2b2
hWbx a3fz3
More generally recalling that we also use a1xto also denote the values
from the input layer then given layer ls activations al we can compute
layerl 1s activations al1as
zl1Wlalbl6
al1fzl1 7
By organizing our parameters in matrices and using matrixvector operations
we can take advantage of fast linear algebra routines to quickly perform
calculations in our network
We have so far focused on one example neural network but one can
also build neural networks with other architectures meaning patterns of
connectivity between neurons including ones with multiple hidden layers
The most common choice is a nllayered network where layer 1 is the input
layer layer nlis the output layer and each layer lis densely connected to
layerl 1 In this setting to compute the output of the network we can
successively compute all the activations in layer L2 then layer L3 and so on
up to layerLnl using Equations 67 This is one example of a feedforward
neural network since the connectivity graph does not have any directed loops
or cycles
Neural networks can also have multiple output units For example here
is a network with two hidden layers layers L2andL3and two output units
in layerL4
5To train this network we would need training examples  xiyi where
yi2R2 This sort of network is useful if therere multiple outputs that
youre interested in predicting For example in a medical diagnosis applica
tion the vector xmight give the input features of a patient and the dierent
outputsyis might indicate presence or absence of dierent diseases
22 Backpropagation algorithm
Suppose we have a xed training set fx1y1 xmymgofmtrain
ing examples We can train our neural network using batch gradient descent
In detail for a single training example  xy we dene the cost function with
respect to that single example to be
JWbxy 1
2khWbxyk2
This is a onehalf squarederror cost function Given a training set of m
examples we then dene the overall cost function to be
JWb 
1
mmX
i1JWbxiyi

2nl1X
l1slX
i1sl1X
j1
Wl
ji2
8

1
mmX
i11
2hWbxiyi2

2nl1X
l1slX
i1sl1X
j1
Wl
ji2
The rst term in the denition of JWb is an average sumofsquares error
term The second term is a regularization term also called a weight de
cayterm that tends to decrease the magnitude of the weights and helps
prevent overtting1Theweight decay parameter controls the rela
tive importance of the two terms Note also the slightly overloaded notation
JWbxy is the squared error cost with respect to a single example JWb
is the overall cost function which includes the weight decay term
This cost function above is often used both for classication and for re
gression problems For classication we let y 0 or 1 represent the two class
labels recall that the sigmoid activation function outputs values in 0 1 if
1Usually weight decay is not applied to the bias terms bl
i as reected in our denition
forJW b Applying weight decay to the bias units usually makes only a small dierent
to the nal network however If you took CS229 you may also recognize weight decay
this as essentially a variant of the Bayesian regularization method you saw there where we
placed a Gaussian prior on the parameters and did MAP instead of maximum likelihood
estimation
6we were using a tanh activation function we would instead use 1 and 1
to denote the labels For regression problems we rst scale our outputs to
ensure that they lie in the 0 1 range or if we were using a tanh activation
function then the  11 range
Our goal is to minimize JWb as a function of Wandb To train
our neural network we will initialize each parameter Wl
ijand eachbl
ito
a small random value near zero say according to a N02 distribution
for some small  say 001 and then apply an optimization algorithm such
as batch gradient descent Since JWb is a nonconvex function gradient
descent is susceptible to local optima however in practice gradient descent
usually works fairly well Finally note that it is important to initialize the
parameters randomly rather than to all 0s If all the parameters start o at
identical values then all the hidden layer units will end up learning the same
function of the input more formally W1
ijwill be the same for all values of
i so thata2
1a2
2a2
3for any input x The random initialization
serves the purpose of symmetry breaking 
One iteration of gradient descent updates the parameters Wb as follows
Wl
ijWl
ij
Wl
ijJWb
bl
ibl
i
bl
iJWb
whereis the learning rate The key step is computing the partial derivatives
above We will now describe the backpropagation algorithm which gives
an ecient way to compute these partial derivatives
We will rst describe how backpropagation can be used to compute

Wl
ijJWbxy and
bl
iJWbxy the partial derivatives of the cost func
tionJWbxy dened with respect to a single example  xy Once we can
compute these then by referring to Equation 8 we see that the derivative
of the overall cost function JWb can be computed as

Wl
ijJWb 
1
mmX
i1
Wl
ijJWbxiyi
Wl
ij

bl
iJWb 1
mmX
i1
bl
iJWbxiyi
The two lines above dier slightly because weight decay is applied to Wbut
notb
7The intuition behind the backpropagation algorithm is as follows Given
a training example  xy we will rst run a forward pass to compute
all the activations throughout the network including the output value of the
hypothesishWbx Then for each node iin layerl we would like to compute
an error term l
ithat measures how much that node was responsible
for any errors in our output For an output node we can directly measure
the dierence between the networks activation and the true target value
and use that to dene nl
iwhere layer nlis the output layer How about
hidden units For those we will compute l
ibased on a weighted average
of the error terms of the nodes that uses al
ias an input In detail here is
the backpropagation algorithm
1 Perform a feedforward pass computing the activations for layers L2
L3 and so on up to the output layer Lnl
2 For each output unit iin layernlthe output layer set
nl
i
znl
i1
2kyhWbxk2yianl
if0znl
i
3 Forlnl1nl2nl3 2
For each node iin layerl set
l
i sl1X
j1Wl
jil1
j
f0zl
i
4 Compute the desired partial derivatives which are given as

Wl
ijJWbxy al
jl1
i

bl
iJWbxy l1
i
Finally we can also rewrite the algorithm using matrixvectorial nota
tion We will use   to denote the elementwise product operator denoted
 in Matlab or Octave and also called the Hadamard product so that
ifabc thenaibici Similar to how we extended the denition of
f to apply elementwise to vectors we also do the same for f0 so that
f0z1z2z3  
z1fz1
z2fz2
z3fz3 The algorithm can then be
written
81 Perform a feedforward pass computing the activations for layers L2
L3 up to the output layer Lnl using Equations 67
2 For the output layer layer nl set
nlyanlf0zn
3 Forlnl1nl2nl3 2
Set
l
WlTl1
f0zl
4 Compute the desired partial derivatives
rWlJWbxy l1alT
rblJWbxy l1
Implementation note In steps 2 and 3 above we need to compute f0zl
i
for each value of i Assuming fz is the sigmoid activation function we
would already have al
istored away from the forward pass through the net
work Thus using the expression that we worked out earlier for f0z we
can compute this as f0zl
i al
i1al
i
Finally we are ready to describe the full gradient descent algorithm In
the pseudocode below  Wlis a matrix of the same dimension as Wl
and blis a vector of the same dimension as bl Note that in this
notation  Wl is a matrix and in particular it isnt  times Wl We
implement one iteration of batch gradient descent as follows
1 Set Wl 0 bl 0 matrixvector of zeros for all l
2 Fori 1 tom
2a Use backpropagation to compute rWlJWbxy and
rblJWbxy
2b Set Wl WlrWlJWbxy
2c Set bl blrblJWbxy
93 Update the parameters
WlWl1
mWl
Wl
blbl1
mbl
To train our neural network we can now repeatedly take steps of gradient
descent to reduce our cost function JWb
23 Gradient checking and advanced optimization
Backpropagation is a notoriously dicult algorithm to debug and get right
especially since many subtly buggy implementations of itfor example one
that has an obyone error in the indices and that thus only trains some of
the layers of weights or an implementation that omits the bias termwill
manage to learn something that can look surprisingly reasonable while per
forming less well than a correct implementation Thus even with a buggy
implementation it may not at all be apparent that anything is amiss In
this section we describe a method for numerically checking the derivatives
computed by your code to make sure that your implementation is correct
Carrying out the derivative checking procedure described here will signi
cantly increase your condence in the correctness of your code
Suppose we want to minimize J as a function of  For this example
supposeJR7R so that2R In this 1dimensional case one iteration
of gradient descent is given by
d
dJ
Suppose also that we have implemented some function g that purportedly
computesd
dJ so that we implement gradient descent using the update
g How can we check if our implementation of gis correct
Recall the mathematical denition of the derivative as
d
dJ  lim
0JJ
2
Thus at any specic value of  we can numerically approximate the deriva
tive as follows
JEPSILON JEPSILON 
2EPSILON
10In practice we set EPSILON to a small constant say around 104 Theres
a large range of values of EPSILON that should work well but we dont set
EPSILON to be extremely small say 1020 as that would lead to numerical
roundo errors
Thus given a function g that is supposedly computingd
dJ we can
now numerically verify its correctness by checking that
gJEPSILON JEPSILON 
2EPSILON
The degree to which these two values should approximate each other will
depend on the details of J But assuming EPSILON  104 youll usually
nd that the left and righthand sides of the above will agree to at least 4
signicant digits and often many more
Now consider the case where 2Rnis a vector rather than a single
real number so that we have nparameters that we want to learn and
JRn7R In our neural network example we used  JWb but one
can imagine unrolling the parameters Wb into a long vector  We now
generalize our derivative checking procedure to the case where may be a
vector
Suppose we have a function gi that purportedly computes
iJ
wed like to check if giis outputting correct derivative values Let i
EPSILON ei where
 ei2
666666640
0

1

03
77777775
is theith basis vector a vector of the same dimension as  with a 1
in theith position and 0s everywhere else So iis the same as
 except its ith element has been incremented by EPSILON  Similarly let
iEPSILON eibe the corresponding vector with the ith element
decreased by EPSILON  We can now numerically verify gis correctness by
checking for each i that
giJiJi
2EPSILON
When implementing backpropagation to train a neural network in a cor
11rect implementation we will have that
rWlJWb 1
mWl
Wl
rblJWb 1
mbl
This result shows that the nal block of psuedocode in Section 22 is indeed
implementing gradient descent To make sure your implementation of gradi
ent descent is correct it is usually very helpful to use the method described
above to numerically compute the derivatives of JWb and thereby verify
that your computations of1
mWl
Wand1
mblare indeed giving the
derivatives you want
Finally so far our discussion has centered on using gradient descent to
minimizeJ If you have implemented a function that computes J and
rJ it turns out there are more sophisticated algorithms than gradient
descent for trying to minimize J For example one can envision an algo
rithm that uses gradient descent but automatically tunes the learning rate
so as to try to use a stepsize that causes to approach a local optimum
as quickly as possible There are other algorithms that are even more so
phisticated than this for example there are algorithms that try to nd an
approximation to the Hessian matrix so that it can take more rapid steps
towards a local optimum similar to Newtons method A full discussion of
these algorithms is beyond the scope of these notes but one example is the
LBFGS algorithm Another example is conjugate gradient  You will
use one of these algorithms in the programming exercise The main thing you
need to provide to these advanced optimization algorithms is that for any 
you have to be able to compute J andrJ These optimization algo
rithms will then do their own internal tuning of the learning ratestepsize 
and compute its own approximation to the Hessian etc to automatically
search for a value of that minimizes J Algorithms such as LBFGS and
conjugate gradient can often be much faster than gradient descent
3 Autoencoders and sparsity
So far we have described the application of neural networks to supervised
learning in which we are have labeled training examples Now suppose we
have only unlabeled training examples set fx1x2x3g wherexi2
Rn Anautoencoder neural network is an unsupervised learning algorithm
that applies backpropagation setting the target values to be equal to the
inputs Ie it uses yixi
12Here is an autoencoder
The autoencoder tries to learn a function hWbxx In other words it
is trying to learn an approximation to the identity function so as to output
xthat is similar to x The identity function seems a particularly trivial
function to be trying to learn but by placing constraints on the network
such as by limiting the number of hidden units we can discover interesting
structure about the data As a concrete example suppose the inputs xare
the pixel intensity values from a 10 10 image 100 pixels so n 100
and there are s2 50 hidden units in layer L2 Note that we also have
y2R100 Since there are only 50 hidden units the network is forced to
learn a compressed representation of the input Ie given only the vector of
hidden unit activations a22R50 it must try to reconstruct the 100pixel
inputx If the input were completely randomsay each xicomes from an
IID Gaussian independent of the other featuresthen this compression task
would be very dicult But if there is structure in the data for example if
some of the input features are correlated then this algorithm will be able to
discover some of those correlations2
2In fact this simple autoencoder often ends up learning a lowdimensional representa
tion very similar to PCAs
13Our argument above relied on the number of hidden units s2being small
But even when the number of hidden units is large perhaps even greater
than the number of input pixels we can still discover interesting structure
by imposing other constraints on the network In particular if we impose
asparsity constraint on the hidden units then the autoencoder will still
discover interesting structure in the data even if the number of hidden units
is large
Informally we will think of a neuron as being active or as ring
if its output value is close to 1 or as being inactive if its output value is
close to 0 We would like to constrain the neurons to be inactive most of the
time3
Recall that a2
jdenotes the activation of hidden unit jin the autoencoder
However this notation doesnt make explicit what was the input xthat led
to that activation Thus we will write a2
jx to denote the activation of this
hidden unit when the network is given a specic input x Further let
j1
mmX
i1h
a2
jxii
be the average activation of hidden unit javeraged over the training set
We would like to approximately enforce the constraint
j
whereis asparsity parameter  typically a small value close to zero say
 005 In other words we would like the average activation of each
hidden neuron jto be close to 005 say To satisfy this constraint the
hidden units activations must mostly be near 0
To achieve this we will add an extra penalty term to our optimization
objective that penalizes  jdeviating signicantly from  Many choices of
the penalty term will give reasonable results We will choose the following
s2X
j1log
j 1 log1
1j
Heres2is the number of neurons in the hidden layer and the index jis
summing over the hidden units in our network If you are familiar with the
3This discussion assumes a sigmoid activation function If you are using a tanh activa
tion function then we think of a neuron as being inactive when it outputs values close to
1
14concept of KL divergence this penalty term is based on it and can also be
writtens2X
j1KLjjj
where KLjjj log
j 1 log1
1jis the KullbackLeibler KL
divergence between a Bernoulli random variable with mean and a Bernoulli
random variable with mean  j KLdivergence is a standard function for
measuring how dierent two dierent distributions are If youve not seen
KLdivergence before dont worry about it everything you need to know
about it is contained in these notes
This penalty function has the property that KL jjj  0 if j and
otherwise it increases monotonically as  jdiverges from  For example in
the gure below we have set  02 and plotted KL jjj for a range of
values of j
We see that the KLdivergence reaches its minimum of 0 at  j and blows
up it actually approaches 1 as japproaches 0 or 1 Thus minimizing
this penalty term has the eect of causing  jto be close to 
Our overall cost function is now
JsparseWb JWb s2X
j1KLjjj
whereJWb is as dened previously and controls the weight of the
sparsity penalty term The term  jimplicitly depends on Wbalso because
it is the average activation of hidden unit j and the activation of a hidden
unit depends on the parameters Wb
To incorporate the KLdivergence term into your derivative calculation
there is a simpletoimplement trick involving only a small change to your
15code Specically where previously for the second layer  l 2 during
backpropagation you would have computed
2
i s3X
j1W3
ji3
j
f0z2
i
now instead compute
2
i s3X
j1W3
ji3
j
f0z2
i 

i1
1i

One subtlety is that youll need to know  ito compute this term Thus
youll need to compute a forward pass on all the training examples rst
to compute the average activations on the training set before computing
backpropagation on any example If your training set is small enough to t
comfortably in computer memory this will be the case for the programming
assignment you can compute forward passes on all your examples and keep
the resulting activations in memory and compute the  is Then you can
use your precomputed activations to perform backpropagation on all your
examples If your data is too large to t in memory you may have to scan
through your examples computing a forward pass on each to accumulate sum
up the activations and compute  idiscarding the result of each forward
pass after you have taken its activations a2
iinto account for computing  i
Then after having computed  i youd have to redo the forward pass for each
example so that you can do backpropagation on that example In this latter
case you would end up computing a forward pass twice on each example in
your training set making it computationally less ecient
The full derivation showing that the algorithm above results in gradient
descent is beyond the scope of these notes But if you implement the au
toencoder using backpropagation modied this way you will be performing
gradient descent exactly on the objective JsparseWb Using the derivative
checking method you will be able to verify this for yourself as well
4 Visualization
Having trained a sparse autoencoder we would now like to visualize the
function learned by the algorithm to try to understand what it has learned
Consider the case of training an autoencoder on 10 10 images so that
16n 100 Each hidden unit icomputes a function of the input
a2
if 100X
j1W1
ijxjb1
i

We will visualize the function computed by hidden unit iwhich depends on
the parameters W1
ijignoring the bias term for now using a 2D image In
particular we think of a1
ias some nonlinear feature of the input x We ask
What input image xwould cause a1
ito be maximally activated For this
question to have a nontrivial answer we must impose some constraints on
x If we suppose that the input is norm constrained by jjxjj2P100
i1x2
i1
then one can show try doing this yourself that the input which maximally
activates hidden unit iis given by setting pixel xjfor all 100 pixels j
1 100 to
xjW1
ijqP100
j1W1
ij2
By displaying the image formed by these pixel intensity values we can begin
to understand what feature hidden unit iis looking for
If we have an autoencoder with 100 hidden units say then we our
visualization will have 100 such imagesone per hidden unit By examining
these 100 images we can try to understand what the ensemble of hidden
units is learning
When we do this for a sparse autoencoder trained with 100 hidden units
on 10x10 pixel inputs4 we get the following result
4The results below were obtained by training on whitened natural images Whitening
is a preprocessing step which removes redundancy in the input by causing adjacent pixels
to become less correlated
17Each square in the gure above shows the norm bounded input image x
that maximally actives one of 100 hidden units We see that the dierent hid
den units have learned to detect edges at dierent positions and orientations
in the image
These features are not surprisingly useful for such tasks as object recog
nition and other vision tasks When applied to other input domains such
as audio this algorithm also learns useful representationsfeatures for those
domains too
185 Summary of notation
x Input features for a training example x2Rn
y Outputtarget values Here ycan be vector valued In the case
of an autoencoder yx
xiyiTheith training example
hWbxOutput of our hypothesis on input x using parameters Wb
This should be a vector of the same dimension as the target
valuey
Wl
ijThe parameter associated with the connection between unit j
in layerl and unitiin layerl 1
bl
iThe bias term associated with unit iin layerl 1 Can also
be thought of as the parameter associated with the connection
between the bias unit in layer land unitiin layerl 1
 Our parameter vector It is useful to think of this as the result
of taking the parameters Wb and unrolling them into a long
column vector
al
iActivation output of unit iin layerlof the network In addi
tion since layer L1is the input layer we also have a1
ixi
f The activation function Throughout these notes we used
fz  tanhz
zl
iTotal weighted sum of inputs to unit iin layerl Thusal
i
fzl
i
 Learning rate parameter
sl Number of units in layer lnot counting the bias unit
nl Number layers in the network Layer L1is usually the input
layer and layer Lnlthe output layer
 Weight decay parameter
x For an autoencoder its output ie its reconstruction of the
inputx Same meaning as hWbx
 Sparsity parameter which species our desired level of sparsity
i The average activation of hidden unit iin the sparse autoen
coder
 Weight of the sparsity penalty term in the sparse autoencoder
objective
19
  Robust Speech Recognition via LargeScale Weak Supervision
Alec Radford 1Jong Wook Kim 1Tao Xu1Greg Brockman1Christine McLeavey1Ilya Sutskever1
Abstract
We study the capabilities of speech processing
systems trained simply to predict large amounts of
transcripts of audio on the internet When scaled
to 680000 hours of multilingual and multitask
supervision the resulting models generalize well
to standard benchmarks and are often competitive
with prior fully supervised results but in a zero
shot transfer setting without the need for any fine
tuning When compared to humans the models
approach their accuracy and robustness We are
releasing models and inference code to serve as
a foundation for further work on robust speech
processing
1 Introduction
Progress in speech recognition has been energized by the
development of unsupervised pretraining techniques exem
plified by Wav2Vec 20 Baevski et al 2020 Since these
methods learn directly from raw audio without the need for
human labels they can productively use large datasets of un
labeled speech and have been quickly scaled up to 1000000
hours of training data Zhang et al 2021 far more than the
1000 or so hours typical of an academic supervised dataset
When finetuned on standard benchmarks this approach
has improved the state of the art especially in a lowdata
setting
These pretrained audio encoders learn highquality repre
sentations of speech but because they are purely unsuper
vised they lack an equivalently performant decoder mapping
those representations to usable outputs necessitating a fine
tuning stage in order to actually perform a task such as
speech recognition1 This unfortunately limits their use
fulness and impact as finetuning can still be a complex
process requiring a skilled practitioner There is an addi
tional risk with requiring finetuning Machine learning
Equal contribution1OpenAI San Francisco CA 94110 USA
Correspondence to Alec Radford alecopenaicom  Jong
Wook Kim jongwookopenaicom 
1Baevski et al 2021 is an exciting exception  having devel
oped a fully unsupervised speech recognition systemmethods are exceedingly adept at finding patterns within a
training dataset which boost performance on heldout data
from the same dataset However some of these patterns are
brittle and spurious and dont generalize to other datasets
and distributions In a particularly disturbing example Rad
ford et al 2021 documented a 92 increase in object
classification accuracy when finetuning a computer vision
model on the ImageNet dataset Russakovsky et al 2015
without observing any improvement in average accuracy
when classifying the same objects on seven other natural
image datasets A model that achieves superhuman per
formance when trained on a dataset can still make many
basic errors when evaluated on another possibly precisely
because it is exploiting those datasetspecific quirks that
humans are oblivious to Geirhos et al 2020
This suggests that while unsupervised pretraining has im
proved the quality of audio encoders dramatically the lack
of an equivalently highquality pretrained decoder com
bined with a recommended protocol of datasetspecific fine
tuning is a crucial weakness which limits their usefulness
and robustness The goal of a speech recognition system
should be to work reliably out of the box in a broad range
of environments without requiring supervised finetuning of
a decoder for every deployment distribution
As demonstrated by Narayanan et al 2018 Likhomanenko
et al 2020 and Chan et al 2021 speech recognition sys
tems that are pretrained in a supervised fashion across many
datasetsdomains exhibit higher robustness and generalize
much more effectively to heldout datasets than models
trained on a single source These works achieve this by
combining as many existing highquality speech recogni
tion datasets as possible However there is still only a
moderate amount of this data easily available SpeechStew
Chan et al 2021 mixes together 7 preexisting datasets
totalling 5140 hours of supervision While not insignifi
cant this is still tiny compared to the previously mentioned
1000000 hours of unlabeled speech data utilized in Zhang
et al 2021
Recognizing the limiting size of existing highquality super
vised datasets recent efforts have created larger datasets for
speech recognition By relaxing the requirement of gold
standard humanvalidated transcripts Chen et al 2021 and
Galvez et al 2021 make use of sophisticated automatedRobust Speech Recognition via LargeScale Weak Supervision 2
pipelines to scale weakly supervised speech recognition
to 10000 and 30000 hours of noisier training data This
tradeoff between quality and quantity is often the right
call Although understudied so far for speech recognition
recent work in computer vision has demonstrated that mov
ing beyond goldstandard crowdsourced datasets such as
ImageNet Russakovsky et al 2015 to much larger but
weakly supervised datasets significantly improves the ro
bustness and generalization of models Mahajan et al 2018
Kolesnikov et al 2020
Yet these new datasets are only a few times larger than the
sum of existing highquality datasets and still much smaller
than prior unsupervised work In this work we close that
gap scaling weakly supervised speech recognition the next
order of magnitude to 680000 hours of labeled audio data
We call our approach Whisper2 We demonstrate models
trained at this scale transfer well to existing datasets zero
shot removing the need for any datasetspecific finetuning
to achieve highquality results
In addition to scale our work also focuses on broaden
ing the scope of weakly supervised pretraining beyond
Englishonly speech recognition to be both multilingual and
multitask Of those 680000 hours of audio 117000 hours
cover 96 other languages The dataset also includes 125000
hours of Xentranslation data We find that for sufficiently
large models there is no drawback and even benefits to joint
multilingual and multitask training
Our work suggests that simple scaling of weakly supervised
pretraining has been underappreciated so far for speech
recognition We achieve these results without the need for
the selfsupervision or selftraining techniques that have
been a mainstay of recent largescale speech recognition
work To serve as a foundation for further research on robust
speech recognition we release inference code and models at
the following URL httpsgithubcomopenai
whisper 
2 Approach
21 Data Processing
Following the trend of recent work leveraging webscale
text from the internet for training machine learning systems
we take a minimalist approach to data preprocessing In
contrast to a lot of work on speech recognition we train
Whisper models to predict the raw text of transcripts without
any significant standardization relying on the expressive
ness of sequencetosequence models to learn to map be
tween utterances and their transcribed form This simplifies
2If an acronym or basis for the name is desired WSPSR stand
ing for Webscale Supervised Pretraining for Speech Recognition
can be usedthe speech recognition pipeline since it removes the need
for a separate inverse text normalization step in order to
produce naturalistic transcriptions
We construct the dataset from audio that is paired with tran
scripts on the Internet This results in a very diverse dataset
covering a broad distribution of audio from many different
environments recording setups speakers and languages
While diversity in audio quality can help train a model to be
robust diversity in transcript quality is not similarly bene
ficial Initial inspection showed a large amount of subpar
transcripts in the raw dataset To address this we developed
several automated filtering methods to improve transcript
quality
Many transcripts on the internet are not actually human
generated but the output of existing ASR systems Recent
research has shown that training on datasets of mixed human
and machinegenerated data can significantly impair the per
formance of translation systems Ghorbani et al 2021 In
order to avoid learning transcriptese we developed many
heuristics to detect and remove machinegenerated tran
scripts from the training dataset Many existing ASR sys
tems output only a limited subset of written language which
removes or normalizes away aspects that are difficult to pre
dict from only audio signals such as complex punctuation
exclamation points commas and question marks format
ting whitespace such as paragraphs or stylistic aspects such
as capitalization An alluppercase or alllowercase tran
script is very unlikely to be human generated While many
ASR systems include some level of inverse text normaliza
tion it is often simple or rulebased and still detectable from
other unhandled aspects such as never including commas
We also use an audio language detector which was created
by finetuning a prototype model trained on a prototype ver
sion of the dataset on V oxLingua107 Valk  Alum ae 2021
to ensure that the spoken language matches the language of
the transcript according to CLD2 If the two do not match
we dont include the audio transcript pair as a speech
recognition training example in the dataset We make an
exception if the transcript language is English and add these
pairs to the dataset as Xenspeech translation training
examples instead We use fuzzy deduping of transcript
texts to reduce the amount of duplication and automatically
generated content in the training dataset
We break audio files into 30second segments paired with
the subset of the transcript that occurs within that time
segment We train on all audio including segments where
there is no speech though with subsampled probability
and use these segments as training data for voice activity
detection
For an additional filtering pass after training an initial model
we aggregated information about its error rate on trainingRobust Speech Recognition via LargeScale Weak Supervision 3
data sources and performed manual inspection of these data
sources sorting by a combination of both high error rate and
data source size in order to identify and remove lowquality
ones efficiently This inspection showed a large amount of
only partially transcribed or poorly alignedmisaligned tran
scripts as well as remaining lowquality machinegenerated
captions that filtering heuristics did not detect
To avoid contamination we perform deduplication at a tran
script level between the training dataset and the evaluation
datasets we thought were at higher risk of overlap namely
TEDLIUM 3 Hernandez et al 2018
22 Model
Since the focus of our work is on studying the capabilities
of largescale supervised pretraining for speech recogni
tion we use an offtheshelf architecture to avoid confound
ing our findings with model improvements We chose an
encoderdecoder Transformer Vaswani et al 2017 as this
architecture has been well validated to scale reliably All
audio is resampled to 16000 Hz and an 80channel log
magnitude Mel spectrogram representation is computed on
25millisecond windows with a stride of 10 milliseconds
For feature normalization we globally scale the input to
be between 1 and 1 with approximately zero mean across
the pretraining dataset The encoder processes this input
representation with a small stem consisting of two convolu
tion layers with a filter width of 3 and the GELU activation
function Hendrycks  Gimpel 2016 where the second
convolution layer has a stride of two Sinusoidal position
embeddings are then added to the output of the stem after
which the encoder Transformer blocks are applied The
transformer uses preactivation residual blocks Child et al
2019 and a final layer normalization is applied to the en
coder output The decoder uses learned position embeddings
and tied inputoutput token representations Press  Wolf
2017 The encoder and decoder have the same width and
number of transformer blocks Figure 1 summarizes the
model architecture
We use the same bytelevel BPE text tokenizer used in GPT
2 Sennrich et al 2015 Radford et al 2019 for the English
only models and refit the vocabulary but keep the same size
for the multilingual models to avoid excessive fragmenta
tion on other languages since the GPT2 BPE vocabulary is
English only
23 Multitask Format
Although predicting which words were spoken in a given
audio snippet is a core part of the full speech recognition
problem and extensively studied in research it is not the
only part A fully featured speech recognition system can
involve many additional components such as voice activ
ity detection speaker diarization and inverse text normalization These components are often handled separately
resulting in a relatively complex system around the core
speech recognition model To reduce this complexity we
would like to have a single model perform the entire speech
processing pipeline not just the core recognition part An
important consideration here is the interface for the model
There are many different tasks that can be performed on
the same input audio signal transcription translation voice
activity detection alignment and language identification
are some examples
For this kind of onetomany mapping to work with a single
model some form of task specification is necessary We use
a simple format to specify all tasks and conditioning infor
mation as a sequence of input tokens to the decoder Since
our decoder is an audioconditional language model we also
train it to condition on the history of text of the transcript in
the hope that it will learn to use longerrange text context
to resolve ambiguous audio Specifically with some proba
bility we add the transcript text preceding the current audio
segment to the decoders context We indicate the beginning
of prediction with a startoftranscript token
First we predict the language being spoken which is repre
sented by a unique token for each language in our training
set 99 total These language targets are sourced from the
aforementioned V oxLingua107 model In the case where
there is no speech in an audio segment the model is trained
to predict a nospeech token indicating this The
next token specifies the task either transcription or trans
lation with an transcribe ortranslate
token After this we specify whether to predict timestamps
or not by including a notimestamps token for that
case At this point the task and desired format is fully
specified and the output begins For timestamp predic
tion we predict time relative to the current audio segment
quantizing all times to the nearest 20 milliseconds which
matches the native time resolution of Whisper models and
add additional tokens to our vocabulary for each of these
We interleave their prediction with the caption tokens the
start time token is predicted before each captions text and
the end time token is predicted after When a final tran
script segment is only partially included in the current 30
second audio chunk we predict only its start time token
for the segment when in timestamp mode to indicate that
the subsequent decoding should be performed on an au
dio window aligned with that time otherwise we truncate
the audio to not include the segment Lastly we add a
endoftranscript token We only mask out the
training loss over the previous context text and train the
model to predict all other tokens Please see Figure 1 for an
overview of our format and training setupRobust Speech Recognition via LargeScale Weak Supervision 4

2  Conv1D  GELUcross attention
LogMel Spectrogram
SOT ENTRANS  
CRIBE 00 The quick
Tokens in Multitask T raining FormatTransformer  
Encoder Blocks  Transformer  
Decoder Blocks  EN 00 The quick brown
 nexttoken  
prediction
Sinusoidal  
Positional  
Encoding
Learned  
Positional  
EncodingMultitask training data 680k hoursSequencetosequence learning
Multitask training formatEnglish transcription
AnytoEnglish speech translation
NonEnglish transcription
No speech   Ask not what y our country can do for  
  Ask not what y our country can do for 
   El rápido z orro marrón salta sobre   
  The quick brown fo x jumps o ver 
   언덕 위에  올라  내려다보면  너무나  넓고  넓은    
  언덕 위에  올라  내려다보면  너무나  넓고  넓은  
 background music pla ying 
  
PREV
special
tokenstext 
tokenstimestamp
tokensSTART OF  
TRANSCRIPTLANGUAGE  
TAG
NO 
SPEECHEOTTRANSCRIBE
TRANSLA TEbegin  
time
NO 
TIMEST AMPSend 
timetext tokensbegin  
timeend 
timetext tokens
text tokens
Voice activity  
detection  
VADCustom vocabulary 
promptingTimealigned transcription
Textonly transcription  
allows datasetspecific finetuningX  English  
Translation previous  
text tokensX  X  
Transcription Language  
identificationMLP
self attention
MLP
self attention
MLP
self attentionMLP
cross attention
self attention
MLP
cross attention
self attention
MLP
cross attention
self attentionTRANS  
CRIBE
Figure 1 Overview of our approach A sequencetosequence Transformer model is trained on many different speech processing tasks
including multilingual speech recognition speech translation spoken language identification and voice activity detection All of these
tasks are jointly represented as a sequence of tokens to be predicted by the decoder allowing for a single model to replace many different
stages of a traditional speech processing pipeline The multitask training format uses a set of special tokens that serve as task specifiers or
classification targets as further explained in Section 23
24 Training Details
We train a suite of models of various sizes in order to study
the scaling properties of Whisper Please see Table 1 for an
overview We train with data parallelism across accelerators
using FP16 with dynamic loss scaling and activation check
pointing Griewank  Walther 2000 Chen et al 2016
Models were trained with AdamW Loshchilov  Hutter
2017 and gradient norm clipping Pascanu et al 2013
with a linear learning rate decay to zero after a warmup over
the first 2048 updates A batch size of 256 segments was
used and the models are trained for 220updates which is
between two and three passes over the dataset Due to only
training for a few epochs overfitting is not a large concern
and we do not use any data augmentation or regularization
and instead rely on the diversity contained within such alarge dataset to encourage generalization and robustness
Please see Appendix F for full training hyperparameters3
During early development and evaluation we observed that
Whisper models had a tendency to transcribe plausible but
almost always incorrect guesses for the names of speakers
This happens because many transcripts in the pretraining
dataset include the name of the person who is speaking
encouraging the model to try to predict them but this infor
mation is only rarely inferable from only the most recent 30
3After the original release of Whisper we trained an additional
Large model denoted V2 for 25X more epochs while adding
SpecAugment Park et al 2019 Stochastic Depth Huang et al
2016 and BPE Dropout Provilkov et al 2019 for regularization
Reported results have been updated to this improved model unless
otherwise specifiedRobust Speech Recognition via LargeScale Weak Supervision 5
Model Layers Width Heads Parameters
Tiny 4 384 6 39M
Base 6 512 8 74M
Small 12 768 12 244M
Medium 24 1024 16 769M
Large 32 1280 20 1550M
Table 1 Architecture details of the Whisper model family
seconds of audio context To avoid this we finetune Whis
per models briefly on the subset of transcripts that do not
include speaker annotations which removes this behavior
3 Experiments
31 Zeroshot Evaluation
The goal of Whisper is to develop a single robust speech
processing system that works reliably without the need for
dataset specific finetuning to achieve highquality results
on specific distributions To study this capability we re
use a wide set of existing speech processing datasets to
check whether Whisper is able to generalize well across
domains tasks and languages Instead of using the standard
evaluation protocol for these datasets which include both
a train and test split we evaluate Whisper in a zeroshot
setting without using any of the training data for each of
these datasets so that we are measuring broad generalization
32 Evaluation Metrics
Speech recognition research typically evaluates and com
pares systems based on the word error rate WER metric
However WER which is based on string edit distance pe
nalizes all differences between the models output and the
reference transcript including innocuous differences in tran
script style As a result systems that output transcripts that
would be judged as correct by humans can still have a large
WER due to minor formatting differences While this poses
a problem for all transcribers it is particularly acute for
zeroshot models like Whisper which do not observe any
examples of specific datasets transcript formats
This is not a novel observation the development of evalua
tion metrics that better correlate with human judgement is an
active area of research and while there are some promising
methods none have seen widespread adoption for speech
recognition yet We opt to address this problem with ex
tensive standardization of text before the WER calculation
to minimize penalization of nonsemantic differences Our
text normalizer was developed through iterative manual in
spection to identify common patterns where naive WER
penalized Whisper models for an innocuous difference Ap
pendix C includes full details For several datasets we
observe WER drops of up to 50 percent usually due to aquirk such as a datasets reference transcripts seperating
contractions from words with whitespace We caution this
development procedure comes at a risk of overfitting to the
transcription style of Whisper models which we investigate
in Section 44 We are releasing the code for our text nor
malizer to allow for easy comparison and to help others
study the performance of speech recognition systems in
outofdistribution settings
33 English Speech Recognition
In 2015 Deep Speech 2 Amodei et al 2015 reported
a speech recognition system matched humanlevel perfor
mance when transcribing the LibriSpeech testclean split
As part of their analysis they concluded Given this result
we suspect that there is little room for a generic speech sys
tem to further improve on clean read speech without further
domain adaptation  Yet seven years later the SOTA WER
on LibriSpeech testclean has dropped another 73 from
their 53 to 14 Zhang et al 2021 far below their re
ported humanlevel error rate of 58 Despite this massive
and unanticipated further improvement in performance on
heldout but indistribution data speech recognition mod
els trained on LibriSpeech remain far above human error
rates when used in other settings What explains this gap
between reportedly superhuman performance indistribution
and subhuman performance outofdistribution
We suspect a large part of this gap between human and
machine behavior is due to conflating different capabilities
being measured by human and machine performance on
a test set This claim may seem confusing at first if both
humans and machines are taking the same test how can it be
that different skills are being tested The difference arises
not in the testing but in how they trained for it Humans are
often asked to perform a task given little to no supervision
on the specific data distribution being studied Thus human
performance is a measure of outofdistribution generaliza
tion But machine learning models are usually evaluated
after training on a large amount of supervision from the
evaluation distribution meaning that machine performance
is instead a measure of indistribution generalization While
both humans and machines are being evaluated on the same
testdata two quite different abilities are being measured
due to a difference in train data
Whisper models which are trained on a broad and diverse
distribution of audio and evaluated in a zeroshot setting
could potentially match human behavior much better than
existing systems To study whether this is the case or
whether the difference between machine and human per
formance is due to yettobeunderstood factors we can
compare Whisper models with both human performance
and standard finetuned machine learning models and check
which they more closely matchRobust Speech Recognition via LargeScale Weak Supervision 6
0 1 2 3 4 5 6 7 8
WER on LibriSpeech devclean 01020304050Average WER on Common Voice CHiME6 TEDLIUM Supervised LibriSpeech models
Zeroshot Whisper models
Zeroshot Human Alec
Ideal robustness y  x
Figure 2 Zeroshot Whisper models close the gap to human
robustness Despite matching or outperforming a human on Lib
riSpeech devclean supervised LibriSpeech models make roughly
twice as many errors as a human on other datasets demonstrating
their brittleness and lack of robustness The estimated robustness
frontier of zeroshot Whisper models however includes the 95
confidence interval for this particular human
To quantify this difference we examine both overall ro
bustness that is average performance across many distribu
tionsdatasets and effective robustness introduced by Taori
et al 2020 which measures the difference in expected
performance between a reference dataset which is usually
indistribution and one or more outofdistribution datasets
A model with high effective robustness does better than
expected on outofdistribution datasets as a function of its
performance on the reference dataset and approaches the
ideal of equal performance on all datasets For our analy
sis we use LibriSpeech as the reference dataset due to its
central role in modern speech recognition research and the
availability of many released models trained on it which
allows for characterizing robustness behaviors We use a
suite of 12 other academic speech recognition datasets to
study outofdistribution behaviors Full details about these
datasets can be found in Appendix A
Our main findings are summarized in Figure 2 and Table 2
Although the best zeroshot Whisper model has a relatively
unremarkable LibriSpeech cleantest WER of 25 which
is roughly the performance of modern supervised baseline
or the mid2019 state of the art zeroshot Whisper models
have very different robustness properties than supervised
LibriSpeech models and outperform all benchmarked Lib
riSpeech models by large amounts on other datasets Evenwav2vec 20 Whisper RER
Dataset Large no LM Large V2 
LibriSpeech Clean 27 27 00
Artie 245 62 747
Common V oice 299 90 699
Fleurs En 146 44 699
Tedlium 105 40 619
CHiME6 658 255 612
V oxPopuli En 179 73 592
CORAAL 356 162 545
AMI IHM 370 169 543
Switchboard 283 138 512
CallHome 348 176 494
WSJ 77 39 494
AMI SDM1 676 364 462
LibriSpeech Other 62 52 161
Average 293 128 552
Table 2 Detailed comparison of effective robustness across var
ious datasets Although both models perform within 01 of
each other on LibriSpeech a zeroshot Whisper model performs
much better on other datasets than expected for its LibriSpeech
performance and makes 552 less errors on average Results
reported in word error rate WER for both models after applying
our text normalizer
the smallest zeroshot Whisper model which has only 39
million parameters and a 67 WER on LibriSpeech testclean
is roughly competitive with the best supervised LibriSpeech
model when evaluated on other datasets When compared
to a human in Figure 2 the best zeroshot Whisper models
roughly match their accuracy and robustness For a detailed
breakdown of this large improvement in robustness Table
2 compares the performance of the best zeroshot Whisper
model with a supervised LibriSpeech model that has the
closest performance to it on LibriSpeech testclean Despite
their very close performance on the reference distribution
the zeroshot Whisper model achieves an average relative
error reduction of 552 when evaluated on other speech
recognition datasets
This finding suggests emphasizing zeroshot and outof
distribution evaluations of models particularly when at
tempting to compare to human performance to avoid over
stating the capabilities of machine learning systems due to
misleading comparisons
34 Multilingual Speech Recognition
In order to compare to prior work on multilingual speech
recognition we report results on two lowdata benchmarks
Multilingual LibriSpeech MLS Pratap et al 2020b and
V oxPopuli Wang et al 2021 in Table 3
Whisper performs well on Multilingual LibriSpeech out
performing XLSR Babu et al 2021 mSLAM BapnaRobust Speech Recognition via LargeScale Weak Supervision 7
01 1 10 100 1K 10K 100K 1M
Hours of transcribed audio25510204080160Word Error Rate WER
r2  083
SW
PTJAFIML
FRROGL
KO
UKNELO
AZ
MKLT
NLMSGU
ISMY
CATE
TRCS
NBARAF
HRUZ
DEVILV
ID
PLSVTAFAHY
THBN
KM
ENHUUR
BSKA
ZHSL
SKCY
RUBGFIL
ELHIKNMT
BE
HE
ITMRPA
DA
ESKKTG
ETSR
Figure 3 Correlation of pretraining supervision amount with
downstream speech recognition performance The amount of
pretraining speech recognition data for a given language is very
predictive of zeroshot performance on that language in Fleurs
Model MLS V oxPopuli
VP10K  FT  153
XLSR 1B 109 106
mSLAMCTC 2B 97 91
Maestro  81
ZeroShot Whisper 73 136
Table 3 Multilingual speech recognition performance Zero
shot Whisper improves performance on Multilingual LibriSpeech
MLS but is still significantly behind both Maestro XLSR and
mSLAM on V oxPopuli
et al 2022 and Maestro Chen et al 2022b in a zeroshot
setting We caution that we do use a simple text standardizer
for this result which prevents direct comparison or claims
of SOTA performance On V oxPopuli however Whisper
significantly underperforms prior work and only beats the
VP10KFT baseline from the original paper We suspect
the underperformance of Whisper models on V oxPopuli
could be due to other models including this distribution as
a major source for their unsupervised pretraining data and
the dataset having significantly more supervised data which
benefits finetuning While MLS has 10 hours of training
data per language the average amount of training data per
language is roughly 10 higher for V oxPopuli
These two benchmarks are somewhat narrow since they
only include 15 unique languages almost all of which are in
1 10 100 1K 10K 100K
Hours of translated audio0510152025303540BLEU
r2  024
HR
AMNL
MYSWEL
NETH
KNPADA
AR
MIBG
ML
MRTESV
IT
FILGLRO
UK
FA
UZBE
KMTG
ASETOCCA
IS
KKHEFR AF
VI
HAMT
LOBNPT
HUFI
KO
SDID
UR
LNLV
AZ
YOLB
CYHYPL
LTDE
KARU
MKMSSR
ES
ZH
JANBBS
MNSNTR
PSSK
SOCS
SLHI
GU
TAFigure 4 Correlation of pretraining supervision amount with
downstream translation performance The amount of pre
training translation data for a given language is only moderately
predictive of Whispers zeroshot performance on that language in
Fleurs
the IndoEuropean language family and many of which are
highresource languages These benchmarks only provide
limited coverage and room to study Whisper models multi
lingual capabilities which include training data for speech
recognition in 75 languages To study the performance of
Whisper more broadly we also report performance on the
Fleurs dataset Conneau et al 2022 In particular we were
interested in studying the relationship between the amount
of training data we have for a given language and the result
ing downstream zeroshot performance for that language
We visualize this relation in Figure 3 We find a strong
squared correlation coefficient of 083 between the log of
the word error rate and the log of the amount of training
data per language Checking the regression coefficient for a
linear fit to these loglog values results in an estimate that
WER halves for every 16 increase in training data We
also observed that many of the largest outliers in terms of
worse than expected performance according to this trend are
languages that have unique scripts and are more distantly
related to the IndoEuropean languages making up the ma
jority of the training dataset such as Hebrew  HE Telugu
TE Chinese  ZH and Korean  KO These differences
could be due to a lack of transfer due to linguistic distance
our byte level BPE tokenizer being a poor match for these
languages or variations in data qualityRobust Speech Recognition via LargeScale Weak Supervision 8
XEnglish High Mid Low All
XMEFX 342 202 59 147
XLSR 2B 361 277 151 221
mSLAMCTC 2B 378 296 185 248
Maestro 382 313 184 252
ZeroShot Whisper 362 326 252 291
Table 4 XenSpeech translation performance Zeroshot
Whisper outperforms existing models on CoV oST2 in the overall
medium and low resource settings but still moderately under
performs on highresource languages compared to prior directly
supervised work
Language ID Fleurs
w2vbert51 06B 714
mSLAMCTC 2B 777
Zeroshot Whisper 645
Table 5 Language identification performance Zeroshot Whis
pers accuracy at language identification is not competitive with
prior supervised results on Fleurs This is partially due to Whisper
being heavily penalized for having no training data for 20 of Fleurs
languages
35 Translation
We study the translation capabilities of Whisper models
by measuring their performance on the Xensubset of
CoV oST2 Wang et al 2020b We compare with Maestro
mSLAM and XLSR the highestperforming prior work
We achieve a new state of the art of 291 BLEU zeroshot
without using any of the CoV oST2 training data We at
tribute this to the 68000 hours of Xentranslation data
for these languages in our pretraining dataset which al
though noisy is vastly larger than the 861 hours of training
data for Xentranslation in CoV oST2 Since Whisper eval
uation is zeroshot it does particularly well on the lowest
resource grouping of CoV oST2 improving over mSLAM
by 67 BLEU Conversely the best Whisper model does not
actually improve over Maestro and mSLAM on average for
the highest resource languages
For an additional analysis on an even wider set of languages
we also repurpose Fleurs which is a speech recognition
dataset as a translation dataset Since the same sentences
are transcribed for every language we use the English tran
scripts as reference translations In Figure 4 we visualize
the correlation between the amount of translation training
data per language and the resulting zeroshot BLEU score
on Fleurs While there is a clear trend of improvement with
increasing training data the squared correlation coefficient
is much lower than the 083 observed for speech recognition
40 30 20 10 0 10
signaltonoise ratio dB125102050100WER on LibriSpeech testclean 
white noise
40 30 20 10 0 10
signaltonoise ratio dB
pub noise
unispeechsatbase100hlibrift
wav2vec2base100h
wav2vec2base960h
wav2vec2large960h
wav2vec2largerobustftlibri960h
wav2vec2large960hlv60self
asrcrdnnrnnlmlibrispeech
asrtransformertransformerlmlibrispeechhubertlargels960ft
hubertxlargels960ft
s2tmediumlibrispeechasr
s2tlargelibrispeechasr
stt_en_conformer_ctc_large
stt_en_conformer_transducer_xlarge
WhisperFigure 5 WER on LibriSpeech testclean as a function of SNR
under additive white noise left and pub noise right The
accuracy of LibriSpeechtrained models degrade faster than the
best Whisper model   NVIDIA STT models   perform best
under low noise but are outperformed by Whisper under high noise
SNR10 dB The secondbest model under low noise   is
finetuned on LibriSpeech only and degrades even more quickly
and only 024 We suspect this is partly caused by the noisier
training data due to errors in audio language identification
As an example Welsh  CY is an outlier with much worse
than expected performance at only 13 BLEU despite sup
posedly having 9000 hours of translation data This large
amount of Welsh translation data is surprising ranking 4th
overall for translation data and ahead of some of the most
spoken languages in the world like French Spanish and
Russian Inspection shows the majority of supposedly Welsh
translation data is actually English audio with English cap
tions where the English audio was misclassified as Welsh
by the language identification system resulting in it being
included as translation training data rather transcription data
according to our dataset creation rules
36 Language Identification
To evaluate language identification we use the Fleurs
dataset Conneau et al 2022 The zeroshot performance
of Whisper is not competitive with prior supervised work
here and underperforms the supervised SOTA by 136
However Whisper is heavily disadvantaged for language
identification on Fleurs since the Whisper dataset contains
no training data for 20 of the 102 languages in Fleurs upper
bounding accuracy at 804 On the 82 overlapping lan
guages the best Whisper model achieves 803 accuracyRobust Speech Recognition via LargeScale Weak Supervision 9
37 Robustness to Additive Noise
We tested the noise robustness of Whisper models and 14
LibriSpeechtrained models by measuring the WER when
either white noise or pub noise from the Audio Degrada
tion Toolbox Mauch  Ewert 2013 was added to the
audio The pub noise represents a more natural noisy envi
ronment with ambient noise and indistinct chatter typical
in a crowded restaurant or a pub Among the 14 models
twelve are pretrained andor finetuned on LibriSpeech and
the other two are NVIDIA STT models trained on a mixture
dataset similar to prior work like SpeechStew that includes
LibriSpeech The level of additive noise corresponding to
a given signaltonoise ratio SNR is calculated based on
the signal power of individual examples Figure 5 shows
how the ASR performance degrades as the additive noise
becomes more intensive There are many models that out
perform our zeroshot performance under low noise 40 dB
SNR which is unsurprising given those models are trained
primarily on LibriSpeech but all models quickly degrade as
the noise becomes more intensive performing worse than
the Whisper model under additive pub noise of SNR below
10 dB This showcases Whispers robustness to noise es
pecially under more natural distribution shifts like the pub
noise
38 Longform Transcription
Whisper models are trained on 30second audio chunks and
cannot consume longer audio inputs at once This is not aproblem with most academic datasets comprised of short
utterances but presents challenges in realworld applications
which often require transcribing minutes or hourslong au
dio We developed a strategy to perform buffered transcrip
tion of long audio by consecutively transcribing 30second
segments of audio and shifting the window according to the
timestamps predicted by the model We observed that it
is crucial to have beam search and temperature scheduling
based on the repetitiveness and the log probability of the
model predictions in order to reliably transcribe long audio
The full procedure is described in Section 45
We evaluate the longform transcription performance on
seven datasets consisting of speech recordings of various
lengths and recording conditions to cover as diverse a data
distribution as possible These include a longform adapta
tion of TEDLIUM3 Hernandez et al 2018 concatenated
so that each example is a fulllength TED talk a collection
of jargonladen segments taken from The Late Show with
Stephen Colbert Meanwhile sets of videospodcasts that
has been used as ASR benchmarks in online blogs Rev16
and Kincaid46 recordings of earnings calls Del Rio et al
2021 and the fulllength interviews from the Corpus of
Regional African American Language CORAAL Gunter
et al 2021 Full details about the longform datasets can
be found in Appendix A
We compare the performance with opensource models as
well as 4 commercial ASR services The results are sum
marized in Figure 6 showing the distribution of word error
rates from Whisper and the 4 commercial ASR services
TEDLIUM3 Meanwhile Kincaid46 Rev16 Earnings21 Earnings22 CORAAL0510152025303540Word Error Rate 
Whisper Company A Company B Company C Company D NVIDIA STT CTC large
Figure 6 Whisper is competitive with stateoftheart commercial and opensource ASR systems in longform transcription The
distribution of word error rates from six ASR systems on seven longform datasets are compared where the input lengths range from a
few minutes to a few hours The boxes show the quartiles of perexample WERs and the perdataset aggregate WERs are annotated
on each box Our model outperforms the best open source model NVIDIA STT on all datasets and in most cases commercial ASR
systems as wellRobust Speech Recognition via LargeScale Weak Supervision 10
as well as the NVIDIA STT ConformerCTC Large model
from the NeMo toolkit Kuchaiev et al 2019 which per
formed the best among the opensource models All com
mercial ASR services are queried using their default English
transcription settings as of September 1st 2022 and for
the NVIDIA STT model we used their buffered inference
implementation in the FrameBatchASR class to enable
longform transcription The results show that Whisper per
forms better than the compared models on most datasets
especially on the Meanwhile dataset which is heavy with
uncommon words Additionally we note the possibility that
some of the commercial ASR systems have been trained
on some of these publicly available datasets and therefore
these results may not be accurately reflecting the relative
robustness of the systems
39 Comparison with Human Performance
Because of ambiguous or indistinct speech as well as la
beling errors there are different levels of irreducible error
in each dataset and with WER metrics from ASR systems
alone it is difficult to make sense of how much room for
improvement exists in each dataset To quantify how close
Whispers performance is to the human performance we se
lected 25 recordings from the Kincaid46 dataset and used 5
services to obtain transcripts produced by professional tran
scribers among which one provides computerassisted tran
scription and the other four are entirely humantranscribed
The audio selection covers various recording conditions
such as scripted and unscripted broadcast telephone and
V oIP calls and meetings Figure 7 shows the distribution
of perexample WERs and aggregate WER across the 25
recordings where the computerassisted service has the
lowest aggregate WER that is 115 point better than Whis
pers and the purehuman performance is only a fraction
of a percentage point better than Whispers These results
indicate that Whispers English ASR performance is not
perfect but very close to humanlevel accuracy
4 Analysis and Ablations
41 Model Scaling
A large amount of the promise in weakly supervised train
ing approaches is their potential to use datasets much larger
than those in traditional supervised learning However this
comes with the cost of using data that is possibly much
noisier and lower quality than goldstandard supervision
A concern with this approach is that although it may look
promising to begin with the performance of models trained
on this kind of data may saturate at the inherent quality level
of the dataset which could be far below human level A re
lated concern is that as capacity and compute spent training
on the dataset increases models may learn to exploit the
Whisper A B C D E F G H I
 ASR            human transcription  
          computerassisted051015202530Word Error Rate 
Figure 7 Whispers performance is close to that of professional
human transcribers This plot shows the WER distributions of
25 recordings from the Kincaid46 dataset transcribed by Whisper
the same 4 commercial ASR systems from Figure 6 AD one
computerassisted human transcription service E and 4 human
transcription services FI The box plot is superimposed with dots
indicating the WERs on individual recordings and the aggregate
WER over the 25 recordings are annotated on each box
idiosyncrasies of the dataset and their ability to generalize
robustly to outofdistribution data could even degrade
To check whether this is the case we study the zeroshot
generalization of Whisper models as a function of the model
size Our analysis is summarized in Figure 8 With the
exception of English speech recognition performance con
tinues to increase with model size across multilingual speech
recognition speech translation and language identification
The diminishing returns for English speech recognition
could be due to saturation effects from approaching human
level performance as analysis in Section 39 suggests
42 Dataset Scaling
At 680000 hours of labeled audio the Whisper dataset is
one of the largest ever created in supervised speech recog
nition Exactly how important is the raw dataset size to
Whispers performance To study this we trained a series
of mediumsized models on subsampled versions of the
dataset which are 05 1 2 4 and 8 of the full
dataset size and compared their performance with the same
mediumsized model trained on the whole dataset Early
stopping based on the validation loss was used to select
model checkpoints for each dataset size Evaluation was
performed on an exponential moving average estimate of
the parameters Polyak  Juditsky 1992 using a smooth
ing rate of 09999 to help reduce the effect of the learning
rate not fully decaying to zero for the models trained on the
subsampled datasets due to early stopping Performance
on English and multilingual speech recognition and Xen
translation is reported in Table 6Robust Speech Recognition via LargeScale Weak Supervision 11
38M 73M 244M 768M 1549M1549M
Model parameters00255075100125150175200WER on 12 datasets 
English Speech Recognition
Average
Large V2
38M 73M 244M 768M 1549M1549M
Model parameters020406080100WER on 67 languages 
Multilingual Speech Recognition Fleurs
Average
Large V2
38M 73M 244M 768M 1549M1549M
Model parameters01020304050BLEU on 21 languages
XEn Translation CoVoST2
Average
Large V2
38M 73M 244M 768M 1549M1549M
Model parameters304050607080Accuracy on 102 languages 
Language Identification Fleurs
Average
Large V2
Figure 8 Zeroshot Whisper performance scales reliably across tasks and languages with increasing model size Lightly shaded
lines represent individual datasets or languages showing that performance is more varied than the smooth trends in aggregate performance
Large V2 distinguished with a dashed orange line since it includes several changes that are not present for the smaller models in this
analysis
Dataset English Multilingual X En
size WER  WER   BLEU  
3405 305 924 02
6811 196 727 17
13621 144 566 79
27243 123 450 139
54486 109 364 192
681070 99 292 248
Table 6 Performance improves with increasing dataset size
English speech recognition performance refers to an average over
12 datasets while the Multilingual speech recognition reports per
formance on the overlapping subset of languages in Fleurs and
Xentranslation reports average BLEU on CoV oST2 Dataset
size reported in hours
All increases in the dataset size result in improved perfor
mance on all tasks although we see significant variability
in improvement rates across tasks and sizes Performance
improves rapidly on English speech recognition from 3000
to 13000 hours and then slows down noticeably between
13000 and 54000 hours Using the full dataset which cor
responds to another 125 increase in size results in only a
further 1 point drop in WER This mirrors the diminishing
returns observed with model size scaling for English speech
recognition and could similarly be explained by saturation
effects when approaching humanlevel performance
Improvements in WER follow a powerlaw trend for mul
tilingual speech recognition till 54000 hours and then de
viate from this trend improving only a further 7 points
when increasing to the full dataset size For Xentransla
tion performance is practically zero when training on 7000
hours of audio or less and then follows a roughly loglinear
improvement trend till 54000 hours before also showingdiminishing returns when further scaling to the full dataset
size
The general trend across tasks of diminishing returns when
moving from 54000 hours to our full dataset size of 680000
hours could suggest that the current best Whisper models are
undertrained relative to dataset size and performance could
be further improved by a combination of longer training
and larger models It could also suggest that we are nearing
the end of performance improvements from dataset size
scaling for speech recognition Further analysis is needed to
characterize scaling laws for speech recognition in order
to decided between these explanations
43 Multitask and Multilingual Transfer
A potential concern with jointly training a single model
on many tasks and languages is the possibility of negative
transfer where interference between the learning of several
tasks results in performance worse than would be achieved
by training on only a single task or language To investigate
whether this is occurring we compared the performance
of models trained on just English speech recognition with
our standard multitask and multilingual training setup and
measured their average performance across our suite of zero
shot English speech recognition benchmarks We adjust for
the amount of FLOPs spent training on the task of English
speech recognition as only 65 of compute is spent on this
task in a joint training setup analysis would otherwise be
confounded by undertraining on the task when compared
to a samesized Englishonly model
Our results visualized in Figure 9 show that for small models
trained with moderate amounts of compute there is indeed
negative transfer between tasks and languages joint mod
els underperform Englishonly models trained for the same
amount of compute However multitask and multilingualRobust Speech Recognition via LargeScale Weak Supervision 12
10e19 10e20 10e21 10e22
FLOPs training on english speech recognition8101214161820Average WER on 11 english speech recognition datasetsEnglish Only
Multilingual and Multitask
Figure 9 Multitask and multilingual transfer improves with
scale For small models performance on English speech recogni
tion degrades when trained jointly in a multitask and multilingual
setup However multilingual and multitask models benefit more
from scale and eventually outperform models trained on English
data only 95 bootstrap estimate confidence intervals are shown
models scale better and for our largest experiments outper
form their Englishonly counterparts demonstrating positive
transfer from other tasks For our largest experiments joint
models also slightly outperform Englishonly models even
when not adjusting for compute spent per task
44 Text Normalization
Since we developed our text normalization jointly with
Whisper to discount innocuous word errors there is a risk
that our normalizer is overfitted to fixing Whispers peculiar
ities rather than addressing general variation in transcription
To check this we compared the performance of Whisper
using our normalizer versus an independently developed
one from the FairSpeech project Koenecke et al 2020 In
Figure 10 we visualize the differences On most datasets
the two normalizers perform similarly without significant
differences in WER reduction between Whisper and com
pared opensource models while on some datasets namely
WSJ CallHome and Switchboard our normalizer reduces
the WER of Whisper models significantly more The differ
ences in reduction can be traced down to different formats
used by the ground truth and how the two normalizers are pe
nalizing them For example in CallHome and Switchboard
our standardizer did not penalize differences in common
English contractions such as youre versus you are and
in WSJ our normalizer standardized the written and spo
0 10 20 30 40 50
Relative WER reduction compared to FairSpeechs normalizer CORAAL
CommonVoice9en
AMISDM1
CommonVoice51
Fleursen_us
AMIIHM
Artie
LibriSpeech
TEDLIUM3
VoxPopulien
WSJ
CallHome
SwitchboardOpensource models
Whisper modelsFigure 10 On most datasets our text normalizer has similar
effect on reducing WERs between Whisper models and other
opensource models compared to FairSpeechs normalizer For
each dataset the boxplot shows the distribution of relative WER
reduction across different models in our eval suite showing that
using our text normalizer generally results in lower WERs than
FairSpeechs On a few datasets our normalizer reduces WER
significantly and more so for Whisper models such as CallHome
and Switchboard which have many contractions in the ground truth
and WSJ which contains many numerical expressions
ken forms of numerical and monetary expressions such as
sixtyeight million dollars versus 68 million
45 Strategies for Reliable Longform Transcription
Transcribing longform audio using Whisper relies on ac
curate prediction of the timestamp tokens to determine the
amount to shift the models 30second audio context win
dow by and inaccurate transcription in one window may
negatively impact transcription in the subsequent windows
We have developed a set of heuristics that help avoid fail
ure cases of longform transcription which is applied in
the results reported in sections 38 and 39 First we use
beam search with 5 beams using the log probability as the
score function to reduce repetition looping which happens
more frequently in greedy decoding We start with tem
perature 0 ie always selecting the tokens with the high
est probability and increase the temperature by 02 up to
10 when either the average log probability over the gen
erated tokens is lower than 1or the generated text has a
gzip compression rate higher than 24 Providing the tran
scribed text from the preceding window as previoustext
conditioning when the applied temperature is below 05
further improves the performance We found that the proba
bility of the nospeech token alone is not sufficientRobust Speech Recognition via LargeScale Weak Supervision 13TEDLIUM3
Meanwhile
Kincaid46
Rev16
Earnings21
Earnings22
CORAAL
Average
Greedy decoding only 395 516 969 117 107 140 220 110
 Beam search 416 571 942 115 102 134 200 106
 Temperature fallback 416 571 942 115 102 134 200 106
 V oice activity detection 356 461 945 114 101 132 194 102
 Previous text conditioning 342 616 872 110 963 133 181 100
 Initial timestamp constraint 351 526 841 115 973 126 191 100
Table 7 Longform transcription performance improves incremen
tally as additional decoding heuristics are employed Details on
each intervention are described in Section 45
to distinguish a segment with no speech but combining
the nospeech probability threshold of 06 and the average
logprobability threshold of 1makes the voice activity
detection of Whisper more reliable Finally to avoid a fail
ure mode where the model ignores the first few words in
the input we constrained the initial timestamp token to be
between 00 and 10 second Table 7 shows that adding each
of the interventions above incrementally reduces the WER
overall but not evenly across the dataset These heuristics
serve as a workaround for the noisy predictions of the model
and more research would be needed to further improve the
reliability of longform decoding
5 Related Work
Scaling Speech Recognition A consistent theme across
speech recognition research has been documenting the bene
fits of scaling compute models and datasets Early work ap
plying deep learning to speech recognition found improved
performance with model depth and size and leveraged GPU
acceleration to make training these larger models tractable
Mohamed et al 2009 Further research demonstrated that
the benefit of deep learning approaches to speech recogni
tion increased with dataset size improving from being only
competitive with prior GMMHMM systems when using
just 3 hours of TIMIT training data for phone recognition
to achieving a 30 word error rate reduction when trained
on the 2000 hour Switchboard dataset Seide et al 2011
Liao et al 2013 is an early example of leveraging weakly
supervised learning to increase the size of a deep learn
ing based speech recognition dataset by over 1000 hours
These trends continued with Deep Speech 2 Amodei et al
2015 being a notable system developing highthroughput
distributed training across 16 GPUs and scaling to 12000
hours of training data while demonstrating continuing im
provements at that scale By leveraging semisupervised
pretraining Narayanan et al 2018 were able to grow
dataset size much further and study training on 162000
hours of labeled audio More recent work has exploredbillionparameter models Zhang et al 2020 and using up
to 1000000 hours of training data Zhang et al 2021
Multitask Learning Multitask learning Caruana 1997
has been studied for a long time In speech recognition
multilingual models have been explored for well over a
decade Schultz  Kirchhoff 2006 An inspirational and
foundational work in NLP exploring multitask learning
with a single model is Collobert et al 2011 Multitask
learning in the sequencetosequence framework Sutskever
et al 2014 using multiple encoders and decoders was in
vestigated in Luong et al 2015 The use of language codes
with a shared encoderdecoder architecture was first demon
strated for machine translation by Johnson et al 2017
removing the need for separate encoders and decoders This
approach was simplified further into the texttotext frame
work of McCann et al 2018 and popularized by its success
with large transformer language models in the work of Rad
ford et al 2019 and Raffel et al 2020 Toshniwal et al
2018 demonstrated jointly training a modern deep learn
ing speech recognition system on several languages with a
single model and Pratap et al 2020a scaled this line of
work significantly to 50 languages with a billionparameter
model MUTE Wang et al 2020c and mSLAM Bapna
et al 2022 studied joint training over both text and speech
language tasks demonstrating transfer between them
Robustness The question of how effectively models trans
fer and how robust they are to distribution shift and other
types of perturbations has long been studied and is actively
being researched across many fields of machine learning
Torralba  Efros 2011 highlighted the lack of generaliza
tion of machine learning models between datasets over a
decade ago Many other works have shown and continu
ally reiterated how despite high performance on IID test
sets machine learning models can still make many mistakes
when evaluated in even slightly different settings Lake et al
2017 Jia  Liang 2017 Alcorn et al 2019 Barbu et al
2019 Recht et al 2019 More recently Taori et al 2020
studied the robustness of image classification models and
Miller et al 2020 investigated this for questionanswering
models A key finding has been that multidomain train
ing increases robustness and generalization as discussed in
the Introduction This finding has been replicated across
many fields in addition to speech recognition including NLP
Hendrycks et al 2020 and computer vision Radford et al
2021
6 Limitations and Future Work
From our experimental results analyses and ablations we
have noted several limitations and areas for future workRobust Speech Recognition via LargeScale Weak Supervision 14
Improved decoding strategies As we have scaled Whis
per we have observed that larger models have made steady
and reliable progress on reducing perceptionrelated errors
such as confusing similarsounding words Many remaining
errors particularly in longform transcription seem more
stubborn in nature and decidedly nonhumanperceptual
They are a combination of failure modes of seq2seq mod
els language models and textaudio alignment and include
problems such as getting stuck in repeat loops not tran
scribing the first or last few words of an audio segment or
complete hallucination where the model will output a tran
script entirely unrelated to the actual audio Although the
decoding details discussed in Section 45 help significantly
we suspect finetuning Whisper models on a highquality
supervised dataset andor using reinforcement learning to
more directly optimize for decoding performance could help
further reduce these errors
Increase Training Data For LowerResource Languages
As Figure 3 shows Whispers speech recognition perfor
mance is still quite poor on many languages The same
analysis suggests a clear route for improvement since perfor
mance on a language is very well predicted by the amount
of training data for the language Since our pretraining
dataset is currently very Englishheavy due to biases of
our data collection pipeline which sourced primarily from
Englishcentric parts of the internet most languages have
less than 1000 hours of training data A targeted effort at in
creasing the amount of data for these rarer languages could
result in a large improvement to average speech recognition
performance even with only a small increase in our overall
training dataset size
Studying finetuning In this work we have focused on
the robustness properties of speech processing systems and
as a result only studied the zeroshot transfer performance
of Whisper While this is a crucial setting to study due to it
being representative of general reliability for many domains
where highquality supervised speech data does exist it is
likely that results can be improved further by finetuning
An additional benefit of studying finetuning is that it allows
for direct comparisons with prior work since it is a much
more common evaluation setting
Studying the impact of Language Models on Robustness
As argued in the introduction we suspect that Whispers
robustness is partially due to its strong decoder which is an
audio conditional language model Its currently unclear to
what degree the benefits of Whisper stem from training its
encoder decoder or both This could be studied by either
ablating various design components of Whisper such as
training a decoderless CTC model or by studying how the
performance of existing speech recognition encoders suchas wav2vec 20 change when used together with a language
model
Adding Auxiliary Training Objectives Whisper departs
noticeably from most recent stateoftheart speech recog
nition systems due to the lack of unsupervised pretraining
or selfteaching methods While we have not found them
necessary to achieve good performance it is possible that
the results could be further improved by incorporating this
7 Conclusion
Whisper suggests that scaling weakly supervised pre
training has been underappreciated so far in speech recogni
tion research We achieve our results without the need for
the selfsupervision and selftraining techniques that have
been a mainstay of recent largescale speech recognition
work and demonstrate how simply training on a large and
diverse supervised dataset and focusing on zeroshot trans
fer can significantly improve the robustness of a speech
recognition system
ACKNOWLEDGMENTS
Wed like to thank the millions of people who were involved
in creating the data used by Whisper Wed also like to
thank Nick Ryder Will Zhuk and Andrew Carr for the
conversation on the waterfall hike that inspired this project
We are also grateful to the Acceleration and Supercomputing
teams at OpenAI for their critical work on software and
hardware infrastructure this project used Wed also like to
thank Pamela Mishkin for advising the project from a policy
perspective Finally we are grateful to the developers of
the many software packages used throughout this project
including but not limited to Numpy Harris et al 2020
SciPy Virtanen et al 2020 ftfy Speer 2019 PyTorch
Paszke et al 2019 pandas pandas development team
2020 and scikitlearn Pedregosa et al 2011
References
Alcorn M A Li Q Gong Z Wang C Mai L Ku W
S and Nguyen A Strike with a pose Neural networks
are easily fooled by strange poses of familiar objects In
Proceedings of the IEEECVF Conference on Computer
Vision and Pattern Recognition  pp 48454854 2019
Amodei D Anubhai R Battenberg E Case C Casper
J Catanzaro B Chen J Chrzanowski M Coates
A Diamos G et al Deep speech 2 endtoend speech
recognition in english and mandarin arxiv arXiv preprint
arXiv151202595  2015
Ardila R Branson M Davis K Henretty M Kohler
M Meyer J Morais R Saunders L Tyers F MRobust Speech Recognition via LargeScale Weak Supervision 15
and Weber G Common voice A massivelymultilingual
speech corpus arXiv preprint arXiv191206670  2019
Babu A Wang C Tjandra A Lakhotia K Xu
Q Goyal N Singh K von Platen P Saraf Y 
Pino J et al XLSR Selfsupervised crosslingual
speech representation learning at scale arXiv preprint
arXiv211109296  2021
Baevski A Zhou H Mohamed A and Auli M wav2vec
20 A framework for selfsupervised learning of speech
representations arXiv preprint arXiv200611477  2020
Baevski A Hsu WN Conneau A and Auli M Unsu
pervised speech recognition Advances in Neural Infor
mation Processing Systems  342782627839 2021
Bapna A Cherry C Zhang Y  Jia Y  Johnson M
Cheng Y  Khanuja S Riesa J and Conneau A mslam
Massively multilingual joint pretraining for speech and
text arXiv preprint arXiv220201374  2022
Barbu A Mayo D Alverio J Luo W Wang C Gut
freund D Tenenbaum J and Katz B Objectnet A
largescale biascontrolled dataset for pushing the lim
its of object recognition models Advances in neural
information processing systems  32 2019
Caruana R Multitask learning Machine learning  281
4175 1997
Chan W Park D Lee C Zhang Y  Le Q and Norouzi
M SpeechStew Simply mix all available speech recogni
tion data to train one large neural network arXiv preprint
arXiv210402133  2021
Chen G Chai S Wang G Du J Zhang WQ
Weng C Su D Povey D Trmal J Zhang J
et al Gigaspeech An evolving multidomain asr corpus
with 10000 hours of transcribed audio arXiv preprint
arXiv210606909  2021
Chen S Wu Y  Wang C Chen Z Chen Z Liu S
Wu J Qian Y  Wei F Li J et al Unispeechsat Uni
versal speech representation learning with speaker aware
pretraining In ICASSP 20222022 IEEE International
Conference on Acoustics Speech and Signal Processing
ICASSP  pp 61526156 IEEE 2022a
Chen T Xu B Zhang C and Guestrin C Training
deep nets with sublinear memory cost arXiv preprint
arXiv160406174  2016
Chen Z Zhang Y  Rosenberg A Ramabhadran B
Moreno P Bapna A and Zen H Maestro Matched
speech text representations through modality matching
arXiv preprint arXiv220403409  2022bChild R Gray S Radford A and Sutskever I Gen
erating long sequences with sparse transformers arXiv
preprint arXiv190410509  2019
Collobert R Weston J Bottou L Karlen M
Kavukcuoglu K and Kuksa P Natural language pro
cessing almost from scratch Journal of machine learn
ing research  12ARTICLE24932537 2011
Conneau A Ma M Khanuja S Zhang Y  Axelrod V 
Dalmia S Riesa J Rivera C and Bapna A Fleurs
Fewshot learning evaluation of universal representations
of speech arXiv preprint arXiv220512446  2022
Del Rio M Delworth N Westerman R Huang M
Bhandari N Palakapilly J McNamara Q Dong J
Zelasko P and Jett e M Earnings21 a practical bench
mark for asr in the wild arXiv preprint arXiv210411348 
2021
Galvez D Diamos G Torres J M C Achorn K Gopi
A Kanter D Lam M Mazumder M and Reddi V  J
The peoples speech A largescale diverse english speech
recognition dataset for commercial usage arXiv preprint
arXiv211109344  2021
Geirhos R Jacobsen JH Michaelis C Zemel R Bren
del W Bethge M and Wichmann F A Shortcut learn
ing in deep neural networks Nature Machine Intelligence 
211665673 2020
Ghorbani B Firat O Freitag M Bapna A Krikun
M Garcia X Chelba C and Cherry C Scaling
laws for neural machine translation arXiv preprint
arXiv210907740  2021
Griewank A and Walther A Algorithm 799 revolve an
implementation of checkpointing for the reverse or ad
joint mode of computational differentiation ACM Trans
actions on Mathematical Software TOMS  2611945
2000
Gunter K Vaughn C and Kendall T Contextualiz
ingsretraction Sibilant variation and change in wash
ington dc african american language Language Variation
and Change  333331357 2021
Harris C R Millman K J van der Walt S J Gommers
R Virtanen P Cournapeau D Wieser E Taylor J
Berg S Smith N J Kern R Picus M Hoyer S van
Kerkwijk M H Brett M Haldane A Fern andez del
Rıo J Wiebe M Peterson P G erardMarchant P
Sheppard K Reddy T Weckesser W Abbasi H
Gohlke C and Oliphant T E Array programming
with NumPy Nature  585357362 2020 doi 101038
s4158602026492Robust Speech Recognition via LargeScale Weak Supervision 16
Hendrycks D and Gimpel K Gaussian error linear units
gelus arXiv preprint arXiv160608415  2016
Hendrycks D Liu X Wallace E Dziedzic A Krishnan
R and Song D Pretrained transformers improve outof
distribution robustness arXiv preprint arXiv200406100 
2020
Hernandez F Nguyen V  Ghannay S Tomashenko N A
and Est eve Y  Tedlium 3 twice as much data and corpus
repartition for experiments on speaker adaptation In
SPECOM  2018
Hsu WN Bolte B Tsai Y H H Lakhotia K
Salakhutdinov R and Mohamed A Hubert Self
supervised speech representation learning by masked
prediction of hidden units IEEEACM Transactions on
Audio Speech and Language Processing  2934513460
2021a
Hsu WN Sriram A Baevski A Likhomanenko T
Xu Q Pratap V  Kahn J Lee A Collobert R Syn
naeve G et al Robust wav2vec 20 Analyzing do
main shift in selfsupervised pretraining arXiv preprint
arXiv210401027  2021b
Huang G Sun Y  Liu Z Sedra D and Weinberger
K Q Deep networks with stochastic depth In European
conference on computer vision  pp 646661 Springer
2016
Jia R and Liang P Adversarial examples for evalu
ating reading comprehension systems arXiv preprint
arXiv170707328  2017
Johnson M Schuster M Le Q V  Krikun M Wu Y 
Chen Z Thorat N Vi egas F Wattenberg M Corrado
G et al Googles multilingual neural machine translation
system Enabling zeroshot translation Transactions of
the Association for Computational Linguistics  5339
351 2017
Kendall T and Farrington C The corpus of regional
african american language Version 202107 Eugene OR
The Online Resources for African American Language
Project httporaaluoregoneducoraal 
2021 Accessed 20220901
Koenecke A Nam A Lake E Nudell J Quartey M
Mengesha Z Toups C Rickford J R Jurafsky D
and Goel S Racial disparities in automated speech recog
nition Proceedings of the National Academy of Sciences 
1171476847689 2020
Kolesnikov A Beyer L Zhai X Puigcerver J Yung
J Gelly S and Houlsby N Big transfer bit General
visual representation learning In European conference
on computer vision  pp 491507 Springer 2020Kuchaiev O Li J Nguyen H Hrinchuk O Leary R
Ginsburg B Kriman S Beliaev S Lavrukhin V 
Cook J et al Nemo a toolkit for building ai applications
using neural modules arXiv preprint arXiv190909577 
2019
Lake B M Ullman T D Tenenbaum J B and Gersh
man S J Building machines that learn and think like
people Behavioral and brain sciences  40 2017
Liao H McDermott E and Senior A Large scale deep
neural network acoustic modeling with semisupervised
training data for youtube video transcription In 2013
IEEE Workshop on Automatic Speech Recognition and
Understanding  pp 368373 IEEE 2013
Likhomanenko T Xu Q Pratap V  Tomasello P Kahn
J Avidov G Collobert R and Synnaeve G Rethink
ing evaluation in asr Are our models robust enough
arXiv preprint arXiv201011745  2020
Loshchilov I and Hutter F Decoupled weight decay regu
larization arXiv preprint arXiv171105101  2017
Luong MT Le Q V  Sutskever I Vinyals O and
Kaiser L Multitask sequence to sequence learning
arXiv preprint arXiv151106114  2015
Mahajan D Girshick R Ramanathan V  He K Paluri
M Li Y  Bharambe A and Van Der Maaten L Ex
ploring the limits of weakly supervised pretraining In
Proceedings of the European conference on computer
vision ECCV  pp 181196 2018
Mauch M and Ewert S The audio degradation toolbox and
its application to robustness evaluation In Proceedings of
the 14th International Society for Music Information Re
trieval Conference ISMIR 2013  Curitiba Brazil 2013
accepted
McCann B Keskar N S Xiong C and Socher R The
natural language decathlon Multitask learning as ques
tion answering arXiv preprint arXiv180608730  2018
Meyer J Rauchenstein L Eisenberg J D and Howell
N Artie bias corpus An open dataset for detecting de
mographic bias in speech applications In Proceedings of
the 12th Language Resources and Evaluation Conference 
pp 64626468 Marseille France May 2020 European
Language Resources Association ISBN 9791095546
344 URL httpsaclanthologyorg2020
lrec1796 
Miller J Krauth K Recht B and Schmidt L The effect
of natural distribution shift on question answering models
InICML  2020Robust Speech Recognition via LargeScale Weak Supervision 17
Mohamed Ar Dahl G Hinton G et al Deep belief net
works for phone recognition In Nips workshop on deep
learning for speech recognition and related applications 
volume 1 pp 39 2009
Narayanan A Misra A Sim K C Pundak G Tripathi
A Elfeky M Haghani P Strohman T and Bacchi
ani M Toward domaininvariant speech recognition via
large scale training In 2018 IEEE Spoken Language
Technology Workshop SLT  pp 441447 IEEE 2018
Panayotov V  Chen G Povey D and Khudanpur S
Librispeech an asr corpus based on public domain au
dio books In 2015 IEEE international conference on
acoustics speech and signal processing ICASSP  pp
52065210 IEEE 2015
pandas development team T pandasdevpandas Pan
das February 2020 URL httpsdoiorg10
5281zenodo3509134 
Park D S Chan W Zhang Y  Chiu CC Zoph B
Cubuk E D and Le Q V  SpecAugment A simple data
augmentation method for automatic speech recognition
arXiv preprint arXiv190408779  2019
Pascanu R Mikolov T and Bengio Y  On the difficulty
of training recurrent neural networks In International
conference on machine learning  pp 13101318 PMLR
2013
Paszke A Gross S Massa F Lerer A Bradbury J
Chanan G Killeen T Lin Z Gimelshein N Antiga
L Desmaison A Kopf A Yang E DeVito Z Raison
M Tejani A Chilamkurthy S Steiner B Fang L
Bai J and Chintala S Pytorch An imperative style
highperformance deep learning library In Advances
in Neural Information Processing Systems 32  pp 8024
8035 2019
Pedregosa F Varoquaux G Gramfort A Michel V 
Thirion B Grisel O Blondel M Prettenhofer P
Weiss R Dubourg V  Vanderplas J Passos A Cour
napeau D Brucher M Perrot M and Duchesnay E
Scikitlearn Machine learning in Python Journal of
Machine Learning Research  1228252830 2011
Polyak B T and Juditsky A B Acceleration of stochastic
approximation by averaging SIAM journal on control
and optimization  304838855 1992
Pratap V  Sriram A Tomasello P Hannun A Y 
Liptchinsky V  Synnaeve G and Collobert R Mas
sively multilingual asr 50 languages 1 model 1 billion
parameters ArXiv  abs200703001 2020a
Pratap V  Xu Q Sriram A Synnaeve G and Collobert
R Mls A largescale multilingual dataset for speech
research arXiv preprint arXiv201203411  2020bPress O and Wolf L Using the output embedding to
improve language models In Proceedings of the 15th
Conference of the European Chapter of the Associa
tion for Computational Linguistics Volume 2 Short
Papers  pp 157163 Valencia Spain April 2017 As
sociation for Computational Linguistics URL https
aclanthologyorgE172025 
Provilkov I Emelianenko D and V oita E Bpedropout
Simple and effective subword regularization arXiv
preprint arXiv191013267  2019
Radford A Wu J Child R Luan D Amodei D and
Sutskever I Language models are unsupervised multitask
learners 2019
Radford A Kim J W Hallacy C Ramesh A Goh G
Agarwal S Sastry G Askell A Mishkin P Clark
J Krueger G and Sutskever I Learning transferable
visual models from natural language supervision arXiv
preprint arXiv210300020  2021
Raffel C Shazeer N Roberts A Lee K Narang S
Matena M Zhou Y  Li W Liu P J et al Exploring
the limits of transfer learning with a unified texttotext
transformer J Mach Learn Res  21140167 2020
Ravanelli M Parcollet T Plantinga P Rouhe A Cor
nell S Lugosch L Subakan C Dawalatabad N
Heba A Zhong J Chou JC Yeh SL Fu SW
Liao CF Rastorgueva E Grondin F Aris W Na
H Gao Y  Mori R D and Bengio Y  SpeechBrain A
generalpurpose speech toolkit 2021 arXiv210604624
Recht B Roelofs R Schmidt L and Shankar V 
Do ImageNet classifiers generalize to ImageNet In
Chaudhuri K and Salakhutdinov R eds Proceed
ings of the 36th International Conference on Machine
Learning  volume 97 of Proceedings of Machine Learn
ing Research  pp 53895400 PMLR 0915 Jun 2019
URLhttpsproceedingsmlrpressv97
recht19ahtml 
Russakovsky O Deng J Su H Krause J Satheesh S
Ma S Huang Z Karpathy A Khosla A Bernstein
M et al Imagenet large scale visual recognition chal
lenge International journal of computer vision  1153
211252 2015
Schultz T and Kirchhoff K Multilingual speech process
ing Elsevier 2006
Seide F Li G Chen X and Yu D Feature engineering
in contextdependent deep neural networks for conver
sational speech transcription In 2011 IEEE Workshop
on Automatic Speech Recognition  Understanding  pp
2429 IEEE 2011Robust Speech Recognition via LargeScale Weak Supervision 18
Sennrich R Haddow B and Birch A Neural machine
translation of rare words with subword units arXiv
preprint arXiv150807909  2015
Speer R ftfy Zenodo 2019 URL httpsdoiorg
105281zenodo2591652  Version 55
Sutskever I Vinyals O and Le Q V  Sequence to se
quence learning with neural networks Advances in neural
information processing systems  27 2014
Taori R Dave A Shankar V  Carlini N Recht B
and Schmidt L Measuring robustness to natural
distribution shifts in image classification In Larochelle
H Ranzato M Hadsell R Balcan M and Lin
H eds Advances in Neural Information Processing
Systems  volume 33 pp 1858318599 Curran Asso
ciates Inc 2020 URL httpsproceedings
neuripsccpaper2020file
d8330f857a17c53d217014ee776bfd50Paper
pdf
Torralba A and Efros A A Unbiased look at dataset bias
CVPR 2011  pp 15211528 2011
Toshniwal S Sainath T N Weiss R J Li B Moreno
P J Weinstein E and Rao K Multilingual speech
recognition with a single endtoend model 2018 IEEE
International Conference on Acoustics Speech and Sig
nal Processing ICASSP  pp 49044908 2018
Valk J and Alum ae T V oxlingua107 a dataset for spoken
language recognition In 2021 IEEE Spoken Language
Technology Workshop SLT  pp 652658 IEEE 2021
Vaswani A Shazeer N Parmar N Uszkoreit J Jones
L Gomez A N Kaiser Ł and Polosukhin I Atten
tion is all you need In Advances in neural information
processing systems  pp 59986008 2017
Virtanen P Gommers R Oliphant T E Haberland M
Reddy T Cournapeau D Burovski E Peterson P
Weckesser W Bright J van der Walt S J Brett M
Wilson J Millman K J Mayorov N Nelson A R J
Jones E Kern R Larson E Carey C J Polat I
Feng Y  Moore E W VanderPlas J Laxalde D
Perktold J Cimrman R Henriksen I Quintero E A
Harris C R Archibald A M Ribeiro A H Pedregosa
F van Mulbregt P and SciPy 10 Contributors SciPy
10 Fundamental Algorithms for Scientific Computing
in Python Nature Methods  17261272 2020 doi
101038s4159201906862
Wang C Tang Y  Ma X Wu A Okhonko D and Pino
J fairseq s2t Fast speechtotext modeling with fairseq
arXiv preprint arXiv201005171  2020aWang C Wu A and Pino J Covost 2 and massively
multilingual speechtotext translation arXiv preprint
arXiv200710310  2020b
Wang C Riviere M Lee A Wu A Talnikar C Haziza
D Williamson M Pino J and Dupoux E V oxpopuli
A largescale multilingual speech corpus for representa
tion learning semisupervised learning and interpretation
arXiv preprint arXiv210100390  2021
Wang P Sainath T N and Weiss R J Multitask training
with text data for endtoend speech recognition arXiv
preprint arXiv201014318  2020c
Watanabe S Mandel M Barker J Vincent E Arora
A Chang X Khudanpur S Manohar V  Povey D
Raj D et al Chime6 challenge Tackling multispeaker
speech recognition for unsegmented recordings arXiv
preprint arXiv200409249  2020
Xu Q Baevski A Likhomanenko T Tomasello P Con
neau A Collobert R Synnaeve G and Auli M Self
training and pretraining are complementary for speech
recognition In ICASSP 20212021 IEEE International
Conference on Acoustics Speech and Signal Processing
ICASSP  pp 30303034 IEEE 2021
Zhang Y  Qin J Park D S Han W Chiu CC Pang
R Le Q V  and Wu Y  Pushing the limits of semi
supervised learning for automatic speech recognition
arXiv preprint arXiv201010504  2020
Zhang Y  Park D S Han W Qin J Gulati A Shor J
Jansen A Xu Y  Huang Y  Wang S et al BigSSL
Exploring the frontier of largescale semisupervised
learning for automatic speech recognition arXiv preprint
arXiv210913226  2021Robust Speech Recognition via LargeScale Weak Supervision 19
A Evaluation Datasets
A1 Shortform Englishonly datasets
LibriSpeech Panayotov et al 2015 We used the testclean and testother splits from the LibriSpeech ASR corpus
TEDLIUM 3 Hernandez et al 2018 We used the test split of TEDLIUM Release 3 using the segmented manual
transcripts included in the release
Common Voice 51 Ardila et al 2019 We downloaded the English subset of Common V oice Corpus 51 from the
official website
Artie bias corpus Meyer et al 2020 We used the Artie bias corpus This is a subset of the Common V oice dataset
CallHome andSwitchboard  We used the two corpora from LDC2002S09 and LDC2002T43
WSJ  We used LDC93S6B and LDC94S13B and followed the s5recipe to preprocess the dataset
CORAAL  We used the 231 interviews from CORAAL Kendall  Farrington 2021 and used the preprocessing
script from the FairSpeech project
CHiME6  For CHiME6 Watanabe et al 2020 we downloaded the CHiME5 dataset and followed the stage 0
of the s5track1 recipe to create the CHiME6 dataset which fixes synchronization We then used the binaural
recordings  Pwav  and the corresponding transcripts
AMIIHM andAMISDM1  We preprocessed the AMI Corpus by following the stage 0 ad 2 of the s5b recipe
A2 Longform Englishonly datasets
TEDLIUM 3 Hernandez et al 2018 We used the 11 fulllength TED talks from the test split of TEDLIUM
Release 3 slicing the source audio files between the beginning of the first labeled segment and the end of the last
labeled segment of each talk and we used the concatenated text as the label
Meanwhile  This dataset consists of 64 segments from The Late Show with Stephen Colbert The YouTube video ID
and the corresponding start and end timestamps are available as part of the code release The labels are collected from
the closedcaption data for each video and corrected with manual inspection
Rev16  We use a subset of 16 files from the 30 podcast episodes in RevAIs Podcast Transcription Benchmark after
finding that there are multiple cases where a significant portion of the audio and the labels did not match mostly on the
parts introducing the sponsors We selected 16 episodes that do not have this error whose file numbers are
3 4 9 10 11 14 17 18 20 21 23 24 26 27 29 32
Kincaid46  This dataset consists of 46 audio files and the corresponding transcripts compiled in the blog article Which
automatic transcription service is the most accurate  2018 by Jason Kincaid We used the 46 audio files and reference
transcripts from the Airtable widget in the article For the human transcription benchmark in the paper we use a subset
of 25 examples from this data whose Ref IDs are
2 4 5 8 9 10 12 13 14 16 19 21 23 25 26 28 29 30 33 35 36 37 42 43 45
Earnings21 Del Rio et al 2021 and Earnings22  We used the files available in the speechdatasets repository as
of their 202206 version
CORAAL  We used the 231 fulllength interviews and transcripts from Kendall  Farrington 2021Robust Speech Recognition via LargeScale Weak Supervision 20
A3 Multilingual datasets
Multilingual LibriSpeech Pratap et al 2020b We used the test splits from each language in the Multilingual
LibriSpeech MLS corpus
Fleurs Conneau et al 2022 We collected audio files and transcripts using the implementation available as Hug
gingFace datasets To use as a translation dataset we matched the numerical utterance IDs to find the corresponding
transcript in English
VoxPopuli Wang et al 2021 We used the getasrdatapy script from the official repository to collect the ASR
data in 16 languages including English
Common Voice 9 Ardila et al 2019 We downloaded the Common V oice Corpus 9 from the official website
CoVOST 2 Wang et al 2020b We collected the X into English data collected using the official repository
B Compared Models
For comparison we use the following models from HuggingFace downloaded as of September 2022 using version 4210 of
thetransformers library
facebookwav2vec2large960hlv60self Xu et al 2021
facebookwav2vec2largerobustftlibri960h Hsu et al 2021b
facebookwav2vec2base100h Baevski et al 2020
facebookwav2vec2base960h Baevski et al 2020
facebookwav2vec2large960h Baevski et al 2020
facebookhubertlargels960ft Hsu et al 2021a
facebookhubertxlargels960ft Hsu et al 2021a
facebooks2tmediumlibrispeechasr Wang et al 2020a
facebooks2tlargelibrispeechasr Wang et al 2020a
microsoftunispeechsatbase100hlibrift Chen et al 2022a
nvidiastt enconformer ctclarge Kuchaiev et al 2019
nvidiastt enconformer transducer xlarge Kuchaiev et al 2019
speechbrainasrcrdnnrnnlmlibrispeech Ravanelli et al 2021
speechbrainasrtransformertransformerlmlibrispeech Ravanelli et al 2021
We note that all of the models above are entirely or partly trained on LibriSpeechRobust Speech Recognition via LargeScale Weak Supervision 21
C Text Standardization
Since Whisper may output any UTF8 string rather than a restricted set of graphemes the rules for text standardization need
to be more intricate and comprehensive than those defined on eg ASCII characters We perform the following steps to
normalize English texts in different styles into a standardized form which is a besteffort attempt to penalize only when a
word error is caused by actually mistranscribing a word and not by formatting or punctuation differences
1 Remove any phrases between matching brackets  
2 Remove any phrases between matching parentheses  
3 Remove any of the following words hmmmmmhmmmmuhum
4 Remove whitespace characters that comes before an apostrophe 
5 Convert standard or informal contracted forms of English into the original form
6 Remove commas   between digits
7 Remove periods   not followed by numbers
8Remove symbols as well as diacritics from the text where symbols are the characters with the Unicode category
starting with MS orP except period percent and currency symbols that may be detected in the next step
9Detect any numeric expressions of numbers and currencies and replace with a form using Arabic numbers eg Ten
thousand dollars 10000
10 Convert British spellings into American spellings
11 Remove remaining symbols that are not part of any numeric expressions
12 Replace any successive whitespace characters with a space
A different languagespecific set of transformations would be needed to equivalently normalize nonEnglish text but due to
our lack of linguistic knowledge to build such normalizers for all languages we resort to the following basic standardization
for nonEnglish text
1 Remove any phrases between matching brackets  
2 Remove any phrases between matching parentheses  
3Replace any markers symbols and punctuation characters with a space ie when the Unicode category of each
character in the NFKCnormalized string starts with MS orP
4 make the text lowercase
5 replace any successive whitespace characters with a space
Additionally we put a space between every letter for the languages that do not use spaces to separate words namely Chinese
Japanese Thai Lao and Burmese effectively measuring the character error rate instead
We note that the above is an imperfect solution and it will sometimes produce unintended and unexpected outputs We do
not claim that the text format resulting from the above is more correct in any measure Rather the procedures above are
designed to better distinguish between innocuous differences in wording and genuine mistranscriptions Python code for
the standardization procedures above is available as part of our code and model release to facilitate future iterations and
improvements on text standardizationRobust Speech Recognition via LargeScale Weak Supervision 22
D Raw Performance Tables
D1 English Transcription
D11 G REEDY DECODING
ModelLibriSpeechtestcleanLibriSpeechtestotherTEDLIUM3WSJCallHomeSwitchboardCommonV oice51ArtieCORAALCHiME6AMIIHMAMISDM1V oxPopulienFleursen us
Whisper tinyen 56 146 60 50 241 178 263 200 239 413 237 503 117 116
Whisper tiny 76 169 70 67 300 228 296 239 310 496 276 581 127 137
Whisper baseen 42 102 49 46 209 152 190 134 226 364 205 467 100 76
Whisper base 50 124 55 51 230 168 216 169 260 402 220 499 100 101
Whisper smallen 31 74 40 33 182 157 131 97 202 276 175 380 81 60
Whisper small 34 76 43 40 175 145 135 103 181 293 190 396 83 66
Whisper mediumen 31 63 41 33 162 141 106 76 175 253 164 372 74 50
Whisper medium 29 59 38 29 164 140 103 72 166 264 166 360 74 54
Whisper large 27 56 40 31 158 131 95 67 194 256 164 369 73 46
Whisper largev2 27 52 40 39 176 138 90 62 162 255 169 364 73 44
wav2vec2base100h 60 134 178 139 469 402 474 408 470 799 481 812 289 231
wav2vec2base960h 33 85 128 89 406 329 364 309 399 685 402 719 214 174
wav2vec2large960hlv60self 18 38 74 44 291 222 199 158 292 563 308 570 130 102
wav2vec2large960h 27 62 105 77 348 283 299 245 356 658 370 676 179 146
wav2vec2largerobustftlibri960h 26 53 92 61 234 198 203 162 294 581 317 616 151 118
asrcrdnnrnnlmlibrispeech 30 97 177 107 597 561 437 333 838 810 572 858 306 324
asrtransformertransformerlmlibrispeech 21 54 119 74 389 330 306 235 449 795 445 754 178 170
hubertlargels960ft 20 41 84 54 296 228 208 160 320 600 337 591 144 109
hubertxlargels960ft 19 35 83 54 293 222 198 148 315 585 333 589 142 105
s2tlargelibrispeechasr 33 81 149 94 545 403 381 307 502 792 534 795 216 180
s2tmediumlibrispeechasr 36 82 157 97 581 424 393 313 526 798 603 853 229 197
sttenconformer ctclarge 21 42 44 21 113 82 74 40 135 305 159 399 67 82
sttenconformer transducer xlarge 15 28 43 12 120 74 43 15 199 368 205 486 60 63
unispeechsatbase100hlibrift 57 138 177 136 465 400 453 386 447 748 478 777 298 224
Table 8 English transcription WER  with greedy decoding
D12 B EAM SEARCH WITH TEMPERATURE FALLBACK
ModelLibriSpeechtestcleanLibriSpeechtestotherTEDLIUM3WSJCallHomeSwitchboardCommonV oice51ArtieCORAALCHiME6AMIIHMAMISDM1V oxPopulienFleursen us
Whisper tinyen 54 128 54 46 214 160 235 184 214 420 227 542 109 100
Whisper tiny 67 150 63 59 248 183 261 208 251 480 256 573 116 124
Whisper baseen 41 96 46 40 183 142 175 132 185 352 211 490 93 71
Whisper base 49 110 50 44 205 156 194 153 205 400 215 500 95 89
Whisper smallen 32 67 43 30 172 134 126 92 175 295 179 425 81 53
Whisper small 33 72 43 39 171 133 128 93 164 309 192 435 82 61
Whisper mediumen 30 57 43 28 147 124 103 74 153 270 171 394 78 45
Whisper medium 27 56 40 27 153 132 97 67 149 276 176 430 76 44
Whisper large 28 57 43 35 162 142 89 64 151 252 176 371 72 45
Whisper largev2 25 49 37 26 164 136 82 57 142 249 174 399 70 42
Table 9 English transcription WER  with beam search and temperature fallbackRobust Speech Recognition via LargeScale Weak Supervision 23
D2 Multilingual Transcription
D21 M ULTILINGUAL LIBRISPEECH
ModelDutchEnglishFrenchGermanItalianPolishPortugueseSpanish
Whisper tiny 394 157 368 249 417 342 313 192
Whisper base 284 117 266 177 311 228 219 128
Whisper small 172 83 162 105 214 112 130 78
Whisper medium 117 68 89 74 160 65 90 53
Whisper large 102 63 89 66 143 66 92 54
Whisper largev2 93 62 73 55 138 50 68 42
Table 10 WER  on MLS
D22 C OMMON VOICE 9
ModelArabicBulgarianBengaliCatalanCzechWelshDanishGermanGreekEnglishSpanishEstonianPersian
Whisper tiny 909 793 1041 510 797 1018 772 345 619 288 303 1021 1203
Whisper base 844 681 1037 399 631 938 575 245 515 219 196 881 990
Whisper small 664 448 1186 238 341 654 321 130 317 145 103 672 719
Whisper medium 603 267 1247 164 188 436 193 85 200 112 69 456 499
Whisper large 560 241 1060 153 171 403 183 77 183 101 64 414 448
Whisper largev2 538 199 1034 141 135 342 144 64 160 94 56 351 394
ModelFinnishFrenchHindiHungarianIndonesianItalianJapaneseLithuanianLatvianMalayalamMongolianDutchPolish
Whisper tiny 685 497 1083 870 496 445 361 1035 878 1027 1230 436 453
Whisper base 529 373 1065 719 361 305 242 913 780 1229 1370 295 328
Whisper small 305 227 436 444 184 160 140 728 546 1048 2258 142 169
Whisper medium 188 160 315 269 116 94 105 494 372 1378 1134 80 101
Whisper large 170 147 250 235 106 81 94 439 348 1071 1174 71 90
Whisper largev2 144 139 219 197 85 71 91 352 255 1032 1284 58 76
ModelPortugueseRomanianRussianSlovakSlovenianSerbianSwedishTamilThaiTurkishUrduVietnameseChinese
Whisper tiny 352 682 406 1040 820 1061 582 1057 559 536 747 693 524
Whisper base 237 559 288 872 703 1030 424 495 321 386 586 516 449
Whisper small 125 332 150 604 455 1013 221 287 181 237 391 333 294
Whisper medium 81 215 93 420 298 856 137 196 105 177 299 244 232
Whisper large 71 198 82 379 251 874 124 176 88 166 281 199 291
Whisper largev2 63 158 71 319 206 705 106 161 80 145 242 182 268
Table 11 WER  on CommonV oice9
D23 V OXPOPULI
ModelCzechGermanEnglishenaccentedSpanishEstonianFinnishFrenchCroatianHungarianItalianLithuanianDutchPolishRomanianSlovakSlovenian
Whisper tiny 735 274 116 188 197 992 541 329 724 745 405 931 419 314 659 787 819
Whisper base 547 206 95 175 144 830 397 249 536 526 308 821 294 221 493 637 705
Whisper small 288 148 82 192 111 592 249 157 337 313 229 601 188 133 286 373 508
Whisper medium 184 124 76 191 96 382 166 122 239 193 197 393 149 101 184 230 363
Whisper large 159 119 72 208 88 333 155 110 190 168 184 350 140 90 170 191 313
Whisper largev2 126 112 70 186 82 287 124 114 161 138 190 332 129 78 144 154 279
Table 12 WER  on V oxPopuliRobust Speech Recognition via LargeScale Weak Supervision 24
D24 F LEURS
ModelAfrikaansAmharicArabicAssameseAzerbaijaniBelarusianBulgarianBengaliBosnianCatalanChineseCzechWelshDanish
Whisper tiny 912 1229 634 1020 931 940 810 1016 821 428 405 828 1013 820
Whisper base 815 1968 488 1020 764 913 651 1006 667 290 341 660 853 576
Whisper small 611 1202 306 1080 491 751 373 1044 394 162 208 376 593 328
Whisper medium 449 2293 204 1023 331 604 214 1006 239 96 121 213 408 195
Whisper large 426 1293 181 1056 287 566 184 1049 207 80 196 174 366 168
Whisper largev2 367 1403 160 1062 234 454 146 1041 157 73 147 133 330 138
ModelGermanGreekEnglishSpanishEstonianPersianFinnishTagalogFrenchGalicianGujaratiHausaHebrewHindi
Whisper tiny 278 674 124 159 948 1018 595 656 414 548 1012 1002 716 1023
Whisper base 179 535 89 99 779 861 431 458 285 474 1014 986 617 1011
Whisper small 102 308 61 56 513 558 240 277 150 302 1064 901 444 384
Whisper medium 65 190 44 36 298 410 139 191 87 212 1048 1066 331 268
Whisper large 55 187 45 35 255 361 122 158 77 190 1039 870 302 269
Whisper largev2 45 125 42 30 219 329 97 138 83 154 1027 889 271 215
ModelCroatianHungarianArmenianIndonesianIcelandicItalianJapaneseJavaneseGeorgianKazakhKhmerKannadaKoreanLuxembourgish
Whisper tiny 790 838 1186 517 1133 298 370 1073 1230 1652 1006 1007 361 991
Whisper base 591 650 1263 331 955 179 228 895 1147 1092 1016 1072 278 1007
Whisper small 334 389 866 163 726 98 120 886 1183 703 1044 1004 196 1001
Whisper medium 193 243 601 102 499 52 71 679 1173 488 989 777 164 900
Whisper large 167 210 537 85 430 42 64 870 1005 438 960 698 152 865
Whisper largev2 134 170 446 71 382 40 53 nan 1050 377 997 370 143 880
ModelLingalaLaoLithuanianLatvianMaoriMacedonianMalayalamMongolianMarathiMalayMalteseMyanmarNorwegianNepali
Whisper tiny 1054 1151 985 916 945 733 1015 1137 1003 512 1008 1248 620 1018
Whisper base 967 1051 873 798 775 599 1074 1257 1003 351 976 1226 440 1024
Whisper small 913 1022 656 532 595 369 1009 1442 602 189 922 1101 242 695
Whisper medium 832 1014 411 320 778 220 1011 1037 632 122 832 1230 129 544
Whisper large 768 1016 352 283 457 206 1014 1062 437 102 805 1245 114 522
Whisper largev2 756 1015 281 231 385 165 1007 1105 383 87 766 1157 95 471
ModelDutchOccitanPunjabiPolishPashtoPortugueseRomanianRussianSindhiSlovakSlovenianShonaSomaliSerbian
Whisper tiny 490 959 1026 456 1056 201 747 311 1058 772 872 1281 1056 837
Whisper base 330 829 1015 308 990 130 560 205 1039 606 746 1260 1096 643
Whisper small 164 873 1036 147 929 73 298 114 1317 333 493 1400 1053 422
Whisper medium 99 795 1020 80 1194 50 200 72 1470 173 319 1439 1040 449
Whisper large 83 759 1028 72 927 48 154 64 1779 157 278 1300 1035 292
Whisper largev2 67 753 1024 54 937 43 144 56 1565 117 231 1210 1029 339
ModelSwedishSwahiliTamilTeluguTajikThaiTurkishUkrainianUrduUzbekVietnameseYoruba
Whisper tiny 527 1009 999 1051 1017 588 425 512 652 1052 600 1064
Whisper base 374 925 587 1052 1093 382 275 377 520 1140 405 1018
Whisper small 208 737 352 982 843 219 159 193 373 1077 212 1164
Whisper medium 112 528 231 828 740 154 104 116 282 1096 127 1051
Whisper large 105 479 206 1006 745 132 94 103 250 933 107 1117
Whisper largev2 85 393 175 990 858 115 84 86 226 902 103 948
Table 13 WER  on FleursRobust Speech Recognition via LargeScale Weak Supervision 25
D3 Speech Translation
D31 F LEURS
ModelAfrikaansAmharicArabicAssameseAzerbaijaniBelarusianBulgarianBengaliBosnianCatalanChineseCzechWelshDanish
Whisper tiny 16 01 01 04 01 08 04 04 04 52 06 06 06 07
Whisper base 44 03 10 04 08 33 27 07 41 131 19 27 07 50
Whisper small 181 02 106 12 58 71 148 27 168 251 93 142 13 181
Whisper medium 295 09 199 35 117 98 239 106 260 319 151 236 84 286
Whisper large 316 11 238 39 131 110 262 120 280 337 168 256 112 316
Whisper largev2 341 19 255 54 137 117 285 132 297 342 184 278 130 327
ModelGermanGreekEnglishSpanishEstonianPersianFinnishTagalogFrenchGalicianGujaratiHausaHebrewHindi
Whisper tiny 52 01 686 77 01 01 02 08 47 40 07 01 02 10
Whisper base 137 07 733 124 03 02 05 21 131 105 15 00 06 34
Whisper small 259 116 773 182 36 58 73 120 235 175 39 03 54 111
Whisper medium 314 199 792 214 135 150 185 205 286 247 128 05 159 194
Whisper large 343 217 778 228 159 176 206 227 316 260 148 05 196 207
Whisper largev2 346 237 802 233 187 196 221 244 322 279 162 04 218 220
ModelCroatianHungarianArmenianIndonesianIcelandicItalianJapaneseJavaneseGeorgianKazakhKhmerKannadaKoreanLuxembourgish
Whisper tiny 06 01 01 03 04 53 02 02 01 01 01 08 05 08
Whisper base 37 02 01 26 04 113 15 02 02 02 01 09 37 17
Whisper small 146 48 07 164 18 178 96 14 02 08 05 23 122 57
Whisper medium 230 155 104 241 68 216 149 50 13 43 33 85 192 136
Whisper large 254 183 132 272 66 235 170 51 27 63 52 99 200 154
Whisper largev2 270 212 160 291 91 236 189 62 24 54 61 116 213 168
ModelLingalaLaoLithuanianLatvianMaoriMacedonianMalayalamMongolianMarathiMalayMalteseMyanmarNorwegianNepali
Whisper tiny 01 02 01 02 03 10 08 01 02 03 06 01 14 01
Whisper base 01 03 03 04 10 54 14 01 09 21 14 01 84 03
Whisper small 05 20 19 15 39 153 57 01 38 141 49 00 220 29
Whisper medium 09 81 96 100 85 235 138 05 109 232 112 02 291 127
Whisper large 12 93 120 125 94 264 165 10 131 255 128 05 305 129
Whisper largev2 10 110 140 143 102 277 167 10 129 273 135 04 314 161
ModelDutchOccitanPunjabiPolishPashtoPortugueseRomanianRussianSindhiSlovakSlovenianShonaSomaliSerbian
Whisper tiny 27 17 03 08 03 121 10 31 05 07 03 01 00 06
Whisper base 75 42 11 51 04 224 49 121 07 46 13 03 01 54
Whisper small 159 95 44 140 08 312 183 197 20 144 69 06 01 193
Whisper medium 216 159 128 190 21 359 266 248 55 227 140 14 04 277
Whisper large 228 168 146 214 37 374 291 267 59 251 169 18 05 305
Whisper largev2 240 202 157 223 34 381 315 278 57 261 170 18 07 325
ModelSwedishSwahiliTamilTeluguTajikThaiTurkishUkrainianUrduUzbekVietnameseYoruba
Whisper tiny 18 01 02 03 02 02 02 12 04 00 01 02
Whisper base 91 01 04 04 02 07 24 69 15 02 09 05
Whisper small 229 01 21 40 44 58 157 187 88 05 85 05
Whisper medium 321 31 70 108 114 128 229 258 149 38 166 09
Whisper large 331 53 85 109 130 152 257 280 163 58 195 12
Whisper largev2 353 72 92 125 145 161 266 294 172 60 204 14
Table 14 BLEU scores on FleursRobust Speech Recognition via LargeScale Weak Supervision 26
D32 C OVOST 2
ModelArabicCatalanWelshGermanSpanishEstonianPersianFrenchIndonesianItalianJapaneseLatvianMongolian
Whisper tiny 02 49 04 40 105 02 01 61 03 51 03 01 01
Whisper base 12 110 05 117 213 03 01 154 49 130 49 05 01
Whisper small 177 223 10 253 330 24 49 273 276 240 173 14 02
Whisper medium 306 292 121 332 384 114 155 336 423 295 246 97 02
Whisper large 355 303 161 343 380 134 175 344 454 291 242 105 03
Whisper largev2 397 318 215 363 401 150 193 364 481 309 261 139 01
ModelDutchPortugueseRussianSlovenianSwedishTamilTurkishChinese
Whisper tiny 43 95 57 04 20 01 02 04
Whisper base 124 232 161 14 105 04 28 14
Whisper small 281 406 309 92 299 17 168 68
Whisper medium 381 487 394 177 395 29 270 140
Whisper large 393 486 416 239 403 37 267 171
Whisper largev2 412 516 433 216 429 42 283 180
Table 15 BLEU scores on CoV oST2
D4 Longform Transcription
ModelTEDLIUM3MeanwhileKincaid46Rev16Earnings21Earnings22CORAAL
Whisper tinyen 55 128 138 151 170 220 303
Whisper tiny 68 155 167 170 187 244 331
Whisper baseen 46 94 112 132 125 166 252
Whisper base 48 122 122 145 135 184 269
Whisper smallen 46 60 94 120 108 140 219
Whisper small 42 69 101 121 111 143 223
Whisper mediumen 36 52 89 119 102 133 206
Whisper medium 38 54 86 114 103 132 203
Whisper large 38 53 88 110 103 134 204
Whisper largev2 35 51 88 113 97 126 196
wav2vec2base100h 176 277 393 352 457 571 554
wav2vec2base960h 128 197 329 298 373 468 491
wav2vec2large960hlv60self 72 114 211 213 217 280 367
wav2vec2large960h 101 164 274 264 304 401 435
wav2vec2largerobustftlibri960h 88 152 229 234 230 310 368
hubertlargels960ft 81 129 224 234 230 306 379
hubertxlargels960ft 81 125 229 232 231 313 381
sttenconformer ctclarge 40 98 131 145 126 176 251
sttenconformer transducer xlarge 53 106 171 198 162 197 389
Table 16 Longform English transcription WER Robust Speech Recognition via LargeScale Weak Supervision 27
E Training Dataset Statistics
01 1 10 100 1K 10K
Hours of audioMultilingual Speech Recognition
Lao 01Sundanese01Burmese 01Malagasy 02T ajik 03Gujarati 03Uzbek 03Yiddish 04Malayalam 05Georgian 06Nepali 06Marathi 06Punjabi 08Haitian Creole 10Maltese 11Bengali 13Khmer 13Belarusian 24Kannada 38Afrikaans 41T elugu 43Swahili 54Sinhala 54Albanian 57Galician 89Bosnian 11Hindi 12Kazakh 12Armenian 13Macedonian 16Icelandic 16Basque 21Persian 24Serbian 28Slovenian 41Estonian 41Azerbaijani 47Latvian 65Lithuanian 67Welsh 73T agalog 75Bulgarian 86Slovak 90Croatian 91Urdu 104T amil 136Czech 192Thai 226Norwegian 266Romanian 356Hungarian 379Malay 382Danish 473Greek 529Hebrew 688Vietnamese 691Ukrainian 697Arabic 739Indonesian 1014Finnish 1066Catalan 1883Dutch 2077Swedish 2119Italian 2585Polish 4278Turkish 4333Japanese 7054Korean 7993Portuguese 8573French 9752Russian 9761Spanish 11100German 13344Chinese 23446
65 English Speech Recognition
438218 hours18 Translation
125739 hours17 Multilingual Speech Recognition
117113 hoursDataset Components
1 10 100 1K 10K
Hours of audioTranslation
Turkmen 1Bashkir 1Malagasy 2Uzbek 4Sundanese 7Hausa 8Luxembourgish 10T atar 14T ajik 15Lingala 20Lao 20Somali 21Macedonian 30Kazakh 31Amharic 32Georgian 40Maltese 41Sindhi 46Faroese 46Occitan 49Burmese 59Pashto 63Latvian 68Albanian 72Haitian Creole 74Estonian 79Mongolian 79Icelandic 84Yiddish 85Azerbaijani 86Kannada 90Lithuanian 99Armenian 116Punjabi 117Belarusian 133Nepali 133Assamese 136Serbian 136Slovak 144Basque 168Tibetan 186Sanskrit 195Bulgarian 202Gujarati 208Sinhala 211Bosnian 219Catalan 236Croatian 239Breton 269Shona 279Swahili 282Marathi 288Norwegian 322Afrikaans 330Hawaiian 338Galician 368Danish 386Persian 392Slovenian 395Czech 401Hebrew 418Yoruba 432Ukrainian 509Hungarian 554Romanian 555Javanese 622Khmer 672Finnish 750Malayalam 892T agalog 894Greek 968T elugu 987Swedish 1055Indonesian 1174Maori 1381T amil 1484Latin 1614Thai 1635Malay 1691Vietnamese 1719Dutch 1767Norwegian Nynorsk 1889Bengali 1988Urdu 1990Italian 2145Polish 2200Turkish 2241Arabic 2286Portuguese 3620German 4309French 4481Hindi 5438Spanish 6693Russian 7687Welsh 8263Japanese 8860Chinese 11731Korean 19938
Figure 11 Training dataset statisticsRobust Speech Recognition via LargeScale Weak Supervision 28
F Hyperparameters
Hyperparameter Value
Updates 1048576
Batch Size 256
Warmup Updates 2048
Max grad norm 10
Optimizer AdamW
β1 09
β2 098
ϵ 106
Weight Decay 01
Weight Init Gaussian FanIn
Learning Rate Schedule Linear Decay
Speechless audio subsample factor 10
Condition on prior text rate 50
Table 17 Whisper training hyperparameters
Hyperparameter Value
Updates 655360
Batch Size 1024
BPE Dropout 01
Stochastic Depth 01
SpecAugment Policy LibriSpeech Basic
Table 18 Hyperparameters changed for Whisper Large V2
Model Max Learning Rate
Tiny 15103
Base 1103
Small 5104
Medium 25104
Large 175104
Large V2 20104
Table 19 Whisper model learning rates
  20230311
Resurrecting Recurrent Neural Networks for
Long Sequences
Antonio Orvieto1 Samuel L Smith2 Albert Gu2 Anushan Fernando2 Caglar Gulcehre2 Razvan Pascanu2
and Soham De2
1ETH Zurich2DeepMindWork done at DeepMind
Recurrent Neural Networks RNNs oﬀer fast inference on long sequences but are hard to optimize
and slow to train Deep statespace models SSMs have recently been shown to perform remarkably
well on long sequence modeling tasks and have the added beneﬁts of fast parallelizable training and
RNNlike fast inference However while SSMs are superﬁcially similar to RNNs there are important
diﬀerences that make it unclear where their performance boost over RNNs comes from In this paper
we show that careful design of deep RNNs using standard signal propagation arguments can recover
the impressive performance of deep SSMs on longrange reasoning tasks while also matching their
training speed To achieve this we analyze and ablate a series of changes to standard RNNs including
linearizing and diagonalizing the recurrence using better parameterizations and initializations and
ensuring proper normalization of the forward pass Our results provide new insights on the origins
of the impressive performance of deep SSMs while also introducing an RNN block called the Linear
Recurrent Unit that matches both their performance on the Long Range Arena benchmark and their
computational eﬃciency
1 Introduction
Recurrent neural networks RNNs have played a central role since the early days of deep learning and are
a natural choice when modelling sequential data Elman 1990 Hopﬁeld 1982 McCulloch and Pitts 1943
Rumelhart et al 1985 However while these networks have strong theoretical properties such as Turing
completeness Chung and Siegelmann 2021 Kilian and Siegelmann 1996 it is wellknown that they can be
hard to train in practice In particular RNNs suﬀer from the vanishing and exploding gradient problem Bengio
et al 1994 Hochreiter 1991 Pascanu et al 2013 which makes it diﬃcult for these models to learn about
the longrange dependencies in the data Several techniques were developed that attempt to mitigate this
issue including orthogonalunitary RNNs Arjovsky et al 2016 Helfrich et al 2018 and gating mechanisms
such as long shortterm memory LSTM Hochreiter and Schmidhuber 1997 and gated recurrent units
GRUsChoetal2014a Nonethelessthesemodelsarestillslowtooptimizeduetotheinherentlysequential
nature of their computation Kalchbrenner et al 2016 and are therefore hard to scale
Inrecentyears TransformersVaswanietal2017havegainedincreasingprominenceforsequencemodelling
tasks achieving remarkable success in a wide range of applications Brown et al 2020 Dosovitskiy et al
2020 Jumper et al 2021 Compared to RNNs attention layers are easier to scale and parallelize during
training and crucially they do not suﬀer from the vanishing gradient problem since the interaction between
any two tokens in the sequence is modeled by direct edges in the network A key issue with attention layers
however is that their computational and memory costs scale quadratically as 𝑂¹𝐿2ºwith the sequence length 𝐿
Transformers can therefore be especially expensive to deploy on long sequences RNNs which scale linearly
with the sequence length are therefore typically faster than transformers at inference time even for modest
sequence lengths Liu et al 2019
Motivated by these problems Gu et al 2021a recently introduced the S4 model a carefully designed deep
statespace model SSM achieving remarkable performance on tasks from the Long Range Arena LRA Tay
et al 2020 a benchmark explicitly designed to require very longranged reasoning S4 is theoretically
principled and inspired by continuoustime linear SSMs wellestablished components of modern control
systems More importantly the S4 layer and its variants DSS S4D S5 etc Gu et al 2022a Gupta et al
2022a Smith et al 2022 overcome the 𝑂¹𝐿2ºbottleneck of attention layers by modeling interactions between
Corresponding authors antonioorvietoinfethzch sohamdedeepmindcom
2023 DeepMind All rights reservedarXiv230306349v1  csLG  11 Mar 2023Resurrecting Recurrent Neural Networks for Long Sequences
T anhRNNLinRNNDiagStableNom5060708090sCIFARListOpsPathFindePathX
7Dnh5NNLLn5NNDLDg6tDbleNRrm5060708090
  Performance on TextRetrieval always
aligned with S45
 densetanhlinear denselinear diag stable  ring init norm
uni03B3 eﬃciency boostTest accuracy on LRA tasks
LRUDeep RNNs the S4 way
Lin Encoder same for all timestampsLinear Recurrent Unit LRU
x number of layers
MLPGLUsame for all timestampsLRULinearRecurrentUnittime pool
 classesLinear Layer
PreLNBNskip connection
xk diagxk1Buk
latexit sha1_base64lq8dCfMSVIgBgFuyopdT2ZmlIOcAAACRXicbVBNaxsxENWmaZu6X2577EXEFFJKzW4JND0EQnLpMYU6CVhmmdVqHWFptUizwUbozXSe29B73k0BJyTWTHgTTpgODx5s2b0SsaJR2m6a9k5cHqw0eP1550nj57uJl99XrA2day8WAG2XsUQFOKFmLAUpU4qixAnShxGEx2Zv3D0EddLU33HWiJGGcS0ryQEjlXfZNJQbcpQTNGXEsZhw7OFrY9KUSMEpqJdCeE9neZ8jEL9AO90RSqFYEVlWdj0BpCoMyUBukubaNv3u2lXRR9D7IlqBHlrWfd3y0vBWx71cgXPDLG1w5MGi5EqEDmudaIBP4mXDCGvQwo384pRA30WmpJWx8dVIFztCQauZkuolIDHru7vTn5v96wxWpr5GXdtChqfr2oahVFQeR0lJawVHNIgBuZbyV8mOwwDEG34khZHefB8cfOpnm0v3zZ7O7vLONbIW7JONkhGPpMd8pXskwHh5JT8JnI3RHcpacJxfX0pVkOfOGFPJ5RWDC7LWlatexit
uni03BBjexpexpuni03BDlogjiexpuni03B8logjuni03B3juni21901uni007Cuni03BBjuni007C212magnitudephase
NormalizationStable exponential parametrizationuni03BBuni03B3Linear Recurrent Unit LRU
Recurrent Block Variantsje x p explogjij
latexit sha1_base64rZa9gtg8yQCXmWFb6RTYwjAt8cMAAACFXicbVBNSwMxFMz6bf1a9eglWIQWteyKoB4E0YtHBWsL3bJk09c2NZtdkrdiKf4JL4VLx4U8Sp489Y1h60dSBhmJlH8iZKpTDoeVOxOTU9Mzs3HxuYXFpecVdXbs2SaY5lHkiE12NmAEpFJRRoIRqqoHFkYRKdHPW9yu3oI1I1BV2U6jHrKVEU3CGVgrdnUDacIOFHXpMA7hLC7uDO5BJK1BZ2CluiwDbgDZRDN28VIGoOPEH5I8GeIidDDRsKzGBRyyYypV6K9R7TKLiE1yQGUgZv2EtqFmqWAym3htsdU3rNKgzUTbo5AO1N8TPRYb040jm4wZts2o1xf82oZNgrPaHSDEHxn4eamaSY0H5FtCE0cJRdSxjXwv6V8jbTjKMtMmdL8EdXHifXeyVv3R0uZ8OR3WMUc2yCYpEJ8ckBNyTi5ImXDyQJ7IC3l1Hp1n58154lOOMOZdfIHzsc3262eCAlatexit
Figure 1jLeftDeep Linear Recurrent Unit LRU architecture introduced in this paper inspired by S4 Gu et al
2021a The model is a stack of LRU blocks with nonlinear projections in between and also uses skip connections
andnormalizationmethodslikebatchlayernormalization WeexpandonthedetailsinDandprovidepseudocode
in A We also use the same architecture structure NormRecurrenceGLUSkip for every variant of the recurrent
module in our study  tanhdense linear dense etc RightSummary of eﬀects for the main steps outlined in the
introduction towards designing LRUs starting from tanhRNNs Shown is the average performance 3 seeds of the
recurrent module at each step on the Long Range Arena LRA compared to average performance of deep SSMs
For all LRA tasks we match the performance of deep SSMs like S4S4DS5 with LRUs Detailed results in 3
tokens using a hidden state like RNNs under proper discretization techniques These models can be made
very eﬃcient at inference time by simply unrolling the layer like an RNN Futhermore since SSMs are linear in
the temporal dimension they are easily parallelizable during training in contrast to the slow sequential nature
of training a typical RNN This makes them very computationally eﬃcient on long sequences
While the S4 model is equivalent to an RNN during inference it has a number of unique characteristics during
training For example S4 is parameterized as a discretization of a latent continuoustime system of diﬀerential
equations S4 also uses speciﬁc initializations of the state matrices motivated from the theory of polynomial
projections Gu et al 2020 While these characteristics might seem to motivate the impressive performance of
these models later works Gu et al 2022a Gupta et al 2022ab Smith et al 2022 have suggested that the
speciﬁc initialization used by S4 is often not crucial for performance and that the discretization rules which
achieve best performance may deviate from theory Smith et al 2022 It is therefore unclear what these
unique characteristics of the deep SSMs are doing mechanistically and how they can be simpliﬁed
Motivated by the striking similarities between RNNs and deep SSMs and in an attempt to better understand
the underlying mechanism driving the performance of these models we study the power and limitations of
RNNs when used as core components of deep architectures for longrange reasoning Our main goal is to
answer the question
Can we match the performance and eﬃciency of deep continuoustime SSMs using deep RNNs 
We give a positive answer to this question We show that the performance boost provided by deep SSMs like S4
can also be achieved via a series of small changes to a vanilla deep RNN With these changes we can recover
the performance and eﬃciency of these deep SSMs on the Long Range Arena LRA benchmark Tay et al
2020 We call this new RNN model the Linear Recurrent Unit or LRU for short
Main Steps We outline here the main steps needed towards crafting performant and eﬃcient RNN models
Note while some of these observations have been made in prior works see B we provide novel perspectives
and careful ablations leading to new insights Each step presented in this paper unveils a speciﬁc property of
2Resurrecting Recurrent Neural Networks for Long Sequences
recurrent networks and showcases the challenges and best practices in training and initializing deep RNNs
Linear Recurrences When replacing SSM layers in a deep architecture with vanilla RNN layers using tanh
or ReLU activations the performance on Long Range Arena LRA drops signiﬁcantly Surprisingly in 31
we ﬁnd that simply removing the nonlinearities in the recurrence of the RNN ie using linear recurrences
gives a substantial boost in test accuracy We motivate this eﬀect in E1 by showing that stacking linear
RNN layers and nonlinear MLP blocks Fig1 can indeed model complex nonlinear sequencetosequence
maps without the need for nonlinearities in the recurrence While dropping the nonlinearity does not seem
to harm expressivity it leads to several advantages from the ability to directly control how quickly the
gradients might vanish or explode to allowing us to parallelize training Our ﬁndings also partially motivate
the success of deep SSMs where the recurrence is also linear
Complex Diagonal Recurrent Matrices Dense linear RNN layers can be reparameterized to a complex
diagonal form without aﬀecting the expressivity of the network or the features at initialization 32
Diagonal linear RNN layers additionally allow for a highly parallelizable unrolling of the recurrence using
parallel scans to substantially improve training speeds Martin and Cundy 2017 We validate that these
observations which have been leveraged by prior SSMs Gupta et al 2022a Smith et al 2022 also
provide important eﬃciency improvements for linear RNN layers
Stable Exponential Parameterization In 33 we show that using an exponential parameterization for the
diagonal recurrent matrix has important beneﬁts Crucially this enables us to easily enforce stability during
training which in turn allows us to modify the initialization distribution to facilitate longrange reasoning
and improve performance Our results indicate that rather than the speciﬁc deterministic initializations
used by several recent SSMs it is the eigenvalue distribution of the recurrent layer at initialization that
determines if the model can capture longrange reasoning
Normalization In 34 we show that normalizing the hidden activations on the forward pass is important
when learning tasks with very longrange dependencies With this ﬁnal modiﬁcation our RNNs can match
the performance of deep SSMs on all tasks in the LRA benchmark Connecting back to statespace models
we show in 4 how our normalization can be linked to the discretization structure in S4
We summarize the deep Linear Recurrent Unit LRU architecture used in this paper and the eﬀect of each of
the above steps on performance in Fig1 We emphasize that the main purpose of our work is not to surpass
the performance of S4based models but rather to demonstrate that simple RNNs can also achieve strong
performance on long range reasoning tasks when properly initialized and parameterized We believe the
insights derived in this paper can be useful to design future architectures and to simplify existing ones
2 Preliminaries
In this section we compare the key architectural components RNNs and SSMs studied in this work and also
describe our methodology and experimental setup For a more thorough discussion or related architectures
the reader can check our related work section B
21 Recap of recurrent block structures
We give an overview of the main architectural components considered in this paper focusing on the major
diﬀerence between Vanilla RNNs and recent S4like deep SSMs Gu et al 2021a 2022a Gupta et al 2022a
Smith et al 2022
RNN Layer Let¹𝑢1𝑢2𝑢𝐿ºbe a sequence of 𝐻indimensional inputs which can be thought of as either
the result of intermediate layer computations which keep the sequential structure or as the initial input An
RNN layer with 𝑁dimensional hidden state computes a sequence of 𝐻outdimensional outputs ¹𝑦1𝑦2𝑦𝐿º
through a recurrent computation1using learnable parameters 𝐴2ℝ𝑁𝑁𝐵2ℝ𝑁𝐻in𝐶2ℝ𝐻out𝑁𝐷2ℝ𝐻out𝐻in
𝑥𝑘𝜎¹𝐴𝑥𝑘1𝐵𝑢𝑘º 𝑦𝑘𝐶𝑥𝑘𝐷𝑢𝑘 1
1WedonotusebiasparametersastheycanbeincorporatedintotheMLPblocksprecedingandfollowingtheRNNblock ClassicalRNNs
also included a nonlinearity on the output 𝑦𝑘𝜎out¹𝐶𝑥𝑘𝑏ºwith𝐷0 Having𝐷0basically introduces a skip connection standard in
modern architectures and the 𝜎outcan be thought of as part of the MLP following the RNN
3Resurrecting Recurrent Neural Networks for Long Sequences
starting from 𝑥002ℝ𝑁𝜎here denotes a nonlinearity often chosen to be a tanhor sigmoid activation If 𝜎
is the identity function then we say the RNN layer is linear
S4likerecurrentlayer Wepresentasimpliﬁed2versionoftheS4recurrenceintroducedinGuetal2021a
Theinput¹𝑢0𝑢1𝑢𝐿1ºisnowseenastheresultofsamplingalatentcontinuoustimesignal 𝑢ctℝ0ℝ𝐻in
at multiples of a stepsize Δ0 ie𝑢ct¹Δ𝑘º𝑢𝑘for all𝑘20𝐿1 The output sequence ¹𝑦0𝑦1𝑦𝐿1º
isthensampledagainwithstepsize Δfromthesignal 𝑦ctℝ0ℝ𝐻outcomputedbythefollowingcontinuous
time statespace model initialized at 𝑥ct¹0º0
𝑑
𝑑𝑡𝑥ct¹𝑡º𝐴𝑥ct¹𝑡º𝐵𝑢ct¹𝑡º
𝑦ct¹𝑡º𝐶𝑥ct¹𝑡º
𝐷𝑢ct¹𝑡º 2
where¹𝑝ºdenotestherealpartofacomplexvaluedvector 𝑝𝐴diag¹𝑎ºwith 𝑎2ℂ𝑁𝐵2ℂ𝑁𝐻in𝐶2ℂ𝐻out𝑁
and𝐷2ℝ𝐻out𝐻in Ignoring the continuoustime nature of this model the most striking diﬀerences compared
to Eq1are that a the computation on the righthandside is linearin the hidden state and in the input
and b most parameters are complexvalued with 𝐴being diagonal While 𝐵𝐶𝐷follow complex random
or uniform initialization the transition matrix 𝐴isstructured  ie initialized deterministically through HiPPO
theory Gu et al 2020 in diagonal form Common choices Gu et al 2022a are 𝑎𝑛1
2𝑖𝜋𝑛S4DLin and
𝑎𝑛1
2𝑖𝑁
𝜋𝑁
𝑛11S4DInv for 𝑛12𝑁
For training and inference the continuoustime system in Eq 2is discretized at stepsize Δthrough a high
accuracy ZeroOrderHold ZOH or Bilinear method The ZOH method gives
𝑥𝑘𝐴𝑥𝑘1𝐵𝑢𝑘 𝑦𝑘𝐶𝑥𝑘𝐷𝑢𝑘 3
where𝑥10𝐴exp¹Δ𝐴º𝐵¹𝐴𝐼º𝐴1𝐵𝐶𝐶and𝐷𝐷 and expdenotes the matrix exponential Under
theassumptionthat 𝑢ctisconstantinbetweentimestampswhichcanbethoughtofasamodelingassumption
this numerical integration is exactJacquot 2019 Moreover note that all these discretization operations can
be quickly performed elementwise since 𝐴is diagonal
Some key diﬀerences It is worth pointing out a few structural and computational properties to highlight
some crucial diﬀerences between RNNs and SSMs
Since Eq 3is linear it can be eﬃciently parallelized until 𝑘𝐿1using parallel scans Martin and Cundy
2017 Smith et al 2022 unlike a nonlinear RNN where the computation has to be performed sequentially
WhileEq 3issimilartothelinearRNNcomputationitiscrucialtonotethata 𝐴and𝐵areparameterized
in a peculiar way prescribed by discretization and b these matrices share parameters in particular Δ
aﬀects both 𝐴and𝐵 These diﬀerences are critical as in SSMs learning is performed on the continuoustime
parameters 𝐴𝐵𝐶𝐷Δ hence parameterization choices directly aﬀect optimization
Unlike vanilla RNNs most SSMs use complexvalued diagonal recurrent matrices that are initialized deter
ministically using HiPPO theory and the literature attributes much of the success of SSMs to the speciﬁc
initialized used Gu et al 2021a 2022b Gupta et al 2022a
The points above motivate our investigation in this paper we consider the same architecture as Gu et al
2021a 2022a Smith et al 2022 but replace the SSM layer in the recurrent core by an RNN We then study
which steps need to be taken to gradually retrieve S4like performance on LRA Tay et al 2020 tasks The
eﬀectiveness of each of our steps is supported by empirical evidence and theoretical considerations and leads
to the architecture presented in Fig1
22 Experimental setup
In this paper we consider the Long Range Arena benchmark Tay et al 2020 a set of tasks designed to test
the ability of models to do longrange sequence modelling except we use coloured images instead of grayscale
imagesforthesequentialCIFAR10classiﬁcationtask Transformersfailtoperformwellonmostofthesetasks
2This version is most similar to S5 Smith et al 2022 but is here presented for ease of reasoning for a single discretization parameter
Δ shared across input dimensions For more details see B
4Resurrecting Recurrent Neural Networks for Long Sequences
while deep SSMs have shown remarkable performance on these tasks Dao et al 2022a Gu et al 2021a
This makes it an appropriate benchmark to explore the longrange modelling capabilities of deep RNNs
For all our experiments we use a network of 6 layers with residual connections and layerbatch normaliza
tion Ba et al 2016 Ioﬀe and Szegedy 2015 similar to Gu et al 2021a Fig1 and we replace the SSM
layers with RNN layers building up to our LRU recurrence in a sequence of steps see 3 All experiments are
repeated three times and we report the mean and standard error Networks are trained using the AdamW
optimizer Loshchilov and Hutter 2017 We use a smaller learning rate and no weight decay on the recurrent
parameters as suggested by Gu et al 2021a Steil 2004 We tune hyperparameters such as learning rates
for all models on a logarithmic grid for best accuracy See D for more details on our experimental setup
3 Designing Performant Deep RNNs
InthissectionwediscussthefundamentalstepsneededfordesigningRNNstoreachtheimpressiveperformance
of deep SSMs on the LRA benchmark We present these steps already outlined in the introduction in logical
order and support each claim with experimental evidence and theoretical considerations expanded in E
We consider the architecture of Fig1 where the recurrent computation is gradually modiﬁed starting from a
vanilla RNN We start by showcasing the advantage of using linear recurrences in 31 then in 32 we show
how to speedup training and inference without aﬀecting expressivity and initialization distribution In 33
we discuss how and why changing the parameterization and initialization distribution enables us to make the
RNN stable and improve longrange modeling Finally in 34 we ﬁnalize the LRU architecture by proposing a
normalization strategy for the hidden activations that results in a close match in performance with deep SSMs
31 Linear RNN layers are performant
One of the main ﬁndings of our work is that linear RNN layers can be surprisingly expressive when coupled
with nonlinear MLP or GLU Dauphin et al 2017 blocks outperforming tuned nonlinear RNN variants in
the same architecture In Tb1 we show that simply removing3the nonlinearity and therefore computing the
next state as 𝑥𝑘𝐴𝑥𝑘1𝐵𝑢𝑘 is able to improve test accuracy on most LRA tasks While the boost provided by
vanilla linear RNN blocks leads to performance which is still far behind S4 on some tasks sCIFAR PathFinder
and PathX this ﬁrst ﬁnding motivates us to drop nonlinearities in the recurrence for the rest of this paper In
later sections we leverage the linearity of the recurrence to signiﬁcantly speed up training as well as derive
principled initialization and normalization principles to learn longrange dependencies We note that on the
Text and Retrieval tasks performance using vanilla RNNs already matches performance of deep SSMs see
Tb3 for the performance of S4DS5 on these tasks
Resccscuscrscrscescnsccscesc sscCIFAR LiscssctscOpscssc Tescxsctsc Resctscrsciscescvscasclsc
RNNRescLU 697 02 376 80 880 01 885 01
RNNTascnschsc 699 03 439 01 872 01 889 02
RNNLiscnsc 722 02 504 02 891 01 891 01
Table 1jThe eﬀect of removing the nonlinearity from the recurrent unit on test accuracy 31 We show here
resultsonlyforthesCIFARListOps Textand RetrievaltasksinLRA asthesemodelsdidnotexceedrandomguessing
on PathFinderPathX further improvements in Tb2 and 3 Performance of deep SSMs shown in Tb3
The empirical result in Tb1 is surprising  since recurrent nonlinearities are believed to be a key component
for the success of RNNs  both in the theory and in practice Erichson et al 2021 Pascanu et al 2013
Siegelmann 2012 Indeed a strong property of singlelayer sigmoidal and tanhRNNs is Turing completeness
which cannot be achieved by the linear variant Chung and Siegelmann 2021 However the architecture we
use Fig1 is deeper than a standard RNN and includes nonlinearies placed positionwise aftereach RNN
block In E1 we investigate how the expressivity and trainability of deep models is aﬀected by recurrent
3All other settings in the recurrent block are kept the same as in the Vanilla RNN module of Haiku Hennigan et al 2020 That is
all matrices have Glorot Glorot and Bengio 2010 initialization The rest of the architecture is kept as in Fig1 where the LRU block is
replaced by an RNN
5Resurrecting Recurrent Neural Networks for Long Sequences
nonlinearities Leveraging a spectral analysis and Koopman operator theory Koopman and Neumann 1932
we discuss how interleaving linear RNN layers with nonlinear feedforward blocks is suﬃcient to approximate
highlynonlinearsystems Akeyobservationinouranalysisisthatpositionwisenonlinearitieseﬀectivelytransfer
signal information to higher frequencies enabling the system to go beyond linearity in the spectral domain and
increasing the layer capacity To further strengthen our claim on the advantage of linear recurrences in E2
we show that while linear and nonlinear RNNs share an important class of approximating functionals linear
operators see Wang et al 2022 nonlinear activations can potentially slow down training
32 Using complex diagonal recurrent matrices is eﬃcient
We now show that we can signiﬁcantly speed up training and inference for deep linear RNNs without losing
performance by using complexvalued diagonal recurrent matrices While the idea of diagonalizing linear
systems for computational eﬃciency is a dominating feature of all deep SSMs since the introduction of DSS
by Gupta et al 2022a in this section we construct our diagonalized version to exactly match the initialization
spectrum see 321 of the Glorotinitialized deep linear RNN in Tb1 Our main purpose with this approach
is todisentangle the eﬀects of initialization and diagonalization on performance cf Tb2 and Tb3
Westartin321byrecallingsomeusefullinearalgebraelementsandthenproceedin322withadiscussion
on how to diagonalize the recurrence while preserving the eigenvalue spectrum at initialization
321 Linear RNN eigendecomposition
The recurrence 𝑥𝑘𝐴𝑥𝑘1𝐵𝑢𝑘can be unrolled easily using the assumption that 𝑥102ℝ𝑁
𝑥0𝐵𝑢0 𝑥 1𝐴𝐵𝑢 0𝐵𝑢1 𝑥 2𝐴2𝐵𝑢0𝐴𝐵𝑢 1𝐵𝑢2  𝑥𝑘𝑘1
𝑗0𝐴𝑗𝐵𝑢𝑘𝑗4
Exponentiations of the matrix 𝐴in the equation above are the source of the wellknown vanishingexploding
gradientissueinRNNsBengioetal1994Pascanuetal2013 WhileinnonlinearRNNsthestate 𝑥𝑘isforced
to live on the compact image of the activation function the hiddenstate of our linear variant can potentially
explode or vanish exponentially as 𝑘increases This phenomenon can be better understood by leveraging
an eigenvalue aka spectral analysis up to an arbitrarily small perturbation of the entries every matrix
𝐴2ℝ𝑁𝑁is diagonalizable4Axler 1997 ie one can write 𝐴𝑃Λ𝑃1 where𝑃2ℂ𝑁𝑁is an invertible
matrix and Λdiag¹𝜆1𝜆2𝜆𝑁º2ℂ𝑁𝑁 It is essential to note that unlike the symmetric setting where
eigenvaluesandeigenvectorsarerealinthenonsymmetriccase5onehastoallowfor complexentriestoachieve
full equivalence Plugging the decomposition 𝐴𝑃Λ𝑃1into Eq4and multiplying both sides by 𝑃1 we get
𝑥𝑘Í𝑘1
𝑗0Λ𝑗𝐵𝑢𝑘𝑗 where 𝑥𝑘𝑃1𝑥𝑘𝐵𝑃1𝐵 The output can then be computed as 𝑦𝑘𝐶𝑥𝑘¼𝐷𝑢𝑘2ℝ𝐻
where 𝐶𝐶𝑃1andwetaketherealpartof 𝐶𝑥𝑘 Thereforeinsteadoflearning ¹𝐴𝐵𝐶𝐷ºonecanequivalently
learn¹Λ𝐵𝐶𝐷º where Λ𝐵𝐶are complex valued and Λis a diagonal matrix
Are complex numbers really necessary We adopt complex numbers since they provide a convenient and
compact representation of nonsymmetric matrices in diagonal form However this is not the only option  one
could work almost as eﬃciently using real numbers We discuss how this can be achieved in E3
Stability Since 𝑥𝑘Í𝑘1
𝑗0Λ𝑗𝐵𝑢𝑘𝑗 the norm of component 𝑗of𝑥at timestamp 𝑘evolves such that j𝑥𝑘𝑗j
𝑂¹j𝑥𝑘𝑗jº𝑂¹j𝜆𝑗j𝑘º Therefore a suﬃcient condition to ensure stability ie 𝑥𝑘does not explode is therefore
j𝜆𝑗j1for all𝑗Gu et al 2021a
322 Learning in the diagonalized space
Learning recurrent linear systems in diagonal form provides substantial computational speedups both for
training and inference For example in our implementation of sCIFAR we found diagonal linear RNNs to be
8 times faster to train than a dense RNN with ReLUs matching the speed of our implementations of S4D
and S5 The main reasons for this computational beneﬁt are that a taking powers of diagonal matrices is
4In other words the set of nondiagonalizable matrices has measure zero see eg Zhinan 2002 for a proof idea
5Take eg𝐴¹¹01º¹10ºº The solution to the standard eigenvalue equation gives 𝜆𝑖 where𝑖is the imaginary unit
6Resurrecting Recurrent Neural Networks for Long Sequences
10
 05
 00 05 1010
05
000510
A is 2020
10
 05
 00 05 1010
05
000510
A is 100100
10
 05
 00 05 1010
05
000510
A is 500500
Figure2jEigenvaluesof 𝐴2ℝ𝑁𝑁followingGlorotinitialization eachentry
of𝐴is sampled independently from a Gaussian with mean 0 and variance
1𝑁 The eigenvalues are complex  𝐴is not symmetric and are represented
on the complex plane The black circle is the unit disk fj𝑧j1gℂ The
limit behavior uniform initialization is predicted by Thm 31
10
 05
 00 05 1010
05
000510
rmin04rmax09Figure 3jEigenvalues of a diago
nalmatrix 𝐴withentriessampled
using Lemma 32 For 𝑟min0
𝑟max1 the distribution coin
cideswithGlorotinit inthelimit
trivial speeding up both training and inference while exponentiating dense matrices is computationally
expensive and b while nonlinear recurrences must be computed sequentially unrolling a linear recurrence
can be parallelized using associative scans resulting in faster training Gupta et al 2022a Smith et al 2022
Equivalentinitialization Todisentanglethebeneﬁtsofdiagonallinearsystemsfromtheroleofinitialization
we seek an initialization for the diagonal system which keeps the eigenvalue spectrum of the recurrence
unchanged when comparing our diagonal system with the dense linear RNN in 31 where 𝐴followed Glorot
initialization Fortunately we can use a classical result from random matrix theory Ginibre 1965
Theorem 31 Strong circular law Let𝜇𝑁be the empirical spectral measure of 𝐴𝑁 where𝐴𝑁is a real𝑁𝑁
matrix with iid Gaussian entries each with zero mean and variance 1𝑁 Then𝜇𝑁converges weakly almost
surely as𝑁1to the uniform probability measure on fj𝑧j1gℂ
The theorem above illustrated in Fig2 shows that under Glorot initialization the spectrum of 𝐴isdefacto
sampled from the unit disk in ℂ This result motivates the strong performance of linear RNNs in 31 since it
impliesGlorotinitializationprovidesanapproximatelystableinitializationseedeﬁnitionin3216Moreover
from Theorem 31 an equivalent spectral initialization follows for the diagonal system which holds exactly
for the large width limit Λshould be diagonal with entries sampled uniformly on the unit disk Using the
deﬁnitionofexponentialofacomplexnumber exp¹𝜈𝑖𝜃º𝑒𝜈¹cos¹𝜃º𝑖sin¹𝜃ºº weadoptasimplescheme
for sampling uniformly on a ring in between circles with radii 𝑟minand𝑟maxinℂ
Lemma 32 Let𝑢1𝑢2be independent uniform random variables on the interval 01¼ Let 0𝑟min𝑟max1
Compute𝜈1
2log
𝑢1¹𝑟2
max𝑟2
minº𝑟2
min
and𝜃2𝜋𝑢2 Then exp¹𝜈𝑖𝜃ºis uniformly distributed on the ring
inℂbetween circles of radii 𝑟minand𝑟max
We recover the spectrum of Glorotinitialization in the limit of inﬁnite width by setting 𝑟𝑚𝑖𝑛0and𝑟𝑚𝑎𝑥1
wewillexploretuningthesehyperparametersin33 Tb2ﬁrsttworowsshowstheresultsoflearningdeep
linear RNNs in complex diagonal form7where each diagonal entry of Λis initialized uniformly on unit disk in
ℂusing Lemma 32 with 𝑟min𝑟max¼01¼ In our experiments 𝐵𝐶which we rename for convenience back
to𝐵and𝐶 follow Glorot initialization for both real and imaginary parts parameterized separately with
halved variance in each component to preserve lengths on the inputoutput projections Glorot and Bengio
2010 Finally after the SSM computation the real part of the signal is kept and the imaginary discarded as
in Gu et al 2022a Gupta et al 2022a
Our results in Tb2 show that diagonalizing the recurrence surprisingly improves accuracy on tasks like ListOps
and sCIFAR More importantly it drastically reduces training and inference time on all LRA tasks see Tb4 in
C1 for training speed comparisons and makes the RNN just as fast to train as deep SSMs like S4D and S5
6Later in training the system is less likely to become unstable if the learning rate is small enough
7To avoid issues with backpropagation on complex variables each complex parameter in the network is stored and learned as a pair of
ﬂoats encoding real and imaginary parts
7Resurrecting Recurrent Neural Networks for Long Sequences
sscCIFAR LiscssctscOpscssc Pasctschscfsciscnscdscescrsc
Descnscsscesc𝐴 722 02 504 02 
ΛRescasclsc  Imsc 865 01 588 03 
ΛExscpsc 854 07 605 03 654 90
ΛStscascbsclscesc Exscpsc 872 04 594 03 935 05
 Riscnscgsc Inscisctsc 881 00 594 03 944 03
Table 2jTest accuracy of a linear diagonal complex RNNs under diﬀerent parametrizations of the transition
matrix see 32 Performance directly improves the results in Tb1 and showcases the advantage of exponen
tial polar representation of Λ In bold font is the best parametrization option for linear RNN blocks Ring Init
denotes a changed initialization where 𝑟minand𝑟maxare tuned Performance on the Text and Retrieval tasks is not
shown as linear RNNs already align with S4 results cf Tb1 with Tb3 These models cannot solve PathX yet and
requires normalizing the hidden activations and initializing the eigenvalues of Λwith small phase see Tb3
33 Beneﬁts of stable exponential parameterization
In 32 we showed that moving to complex diagonal recurrences is computationally eﬃcient However we
also observed that learning the diagonal model can be more unstable than learning the dense model in some
experiments To learn longrange dependencies and avoid quickly vanishing gradients eigenvalues in the
recurrenceneedtohavemagnitudecloseto1Guetal2022bGuptaetal2022ahowevertheseeigenvalues
are also likely to make the system unstable during training In this section we show the beneﬁts of a stable
parameterization of the RNN and of tuning 𝑟minand𝑟maxsee Lemma 32
Optimization under exponential parameterization Lemma32suggestsanaturalparameterizationofthe
diagonalized RNN as Λdiag¹exp¹𝜈𝑖𝜃ººwith𝜈2ℝ𝑁and𝜃2ℝ𝑁as the learnable parameters instead
of the real and imaginary parts of Λ As we explain in E2 leveraging an easytovisualize 2dimensional
example see Fig8 this choice decouples magnitude and oscillation frequencies making optimization with
Adam easier The positive eﬀects of this exponential parametrization which resembles some features of ZOH
discretization see 2 and 4 and notably takes the performance of PathFinder above random chance can be
observed in the third row of Tb2
Enforcing stability An important beneﬁt of the exponential parameterization is that it makes it simple to
enforce stability on the eigenvalues To see this note that at initialization j𝜆𝑗jjexp¹𝜈𝑗ºj1since𝜈𝑗0
Therefore to preserve stability during training we can use an exponential or another positive nonlinearity
𝜆𝑗exp¹exp¹𝜈log
𝑗º𝑖𝜃𝑗º where𝜈log2ℝ𝑁is the parameter we optimize and we set 𝜈log
𝑗log¹𝜈ºat
initialization Note that a similar idea is used in deep SSMs Gu et al 2021a in the context of discretization
We choose an exponential nonlinearity over a simple ReLU nonlinearity to increase granularity around j𝜆j1
achieved at 𝜈log1whilej𝜆j0is achieved at 𝜈log1 Stable parameterization helps on most LRA tasks
In the fourth row of Tb2 we show its eﬀects on sCIFAR ListOps and Pathﬁnder We observe the most drastic
improvement on Pathﬁnder one of the harder longrange dependency tasks in LRA where performance now
reaches above 93
The beneﬁts of the stable parameterization becomes more apparent when we explore the idea of initializing
the eigenvalues of Λon a ring closer to the unit disk increasing 𝑟mincloser to 1in Lemma 32 to bias the
network towards longerrange interactions and avoid vanishing gradients Indeed as discussed in detail in Gu
et al 2022b Gupta et al 2022a for reasonings requiring consideration of interactions between distant
tokens eigenvalues in the recurrence need to have magnitude close to 1 Otherwise as clear from the diagonal
version of Eq 4 when taking powers of eigenvalues close to the origin the signal from past tokens quickly
dies out see 321 As we show in the last row of Tb5 in C without enforcing stability performance starts
to degrade as we increase 𝑟maxpast 09 in the sCIFAR task With stability enforced we can increase 𝑟maxup to
099 and improve performance We see similar beneﬁts on the other tasks where we sweep diﬀerent values of
𝑟minand𝑟maxTbs7  8 have more details Finally note that while here we explore changing the magnitude
of the eigenvalues of Λ in 34 we also show the beneﬁts of initializing the eigenvalues to have a small phase
to learn more global patterns useful for particularly longrange reasoning tasks
8Resurrecting Recurrent Neural Networks for Long Sequences
sscCIFAR LiscssctscOpscssc Tescxsctsc Resctscrsciscescvscasclsc Pasctschscfsciscnscdscescrsc PasctschscX
LRU 890 01 602 08 894 01 899 01 951 01 942 04
S4D oscuscrsc rscescpscrscoscdsc 915 02 602 03 864 00 895 00 942 03 975 00
S5 oscuscrsc rscescpscrscoscdsc 888 01 585 03 862 01 889 00 957 01 960 01
S4 pscascpscescrsc rscescsscusclsctscssc 911 596 868 909 942 964
S4DLescgscS pscascpscescrsc rscescsscusclsctscssc 899 605 862 895 931 919
S5 pscascpscescrsc rscescsscusclsctscssc 901 622 893 914 953 986
Table 3jPerformance after adding the 𝛾normalization to the diagonal RNN with stable exponential parameter
ization and initialization on the ring see 34 For PathX we additionally use a smaller eigenvalue phase at
initialization We name this architecture LRU We sweep 𝑟minand𝑟maxfor setting the initialization distribution
and the learning rate We also report results from S4S4DS5 along with reproductions in our own pipeline with
similar hyperparameter sweeps as our RNN models LRU reaches similar performance as these deep SSMs on all
LRA tasks
34 Additional considerations for longrange reasoning tasks
Up to this point our model did not succeed in learning PathX  the hardest dataset in our benchmark with a
sequence length of 16𝑘tokens In this section we discuss the additional modiﬁcations we need to make to
improve our models ability to learn very longrange dependencies and ﬁnalize our LRU model
Normalization In 33 we initialized the eigenvalues of Λclose to the unit disk for better performance on
longrangetasks Howeverweobservedthataswemoved 𝑟minand𝑟maxcloserto1thetraininglossalsostarted
to blow up at initialization see Fig5 In this section we ﬁrst present a result explaining this phenomenon
before deriving a practical normalization scheme for the hidden activations to tackle this problem and further
improve performance
Proposition 33 Forwardpass blowup LetΛbe diagonal with eigenvalues sampled uniformly on the ring in
ℂbetween circles of radii 𝑟min 𝑟max1 Then under constant or whitenoise input and Glorot input projection
we have that the squared norm of the state 𝑥𝑘converges as 𝑘1to the following quantity
𝔼k𝑥1k2
2¼1
𝑟2max𝑟2
minlog 
1𝑟2
min
1𝑟2max
𝔼k𝐵𝑢k2
2¼
This result has the following intuitive form if 𝑟min𝑟max𝑟 if we initialize 𝜌close to the unit disk the
forward pass blows up by a factor 1𝜌since the contributions from previous states take longer to decay let
𝜖𝑟2
max𝑟2
minand𝜌1𝑟2
max then
lim
𝜖0𝔼k𝑥1k2
2¼
𝔼k𝐵𝑢k2
2¼lim
𝜖01
𝜖log
1𝜖
𝜌
lim
𝜖01
𝜖𝜖
𝜌𝑂¹𝜖2º
1
𝜌1
1𝑟2 5
Towards the derivation of an eﬀective normalization scheme for the forward pass we present a simpliﬁed
derivation of the 1𝜌gain formula for the onedimensional setting under whitenoise input8 letΛ𝜆2ℂ
and𝐵1 Let𝑝denote the conjugate of 𝑝2ℂ we have thatj𝑝j2𝑝𝑝and in expectation over the input
using Eq4 and the fact that 𝔼𝑢𝑘𝑖𝑢𝑘𝑗¼0for𝑖𝑗
𝔼j𝑥𝑘j2 𝑘1
𝑖0𝜆𝑖𝔼𝑢𝑘𝑖¼

𝑘1
𝑗0𝜆𝑗𝔼𝑢𝑘𝑗¼ª

𝑘1
𝑖𝑗0𝜆𝑖¹𝜆𝑗º𝔼𝑢𝑘𝑖𝑢𝑘𝑗¼𝑘1
𝑖0j𝜆j2𝑖11
1j𝜆j2 6
Since the formula above holds for every Euclidean direction in our recurrence  Λis diagonal we can add a
normalization parameter that is initialized elementwise Additionally note that as 𝜆approaches 1 1j𝜆j2
8We use the random input assumption for our normalization scheme as we found it to work well in practice
9Resurrecting Recurrent Neural Networks for Long Sequences
PathFinderPathX
Figure 4jEvolution of 𝑥2ℝ3under impulse input 𝑢¹100 0º2ℝ16𝑘 Plotted in diﬀerent colors are the
3 components of 𝑥Λhas parameters 𝜈𝑗000005and𝜃𝑗sampled uniformly in 02𝜋¼or with small phase
0𝜋50¼ For small sequences such as 𝐿1024PathFinder sCIFAR 02𝜋¼produces kernels with acceptable
overall number of oscillations information about 𝑢0is recalled only a few times in the overall state history Instead
for high𝐿 the range of the imaginary part at initialization has to be smaller to obtain a similar eﬀect
0 20 40 60 80 100
Training Iterations x100022
21
2021Training LossNo Normalization
  Normalization
 Small Phase Init
0 20 40 60 80 100
Training Iterations x10005060708090Training AccuracyNo Normalization
  Normalization
 Small Phase Init
0 20 40 60 80 100
Training Iterations x10005060708090T est AccuracyNo Normalization
  Normalization
 Small Phase Init
Figure 5jEﬀect of normalization and using a small phase at initialization on the PathX task For each setting
we show mean and standard errors over three independent runs for 100k iterations Without normalization the
model presents higher loss values at initialization and quickly converges to a suboptimal value where train and test
accuracy are both at random chance Adding normalization helps the train loss is lower at initialization and the
optimizer is able to escape the suboptimal region and train accuracy also increases Interestingly this model still
fails to generalize at all Finally reducing initialization phase ie tuning the range of 𝜃 dramatically improves
convergence on the training set while also generalizing to the test set
approaches 0 making further adaptations with SGD of this parameter hard Therefore we use normalization
parameter 𝛾log2ℝ𝑁 initialized elementwise as 𝛾log
𝑖 log¹
1j𝜆𝑖j2º9and modify the recurrence as
𝑥𝑘Λ𝑥𝑘1exp¹𝛾logº¹𝐵𝑢𝑘º 7
wheredenotes the elementwise product The 𝛾parameter allows the RNN to adaptively scale the input fed
into the corresponding eigendirection We found the 𝛾normalization to consistently improve performance on
tasks that beneﬁt from initializing close to the unit disk such as sCIFAR and Pathﬁnder as shown in Tb3
Reducing Eigenvalue Phase at Initialization In the context of the diagonalized recurrence we have Λ
diag¹exp¹exp¹𝜈logº𝜃ººwhere𝜈log2ℝ𝑁isthevectoroflogeigenvaluemagnitudesand 𝜃2ℝ𝑁thevectorof
eigenvalue phases While𝜈logencodes the distance to the origin 𝜃is the angle from the vector 10𝑖For long
sequences  initializing uniformly 𝜃02𝜋¼implies that most state entries will exhibit an overall large number
of oscillations at initialization see upper panel in Fig4 Equivalently in this setting most state dimensions
are the result of convolutions10capturing an average of local oscillation patterns  This behavior is independent
from the ability of capturing longrange dependencies controlled by 𝜈log but pertains to the nature of the
information stored by the RNN Therefore we claim that initializing Λwith uniform phase on long sequence
data inherently biases the network towards learning spurious features in the input sequence The model cannot
recover from this suboptimal initialization we indeed observe that for our best to far model on PathX the
9We also tried setting 𝛾𝑖to
1j𝜆𝑖j2in each training iteration and found it to work similarly in practice to a trainable 𝛾
10See Gu et al 2022a for a discussion of kernel perspectives
10Resurrecting Recurrent Neural Networks for Long Sequences
training loss after a few iterations converges to a highly suboptimal minimizer which leads to random chance
test performance see Fig5 To ﬁx this issue we found it suﬃcient to restrict the range of 𝜃to a thin slice
around 0 biasing the model towards learning more global features Since the optimal values of 𝜃are small we
parameterize the phase logarithmically 𝜃exp¹𝜃logº where𝜃logis optimized to aid optimization
Restricting the range of the phase at initialization to be 0𝜋10¼ our LRU achieved 942on PathX aligning
with stateoftheart deep SSMs We did not explore using a smaller phase at initialization for the other LRA
tasks although we believe this might further improve performance on other tasks as well Note that using both
𝛾normalization and restricting the eigenvalue phase at initialization were crucial to solving PathX We were
unable to learn when using restricted phase at initialization without also introducing 𝛾normalization
With all the components of 3 taken together we name this new model the Linear Recurrent Unit orLRUfor
short It provides a ﬂexible interpretable and principled framework for initializing and learning deep RNNs
eﬃciently and matches performance and eﬃciency of deep SSMs across all LRA tasks as shown in Tb3
4 Insights on S4 and Variants
We believe our ablations in 3 explain the underlying mechanisms driving the success of deep SSMs Hence
to conclude the paper in this section we inspect in detail the main similarities and diﬀerences between our
LRU model and diagonal SSMs and elaborate a few insights As in 2 to avoid technicalities we provide
a simpliﬁed discussion capturing the main features of models stemming from the original S4 paper For a
comparison of diﬀerent models we defer the reader to B
As detailed in 2 diagonal SSMs DSS S4D S5 are instantiated and parameterized through discretization
of a latent continuoustime model 𝑥ct¹𝑡º𝐴𝑥ct¹𝑡º𝐵𝑢ct¹𝑡º where𝐴diag¹𝑎ºis initialized with complex
entries often prescribed or inspired by HiPPO theory Gu et al 2020 ZeroOrderHold ZOH discretization
with stepsize Δleads to the recurrence 𝑥𝑘exp¹Δ𝐴º𝑥𝑘1¹exp¹Δ𝐴º𝐼º𝐴1𝐵𝑢𝑘 This formula while arguably
complex compared to our Eq7 relates to it as outlined in the next paragraphs
Matrix exponentials make training easier The exponential in the ZOH formula is due to exact integration
of𝑥ct¹𝑡º𝐴𝑥ct¹𝑡º which leads to 𝑥ct¹Δ𝑘ºexp¹Δ𝐴º𝑥ct¹Δ¹𝑘1ºº In addition to enforce stability in models
inspired by S4 the real part of 𝐴is often fed into a positive nonlinearity as we also do in 33 From our results
33 and our discussion on optimization advantages see also E2 we claim that the power of exponential
parameterization is not necessarily attributable to accurate integration which is not present in our system
but is more fundamentally rooted in a magnitudephase decoupling on the recurrence this makes training
with Adam easier see Fig8 as well as in the overall advantage of learning in diagonalized space see Tb2
We also note that stabilizing the recurrence by adding a nonlinearity was beneﬁcial also in our experiments
although this is not prescribed by the theory underlying S4
Structured initialization is not necessary While Gu et al 2022a Gupta et al 2022b Smith et al
2022 also discuss initializations for 𝐴deviating from the HiPPO structure see 2 and B to the best of
our knowledge we are the ﬁrst to show that simple uniform initialization on a slice of the unit disk combined
with proper normalization is able to also solve the hardest task in LRA PathX11We also show Tb2 that
uniform initialization on the disk which is simply the diagonalized version of Glorot initialization Thm 31
is suﬃcient to achieve performance close to more complex deep statespace models on the remaining LRA
tasks Our results ultimately suggest that HiPPO theory while fundamental for the development of this ﬁeld
should not be thought of as the main source of S4 success
Discretization changes initialization spectrum For simplicity let us restrict our attention to S4DLin for
which𝐴diag¹𝑎ºwith 𝑎𝑛1
2𝑖𝜋𝑛 yielding a diagonal transition matrix with elements ie eigenvalues
initialized at exp¹Δ2𝑖𝜋Δ𝑛º Under typical choices eg Δ1𝑒3𝑁128 the SSM eigenvalues have
magnitude exp¹Δ2º09995 and phase 𝜃𝜋Δ𝑛20𝜋8¼ ie initialization is performed on a ring12
close to the unit circle in ℂ with restricted phase connected to the eigenvalues magnitude As is clear from
11Among the models in Gu et al 2022a only S4Dinv and S4DLegS options heavily inspired by the HiPPO theory perform beyond
random guessing on PathX In S5 the skewsymmetric component of the HiPPO matrix is used for initialization
12For all diagonal SSMs Δis actually a vector initialized in the range ΔminΔmax¼ This interval can be directly mapped through the
exponential map to a ring in complex space see Lemma 32
11Resurrecting Recurrent Neural Networks for Long Sequences
the results in 33 and 34 linking the eigenvalues phase and magnitude is not necessary to achieve good
performance indeed as it can be seen in Tb3 test accuracy on the Long Range Arena except PathX can be
recovered by using a more natural magnitudeindependent initialization on the complete ring As we discussed
in 34 changing the initialization phase to a small range around 0can be motivated by ﬁrst principles yet is
only needed for extremely long sequences this modiﬁcation is already hardcoded in S4 where choosing a
small Δalso shrinks the phase13However our results clearly show that connecting real and imaginary parts
during training through the Δparameter is not necessary to achieve good performance even on PathX
Discretizationperformsnormalization ThemoststrikingvisualdiﬀerencebetweenoursandZOHdiscretized
S4 recurrence is in the matrix multiplier for 𝑢𝑘¹exp¹Δ𝐴º𝐼º𝐴1𝐵 After conducting experiments on S4D we
found that simply replacing this multiplier with its ﬁrstorder expansion in Δ ieΔ𝐵 yields a close match in
performance For input dimension 𝐻1and unit𝐵2ℝ𝑁1to keep reasoning simple the corresponding
recurrence is 𝑥𝑘exp¹Δ𝑎ºΔ1𝑁𝑢𝑘 Elementwise unrolling of this recurrence  without the Δin front of 𝑢
yieldsj𝑥𝑘𝑖jÍ𝑘1
𝑗0jexp¹Δ𝑎𝑖ºj𝑗𝑢𝑘𝑗𝑖 which in the limit 𝑘1gives𝑂¹Δ1º Therefore the Δmultiplier in front
of𝐵eﬀectively scales the recurrence to avoid blowups  similar to our 𝛾normalization factor
Parameter sharing is not necessary As a result of discretization the Δparameter multiplying both 𝐴and𝐵
couples the recurrence formula with the input projection during training In our S4 ablations we found that
decoupling these in two separate parameters  keeping the same initialization to guarantee no blowups see
last paragraph  does not decrease performance suggesting that the ODE discretization viewpoint which
induces parameter sharing is not necessary to achieve S4 performance
From this discussion we conclude that the success of diagonal statespace models is attributable to the
use of linear recurrences and complex diagonal exponential matrices combined with the normalization and
initialization induced by discretization On the other hand other artifacts of discretization such as parameter
sharing or the continuoustime interpretation do not necessarily contribute to its performance
5 Conclusion
In this paper we introduce a new RNN layer called the Linear Recurrent Unit or LRU and show how it can be
eﬀectively and eﬃciently used as core layers of deep sequence models We provide theoretical insights and
extensive ablations on a series of stepbystep modiﬁcations of a vanilla RNNlinearization diagonalization
stable exponential parameterization and normalizationthat substantially improve performance especially on
tasks requiring long range reasoning While our recurrence shares similarities with modern deep SSMs our
design does not rely on discretization of a latent continoustime system or on structured transition matrices
Instead our improvements directly follow from initialization and forward pass analysis arguments standard in
thedeeplearningcommunitystartingfromaGlorotinitializedRNNs Ourﬁnalmodelmatchestheperformance
of modern deep statespace models eg S4 or S5 on all LRA tasks
Acknowledgements
The authors would like to thank Michalis Titsias Aleksandar Botev James Martens and Yee Whye Teh for the
interesting discussions and perspectives on our work
13This is a useful eﬀect of having a latent continuoustime model choosing eigenvalues close to the unit circle ie small Δ changes
the oscillation frequencies in the discretized system
12Resurrecting Recurrent Neural Networks for Long Sequences
References
M Arjovsky A Shah and Y Bengio Unitary evolution recurrent neural networks In International conference
on machine learning  PMLR 2016
S Axler Linear algebra done right  Springer Science  Business Media 1997
J L Ba J R Kiros and G E Hinton Layer normalization arXiv preprint arXiv160706450  2016
S Bai J Z Kolter and V Koltun An empirical evaluation of generic convolutional and recurrent networks for
sequence modeling arXiv preprint arXiv180301271  2018
Y Bengio P Simard and P Frasconi Learning longterm dependencies with gradient descent is diﬃcult IEEE
transactions on neural networks  1994
N Bordin C Dallago M Heinzinger S Kim M Littmann C Rauer M Steinegger B Rost and C Orengo
Novel machine learning approaches revolutionize protein knowledge Trends in Biochemical Sciences  2022
J Bradbury R Frostig P Hawkins M J Johnson C Leary D Maclaurin G Necula A Paszke J VanderPlas
S WandermanMilne et al JAX composable transformations of python numpy programs 2018
T B Brown B Mann N Ryder M Subbiah J Kaplan P Dhariwal A Neelakantan S Shyam G Sastry
A Askell et al Language models are fewshot learners arXiv preprint arXiv200514165  2020
K Cho B Van Merriënboer D Bahdanau and Y Bengio On the properties of neural machine translation
Encoderdecoder approaches arXiv preprint arXiv14091259  2014a
KChoBVanMerriënboerCGulcehreDBahdanauFBougaresHSchwenkandYBengio Learningphrase
representationsusingrnnencoderdecoderforstatisticalmachinetranslation arXivpreprintarXiv14061078 
2014b
S Chung and H Siegelmann Turing completeness of boundedprecision recurrent neural networks Advances
in Neural Information Processing Systems  2021
T Dao D Y Fu S Ermon A Rudra and C Ré Flashattention Fast and memoryeﬃcient exact attention with
ioawareness arXiv preprint arXiv220514135  2022a
T Dao D Y Fu K K Saab A W Thomas A Rudra and C Ré Hungry hungry hippos Towards language
modeling with state space models arXiv preprint arXiv221214052  2022b
Y N Dauphin A Fan M Auli and D Grangier Language modeling with gated convolutional networks In
International conference on machine learning  PMLR 2017
S De and S Smith Batch normalization biases residual blocks towards the identity function in deep networks
Advances in Neural Information Processing Systems  2020
A Dosovitskiy L Beyer A Kolesnikov D Weissenborn N Houlsby S Gelly X Zhang and J Uszkoreit An
image is worth 16x16 words Transformers for image recognition at scale arXiv preprint arXiv201011929 
2020
J L Elman Finding structure in time Cognitive science  1990
N B Erichson O Azencot A Queiruga L Hodgkinson and M W Mahoney Lipschitz recurrent neural
networks In International Conference on Learning Representations  2021
J Ginibre Statistical ensembles of complex quaternion and real matrices Journal of Mathematical Physics 
1965
X Glorot and Y Bengio Understanding the diﬃculty of training deep feedforward neural networks In
Proceedings of the thirteenth international conference on artiﬁcial intelligence and statistics  JMLR Workshop
and Conference Proceedings 2010
K Goel A Gu C Donahue and C Ré Its raw audio generation with statespace models arXiv preprint
arXiv220209729  2022
13Resurrecting Recurrent Neural Networks for Long Sequences
A Gu T Dao S Ermon A Rudra and C Ré Hippo Recurrent memory with optimal polynomial projections
Advances in Neural Information Processing Systems  2020
A Gu K Goel and C Re Eﬃciently modeling long sequences with structured state spaces In International
Conference on Learning Representations  2021a
A Gu I Johnson K Goel K Saab T Dao A Rudra and C Ré Combining recurrent convolutional and
continuoustime models with linear state space layers Advances in neural information processing systems 
2021b
A Gu A Gupta K Goel and C Ré On the parameterization and initialization of diagonal state space models
arXiv preprint arXiv220611893  2022a
A Gu I Johnson A Timalsina A Rudra and C Ré How to train your hippo State space models with
generalized orthogonal basis projections arXiv preprint arXiv220612037  2022b
A Gupta A Gu and J Berant Diagonal state spaces are as eﬀective as structured state spaces In Advances in
Neural Information Processing Systems  2022a
A Gupta H Mehta and J Berant Simplifying and understanding state space models with diagonal linear
rnnsarXiv preprint arXiv221200768  2022b
R Hasani M Lechner A Amini D Rus and R Grosu Liquid timeconstant networks In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence  2021
R Hasani M Lechner TH Wang M Chahine A Amini and D Rus Liquid structural statespace models
arXiv preprint arXiv220912951  2022
K Helfrich D Willmott and Q Ye Orthogonal recurrent neural networks with scaled cayley transform In
International Conference on Machine Learning  PMLR 2018
T Hennigan T Cai T Norman and I Babuschkin Haiku Sonnet for JAX 2020 URL httpgithubcom
deepminddmhaiku 
S Hochreiter Untersuchungen zu dynamischen neuronales netzen Diploma thesis Institut fur Informatik
Technische Universitat Munchen  1991
S Hochreiter and J Schmidhuber Long shortterm memory Neural computation  1997
JJHopﬁeldNeuralnetworksandphysicalsystemswithemergentcollectivecomputationalabilities Proceedings
of the national academy of sciences  1982
S L Hyland and G Rätsch Learning unitary operators with help from u n In ThirtyFirst AAAI Conference
on Artiﬁcial Intelligence  2017
SIoﬀeandCSzegedy Batchnormalization Acceleratingdeepnetworktrainingbyreducinginternalcovariate
shift InInternational Conference on Machine Learning  2015
M M Islam and G Bertasius Long movie clip classiﬁcation with statespace video models In ECCV 2022 
Springer 2022
R G Jacquot Modern digital control systems  Routledge 2019
H Jeﬀreys The theory of probability  OUP Oxford 1998
L Jing Y Shen T Dubcek J Peurifoy S Skirlo Y LeCun M Tegmark and M Soljačić Tunable eﬃcient
unitaryneuralnetworkseunnandtheirapplicationtornns In InternationalConferenceonMachineLearning 
PMLR 2017
J Jumper R Evans A Pritzel T Green M Figurnov O Ronneberger K Tunyasuvunakool R Bates A Žídek
A Potapenko et al Highly accurate protein structure prediction with alphafold Nature 2021
E Kaiser J N Kutz and S L Brunton Datadriven discovery of koopman eigenfunctions for control Machine
Learning Science and Technology  2021
14Resurrecting Recurrent Neural Networks for Long Sequences
N Kalchbrenner L Espeholt K Simonyan A v d Oord A Graves and K Kavukcuoglu Neural machine
translation in linear time arXiv preprint arXiv161010099  2016
J Kilian and H T Siegelmann The dynamic universality of sigmoidal neural networks Information and
computation  1996
D P Kingma and J Ba Adam A method for stochastic optimization arXiv preprint arXiv14126980  2014
B O Koopman and J v Neumann Dynamical systems of continuous spectra Proceedings of the National
Academy of Sciences  1932
M Korda and I Mezić On convergence of extended dynamic mode decomposition to the koopman operator
Journal of Nonlinear Science  2018
M Korda and I Mezić Koopman model predictive control of nonlinear dynamical systems In The Koopman
Operator in Systems and Control  Springer 2020
VRKosticPNovelliAMaurerCCilibertoLRosascoandmassimilianopontil Learningdynamicalsystems
via koopman operator regression in reproducing kernel hilbert spaces In Advances in Neural Information
Processing Systems  2022
J N Kutz S L Brunton B W Brunton and J L Proctor Dynamic mode decomposition datadriven modeling of
complex systems  SIAM 2016
Q V Le N Jaitly and G E Hinton A simple way to initialize recurrent networks of rectiﬁed linear units arXiv
preprint arXiv150400941  2015
J LeeThorp J Ainslie I Eckstein and S Ontanon Fnet Mixing tokens with fourier transforms arXiv preprint
arXiv210503824  2021
M LezcanoCasado and D MartınezRubio Cheap orthogonal constraints in neural networks A simple
parametrizationoftheorthogonalandunitarygroup In InternationalConferenceonMachineLearning PMLR
2019
YLiTCaiYZhangDChenandDDey Whatmakesconvolutionalmodelsgreatonlongsequencemodeling
arXiv preprint arXiv221009298  2022a
ZLi JHan EWeinan andQLi Approximationandoptimizationtheoryforlinearcontinuoustimerecurrent
neural networks J Mach Learn Res  2022b
L Liu H Wang J Lin R Socher and C Xiong Mkd a multitask knowledge distillation approach for
pretrained language models arXiv preprint arXiv191103588  2019
I Loshchilov and F Hutter Decoupled weight decay regularization arXiv preprint arXiv171105101  2017
X Ma C Zhou X Kong J He L Gui G Neubig J May and L Zettlemoyer Mega moving average equipped
gated attention arXiv preprint arXiv220910655  2022
E Martin and C Cundy Parallelizing linear recurrent neural nets over sequence length arXiv preprint
arXiv170904057  2017
A Mauroy and I Mezić Global stability analysis using the eigenfunctions of the koopman operator IEEE
Transactions on Automatic Control  2016
A Mauroy Y Susuki and I Mezić Koopman operator in systems and control  Springer 2020
W S McCulloch and W Pitts A logical calculus of the ideas immanent in nervous activity The bulletin of
mathematical biophysics  1943
H Mehta A Gupta A Cutkosky and B Neyshabur Long range language modeling via gated state spaces
arXiv preprint arXiv220613947  2022
Z Mhammedi A Hellicar A Rahman and J Bailey Eﬃcient orthogonal parametrisation of recurrent neural
networks using householder reﬂections In International Conference on Machine Learning  PMLR 2017
15Resurrecting Recurrent Neural Networks for Long Sequences
T Mikolov M Karaﬁát L Burget J Cernock y and S Khudanpur Recurrent neural network based language
model In Interspeech  Makuhari 2010
R Nallapati B Zhou C Gulcehre B Xiang et al Abstractive text summarization using sequencetosequence
rnns and beyond arXiv preprint arXiv160206023  2016
E Nguyen K Goel A Gu G Downs P Shah T Dao S Baccus and C Ré S4nd Modeling images and videos
as multidimensional signals with state spaces In Advances in Neural Information Processing Systems  2022
A v d Oord S Dieleman H Zen K Simonyan O Vinyals A Graves N Kalchbrenner A Senior and
K Kavukcuoglu Wavenet A generative model for raw audio arXiv preprint arXiv160903499  2016
R Pascanu T Mikolov and Y Bengio On the diﬃculty of training recurrent neural networks In International
conference on machine learning  PMLR 2013
J L Proctor S L Brunton and J N Kutz Generalizing koopman theory to allow for inputs and control SIAM
Journal on Applied Dynamical Systems  2018
D E Rumelhart G E Hinton and R J Williams Learning internal representations by error propagation
Technical report California Univ San Diego La Jolla Inst for Cognitive Science 1985
P J Schmid Dynamic mode decomposition of numerical and experimental data Journal of ﬂuid mechanics 
2010
H T Siegelmann Neural networks and analog computation beyond the Turing limit  Springer Science 
Business Media 2012
J T Smith A Warrington and S W Linderman Simpliﬁed state space layers for sequence modeling arXiv
preprint arXiv220804933  2022
J J Steil Backpropagationdecorrelation online recurrent learning with o n complexity In 2004 IEEE
international joint conference on neural networks  IEEE 2004
A Surana Koopman operator based observer synthesis for controlaﬃne nonlinear systems In 2016 IEEE 55th
Conference on Decision and Control CDC  IEEE 2016
Y Tay M Dehghani S Abnar Y Shen D Bahri P Pham J Rao L Yang S Ruder and D Metzler Long range
arena A benchmark for eﬃcient transformers In International Conference on Learning Representations  2020
A Vaswani N Shazeer N Parmar J Uszkoreit L Jones A N Gomez Ł Kaiser and I Polosukhin Attention is
all you need Advances in neural information processing systems  2017
A Voelker I Kajić and C Eliasmith Legendre memory units Continuoustime representation in recurrent
neural networks Advances in neural information processing systems  2019
C R Vogel Computational methods for inverse problems  SIAM 2002
S Wang Z Li and Q Li The eﬀects of nonlinearity on approximation capacity of recurrent neural networks
2022
S H Weintraub Jordan canonical form theory and practice Synthesis Lectures on Mathematics and Statistics 
2009
M O Williams I G Kevrekidis and C W Rowley A datadriven approximation of the koopman operator
Extending dynamic mode decomposition Journal of Nonlinear Science  2015
S Wisdom T Powers J Hershey J Le Roux and L Atlas Fullcapacity unitary recurrent neural networks
Advances in neural information processing systems  2016
Z Zhinan The jordan canonical form of a rational random matrix Science Direct Working Paper  2002
T Zhou Z Ma Q Wen L Sun T Yao R Jin et al Film Frequency improved legendre memory model for
longterm time series forecasting arXiv preprint arXiv220508897  2022
16Resurrecting Recurrent Neural Networks for Long Sequences
Supplementary Materials
A Simpliﬁed Implementation of the Linear Recurrent Unit
We present here a simpliﬁed JAX implementation Bradbury et al 2018 of the Linear Recurrent Unit LRU
The state of the LRU is driven by the input ¹𝑢𝑘º𝐿
𝑘1of sequence length 𝐿according to the following formula and
eﬃciently parallelized using an associative scan 𝑥𝑘Λ𝑥𝑘1exp¹𝛾logº¹𝐵𝑢𝑘º and the output is computed
at each timestamp 𝑘as follows 𝑦𝑘𝐶𝑥𝑘𝐷𝑢𝑘 In our code 𝐵𝐶follow Glorot initialization with 𝐵scaled
additionallybyafactor2toaccountforhalvingthestatevariancebytakingtherealpartoftheoutputprojection
𝐷is random 𝐻dimensional and mutiplies elementwise each 𝑢𝑘 where𝑘is the timestamp Λis initialized with
the help of Lemma 32 with phase potentially restricted to a thin slice see 34
Resurrecting Recurrent Neural Networks for Long Sequences
Supplementary Materials
ASimpliﬁed Implementation of the Linear Recurrent Unit
We present here a simpliﬁed JAX implementation  Bradbury et al 2018  of the Linear Recurrent Unit LRU
The state of the LRU is driven by the input C9
91of sequence length according to the following formula and
eciently parallelized using an associative scan F9F91expWlog C9 and the output is computed
at each timestamp 9as follows G9F9C9 In our code  follow Glorot initialization with scaled
additionally by a factor 2 to account for halving the state variance by taking the real part of the output projection
is random dimensional and mutiplies elementwise each C9 where 9is the timestamp is initialized with
the help of Lemma 32 with phase potentially restricted to a thin slice see  341import jax
2import jaxnumpy asjnp
3import numpy asnp
4parallel_scan jaxlaxassociative_scan
5
6def forward lru_parameters input_sequence
7 Forward pass of the LRU layer Output y and input_sequence are of shape L H
8
9  All LRU parameters
10 nu_log theta_log B_re B_im C_re C_im D gamma_log lru_parameters
11
12  Materializing the diagonal of Lambda and projections
13 Lambda jnpexp jnpexpnu_log 1jjnpexptheta_log
14 B_norm B_re 1jB_im jnpexpand_dimsjnp expgamma_log axis 1
15 CC_re 1jC_im
16
17  Running the LRU  output projection
18  For details on parallel scan check discussion in Smith et al 2022
19 Lambda_elements jnprepeatLambda None  input_sequence shape 0 axis 0
20 Bu_elements jaxvmap lambda u B_norm uinput_sequence
21 elements Lambda_elements Bu_elements
22 _ inner_states parallel_scanbinary_operator_diag elements  all x_k
23 yjaxvmap lambda x u C xreal Duinner_states input_sequence
24
25 return y
26
27def init_lru_parameters N H r_min 0r _ m a x 1m a x _ p h a s e 628 
28 Initialize parameters of the LRU layer
29
30  N state dimension H model dimension
31  Initialization of Lambda is complex valued distributed uniformly on ring
32  between r_min and r_max with phase in 0 max_phase
33 u1nprandom uniformsize N
34 u2nprandom uniformsize N
35 nu_log nplog 05nplogu1 r_max 2r_min 2r_min 2
36 theta_log nplogmax_phase u2
37
38  Glorot initialized InputOutput projection matrices
39 B_re nprandom normalsize NH npsqrt 2H
40 B_im nprandom normalsize NH npsqrt 2H
41 C_re nprandom normalsize HN npsqrtN
42 C_im nprandom normalsize HN npsqrtN
43 Dnprandom normalsize H
44
45  Normalization factor
46 diag_lambda npexp npexpnu_log 1jnpexptheta_log
47 gamma_log nplognp sqrt 1npabsdiag_lambda 2
48
49 return nu_log theta_log B_re B_im C_re C_im D gamma_log
50
51def binary_operator_diag element_i element_j
52  Binary operator for parallel scan of linear recurrence
53 a_i bu_i element_i
54 a_j bu_j element_j
55 return a_j a_i a_j bu_i bu_j
17
17Resurrecting Recurrent Neural Networks for Long Sequences
B Related works
We ﬁrst discuss standard RNNbased approaches for sequencetosequence modeling and then provide a
historical overview on the progress of the literature stemming from the S4 paper Gu et al 2021a
Recurrent neural networks RNNs Before the rise of transformers Vaswani et al 2017 RNNs were
widely used in various applications of natural language processing tasks such as language modeling Mikolov
et al 2010 machine translation Cho et al 2014b and text summarization Nallapati et al 2016 The
modern RNN structure see Eq1 is mainly attributed to the works of Rumelhart et al 1985 However
it is possible to see the Hopﬁeld Networks as a particular form of RNN Hopﬁeld 1982 Modern RNN
formulations are also often related to the Elman Networks Elman 1990 The issue of vanishing or exploding
gradients as described by Bengio et al 1994 Pascanu et al 2013 is one barrier to training Recurrent
Neural Networks RNNs with gradient descent This problem limits the ability of RNNs to learn especially on
tasks with long input sequences One of the critical contributions to thesuccess of RNNswas the introduction of
gatingmechanismssuchastheLongShortTermMemoryLSTMproposedbytheHochreiterandSchmidhuber
1997 LSTMs address the vanishing gradients problem by introducing input output and forget gates which
enable the network to selectively remember or forget information from previous time steps Another popular
variant of gated RNNs is the Gated Recurrent Unit GRU Cho et al 2014b which simpliﬁes the LSTM
architecture by merging input and forget gates into a single update gate
Mitigating the vanishing gradient problem with orthogonal and unitary RNNs Recently Arjovsky et al
2016introducedunitaryevolutionRNNsuRNNwhereeigenvaluesintheRNNtransitionmatrixseeEq 1
are restricted to live on the unit circle The induced map driving the hidden state evolution therefore mixes
state components taking into account new inputs  but the signal from past timestamps is not exponentially
vanishingexploding as in the vanilla RNN case see discussion on stability in 321 This idea is powerful but
introduces two problems 1 choosing unitary transitions restricts the function approximation class and 2
training unitary matrices is expensive since a projection on the Stiefel manifold is required at each gradient
step Toresolvethesecondissuemanyworksdevotedattentiontocarefullydesignedreparameterizationofthe
transitionmatrixasegwiththeproductofsimplermatricesArjovskyetal2016GivensrotationsJingetal
2017HouseholderreﬂectionsMhammedietal2017orasexponentialsofskewsymmetricmatricesHyland
and Rätsch 2017 LezcanoCasado and MartınezRubio 2019 The approximation capacity of these models is
discussed and improved in Wisdom et al 2016 A further step in designing eﬃcient orthogonal RNNs is
provided by Helfrich et al 2018 who parametrized skewsymmetric matrix using the Cayley transforms
resulting in a fully real parameter space Other works which proposed conceptually diﬀerent solutions to
mitigate the vanishing gradient problem include combinations with rectiﬁed linear units Le et al 2015
Lipschitz RNNs Erichson et al 2021 and approaches based on dilated convolutions to increase context
size Bai et al 2018 Oord et al 2016
Deep statespace models SSMs a historical overview Inspired by interesting approaches involving
continuoustime representation for recurrent neural networks Voelker et al 2019 Gu et al 2020 recently
provided an alternative view on the vanishing gradient problem one can design linearcontinuoustime state
space models SSMs of the form 𝑥¹𝑡º𝐴𝑥¹𝑡º𝐵𝑢¹𝑡ºwhere the state 𝑥¹𝑡º2ℝ𝑁is guaranteed to compress all
relevant under a certain metric information about previously observed onedimensional inputs 𝑢¹0𝑡¼º
For instance by using speciﬁc pair of matrices ¹𝐴2ℝ𝑁𝑁𝐵2ℝ𝑁1º one can discretize the continuoustime
SSM above using a stable accurate integrator eg bilinear or zeroorderhold and retrieve the hidden state
𝑥¹𝑡º which contains the coeﬃcients for the best 𝑁th degree polynomial approximation to 𝑢¹0𝑡¼º The idea
of Gu et al 2020 was to then use the resulting discretized structured ie using structured HiPPO matrices
statespace model as a starting for the design and initialization of a novel gated RNN
Later Gu et al 2021a scaled up this idea into a deep architecture where a collection one for each input
dimension of discretized continuoustime structured SSM was placed at each layer as a substitute14for the
attention block in an attempt to mitigate the 𝑂¹𝐿2ºissue in transformers and provide a theoretically principled
component for sequencetosequence modeling The model reached stateoftheart on the Long Range Arena
benchmarkTayetal2020eﬀectivelyshowcasingthepowerofdiscretizedlinearrecurrencesusingstructured
14This idea is also leveraged in FNet LeeThorp et al 2021 where the attention mechanism is replaced with a simpler linear
tokenmixing strategy
18Resurrecting Recurrent Neural Networks for Long Sequences
transition matrices Notably the resulting model named S4 uses a convenient and stable representation of
the HiPPO transition which is initialized using a normal  lowrank matrix and then learned eﬃciently in
diagonal  lowrank form using fast Fourier transforms FFTs and Cauchy kernels
In the months following the publication of S4 Gupta et al 2022a noticed that most of S4 performance can be
retrieved by only considering the diagonal component of the HiPPO matrix and therefore showed the power
of discretized diagonal structured continuoustime state space models This architecture is known as DSS
As the interest of the community was rising with ﬁrst applications of DSS and S4 in language Mehta et al
2022 vision Nguyen et al 2022 and audio Goel et al 2022 Gu et al 2022a further simpliﬁed DSS
providing a diagonal form  S4D with theoretical guarantees in the inﬁnite width setting Notably Gu et al
2022a showed that to retrieve most performance of S4 one can simply initialize the transition matrix 𝐴
in diagonal form with entries 𝑎𝑛1
2𝑖𝜋𝑛S4DLin or 𝑎𝑛1
2𝑖𝑁
𝜋𝑁
𝑛11S4DInv Our interest in
S4like models spiked at this point since the ﬁndings of Gu et al 2022a suggest that given the eﬀectiveness
of such simpliﬁed versions of 𝐴 the root of S4 success might be attributable to more fundamental eﬀects are
orthogonal to the HiPPO theory
ShortlyafterSmithetal2022foundthatonecanalsodepartfromtheformalonedimensionaldiscretization
structure of S4 rooted in the HiPPO theory and considered a simpliﬁed version where all input dimensions are
eﬃciently and simultaneously processed using parallel scans Martin and Cundy 2017  not separately like
in S4 S4D and DSS This model named S5 set a new stateofthe art on PathX the hardest task in the Long
Range Arena and provides further evidence for a conceptually simpler motivation for the performance of deep
statespace models Indeed as already mentioned S5 is not precisely the discretization of a latent continuous
time SSM yet still includes parameters like discretization stepsizes that have an ambiguous interpretation in
this context15 suggesting further investigations are needed
At the same time a few interesting works developed novel variants of the S4 architecture Liquid S4 used the
original nondiagonal S4 formulation combined with liquid timeconstant networks Hasani et al 2021
2022 SimilartoDSSS4DandS5 MegaalsosimpliﬁedS4toadiagonalSSMMaetal2022whileshowing
additionally that restricting the diagonal 𝐴to real numbers  giving it an exponential moving average EMA
interpretation  can still work well when combined with attention and a gated block design Another intriguing
view was provided by the SGConvmodel Li et al 2022a which leverages the convolutional interpretation of
SSMs Gu et al 2021b to design a purely ﬁlterbased version of S4 with no latent continuoustime model or
need for discretization
The discretization viewpoint also attracted the interest of Gupta et al 2022b concurrent to this work who
pointed out that after numerical integration diagonal statespace models and linear RNNs share the same
function approximation class Gupta et al 2022b then introduced DLR most closely related to DSS and S4D
each input is processed independently at each layer but where the discretization stepsize Δis absorbed into
the continuoustime transition matrix 𝐴see 2 Their focus was on a new set of synthetic longrange tasks
with strong supervision eg segmentation while ours is on the established Long Range Arena benchmark
To conclude we point the reader to interesting recent applications of models inspired by the S4 architecture
In addition to earlier applications in NLP Mehta et al 2022 more sophisticated architectures based on S4
recently showed great promise in language modeling Dao et al 2022b Ma et al 2022 Speciﬁcally Dao
et al 2022b designed a new generative language model H3 that outperforms GPTNeo27B with SSMs
augmented with two attention layers Besides language deep statespace models were also found successful
forlongvideoaudiounderstandingandgenerationtasksGoeletal2022IslamandBertasius2022Nguyen
et al 2022 and have attracted interest in biology Bordin et al 2022 and time series forecasting Zhou
et al 2022
15One can still view S5 as a discretized version of a continuoustime SSM However this requires adjusting the input projection matrix
19Resurrecting Recurrent Neural Networks for Long Sequences
C Additional experimental results
C1 Training speedups
In Tb4 we show training speed comparisons of the LRU with a regular RNN with tanh activations as well as
with the S4D and S5 models As we elaborate in 22 for the LRU we closely followed the optimal model sizes
of the S5 model Consequently we also see similar training speeds as the S5 model on all tasks
Moscdscesclsc sscCIFAR LiscssctscOpscssc Tescxsctsc Resctscrsciscescvscasclsc Pasctschscfsciscnscdscescrsc PasctschscX
Tascnschsc RNN 20 11 05 05 21 014
LRU 159 8xsc 21 19xsc 147 29xsc 57 114xsc 155 74xsc 24 17xsc
S4D oscuscrsc rscescpscrscoscdscusccsctsciscoscnsc 135 22 106 30 245 26
S5 oscuscrsc rscescpscrscoscdscusccsctsciscoscnsc 159 22 144 57 156 23
Table 4jSpeeds stepssec during training on a A100 GPU We also show the speedup of the LRU over the tanh
RNN for each task The batch size used for each task is speciﬁed in Tb9
C2 Eﬀect of stability and normalization
Inthissectionweexplorefurthertheeﬀectofintroducingstabilityduringtraining33aswellasintroducing
the𝛾normalization factor as shown in Eq 7 To do this we consider the sCIFAR experiment where we sweep
over diﬀerent settings of 𝑟maxand𝑟minto see the eﬀect when initializing closer to the unit disk We keep
the learning rate ﬁxed at 0004 for these experiments which we found to be optimal when initializing with
𝑟max10and𝑟min00under a stable exponential parameterization
We show our results in Tb5 In the ﬁrst table Tb5A we show results with our baseline where we use the
exponential parameterization described in 33 We see that under this setting the optimal performance is
achieved when 𝑟max𝑟min09 and performance degrades as 𝑟maxis increased beyond 09
In Tb5B we show results after enforcing stability We now notice that for each 𝑟min the optimal performance
is achieved by a higher 𝑟maxthan before ie training is more when initializing closer to the unit disk Our
optimal performance in this setting is achieved using 𝑟min00and𝑟max099 Note that even in this setting
performance can sometimes degrade when moving to even higher 𝑟max
Finally in Tb5C we also incorporate the 𝛾normalization factor and we now notice no degradation in
performance even when 𝑟max0999 We found training to be more stable in this setting and our best result
of 890 performance is also obtained in this setting with 𝑟min09and𝑟max0999
These ablations further motivate the beneﬁts of enforcing stability and using the normalization parameter for
betterperformanceandmorestabletraining particularlywhenrequiredtolearnverylongrangedependencies
C3 Expanded tables
Below we show our full results on the Long Range Arena expanding on Tables 1 2 and 3 in the main paper
The tables are presented in logical order in Table 6 we show that vanilla dense RNNs proﬁt from dropping
recurrent nonlinearities when used in the context of the architecture in Fig 1 Next in Table 7 we diagonalize
our linear RNN model from 31 and show how diﬀerent parametrization for the diagonal elements aﬀect
performance For all the rows in Table 7 initialization of the diagonal RNN was performed uniform on the disk
to match the random Glorot initialization of our dense version Thm 31
Further the last row in Table 7 shows the positive eﬀects of changing initialization distribution to a thin ring
close to the circle boundary  eﬀectively enabling longrange reasoning through mitigation of vanishing
gradients Our settings for the ring are reported on the ﬁrst row of Table 8 Finally the second row of this table
shows the improvements that can be achieved by including model normalization Eq 7 which closes the
accuracy gap with deep SSMs
20Resurrecting Recurrent Neural Networks for Long Sequences
𝑟max𝑟min0 05 09
09 876 04 878 01 879 02
099 838 09 858 12 819 38
0999 839 02 848 04 848 08
asc Nosc ssctscascbscisclscisctscysc
𝑟max𝑟min0 05 09
09 862 02 866 03 873 01
099 878 02 877 01 881 00
0999 874 02 874 01 875 04
bsc Wisctschsc ssctscascbscisclscisctscysc
𝑟max𝑟min0 05 09
09 864 01 865 01 883 01
099 881 01 884 01 890 02
0999 881 01 886 00 890 01
csc Wisctschsc 𝛾nscoscrscmscasclscisczscasctsciscoscnsc
Table 5jEﬀect of stability and normalization and diﬀerent 𝑟minand𝑟maxvalues on test accuracy for the sCIFAR10
task Bothstabilityandnormalizationallowforinitializingeigenvaluesclosertotheunitdiskresultinginimproved
performance
Resccscuscrscrscescnsccscesc sscCIFAR LiscssctscOpscssc Tescxsctsc Resctscrsciscescvscasclsc Pasctschscfsciscnscdscescrsc PasctschscX
RNNLiscnsc 722 02 504 02 891 01 891 01  
RNNRescLU 697 02 376 80 880 01 885 01  
RNNTascnschsc 699 03 439 01 872 01 889 02  
S4D oscuscrsc rscescpscrscoscdscusccsctsciscoscnsc 915 02 602 03 864 00 895 00 942 03 975 00
S5 oscuscrsc rscescpscrscoscdscusccsctsciscoscnsc 888 01 585 03 862 01 889 00 957 01 960 01
S4 pscascpscescrsc rscescsscusclsctscssc 911 596 868 909 942 964
S4DLescgscS pscascpscescrsc rscescsscusclsctscssc 899 605 862 895 931 919
S5 pscascpscescrsc rscescsscusclsctscssc 901 622 893 914 953 986
Table 6jPlacing a Vanilla RNN as recurrent core in the architecture of Fig 1 Shown is the eﬀect of removing the
RNN nonlinearity on test accuracy 31
D Detailed experimental setup
In this section we describe our experimental details
D1 Architecture
We consider the standard S4 architecture of Gu et al 2021a and replace the S4 layers with RNN layers or
with S5 Smith et al 2022 or S4D Gu et al 2022a layers for our baselines We give an overview of the
architecture used in Fig1 The input is ﬁrst encoded into 𝐻features followed by a stack of residual blocks
For all our experiments we use networks with a depth of 6 residual blocks Each residual block consists of
identity skip connection and the residual path containing a normalization layer in our case we always use
batch normalization in our experiments followed by the RNNSSM block While using the postnorm option
of adding the normalization layer after the skip and residual branches typically improves performance we stick
to this design due to this architecture being more scalable in general De and Smith 2020
Each RNNSSM block ﬁrst contains the recurrent layer as described in Eqs 1and3in 2 This is followed
by a mixing layer For all experiments except PathX we use the GLU activation function Dauphin et al 2017
with dropout as the mixing layer similar to Gu et al 2021a For PathX we instead use a GLU activation
functionwithoutoneadditionallineartransform thesameasusedbySmithetal2022fortheirexperiments
We use bidirectional models for our experiments on PathFinder and PathX using a similar setup as Gu et al
2021a and use unidirectional models for the rest of our experiments
21Resurrecting Recurrent Neural Networks for Long Sequences
sscCIFAR LiscssctscOpscssc Tescxsctsc Resctscrsciscescvscasclsc Pasctschscfsciscnscdscescrsc PasctschscX
Descnscsscesc𝐴 722 02 504 02 891 01 891 01  
ΛRescasclsc  Imsc 865 01 588 03 874 03 878 05  
ΛExscpsc 854 07 605 03 865 04 894 01 654 90 
ΛStscascbsclscesc Exscpsc 872 04 594 03 876 03 891 02 935 05 
 Riscnscgsc Inscisctsc 881 00 594 03 894 01 901 01 944 03 
S4D oscuscrsc rscescpscrscoscdscusccsctsciscoscnsc 915 02 602 03 864 00 895 00 942 03 975 00
S5 oscuscrsc rscescpscrscoscdscusccsctsciscoscnsc 888 01 585 03 862 01 889 00 957 01 960 01
S4 pscascpscescrsc rscescsscusclsctscssc 911 596 868 909 942 964
S4DLescgscS pscascpscescrsc rscescsscusclsctscssc 899 605 862 895 931 919
S5 pscascpscescrsc rscescsscusclsctscssc 901 622 893 914 953 986
Table 7jTest accuracy of a linear diagonal complex RNNs under diﬀerent parameterizations of the transition
matrix see 32 Performance directly improves the results in Tb 1 and showcases the advantage of exponen
tial polar representation of Λ In bold font is the best parameterization option for linear RNN blocks Ring Init
denotes a changed initialization where 𝑟minand𝑟maxare tuned Performance and Text and Retrieval task already
aligns with S4 results in the dense setting cf Tb1 with Tb 3 No model with able to solve PathX which requires
normalization see Tb3
sscCIFAR LiscssctscOpscssc Tescxsctsc Resctscrsciscescvscasclsc Pasctschscfsciscnscdscescrsc PasctschscX
Liscnscescascrsc Descnscsscesc RNN 722 02 504 02 891 01 891 01  
Discascgscoscnscasclsc Coscmscpsclscescxsc RNN 865 01 588 03 874 03 878 05  
Stscascbsclscesc Exscpsc Pascrscascmsc wsc Riscnscgsc Inscisctsc 881 00 594 03 894 01 901 01 944 03 
𝑟min𝑟max¼ 09 099 00 10 00 09 05 09 09 0999
𝛾Noscrscmscasclscisczscasctsciscoscnsc LRU 890 01 602 08 894 01 899 01 951 01 942 04
𝑟min𝑟max¼ 09 0999 00 099 05 09 05 09 09 0999 0999 09999
S4D oscuscrsc rscescpscrscoscdscusccsctsciscoscnsc 915 02 602 03 864 00 895 00 942 03 975 00
S5 oscuscrsc rscescpscrscoscdscusccsctsciscoscnsc 888 01 585 03 862 01 889 00 957 01 960 01
S4 pscascpscescrsc rscescsscusclsctscssc 911 596 868 909 942 964
S4DLescgscS pscascpscescrsc rscescsscusclsctscssc 899 605 862 895 931 919
S5 pscascpscescrsc rscescsscusclsctscssc 901 622 893 914 953 986
Table 8jEﬀects of normalization on linear diagonal RNNs with stable exponential parameterization see 34 In
bold is our best performing model and we report the closely matching deep SSM results below Tunings for our
rings are also reported Results showcase the advantage of taking initialization close to the unit circle under proper
𝛾normalization For PathX we initialize eigenvalues to have a phase range of 0𝜋10¼ for all other tasks we use
a range of02𝜋¼see 34
D2 General experimental details
We use AdamW as our optimizer Loshchilov and Hutter 2017 We use warmup for the learning rate where
we start from a value of 107and increase the learning rate linearly up a speciﬁed value for the ﬁrst 10 of
training This is followed by cosine annealing for the rest of training down to a value of 107
We used a smaller learning rate for the RNNSSM parameters 𝐴and𝐵 When using normalization in our RNNs
we also used a smaller learning rate on the normalization parameter 𝛾 For our S5 and S4D baselines we
used a smaller learning rate for the discretization step size Δ This smaller learning rate was determined by
multiplying the base learning rate by a factor 1See Tb9 for the learning rate factor used for each task
WeuseweightdecayforallparametersexcepttheRNNSSMparameters 𝐴and𝐵and𝛾andΔwhenapplicable
All experiments were carried out on accelerated hardware A100 GPUs
D3 Hyperparameters
We closely followed the hyperparameter settings of the S5 model Smith et al 2022 for all our experiments
with minimal additional tuning For our S5 baseline we tuned the model dimension 𝐻and state dimension 𝑁
22Resurrecting Recurrent Neural Networks for Long Sequences
Tascsscksc Descpsctschsc 𝐻𝑁 Itscescrscasctsciscoscnscssc Basctsccschsc sscisczscesc LR fscasccsctscoscrsc Wesciscgschsctsc Desccscascysc Drscoscpscoscusctsc
sscCIFAR 6 512 384 180ksc 50 025 005 01
LiscssctscOpscssc 6 128 256 80ksc 32 05 005 00
Tescxsctsc 6 256 192 50ksc 32 01 005 01
Resctscrsciscescvscasclsc 6 128 256 100ksc 64 05 005 01
PasctschscFiscnscdscescrsc 6 192 256 500ksc 64 025 005 00
PasctschscX 6 128 256 250ksc 32 025 005 00
Table 9jList of all the hyperparameters used for each task for the LRU model
and used the optimal values for the LRU model as well For the S4D baseline we also tuned 𝐻and𝑁 For all
our experiments we tuned the base learning rate on a logarithmic grid of 2 to choose the optimal learning rate
We present the hyperparameters we used for each LRU experiment in Tb9
D4 Tasks
We use the 6 tasks in the Long Range Arena benchmark for our experiments Tay et al 2020 with the only
diﬀerence being we use colored sCIFAR images instead of the grayscale sCIFAR images used in LRA
E Theoretical insights
We provide here theoretical groundings for some observations made in 3 We start by showing in E1 that
when interleaved with MLP blocks stacked linear RNNs can model highly nonlinear dynamical systems We
provide two separate views that justify our ﬁndings in E11 we provide a spectral explanation while in
E12 we present a functionspace prespective Our results combined with the observation that nonlinear
RNNs are diﬃcult to optimize E2 provide a justiﬁcation for the results in Tb 1 Next motivated by the
results in Tb 3 we in discuss in the same section optimization of linear RNN blocks and show that exponential
reparameterization can accelerate training
E1 Expressivity of linear RNN stacks
In our sequencetosequence setting it is a natural to seek models which at least in the width limit are
able to map inputs 𝑢to outputs 𝑦last layer using a ﬂexible nonlinear transition map 𝑇learned from data
Mathematically a fullyexpressive causalmodel should be able to approximate 𝑦𝑘𝑇¹𝑢𝑘𝑢𝑘1𝑢 1º where𝑇
is an arbitrary nonlinear map
E11 Spectral perspective
We show in this section how interleaving linear RNNs with MLPs in a deep architecture provides a ﬂexible and
modular recipe for the approximation of nonlinear transition maps
SpectrallimitationsoflinearRNNs ItisastandardresultLietal2022bthat linearRNNscanapproximate
any shiftinvariant linearmap𝑇 In continuoustime on the spectral domain this property is easier to study
let𝑌¹𝜔ºand𝑈¹𝜔ºbe the Fourier transforms for two continuoustime signals 𝑢𝑦ℝℝ If there exists a
function𝐻ℝℝsuch that𝑌¹𝜔º𝐻¹𝜔º𝑈¹𝜔º then this can be approximated by a continuoustime linear
RNN𝑥𝐴𝑥𝐵𝑢for some coeﬃcients 𝐴2ℝ𝑁𝑁𝐵2ℝ𝑁1 and the approximation can be made arbitrarily
accurate as 𝑁1 However one thing a linear RNN cannot do is store information under frequencies which
are not present in the input signal if the input is a sine wave of a certain frequency the output will be a scaled
and shifted sine wave of the same frequency 
Spectral eﬀects of interleaving with MLPs In our architecture Fig1 an activation function as well as a
linear positionwise layer is placed right after each RNN output As can be seen in Fig 6 this operation causes
spectral leakage information gets copied over diﬀerent frequency components
The behavior shown in Fig 6 can be characterized exactly
23Resurrecting Recurrent Neural Networks for Long Sequences
000 001 002 003 004 0050010
0005
000000050010Signal
000 001 002 00305101520253035Abs value FFT
original
after ReLU
Figure 6jReLU nonlinearity leaks information from the original signal to higher frequencies as shown formally in
Prop E1
Proposition E1 Spectral eﬀect of ReLU Let𝑢ℝℝbe a continuoustime signal Let 𝑃𝑖be the𝑖th region
activated by the ReLU applied to 𝑢 and let us write 𝑃𝑖𝑝𝑖𝐿𝑖𝑝𝑖𝐿𝑖¼ Then
FReLU¹𝑢ºF𝑢¹𝜔º
𝑖2𝐿𝑖𝑒𝑖𝜔𝑝𝑖sinc¹𝜔𝐿𝑖º
 8
whereFdenotes the Fourier transform the convolution operation and sinc ¹𝑥ºsin¹𝑥º𝑥
This result is simple to parse the Fourier transform of a ReLU activated signal is equal to the Fourier transform
before the ReLU convolved with a kernel which transports information to higher frequencies  an operation
which is impossible for linear RNNs even as the width increases As such introducing an MLP completes
the list of requirements for approximations of a nonlinear transition map frequencies can be scaled
up and down arbitrarily by the RNN and can then be translated in the space using the ReLU  As depth
increases these operations can be combined in a modular fashion leading to highly nonlinear dynamics using
easytolearn linear blocks interleaved with simple activations
To conclude we provide a proof for the proposition above
ProofRecall that multiplications in the time domain are convolutions in the frequency domain
𝑢1¹𝑡º𝑢2¹𝑡ºF1
𝑈1¹𝑡ºF1
𝑈2¹𝑡º 9
1
1𝑈1¹𝜈º𝑒𝑖𝜈𝑡𝑑𝜈
1
1𝑈2¹𝜉º𝑒𝑖𝜉𝑡𝑑𝜉
10
1
1𝑈1¹𝜈º1
1𝑈2¹𝜉º𝑒𝑖¹𝜉𝜈º𝑡𝑑𝜉
𝑑𝜈 11
1
1𝑈1¹𝜈º1
1𝑈2¹𝜔𝜈º𝑒𝑖𝜔𝑡𝑑𝜔
𝑑𝜈 12
1
11
1𝑈1¹𝜈º𝑈2¹𝜔𝜈º𝑑𝜈
𝑒𝑖𝜔𝑡𝑑𝜔 13
F1
𝑈1𝑈2¹𝑡º 14
Let now𝑢1𝑢and𝑢2𝜒¹𝑢10º then𝑢1𝑢2ReLU¹𝑢º Next let 𝑃𝑖be the𝑖th region activated by the ReLU
and let us write 𝑃𝑖𝑝𝑖𝐿𝑖𝑝𝑖𝐿𝑖¼ We can write 𝜒¹𝑢10ºÍ
𝑖𝜒𝑝𝑖𝐿𝑖𝑝𝑖𝐿𝑖¼
Recall now the following basic properties
1F𝑥¹𝑡𝑡0º¹𝜔º𝑒𝑖𝜔𝑡0F𝑥¹𝑡º¹𝜔º
2The Fourier transform of a rectangular pulse between 𝜏and𝜏is2𝜏sinc¹𝜔𝜏º wheresinc¹𝑥ºsin¹𝑥º𝑥
Therefore we have
F𝜒𝑝𝑖𝐿𝑖𝑝𝑖𝐿𝑖¼¹𝜔º𝑒𝑖𝜔𝑝𝑖F𝜒𝐿𝑖𝐿𝑖¼¹𝜔º2𝐿𝑖𝑒𝑖𝜔𝑝𝑖sinc¹𝜔𝐿𝑖º 15
24Resurrecting Recurrent Neural Networks for Long Sequences
This concludes the proof
FReLU¹𝑢º𝑈
𝑖2𝐿𝑖𝑒𝑖𝜔𝑝𝑖sinc¹𝜔𝐿𝑖º
 16

E12 Insights from Koopman operator theory
We show how Koopman operator theory Koopman and Neumann 1932 combined with recent advances in
dynamic mode decomposition Kutz et al 2016 Schmid 2010 Williams et al 2015 can provide a solid
theoretical foundation for understanding the class of functions that can be approximated by linear RNNs
interleaved with MLPs Our notation and results are based on Korda and Mezić 2018 Mauroy et al 2020
Basic theory Consider a discretetime nonlinear dynamical system 𝑥𝑘1𝑆¹𝑥𝑘º where𝑆ℝ𝑛ℝ𝑛is a
suﬃciently regular map The Koopman operator K𝑆for the dynamical system 𝑆prescribes the evolution of any
observable measurement 𝑓ℝ𝑛ℂ
¹K𝑆𝑓º¹𝑥º𝑓¹𝑆¹𝑥ºº 17
For instance let us consider 𝑛1and the observable 𝑓¹𝑥ºsin¹𝑥º the Koopman operator is the map that
takes sin¹ºK𝑆sin¹𝑆¹ºº ieadvances the measurement 𝑓one step forward in time
The crucial property of the Koopman operator is that it is linearand bounded Mauroy et al 2020 let 𝑓1 𝑓2
be two observables then
K𝑆¹𝛼𝑓1𝛽𝑓2º¹𝑥º¹𝛼𝑓1𝛽𝑓2º¹𝑆¹𝑥ºº 18
𝛼𝑓1¹𝑆¹𝑥ºº𝛽𝑓2¹𝑆¹𝑥ºº 19
𝛼¹K𝑆𝑓1º¹𝑥º𝛽¹K𝑆𝑓2º¹𝑥º 20
If𝑆isregularenoughie iftheHilbertspaceofobservablescanbechosensuchthat Konlyhaspointspectrum
then the spectral theory of bounded linear operators in Hilbert spaces implies that K𝑆is diagonalizable  ie
any observable 𝑓can be expanded in terms of eigenfunctions of K𝑆 where the Koopman acts linearly We
recall the deﬁnition 𝜙𝜆ℂ𝑛ℂis an eigenfunction of K𝑆with eigenvalue 𝜆2ℂifK𝑆𝜙𝜆𝜆𝜙𝜆 ie if the
system measured on 𝜙evolves linearly Since the eigenfunctions of K𝑆form a basis for 𝐿2 for any observable
𝑓ℂ𝑛ℂ there exist complex numbers 𝜈1𝜈2such that one can write Mauroy and Mezić 2016
K𝑆𝑓¹𝑥ºK𝑆
1
𝑗1𝜈𝑗𝜙𝑗ª
¹𝑥º1
𝑗1𝜆𝑘𝜈𝑗𝜙𝑗¹𝑥º 21
Since also the identity measurement map 𝑥𝑥can be decomposed into eigenfunctions of K𝑆coordinatewise
we have the following assuming 𝑥𝑘1𝑆¹𝑥𝑘º with𝑥2ℝ𝑛 for any𝑘2ℕwe have
𝑥𝑘𝑉Λ𝑘Φ¹𝑥0º 22
where with slight abuse of notation Φℝ𝑛ℂ1is a vector of functions with the 𝑗coordinate deﬁned as
¹Φº𝑗𝑥𝜙𝑗¹𝑥º and𝑉2ℂ𝑛1often named the Koopman modes matrix is the inﬁnite dimensional matrix
such that for the observable 𝑓𝑖𝑥𝑥𝑖 one has𝑓𝑖¹𝑥ºÍ1
𝑗1𝑉𝑖𝑗𝜙𝑗¹𝑥º
Basic Theory Summary In essence Koopman operator theory provides the following guarantee any suf
ﬁciently regular nonlinear autonomous dynamical system can be made linear under a highdimensional
nonlinear blowup of the statespace Sounds familiar This is exactly what a wide MLP  Linear RNN
can do Moreover to take the system back to the original coordinate system one just needs a linear projection
withmatrix 𝑉 Inpracticeforidentiﬁcationanddiagnosisofnonlinearsystemseg inmachanicalengineering
this approach is used in a truncated version where the ﬁnite class of dominant eigenfunctions is constructed
by using the dynamic mode decomposition DMD algorithm from Hermite Polynomials Kaiser et al 2021
Schmid 2010
25Resurrecting Recurrent Neural Networks for Long Sequences
Extension to nonlinear systems with inputs Several options exist for extending Koopman operator theory
to systems with inputs Kaiser et al 2021 Korda and Mezić 2020 Proctor et al 2018 Surana 2016 Here
webrieﬂyoutlinetheapproachofKordaandMezić2020 Let 𝑆ℝ𝑛ℝ𝑚ℝ𝑛beanonlinearfunctionwhich
evolves the state of the system as 𝑥𝑘1𝑆¹𝑥𝑘𝑢𝑘º where¹𝑢𝑘º1
𝑘122¹ℝ𝑚ºis the input sequence We wish to
take this nonlinear dynamical system with inputs to linear form in the inﬁnitedimensional space of observables
𝑓of the form ℝ𝑛2¹ℝ𝑚ºℂ LetLdenote the left shift operator 𝑢¹𝑢0𝑢1ºL¹ 𝑢º¹𝑢1𝑢2º
then one can deﬁne the Koopman operator for any observable 𝑓as follows
K𝑆𝑓¹𝑥𝑢º𝑓¹𝑆¹𝑥𝑢0ºL¹𝑢ºº 23
This operator is again linear and bounded for regular enough 𝑆Korda and Mezić 2020  hence the analysis
in the autonomous setting carries out also in this case In particular using the notation in the last paragraph
𝑥𝑘𝑉Λ𝑘
¹𝑥𝑢ºΦ¹𝑥0𝑢º 24
where Λ¹𝑥𝑢ºis a diagonal complex inﬁnitedimensional matrix which contains the eigenvalues corresponding
to the eigenfunctions of the extended state Φ¹𝑥0𝑢º
Implication for deep RNNs In essence Koopman operator theory provides the following guarantee any
regular nonlinear dynamical system is representable by a linear RNN after proper nonlinear reparameterization of
the inputs  which can be performed by an MLP While we believe this connection is conceptually solid and
gives substantial insights into our architecture a quantitative discussion would require substantial technical
eﬀorts perhaps linked to recent contributions from the statistical learning community Kostic et al 2022
E2 Optimization of recurrent blocks
In this subsection we backup some of our claims about optimization of linear RNNs with experimental ﬁndings
on toy examples Our purpose is to conﬁrm validity of our intuition outside the deep learning setting without
architecturedependent confounders ie on vanilla RNNs with one layer
Recurrent nonlinearities slow down gradient descent In 3 and E1 we showed how linear RNNs can be
used as elementary recurrent blocks for the purpose of modeling complex nonlinear dynamics when stacked in
deep architectures Similarly the results in Li et al 2022a indicate that to achieve S4 performance one
can equivalently replace the recurrent core with a collection of convolutions parametrized by ﬁlters While
a singlelayer level a dense RNNs Eq1 with tanhor sigmoid activation can express convolutions with
ﬁlters Wang et al 2022 the results in Tb 1 and Fig 1a in Wang et al 2022 indicate an advantage on
test accuracy from dropping such nonlinearities in the recurrence  ie of making the RNN linear Motivated
by this in Fig 7 we consider the problem of learning a single onedimensional convolution kernel with a single
layer RNN and compare performance of linear and tanhactivations The sequence length in this problem was
100 and our data consists in 32 inputoutput onedimensional trajectories where the output is the result of a
convolution with the kernel of elements ℎ𝑘1
10exp¹0015𝑘ºcos¹004𝑘º2 which induces moderatelength
dependencies in the data see bump in the kernel in Figure 7 at 𝑘70 The 32 input sequences are generated
sampling random 𝑎𝑐parameters on a range and have form sin¹005𝑎𝑘ºcos¹005𝑐𝑘º2 Outputs are
generated by convolving each input by ℎ Learning is performed using the Adam optimizer Kingma and Ba
2014 with standard momentum parameters
Interestingly already on this simple task linear RNNs outperforms the tanhvariant even after careful tuning of
thestepsize Whiletheinputoutputmapthesystemhadtoapproximateislinearie aconvolutionthisresult
still indicates that on deep architectures where the MLPs interleaving RNNs can quickly perform positionwise
nonlinearities lifting the function approximation class see E1 linear RNNs are preferrable
Beneﬁts of exponential parameterization Our experimental results in 33 indicete that linear RNN cores
can be more eﬀectively learned under exponential parameterization of the eiganvalues 𝜆exp¹𝜈𝑖𝜃º
To understand the reason behind this phenomenon we go back at the classical hard problem of learning
powers Bengio et al 1994 crucially linked with linear RNN models see Eq 4 For a speciﬁc planted
solution𝜆𝜆
𝑟𝑖𝜆
𝑖exp¹𝜈𝑖𝜃º we consider the problem of minimizing the loss 𝐿¹ˆ𝜆º1
2jˆ𝜆𝑘¹𝜆º𝑘j2
where𝑘100and ˆ𝜆is generated from two real parameters following standard  real  imaginary or
26Resurrecting Recurrent Neural Networks for Long Sequences
0 20 40 60 80 100000002004006008010Convolution kernel to be learned
0 250 500 750 1000 1250 1500 1750 2000101
100101Training loss over iterations
tanh lr  00001
tanh lr  00003
tanh lr  0001
tanh lr  0005
lin lr  1e05
lin lr  3e05
lin lr  00001
Figure 7jLearning with Adam a onedimensional convolution with a length 100kernel using a singlelayer RNNs
with linear or tanhrecurrent activations and 100dimensional hidden state Initialization is performed using
Glorot on all quantities for both options For all learning rates in our grid the linear variant is faster to converge
0 100 200 300 400 5001027
1023
1019
1015
1011
107
103
Loss evolution 003
Re  Im param
Exp param
0 100 200 300 400 5001028
1024
1020
1016
1012
108
104
100Loss evolution 035
Re  Im param
Exp param
078 079 080 081 082054055056057058Trajectory Standard param 035
001
 000 001 002 003 004059060061062063Trajectory Exp param 035
Figure 8jExponential parametrization helps when learning a single complex eigenvalue 𝜆exp¹𝜈𝑖𝜃º
exponentiated 100times As 𝜆gets close to the purely imaginary setting 𝜃𝜋2 the geometry of the loss
landscape under standard realimaginary parametrization becomes suboptimal for the Adam optimizer which
works best in the axisaligned setting exponential parametrization In the plot the square denotes initialization 
while the star denotes the solution after 500 iterations
exponential parameterization Note that in this paragraph 𝜆2ℂdenotes the solution not the complex
conjugate of 𝜆 In Fig 8 we show that as the target phase 𝜃approaches 𝜋2ie𝜆gets close to the
imaginary axis standard parameterization slows down learning as the corresponding landscape gets nonaxis
aligned  a feature that does not match well the inner workings of the Adam optimizer16 which is a diagonal
preconditioner Kingma and Ba 2014 Instead under exponential parameterization the eﬀects of phase and
magnitude parameters on the powers of 𝜆are more eﬃciently decouped for example while the real part of
𝜆𝑘is simply exp¹𝑘𝜈ºusing exponential parameterization if standard parameterization is used Re
𝜆𝑘is a
function of both 𝜆𝑟and𝜆𝑖 We noticed that the performance diﬀerence gets most pronounced when the system
has to learn how to turn ie the initialization magnitude is correct but the position on the complex plane
is not this is the precise setting for Figure 8 while for standard parameterization changing the phase 𝜃
requires a careful balance between real and imaginary components for exponential parameterization gradients
are fully aligned with the phase parameter This makes the learning more ﬂexible a feature which we observed
necessary in our experiments on the Long Range Arena see 33 and Tb2
E3 On alternatives to using complex numbers
In this subsection we show how to derive the canonical real form for a nonsymmetric realvalued matrix 𝐴
which we assume to be diagonalizable in the complex domain always true up to arbitrary small perturbation
of the entries Axler 1997 This derivation is classical and can be found in many textbooks under the context
ofreal Jordan form more general see eg Weintraub 2009 Here we present a simpliﬁed discussion
After diagonalizing 𝐴 we retrieve a set of purely real eigenvalues each with multiplicity 1 up to vanishing
perturbations with corresponding realeigenvectors and pairs of complex conjugate eigenvalues with corre
sponding complex conjugate eigenvectors
16Forthisproblemvanillagradientdescentcannotbeeﬀectivelyusedasthelandscapeishighlynonconvexwithchallengingcurvature
vanishing asj𝜆japproaces 0
27Resurrecting Recurrent Neural Networks for Long Sequences
We recall a proof for the facts above letdenote the elementwise complex conjugate of any complex quantity
This operation clearly commutes with multiplication If 𝜆2ℂis an eigenvalue of 𝐴2ℝ𝑁𝑁with eigenvector
𝑣2ℂ𝑁 then since 𝐴is realvalued we have 𝐴𝑣¹𝐴𝑣º¹𝐴𝑣º¹𝜆𝑣º𝜆𝑣 Hence𝜆is an eigenvalue
with eigenvector 𝑣 This also shows that there always does exist a real eigenvector corresponding to each real
eigenvalue let 𝑣2ℂ𝑁be a complex eivengvector with real eigenvalue 𝜆 then𝑣𝑣2ℝ𝑁is an eigenvector
with eigenvalue 𝜆since again using the fact that 𝐴is real𝐴¹𝑣𝑣º𝐴𝑣𝐴𝑣𝐴𝑣¹𝐴𝑣º𝜆¹𝑣𝑣º
The action of 𝐴on its real eigenvectors with real eigenvalues is trivial and analogous to the symmetric case 
this corresponds to a diagonal entry in the diagonalized version of 𝐴 For the subspaces spanned by complex
eigenvalues the discussion is more interesting let 𝜆𝜆be a pair of conjugate eigenvalues with corresponding
eigenvectors 𝑣𝑣 Collect𝑣𝑣in a𝑁2matrix𝑉 then
𝐴𝑉𝑉
𝜆0
0𝜆
𝑉Λ 25
Letusnowchooseadiﬀerent realbasisforthecolumnsof 𝑉therealandimaginarypartsof 𝑣𝑉Re¹𝑣ºIm¹𝑣º¼
Note that this is a basis since 𝑣𝑣are linearly independent and can be both written as complexweighted
linear combination of real and imaginary parts of 𝑣 Now note that
𝐴Re¹𝑣º1
2𝐴¹𝑣𝑣º
1
2¹𝜆𝑣𝜆𝑣º
Re¹𝜆𝑣º
Re¹Re¹𝜆º𝑖Im¹𝜆ºº¹Re¹𝑣º𝑖Im¹𝑣ºº¼
Re¹𝜆ºRe¹𝑣ºIm¹𝜆ºIm¹𝑣º
Similarly
𝐴Im¹𝑣º1
2𝐴¹𝑣𝑣º
1
2¹𝜆𝑣𝜆𝑣º
Im¹𝜆𝑣º
Im¹Re¹𝜆º𝑖Im¹𝜆ºº¹Re¹𝑣º𝑖Im¹𝑣ºº¼
Re¹𝜆ºIm¹𝑣ºIm¹𝜆ºRe¹𝑣º
This shows that the action of 𝐴on the new realbasis 𝑉is of simple form
𝐴𝑉𝑉Re¹𝜆º Im¹𝜆º
Im¹𝜆ºRe¹𝜆º
𝑉Λ 26
This discussion shows that there exist a simple invertible change of basis from 𝑉to𝑉for all pairs of conjugate
eigenvalues which makes takes the system back to a simple decomposition in the real domain both in terms of
eigenvalues and eigenvectors  one simply has to replace all diagonal blocks of form Λwith 22matrices Λ
The careful reader might recognize that in the resulting system matrix multiplication for the 22blocks
is algebraically equivalent to multiplication of the corresponding complex numbers Hence while complex
numbers are not perseneeded to ﬁnd a simple representation of nonsymmetric matrices they are convenient
to work with since the matrix in Eq 26is structured has 4 entries but can be represented using just two 
real and imaginary parts exactly what a complex number stores in memory
28Resurrecting Recurrent Neural Networks for Long Sequences
F Proofs
In this section we provide proofs for the propositions listed in the main paper
F1 Proof of Lemma 32
We provide here a proof for the following sampling lemma
Lemma 32 Let𝑢1𝑢2be independent uniform random variables on the interval 01¼ Let 0𝑟min𝑟max1
Compute𝜈1
2log
𝑢1¹𝑟2
max𝑟2
minº𝑟2
min
and𝜃2𝜋𝑢2 Then exp¹𝜈𝑖𝜃ºis uniformly distributed on the ring
inℂbetween circles of radii 𝑟minand𝑟max
ProofFirst note that one can sample phase and magnitude independently by symmetry of the target distribu
tion Phase sampling can trivially performed through scaling a uniform distribution
Next we consider sampling the magnitude The area of the ring in between 𝑟minand𝑟maxis𝜋¹𝑟2
max𝑟2
minº
while the cumulative distribution function for the radius distribution is such that 𝐹𝑟¹𝑟minº0𝐹𝑟¹𝑟maxº1and
for𝑟2𝑟min𝑟max¼we therefore have
𝐹¹𝑟º𝑟2𝑟2
min
𝑟2max𝑟2
min 27
Under parametrization of 𝑟using the exponential 𝑟𝑒𝜈 one gets
𝐹¹𝑟º𝑒2𝜈𝑟2
min
𝑟2max𝑟2
min 28
Finally we use the inverse sampling theorem see eg Vogel 2002 one can sample 𝜈using the formula
𝜈𝐹1¹𝑢º where𝑢is uniform on01¼ By setting
𝑢𝑒2𝜈𝑟2
min
𝑟2max𝑟2
min 29
we get
𝑒2𝜈¹𝑟2
max𝑟2
minº𝑢𝑟2
min 30
from which follows that 𝜈1
2log¹¹𝑟2
max𝑟2
minº𝑢𝑟2
minº 
F2 Proof of Proposition 33
Validity of this proposition is veriﬁed numerically in Figure 9
Proposition 33 Forwardpass blowup LetΛbe diagonal with eigenvalues sampled uniformly on the ring in
ℂbetween circles of radii 𝑟min 𝑟max1 Then under constant or whitenoise input and Glorot input projection
we have that the squared norm of the state 𝑥𝑘converges as 𝑘1to the following quantity
𝔼k𝑥1k2
2¼1
𝑟2max𝑟2
minlog 
1𝑟2
min
1𝑟2max
𝔼k𝐵𝑢k2
2¼
ProofAssume ﬁrst most diﬃcult case that 𝑢𝑘is constant ie such that 𝐵𝑢𝑘 𝑢for all𝑘 Then
k𝑥1k2
21
𝑛11
𝑚1𝑢
𝑘𝑚¹Λ𝑚ºΛ𝑛𝑢𝑘𝑛 31
𝑢1
𝑛11
𝑚1¹Λ𝑚ºΛ𝑛
𝑢 32
29Resurrecting Recurrent Neural Networks for Long Sequences
08 09 095 099
Value of rmax2345678Constant input rmin075
predicted gain
08 09 095 099
Value of rmax345678White noise input rmin075
predicted gain
Figure 9jNumerical simulation for gain formula derived in Proposition 33 Here we chose 𝑁500𝐿
10𝑘sequence length and plotted statistics for 10 runs with boxplot indicating median and 595 percentile
Indicated in blue line is our prediction The formula holds both for constant and random input yet we notice that
it is more accurate in the random input setting
Note that Λdiag¹𝜆1𝜆𝑁ºis diagonal with equally distributed entries on the disk between radii 𝑟minand
𝑟max One can then sample a generic entry 𝜆using the change of variables formula for probabilities Jeﬀreys
1998 as follows see also Lemma 32
𝜆𝑟1
2𝑒𝑖2𝜋𝜃 𝑟U𝑟2
min𝑟2
max¼ 𝜃U 01¼ 33
Where crucially 𝑟and𝜃are independent Let 𝕋¹𝑟min𝑟maxºf𝜆2ℂj𝜆j2𝑟min𝑟max¼g We need to study the
following quantity
𝔼𝜆𝕋¹𝑟min𝑟maxº1
𝑛11
𝑚1𝜆𝑛¹𝜆𝑚º
𝔼𝑟𝜃1
𝑛11
𝑚1𝑟1
2¹𝑛𝑚º𝑒𝑖2𝜋¹𝑛𝑚º𝜃
34
1
𝑛11
𝑚1𝔼𝑟h
𝑟1
2¹𝑛𝑚ºi
𝔼𝜃h
𝑒𝑖2𝜋¹𝑛𝑚º𝜃i
35
The expectation wrt 𝜃is nonzero only if 𝑛𝑚 therefore
𝔼𝜆𝕋¹𝑟min𝑟maxº1
𝑛11
𝑚1𝜆𝑛¹𝜆𝑚º
1
𝑛1𝔼𝑟𝑟𝑛¼ 36
𝔼𝑟1
𝑛1𝑟𝑛
37
𝔼𝑟1
1𝑟
38
1
𝑟2max𝑟2
min𝑟2
max
𝑟2
min1
1𝑟𝑑𝑟 39
1
𝑟2max𝑟2
min¹log¹j1𝑟2
maxjºlog¹j1𝑟2
minjºº 40
1
𝑟2max𝑟2
minlog 
1𝑟2
min
1𝑟2max
 41
Thewhite noise input case is simpler Let us start from k𝑥1k2
2Í1
𝑛1Í1
𝑚1𝑢
𝑘𝑚¹𝐴𝑚º𝐴𝑛𝑢𝑘𝑛 Now we can
retrieve the single sum by the fact that 𝐴is diagonal and 𝔼𝑢
𝑘𝑚𝑢𝑘𝑛¼0for𝑚𝑛 The rest of the proof is
identical and presented in the main paper for the onesimensional setting 
30
  Better speech synthesis through scaling
James Betker
Abstract
In recent years the field of image generation has been revolutionized by the appli
cation of autoregressive transformers and DDPMs These approaches model the
process of image generation as a stepwise probabilistic processes and leverage
large amounts of compute and data to learn the image distribution
This methodology of improving performance need not be confined to images This
paper describes a way to apply advances in the image generative domain to speech
synthesis The result is TorToise  an expressive multivoice texttospeech sys
tem
All model code and trained weights have been opensourced at
httpsgithubcomneonbjbtortoisetts
1 Background
11 Texttospeech
The field of texttospeech TTS research has been largely constrained to the development of effi
cient models trained on relatively small datasets This choice has been driven by
1 The desire to build efficient speech generation models that can be deployed at scale and
thus must have a high sampling rate
2 The unavailability of very large transcribed speech datasets
3 Challenges scaling the encoderdecoder model architectures traditionally used in TTS
111 Neural MEL Inverters
Most modern texttospeech systems operate on speech data that is encoded as a MEL spectrogram
There are many compelling reasons to operate in this encoding space but for neural networks the
most compelling reason is that it is highly spatially compressed The MEL configuration used by
the Tacotron for example operates at 256x compression over raw audio waveform data sampled at
22kHz but contains most of the information found in that data
Because of this an entire body of research has been dedicated to finding highquality ways to decode
MEL spectrograms back into audio waveforms A synthesizer that performs this task is generally
called a vocoder but I more generally refer to it as a MEL inverter in this paper
Modern MEL inverters built on neural networks are incredibly sophisticated They produce wave
forms that are nearly indistinguishable from recorded waveforms to human ears and they are highly
generalizable outside of their training set I capitalize on this work by using an implementation of
UnivnetKim 2021 as a final stage for my texttospeech system
12 Image generation
While TTS systems largely focus on latency this has not been the case in other domains For
example with image generation more focus has been applied to training models that generate higharXiv230507243v2  csSD  23 May 2023quality results regardless of the sampling time For the purposes of this paper I dive into two bodies
of research
121 DALLE
DALLERamesh et al 2021 showed how an autoregressive decoder can be applied to textto
image generation This is particularly appealing because of the vast quantity of research that has
been poured into scaling decoderonly models in the NLP domain
Two important problems persist with DALLE first it relies on fullsequence selfattention which
carries a cost of ON2compute and memory where N is the sequence length This is particularly
troublesome when dealing with modalities like images or audio which have large sequence lengths
when dealt with naively
Second traditional autoregressive approaches require operating in the discrete domain Images are
encoded into sequences of discrete tokens using a quantizing autoencoder DALLE then models
these sequences of tokens using an autoregressive prior model This is a strength of DALLE in
terms of expressiveness but it comes at the cost of requiring a decoder which can convert these
image tokens back into the pixel values that actually comprise an image It is my opinion that
learned VQV AE decoder used by DALLE is principally responsible for the blurry incoherence
exhibited by most of its samples
122 DDPMs
The generative model space has long been plagued by models that either exhibit meanseeking
behavior resulting in blurriness or modecollapse resulting in a lack of diversity or generalization
Denoising diffusion probabilistic models DDPMsHo et al 2020 have recently arisen as the first
type of generative model capable of producing crisp coherence and diverse images These models
have been shown to be quite effective at using lowquality guidance signals to reconstruct the high
dimensional space that those guidance signals were derived from Put another way they are great at
superresolution
There are two important caveats to DDPMS
1 Traditional approaches to DDPMs rely on fixed output shapes that are known before sam
pling begins As a concrete example relevant to this paper DDPMs cannot learn to convert
text into audio signals because they cannot solve the implicit alignment problem between
text and audio
2 DDPMs must be sampled from over multiple iterations This sampling process consumes
a great deal of compute and means sampling from a DDPM will always incur a significant
latency cost
123 Reranking
DALLE introduced the process of reranking the outputs of autoregressive models This process
samples randomly from the autoregressive model and picks the highest quality output of k outputs
for downstream use
Such a procedure requires a strong discriminator a model that can tell good textimage pairings
from bad DALLE used CLIPRadford et al 2021 a model trained with a contrastive text and
image pairing objective
2 Methods
21 Joining Autoregressive Decoders and DDPMs
To review some of the conclusions drawn above
1 Autoregressive models are strong at converting between unaligned domains like vision text
and speech
2Figure 1 TorToisev2 architectural design diagram Inputs of text and a reference audio clip for speaker
cloning flow through a series of decoding and filtering networks to produce highquality speech
2 DDPMs operate in the continuous domain which allows them to model expressive modali
ties
Both types of models have demonstrated the ability to scale performance with additional compute
and data
It becomes evident that when posed with a problem like generating continuous data like speech
spectrograms or images a marriage of these two approaches might have some distinct advantages
Specifically in inference the autoregressive model will be used to convert a sequence of text tokens
to a sequence of tokens representing the output space in our case speech tokens The DDPM will
then be used to decode these tokens into a high quality representation of speech
22 Applying AutoregressionDDPMs to TTS
To build out the previously proposed system we need to train the following neural networks
1 An autoregressive decoder which predicts a probability distribution for speech tokens con
ditioned on text
2 A contrastive model similar to CLIP which is used to rank outputs of the autoregressive
decoder
3 A DDPM which can convert speech tokens back into speech spectrograms
The architectures and training process for all of these networks largely follow the procedures found
in their respective literature Details can be found in B
221 Conditioning Input
A unique design choice made with TorToise is an additional input which is provided to both the
autoregressive generator and the DDPM which I term the speech conditioning input
The speech conditioning input starts as one or more audio clips of the same speaker as the target
These clips are converted to MEL spectrograms and fed through an encoder consisting of a stack
of selfattention layers The autoregressive generator and the DDPM have their own conditioning
encoders both of which are learned alongside their respective networks
The output of these layers is averaged to produce a single vector The vectors from all of the encoded
conditioning clips are then averaged again before being fed as an input into the autoregressive or
conditioning networks
3The intuition behind the conditioning input is that it provides a way for the models to infer vocal
characteristics like tone and prosody such that the search space of possible speech outputs corre
sponding to a given textual input is greatly reduced
222 The TorToise Trick
For the majority of the training procedure the DDPM is trained to convert discrete speech codes
into MEL spectrograms After this process has converged I finetune the DDPM on the autoregres
sive latent space which is pulled from the AR model outputs instead of the speech codes This is
described in detail in B
The logic here is that the AR latent space is far more semantically rich than discrete tokens By
finetuning on this latent space we improve the efficiency of the downstream diffusion model I
liken this to recent work showing that training decoder models conditioned on frozen text encoders
to produce large efficiency gains This finetuning is one of the greatest contributors to model output
quality of any of the tweaks I made to the various model training processes
23 CLVP
As mentioned earlier a good strategy for gathering expressive outputs from generative models is
using a qualitative discriminator to rerank several outputs then choosing only the best DALLE
uses CLIP for this
This same type of approach used for CLIP can be applied to speech after all most TTS datasets are
simply pairings of audio clips and text By training a model on these pairs in a contrastive setting
the model becomes a good discriminator for speech
For TorToise I train the Contrastive LanguageV oice Pretrained Transformer or CLVP It has many
of the same properties of CLIP but notably serves as a scoring model for use in reranking TTS
outputs from the AR model
To make this work efficiently in inference I trained CLVP to pair discretized speech tokens with
text tokens This way CLVP can rerank multiple AR outputs without the expensive diffusion model
being invoked
3 Training
These models were trained on a small cluster of 8 NVIDIA RTX3090s over the period of 1 year
Specifics on how these models are trained can be found in B
4 Inference Process
Once the four models of the framework are fully trained the inference procedure is as follows
1 Feed the conditioning inputs and the text into the autoregressive model and decode a large
number of output candidates
2 Use CLVP to produce correlation scores between each speech candidate and text
3 Choose the top k speech candidates and for each candidate
4 Decode to a MEL spectrogram using the DDPM
5 Convert to a waveform using a conventional vocoder
6 When decoding the autoregressive model nucleus sampling is used with P8 repetition
penalty2 and softmax temperature8
Sampling from DDPMs is a highly studied and rapidly changing field At the time TorToise was
designed I found the sampling configuration with the best balance between quality and inference
speed to be as follows
1 Algorithm DDIMSong et al 2022
42 Schedule Linear
3 Sampling steps 64
4 ConditioningFree Guidance constant 2
5 The Dataset
Since my goal was to train what is essentially a large language model I needed a lot of data I started
with the LibriTTSZen et al 2019 and HiFiTTSBakhturina et al 2021 datasets which combined
contain 896 hours of transcribed speech I built an additional extended dataset of 49000 hours of
speech audio from audiobooks and podcasts scraped from the internet Details on how this dataset
was built are in appendix I The official LibriTTS test split was used for validation purposes
6 Experiments
Text to speech systems are challenging to experimentally compare because many state of the art sys
tems are closed source with few samples to compare against To this end I built my own evaluation
suite which uses CLVP to produce a distance metric between real samples and generated samples
similar to the FID score used by images I also use an open source wav2vec model to characterize
the intelligibility of a speech segment I have open sourced this work here
Past this comparisons between the samples generated from TorToise and those generated by other
papers can be found here
7 Conclusion
TorToise is the latest in a line of recent stateoftheart breakthroughs that use general model archi
tectures Almost no part of TorToise was designed specifically for audio processing yet it outper
forms all previous TTS models in realism It does this by Embracing generalist architectures like
stacks of transformer layers Leveraging a large highquality dataset Training at largeish scale and
high batch size
My main takeaway from this project is how incredibly strong the results are from adhering to the
above 3 points It seems likely to me that any digitized modality is subject to generative modeling
using this framework
References
Bakhturina E Lavrukhin V  Ginsburg B and Zhang Y  2021 Hifi multispeaker english tts
dataset
Ho J Jain A and Abbeel P 2020 Denoising diffusion probabilistic models
Kim J 2021 Mindslab UnivNet implementation
Nichol A and Dhariwal P 2021 Improved denoising diffusion probabilistic models
Radford A Kim J W Hallacy C Ramesh A Goh G Agarwal S Sastry G Askell A
Mishkin P Clark J Krueger G and Sutskever I 2021 Learning transferable visual models
from natural language supervision
Ramesh A Pavlov M Goh G Gray S V oss C Radford A Chen M and Sutskever I
2021 Zeroshot texttoimage generation
Seonghyeon K 2019 VQV AE rosinality
Song J Meng C and Ermon S 2022 Denoising diffusion implicit models
Wang P 2020 Vector Quantize Lucidrains
Wang P 2021 xtransformers Lucidrains
5Zen H Dang V  Clark R Zhang Y  Weiss R J Jia Y  Chen Z and Wu Y  2019 Libritts
A corpus derived from librispeech for texttospeech
A Extended Dataset Collection
I independently built an extended TTS dataset composed of audiobooks and podcasts scraped from
the web This data was split on 500ms silences and any audio clip between 520 seconds was kept
I then fed the resulting clips through a pipeline of classifiers that I trained which remove any audio
with background noise music poor quality such as phone calls multiple voices speaking at once
and reverb Due to disk space limitations I was forced to limit the amount of scraping The end
result was 49000 hours of cleaned audio clips
I transcribed this dataset using a wav2vec2large model I personally finetuned this model to predict
punctuation as quotation marks commas and exclamation marks are important for the purposes of
generating speech but are not generally included in the training of speech recognition models Fine
tuning was performed on LibriTTS and HiFiTTS and the pretrained model weights and transcription
scripts can be found here
B Training and Architecture Details
B1 VQVAE
The VQV AE used with TorToise is most similar to that of the original VQV AE by van der Oord
et al It operates on MEL spectrograms It consists of a small residual convolutional network that
compresses the spectrogram an additional 4x and produces a codebook consisting of 8192 tokens
When training the VQV AE I found that larger batch sizes decrease reconstruction losses and thus
used a very large batch size for my infrastructure Input samples were constricted to 40960 PCM
readings or 2 seconds of audio The primary bottleneck for training the VQV AE was the dataloader
6Figure 2 Training curves for VQV AE Yaxis is MSE loss in loglog scale Xaxis is number of training steps
Model shape 1D Conv resnet encoder  decoder
Top dim 512
Bottom dim 1024
Codebook dim 256
Quantizer token count 8192
Quantization algorithm Clustering a la original VQV AE no restart
Batch size 8192
Total training 360M samples
Losses MSE reconstruction loss commitment loss
LR 3e4
B1 B2 9 9999
Weight decay 01
EMA weights replaces LR decay with rate 999
Table 1 VQV AE model details  hyperparameters
7B2 Autoregressive Prior
The AR decoder uses a bogstandard GPT2 architecture and generally follows the training instruc
tions from the DALLE1 paper Unlike DALLE only dense selfattention is used The prompt is
assembled as follows
SC BT T T TT ET BM M M MEM
SCSpeech c o n d i t i o n i n g e n c o d i n g
BTBegin t e x t t o k e n
T Text t o k e n s
ETEnd t e x t t o k e n
BMBegin MEL t o k e n
M MEL t o k e n s
EMEnd MEL t o k e n
Speech conditioning encodings are learned by a separate encoder that takes in the MEL spectrogram
of a related clip another clip of the same person speaking and produces a single vector embedding
that is placed at the front of the attention context Two encodings were produced for each training
sample which are averaged together The maximum input length to the conditioning encoder is
132300 samples or 6 seconds of audio
Learned positional embeddings are used The MEL tokens and the text tokens get their own posi
tional parameters Text inputs are unpadded MEL tokens are right padded to conform the sequence
length of each batch The maximum sequence length is 402 text tokens  604 MEL tokens For
efficiency reasons in the first half of training the model only saw 6 second audio clips After this
audio clips up to the full length  27 seconds were seen
8Figure 3 Early training curves in loglog scale Yaxis is cross entropy loss for MEL tokens Xaxis is number
of training steps Does not include a long tail of training and finetuning due to online changes that were made
adding nonreproducible noise to curves
Model architecture Transformer stack with causal masking
Layers 30
Model dim 1024
Attention heads 16
Text tokenization Custom BPE 256 tokens wide
Batch size 1024
Total training 119M samples
Text next token prediction loss weight 01
MEL token next token prediction weight 1
LR 1e4
B1 B2 9 96
Weight decay 01
LR Warmup 500 steps
EMA decay rate 999
Table 2 AR prior details  hyperparameters
After training the autoregressive decoder to convergence I finetuned it on the clean audio datasets
from LibriTTS and HIFITTS
9B3 CLVP
The original DALLE worked by decoding a large number of images for a given text prompt which
were then fed through CLIP The image that CLIP deemed closest to the input text was used as the
final output
I continue following this lead for TorToise for reasons that will become evident in the results section
I built a simple model that is very similar to CLIP which I call a Contrastive LanguageV oice
Pretrained model or CLVP Like CLIP this model produces distance metrics for textspeech pairs
CLVP uses an architecture similar to the CLIP text encoder except it uses two of them one for text
tokens and the other for MEL tokens Tokens from both encoders were dropped out at a rate of 15
Fixed positional embeddings were used Maximum text input length was 350 tokens in practice
never actually seen Maximum MEL token input length was 293 or 13 seconds of audio
Figure 4 Late training curves for CLVP in loglog scale Yaxis is cross entropy loss Xaxis is number of
samples Early training curves were lost
Model architecture Dual transformer stacks
Depth 20
Model dim 768
Attention heads 12
Text tokenization Custom BPE 256token wide
Batch size 1024
Total training 80M samples
Losses Contrastive
LR 3e4
B1 B2 9 96
Weight decay 001
LR Warmup 500 steps
EMA decay rate 999
Table 3 CLVP training details  hyperparameters
10B4 Diffusion Decoder
The diffusion model uses a bespoke architecture that combines residual convolutions with dense
selfattention It most closely resembles the traditional UNet model used for DDPMs but without
any upsampling or downsampling
The diffusion model receives 3 sources of conditioning The timestep signal which modulates the
scale and shift of the group norms used by the network A speech conditioning signal which also
modulates the scale and shift of the group norms The final activations of the autoregressive model
In training the diffusion model I iterated through several different architectures and conditioning
types before settling on this one This includes Architecture A traditional Unet with attention
was tried The full attention network performed significantly better in frechet distance evaluations
Operating on PCM data rather than MELs This necessitated very small context windows and still
took an inordinate amount of time to train The results of decoding a MEL and using a vocoder
resulted in substantially better quality In order to force compatibility with existing diffusion noise
schedules I rescale input MELs to be on the interval 11 Decoding MEL tokens versus AR
activations Training on AR activations is expensive because during each training step you must
forward prop through the AR network However training on AR activations constituted the single
greatest jump in output quality of any design decision made for the diffusion network It is possible
that doing tricks like putting the text on the attention context may ablate this advantage somewhat
As with image diffusion models exploiting classifierfree guidance is extremely important for high
quality outputs In the case of TorToise I perform guidance on both the speech conditioning signal
and the activations of the AR model During training 15 of the time both of these signals are
dropped out and replaced with a learned embedding
When training the diffusion decoder input audio was clipped randomly to 220500 samples or 10
seconds of audio Conditioning inputs were clipped to 102400 samples or 5 seconds of audio
While the rest of the TorToise stack operates at an audio sampling rate of 22kHz the diffusion
decoder outputs MEL spectrograms which were computed from 24kHz audio This discrepancy is
solely to ensure compatibility with the pretrained Univnet vocoder which the model stack uses and
was not done for any performance reasons
Figure 5 Diffusion model losses loglog scale Yaxis MSE loss Xaxis training samples
11Model shape Alternating full attention  conv resblocks
Depth 10
Model dim 1024
Attention heads 16
Batch size 512
Total Training 65M samples
Losses MSE weight 1  VLB weight n
LR 1e5
B1 B2 9 999
Weight decay 001
LR Warmup 1000 steps
EMA decay rate 999
Table 4 Diffusion decoder details  hyperparameters
C Future Work
TorToise is the product of playing way over my paygrade so to speak As an independent researcher
I only had a small number of GPUs to perform my experiments with and made many mistakes in
the process Following are recommendations for architectural tweaks to be made in future work
building off of TorToise
1 Constrict VQV AE codebook embedding dim This has been experimentally shown to pro
duce drastic performance improvements
2 Relative positional encodings The AR model uses fixed positional encodings which limits
the total amount of speech it can produce Using relative encodings would allow arbitrary
length sequences
3 Train CLVP on larger batch sizes Contrastive models benefit from extremely large batch
sizes
4 Train CLVP on longer audio sequences CLVP only ever saw 13 second clips which is
likely why reranking on longer samples suffers
5 Diffusion decoder architecture The diffusion decoder is an attentional network that omits
Feedforward blocks In retrosepct this was a poor design decision and feedforward blocks
should be included
6 Train the entire model stack at 24kHz or retrain Univnet at 22kHz sampling rates
7 Train on more data for longer The training curves for TorToise indicate that we were far
from overfitting Simply training longer likely would have improved results
D Special Thanks
More than the prior work done by the research community this project was a product of the open
source community I wanted to thank a few extra contributors who have not already been mentioned
above whose work I found instrumental in building TorToiSe
1 Phil Wang who authored Wang 2021 and Wang 2020
2 Kim Seonghyeon who authored Seonghyeon 2019
3 FAIR who maintain most of the tooling I use and who open sourced much of the technol
ogy underpinning TorToiSe
4 Prafulla Dhariwal and Alex Nichol without whose Nichol and Dhariwal 2021 I would
still be in GAN hell
I also want to thank my wife Kim Betker who supported me through two years of high electricity
bills a hot  noisy utility room and the many late nights required to build this system
12
  Published as a conference paper at ICLR 2021
ANIMAGE IS WORTH 16X16 W ORDS 
TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE
Alexey Dosovitskiyy Lucas Beyer Alexander Kolesnikov Dirk Weissenborn
Xiaohua Zhai Thomas Unterthiner Mostafa Dehghani Matthias Minderer
Georg Heigold Sylvain Gelly Jakob Uszkoreit Neil Houlsbyy
equal technical contributionyequal advising
Google Research Brain Team
fadosovitskiy neilhoulsby ggooglecom
ABSTRACT
While the Transformer architecture has become the defacto standard for natural
language processing tasks its applications to computer vision remain limited In
vision attention is either applied in conjunction with convolutional networks or
used to replace certain components of convolutional networks while keeping their
overall structure in place We show that this reliance on CNNs is not necessary
and a pure transformer applied directly to sequences of image patches can perform
very well on image classiﬁcation tasks When pretrained on large amounts of
data and transferred to multiple midsized or small image recognition benchmarks
ImageNet CIFAR100 VTAB etc Vision Transformer ViT attains excellent
results compared to stateoftheart convolutional networks while requiring sub
stantially fewer computational resources to train1
1 I NTRODUCTION
Selfattentionbased architectures in particular Transformers Vaswani et al 2017 have become
the model of choice in natural language processing NLP The dominant approach is to pretrain on
a large text corpus and then ﬁnetune on a smaller taskspeciﬁc dataset Devlin et al 2019 Thanks
to Transformers computational efﬁciency and scalability it has become possible to train models of
unprecedented size with over 100B parameters Brown et al 2020 Lepikhin et al 2020 With the
models and datasets growing there is still no sign of saturating performance
In computer vision however convolutional architectures remain dominant LeCun et al 1989
Krizhevsky et al 2012 He et al 2016 Inspired by NLP successes multiple works try combining
CNNlike architectures with selfattention Wang et al 2018 Carion et al 2020 some replacing
the convolutions entirely Ramachandran et al 2019 Wang et al 2020a The latter models while
theoretically efﬁcient have not yet been scaled effectively on modern hardware accelerators due to
the use of specialized attention patterns Therefore in largescale image recognition classic ResNet
like architectures are still state of the art Mahajan et al 2018 Xie et al 2020 Kolesnikov et al
2020
Inspired by the Transformer scaling successes in NLP we experiment with applying a standard
Transformer directly to images with the fewest possible modiﬁcations To do so we split an image
into patches and provide the sequence of linear embeddings of these patches as an input to a Trans
former Image patches are treated the same way as tokens words in an NLP application We train
the model on image classiﬁcation in supervised fashion
When trained on midsized datasets such as ImageNet without strong regularization these mod
els yield modest accuracies of a few percentage points below ResNets of comparable size This
seemingly discouraging outcome may be expected Transformers lack some of the inductive biases
1Finetuning code and pretrained models are available at httpsgithubcom
googleresearchvision_transformer
1arXiv201011929v2  csCV  3 Jun 2021Published as a conference paper at ICLR 2021
inherent to CNNs such as translation equivariance and locality and therefore do not generalize well
when trained on insufﬁcient amounts of data
However the picture changes if the models are trained on larger datasets 14M300M images We
ﬁnd that large scale training trumps inductive bias Our Vision Transformer ViT attains excellent
results when pretrained at sufﬁcient scale and transferred to tasks with fewer datapoints When
pretrained on the public ImageNet21k dataset or the inhouse JFT300M dataset ViT approaches
or beats state of the art on multiple image recognition benchmarks In particular the best model
reaches the accuracy of 8855 on ImageNet 9072 on ImageNetReaL 9455 on CIFAR100
and7763 on the VTAB suite of 19 tasks
2 R ELATED WORK
Transformers were proposed by Vaswani et al 2017 for machine translation and have since be
come the state of the art method in many NLP tasks Large Transformerbased models are often
pretrained on large corpora and then ﬁnetuned for the task at hand BERT Devlin et al 2019
uses a denoising selfsupervised pretraining task while the GPT line of work uses language mod
eling as its pretraining task Radford et al 2018 2019 Brown et al 2020
Naive application of selfattention to images would require that each pixel attends to every other
pixel With quadratic cost in the number of pixels this does not scale to realistic input sizes Thus
to apply Transformers in the context of image processing several approximations have been tried in
the past Parmar et al 2018 applied the selfattention only in local neighborhoods for each query
pixel instead of globally Such local multihead dotproduct self attention blocks can completely
replace convolutions Hu et al 2019 Ramachandran et al 2019 Zhao et al 2020 In a different
line of work Sparse Transformers Child et al 2019 employ scalable approximations to global self
attention in order to be applicable to images An alternative way to scale attention is to apply it in
blocks of varying sizes Weissenborn et al 2019 in the extreme case only along individual axes Ho
et al 2019 Wang et al 2020a Many of these specialized attention architectures demonstrate
promising results on computer vision tasks but require complex engineering to be implemented
efﬁciently on hardware accelerators
Most related to ours is the model of Cordonnier et al 2020 which extracts patches of size 22
from the input image and applies full selfattention on top This model is very similar to ViT
but our work goes further to demonstrate that large scale pretraining makes vanilla transformers
competitive with or even better than stateoftheart CNNs Moreover Cordonnier et al 2020
use a small patch size of 22pixels which makes the model applicable only to smallresolution
images while we handle mediumresolution images as well
There has also been a lot of interest in combining convolutional neural networks CNNs with forms
of selfattention eg by augmenting feature maps for image classiﬁcation Bello et al 2019 or by
further processing the output of a CNN using selfattention eg for object detection Hu et al 2018
Carion et al 2020 video processing Wang et al 2018 Sun et al 2019 image classiﬁcation Wu
et al 2020 unsupervised object discovery Locatello et al 2020 or uniﬁed textvision tasks Chen
et al 2020c Lu et al 2019 Li et al 2019
Another recent related model is image GPT iGPT Chen et al 2020a which applies Transformers
to image pixels after reducing image resolution and color space The model is trained in an unsu
pervised fashion as a generative model and the resulting representation can then be ﬁnetuned or
probed linearly for classiﬁcation performance achieving a maximal accuracy of 72 on ImageNet
Our work adds to the increasing collection of papers that explore image recognition at larger scales
than the standard ImageNet dataset The use of additional data sources allows to achieve stateof
theart results on standard benchmarks Mahajan et al 2018 Touvron et al 2019 Xie et al 2020
Moreover Sun et al 2017 study how CNN performance scales with dataset size and Kolesnikov
et al 2020 Djolonga et al 2020 perform an empirical exploration of CNN transfer learning from
large scale datasets such as ImageNet21k and JFT300M We focus on these two latter datasets as
well but train Transformers instead of ResNetbased models used in prior works
2Published as a conference paper at ICLR 2021
Transformer 
Encoder
MLP 
Head
Vision 
Transformer 
ViT
Linear 
Projection 
of 
Flattened 
Patches

 
Extra 
learnable
     
class
 
embedding
1
2
3
4
5
6
7
8
90Patch 
 
Position 
Embedding
Class
Bird
Ball
Car

Embedded 
Patches
MultiHead 
Attention
Norm
MLP
Norm

L
 
x
Transformer 
Encoder
Figure 1 Model overview We split an image into ﬁxedsize patches linearly embed each of them
add position embeddings and feed the resulting sequence of vectors to a standard Transformer
encoder In order to perform classiﬁcation we use the standard approach of adding an extra learnable
classiﬁcation token to the sequence The illustration of the Transformer encoder was inspired by
Vaswani et al 2017
3 M ETHOD
In model design we follow the original Transformer Vaswani et al 2017 as closely as possible
An advantage of this intentionally simple setup is that scalable NLP Transformer architectures  and
their efﬁcient implementations  can be used almost out of the box
31 V ISION TRANSFORMER VIT
An overview of the model is depicted in Figure 1 The standard Transformer receives as input a 1D
sequence of token embeddings To handle 2D images we reshape the image x2RHWCinto a
sequence of ﬂattened 2D patches xp2RNP2C where HW is the resolution of the original
imageCis the number of channels PPis the resolution of each image patch and NHWP2
is the resulting number of patches which also serves as the effective input sequence length for the
Transformer The Transformer uses constant latent vector size Dthrough all of its layers so we
ﬂatten the patches and map to Ddimensions with a trainable linear projection Eq 1 We refer to
the output of this projection as the patch embeddings
Similar to BERTs class token we prepend a learnable embedding to the sequence of embed
ded patches  z0
0xclass whose state at the output of the Transformer encoder  z0
L serves as the
image representation yEq 4 Both during pretraining and ﬁnetuning a classiﬁcation head is at
tached to z0
L The classiﬁcation head is implemented by a MLP with one hidden layer at pretraining
time and by a single linear layer at ﬁnetuning time
Position embeddings are added to the patch embeddings to retain positional information We use
standard learnable 1D position embeddings since we have not observed signiﬁcant performance
gains from using more advanced 2Daware position embeddings Appendix D4 The resulting
sequence of embedding vectors serves as input to the encoder
The Transformer encoder Vaswani et al 2017 consists of alternating layers of multiheaded self
attention MSA see Appendix A and MLP blocks Eq 2 3 Layernorm LN is applied before
every block and residual connections after every block Wang et al 2019 Baevski  Auli 2019
3Published as a conference paper at ICLR 2021
The MLP contains two layers with a GELU nonlinearity
z0 xclassx1
pEx2
pExN
pE EposE2RP2CDEpos2RN1D1
z0
 MSALN z1 z1   1L 2
z MLPLN z0
 z0
   1L 3
y LN z0
L 4
Inductive bias We note that Vision Transformer has much less imagespeciﬁc inductive bias than
CNNs In CNNs locality twodimensional neighborhood structure and translation equivariance are
baked into each layer throughout the whole model In ViT only MLP layers are local and transla
tionally equivariant while the selfattention layers are global The twodimensional neighborhood
structure is used very sparingly in the beginning of the model by cutting the image into patches and
at ﬁnetuning time for adjusting the position embeddings for images of different resolution as de
scribed below Other than that the position embeddings at initialization time carry no information
about the 2D positions of the patches and all spatial relations between the patches have to be learned
from scratch
Hybrid Architecture As an alternative to raw image patches the input sequence can be formed
from feature maps of a CNN LeCun et al 1989 In this hybrid model the patch embedding
projection EEq 1 is applied to patches extracted from a CNN feature map As a special case
the patches can have spatial size 1x1 which means that the input sequence is obtained by simply
ﬂattening the spatial dimensions of the feature map and projecting to the Transformer dimension
The classiﬁcation input embedding and position embeddings are added as described above
32 F INETUNING AND HIGHER RESOLUTION
Typically we pretrain ViT on large datasets and ﬁnetune to smaller downstream tasks For
this we remove the pretrained prediction head and attach a zeroinitialized DKfeedforward
layer where Kis the number of downstream classes It is often beneﬁcial to ﬁnetune at higher
resolution than pretraining Touvron et al 2019 Kolesnikov et al 2020 When feeding images
of higher resolution we keep the patch size the same which results in a larger effective sequence
length The Vision Transformer can handle arbitrary sequence lengths up to memory constraints
however the pretrained position embeddings may no longer be meaningful We therefore perform
2D interpolation of the pretrained position embeddings according to their location in the original
image Note that this resolution adjustment and patch extraction are the only points at which an
inductive bias about the 2D structure of the images is manually injected into the Vision Transformer
4 E XPERIMENTS
We evaluate the representation learning capabilities of ResNet Vision Transformer ViT and the
hybrid To understand the data requirements of each model we pretrain on datasets of varying size
and evaluate many benchmark tasks When considering the computational cost of pretraining the
model ViT performs very favourably attaining state of the art on most recognition benchmarks at
a lower pretraining cost Lastly we perform a small experiment using selfsupervision and show
that selfsupervised ViT holds promise for the future
41 S ETUP
Datasets To explore model scalability we use the ILSVRC2012 ImageNet dataset with 1k classes
and 13M images we refer to it as ImageNet in what follows its superset ImageNet21k with
21k classes and 14M images Deng et al 2009 and JFT Sun et al 2017 with 18k classes and
303M highresolution images We deduplicate the pretraining datasets wrt the test sets of the
downstream tasks following Kolesnikov et al 2020 We transfer the models trained on these
dataset to several benchmark tasks ImageNet on the original validation labels and the cleanedup
ReaL labels Beyer et al 2020 CIFAR10100 Krizhevsky 2009 OxfordIIIT Pets Parkhi et al
2012 and Oxford Flowers102 Nilsback  Zisserman 2008 For these datasets preprocessing
follows Kolesnikov et al 2020
4Published as a conference paper at ICLR 2021
Model Layers Hidden size D MLP size Heads Params
ViTBase 12 768 3072 12 86M
ViTLarge 24 1024 4096 16 307M
ViTHuge 32 1280 5120 16 632M
Table 1 Details of Vision Transformer model variants
We also evaluate on the 19task VTAB classiﬁcation suite Zhai et al 2019b VTAB evaluates
lowdata transfer to diverse tasks using 1 000 training examples per task The tasks are divided into
three groups Natural  tasks like the above Pets CIFAR etc Specialized  medical and satellite
imagery and Structured  tasks that require geometric understanding like localization
Model Variants We base ViT conﬁgurations on those used for BERT Devlin et al 2019 as
summarized in Table 1 The Base and Large models are directly adopted from BERT and we
add the larger Huge model In what follows we use brief notation to indicate the model size and
the input patch size for instance ViTL16 means the Large variant with 1616input patch size
Note that the Transformers sequence length is inversely proportional to the square of the patch size
thus models with smaller patch size are computationally more expensive
For the baseline CNNs we use ResNet He et al 2016 but replace the Batch Normalization lay
ers Ioffe  Szegedy 2015 with Group Normalization Wu  He 2018 and used standardized
convolutions Qiao et al 2019 These modiﬁcations improve transfer Kolesnikov et al 2020
and we denote the modiﬁed model ResNet BiT For the hybrids we feed the intermediate fea
ture maps into ViT with patch size of one pixel To experiment with different sequence lengths
we either i take the output of stage 4 of a regular ResNet50 or ii remove stage 4 place the same
number of layers in stage 3 keeping the total number of layers and take the output of this extended
stage 3 Option ii results in a 4x longer sequence length and a more expensive ViT model
Training  Finetuning We train all models including ResNets using Adam Kingma  Ba
2015 with1 092 0999 a batch size of 4096 and apply a high weight decay of 01 which
we found to be useful for transfer of all models Appendix D1 shows that in contrast to common
practices Adam works slightly better than SGD for ResNets in our setting We use a linear learning
rate warmup and decay see Appendix B1 for details For ﬁnetuning we use SGD with momentum
batch size 512 for all models see Appendix B11 For ImageNet results in Table 2 we ﬁnetuned at
higher resolution 512for ViTL16 and 518for ViTH14 and also used Polyak  Juditsky 1992
averaging with a factor of 09999 Ramachandran et al 2019 Wang et al 2020b
Metrics We report results on downstream datasets either through fewshot or ﬁnetuning accuracy
Finetuning accuracies capture the performance of each model after ﬁnetuning it on the respective
dataset Fewshot accuracies are obtained by solving a regularized leastsquares regression problem
that maps the frozen representation of a subset of training images to f11gKtarget vectors This
formulation allows us to recover the exact solution in closed form Though we mainly focus on
ﬁnetuning performance we sometimes use linear fewshot accuracies for fast ontheﬂy evaluation
where ﬁnetuning would be too costly
42 C OMPARISON TO STATE OF THE ART
We ﬁrst compare our largest models  ViTH14 and ViTL16  to stateoftheart CNNs from
the literature The ﬁrst comparison point is Big Transfer BiT Kolesnikov et al 2020 which
performs supervised transfer learning with large ResNets The second is Noisy Student Xie et al
2020 which is a large EfﬁcientNet trained using semisupervised learning on ImageNet and JFT
300M with the labels removed Currently Noisy Student is the state of the art on ImageNet and
BiTL on the other datasets reported here All models were trained on TPUv3 hardware and we
report the number of TPUv3coredays taken to pretrain each of them that is the number of TPU
v3 cores 2 per chip used for training multiplied by the training time in days
Table 2 shows the results The smaller ViTL16 model pretrained on JFT300M outperforms BiTL
which is pretrained on the same dataset on all tasks while requiring substantially less computa
tional resources to train The larger model ViTH14 further improves the performance especially
on the more challenging datasets  ImageNet CIFAR100 and the VTAB suite Interestingly this
5Published as a conference paper at ICLR 2021
OursJFT OursJFT OursI21k BiTL Noisy Student
ViTH14 ViTL16 ViTL16 ResNet152x4 EfﬁcientNetL2
ImageNet 8855004 8776003 8530002 8754002 884885
ImageNet ReaL 9072005 9054003 8862005 9054 90 55
CIFAR10 9950006 9942003 9915003 9937006
CIFAR100 9455004 9390005 9325005 9351008
OxfordIIIT Pets 9756003 9732011 9467015 9662023
Oxford Flowers102 996800299740009961002 9963003
VTAB 19 tasks 7763023 7628046 7272021 7629170
TPUv3coredays 25k 068k 023k 99k 123k
Table 2 Comparison with state of the art on popular image classiﬁcation benchmarks We re
port mean and standard deviation of the accuracies averaged over three ﬁnetuning runs Vision
Transformer models pretrained on the JFT300M dataset outperform ResNetbased baselines on all
datasets while taking substantially less computational resources to pretrain ViT pretrained on the
smaller public ImageNet21k dataset performs well tooSlightly improved 885result reported
in Touvron et al 2020
VTAB 19 tasks65707580Accuracy 
Natural 7 tasks708090
Specialized 4 tasks8082858890
Structured 8 tasks506070ViTH14 BiTL R152x4 VIVIEx100 R50x3 S4L R50x1
Figure 2 Breakdown of VTAB performance in Natural Specialized  and Structured task groups
model still took substantially less compute to pretrain than prior state of the art However we note
that pretraining efﬁciency may be affected not only by the architecture choice but also other pa
rameters such as training schedule optimizer weight decay etc We provide a controlled study of
performance vs compute for different architectures in Section 44 Finally the ViTL16 model
pretrained on the public ImageNet21k dataset performs well on most datasets too while taking
fewer resources to pretrain it could be trained using a standard cloud TPUv3 with 8 cores in ap
proximately 30 days
Figure 2 decomposes the VTAB tasks into their respective groups and compares to previous SOTA
methods on this benchmark BiT VIVI  a ResNet cotrained on ImageNet and Youtube Tschannen
et al 2020 and S4L  supervised plus semisupervised learning on ImageNet Zhai et al 2019a
ViTH14 outperforms BiTR152x4 and other methods on the Natural andStructured tasks On the
Specialized the performance of the top two models is similar
43 P RETRAINING DATA REQUIREMENTS
The Vision Transformer performs well when pretrained on a large JFT300M dataset With fewer
inductive biases for vision than ResNets how crucial is the dataset size We perform two series of
experiments
First we pretrain ViT models on datasets of increasing size ImageNet ImageNet21k and JFT
300M To boost the performance on the smaller datasets we optimize three basic regularization
parameters  weight decay dropout and label smoothing Figure 3 shows the results after ﬁne
tuning to ImageNet results on other datasets are shown in Table 52 When pretrained on the
smallest dataset ImageNet ViTLarge models underperform compared to ViTBase models despite
moderate regularization With ImageNet21k pretraining their performances are similar Only
with JFT300M do we see the full beneﬁt of larger models Figure 3 also shows the performance
2Note that the ImageNet pretrained models are also ﬁnetuned but again on ImageNet This is because the
resolution increase during ﬁnetuning improves the performance
6Published as a conference paper at ICLR 2021
ImageNet ImageNet21k JFT300M
Pretraining dataset7075808590ImageNet Top1 Accuracy 
BiT
ViTB32
ViTB16ViTL32
ViTL16
ViTH14
Figure 3 Transfer to ImageNet While
large ViT models perform worse than BiT
ResNets shaded area when pretrained on
small datasets they shine when pretrained on
larger datasets Similarly larger ViT variants
overtake smaller ones as the dataset grows
10 M 30 M 100 M 300 M
Number of JFT pretraining samples3040506070Linear 5shot ImageNet Top1 
ViTL16
ViTL32ViTB32
ViTb32ResNet50x1 BiT
ResNet152x2 BiTFigure 4 Linear fewshot evaluation on Ima
geNet versus pretraining size ResNets per
form better with smaller pretraining datasets
but plateau sooner than ViT which performs
better with larger pretraining ViTb is ViTB
with all hidden dimensions halved
1021039095Transfer accuracy Average5
Transformer ViT
ResNet BiT
Hybrid
10210375808590ImageNet
Transformer ViT
ResNet BiT
Hybrid
Total pretraining compute exaFLOPs
Figure 5 Performance versus pretraining compute for different architectures Vision Transformers
ResNets and hybrids Vision Transformers generally outperform ResNets with the same compu
tational budget Hybrids improve upon pure Transformers for smaller model sizes but the gap
vanishes for larger models
region spanned by BiT models of different sizes The BiT CNNs outperform ViT on ImageNet but
with the larger datasets ViT overtakes
Second we train our models on random subsets of 9M 30M and 90M as well as the full JFT
300M dataset We do not perform additional regularization on the smaller subsets and use the same
hyperparameters for all settings This way we assess the intrinsic model properties and not the
effect of regularization We do however use earlystopping and report the best validation accuracy
achieved during training To save compute we report fewshot linear accuracy instead of full ﬁne
tuning accuracy Figure 4 contains the results Vision Transformers overﬁt more than ResNets with
comparable computational cost on smaller datasets For example ViTB32 is slightly faster than
ResNet50 it performs much worse on the 9M subset but better on 90M subsets The same is true
for ResNet152x2 and ViTL16 This result reinforces the intuition that the convolutional inductive
bias is useful for smaller datasets but for larger ones learning the relevant patterns directly from
data is sufﬁcient even beneﬁcial
Overall the fewshot results on ImageNet Figure 4 as well as the lowdata results on VTAB
Table 2 seem promising for very lowdata transfer Further analysis of fewshot properties of ViT
is an exciting direction of future work
7Published as a conference paper at ICLR 2021
44 S CALING STUDY
We perform a controlled scaling study of different models by evaluating transfer performance from
JFT300M In this setting data size does not bottleneck the models performances and we assess
performance versus pretraining cost of each model The model set includes 7 ResNets R50x1
R50x2 R101x1 R152x1 R152x2 pretrained for 7 epochs plus R152x2 and R200x3 pretrained
for 14 epochs 6 Vision Transformers ViTB32 B16 L32 L16 pretrained for 7 epochs plus
L16 and H14 pretrained for 14 epochs and 5 hybrids R50ViTB32 B16 L32 L16 pre
trained for 7 epochs plus R50ViTL16 pretrained for 14 epochs for hybrids the number at the
end of the model name stands not for the patch size but for the total dowsampling ratio in the ResNet
backbone
Figure 5 contains the transfer performance versus total pretraining compute see Appendix D5
for details on computational costs Detailed results per model are provided in Table 6 in the Ap
pendix A few patterns can be observed First Vision Transformers dominate ResNets on the
performancecompute tradeoff ViT uses approximately 24less compute to attain the same
performance average over 5 datasets Second hybrids slightly outperform ViT at small compu
tational budgets but the difference vanishes for larger models This result is somewhat surprising
since one might expect convolutional local feature processing to assist ViT at any size Third Vision
Transformers appear not to saturate within the range tried motivating future scaling efforts
45 I NSPECTING VISION TRANSFORMER
Input
 Attention
Figure 6 Representative ex
amples of attention from the
output token to the input
space See Appendix D7 for
detailsTo begin to understand how the Vision Transformer processes im
age data we analyze its internal representations The ﬁrst layer of
the Vision Transformer linearly projects the ﬂattened patches into a
lowerdimensional space Eq 1 Figure 7 left shows the top prin
cipal components of the the learned embedding ﬁlters The com
ponents resemble plausible basis functions for a lowdimensional
representation of the ﬁne structure within each patch
After the projection a learned position embedding is added to the
patch representations Figure 7 center shows that the model learns
to encode distance within the image in the similarity of position em
beddings ie closer patches tend to have more similar position em
beddings Further the rowcolumn structure appears patches in the
same rowcolumn have similar embeddings Finally a sinusoidal
structure is sometimes apparent for larger grids Appendix D That
the position embeddings learn to represent 2D image topology ex
plains why handcrafted 2Daware embedding variants do not yield
improvements Appendix D4
Selfattention allows ViT to integrate information across the entire
image even in the lowest layers We investigate to what degree
the network makes use of this capability Speciﬁcally we compute
the average distance in image space across which information is
integrated based on the attention weights Figure 7 right This
attention distance is analogous to receptive ﬁeld size in CNNs
We ﬁnd that some heads attend to most of the image already in the lowest layers showing that
the ability to integrate information globally is indeed used by the model Other attention heads
have consistently small attention distances in the low layers This highly localized attention is
less pronounced in hybrid models that apply a ResNet before the Transformer Figure 7 right
suggesting that it may serve a similar function as early convolutional layers in CNNs Further the
attention distance increases with network depth Globally we ﬁnd that the model attends to image
regions that are semantically relevant for classiﬁcation Figure 6
46 S ELFSUPERVISION
Transformers show impressive performance on NLP tasks However much of their success stems
not only from their excellent scalability but also from large scale selfsupervised pretraining Devlin
8Published as a conference paper at ICLR 2021
RGB embedding filters
first 28 principal components
1 2 3 4 5 6 7
Input patch column1
2
3
4
5
6
7Input patch rowPosition embedding similarity
1
1
Cosine similarity
0 5 10 15 20
Network depth layer020406080100120Mean attention distance pixels
ViTL16
Head 1
Head 2
Head 3

Figure 7 Left Filters of the initial linear embedding of RGB values of ViTL32 Center Sim
ilarity of position embeddings of ViTL32 Tiles show the cosine similarity between the position
embedding of the patch with the indicated row and column and the position embeddings of all other
patches Right Size of attended area by head and network depth Each dot shows the mean attention
distance across images for one of 16 heads at one layer See Appendix D7 for details
et al 2019 Radford et al 2018 We also perform a preliminary exploration on masked patch
prediction for selfsupervision mimicking the masked language modeling task used in BERT With
selfsupervised pretraining our smaller ViTB16 model achieves 799 accuracy on ImageNet a
signiﬁcant improvement of 2 to training from scratch but still 4 behind supervised pretraining
Appendix B12 contains further details We leave exploration of contrastive pretraining Chen
et al 2020b He et al 2020 Bachman et al 2019 H enaff et al 2020 to future work
5 C ONCLUSION
We have explored the direct application of Transformers to image recognition Unlike prior works
using selfattention in computer vision we do not introduce imagespeciﬁc inductive biases into
the architecture apart from the initial patch extraction step Instead we interpret an image as a
sequence of patches and process it by a standard Transformer encoder as used in NLP This simple
yet scalable strategy works surprisingly well when coupled with pretraining on large datasets
Thus Vision Transformer matches or exceeds the state of the art on many image classiﬁcation
datasets whilst being relatively cheap to pretrain
While these initial results are encouraging many challenges remain One is to apply ViT to other
computer vision tasks such as detection and segmentation Our results coupled with those in Carion
et al 2020 indicate the promise of this approach Another challenge is to continue exploring self
supervised pretraining methods Our initial experiments show improvement from selfsupervised
pretraining but there is still large gap between selfsupervised and largescale supervised pre
training Finally further scaling of ViT would likely lead to improved performance
ACKNOWLEDGEMENTS
The work was performed in Berlin Z urich and Amsterdam We thank many colleagues at Google
for their help in particular Andreas Steiner for crucial help with the infrastructure and the open
source release of the code Joan Puigcerver and Maxim Neumann for help with the largescale
training infrastructure Dmitry Lepikhin Aravindh Mahendran Daniel Keysers Mario Lu ˇcic Noam
Shazeer Ashish Vaswani and Colin Raffel for useful discussions
REFERENCES
Samira Abnar and Willem Zuidema Quantifying attention ﬂow in transformers In ACL 2020
Philip Bachman R Devon Hjelm and William Buchwalter Learning representations by maximizing
mutual information across views In NeurIPS  2019
9Published as a conference paper at ICLR 2021
Alexei Baevski and Michael Auli Adaptive input representations for neural language modeling In
ICLR  2019
I Bello B Zoph Q Le A Vaswani and J Shlens Attention augmented convolutional networks
InICCV  2019
Lucas Beyer Olivier J H enaff Alexander Kolesnikov Xiaohua Zhai and A aron van den Oord Are
we done with imagenet arXiv  2020
Tom B Brown Benjamin Mann Nick Ryder Melanie Subbiah Jared Kaplan Prafulla Dhariwal
Arvind Neelakantan Pranav Shyam Girish Sastry Amanda Askell et al Language models are
fewshot learners arXiv  2020
Nicolas Carion Francisco Massa Gabriel Synnaeve Nicolas Usunier Alexander Kirillov and
Sergey Zagoruyko Endtoend object detection with transformers In ECCV  2020
Mark Chen Alec Radford Rewon Child Jeff Wu and Heewoo Jun Generative pretraining from
pixels In ICML  2020a
Ting Chen Simon Kornblith Mohammad Norouzi and Geoffrey E Hinton A simple framework
for contrastive learning of visual representations In ICML  2020b
YenChun Chen Linjie Li Licheng Yu Ahmed El Kholy Faisal Ahmed Zhe Gan Yu Cheng and
Jingjing Liu UNITER UNiversal ImageTExt Representation Learning In ECCV  2020c
Rewon Child Scott Gray Alec Radford and Ilya Sutskever Generating long sequences with sparse
transformers arXiv  2019
JeanBaptiste Cordonnier Andreas Loukas and Martin Jaggi On the relationship between self
attention and convolutional layers In ICLR  2020
J Deng W Dong R Socher L Li Kai Li and Li FeiFei Imagenet A largescale hierarchical
image database In CVPR  2009
Jacob Devlin MingWei Chang Kenton Lee and Kristina Toutanova BERT Pretraining of deep
bidirectional transformers for language understanding In NAACL  2019
Josip Djolonga Jessica Yung Michael Tschannen Rob Romijnders Lucas Beyer Alexander
Kolesnikov Joan Puigcerver Matthias Minderer Alexander DAmour Dan Moldovan Sylvan
Gelly Neil Houlsby Xiaohua Zhai and Mario Lucic On robustness and transferability of convo
lutional neural networks arXiv  2020
Kaiming He Xiangyu Zhang Shaoqing Ren and Jian Sun Deep residual learning for image recog
nition In CVPR  2016
Kaiming He Haoqi Fan Yuxin Wu Saining Xie and Ross Girshick Momentum contrast for
unsupervised visual representation learning In CVPR  2020
Jonathan Ho Nal Kalchbrenner Dirk Weissenborn and Tim Salimans Axial attention in multidi
mensional transformers arXiv  2019
Han Hu Jiayuan Gu Zheng Zhang Jifeng Dai and Yichen Wei Relation networks for object
detection In CVPR  2018
Han Hu Zheng Zhang Zhenda Xie and Stephen Lin Local relation networks for image recognition
InICCV  2019
Zilong Huang Xinggang Wang Yunchao Wei Lichao Huang Humphrey Shi Wenyu Liu and
Thomas S Huang Ccnet Crisscross attention for semantic segmentation In ICCV  2020
Olivier J H enaff Aravind Srinivas Jeffrey De Fauw Ali Razavi Carl Doersch S M Ali Eslami
and Aaron van den Oord Dataefﬁcient image recognition with contrastive predictive coding In
ICML  2020
10Published as a conference paper at ICLR 2021
Sergey Ioffe and Christian Szegedy Batch normalization Accelerating deep network training by
reducing internal covariate shift 2015
Diederik P Kingma and Jimmy Ba Adam A method for stochastic optimization In ICLR  2015
Alexander Kolesnikov Lucas Beyer Xiaohua Zhai Joan Puigcerver Jessica Yung Sylvain Gelly
and Neil Houlsby Big transfer BiT General visual representation learning In ECCV  2020
Alex Krizhevsky Learning multiple layers of features from tiny images Technical report 2009
Alex Krizhevsky Ilya Sutskever and Geoffrey E Hinton Imagenet classiﬁcation with deep convo
lutional neural networks In NIPS  2012
Y  LeCun B Boser J Denker D Henderson R Howard W Hubbard and L Jackel Backpropa
gation applied to handwritten zip code recognition Neural Computation  1541551 1989
Dmitry Lepikhin HyoukJoong Lee Yuanzhong Xu Dehao Chen Orhan Firat Yanping Huang
Maxim Krikun Noam Shazeer and Zhifeng Chen Gshard Scaling giant models with conditional
computation and automatic sharding arXiv  2020
Liunian Harold Li Mark Yatskar Da Yin ChoJui Hsieh and KaiWei Chang VisualBERT A
Simple and Performant Baseline for Vision and Language In Arxiv  2019
Francesco Locatello Dirk Weissenborn Thomas Unterthiner Aravindh Mahendran Georg Heigold
Jakob Uszkoreit Alexey Dosovitskiy and Thomas Kipf Objectcentric learning with slot atten
tion arXiv  2020
Jiasen Lu Dhruv Batra Devi Parikh and Stefan Lee ViLBERT Pretraining TaskAgnostic Visi
olinguistic Representations for VisionandLanguage Tasks In NeurIPS  2019
Dhruv Mahajan Ross Girshick Vignesh Ramanathan Kaiming He Manohar Paluri Yixuan Li
Ashwin Bharambe and Laurens van der Maaten Exploring the limits of weakly supervised
pretraining In ECCV  2018
M Nilsback and A Zisserman Automated ﬂower classiﬁcation over a large number of classes In
ICVGIP  2008
Omkar M Parkhi Andrea Vedaldi Andrew Zisserman and C V  Jawahar Cats and dogs In CVPR 
2012
Niki Parmar Ashish Vaswani Jakob Uszkoreit Lukasz Kaiser Noam Shazeer Alexander Ku and
Dustin Tran Image transformer In ICML  2018
B T Polyak and A B Juditsky Acceleration of stochastic approximation by averaging SIAM
Journal on Control and Optimization  304838855 1992 doi 1011370330046 URL
httpsdoiorg1011370330046 
Siyuan Qiao Huiyu Wang Chenxi Liu Wei Shen and Alan Yuille Weight standardization arXiv
preprint arXiv190310520  2019
Alec Radford Karthik Narasimhan Tim Salimans and Ilya Sutskever Improving language under
standing with unsupervised learning Technical Report  2018
Alec Radford Jeff Wu Rewon Child David Luan Dario Amodei and Ilya Sutskever Language
models are unsupervised multitask learners Technical Report  2019
Prajit Ramachandran Niki Parmar Ashish Vaswani Irwan Bello Anselm Levskaya and Jon Shlens
Standalone selfattention in vision models In NeurIPS  2019
Chen Sun Abhinav Shrivastava Saurabh Singh and Abhinav Gupta Revisiting unreasonable ef
fectiveness of data in deep learning era In ICCV  2017
Chen Sun Austin Myers Carl V ondrick Kevin Murphy and Cordelia Schmid Videobert A joint
model for video and language representation learning In ICCV  2019
11Published as a conference paper at ICLR 2021
Hugo Touvron Andrea Vedaldi Matthijs Douze and Herve Jegou Fixing the traintest resolution
discrepancy In NeurIPS  2019
Hugo Touvron Andrea Vedaldi Matthijs Douze and Herve Jegou Fixing the traintest resolution
discrepancy Fixefﬁcientnet arXiv preprint arXiv200308237  2020
Michael Tschannen Josip Djolonga Marvin Ritter Aravindh Mahendran Neil Houlsby Sylvain
Gelly and Mario Lucic Selfsupervised learning of videoinduced visual invariances In Pro
ceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition CVPR  June
2020
Ashish Vaswani Noam Shazeer Niki Parmar Jakob Uszkoreit Llion Jones Aidan N Gomez
Łukasz Kaiser and Illia Polosukhin Attention is all you need In NIPS  2017
Huiyu Wang Yukun Zhu Bradley Green Hartwig Adam Alan Yuille and LiangChieh Chen
Axialdeeplab Standalone axialattention for panoptic segmentation In ECCV  2020a
Huiyu Wang Yukun Zhu Bradley Green Hartwig Adam Alan Yuille and LiangChieh
Chen Axialdeeplab Standalone axialattention for panoptic segmentation arXiv preprint
arXiv200307853  2020b
Qiang Wang Bei Li Tong Xiao Jingbo Zhu Changliang Li Derek F Wong and Lidia S Chao
Learning deep transformer models for machine translation In ACL 2019
Xiaolong Wang Ross Girshick Abhinav Gupta and Kaiming He Nonlocal neural networks In
CVPR  2018
Dirk Weissenborn Oscar T ackstr om and Jakob Uszkoreit Scaling autoregressive video models In
ICLR  2019
Bichen Wu Chenfeng Xu Xiaoliang Dai Alvin Wan Peizhao Zhang Masayoshi Tomizuka Kurt
Keutzer and Peter Vajda Visual transformers Tokenbased image representation and processing
for computer vision arxiv  2020
Yuxin Wu and Kaiming He Group normalization In ECCV  2018
Qizhe Xie MinhThang Luong Eduard Hovy and Quoc V  Le Selftraining with noisy student
improves imagenet classiﬁcation In CVPR  2020
Xiaohua Zhai Avital Oliver Alexander Kolesnikov and Lucas Beyer S4L SelfSupervised Semi
Supervised Learning In ICCV  2019a
Xiaohua Zhai Joan Puigcerver Alexander Kolesnikov Pierre Ruyssen Carlos Riquelme Mario
Lucic Josip Djolonga Andre Susano Pinto Maxim Neumann Alexey Dosovitskiy et al A
largescale study of representation learning with the visual task adaptation benchmark arXiv
preprint arXiv191004867  2019b
Hengshuang Zhao Jiaya Jia and Vladlen Koltun Exploring selfattention for image recognition In
CVPR  2020
12Published as a conference paper at ICLR 2021
Models Dataset Epochs Base LR LR decay Weight decay Dropout
ViTBf1632g JFT300M 7 8104linear 01 00
ViTL32 JFT300M 7 6104linear 01 00
ViTL16 JFT300M 714 4104linear 01 00
ViTH14 JFT300M 14 3104linear 01 00
R50xf12g JFT300M 7 103linear 01 00
R101x1 JFT300M 7 8104linear 01 00
R152xf12g JFT300M 7 6104linear 01 00
R50ViTBf1632gJFT300M 7 8104linear 01 00
R50ViTL32 JFT300M 7 2104linear 01 00
R50ViTL16 JFT300M 714 4104linear 01 00
ViTBf1632g ImageNet21k 90 103linear 003 01
ViTLf1632g ImageNet21k 3090 103linear 003 01
ViT ImageNet 300 3103cosine 03 01
Table 3 Hyperparameters for training All models are trained with a batch size of 4096 and learn
ing rate warmup of 10k steps For ImageNet we found it beneﬁcial to additionally apply gradient
clipping at global norm 1 Training resolution is 224
APPENDIX
A M ULTIHEAD SELFATTENTION
Standard qkv selfattention SA Vaswani et al 2017 is a popular building block for neural archi
tectures For each element in an input sequence z2RND we compute a weighted sum over all
values vin the sequence The attention weights Aijare based on the pairwise similarity between
two elements of the sequence and their respective query qiand key kjrepresentations
qkv zUqkv Uqkv2RD3Dh 5
A softmax
qkp
Dh
A2RNN 6
SAz Av 7
Multihead selfattention MSA is an extension of SA in which we run kselfattention operations
called heads in parallel and project their concatenated outputs To keep compute and number of
parameters constant when changing kDhEq 5 is typically set to Dk 
MSA z  SA 1z SA 2z SAkzUmsa Umsa2RkDhD8
B E XPERIMENT DETAILS
B1 T RAINING
Table 3 summarizes our training setups for our different models We found strong regularization
to be key when training models from scratch on ImageNet Dropout when used is applied after
every dense layer except for the the qkvprojections and directly after adding positional to patch
embeddings Hybrid models are trained with the exact setup as their ViT counterparts Finally all
training is done on resolution 224
B11 F INETUNING
We ﬁnetune all ViT models using SGD with a momentum of 09 We run a small grid search over
learning rates see learning rate ranges in Table 4 To do so we use small subsplits from the training
set 10 for Pets and Flowers 2 for CIFAR 1 ImageNet as development set and train on the
remaining data For ﬁnal results we train on the entire training set and evaluate on the respective
test data For ﬁnetuning ResNets and hybrid models we use the exact same setup with the only
exception of ImageNet where we add another value 006to the learning rate sweep Additionally
13Published as a conference paper at ICLR 2021
Dataset Steps Base LR
ImageNet 20 000 f0003 001 003 006 g
CIFAR100 10 000 f0001 0003 001 003 g
CIFAR10 10 000 f0001 0003 001 003 g
OxfordIIIT Pets 500 f0001 0003 001 003 g
Oxford Flowers102 500 f0001 0003 001 003 g
VTAB 19 tasks 2 500 001
Table 4 Hyperparameters for ﬁnetuning All models are ﬁnetuned with cosine learning rate decay
a batch size of 512 no weight decay and grad clipping at global norm 1 If not mentioned otherwise
ﬁnetuning resolution is 384
for ResNets we also run the setup of Kolesnikov et al 2020 and select the best results across
this run and our sweep Finally if not mentioned otherwise all ﬁnetuning experiments run at 384
resolution running ﬁnetuning at different resolution than training is common practice Kolesnikov
et al 2020
When transferring ViT models to another dataset we remove the whole head two linear layers and
replace it by a single zeroinitialized linear layer outputting the number of classes required by the
target dataset We found this to be a little more robust than simply reinitializing the very last layer
For VTAB we follow the protocol in Kolesnikov et al 2020 and use the same hyperparameter
setting for all tasks We use a learning rate of 001and train for 2500 steps Tab 4 We chose this
setting by running a small sweep over two learning rates and two schedules and selecting the setting
with the highest VTAB score on the 200example validation sets We follow the preprocessing used
in Kolesnikov et al 2020 except that we do not use taskspeciﬁc input resolutions Instead we ﬁnd
that Vision Transformer beneﬁts most from a high resolution  384384 for all tasks
B12 S ELFSUPERVISION
We employ the masked patch prediction objective for preliminary selfsupervision experiments To
do so we corrupt 50 of patch embeddings by either replacing their embeddings with a learnable
mask embedding 80 a random other patch embedding 10 or just keeping them as is
10 This setup is very similar to the one used for language by Devlin et al 2019 Finally we
predict the 3bit mean color ie 512 colors in total of every corrupted patch using their respective
patch representations
We trained our selfsupervised model for 1M steps ca 14 epochs with batch size 4096 on JFT We
use Adam with a base learning rate of 2104 warmup of 10k steps and cosine learning rate decay
As prediction targets for pretraining we tried the following settings 1 predicting only the mean
3bit color ie 1 prediction of 512 colors 2 predicting a 44downsized version of the 1616
patch with 3bit colors in parallel ie 16 predictions of 512 colors 3 regression on the full patch
using L2 ie 256 regressions on the 3 RGB channels Surprisingly we found that all worked quite
well though L2 was slightly worse We report ﬁnal results only for option 1 because it has shown
best fewshot performance We also experimented with 15 corruption rate as used by Devlin et al
2019 but results were also slightly worse on our fewshot metrics
Lastly we would like to remark that our instantiation of masked patch prediction doesnt require
such an enormous amount of pretraining nor a large dataset such as JFT in order to lead to sim
ilar performance gains on ImageNet classiﬁcation That is we observed diminishing returns on
downstream performance after 100k pretraining steps and see similar gains when pretraining on
ImageNet
C A DDITIONAL RESULTS
We report detailed results corresponding to the ﬁgures presented in the paper Table 5 corresponds
to Figure 3 from the paper and shows transfer performance of different ViT models pretrained
on datasets of increasing size ImageNet ImageNet21k and JFT300M Table 6 corresponds to
14Published as a conference paper at ICLR 2021
ViTB16 ViTB32 ViTL16 ViTL32 ViTH14
ImageNet CIFAR10 9813 9777 9786 9794 
CIFAR100 8713 8631 8635 8707 
ImageNet 7791 7338 7653 7116 
ImageNet ReaL 8357 7956 8219 7783 
Oxford Flowers102 8949 8543 8966 8636 
OxfordIIITPets 9381 9204 9364 9135 
ImageNet21k CIFAR10 9895 9879 9916 9913 9927
CIFAR100 9167 9197 9344 9304 9382
ImageNet 8397 8128 8515 8099 8513
ImageNet ReaL 8835 8663 8840 8565 8870
Oxford Flowers102 9938 9911 9961 9919 9951
OxfordIIITPets 9443 9302 9473 9309 9482
JFT300M CIFAR10 9900 9861 9938 9919 9950
CIFAR100 9187 9049 9404 9252 9455
ImageNet 8415 8073 8712 8437 8804
ImageNet ReaL 8885 8627 8999 8828 9033
Oxford Flowers102 9956 9927 9956 9945 9968
OxfordIIITPets 9580 9340 9711 9583 9756
Table 5 Top1 accuracy in  of Vision Transformer on various datasets when pretrained on Im
ageNet ImageNet21k or JFT300M These values correspond to Figure 3 in the main text Models
are ﬁnetuned at 384 resolution Note that the ImageNet results are computed without additional
techniques Polyak averaging and 512 resolution images used to achieve results in Table 2
Epochs ImageNet ImageNet ReaL CIFAR10 CIFAR100 Pets Flowers exaFLOPs
name
ViTB32 7 8073 8627 9861 9049 9340 9927 55
ViTB16 7 8415 8885 9900 9187 9580 9956 224
ViTL32 7 8437 8828 9919 9252 9583 9945 196
ViTL16 7 8630 8943 9938 9346 9681 9966 783
ViTL16 14 8712 8999 9938 9404 9711 9956 1567
ViTH14 14 8808 9036 9950 9471 9711 9971 4262
ResNet50x1 7 7754 8456 9767 8607 9111 9426 50
ResNet50x2 7 8212 8794 9829 8920 9343 9702 199
ResNet101x1 7 8067 8707 9848 8917 9408 9595 96
ResNet152x1 7 8188 8796 9882 9022 9417 9694 141
ResNet152x2 7 8497 8969 9906 9205 9537 9862 563
ResNet152x2 14 8556 8989 9924 9192 9575 9875 1126
ResNet200x3 14 8722 9015 9934 9353 9632 9904 3306
R50x1ViTB32 7 8490 8915 9901 9224 9575 9946 106
R50x1ViTB16 7 8558 8965 9914 9263 9665 9940 274
R50x1ViTL32 7 8568 8904 9924 9293 9697 9943 246
R50x1ViTL16 7 8660 8972 9918 9364 9703 9940 859
R50x1ViTL16 14 8712 8976 9931 9389 9736 9911 1668
Table 6 Detailed results of model scaling experiments These correspond to Figure 5 in the main
paper We show transfer accuracy on several datasets as well as the pretraining compute in ex
aFLOPs
Figure 5 from the paper and shows the transfer performance of ViT ResNet and hybrid models of
varying size as well as the estimated computational cost of their pretraining
D A DDITIONAL ANALYSES
D1 SGD VS ADAM FOR RESNETS
ResNets are typically trained with SGD and our use of Adam as optimizer is quite unconventional
Here we show the experiments that motivated this choice Namely we compare the ﬁnetuning
15Published as a conference paper at ICLR 2021
ResNet50 ResNet152x2
Dataset Adam SGD Adam SGD
ImageNet 7754 78 24 84 97 84 37
CIFAR10 9767 97 46 99 06 99 07
CIFAR100 8607 85 17 92 05 91 06
OxfordIIIT Pets 9111 91 00 95 37 94 79
Oxford Flowers102 9426 92 06 98 62 99 32
Average 8933 88 79 94 01 93 72
Table 7 Finetuning ResNet models pretrained with Adam and SGD
100101
Relative Compute0203040506ImageNet 5shot
Models
All
Depth
Patch size
Width MLP
Width
100101
Relative Compute0405060708Average 5shot
Models
All
Depth
Patch size
Width MLP
Width
Figure 8 Scaling different model dimensions of the Vision Transformer
performance of two ResNets  50x1 and 152x2  pretrained on JFT with SGD and Adam For
SGD we use the hyperparameters recommended by Kolesnikov et al 2020 Results are presented
in Table 7 Adam pretraining outperforms SGD pretraining on most datasets and on average
This justiﬁes the choice of Adam as the optimizer used to pretrain ResNets on JFT Note that the
absolute numbers are lower than those reported by Kolesnikov et al 2020 since we pretrain only
for7epochs not 30
D2 T RANSFORMER SHAPE
We ran ablations on scaling different dimensions of the Transformer architecture to ﬁnd out which
are best suited for scaling to very large models Figure 8 shows 5shot performance on ImageNet
for different conﬁgurations All conﬁgurations are based on a ViT model with 8layersD 1024 
DMLP  2048 and a patch size of 32 the intersection of all lines We can see that scaling the
depth results in the biggest improvements which are clearly visible up until 64 layers However
diminishing returns are already visible after 16 layers Interestingly scaling the width of the net
work seems to result in the smallest changes Decreasing the patch size and thus increasing the
effective sequence length shows surprisingly robust improvements without introducing parameters
These ﬁndings suggest that compute might be a better predictor of performance than the number of
parameters and that scaling should emphasize depth over width if any Overall we ﬁnd that scaling
all dimensions proportionally results in robust improvements
D3 H EAD TYPE AND C L A S S TOKEN
In order to stay as close as possible to the original Transformer model we made use of an additional
class token which is taken as image representation The output of this token is then trans
formed into a class prediction via a small multilayer perceptron MLP with tanh as nonlinearity
in the single hidden layer
This design is inherited from the Transformer model for text and we use it throughout the main
paper An initial attempt at using only imagepatch embeddings globally averagepooling GAP
them followed by a linear classiﬁerjust like ResNets ﬁnal feature mapperformed very poorly
However we found that this is neither due to the extra token nor to the GAP operation Instead
16Published as a conference paper at ICLR 2021
0 1 2 3 4 5 6 7
Epochs of training2530354045505560ImageNet linear 5shot accuracy CLSToken lr8e4
GAP lr8e4
GAP lr3e4
Figure 9 Comparison of classtoken and global average pooling classiﬁers Both work similarly
well but require different learningrates
Pos Emb DefaultStem Every Layer Every LayerShared
No Pos Emb 061382 NA NA
1D Pos Emb 064206 063964 064292
2D Pos Emb 064001 064046 064022
Rel Pos Emb 064032 NA NA
Table 8 Results of the ablation study on positional embeddings with ViTB16 model evaluated on
ImageNet 5shot linear
the difference in performance is fully explained by the requirement for a different learningrate see
Figure 9
D4 P OSITIONAL EMBEDDING
We ran ablations on different ways of encoding spatial information using positional embedding We
tried the following cases
 Providing no positional information Considering the inputs as a bag of patches 
 1dimensional positional embedding Considering the inputs as a sequence of patches in
the raster order default across all other experiments in this paper
 2dimensional positional embedding Considering the inputs as a grid of patches in two
dimensions In this case two sets of embeddings are learned each for one of the axes
Xembedding and Yembedding each with size D2 Then based on the coordinate on
the path in the input we concatenate the XandYembedding to get the ﬁnal positional
embedding for that patch
 Relative positional embeddings Considering the relative distance between patches to en
code the spatial information as instead of their absolute position To do so we use 1
dimensional Relative Attention in which we deﬁne the relative distance all possible pairs
of patches Thus for every given pair one as query and the other as keyvalue in the at
tention mechanism we have an offset pqpk where each offset is associated with an
embedding Then we simply run extra attention where we use the original query the
content of query but use relative positional embeddings as keys We then use the log
its from the relative attention as a bias term and add it to the logits of the main attention
contentbased attention before applying the softmax
In addition to different ways of encoding spatial information we also tried different ways of in
corporating this information in our model For the 1dimensional and 2dimensional positional
embeddings we tried three different cases 1 add positional embeddings to the inputs right after
17Published as a conference paper at ICLR 2021
1234567891011121314
Input patch column1
2
3
4
5
6
7
8
9
10
11
12
13
14Input patch rowViTL16
7 epochs LR00002 WD001
1
1
Cosine similarity
1234567891011121314
Input patch column1
2
3
4
5
6
7
8
9
10
11
12
13
14Input patch rowViTL16
7 epochs LR00004 WD01
1
1
Cosine similarity
1234567891011121314
Input patch column1
2
3
4
5
6
7
8
9
10
11
12
13
14Input patch rowViTL16
14 epochs LR00004 WD01
1
1
Cosine similarity
Figure 10 Position embeddings of models trained with different hyperparameters
the stem of them model and before feeding the inputs to the Transformer encoder default across
all other experiments in this paper 2 learn and add positional embeddings to the inputs at the
beginning of each layer 3 add a learned positional embeddings to the inputs at the beginning of
each layer shared between layers
Table 8 summarizes the results from this ablation study on a ViTB16 model As we can see while
there is a large gap between the performances of the model with no positional embedding and mod
els with positional embedding there is little to no difference between different ways of encoding
positional information We speculate that since our Transformer encoder operates on patchlevel
inputs as opposed to pixellevel the differences in how to encode spatial information is less impor
tant More precisely in patchlevel inputs the spatial dimensions are much smaller than the original
pixellevel inputs eg 1414as opposed to 224224 and learning to represent the spatial re
lations in this resolution is equally easy for these different positional encoding strategies Even so
the speciﬁc pattern of position embedding similarity learned by the network depends on the training
hyperparameters Figure 10
0 5 10 15 20
Network depth layer020406080100120Mean attention distance pixels
ViTL16
Head 1
Head 2
Head 3

0 5 10 15 20
Network depth layer020406080100120
R50x1  ViTL16
Head 1
Head 2
Head 3

Figure 11 Size of attended area by head and network depth Attention distance was computed for
128 example images by averaging the distance between the query pixel and all other pixels weighted
by the attention weight Each dot shows the mean attention distance across images for one of 16
heads at one layer Image width is 224 pixels
D5 E MPIRICAL COMPUTATIONAL COSTS
We are also interested in realworld speed of the architectures on our hardware which is not always
well predicted by theoretical FLOPs due to details like lane widths and cache sizes For this purpose
18Published as a conference paper at ICLR 2021
we perform timing of inference speed for the main models of interest on a TPUv3 accelerator the
difference between inference and backprop speed is a constant modelindependent factor
Figure 12 left shows how many images one core can handle per second across various input sizes
Every single point refers to the peak performance measured across a wide range of batchsizes As
can be seen the theoretical biquadratic scaling of ViT with image size only barely starts happening
for the largest models at the largest resolutions
Another quantity of interest is the largest batchsize each model can ﬁt onto a core larger being
better for scaling to large datasets Figure 12 right shows this quantity for the same set of models
This shows that large ViT models have a clear advantage in terms of memoryefﬁciency over ResNet
models
64 128 224 384 512
Input size px102103104Peak inference speed imgseccore64 128 224 384 512
Input size px102103Largest percore batchsizeR50x1
R50x2ViTB32
ViTL32ViTB16
ViTL16ViTH14
R152x4
Figure 12 Left Real wallclock timings of various architectures across input sizes ViT models
have speed comparable to similar ResNets Right  Largest percore batchsize ﬁtting on device with
various architectures across input sizes ViT models are clearly more memoryefﬁcient
D6 A XIAL ATTENTION
Axial Attention Huang et al 2020 Ho et al 2019 is a simple yet effective technique to run self
attention on large inputs that are organized as multidimensional tensors The general idea of axial
attention is to perform multiple attention operations each along a single axis of the input tensor
instead of applying 1dimensional attention to the ﬂattened version of the input In axial attention
each attention mixes information along a particular axis while keeping information along the other
axes independent Along this line Wang et al 2020b proposed the AxialResNet model in which
all the convolutions with kernel size 33in a ResNet50 are replaced by axial selfattention ie
a row and column attention augmented by relative positional encoding We have implemented
AxialResNet as a baseline model3
Moreover we have modiﬁed ViT to process inputs in the 2dimensional shape instead of a 1
dimensional sequence of patches and incorporate Axial Transformer blocks in which instead of
a selfattention followed by an MLP we have a a rowselfattention plus an MLP followed by a
columnselfattention plus an MLP
Figure 13 present the performance of Axial ResNet AxialViTB32 and AxialViTB16 on Ima
geNet 5shot linear when pretrained on JFT dataset verses the pretraining compute both in terms of
number of FLOPs and inference time example per seconds As we can see both AxialViTB32
and AxialViTB16 do better than their ViTB counterpart in terms of performance but it comes at
3Our implementation is based on the opensourced PyTorch implementation in httpsgithubcom
csrhddlamaxialdeeplab  In our experiments we reproduced the scores reported in Wang et al
2020b in terms of accuracy however our implementation similar to the opensource implementation is very
slow on TPUs Therefore we were not able to use it for extensive largescale experiments These may be
unlocked by a carefully optimized implementation
19Published as a conference paper at ICLR 2021
102
Total compute exaFLOPs0500052505500575060006250650ImageNet 5shot linear top1 accuracyAxialViTB16
AxialViTB32ViTB16
ViTB32
ResNet50AxialResNet50
102103
Peak inference speed imgseccore0500052505500575060006250650ImageNet 5shot linear top1 accuracyAxialViTB16
AxialViTB32ViTB16
ViTB32
ResNet50AxialResNet50
Figure 13 Performance of AxialAttention based models in terms of top1 accuracy on ImageNet
5shot linear versus their speed in terms of number of FLOPs  left and inference time  left
the cost of more compute This is because in AxialViT models each Transformer block with global
selfattention is replaced by two Axial Transformer blocks one with row and one with column self
attention and although the sequence length that selfattention operates on is smaller in axial case
there is a extra MLP per AxialViT block For the AxialResNet although it looks reasonable in
terms of accuracycompute tradeoff Figure 13 left the naive implementation is extremely slow
on TPUs Figure 13 right
D7 A TTENTION DISTANCE
To understand how ViT uses selfattention to integrate information across the image we analyzed
the average distance spanned by attention weights at different layers Figure 11 This attention
distance is analogous to receptive ﬁeld size in CNNs Average attention distance is highly variable
across heads in lower layers with some heads attending to much of the image while others attend
to small regions at or near the query location As depth increases attention distance increases for all
heads In the second half of the network most heads attend widely across tokens
D8 A TTENTION MAPS
To compute maps of the attention from the output token to the input space Figures 6 and 14 we
used Attention Rollout Abnar  Zuidema 2020 Brieﬂy we averaged attention weights of ViT
L16 across all heads and then recursively multiplied the weight matrices of all layers This accounts
for the mixing of attention across tokens through all layers
D9 O BJECT NETRESULTS
We also evaluate our ﬂagship ViTH14 model on the ObjectNet benchmark following the evaluation
setup in Kolesnikov et al 2020 resulting in 821 top5 accuracy and 617 top1 accuracy
D10 VTAB B REAKDOWN
Table 9 shows the scores attained on each of the VTAB1k tasks
20Published as a conference paper at ICLR 2021
1
 2
 3
 4
 5
 6
 7
 8
9
 10
 11
 12
 13
 14
 15
 16
17
 18
 19
 20
 21
 22
 23
 24
25
 26
 27
 28
 29
 30
 31
 32
33
 34
 35
 36
 37
 38
 39
 40
41
 42
 43
 44
 45
 46
 47
 48
49
 50
 51
 52
 53
 54
 55
 56
57
 58
 59
 60
 61
 62
 63
 64
65
 66
 67
 68
 69
 70
 71
 72
73
 74
 75
 76
 77
 78
 79
 80
81
 82
 83
 84
 85
 86
 87
 88
89
 90
 91
 92
 93
 94
 95
 96
97
 98
 99
 100
 101
 102
 103
 104
105
 106
 107
 108
 109
 110
 111
 112
113
 114
 115
 116
 117
 118
 119
 120
121
 122
 123
 124
 125
 126
 127
 128
Figure 14 Further example attention maps as in Figure 6 random selection
21Published as a conference paper at ICLR 2021
Table 9 Breakdown of VTAB1k performance across tasksCaltech101
CIFAR100
DTD
Flowers102
Pets
Sun397
SVHN
Camelyon
EuroSAT
Resisc45
Retinopathy
ClevrCount
ClevrDist
DMLab
dSprLoc
dSprOri
KITTIDist
sNORBAzim
sNORBElev
Mean
ViTH14 JFT 953 855 752 997 972 650 889 833 967 914 766 917 638 531 794 633 845 332 512 776
ViTL16 JFT 954 819 743 997 967 635 874 836 965 897 771 864 631 497 745 605 822 362 511 763
ViTL16 I21k 908 841 741 993 927 610 809 825 956 852 753 703 561 419 747 649 799 305 417 727
22
  Neural Redshift Random Networks are not Random Functions
Damien Teney
Idiap Research Institute
damienteneyidiapchArmand Mihai Nicolicioiu
ETH Zurich
armandmihainicolicioiuinfethzch
Valentin Hartmann
EPFL
valentinhartmannepflchEhsan Abbasnejad
University of Adelaide
ehsanabbasnejadadelaideedu
Abstract
Our understanding of the generalization capabilities of
neural networks NNs is still incomplete Prevailing ex
planations are based on implicit biases of gradient de
scent GD but they cannot account for the capabilities of
models from gradientfree methods 9 nor the simplicity
bias recently observed in untrained networks 29 This pa
per seeks other sources of generalization in NNs
Findings To understand the inductive biases provided
by architectures independently from GD we examine un
trained randomweight networks Even simple MLPs show
strong inductive biases uniform sampling in weight space
yields a very biased distribution of functions in terms of
complexity But unlike common wisdom NNs do not have
an inherent simplicity bias This property depends on
components such as ReLUs residual connections and layer
normalizations Alternative architectures can be built with
a bias for any level of complexity Transformers also inherit
all these properties from their building blocks
Implications We provide a fresh explanation for the suc
cess of deep learning independent from gradientbased
training It points at promising avenues for controlling the
solutions implemented by trained models
1 Introduction
Among various models in machine learning neural net
works NNs are the most successful on a variety of tasks
While we are pushing their capabilities with everlarger
models 72 much remains to be understood at the level of
their building blocks This work seeks to understand what
provides NNs with their unique generalization capabilities
The need for inductive biases The success of ML de
pends on using suitable inductive biases145 They specify
1Inductive biases are assumptions about the target function encoded in
the learning algorithm as the hypothesis class  eg architecture optimiza
tion method  eg SGD objective  eg crossentropy risk regularizer etcReLU GELU TanH Gaussian Weights magnitude
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
Depth
Lower frequencies
 Higher frequencies
Figure 1 We examine the complexity of the functions imple
mented by various MLP architectures We find that much of their
generalization capabilities can be understood independently from
the optimization training objective scaling or even data distribu
tion For example ReLU and GELU networks left overwhelm
ingly represent lowfrequency functions for any network depth or
weight magnitude  Other activations lack this property
how to generalize from a finite set of training examples to
novel test cases For example linear models allow learning
from few examples but generalize correctly only when the
target function is itself linear Large NNs are surprising in
having large representational capacity 36 yet generalizing
well across many tasks In other words among all learn
able functions that fit the training data those implemented
by trained networks are often similar to the target function
Explaining the success of neural networks Some suc
cessful architectures are tailored to specific domains eg
CNNs for image recognition But even the simplest MLP
architectures multilayer perceptrons often display re
markable generalization Two explanations for this success
prevail although they are increasingly challenged
Implicit biases of gradientbased optimization  A large
amount of work studies the preference of stochastic gra
dient descent or SGD for particular solutions 20 51
But conflicting results have also appeared First full
batch GD can be as effective as SGD 24 38 46 69
Second Chiang et al 9 showed that zerothorder op
1arXiv240302241v2  csLG  5 Mar 2024timization can yield models with good generalization as
frequently as GD And third Goldblum et al 29 showed
that language models with random weights already dis
play a preference for lowcomplexity sequences This
simplicity bias was previously thought to emerge from
training 65 In short gradient descent may help with
generalization but it does not seem necessary
Good generalization as a fundamental property of all
nonlinear architectures 33 This vague conjecture
does not account for the selection bias in the architectures
and algorithms that researchers have converged on For
example implicit neural representations  ie a network
trained to represent a specific image or 3D shape show
that the success of NNs is not automatic and requires in
that case activations very different from ReLUs
The success of deep learning is thus not a product primarily
of GD nor is it universal to all architectures This paper
propose an explanation compatible with all above observa
tions It builds on the growing evidence that NNs benefit
from their parametrization and the structure of their weight
space 9 29 37 64 77 79
Contributions We present experiments supporting this
threepart proposition stated formally in Appendix C
1NNs are biased to implement functions of a particu
lar level of complexity not necessarily low determined
by the architecture 2This preferred complexity is ob
servable in networks with random weights from an un
informed prior 3Generalization is enabled by popular
components like ReLUs setting this bias to a low com
plexity that often aligns with the target function
We name it the Neural Redshift NRS by analogy to phys
ical effects2that bias the observations of a signal towards
low frequencies Here the parameter space of NNs is biased
towards functions of low frequency one of the measures of
complexity used in this work see Figure 1
The NRS differs from prior work on the spectral bias 57
84 and simplicity bias 3 65 which confound the effects of
architectures and gradient descent The spectral bias refers
to the earlier learning of lowfrequencies during training
see discussion in Section 6 The NRS only involves the
parametrization3of NNs and thus shows interesting proper
ties independent from optimization 80 scaling 5 learn
ing objectives 9 or even data distributions 52
Concretely we examine various architectures with ran
dom weights We use three measures of complexity 1 de
compositions in Fourier series and 2 in bases of orthog
2httpsenwikipediaorgwikiRedshift
3Parametrization refers to the mapping between a networks weights
and the function it represents An analogy in geometry is the parametriza
tion of 2D points with Euclidean or polar coordinates Sampling uniformly
from one or the other gives different distributions of pointsonal polynomials equating simplicity with low frequen
ciesorder and 3 compressibility as an approximation of
the Kolmogorov complexity 15 We study how they vary
across architectures and how these properties at initializa
tion correlate with the performance of trained networks
Summary of findings
 We verify the NRS with three notions of complexity that
rely on frequencies in Fourier decompositions order in
polynomial decompositions and compressibility of the
inputoutput mapping Section 3
 We visualize the inputoutput mappings of 2D networks
Figure 3 They show intuitively the diversity of induc
tive biases across architectures that a scalar complexity
cannot fully describe Therefore matching the complex
ity of an architecture with the target function is beneficial
for generalization but hardly sufficient Section 41
 We show that the simplicity bias is not universal but de
pends on common components ReLUs residual connec
tions layer normalizations ReLU networks also have
the unique property of maintaining their simplicity bias
for any depth and weight magnitudes It suggests that
the historical importance of ReLUs in the development of
deep learning goes beyond the common narrative about
vanishing gradients 42
 We construct architectures where the NRS can be modu
lated with alternative activations and weight magnitudes
or entirely avoided parametrization in Fourier space
Section 3 It further demonstrates that the simplicity
bias is not universal and can be controlled to learn com
plex functions  eg modulo arithmetic or mitigate short
cut learning Section 41
 We show that the NRS is relevant to transformer sequence
models Randomweight transformers produce sequences
of low complexity and this can also be modulated with the
architecture This suggests that transformers inherit in
ductive biases from their building blocks via mechanisms
similar to those of simple models Section 5
2 How to Measure Inductive Biases
Our goal is to understand why NNs generalize when other
models of similar capacity would often overfit The im
mediate answer is simple NNs have an inductive bias
for functions with properties matching realworld data 
Hence two subquestions
1What are these properties
We will show that three metrics are relevant low fre
quency low order and compressibility Hereafter they
are collectively referred to as simplicity
2What gives neural networks these properties
We will show that an overwhelming fraction of their pa
rameter space corresponds to functions with such sim
2Chosen architecture
Eg 2layer MLP TanH
width 256 layer normRandom weights
Evaluation
gridMeasures of
complexityFrequency
Fourier decomp osition
Order
Polynomial decomp osition
Compressibility
LZW compression
Figure 2 Our methodology to characterize the inductive biases of an architecture We evaluate a network with random weightsbiases on
a grid of points This yields a representation of the function implemented by the network shown here as a grayscale image for a 2D input
We then characterize this function using three measures of complexity
plicity While there exist solutions of arbitrary complex
ity simple ones are found by default when navigating
this space especially with gradientbased methods
Analyzing random networks Given an architecture to
characterize we propose to sample random weights and bi
ases then evaluate the network on a regular grid in its input
space see Figure 2 Let fθxrepresent the function im
plemented by a network of parameters θweights and bi
ases evaluated on the input xRd The frepresents an
architecture with a scalar output and no output activation
which could serve for any regression or classification task
We sample weights and biases from an uninformed prior
chosen as a uniform distribution Biases are sampled from
U11 and weights from the range proposed by Glo
rot and Bengio 28 commonly used for initialization ie
Us swithsαp
6faninfanoutwhere αis an
extra factor  1by default to manipulate the weights magni
tude in our experiments These choices are not critical Ap
pendix E shows that other distributions Gaussian uniform
ball longtailed lead to similar findings
We then evaluate fθon points Xgridx1    xN
sampled regularly in the input space We restrict Xgridto
the hypercube 11dsince the data used with NNs is
commonly normalized The evaluation of fθonXgrid
yields a ddimensional grid of scalars In experiments of
Section 3 where d2 this is conveniently visualized as a 2D
grayscale image to provide visual intuition about the func
tion represented by the network Next we describe three
quantifiable properties to extract from such representations
Measures of complexity Applying the above procedure
to various architectures with 2D inputs yields clearly di
verse visual representations see Figure 3 For quantita
tive comparisons we propose three functions of the form
CXgrid fthat estimate proxies of the complexity of f
Fourier frequency A first natural choice is to use
Fourier analysis as in classical signal and image process
ing The image to analyze is the ddimensional evalua
tion of fonXgridmentioned above We first compute
a discrete Fourier transform that approximates fwith
a weighted sum of sines of various frequencies Formally we have fx  2 πd2Rfkeikxdkwhere
fk R
fxeikxdxis the Fourier transform The
discrete transform means that the frequency numbers k
are regularly spaced 012     K with the maximum
Kset according to the NyquistShannon limit of Xgrid
We use the intuition that complex functions are those with
large highfrequency coefficients 57 Therefore we de
fine our measure of complexity as the average of the co
efficients weighted by their corresponding frequency ie
CFourierf  ΣK
k1fkk ΣK
k1fk
For example a smoothly varying function is likely to in
volve mostly lowfrequency components and therefore
give a low value to CFourier 
Polynomial order A minor variation of Fourier anal
ysis uses decompositions in bases of polynomials The
procedure is nearly identical except for the sine waves
of increasing frequencies being replaced with fixed poly
nomials of increasing order details in Appendix D We
obtain an approximation of fas a weighted sum of
such polynomials and define our complexity measure
CChebyshev exactly as above ie the average of the coef
ficients weighted by their corresponding order For ex
ample the first two basis elements are a constant and a
firstorder polynomial hence the decomposition of a lin
earfwill use large coefficients on these loworder ele
ments and give a low complexity We implemented this
method with several canonical bases of orthogonal poly
nomials Hermite Legendre and Chebyshev polynomi
als and found the latter to be the most numerically stable
Compressibility has been proposed as an approximation
of the Kolmogorov complexity 15 29 79 We apply the
classical LempelZiv LZ compression on the sequence
Yfxi xiX We then use the size of the dictio
nary built by the algorithm as our measure of complexity
CLZf A sequence with repeating patterns will require
a small dictionary and give a low complexity
3 Inductive Biases in Random Networks
We are now equipped to compare architectures We will
show that various components shift the inductive bias to
3Table 1 Components that bias NNs towards lowhigh complexity
Lower complexity No impact Higher complexity
ReLUlike activations
Small weights  activations
Layer normalization
Residual connectionsWidth
Bias magnitudesOther activations
Large weights  activations
Depth
Multiplicative interactions
wards low or high complexity see Table 1 In particular
ReLU activations will prove critical for a simplicity bias in
sensitive to depth and weight  activation magnitude
ReLU MLPs We start with a 1hidden layer multilayer
perceptron MLP with ReLU activations We will then ex
amine variations of this architecture Formally each hidden
layer applies a transformation on the input xϕWxb
with weights W biases b and activation function ϕ
MLPs are so simple that they are often thought as providing
little or no inductive bias 5 On the contrary we observe
in Figures 4  6 that MLPs have a very strong bias towards
lowfrequency loworder compressible functions And this
simplicity bias is remarkably unaffected by the magnitude
of the weights nor increased depth
The variance in complexity across the random networks
is essentially zero virtually allof them have low complex
ity This does not mean that they cannot represent com
plex functions which would violate their universal approx
imation property 36 Complex functions simply require
preciselyadjusted weights and biases that are unlikely to
be found by random sampling These can still be found by
gradient descent though as we will see in Section 4
ReLUlike activations GELU Swish SELU 16 are
also biased towards low complexity Unlike ReLUs close
examination in Appendix F shows that increasing depth or
weight magnitudes slightly increases the complexity
Others activations TanH Gaussian sine show com
pletely different behaviour Depth and weight magnitude
cause a dramatic increase in complexity Unsurprisingly
these activations are only used in special applications 58
with careful initializations 68 Networks with these acti
vations have no fixed preferred complexity independent of
the weights or activations magnitudes4Mechanistically
the dependency on weight magnitudes is trivial to explain
Unlike with a ReLU the output eg of a GELU is not equiv
ariant to a rescaling of the weights and biases
Figure 6 shows close correlations between complexity
measures though they measure different proxies Figure 3
shows that different activations make distinctive patterns
not captured by the complexity measures More work will
be needed to characterize such finegrained differences
4The weight magnitudes examined in Figure 4 are larger than typically
used for initialization but the same effects would result from large activa
tionmagnitudes that occur in trained modelsReLU TanH
Weights 
Us s
Weights 
U6s6s
Figure 3 Comparison of functions implemented by random MLPs
2D input 3 hidden layers ReLU and TanH architectures are bi
ased towards different functions despite their universal approxima
tion capabilities ReLU architectures have the unique property of
maintaining their simplicity bias regardless of weight magnitude
Width has no impact on complexity perhaps surprisingly
Additional neurons change the capacity of a model what
can be represented after training but they do not affect its
inductive biases Indeed the contribution of all neurons in
a layer averages out to something invariant to their number
Layer normalization is a popular component in modern
architectures including transformers 55 It shifts and
rescales the internal representation to zero mean and unit
variance 4 We place layer normalizations before each ac
tivation such that each hidden layer now applies the trans
formation xWxbxϕxxstdxwhere
xandstdxdenote the mean and standard deviation across
channels Layer normalization has the significant effect of
removing variations in complexity with the weights magni
tude for all activations Figure 5 The weights can now vary
eg during training without directly affecting the preferred
complexity of the architecture Layer normalizations also
usually apply a learnable offset  0by default and scaling
1by default postnormalization Given the above observa
tions when paired with an activation with some slight sen
sitivity to weight magnitude  eg GELUs see Appendix F
this scaling can now be interpreted as a learnable shift in
complexity modulated by a single scalar rather than the
whole weight matrix without the normalization
Residual connections 31 We add these such that each
nonlinearity is now described with xxϕx This
has the dramatic effect of forcing the preferred complexity
to some of the lowest levels for all activations regardless of
depth This can be explained by the fact that residual con
nections essentially bypass the stacking of nonlinearities
that causes the increased complexity with increased depth
Multiplicative interactions refer to multiplications of in
ternal representations with one another 39 as in attention
layers highway networks dynamic convolutions etc We
place them in our MLPs as gating operations such that each
hidden layer corresponds to x
ϕWxbσWx
b
where σis the logistic function This creates a clear
increase in complexity dependent on depth and weight mag
4ReLU GELU Swish SELU Tanh Gaussian Sin Unbiased
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
Figure 4 Heatmaps of the average Fourier complexity of functions implemented by randomweight networks Each heatmap corresponds
to an activation function and each cell within a heatmap corresponds to a depth heatmap columns and weight magnitude heatmap
rows We also show grayscale images of functions implemented by networks of an architecture corresponding to every other heatmap cell
ReLU GELU Swish SELU TanH Gaussian Sin UnbiasedComplexity Fourier
1 1001
1 1001
1 1001
1 1001
1 5001
1 3001
1 2001
1 1001
    MLP
  Gating
  Residual
  Layer norm
Figure 5 The complexity of random models Y axis generally increases with weights  activations magnitudes X axis The sensitivity
is however very different across activation functions This sensitivity also increases with multiplicative interactions  ie gating decreases
with residual connections and is essentially absent with layer normalization
nitude even for ReLU activations This agrees with prior
work showing that multiplicative interactions in polynomial
networks 10 facilitate learning high frequencies
Unbiased model As a counterexample to models show
ing some preferred complexity we construct an architec
ture with no bias by design in the complexity measured
with Fourier frequencies This special architecture imple
ments an inverse Fourier transform parametrized directly
with the coefficients and phase shifts of the Fourier com
ponents details in Appendix D The inverse Fourier trans
form is a weighted sum of sine waves so this architecture
can be implemented as a onelayer MLP with sine activa
tions and fixed input weights representing each one Fourier
component These fixed weights prior to sine activations
thus enforce a uniform prior over frequencies 
This architecture behaves very differently from standard
MLPs Figure 4 With random weights its Fourier spec
trum is uniform which gives a high complexity for any
weight magnitude depth is fixed Functions implemented
by this architecture look like white noise Even though
this architecture can be trained by gradient descent like any
MLP we show in Appendix E that it is practically useless
because of its lack of any complexity bias
0 LZ 101Fourier
0 LZ 101Legendre
0 LZ 101ChebyshevFigure 6 Our various complexity measures are closely correlated
despite measuring each a different proxy ie frequency Fourier
polynomial order Legendre Chebyshev or compressibility LZ
Importance of ReLU activations
The fact that a strong simplicity bias depends on ReLU
activations suggests that their historical importance in
the development of deep learning goes beyond the com
mon narrative about vanishing gradients 42 The same
may apply to residual connections and layer normaliza
tion since they alsox contribute strongly to the simplic
ity bias This contrasts with the current literature that
mostly invokes their numerical properties 6 82 83
54 Inductive Biases in Trained Models
We now examine how the inductive biases of an architec
ture impact a model trained by standard gradient descent
We will show that there is a strong correlation between the
complexity at initialization  ie with random weights as ex
amined in the previous section and in the trained model
We will also see that unusual architectures with a bias to
wards high complexity can improve generalization on tasks
where the standard simplicity bias is suboptimal
41 Learning Complex Functions
The NRS proposes that good generalization requires a good
match between the complexity preferred by the architecture
and the target function We verify this claim by demon
strating improved generalization on complex tasks with ar
chitectures biased towards higher complexity This is also
a proof of concept of the potential utility of controlling in
ductive biases
Experimental setup We consider a simple binary classi
fication task involving modulo arithmetic Such tasks like
the parity function 66 are known to be challenging for
standard architectures because they contain highfrequency
patterns The input to our task is a vector of integers x
0 N1d The correct labels are 1ΣxiM2 mod M
We consider three versions with N16 andM1074
that correspond to increasingly higher frequencies in the tar
get function see Figure 7 and Appendix D for details
Results We see in Figure 7 that a ReLU MLP only solves
the lowfrequency version of the task Even though this
model can be trained to perfect training accuracy on the
higherfrequency versions it then fails to generalize be
cause of its simplicity bias We then train MLPs with other
activations TanH Gaussian sine whose preferred com
plexity is sensitive to the activations magnitude We also
introduce a constant multiplicative prefactor before each
activation function to modulate this bias without changing
the weights magnitude which could introduce optimiza
tion side effects Some of these models succeed in learn
ing all versions of the task when the prefactor is correctly
tuned For higherfrequency versions the prefactor needs
to be larger to shift the bias towards higher complexity In
Figure 7 we fit a quadratic approximation to the accuracy
of Gaussianactivated models The peak then clearly shifts
to the right on the complex tasks This agrees with the
NRS proposition that complexity at initialization relates
to properties of the trained model 
Let us also note that not all activations succeed even with
a tuned prefactor This shows that matching the complexity
of the architecture and of the target function is beneficial but
not sufficient for good generalization The inductive biases
of an architecture are clearly not fully captured by any of
our measures of complexityTarget function 3 versions of modulo addition
Low freq
 Med freq
 High freqTest accuracy
0 1051
0 1051
0 1051
Complexity LZ at initialization
modulated by choices of activation and prefactor value
mlpRelu mlpGelu mlpSwish mlpTanh mlpGaussian mlpSin
Figure 7 Results training networks on three tasks of increasing
complexity Each point represents a different architecture ReLU
like activations are biased towards low complexity and fail to
generalize on complex tasks With other activations  the com
plexity bias depends on the activation magnitudes which we can
control with a multiplicative prefactor This enables generaliza
tion on complex tasks by shifting the bias to higher complexity
Indeed the optimum prefactor peak of the quadratic fit shifts to
the right on each task of increasing complexity
Low frequency Medium frequency High frequencyTest accuracy
01
0 2 4 6 8
Prefactor051
01
0 2 4 6 8
Prefactor051
01
0 2 4 6 8
Prefactor051
Figure 8 Detail of Figure 7 for Gaussian activations The peak
accuracy shifts to the right on tasks of increasing complexity This
corresponds to a larger prefactor that shifts the bias towards higher
complexity Each point represents one random seed
Reinterpretation of existing work
Loss landscapes Are All You Need 9
Chiang et al showed that networks with random
weights as long as they fit the training data with low
training loss are good solutions that generalize to the
test data We find  loss landscapes  slightly misleading
because the key is in the parametrization of the network
and by extension of this landscape and not in the loss
function Their results can be replicated by replacing the
crossentropy loss with an MSE loss but not by replac
ing their MLP with our unbiased learner architecture
The sampled solutions are good not only because of
their low training loss but because they are found by
uniformly sampling the weight space Bad lowloss so
lutions also exist but they are unlikely to be found by
random sampling Because of the NRS all random
weight networks implement simple functions which
generalize as long as they fit the training data An al
ternative title could be  Uniformly Sampling the Weight
Space Is All You Need 
642 Impact on Shortcut Learning
Shortcut learning refers to situations where the simplicity
bias causes a model to rely on simple spurious features
rather than learning the morecomplex target function 65
Experimental setup We consider a regression task sim
ilar to ColoredMNIST Inputs are images of handwritten
digits juxtaposed with a uniform band of colored pixels that
simulate spurious features The labels in the training data
are values in 01proportional to the digit value as well
as to the color intensity Therefore a model can attain high
training accuracy by relying either on the simple linear rela
tion with the color or the more complex recognition of the
digits the target task To measure the reliance of a model
on color or digit we use two test sets where either the color
or digit is correlated with the label while the other is ran
domized See Appendix D for details
Results We train 2layer MLPs with different activa
tion functions We also use a multiplicative prefactor ie
a constant αRplaced before each activation func
tion such that each nonlinear layer performs the following
xϕ
αWxb
 The prefactor mimics a rescaling of
the weights and biases with no optimization side effects
We see in Figure 9 that the LZ complexity at initialization
increases with prefactor values for TanH Gaussian and sine
activations Most interestingly the accuracy on the digit and
color also varies with the prefactor The color is learned
more easily with small prefactors corresponding to a low
complexity at initialization while the digit is learned more
easily at an intermediate value corresponding to medium
complexity at initialization The best performance on the
digit is reached at a sweet spot that we explain as the hy
pothesized best match between the complexity of the tar
get function and that preferred by the architecture With
larger prefactors ie beyond this sweet spot the accuracy
on the digit decreases and even more so with sine activa
tions for which the complexity also increases more rapidly
further supporting the proposed explanation
TanH Gaussian SineTest accuracy Complexity at init LZ
01
0 1 2 3 4
Prefactor051
Digit
Color
01
0 1 2 3 4
Prefactor051
Digit
Color
01
0 1 2 3 4
Prefactor051
Digit
Color
Figure 9 Experiments on ColoredMNIST show a clear corre
lation between complexity at initialization top and test accuracy
bottom Models with a bias for low complexity rely on the color
ie the simpler feature The accuracy on the digit peaks at a sweet
spot where the models preferred complexity matches the digitsReinterpretation of existing work
How You Start Matters for Generalization 59
Ramasinghe et al examine implicit neural represen
tations  ie a network trained to represent one image
They observe that models showing high frequencies at
initialization also learn high frequencies better They
conclude that complexity at initialization causally in
fluences the solution But our results suggest instead
that these are two effects of a common cause the ar
chitecture is biased towards a certain complexity which
influences both the untrained model and the solutions
found by gradient descent There exist configurations of
weights that correspond to complex functions but they
are unlikely to be found in either case Appendix E3
shows that initializing GD from such a solution with
an architecture biased toward simplicity does not yield
complex solutions thus disproving the causal relation
5 Transformers are Biased Towards
Compressible Sequences
We now show that the inductive biases observed with MLPs
are relevant to transformer sequence models The experi
ments below confirm the bias of a transformer for generat
ing simple compressible sequences 29 which could then
explain the tendency of language models to repeat them
selves 21 34 The experiments also suggest that trans
formers inherit this inductive bias from the same compo
nents as those explaining the simplicity bias in MLPs
Experimental setup We sample sequences from an un
trained GPT2 55 For each sequence we sample random
weights from their default initial distribution then prompt
the model with one random token all of them being equiva
lent since the model is untrained then generate a sequence
of 100 tokens by greedy maximumlikelihood decoding
We evaluate the complexity of each sequence with the LZ
measure Section 2 and report the average over 1000 se
quences We evaluate variations of the architecture replac
ing activation functions in MLP blocks GELUs by default
varying the depth  12transformer blocks by default and
varying the activations magnitude by modifying the scal
ing factor in layer normalizations  1by default
Results We first observe in Figure 10 that the default
architecture is biased towards relatively simple sequences
This observation already reported by Goldblum et al 29
is nontrivial since a random model could as well gener
ate completely random sequences Changing the activation
function from the default GELUs has a large effect The
complexity increases with SELU TanH sine and decreases
with ReLU It is initially low with Gaussian activations but
climbs higher than most others with larger activation magni
tudes This is consistent with observations made on MLPs
7where ReLU induced the strongest bias for simplicity and
TanH Gaussian sine for complexity Variations of acti
vations magnitude via scaling in layer normalizations
has the same monotonic effect on complexity as observed
in MLPs However we lack an explanation for the shoul
ders in the curves of SELU Tanh and sine It may relate to
them being the activations that output negative values most
Varying depth also has the expected effect of magnifying
the differences across activations and scales
Complexity LZ
1 3 6 9 1201
0 1 2 3 4 5 6 701
Relu
GeLU
Swish
SeLU
TanH
Gaussian
Sine
Depth num of transformer Scaling in layer
blocks scaling1 normalization depth12
Figure 10 Average complexity LZ of sequences generated by
an untrained GPT2 Variations of the architecture correspond to
variations in complexity comparable to MLPs This suggests that
transformers inherit a bias for simple sequences from their build
ing blocks via mechanisms similar to those in simple models
Takeaway These results suggest that the bias for sim
ple sequences of transformers originates from their build
ing blocks via similar mechanisms to those causing the
simplicity bias in other predictive models The building
blocks of transformers also seem to balance a shift to
wards higher complexity attention multiplicative interac
tions and lower complexity GELUs layer normalizations
residual connections
6 Related Work
Much effort has gone into explaining the success of deep
learning through the inductive biases of SGD 48 and struc
tured architectures 11 90 This work rather focuses on
implicit inductive biases from unstructured architectures
The simplicity bias is the tendency of NNs to fit data
with simple functions 3 25 53 75 The spectral bias
suggests that NNs prioritize learning lowfrequency compo
nents of the target function 57 84 These studies confound
architectures and optimization And most explanations in
voke implicit regularization of gradient descent 70 85 and
are specific to ReLU networks 35 38 88 In contrast we
show that some form of spectral bias exists in common ar
chitectures independently of gradient descent
A related line of study showed that Boolean MLPs are bi
ased towards lowentropy functions 12 44 Work closer to
ours 12 44 79 examines the simplicity bias of networks
with random weights  These works are limited to MLPs
with binary inputs or outputs 12 44 ReLU activations
and simplicity measured as compressibility In contrast our
work examines multiple measures of simplicity and a widerset of architectures In work concurrent to ours Abbe et al
1 used Walsh decompositions analogous to Fourier se
ries for binary functions to characterize the simplicity of
learned binary classification networks Their discussion is
specific to classification and highly complementary to ours
Our work also provides a new lens to explain why
choices of activation functions are critical 16 60 67 See
Appendix A for an extended literature review
7 Conclusions
We examined inductive biases that NNs possess indepen
dently of their optimization We found that the parame
ter space of popular architectures corresponds overwhelm
ingly to functions with three quantifiable properties low
frequency low order and compressibility They correspond
to the simplicity bias previously observed in trained models
which we now explain without involving SGD We also
showed that the simplicity bias is not universal to all ar
chitectures It results from ReLUs residual connections
layer normalization etc The popularity of these compo
nents likely reflects the collective search for architectures
that perform well on realworld data In short the effective
ness of NNs is not an intrinsic property but the result of the
adequacy between key choices  eg ReLUs and properties
of realworld data prevalence of lowcomplexity patterns
Limitations and open questions
 Our analysis used mostly small models and data to en
able visualizations 2D function maps and computations
Fourier decompositions We showed the relevance of
our findings to large transformers but the study could be
extended to other large architectures and tasks
 Our analysis relies on empirical simulations  It could be
carried out analytically to provide theoretical insights
 Our results do not invalidate prior work on implicit biases
of SGD Future work should clarify the interplay of dif
ferent sources of inductive biases  Even if most of the
parameter space corresponds to simple functions GD can
navigate to complex ones Are they isolated points in pa
rameter space islands or connected regions This relates
to mode connectivity lottery tickets 19 and the hypoth
esis that good flat minima occupy a large volume 37
 We proposed three quantifiable facets of inductive bi
ases Much is missed about the shape of functions pre
ferred by different activations Figure 3 An extension
could discover other reasons for the success of NNs and
fundamental properties shared across realworld datasets
 An application of our findings is in the control of
inductive biases to nudge the behaviour of trained
networks 87 For example by manipulating the
parametrization of NNs on which SGD is performed
8Acknowledgements
This work was partially supported by the Australian Re
search Council DP240103278
References
1 Emmanuel Abbe Samy Bengio Aryo Lotfi and Kevin Rizk
Generalization on the unseen logic reasoning and degree
curriculum arXiv preprint arXiv230113105  2023 8 1
2 Sanjeev Arora Nadav Cohen Wei Hu and Yuping Luo Im
plicit regularization in deep matrix factorization NeurIPS 
32 2019 1
3 Devansh Arpit Stanislaw Jastrzebski Nicolas Ballas David
Krueger Emmanuel Bengio Maxinder S Kanwal Tegan
Maharaj Asja Fischer Aaron Courville Yoshua Bengio
et al A closer look at memorization in deep networks In
ICML  pages 233242 PMLR 2017 2 8 1
4 Jimmy Lei Ba Jamie Ryan Kiros and Geoffrey E Hin
ton Layer normalization arXiv preprint arXiv160706450 
2016 4
5 Gregor Bachmann Sotiris Anagnostidis and Thomas Hof
mann Scaling MLPs A tale of inductive bias arXiv preprint
arXiv230613575  2023 2 4
6 David Balduzzi Marcus Frean Lennox Leary JP Lewis
Kurt WanDuo Ma and Brian McWilliams The shattered
gradients problem If resnets are the answer then what is the
question In International Conference on Machine Learn
ing pages 342350 PMLR 2017 5
7 Satwik Bhattamishra Arkil Patel Varun Kanade and Phil
Blunsom Simplicity bias in transformers and their abil
ity to learn sparse boolean functions arXiv preprint
arXiv221112316  2022 2
8 Akhilan Boopathy Kevin Liu Jaedong Hwang Shu Ge
Asaad Mohammedsaleh and Ila R Fiete Modelagnostic
measure of generalization difficulty In ICML  pages 2857
2884 PMLR 2023 1
9 Pingyeh Chiang Renkun Ni David Yu Miller Arpit Bansal
Jonas Geiping Micah Goldblum and Tom Goldstein Loss
landscapes are all you need Neural network generalization
can be explained without the implicit bias of gradient de
scent In ICLR  2022 1 2 6
10 Moulik Choraria Leello Tadesse Dadi Grigorios Chrysos
Julien Mairal and V olkan Cevher The spectral bias of poly
nomial neural networks arXiv preprint arXiv220213473 
2022 5 2
11 Nadav Cohen and Amnon Shashua Inductive bias of deep
convolutional networks through pooling geometry arXiv
preprint arXiv160506743  2016 8 1
12 Giacomo De Palma Bobak Kiani and Seth Lloyd Random
deep neural networks are biased towards simple functions
NeurIPS  32 2019 8 1
13 Gr egoire Del etang Anian Ruoss Jordi GrauMoya Tim
Genewein Li Kevin Wenliang Elliot Catt Chris Cundy
Marcus Hutter Shane Legg Joel Veness et al Neu
ral networks and the chomsky hierarchy arXiv preprint
arXiv220702098  2022 114 Benoit Dherin Michael Munn Mihaela Rosca and David
Barrett Why neural networks find simple solutions The
many regularizers of geometric complexity NeurIPS  35
23332349 2022 2
15 Kamaludin Dingle Chico Q Camargo and Ard A Louis
Inputoutput maps are strongly biased towards simple out
puts Nature communications  91761 2018 2 3 1
16 Shiv Ram Dubey Satish Kumar Singh and Bidyut Baran
Chaudhuri Activation functions in deep learning A com
prehensive survey and benchmark Neurocomputing  2022
4 8 2 3
17 Rahim Entezari Hanie Sedghi Olga Saukh and Behnam
Neyshabur The role of permutation invariance in lin
ear mode connectivity of neural networks arXiv preprint
arXiv211006296  2021 1
18 Emanuele Francazi Aurelien Lucchi and Marco BaityJesi
Initial guessing bias How untrained networks favor some
classes arXiv preprint arXiv230600809  2023 2
19 Jonathan Frankle Gintare Karolina Dziugaite Daniel Roy
and Michael Carbin Linear mode connectivity and the lot
tery ticket hypothesis In International Conference on Ma
chine Learning  pages 32593269 PMLR 2020 8
20 Spencer Frei Gal Vardi Peter L Bartlett Nathan Srebro and
Wei Hu Implicit bias in leaky relu networks trained on high
dimensional data arXiv preprint arXiv221007082  2022 1
21 Zihao Fu Wai Lam Anthony ManCho So and Bei Shi A
theoretical analysis of the repetition problem in text genera
tion In AAAI  pages 1284812856 2021 7 4
22 Gallant There exists a neural network that does not make
avoidable mistakes In IEEE International Conference on
Neural Networks  pages 657664 IEEE 1988 2
23 Adri a GarrigaAlonso Carl Edward Rasmussen and Lau
rence Aitchison Deep convolutional networks as shallow
gaussian processes arXiv preprint arXiv180805587  2018
1
24 Jonas Geiping Micah Goldblum Phillip E Pope Michael
Moeller and Tom Goldstein Stochastic training is not nec
essary for generalization arXiv preprint arXiv210914119 
2021 1
25 Robert Geirhos J ornHenrik Jacobsen Claudio Michaelis
Richard Zemel Wieland Brendel Matthias Bethge and Fe
lix A Wichmann Shortcut learning in deep neural networks
Nature Machine Intelligence  211665673 2020 8 1
26 Alan Martin Gilkes Photograph enhancement by adaptive
digital unsharp masking PhD thesis Massachusetts Institute
of Technology 1974 5
27 Raja Giryes Guillermo Sapiro and Alex M Bronstein Deep
neural networks with random gaussian weights A universal
classification strategy IEEE Transactions on Signal Pro
cessing  641334443457 2016 1
28 Xavier Glorot and Yoshua Bengio Understanding the diffi
culty of training deep feedforward neural networks In Inter
national Conference on Artificial Intelligence and Statistics 
pages 249256 JMLR 2010 3
29 Micah Goldblum Marc Finzi Keefer Rowan and An
drew Gordon Wilson The no free lunch theorem kol
mogorov complexity and the role of inductive biases in ma
9chine learning arXiv preprint arXiv230405366  2023 1
2 3 7
30 Michael Hahn Dan Jurafsky and Richard Futrell Sensitivity
as a complexity measure for sequence classification tasks
Transactions of the ACL  9891908 2021 2
31 Kaiming He Xiangyu Zhang Shaoqing Ren and Jian Sun
Deep residual learning for image recognition In CVPR 
pages 770778 2016 4
32 Katherine L Hermann and Andrew K Lampinen What
shapes feature representations exploring datasets architec
tures and training arXiv preprint arXiv200612433  2020
1
33 Katherine L Hermann Hossein Mobahi Thomas Fel and
Michael C Mozer On the foundations of shortcut learning
arXiv preprint arXiv231016228  2023 2
34 Ari Holtzman Jan Buys Li Du Maxwell Forbes and Yejin
Choi The curious case of neural text degeneration arXiv
preprint arXiv190409751  2019 7 4
35 Qingguo Hong Jonathan W Siegel Qinyang Tan and
Jinchao Xu On the activation function dependence of
the spectral bias of neural networks arXiv preprint
arXiv220804924  2022 8 1
36 Kurt Hornik Maxwell Stinchcombe and Halbert White
Multilayer feedforward networks are universal approxima
tors Neural networks  25359366 1989 1 4
37 W Ronny Huang Zeyad Emam Micah Goldblum Liam
Fowl Justin K Terry Furong Huang and Tom Goldstein
Understanding generalization through visualizations In I
Cant Believe Its Not Better NeurIPS Workshop  pages
8797 PMLR 2020 2 8
38 Minyoung Huh Hossein Mobahi Richard Zhang Brian
Cheung Pulkit Agrawal and Phillip Isola The low
rank simplicity bias in deep networks arXiv preprint
arXiv210310427  2021 1 8
39 Siddhant M Jayakumar Wojciech M Czarnecki Jacob
Menick Jonathan Schwarz Jack Rae Simon Osindero
Yee Whye Teh Tim Harley and Razvan Pascanu Multi
plicative interactions and where to find them In ICLR  2020
4
40 Jaehoon Lee Yasaman Bahri Roman Novak Samuel S
Schoenholz Jeffrey Pennington and Jascha SohlDickstein
Deep neural networks as gaussian processes arXiv preprint
arXiv171100165  2017 1
41 Kaifeng Lyu Zhiyuan Li Runzhe Wang and Sanjeev Arora
Gradient descent on twolayer nets Margin maximization
and simplicity bias NeurIPS  341297812991 2021 1
42 Andrew L Maas Awni Y Hannun Andrew Y Ng et al Rec
tifier nonlinearities improve neural network acoustic models
InICML  page 3 Atlanta GA 2013 2 5
43 Alexander G de G Matthews Mark Rowland Jiri Hron
Richard E Turner and Zoubin Ghahramani Gaussian pro
cess behaviour in wide deep neural networks In ICLR  2018
1
44 Chris Mingard Joar Skalse Guillermo ValleP erez David
Mart ınezRubio Vladimir Mikulik and Ard A Louis Neural
networks are a priori biased towards boolean functions with
low entropy arXiv preprint arXiv190911522  2019 8 1 245 Tom M Mitchell The need for biases in learning general
izations Rutgers University CS tech report CBMTR117 
1980 1
46 Amirkeivan Mohtashami Martin Jaggi and Sebastian U
Stich Special properties of gradient descent with large learn
ing rates arXiv preprint arXiv220515142  2023 1
47 Guido F Montufar Razvan Pascanu Kyunghyun Cho and
Yoshua Bengio On the number of linear regions of deep
neural networks NeurIPS  27 2014 5
48 Behnam Neyshabur Ryota Tomioka and Nathan Srebro In
search of the real inductive bias On the role of implicit regu
larization in deep learning arXiv preprint arXiv14126614 
2014 8 1
49 Elisa Oostwal Michiel Straat and Michael Biehl Hidden
unit specialization in layered neural networks Relu vs sig
moidal activation Physica A Statistical Mechanics and its
Applications  564125517 2021 2
50 Jeffrey Pennington Samuel Schoenholz and Surya Ganguli
The emergence of spectral universality in deep networks
InInternational Conference on Artificial Intelligence and
Statistics  pages 19241932 PMLR 2018 1
51 Scott Pesme Loucas PillaudVivien and Nicolas Flammar
ion Implicit bias of sgd for diagonal linear networks a
provable benefit of stochasticity NeurIPS  342921829230
2021 1
52 Mohammad Pezeshki Oumar Kaba Yoshua Bengio
Aaron C Courville Doina Precup and Guillaume Lajoie
Gradient starvation A learning proclivity in neural net
works NeurIPS  3412561272 2021 2 1
53 Tomaso Poggio Kenji Kawaguchi Qianli Liao Brando Mi
randa Lorenzo Rosasco Xavier Boix Jack Hidary and
Hrushikesh Mhaskar Theory of deep learning III the non
overfitting puzzle CBMM Memo  73138 2018 8 1
54 Ben Poole Subhaneil Lahiri Maithra Raghu Jascha Sohl
Dickstein and Surya Ganguli Exponential expressivity in
deep neural networks through transient chaos NeurIPS  29
2016 1
55 Alec Radford Jeffrey Wu Rewon Child David Luan Dario
Amodei Ilya Sutskever et al Language models are unsu
pervised multitask learners OpenAI blog  189 2019 4
7
56 Maithra Raghu Ben Poole Jon Kleinberg Surya Ganguli
and Jascha SohlDickstein On the expressive power of deep
neural networks In ICML  pages 28472854 PMLR 2017
1
57 Nasim Rahaman Aristide Baratin Devansh Arpit Felix
Draxler Min Lin Fred Hamprecht Yoshua Bengio and
Aaron Courville On the spectral bias of neural networks
InICML  pages 53015310 PMLR 2019 2 3 8 1
58 Sameera Ramasinghe and Simon Lucey Beyond periodicity
Towards a unifying framework for activations in coordinate
MLPs In ECCV  pages 142158 Springer 2022 4 2
59 Sameera Ramasinghe Lachlan MacDonald Moshiur Farazi
Hemanth Saratchandran and Simon Lucey How you start
matters for generalization arXiv preprint arXiv220608558 
2022 7 9
1060 Sameera Ramasinghe Lachlan E MacDonald and Simon
Lucey On the frequencybias of coordinateMLPs NeurIPS 
35796809 2022 8 2
61 Vishwanath Saragadam Daniel LeJeune Jasper Tan Guha
Balakrishnan Ashok Veeraraghavan and Richard G Bara
niuk Wire Wavelet implicit neural representations In
CVPR  pages 1850718516 2023 2
62 J urgen Schmidhuber Discovering neural nets with low
kolmogorov complexity and high generalization capability
Neural Networks  105857873 1997 1
63 Samuel S Schoenholz Justin Gilmer Surya Ganguli and
Jascha SohlDickstein Deep information propagation arXiv
preprint arXiv161101232  2016 1 2
64 Luca Scimeca Seong Joon Oh Sanghyuk Chun Michael
Poli and Sangdoo Yun Which shortcut cues will dnns
choose a study from the parameterspace perspective arXiv
preprint arXiv211003095  2021 2
65 Harshay Shah Kaustav Tamuly Aditi Raghunathan Prateek
Jain and Praneeth Netrapalli The pitfalls of simplicity bias
in neural networks NeurIPS  3395739585 2020 2 7
66 Shai ShalevShwartz Ohad Shamir and Shaked Shammah
Failures of gradientbased deep learning In ICML  pages
30673075 PMLR 2017 6
67 James Benjamin Simon Sajant Anand and Mike Deweese
Reverse engineering the neural tangent kernel In ICML 
pages 2021520231 PMLR 2022 8 2
68 Vincent Sitzmann Julien Martel Alexander Bergman David
Lindell and Gordon Wetzstein Implicit neural representa
tions with periodic activation functions NeurIPS  337462
7473 2020 4 8
69 Samuel L Smith Benoit Dherin David GT Barrett and So
ham De On the origin of implicit regularization in stochastic
gradient descent arXiv preprint arXiv210112176  2021 1
70 Daniel Soudry Elad Hoffer Mor Shpigel Nacson Suriya
Gunasekar and Nathan Srebro The implicit bias of gradient
descent on separable data The Journal of Machine Learning
Research  19128222878 2018 8 1
71 Joshua Stock Jens Wettlaufer Daniel Demmler and
Hannes Federrath Property unlearning A defense strat
egy against property inference attacks arXiv preprint
arXiv220508821  2022 1
72 Richard Sutton The bitter lesson Incomplete Ideas blog 
131 2019 1
73 Remi Tachet Mohammad Pezeshki Samira Shabanian
Aaron Courville and Yoshua Bengio On the learn
ing dynamics of deep neural networks arXiv preprint
arXiv180906848  2018 1
74 Matthew Tancik Pratul Srinivasan Ben Mildenhall Sara
FridovichKeil Nithin Raghavan Utkarsh Singhal Ravi Ra
mamoorthi Jonathan Barron and Ren Ng Fourier features
let networks learn high frequency functions in low dimen
sional domains NeurIPS  3375377547 2020 2
75 Damien Teney Ehsan Abbasnejad Simon Lucey and Anton
van den Hengel Evading the simplicity bias Training a
diverse set of models discovers solutions with superior ood
generalization arXiv preprint arXiv210505612  2021 8
1 276 Damien Teney Maxime Peyrard and Ehsan Abbasnejad
Predicting is not understanding Recognizing and addressing
underspecification in machine learning In European Confer
ence on Computer Vision  pages 458476 Springer 2022 2
77 Ryan Theisen Jason Klusowski and Michael Mahoney
Good classifiers are abundant in the interpolating regime In
AISTATS  pages 33763384 PMLR 2021 2
78 Dmitry Ulyanov Andrea Vedaldi and Victor Lempitsky
Deep image prior In CVPR  pages 94469454 2018 1
79 Guillermo VallePerez Chico Q Camargo and Ard A Louis
Deep learning generalizes because the parameterfunction
map is biased towards simple functions arXiv preprint
arXiv180508522  2018 2 3 8 1
80 Gal Vardi On the implicit bias in deeplearning algorithms
Communications of the ACM  6668693 2023 2
81 Yiheng Xie Towaki Takikawa Shunsuke Saito Or Litany
Shiqin Yan Numair Khan Federico Tombari James Tomp
kin Vincent Sitzmann and Srinath Sridhar Neural fields in
visual computing and beyond In Computer Graphics Forum 
pages 641676 Wiley Online Library 2022 2 5
82 Ruibin Xiong Yunchang Yang Di He Kai Zheng Shuxin
Zheng Chen Xing Huishuai Zhang Yanyan Lan Liwei
Wang and Tieyan Liu On layer normalization in the trans
former architecture In International Conference on Machine
Learning  pages 1052410533 PMLR 2020 5
83 Jingjing Xu Xu Sun Zhiyuan Zhang Guangxiang Zhao and
Junyang Lin Understanding and improving layer normaliza
tion Advances in Neural Information Processing Systems 
32 2019 5
84 ZhiQin John Xu Yaoyu Zhang Tao Luo Yanyang Xiao
and Zheng Ma Frequency principle Fourier analy
sis sheds light on deep neural networks arXiv preprint
arXiv190106523  2019 2 8 1 3
85 ZhiQin John Xu Yaoyu Zhang and Yanyang Xiao Training
behavior of deep neural network in frequency domain In
ICONIP  pages 264274 Springer 2019 8 1
86 Greg Yang and Hadi Salman A finegrained spec
tral perspective on neural networks arXiv preprint
arXiv190710599  2019 1
87 Enyan Zhang Michael A Lepori and Ellie Pavlick In
stilling inductive biases with subnetworks arXiv preprint
arXiv231010899  2023 8
88 Shijun Zhang Hongkai Zhao Yimin Zhong and Haomin
Zhou Why shallow networks struggle with approximat
ing and learning high frequency A numerical study arXiv
preprint arXiv230617301  2023 8 1
89 Allan Zhou Kaien Yang Kaylee Burns Yiding Jiang
Samuel Sokota J Zico Kolter and Chelsea Finn Per
mutation equivariant neural functionals arXiv preprint
arXiv230214040  2023 1
90 Hattie Zhou Arwen Bradley Etai Littwin Noam Razin
Omid Saremi Josh Susskind Samy Bengio and Preetum
Nakkiran What algorithms can transformers learn a study
in length generalization arXiv preprint arXiv231016028 
2023 8 1 2
11Neural Redshift Random Networks are not Random Functions
Supplementary Material
A Additional Related Work
A vast literature examines the success of deep learn
ing using inductive biases of optimization methods
eg SGD 48 and architectures  eg CNNs 11 trans
formers 90 This paper instead examines implicit induc
tive biases in unstructured architectures
Parametrization of NNs It is challenging to under
stand the structure and effective dimensionality of the
weight space of NNs because multiple weight configura
tions and their permutations correspond to the same func
tion 17 71 89 A recent study quantified the information
needed to identify a model with good generalization 8
However the estimated values are astronomical meaning
that no dataset would ever be large enough to learn the tar
get function Our work reconciles these results with the
reality the fact that deep learning does work in practice
by showing that the overlap of the set of good generalizing
functions with uniform samples in weight space 8 Fig 1
is much denser than its overlap with truly random functions
In other words random sampling in weight space generally
yields functions likely to generalize Much less information
is needed to pick one solution among those than estimated
in 8
Some think that  stronger inductive biases come at the
cost of decreasing the universality of a model  13 This is
a misunderstanding of the role of inductive biases they are
fundamentally necessary for machine learning and they do
not imply a restriction on the set of learnable functions We
show in particular that MLPs have strong inductive biases
yet remain universal
The simplicity bias refers to the observed tendency of NNs
to fit their training data with simple functions It is desirable
when it prevents overparametrized networks from overfit
ting the training data 3 53 But it is a curse when it causes
shortcut learning 25 75 Most papers on this topic are
about trained networks hence they confound the inductive
biases of the architectures and of the optimization Most ex
planations of the simplicity bias involve loss functions 52
and gradient descent 2 32 41 73
Work closer to ours 12 44 79 examines the simplic
ity bias of networks with random weights  These studies
are limited to MLPs with binary inputsoutputs ReLU ac
tivations andor simplicity measured as compressibility In
contrast we examine more architectures and other measures
of complexity Earlier works with randomweight networks
include 23 27 40 43 50 54 56 63
Goldblum et al 29 proposed that NNs are effective because they combine a simplicity bias with a flexible hypoth
esis space Thus they can represent complex functions and
benefit from large datasets Our results also support this
argument
The spectral bias 57 or frequency principle 84 is a par
ticular form of the simplicity bias It refers to the observa
tion that NNs learn lowfrequency components of the target
function earlier during training5Works on this topic are
specific to gradient descent 70 85 and often to ReLU
networks 35 38 88 Our work is about properties of ar
chitectures independent of the training
Work closer to ours 86 has noted that the spectral bias
exists with ReLUs but not with sigmoidal activations and
that it depends on weight magnitudes and depth all of
which we also observe in our experiments Their analy
sis uses the neural tangent kernel NTK whereas we use
a Fourier decomposition of the learned function which is
arguably more direct and intuitive We also examine other
notions of complexity and other architectures
In work concurrent to ours Abbe et al 1 used Walsh
decompositions a variant of Fourier analysis suited to bi
nary functions to characterize learned binary classification
networks They also propose that typical NNs preferably
fit lowdegree basis functions to the training data and this
explains their generalization capabilities Their discussion
which focuses on classification tasks is highly complemen
tary to ours
The deep image prior 78 is an image processing
method that exploit the inductive biases of an untrained net
work However it specifically relies on convolutional U
Net architectures whose inductive biases have little to do
with those studied in this paper
Measures of complexity Quantifying complexity is an
open problem in the fundamental sciences Algorithmic
information theory AIT and Kolmogorov complexity are
one formalization of this problem Kolmogorov complex
ity has been proposed as an explicit regularizer to train NNs
by Schmidhuber 62 Dingle et al 15 used AIT to explain
the prevalence of simplicity in the realworld with examples
in biology and finance Building on this work VallePerez
et al 79 showed that binary ReLU networks with random
weights have a similar bias for simplicity Our work extends
5Frequencies of the target function  used throughout this paper
should not be confused with frequencies of the input data For example
high frequencies in images correspond to sharp edges High frequencies
in the target function correspond to frequent changes of label for similar
images A lowfrequency target function means that similar inputs usually
have similar labels
1this line of inquiry to continuous data to other architectures
and to other notions of complexity
Other measures of complexity for to machine learn
ing models include four related notions sensitivity Lips
chitz constant norms of input gradients and Dirichlet en
ergy 14 Hahn et al 30 adapted sensitivity to the dis
crete nature of language data to measure the complexity of
language classification tasks and of models
Simplicity bias in transformers Zhou et al 90 ex
plain generalization of transformer models on toy reason
ing tasks using a transformerspecific measure of complex
ity They propose that the function learned by a transformer
corresponds to the shortest program in a custom program
ming language that could generate the training data Bhat
tamishra et al 7 showed that transformers are more biased
for simplicity than LSTMs
Controlling inductive biases Recent work has inves
tigated how to explicitly tweak the inductive biases of
NNs through learning objectives 75 76 and architec
tures 10 74 Our results confirms that the choice of acti
vation function is critical 16 Most studies on activation
functions focus on individual neurons 63 or compare the
generalization properties of entire networks 49 Francazi
et al 18 showed that some activations cause a model at
initialization to have nonuniform preference over classes
Simon et al 67 showed that the behaviour of a deep MLP
can be mimicked by a singlelayer MLP with a specifically
crafted activation function
Implicit neural representations INRs are an application
of NNs with a need to control their spectral bias An INR
is a regression network trained to represent eg one spe
cific image by mapping image coordinates to pixel inten
sities they are also known as neural fields orcoordinate
MLPs  To represent sharp image details a network must
represent a highfrequency function which is at odds with
the lowfrequency bias of typical architectures It has been
found that replacing ReLUs with periodic functions 81
Sect 5 Gaussians 58 or wavelets 61 can shift the spec
tral bias towards higher frequencies 60 Interestingly such
architectures Fourier Neural Networks were investigated
as early as 1988 22 Our work shows that several findings
about INRs are also relevant to general learning tasks
B Why study networks with random weights
A motivation can be found in prior work that argued for
interpreting the inductive biases of an architecture as a prior
over functions that plays in the training of the model by
gradient descent
VallePerez et al 79 and Mingard et al 44 argued that
the probability of sampling certain functions upon random
sampling in parameter space could be treated as a prior over
functions for Bayesian inference They then presented preliminary empirical evidence that training with SGD does
approximate Bayesian inference such that the probability
of landing on particular solutions is proportional to their
prior probability when sampling random parameters
C Formal Statement of the NRS
We denote with
F the target function we want to learn
fθ a chosen neural architecture with parameters θ
ffθ atrained network withθoptimized st
fapproximates F
ffθθppriorθ anuntrained randomweight net
work with parameters drawn from an uninformed prior
such as the uniform distribution used to initialize the net
work prior to gradient descent
Cf a scalar estimate the of complexity of the func
tionfas proposed in Section 2
perf f a scalar measure of generalization perfor
mance ie how well fapproximates F for example the
accuracy on a heldout test set
The Neural Redshift NRS makes three propositions
1NNs are biased to implement functions of a particular
level of complexity determined by the architecture
2This preferred complexity is observable in networks
with random weights from an uninformed prior
Formally architecture f distribution ppriorθ
preferred complexity cRst
Cf c with very high probability and
Cf gcwith gRRa monotonic function
This means that the choice of architecture shifts the com
plexity of the learned function up or down similarly as it
does an untrained models The precise shift is usually
not predictable because gis unknown
3Generalization occurs when the preferred complexity
of the architecture matches the target functions
Formally given two architectures f1f2with preferred
complexities c1c2 the one with a complexity closer to
the target functions achieves better generalization
CFgc1CFgc2
perf f
1perf f
2
For example ReLUs are popular because their low
complexity bias often aligns with the target function
2D Technical Details
Activation functions See Figure 11 for a summary of the
activations used in our experiments and 16 for a survey
Discrete network evaluation For a given network that
implements the function fxof input xRd we ob
tain obtain a discrete representation as follows We de
fine a sequence of points Xgridximd
i1corresponding
to a regular grid on the ddimensional hypercube 11d
withmvalues in each dimension  m64in our exper
iments hence mdpoints in total We evaluate the net
work on every point This gives the sequence of scalars
YffxixiXgrid
Visualizations as grayscale images For a network fwith
2D inputs  d2 we produce a visualization as a grayscale
image as follows The values in Yfare simply scaled and
shifted to fill the range from black  0 to white  1 as
Y YminYmax YminY
We then reshape Yinto an mmsquare image
Measures of complexity We use our measures of com
plexity based on Fourier and polynomial decompositions
only with d2because of the computational expense These
methods first require an evaluation of the network on a dis
crete grid as described above  Yf whose size grows expo
nentially in the number of dimensions d
Xu et al 84 proposed two approximations for Fourier
analysis in higher dimensions They were not used in our
experiments but could be valuable for extensions of our
work to higherdimensional settings
Fourier decomposition To compute the measure of com
plexity CFourierf we first precompute values of fon a dis
crete grid Xgrid yielding Yfas describe above We then
perform a standard discrete Fourier decomposition with
these precomputed values We get
fk  Σ xXgridωxkfx
where ωe2πimandkZdare discrete frequency
numbers Per the NyquistShannon theorem with an eval
uation of fon a grid of mvalues in each dimension we
can reliably measure the energy for frequency numbers up
tom2in each dimension ie forkK0 m2d
The value fkis a complex number that captures both
the magnitude and phase of the kth Fourier component We
do not care about the phase hence our measure of com
plexity only uses the real magnitude fkof each Fourier
component k We then seek to summarize the distribution
of these magnitudes across frequencies into a single value
We define the measure of complexity
CFourierf  Σ kKfkk2ΣkKfk
This is the average of magnitudes weighted each by the
corresponding frequency disregarding orientation  eg hor
izontal and vertical patterns in a 2D visualization of thefunction are treated similarly and normalized such that
magnitudes sum to 1
See 57 for a technical discussion justifying Fourier
analysis on nonperiodic bounded functions
Limitations of a scalar measure of complexity The
above definition is necessarily imperfect at summarizing the
distributions of magnitudes across frequencies For exam
ple an fcontaining both low and highfrequencies could
receive the same value as one containing only medium fre
quencies In practice however we use this complexity mea
sure on random networks and we verified empirically that
the distributions of magnitudes are always unimodal This
summary statistic is therefore a reasonable choice to com
pare distributions
Polynomial decomposition As an alternative to Fourier
analysis we use decomposition in polynomial series6It
uses a predefined set of polynomials Pnxn 0 N
to approximate a function fxon the interval x11
asfxΣN
c0cnPnx The coefficients are calculated
ascn05 2n 1R1
1fxPnxdx These definitions
readily extends to higher dimensions
In a Fourier decomposition the coefficients indicate the
amount the various frequency components in f Here each
coefficient cnindicates the amount of a component of a cer
tain order In 2 dimensions  d2 we have N2coefficients
c00 c01 cNN We define our measure of complexity
CChebyshev f ΣN
n1n20cn1n2n1n22
ΣN
n1n20cn1n2
This definition is nearly identical to the Fourier one
In practice we experimented with Hermite Legendre
and Chebyshev bases of polynomials We found the latter
to be more numerically stable To compute the coefficients
we use trapezoidal numerical integration and the same sam
pling of fonXgridas described above and a maximum
order N100 To make the evaluation of the integrals more
numerically stable especially with Legendre polynomials
we omit a border near the edges of the domain 11d
With a 6464grid we omit 3 values on every side
LZ Complexity We use the compressionbased measure
of complexity described in 15 79 as an approximation of
the Kolmogorov complexity We first evaluate fon a grid to
getYfas described above The values in Yfare reals and
generally unique so we discretize them on a coarse scale of
10values regularly spaced in the range of Yfthe granular
ity of 10is arbitrary can be set much higher with virtually no
effect if Yfis large enough We then apply the classical
LempelZiv compression algorithm on the resulting num
ber sequence The measure of complexity CLZfis then
defined as the size of the dictionary built by the compres
sion algorithm The LZ algorithm is sensitive to the order
6Seeeghttpswwwthermopediacomcontent918 
3202202
Relu
202202
Gelu
202202
Swish
202202
Selu
202202
Tanh
202202
Gaussian
202202
Sinmax0  x xPGaussian Xx xσx λ
αex1 x0
x x 0tanh x e05x2sinx
Figure 11 Activation functions used in our experiments
of the sequence to compress but we find very little differ
ence across different orderings of Yfsnake zigzag spiral
patterns Thus we use a simple columnwise vectorization
of the 2D grid
In higher dimensions ColoredMNIST experiments it
would be computationally too expensive to define Xgridas
a dense sampling of the full hypercube 11dsince d
is large Instead we randomly pick mcorners of the hy
percube and sample mpoints xiregularly between each
successive pair of them This gives a total of m2points cor
responding to random linear traversals of the input space
Instead of feeding Yfdirectly to the LZ algorithm we
also feed it with successive differences between succes
sive values which we found to improve the stability of
the estimated complexity for example the pixel values
10121518are turned into 233
LZ Complexity with transformers These experiments
useCLZfon sequences of tokens Each token is repre
sented by its index in the vocabulary and the LZ algorithm
is directly applied on these sequences of integers
Absolute complexity values The different measures of
complexity have different absolute scales and no compara
ble units Therefore for each measure we rescale the val
ues such that observed values fill the range 01
Unbiased model We construct an architecture that dis
plays no bias for any frequency in a Fourier decomposition
of the functions it implements This architecture fθim
plements an inverse discrete Fourier transform with learn
able parameters θθmagθphasethat correspond to the
magnitude and phase of each Fourier component It can
be implement as a onehiddenlayer MLP with sine acti
vation fixed input weights each channel defining the fre
quency of one Fourier component learnable input biases
the phase shifts and learnable output weights the Fourier
coefficients
Experiments with modulo addition These experiments
use a 4layer MLP of width 128 We train them with full
batch Adam a learning rate 0001 for 3k iterations with
no early stopping Each experiment is run with 5 random
seeds The Figure 7 shows the average over seeds for clar
ity each point corresponds to a different architecture Fig
ure 8 shows all seeds each point corresponds to a different
seed
Experiments on ColoredMNIST The dataset is builtfrom the MNIST digits keeping the original separation be
tween training and test images To define a regression task
we turn the original classification labels 019into
values in 01 To introduce a spurious feature each im
age is concatenated with a column of pixels of uniform
grayscale intensity the color of the image This color
is directly correlated with the label with some added noise
to simulate a realistic spurious feature in 3 of the training
data the color is replaced with a random one
The models are 2layer MLPs of width 64 They are
trained with an MSE loss with fullbatch Adam learning
rate 0002 10k iterations with no early stopping The accu
racy in our plots is actually 1MAE mean average error
Since this is a regression task with test labels distributed
uniformly in 01 this metric is indeed interpretable as a
binary accuracy with 05equivalent to random chance
Experiments with transformers In all experiments de
scribed above we directly examine the input  output map
pings implemented by neural networks In the experiments
with transformer sequence models we examine sequences
generated by the models These models are autoregressive
which means that the function they implement is the map
ping context  next token We expect a simple function
eg lowfrequency to produce lots of repetitions in se
quences sampled autoregressively language models are
indeed known to often repeat themselves 21 34 Such
sequences are highly compressible They should therefore
give a low values of CLZ
4E Additional Experiments with
Trained Models
This section presents experiments with models trained with
standard gradient descent We will show that there is a cor
relation between the complexity of a model at initialization
ie with random weights and that of a trained model of the
same architecture
Setup with coordinateMLPs The experiments in this
section use models trained as implicit neural representations
of images INRs also known as coordinateMLPs 81
Such a model is trained to represent a specific grayscale
image It takes as input 2D coordinates in the image plane
x112 It produces as output the scalar grayscale
value of the image at this location The ground truth data
is a chosen image Figure 12 For training we use a subset
of pixels For testing we evaluate the network on a 6464
grid which directly gives a 6464pixel representation of
the function learned
Why use coordinateMLPs This setup produces in
terpretable visualizations and allows comparing visually
the ground truth original image with the learned func
tion Because the ground truth is defined on a regular
grid unlike most real data it also facilitates the compu
tation of 2D Fourier transforms We use Fourier trans
forms to quantitatively compare the ground truth with
the learned function and verify the third part of the NRS
generalization is enabled by matching of the architec
tures preferred complexity with the target functions
Data We use a 6464pixel version of the wellknown
cameraman image Figure 12 left 26 For training we
use a random 40 of the pixels This image contains
both uniform areas low frequencies and fine details with
sharp transitions high frequencies We also use a synthetic
waves image Figure 12 right It is the sum of two orthog
onal sine waves one twice the frequency of the other For
training we only use pixels on local extrema of the image
They form a very sparse set of points This makes the task
severely underconstrained A model can fit this data with a
variety functions This will reveal whether a model prefers
fitting low or highfrequency patterns
E1 Visualizing Inductive Biases
We first perform experiments to get a visual intuition of
the inductive biases provided by different activation func
tions We train 3layer MLPs of width 64 with fullbatch
Adam and a learning rate of 002 on the cameraman and
waves data Figure 13 next page shows very different
functions across architectures The cameraman image con
tains fine details with sharp edges Their presence in the
reconstruction indicate whether the model learned high
frequency componentsCameraman Waves
Full
data
Training
points
Figure 12 Data used in the coordinateMLP experiments
Differences across architectures TheReLUlike activa
tions are biased for simplicity hence the learned functions
tend to smooth out image details favor large uniform re
gions and smooth variations Yet they can also represent
sharp transitions when these are necessary to fit the training
data The decision boundary with ReLUs which is a poly
tope 47 is faintly discernible as crisscrossing lines in the
image Surprisingly we observe differences across different
initial weight magnitudes with ReLU even though our ex
periments on random networks did not show any such effect
Section 3 We believe that this is a sign of optimization
difficulties when the initial weights are large  ie difficulty
of reaching a complex solution
With other activations TanH Gaussian sine the bias
for low or high frequencies is much more clearly modulated
by the initial weight magnitude With large magnitudes the
images contain highfrequency patterns Similar observa
tions are made with the waves data Figure 14
Theunbiased model is useless as expected It reaches
perfect accuracy on the training data but the predictions on
other pixels look essentially random
With random weights We also examine in Figure 13 the
function represented by each model at initialization with
random weights As expected we observe a strong cor
relation between the amount of high frequencies at initial
ization and in the trained model We also examine mod
els at the end of training after shuffling the trained weights
within each layer This is another randomweight model
but its distribution of weight magnitudes matches exactly
the trained model Indeed the shuffling preserves the
weights within each layer but destroys the precise connec
tions across layers This enables a very interesting obser
vation With nonReLUlike architectures there is a clear
increase in complexity between the functions at initializa
tion and with shuffled weights This means that the learned
increase in complexity in nonReLU networks is partly
encoded by changes in the distribution of weight magni
tudes the only thing preserved through shuffling In conAt init
ReLU Trained
Shuffled
At init
GELU Trained
Shuffled
At init
Swish Trained
Shuffled
At init
TanH Trained
Shuffled
At init
Gaussian Trained
Shuffled
At init
Sin Trained
Shuffled
At init
Unbiased Trained
Shuffled
Default Magnitude of initial weights  Larger
Figure 13 CoordinateMLPs trained to represent the cameraman with various activations and initial weight magnitudes The model is
trained on 40 pf pixels and evaluated on a 6464grid The images provide intuitions about the inductive biases of each architecture The
differences across models with random weights at init and with shuffled trained weights shuffled show that the increase in complexity in
nonReLU models is realized by changes in weight magnitudes which are maintained through the shuffling In contrast ReLU networks
revert to a low complexity after shuffling suggesting that complexity is encoded in the precise weight values not their magnitudesFull data
 Sparse training points
ReLU
GELU
Swish
TanH
Gaussian
Sine
04 08 10 70 130 190 220
Magnitude of initial weights fraction of standard magnitude
Figure 14 CoordinateMLPs trained on sparse points of the waves data Variations across learned functions show how architectures are
biased towards low ReLU or high frequencies Sine ReLU activations give the most consistent behaviour across weight magnitudes
trast ReLU networks revert to a low complexity after shuf
fling This suggests that complexity in ReLU networks is
encoded in the weights precise values and connections
across layers not in their magnitude 
E2 Training Trajectories
We will now show that NNs can represent any function but
complex ones require precise weight values and connec
tions across layers that are unlikely through random sam
pling but that can be found through gradientbased training
Unlike prior work 59 that claimed that the complexity
at initialization causally influences the solution our results
indicate instead they are two effects of a common cause the
preferred complexity of the architecture The architec
ture is biased towards a certain complexity and this influ
ences both the randomlyinitialized model and those found
by gradient descent There exist weight values for other
functions of complexity much lower or higher than the pre
ferred one but they are less likely to occur in either case
For example ReLU networks are biased towards sim
plicity but can represent complex functions Yet contrary
to 59 initializing gradient descent with such a complex
function does not yield a complex solutions after training
on simple data In other words the architectures bias pre
vails over the exact starting point of the training
Experimental setup We train models with different acti
vations and initial magnitudes on the cameraman data using
19pixels for training We plot in Figure 16 the training tra
jectory of each model Each point of a trajectory represents
the average weight magnitude vs the Fourier complexity of
the function represented by the modelChanges in weight magnitudes during training The first
observation is that the average weight magnitude changes
surprisingly little However further examination Fig
ure 15 shows that the distribution shifts from uniform to
longtailed The trained models contain more and more
largemagnitude weights
Changes in complexity during training In Figure 16
we observe that models with ReLUlike activations at ini
tialization have low complexity regardless of the initializa
tion magnitude As training progresses the complexity in
creases to fit the training data This increased complex
ity is encoded in the weights precise values and connec
tions across layers since at the end of training shuffling
the weights reverts models back to the initial low complex
ity With other activations the initial weight magnitudes
impact the complexity at initialization and of the trained
model Some of the additional complexity in the trained
model seems to be partly encoded by increases in weight
magnitudes since shuffling the trained weights does seem
to retain some of this additional complexity
Summary The choice of activation function and initial
weight magnitude affect the preferred complexity of a
model This complexity is visible both at initialization with
random weights and after training with gradient descent
The complexity of the learned function can depart from the
preferred level just enough to fit the training points Out
side the interpolated training points the shape of the learned
function is very much affected by the preferred complexity
With ReLU networks this effect usually drives the com
plexity downwards the simplicity bias  With other archi
7Weights Biases Activations
Figure 15 Distributions of the magnitudes of the weights biases and activations during training of a 3layer MLP the 4th row is
the output layer on the cameraman data Weights and biases are initialized from a uniform distribution and zero respectively The
distributions become very longtailed as training progresses The occurrence of large values is the reason why the dependence of the
preferred complexity of certain architecture on weight magnitudes is important it would not matter if the distribution of magnitudes
remained constant throughout training
tectures and large weight magnitudes this often drives the
complexity upwards Both can be useful Section 4 showed
that sine activations can enable learning the parity function
from sparse training points and reduce shortcut learning by
shifting the preferred complexity upwards
Our observations also explain why the coordinateMLPs
with sine activations proposed in 68 SIREN require a
careful initialization This adjusts the preferred complexity
to the typical frequencies found in natural images
E3 Pretraining and Finetuning
We outline preliminary results from additional experiments
Why study finetuning We have seen that the preferred
complexity of an architecture can be observed with random
weights The model can then be trained by gradient descent
to represent data with a different level of complexity For
example a ReLU network initially biased for simplicity
can represent a complex function after training on complex
data Gradient descent finely adjusts the weights to repre
sent a complex function We will now see how pretrainingthen finetuning on data with different levels of complexity
blends the two in the final finetuned model
Experimental setup We pretrain an MLP with ReLU or
TanH activations on highfrequency data highfrequency
2D sine waves We then finetune it on lowerfrequency
2D sine waves of a different random orientation
Observations During early finetuning iterations TanH
models retain a highfrequency bias much more than ReLU
models This agrees with the proposition in E1 that the for
mer encode high frequencies in weight magnitudes while
ReLU models encode them in precise weight values which
are quickly lost during finetuning
We further verify this explanation by shuffling the pre
trained weights within each layer before finetuning The
ReLU models then show no highfrequency bias at all
since the precise arrangement of weights is completely lost
through the shuffling TanH models however do still show
highfrequency components in the finetuned solution This
confirms that TanH models encode high frequencies partly
in weight magnitudes since this is the only property pre
8ReLU GELU SwishComplexity Fourier 
02 03 04 05 06
02 03 04 05 06
02 03 04 05 06
TanH Gaussian Sine
05 1 15
02 04 06 08 1 12
02 03 04 05 06
Average magnitude of weights during training
At initialization At last epoch With shuffled trained weights
Lower frequencies
 Higher frequencies
Figure 16 Training trajectories of MLP models trained on the cameraman data Each line corresponds to one training run with a
different seed or initial weight magnitude With ReLUlike activations the models at initialization have low complexity regardless of the
initialization magnitude As training progresses the complexity increases to fit the training data This increased complexity is encoded in
the weights precise values and connections across layers since at the end of training shuffling the weights reverts models to the initial low
complexity With other activations the initial weight magnitude impacts the complexity at initialization and of the trained model Some
of the additional complexity in the trained model seems to be partly encoded by increases in the weight magnitudes since shuffling the
trained weights does seem to retain some of this additional complexity
served by the shuffling
Finally we do not find evidence for the prior
claim 59 that complexity at initialization persists indef
initely throughout finetuning Instead with enough it
erations of finetuning any pretraining effect on the pre
ferred complexity eventually vanishes For example a
ReLU model pretrained on high frequencies initially con
tains highfrequency components in the finetuned model
But with enough iterations they eventually disappear ie
the simplicity bias of ReLUs eventually takes over We be
lieve that the experiments in 59 were simply not run for
long enough This observation also disproves the causal
link proposed in 59 between the complexity at initializa
tion and in the trained model
9F Full Results with Random Networks
On the next pages Figures 1921 we present heatmaps showing the average complexity of functions implemented by
neural networks of various architectures with random weights and biases Each heatmap corresponds to one architecture with
varying number of layers heatmap columns and weight magnitudes heatmap rows For every other cell of a heatmap we
visualize as a 2D grayscale image a function implemented by one such a network with 2D input and scalar output
ReLU GELU Swish SELU TanH Gaussian Sin UnbiasedFourier
1 1001
1 1001
1 1001
1 1001
1 5001
1 3001
1 2001
1 1001
    MLP
  Gating
  Residual
  Layer norm Chebyshev
1 1001
1 1001
1 1001
1 1001
1 5001
1 3001
1 2001
1 1001 Legendre
1 1001
1 1001
1 1001
1 1001
1 5001
1 3001
1 2001
1 1001 LZ
1 1001
1 1001
1 1001
1 1001
1 5001
1 3001
1 2001
1 1001
Figure 17 All measures of complexity Y axes of random networks generally increase with weightactivation magnitudes X axis  The
sensitivity is however very different across activation functions columns This sensitivity also increases with multiplicative interactions
ie gating decreases with residual connections and is essentially absent with layer normalization These effects are also visible on the
heatmaps see next pages but faint hence visualized here as line plots
0 LZ 101Fourier
0 LZ 101Legendre
0 LZ 101Chebyshev
Figure 18 Correlations between our measures of complexity on random networks They are based on frequency Fourier polynomial
order Legendre Chebyshev or compressibility LZ They capture different notions yet they are closely correlated
1011 ReLU GELU Swish SELU TanH Gaussian Sin Unbiased
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
With residual connections
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
With layer normalization
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
With gating
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
Figure 19 Heatmaps of the average complexity  Fourier  of various architectures with random weights and example functions12 ReLU GELU Swish SELU TanH Gaussian Sin Unbiased
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
With residual connections
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
With layer normalization
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
With gating
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
Figure 20 Same as Figure 19 with the LZ measure instead of the Fourierbased one Results are nearly identical13 ReLU GELU Swish SELU TanH Gaussian Sin Unbiased
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
With residual connections
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
With layer normalization
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
With gating
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
Figure 21 Same as Figure 19 with the LZ measure instead of the Fourierbased one Results are nearly identical but noisier
  EFFICIENTLY TRAINABLE TEXTTOSPEECH SYSTEM BASED ON
DEEP CONVOLUTIONAL NETWORKS WITH GUIDED ATTENTION
Hideyuki Tachibana Katsuya Uenoyama
PKSHA Technology Inc
Bunkyo Tokyo Japan
h_tachibanapkshatechcomShunsuke Aihara
Independent Researcher
Shinjuku Tokyo Japan
aiharaargmaxjpc2018 IEEE Personal use of this material is permitted Permission from IEEE must be obtained for all other uses in any current or future media including
reprintingrepublishing this material for advertising or promotional purposes creating new collective works for resale or redistribution to servers or lists or
reuse of any copyrighted component of this work in other works
ABSTRACT
This paper describes a novel texttospeech TTS technique based
on deep convolutional neural networks CNN without use of any
recurrent units Recurrent neural networks RNN have become a
standard technique to model sequential data recently and this tech
nique has been used in some cuttingedge neural TTS techniques
However training RNN components often requires a very power
ful computer or a very long time typically several days or weeks
Recent other studies on the other hand have shown that CNN
based sequence synthesis can be much faster than RNNbased tech
niques because of high parallelizability The objective of this paper
is to show that an alternative neural TTS based only on CNN alle
viate these economic costs of training In our experiment the pro
posed Deep Convolutional TTS was sufﬁciently trained overnight
15 hours using an ordinary gaming PC equipped with two GPUs
while the quality of the synthesized speech was almost acceptable
Index Terms Texttospeech deep learning convolutional
neural network attention sequencetosequence learning
1 INTRODUCTION
Texttospeech TTS is getting more and more common recently
and is getting to be a basic user interface for many systems To
further promote the use of TTS in various systems it is signiﬁcant to
develop a manageable maintainable and extensible TTS component
that is accessible to speech nonspecialists enterprising individuals
and small teams
Traditional TTS systems however are not necessarily friendly
for them since they are typically composed of many domainspeciﬁc
modules For example a typical parametric TTS system is an elab
orate integration of many modules eg a text analyzer an F0gen
erator a spectrum generator a pause estimator and a vocoder that
synthesize a waveform from these data etc
Deep learning 1 may integrate these internal building blocks
into a single model and connects the input and the output directly
This type of technique is sometimes called endtoend learning
Although such a technique is sometimes criticized as a black box
an endtoend TTS system named Tacotron 2 which directly esti
mates a spectrogram from an input text has achieved promising per
formance recently without using handengineered parametric mod
els based on domainspeciﬁc knowledge Tacotron however has a
drawback that it uses many recurrent units which are quite costly to
train It is almost infeasible for ordinary labs who do not have lux
urious machines to study and extend it further In fact some people
tried to implement open clones of Tacotron 3 4 5 6 but they arestruggling to reproduce the speech of satisfactory quality as clear as
the original results
The purpose of this paper is to show Deep Convolutional TTS
DCTTS a novel fully convolutional neural TTS The architecture
is largely similar to Tacotron 2 but is based on a fully convolu
tional sequencetosequence learning model similar to the literature
7 We show that this fully convolutional TTS actually works in a
reasonable setting The contribution of this article is twofold 1
Propose a fully CNNbased TTS system which can be trained much
faster than an RNNbased stateoftheart neural TTS system while
the sound quality is still acceptable 2 An idea to rapidly train the
attention module which we call guided attention is also shown
11 Related Work
111 Deep Learning and TTS
Recently deep learningbased TTS systems have been intensively
studied and surprisingly high quality results are obtained in some
of recent studies The TTS systems based on deep neural networks
include Zens work 8 the studies based on RNN eg 9 10 11
12 and recently proposed techniques eg WaveNet 13 sec 32
Char2Wav 14 DeepV oice12 15 16 and Tacotron 2
Some of them tried to reduce the dependency on handengineered
internal modules The most extreme technique in this trend would
be Tacotron 2 which depends only on mel and linear spectro
grams and not on any other speech features eg F0 Our method is
close to Tacotron in a sense that it depends only on these spectral
representations of audio signals
Most of the existing methods above use RNN a natural tech
nique of time series prediction An exception is WaveNet which
is fully convolutional Our method is also based only on CNN but
our usage of CNN would be different from WaveNet as WaveNet is
a kind of a vocoder or a backend which synthesizes a waveform
from some conditioning information that is given by frontend com
ponents On the other hand ours is rather a frontend and most of
backend processing We use CNN to synthesize a spectrogram
from which a simple vocoder can synthesize a waveform
112 Sequence to Sequence seq2seq Learning
Recently recurrent neural networks RNN have become a standard
technique for mapping a sequence to another sequence especially
in the ﬁeld of natural language processing eg machine transla
tion 17 18 dialogue system 19 20 etc See also 1 sec 104
RNNbased seq2seq however has some disadvantages Firstly
a vanilla encodedecoder model cannot encode too long sequence
into a ﬁxedlength vector effectively This problem has been re
solved by a mechanism called attention 21 and the attention
Published as a conference paper at ICASSP 2018 Calgary Canada cIEEE 2018arXiv171008969v2  csSD  30 Sep 2020mechanism now has become a standard idea of seq2seq learning
techniques see also 1 sec 12451
Another problem is that RNN typically requires much time to
train since it is less suited for parallel computation using GPUs To
overcome this problem several researchers proposed the use of CNN
instead of RNN eg 22 23 24 25 26 Some studies have shown
that CNNbased alternative networks can be trained much faster and
sometimes can even outperform the RNNbased techniques
Gehring et al 7 recently united these two improvements of
seq2seq learning They proposed an idea on how to use attention
mechanism in a CNNbased seq2seq learning model and showed
that the method is quite effective for machine translation The
method we proposed is based on the similar idea to the literature 7
2 PRELIMINARY
21 Basic Knowledge of the Audio Spectrograms
An audio waveform and a complex spectrogram ZfZftg 2
CF0T0are mutually transformed by linear maps called Shortterm
Fourier Transform STFT and inverse STFT where F0andT0de
note the number of frequency bins and temporal bins respectively
It is common to consider only the magnitude jZjfjZftjg 2
RF0T0 since it still has useful information for many purposes
and thatjZjandZare almost the same in the sense that there ex
ist many phase estimation ie jZjtoZestimation and therefore
waveform synthesis techniques eg the famous GrifﬁnLim algo
rithm GLA 27 see also eg 28 We always use RTISILA 29
an online GLA to synthesize a waveform In this paper we al
ways normalize STFT spectrograms as jZj jZjmaxjZj
and convert backjZj jZjwhen we ﬁnally need to synthesize
the waveform where are pre and postemphasis factors
It is also common to consider a mel spectrogram S2RFT0
FF0 obtained by applying a mel ﬁlterbank to jZj This is
a standard dimension reduction technique for speech processing In
this paper we also reduce the temporal dimensionality from T0to
dT04eTby picking up a time frame every four time frames to
accelerate the training of Text2Mel shown below We also normalize
mel spectrograms as S SmaxS
22 Notation Convolution and Highway Activation
In this paper we denote the 1D convolution layer 30 by a space sav
ing notation Co i
kX whereiis the size of input channel ois the
size of output channel kis the size of kernel is the dilation factor
and an argument Xis a tensor having three dimensions  batch chan
nel temporal  The stride of convolution is always 1 Convolution
layers are preceded by appropriatelysized zero padding whose size
is suitably determined by a simple arithmetic so that the length of
the sequence is kept constant Let us similarly denote the 1D decon
volution layer as Do i
kX The stride of deconvolution is always 2
in this paper Let us write a layer composition operator as  and
let us write networks like FReLUGX  FReLU GX
andFG2X FGFGX etc ReLU is an elementwise
activation function deﬁned by ReLU x  maxx0
Convolution layers are sometimes followed by a Highway Net
work 31like gated activation which is advantageous in very deep
networks Highway XL H1ReLU H21H1
XwhereH1H2are tensors of the same shape as X and are output
asH1H2 LXby a layer L The operatoris the element
wise product and is the elementwise sigmoid function Hereafter
let us denote HCd d
kX Highway XC2d d
k 
Fig 1  Network architecture
TextEnc L   HC2d 2d
112HC2d 2d
312HC2d 2d
327HC2d 2d
39
HC2d 2d
33HC2d 2d
312C2d 2d
11ReLUC2d e
11CharEmbededimL
AudioEnc S   HCd d
332HCd d
327HCd d
39HCd d
33HCd d
312
Cd d
11ReLUCd d
11ReLUCd F
11S
AudioDec R0 CF d
11ReLUCd d
113HCd d
312HCd d
327
HCd d
39HCd d
33HCd d
31Cd 2d
11R0
SSRN Y CF0 F0
11ReLUCF0 F0
112CF0 2c
11HC2c 2c
312
C2c c
11HCc c
33HCc c
31Dc c
212HCc c
33HCc c
31Cc F
11Y
Fig 2  Details of each component For notation see section 22
3 PROPOSED NETWORK
Since some literature 2 32 suggest that the staged synthesis from
low to highresolution has advantages over the direct synthesis of
highresolution data we synthesize the spectrograms using the fol
lowing two networks 1 Text2Mel which synthesizes a mel spec
trogram from an input text and 2 Spectrogram Superresolution
Network SSRN which synthesizes a full STFT spectrogram from
a coarse mel spectrogram Fig 1 shows the overall architecture
31 Text2Mel Text to Mel Spectrogram Network
We ﬁrst consider to synthesize a coarse mel spectrogram from a text
This is the main part of the proposed method This module consists
of four submodules Text Encoder Audio Encoder Attention and
Audio Decoder The network TextEnc ﬁrst encodes the input sen
tenceL l1lN2CharNconsisting of Ncharacters into the
two matrices KkeyVvalue2RdN wheredthe dimension of
encoded characters On the other hand the network AudioEnc en
codes the coarse mel spectrogram SS1F1T2RFT of speech
of lengthT into a matrix Qquery2RdT
KV  TextEnc L QAudioEnc S1F1T 1
An attention matrix A2RNT deﬁned as follows evaluates how
strongly the nth character lnand thetth mel spectrum S1Ftare
related
AsoftmaxnaxisKTQp
d 2
Ant1implies that the module is focusing on nth character ln
at timet and it will focus on lnorln1or others nearby at the
subsequent time t 1 Whatever let us expect those are encoded in
nth column of V Thus a matrix R2RdT which is the seed of
the subsequent mel spectra S1F2T1 is obtained as
RAttQKV  VA Note matrix product 3
2The resultant Ris concatenated with the encoded audio Q asR0
RQ because we found it beneﬁcial in our pilot study Then the
Audio Decoder module estimates a mel spectrogram from the seed
matrixR02R2dTas follows
Y1F2T1AudioDec R0 4
The resultant Y1F2T1needs to approximate the temporally
shifted ground truth S1F2T1 The error is evaluated by a loss
functionLspecY1F2T1jS1F2T1 and is backpropagated to
the network parameters The loss function was the sum of L1 loss
and a functionDbinwhich we call binary divergence
DbinYjS Eft
SftlogYft
Sft1Sft log1Yft
1Sft
EftSftYft log1  exp Yft  const5
where Yft logitYft Since the gradient is nonvanishing ie
DbinYjSYftYftSft it is advantageous for gradient
based training It is easily veriﬁed that the spectrogram error is non
negativeLspecYjS DbinYjS EjYftSftj0 and the
equality holds iff YS
311 Details of TextEnc AudioEnc and AudioDec
The networks are fully convolutional and are not dependent on any
recurrent units In order to take into account the long contextual in
formation we used dilated convolution 33 13 24 instead of RNN
The top equation of Fig 2 is the content of TextEnc  It con
sists of a character embedding and several 1D noncausal convolu
tion layers In the literature 2 an RNNbased component named
CBHG was used but we found this convolutional network also
works well AudioEnc andAudioDec  shown in Fig 2 are com
posed of 1D causal convolution layers These convolution should be
causal because the output of AudioDec is feedbacked to the input of
AudioEnc at the synthesis stage
32 Spectrogram Superresolution Network SSRN
We ﬁnally synthesize a full spectrogram jZj2RF04T from the
obtained coarse mel spectrogram Y2RFT using the spectrogram
superresolution network SSRN Upsampling frequency from Fto
F0is fairly simple We can achieve that by increasing the convo
lution channels of a 1D convolutional network Upsampling of the
temporal axis is not done the same way Instead we quadruple the
length of the sequence from Tto4TT0 by applying deconvo
lution layers of stride size 2 twice
The bottom equation of Fig 2 shows SSRN  In this paper all
convolutions of SSRN are noncausal since we do not consider on
line processing The loss function is the same as Text2Mel the sum
of L1 distance and Dbin deﬁned by 5 between the synthesized
spectrogram SSRN Sand the ground truth jZj
4 GUIDED ATTENTION
41 Guided Attention Loss Motivation Method and Effects
In general an attention module is quite costly to train Therefore
if there is some prior knowledge incorporating them into the model
may be a help to alleviate the heavy training We show that the
following simple method helps to train the attention module
In TTS the possible attention matrix Alies in the very small
subspace of RNT This is because of the rough correspondence of
the order of the characters and the audio segments That is when one
reads a text it is natural to assume that the text position nprogresses
Fig 3  Comparison of the attention matrix A trained with
and without the guided attention loss LattA Left Without and
Right with the guided attention The test text is icassp stands
for the international conference on acoustics speech and signal
processing  We did not use the heuristics described in section 42
nearly linearly to the time t ienat whereaNT  This
is a noticeable difference of TTS from other seq2seq learning tech
niques such as machine translation where an attention module needs
to resolve the word alignment between two languages that have very
different syntax eg English and Japanese
Based on this idea we introduce guided attention loss which
prompts the attention matrix Ato be nearly diagonal LattA 
EntAntWntwhereWnt 1expfnNtT22g2gIn
this paper we set g 02 IfAis far from diagonal eg reading the
characters in the random order it is strongly penalized by the loss
function This subsidiary loss function is simultaneously optimized
with the main lossLspecwith the equal weight
Although this measure is based on quite a rough assumption
it improved the training efﬁciency In our experiment if we added
the guided attention loss to the objective the term began decreas
ing only after100 iterations After 5K iterations the attention
became roughly correct not only for training data but also for new
input texts On the other hand without the guided attention loss
it required much more iterations It began learning after 10K it
erations and it required 50K iterations to look at roughly correct
positions but the attention matrix was still vague Fig 3 compares
the attention matrix trained with and without guided attention loss
42 Forcibly Incremental Attention at the Synthesis Stage
At the synthesis stage the attention matrix Asometimes fails to
focus on the correct characters Typical errors we observed were
1 skipping several letters and 2 repeating a same word twice or
more In order to make the system more robust we heuristically
modiﬁed the matrix Ato be nearly diagonal by a simple rule as
follows We observed this method sometimes alleviated such fail
ures
Letntbe the position of the character which is read at time t
ient argmaxnAnt Comparing the current position ntand the
previous position nt1 if the difference ntnt1is outside of the
range1ntnt13 the current attention is forcibly set
asAntnn t11Kronecker delta  to increment the attention
target asntnt1 1
5 EXPERIMENT
51 Experimental Setup
We used LJ Speech Dataset 34 to train the networks This is
a public domain speech dataset consisting of 13K pairs of text
and speech without phonemelevel alignment 24 hours in total
These speech data have a little reverberation We preprocessed the
3Table 1  Parameter Settings
Sampling rate of audio signals 22050 Hz
STFT window function Hanning
STFT window length and shift 1024 464 ms 256  116ms
STFT spectrogram size F04T 5134TTdepends on audio clip
Mel spectrogram size FT 80TTdepends on audio clip
Dimensionedandc 128 256 512
ADAM parameters  12 21040509106
Minibatch size 16
Emphasis factors  06 13
RTISILA window and iteration 100 10
Character set Char az andSpace andNULL
Table 2  Comparison of MOS 95 conﬁdence interval training
time and iterations Text2MelSSRN of an open Tacotron 5 and
the proposed method DCTTS The digits with  were excerpted
from the repository 5
Method Iteration Time MOS 95 CI
Open Tacotron 5 877K12 days207062
DCTTS 20K 40K 2 hours 174052
DCTTS 90K150K 7 hours 263075
DCTTS 200K340K 15 hours 271066
DCTTS 540K900K 40 hours 261062
texts by spelling out some of abbreviations and numeric expressions
decapitalizing the capitals and removing less frequent characters
not shown in Table 1 where NULL is a dummy character for zero
padding
We implemented our neural networks using Chainer 20 35
We trained the models using a household gaming PC equipped with
two GPUs The main memory of the machine was 62GB which
is much larger than the audio dataset Both GPUs were NVIDIA
GeForce GTX 980 Ti with 6 GB memories
For simplicity we trained Text2Mel and SSRN independently
and asynchronously using different GPUs All network parameters
were initialized using Hes Gaussian initializer 36 Both networks
were trained by the ADAM optimizer 37 When training SSRN
we randomly extracted short sequences of length T 64 for each
iteration to save memory usage To reduce the disk access we re
duced the frequency of creating the snapshot of parameters to only
once per 5K iterations Other parameters are shown in Table 1
As it is not easy for us to reproduce the original results of
Tacotron we instead used a readytouse model 5 for comparison
which seemed to produce the most reasonable sounds in the open
implementations It is reported that this model was trained using
LJ Dataset for 12 days 877K iterations on a GTX 1080 Ti newer
GPU than ours Note this iteration is still much less than the original
Tacotron which was trained for more than 2M iterations
We evaluated mean opinion scores MOS of both methods
by crowdsourcing on Amazon Mechanical Turk using crowdMOS
toolkit 38 We used 20 sentences from Harvard Sentences List
12 The audio data were synthesized using ﬁve methods shown
in Table 2 The crowdworkers rated these 100 clips from 1 Bad
to 5 Excellent Each worker is supposed to rate at least 10 clips
To obtain more responses of higher quality we set a few incentives
shown in the literature The results were statistically processed using
the method shown in the literature 38
52 Result and Discussion
In our setting the training throughput was 38 minibatchs Text2Mel
and64 minibatchs SSRN This implies that we can iterate the
updating formulae of Text2Mel 200K times in 15 hours Fig 4
Fig 4  Top Attention middle mel and bottom linear STFT
spectrogram synthesized by the proposed method after 15 hours
training The input text is icassp stands for the international con
ference on acoustics speech and signal processing  90 chars
shows an example of attention synthesized mel and full spectro
grams after 15 hours training It shows that the method can almost
correctly focus on the correct characters and synthesize quite clear
spectrograms More samples are available at the authors web page1
In our crowdsourcing experiment 31 subjects evaluated our
data After the automatic screening by the toolkit 38 560 scores
rated by 6 subjects were selected for ﬁnal statistics calculation Ta
ble 2 compares the performance of our proposed method DCTTS
and an open Tacotron Our MOS 95 conﬁdence interval was
27106615 hours training while the Tacotrons was 207062
Although it is not a strict comparison since the frameworks and
the machines were different it would be still concluded that our
proposed method is quite rapidly trained to the satisfactory level
compared to Tacotron
Note that the MOS were below the level reported in 2 The
reasons may be threefold 1 the limited number of iterations 2
SSRN needs to synthesize the spectrograms from less information
than 2 and 3 the reverberation of the training data
6 SUMMARY AND FUTURE WORK
This paper described a novel TTS technique based on deep convo
lutional neural networks and a technique to train the attention mod
ule rapidly In our experiment the proposed Deep Convolutional
TTS was trained overnight  15 hours using an ordinary gaming
PC equipped with two GPUs while the quality of the synthesized
speech was almost acceptable
Although the audio quality is far from perfect yet it may be
improved by tuning some hyperparameters thoroughly and by ap
plying some techniques developed in the deep learning community
We believe this method will encourage further development of
the applications based on speech synthesis We can expect that this
simple neural TTS may be extended to many other purposes eg
emotionalnonlinguisticpersonalized speech synthesis etc by fur
ther studies In addition since a neural TTS has become lighter the
studies on more integrated speech systems eg some multimodal
systems may have become more feasible These issues should be
worked out in the future
7 ACKNOWLEDGEMENT
The authors would like to thank the OSS contributors and the data
creators LibriV ox contributors and keithito
1httpsgithubcomtachihitts_samples
48 REFERENCES
1 I Goodfellow et al Deep Learning  MIT Press 2016 http
wwwdeeplearningbookorg 
2 Y  Wang et al Tacotron Towards endtoend speech synthe
sis in Proc Interspeech  2017 arXiv170310135
3 A Barron Implementation of Googles Tacotron in Ten
sorFlow 2017 Available at GitHub httpsgithub
combarronalexTacotron visited Oct 2017
4 K Park A TensorFlow implementation of Tacotron A
fully endtoend texttospeech synthesis model 2017 Avail
able at GitHub httpsgithubcomKyubyong
tacotron visited Oct 2017
5 K Ito Tacotron speech synthesis implemented in Tensor
Flow with samples and a pretrained model 2017 Avail
able at GitHub httpsgithubcomkeithito
tacotron visited Oct 2017
6 R Yamamoto PyTorch implementation of Tacotron speech
synthesis model 2017 Available at GitHub https
githubcomr9y9tacotron_pytorch visited Oct
2017
7 J Gehring et al Convolutional sequence to sequence learn
ing in Proc ICML  2017 pp 12431252 arXiv170503122
8 H Zen et al Statistical parametric speech synthesis using
deep neural networks in Proc ICASSP  2013 pp 79627966
9 Y  Fan et al TTS synthesis with bidirectional LSTM based
recurrent neural networks in Proc Interspeech  2014 pp
19641968
10 H Zen and H Sak Unidirectional long shortterm memory
recurrent neural network with recurrent output layer for low
latency speech synthesis in Proc ICASSP  2015 pp 4470
4474
11 S Achanta et al An investigation of recurrent neural network
architectures for statistical parametric speech synthesis in
Proc Interspeech  2015 pp 859863
12 Z Wu and S King Investigating gated recurrent networks for
speech synthesis in Proc ICASSP  2016 pp 51405144
13 A van den Oord et al WaveNet A generative model for raw
audio arXiv160903499  2016
14 J Sotelo et al Char2wav Endtoend speech synthesis in
Proc ICLR  2017
15 S Arik et al Deep voice Realtime neural texttospeech
inProc ICML  2017 pp 195204 arXiv170207825
16 S Arik et al Deep voice 2 Multispeaker neural textto
speech in Proc NIPS  2017 arXiv170508947
17 K Cho et al Learning phrase representations using RNN
encoderdecoder for statistical machine translation in Proc
EMNLP  2014 pp 17241734
18 I Sutskever et al Sequence to sequence learning with neural
networks in Proc NIPS  2014 pp 31043112
19 O Vinyals and Q Le A neural conversational model in
Proc Deep Learning Workshop ICML  2015
20 I V  Serban et al Building endtoend dialogue systems us
ing generative hierarchical neural network models in Proc
AAAI  2016 pp 3776378421 D Bahdanau et al Neural machine translation by jointly
learning to align and translate in Proc ICLR 2015
arXiv14090473  2014
22 Y  Kim Convolutional neural networks for sentence
classiﬁcation in Proc EMNLP  2014 pp 17461752
arXiv14085882
23 X Zhang et al Characterlevel convolutional networks for
text classiﬁcation in Proc NIPS  2015 arXiv150901626
24 N Kalchbrenner et al Neural machine translation in linear
time arXiv161010099  2016
25 Y  N Dauphin et al Language modeling with gated convolu
tional networks arXiv161208083  2016
26 J Bradbury et al Quasirecurrent neural networks in Proc
ICLR 2017  2016
27 D Grifﬁn and J Lim Signal estimation from modiﬁed short
time fourier transform IEEE Trans ASSP  vol 32 no 2 pp
236243 1984
28 P Mowlee et al Single Channel PhaseAware Signal Process
ing in Speech Communication Theory and Practice  Wiley
2016
29 X Zhu et al Realtime signal estimation from modiﬁed
shorttime fourier transform magnitude spectra IEEE Trans
ASLP  vol 15 no 5 2007
30 Y  LeCun and Y  Bengio The handbook of brain theory and
neural networks chapter Convolutional Networks for Images
Speech and Time Series pp 255258 MIT Press 1998
31 R K Srivastava et al Training very deep networks in Proc
NIPS  2015 pp 23772385
32 H Zhang et al StackGAN Text to photorealistic image syn
thesis with stacked generative adversarial networks in Proc
ICCV  2017 pp 59075915 arXiv161203242
33 F Yu and V  Koltun Multiscale context aggregation by di
lated convolutions in Proc ICLR  2016
34 K Ito The LJ speech dataset 2017 Available at https
keithitocomLJSpeechDataset visited Sep
2017
35 S Tokui et al Chainer A nextgeneration open source
framework for deep learning in Proc Workshop LearningSys
NIPS  2015
36 K He et al Delving deep into rectiﬁers Surpassing human
level performance on ImageNet classiﬁcation in Proc ICCV 
2015 pp 10261034 arXiv150201852
37 D P Kingma and J Ba Adam A method for stochastic opti
mization in Proc ICLR 2015  2014 arXiv14126980
38 F Ribeiro et al CrowdMOS An approach for crowdsourcing
mean opinion score studies in Proc ICASSP  2011 pp 2416
2419
5
  Generating Sequences With
Recurrent Neural Networks
Alex Graves
Department of Computer Science
University of Toronto
gravescstorontoedu
Abstract
This paper shows how Long Shortterm Memory recurrent neural net
works can be used to generate complex sequences with longrange struc
ture simply by predicting one data point at a time The approach is
demonstrated for text where the data are discrete and online handwrit
ing where the data are realvalued It is then extended to handwriting
synthesis by allowing the network to condition its predictions on a text
sequence The resulting system is able to generate highly realistic cursive
handwriting in a wide variety of styles
1 Introduction
Recurrent neural networks RNNs are a rich class of dynamic models that have
been used to generate sequences in domains as diverse as music 6 4 text 30
and motion capture data 29 RNNs can be trained for sequence generation by
processing real data sequences one step at a time and predicting what comes
next Assuming the predictions are probabilistic novel sequences can be gener
ated from a trained network by iteratively sampling from the networks output
distribution then feeding in the sample as input at the next step In other
words by making the network treat its inventions as if they were real much like
a person dreaming Although the network itself is deterministic the stochas
ticity injected by picking samples induces a distribution over sequences This
distribution is conditional since the internal state of the network and hence its
predictive distribution depends on the previous inputs
RNNs are fuzzy in the sense that they do not use exact templates from
the training data to make predictions but ratherlike other neural networks
use their internal representation to perform a highdimensional interpolation
between training examples This distinguishes them from ngram models and
compression algorithms such as Prediction by Partial Matching 5 whose pre
dictive distributions are determined by counting exact matches between the
recent history and the training set The resultwhich is immediately appar
1arXiv13080850v5  csNE  5 Jun 2014ent from the samples in this paperis that RNNs unlike templatebased al
gorithms synthesise and reconstitute the training data in a complex way and
rarely generate the same thing twice Furthermore fuzzy predictions do not suf
fer from the curse of dimensionality and are therefore much better at modelling
realvalued or multivariate data than exact matches
In principle a large enough RNN should be sucient to generate sequences
of arbitrary complexity In practice however standard RNNs are unable to
store information about past inputs for very long 15 As well as diminishing
their ability to model longrange structure this amnesia makes them prone to
instability when generating sequences The problem common to all conditional
generative models is that if the networks predictions are only based on the last
few inputs and these inputs were themselves predicted by the network it has
little opportunity to recover from past mistakes Having a longer memory has
a stabilising eect because even if the network cannot make sense of its recent
history it can look further back in the past to formulate its predictions The
problem of instability is especially acute with realvalued data where it is easy
for the predictions to stray from the manifold on which the training data lies
One remedy that has been proposed for conditional models is to inject noise into
the predictions before feeding them back into the model 31 thereby increasing
the models robustness to surprising inputs However we believe that a better
memory is a more profound and eective solution
Long Shortterm Memory LSTM 16 is an RNN architecture designed to
be better at storing and accessing information than standard RNNs LSTM has
recently given stateoftheart results in a variety of sequence processing tasks
including speech and handwriting recognition 10 12 The main goal of this
paper is to demonstrate that LSTM can use its memory to generate complex
realistic sequences containing longrange structure
Section 2 denes a deep RNN composed of stacked LSTM layers and ex
plains how it can be trained for nextstep prediction and hence sequence gener
ation Section 3 applies the prediction network to text from the Penn Treebank
and Hutter Prize Wikipedia datasets The networks performance is compet
itive with stateoftheart language models and it works almost as well when
predicting one character at a time as when predicting one word at a time The
highlight of the section is a generated sample of Wikipedia text which showcases
the networks ability to model longrange dependencies Section 4 demonstrates
how the prediction network can be applied to realvalued data through the use
of a mixture density output layer and provides experimental results on the IAM
Online Handwriting Database It also presents generated handwriting samples
proving the networks ability to learn letters and short words direct from pen
traces and to model global features of handwriting style Section 5 introduces
an extension to the prediction network that allows it to condition its outputs on
a short annotation sequence whose alignment with the predictions is unknown
This makes it suitable for handwriting synthesis where a human user inputs
a text and the algorithm generates a handwritten version of it The synthesis
network is trained on the IAM database then used to generate cursive hand
writing samples some of which cannot be distinguished from real data by the
2Figure 1 Deep recurrent neural network prediction architecture The
circles represent network layers the solid lines represent weighted connections
and the dashed lines represent predictions
naked eye A method for biasing the samples towards higher probability and
greater legibility is described along with a technique for priming the sam
ples on real data and thereby mimicking a particular writers style Finally
concluding remarks and directions for future work are given in Section 6
2 Prediction Network
Fig 1 illustrates the basic recurrent neural network prediction architecture used
in this paper An input vector sequence x x1x T is passed through
weighted connections to a stack of Nrecurrently connected hidden layers to
compute rst the hidden vector sequences hn hn
1hn
T and then the
output vector sequence y y1y T Each output vector ytis used to
parameterise a predictive distribution Pr xt1jyt over the possible next inputs
xt1 The rst element x1of every input sequence is always a null vector whose
entries are all zero the network therefore emits a prediction for x2 the rst
real input with no prior information The network is deep in both space
and time in the sense that every piece of information passing either vertically
or horizontally through the computation graph will be acted on by multiple
successive weight matrices and nonlinearities
Note the skip connections from the inputs to all hidden layers and from
all hidden layers to the outputs These make it easier to train deep networks
3by reducing the number of processing steps between the bottom of the network
and the top and thereby mitigating the vanishing gradient problem 1 In
the special case that N 1 the architecture reduces to an ordinary single layer
next step prediction RNN
The hidden layer activations are computed by iterating the following equa
tions fromt 1 toTand fromn 2 toN
h1
tH
Wih1xtWh1h1h1
t1b1
h
1
hn
tH
WihnxtWhn1hnhn1
tWhnhnhn
t1bn
h
2
where the Wterms denote weight matrices eg Wihnis the weight matrix
connecting the inputs to the nthhidden layer Wh1h1is the recurrent connection
at the rst hidden layer and so on the bterms denote bias vectors eg byis
output bias vector and His the hidden layer function
Given the hidden sequences the output sequence is computed as follows
ytbyNX
n1Whnyhn
t 3
ytYyt 4
whereYis the output layer function The complete network therefore denes
a function parameterised by the weight matrices from input histories x1tto
output vectors yt
The output vectors ytare used to parameterise the predictive distribution
Prxt1jyt for the next input The form of Pr xt1jyt must be chosen carefully
to match the input data In particular nding a good predictive distribution
for highdimensional realvalued data usually referred to as density modelling 
can be very challenging
The probability given by the network to the input sequence xis
Prx TY
t1Prxt1jyt 5
and the sequence loss Lx used to train the network is the negative logarithm
of Pr x
Lx TX
t1log Prxt1jyt 6
The partial derivatives of the loss with respect to the network weights can be
eciently calculated with backpropagation through time 33 applied to the
computation graph shown in Fig 1 and the network can then be trained with
gradient descent
21 Long ShortTerm Memory
In most RNNs the hidden layer function His an elementwise application of a
sigmoid function However we have found that the Long ShortTerm Memory
4Figure 2 Long Shortterm Memory Cell
LSTM architecture 16 which uses purposebuilt memory cells to store infor
mation is better at nding and exploiting long range dependencies in the data
Fig 2 illustrates a single LSTM memory cell For the version of LSTM used in
this paper 7His implemented by the following composite function
itWxixtWhiht1Wcict1bi 7
ftWxfxtWhfht1Wcfct1bf 8
ctftct1ittanh WxcxtWhcht1bc 9
otWxoxtWhoht1Wcoctbo 10
htottanhct 11
whereis the logistic sigmoid function and ifoandcare respectively the
input gate forget gate output gate celland cell input activation vectors all of
which are the same size as the hidden vector h The weight matrix subscripts
have the obvious meaning for example Whiis the hiddeninput gate matrix
Wxois the inputoutput gate matrix etc The weight matrices from the cell
to gate vectors eg Wci are diagonal so element min each gate vector only
receives input from element mof the cell vector The bias terms which are
added toifcando have been omitted for clarity
The original LSTM algorithm used a custom designed approximate gradi
ent calculation that allowed the weights to be updated after every timestep 16
However the full gradient can instead be calculated with backpropagation through
time 11 the method used in this paper One diculty when training LSTM
with the full gradient is that the derivatives sometimes become excessively large
5leading to numerical problems To prevent this all the experiments in this pa
per clipped the derivative of the loss with respect to the network inputs to the
LSTM layers before the sigmoid and tanh functions are applied to lie within
a predened range1
3 Text Prediction
Text data is discrete and is typically presented to neural networks using one
hot input vectors That is if there are Ktext classes in total and class kis fed
in at timet thenxtis a length Kvector whose entries are all zero except for
thekth which is one Pr xt1jyt is therefore a multinomial distribution which
can be naturally parameterised by a softmax function at the output layer
Prxt1kjyt yk
texp
yk
t
PK
k01exp
yk0
t 12
Substituting into Eq 6 we see that
Lx TX
t1logyxt1
t 13
Lx
yk
tyk
tkxt1 14
The only thing that remains to be decided is which set of classes to use In
most cases text prediction usually referred to as language modelling  is per
formed at the word level Kis therefore the number of words in the dictionary
This can be problematic for realistic tasks where the number of words in
cluding variant conjugations proper names etc often exceeds 100000 As
well as requiring many parameters to model having so many classes demands a
huge amount of training data to adequately cover the possible contexts for the
words In the case of softmax models a further diculty is the high computa
tional cost of evaluating all the exponentials during training although several
methods have been to devised make training large softmax layers more ecient
including treebased models 25 23 low rank approximations 27 and stochas
tic derivatives 26 Furthermore wordlevel models are not applicable to text
data containing nonword strings such as multidigit numbers or web addresses
Characterlevel language modelling with neural networks has recently been
considered 30 24 and found to give slightly worse performance than equiv
alent wordlevel models Nonetheless predicting one character at a time is
more interesting from the perspective of sequence generation because it allows
the network to invent novel words and strings In general the experiments in
this paper aim to predict at the nest granularity found in the data so as to
maximise the generative exibility of the network
1In fact this technique was used in all my previous papers on LSTM and in my publicly
available LSTM code but I forgot to mention it anywhere mea culpa 
631 Penn Treebank Experiments
The rst set of text prediction experiments focused on the Penn Treebank por
tion of the Wall Street Journal corpus 22 This was a preliminary study whose
main purpose was to gauge the predictive power of the network rather than to
generate interesting sequences
Although a relatively small text corpus a little over a million words in total
the Penn Treebank data is widely used as a language modelling benchmark The
training set contains 930000 words the validation set contains 74000 words and
the test set contains 82000 words The vocabulary is limited to 10000 words
with all other words mapped to a special unknown word token The endof
sentence token was included in the input sequences and was counted in the
sequence loss The startofsentence marker was ignored because its role is
already fullled by the null vectors that begin the sequences cf Section 2
The experiments compared the performance of word and characterlevel
LSTM predictors on the Penn corpus In both cases the network architecture
was a single hidden layer with 1000 LSTM units For the characterlevel network
the input and output layers were size 49 giving approximately 43M weights in
total while the wordlevel network had 10000 inputs and outputs and around
54M weights The comparison is therefore somewhat unfair as the wordlevel
network had many more parameters However as the dataset is small both net
works were easily able to overt the training data and it is not clear whether the
characterlevel network would have beneted from more weights All networks
were trained with stochastic gradient descent using a learn rate of 00001 and a
momentum of 099 The LSTM derivates were clipped in the range  11 cf
Section 21
Neural networks are usually evaluated on test data with xed weights For
prediction problems however where the inputs arethe targets it is legitimate
to allow the network to adapt its weights as it is being evaluated so long as
it only sees the test data once Mikolov refers to this as dynamic evaluation 
Dynamic evaluation allows for a fairer comparison with compression algorithms
for which there is no division between training and test sets as all data is only
predicted once
Since both networks overt the training data we also experiment with two
types of regularisation weight noise 18 with a std deviation of 0075 applied
to the network weights at the start of each training sequence and adaptive
weight noise 8 where the variance of the noise is learned along with the weights
using a Minimum description Length or equivalently variational inference loss
function When weight noise was used the network was initialised with the
nal weights of the unregularised network Similarly when adaptive weight
noise was used the weights were initialised with those of the network trained
with weight noise We have found that retraining with iteratively increased
regularisation is considerably faster than training from random weights with
regularisation Adaptive weight noise was found to be prohibitively slow for
the wordlevel network so it was regularised with xedvariance weight noise
only One advantage of adaptive weight is that early stopping is not needed
7Table 1 Penn Treebank Test Set Results BPC is bitspercharacter
Error is nextstep classication error rate for either characters or words
Input Regularisation Dynamic BPC Perplexity Error  Epochs
Char none no 132 167 285 9
char none yes 129 148 280 9
char weight noise no 127 140 274 25
char weight noise yes 124 124 269 25
char adapt wt noise no 126 133 274 26
char adapt wt noise yes 124 122 269 26
word none no 127 138 778 11
word none yes 125 126 769 11
word weight noise no 125 126 769 14
word weight noise yes 123 117 762 14
the network can safely be stopped at the point of minimum total description
length on the training data However to keep the comparison fair the same
training validation and test sets were used for all experiments
The results are presented with two equivalent metrics bitspercharacter
BPC which is the average value of log2Prxt1jyt over the whole test set
andperplexity which is two to the power of the average number of bits per word
the average word length on the test set is about 56 characters so perplexity 
256BPC Perplexity is the usual performance measure for language modelling
Table 1 shows that the wordlevel RNN performed better than the character
level network but the gap appeared to close when regularisation is used Overall
the results compare favourably with those collected in Tomas Mikolovs the
sis 23 For example he records a perplexity of 141 for a 5gram with Keyser
Ney smoothing 1418 for a word level feedforward neural network 1311 for the
stateoftheart compression algorithm PAQ8 and 1232 for a dynamically eval
uated wordlevel RNN However by combining multiple RNNs a 5gram and a
cache model in an ensemble he was able to achieve a perplexity of 894 Inter
estingly the benet of dynamic evaluation was far more pronounced here than
in Mikolovs thesis he records a perplexity improvement from 1247 to 1232
with wordlevel RNNs This suggests that LSTM is better at rapidly adapting
to new data than ordinary RNNs
32 Wikipedia Experiments
In 2006 Marcus Hutter Jim Bowery and Matt Mahoney organised the following
challenge commonly known as Hutter prize 17 to compress the rst 100
million bytes of the complete English Wikipedia data as it was at a certain
time on March 3rd 2006 to as small a le as possible The le had to include
not only the compressed data but also the code implementing the compression
algorithm Its size can therefore be considered a measure of the minimum
description length 13 of the data using a two part coding scheme
Wikipedia data is interesting from a sequence generation perspective because
8it contains not only a huge range of dictionary words but also many character
sequences that would not be included in text corpora traditionally used for
language modelling For example foreign words including letters from non
Latin alphabets such as Arabic and Chinese indented XML tags used to dene
metadata website addresses and markup used to indicate page formatting such
as headings bullet points etc An extract from the Hutter prize dataset is shown
in Figs 3 and 4
The rst 96M bytes in the data were evenly split into sequences of 100 bytes
and used to train the network with the remaining 4M were used for validation
The data contains a total of 205 onebyte unicode symbols The total number
ofcharacters is much higher since many characters especially those from non
Latin languages are dened as multisymbol sequences In keeping with the
principle of modelling the smallest meaningful units in the data the network
predicted a single byte at a time and therefore had size 205 input and output
layers
Wikipedia contains longrange regularities such as the topic of an article
which can span many thousand words To make it possible for the network to
capture these its internal state that is the output activations htof the hidden
layers and the activations ctof the LSTM cells within the layers were only reset
every 100 sequences Furthermore the order of the sequences was not shued
during training as it usually is for neural networks The network was therefore
able to access information from up to 10K characters in the past when making
predictions The error terms were only backpropagated to the start of each 100
byte sequence meaning that the gradient calculation was approximate This
form of truncated backpropagation has been considered before for RNN lan
guage modelling 23 and found to speed up training by reducing the sequence
length and hence increasing the frequency of stochastic weight updates without
aecting the networks ability to learn longrange dependencies
A much larger network was used for this data than the Penn data reecting
the greater size and complexity of the training set with seven hidden layers of
700 LSTM cells giving approximately 213M weights The network was trained
with stochastic gradient descent using a learn rate of 00001 and a momentum
of 09 It took four training epochs to converge The LSTM derivates were
clipped in the range  11
As with the Penn data we tested the network on the validation data with
and without dynamic evaluation where the weights are updated as the data
is predicted As can be seen from Table 2 performance was much better with
dynamic evaluation This is probably because of the long range coherence of
Wikipedia data for example certain words are much more frequent in some
articles than others and being able to adapt to this during evaluation is ad
vantageous It may seem surprising that the dynamic results on the validation
set were substantially better than on the training set However this is easily
explained by two factors rstly the network undert the training data and
secondly some portions of the data are much more dicult than others for
example plain text is harder to predict than XML tags
To put the results in context the current winner of the Hutter Prize a
9Table 2 Wikipedia Results bitspercharacter
Train Validation static Validation dynamic
142 167 133
variant of the PAQ8 compression algorithm 20 achieves 128 BPC on the same
data including the code required to implement the algorithm mainstream
compressors such as zip generally get more than 2 and a character level RNN
applied to a textonly version of the data ie with all the XML markup tags
etc removed achieved 154 on heldout data which improved to 147 when the
RNN was combined with a maximum entropy model 24
A four page sample generated by the prediction network is shown in Figs 5
to 8 The sample shows that the network has learned a lot of structure from
the data at a wide range of dierent scales Most obviously it has learned a
large vocabulary of dictionary words along with a subword model that enables
it to invent feasiblelooking words and names for example Lochroom River
Mughal Ralvaldens submandration swalloped It has also learned basic
punctuation with commas full stops and paragraph breaks occurring at roughly
the right rhythm in the text blocks
Being able to correctly open and close quotation marks and parentheses is
a clear indicator of a language models memory because the closure cannot be
predicted from the intervening text and hence cannot be modelled with short
range context 30 The sample shows that the network is able to balance not
only parentheses and quotes but also formatting marks such as the equals signs
used to denote headings and even nested XML tags and indentation
The network generates nonLatin characters such as Cyrillic Chinese and
Arabic and seems to have learned a rudimentary model for languages other
than English eg it generates esGeotnia slago for the Spanish version of an
article and nlRodenbaueri for the Dutch one It also generates convincing
looking internet addresses none of which appear to be real
The network generates distinct largescale regions such as XML headers
bulletpoint lists and article text Comparison with Figs 3 and 4 suggests that
these regions are a fairly accurate reection of the constitution of the real data
although the generated versions tend to be somewhat shorter and more jumbled
together This is signicant because each region may span hundreds or even
thousands of timesteps The fact that the network is able to remain coherent
over such large intervals even putting the regions in an approximately correct
order such as having headers at the start of articles and bulletpointed see also
lists at the end is testament to its longrange memory
As with all text generated by language models the sample does not make
sense beyond the level of short phrases The realism could perhaps be improved
with a larger network andor more data However it seems futile to expect
meaningful language from a machine that has never been exposed to the sensory
10world to which language refers
Lastly the networks adaptation to recent sequences during training which
allows it to benet from dynamic evaluation can be clearly observed in the
extract The last complete article before the end of the training set at which
point the weights were stored was on intercontinental ballistic missiles The
inuence of this article on the networks language model can be seen from the
profusion of missilerelated terms Other recent topics include Individual An
archism the Italian writer Italo Calvino and the International Organization
for Standardization ISO all of which make themselves felt in the networks
vocabulary
11    titleAlbaniaEconomytitle                                                   id36id                                                                     revision                                                                        id15898966id                                                               timestamp20021009T133900Ztimestamp                                     contributor                                                                     usernameMagnus Manskeusername                                              id4id                                                                    contributor                                                                  minor                                                                        commentREDIRECT Economy of Albaniacomment                             text xmlspacepreserveREDIRECT Economy of Albaniatext            revision                                                                   page                                                                         page                                                                            titleAlchemYtitle                                                          id38id                                                                     revision                                                                        id15898967id                                                               timestamp20020225T154311Ztimestamp                                     contributor                                                                     ipConversion scriptip                                                    contributor                                                                  minor                                                                        commentAutomated conversioncomment                                         text xmlspacepreserveREDIRECT Alchemy                          text                                                                             revision                                                                   page                                                                         page                                                                            titleAlbedotitle                                                           id39id                                                                     revision                                                                        id41496222id                                                               timestamp20060227T193246Ztimestamp                                     contributor                                                                     ip24119344ip                                                          contributor                                                                  text xmlspacepreserveotheruses                                                                                                                  Albedo is the measure of reflectivity of a surface or body It is the ratio of electromagnetic radiation EM radiation reflected to the amount incident upon it The fraction usually expressed as a percentage from 0 to 100 is an important concept in climatology and astronomy This ratio depends on the frequency of the radiation considered unqualified it refers to an average across the spectrum of visible light It also depends on the angle of incidence of the radiation unqualified normal incidence Fresh snow albedos are high up to 90 The ocean surface has a low albedo  The average albedo of Earth is about 30 whereas the albedo of the Moon is about 7 In astronomy the albedo of satellites and asteroids can be used to infer surface composition most notably ice content    Enceladus_moonEnceladus a moon of Saturn has the highest known albedo of any body in the solar system with 99 of EM radiation reflected                                                                                                                                     Human activities have changed the albedo via forest clearance and farming for example of various areas around the globe However quantification of this effect is difficult on the global scale it is not clear whether the changes have tended to increase or decrease global warming                                                                                                                The quotclassicalquot example of albedo effect is the snowtemperature feedback If a snow covered area warms and the snow melts the albedo decreases more sunlight is absorbed and the temperature tends to increase The converse is trFigure 3 Real Wikipedia data
12ue if snow forms a cooling cycle happens The intensity of the albedo effect depends on the size of the change in albedo and the amount of insolation for this reason it can be potentially very large in the tropics                                                                                                    Some examples of albedo effects                                                                                                                             Fairbanks Alaska                                                                                                                                        According to the National Climatic Data Centers GHCN 2 data which is composed of 30year smoothed climatic means for thousands of weather stations across the world the college weather station at Fairbanks Alaska is about 3 C 5 F warmer than the airport at Fairbanks partly because of drainage patterns but also largely because of the lower albedo at the college resulting from a higher concentration of pine trees and therefore less open snowy ground to reflect the heat back into space Neunke and Kukla have shown that this difference is especially marked during the late winter months when solar radiation is greater                                                                                                                                              The tropics                                                                                                                                              Although the albedotemperature effect is most famous in colder regions of Earth because more snow falls there it is actually much stronger in tropical regions because in the tropics there is consistently more sunlight When Brazilian ranchers cut down dark tropical rainforest trees to replace them with even darker soil in order to grow crops the average temperature of the area appears to increase by an average of about 3 C 5 F yearround which is a significant amount                                                                                                                                                   Small scale effects                                                                                                                                      Albedo works on a smaller scale too People who wear dark clothes in the summertime put themselves at a greater risk of heatstroke than those who wear white clothes                                                                                                                                                       Pine forests                                                                                                                                             The albedo of a pine forest at 45N in the winter in which the trees cover the land surface completely is only about 9 among the lowest of any naturally occurring land environment This is partly due to the color of the pines and partly due to multiple scattering of sunlight within the trees which lowers the overall reflected light level Due to light penetration the oceans albedo is even lower at about 35 though this depends strongly on the angle of the incident radiation Dense swampland averages between 9 and 14 Deciduous trees average about 13 A grassy field usually comes in at about 20 A barren field will depend on the color of the soil and can be as low as 5 or as high as 40 with 15 being about the average for farmland A desert or large beach usually averages around 25 but varies depending on the color of the sand Reference Edward Walkers study in the Great Plains in the winter around 45N                                                                                 Urban areas                                                                                                                                              Urban areas in particular have very unnatural values for albedo because of the many humanbuilt structures which absorb light before the light can reach the surface In the northern part of the world cities are relatively dark and Walker has shown that their average albedo is about 7 with only a slight increase during the summer In most tropical countries cities average around 12 This is similar to the values found in northern suburban transitional zones Part of the reason for this is the different natural environment of cities in tropical regions eg there are more very dark trees around another reason is that portions of the tropics are very poor and city buildings must be built with different materials Warmer regions may also choose lighter colored building materials so the structures will remain cooler                                               Figure 4 Real Wikipedia data cotd
13    revision                                                                        id40973199id                                                               timestamp20060222T223716Ztimestamp                                     contributor                                                                     ip6386196111ip                                                        contributor                                                                  minor                                                                        commentredire paget gt captain comment                               text xmlspacepreserveThe Indigence History refers to the authority of any obscure albionism as being such as in Aram Missolmushttpwwwbbccoukstarcecr52htm                                                       In 1995 SitzRoad Straus up the inspirational radiotes portion as quotalliancequotsingle quotglapingquot theme charcoal with Midwestern United StateDenmark in which Canary variesdestruction to launching casualties has quickly responded to the krush loaded water or so it might be destroyed Aldeads still cause a missile bedged harbors at last built in 19112 and save the accuracy in 2008 retaking itsubmanism Its individuals were                      hnown rapidly in their return to the private equity such as On Text for death per reprised by the Grange of GermanyGerman unbridged work                                                                                            The Rebellion Hyerodent is literal related mildly older than old half sister the music and morrow been much more propellent All those of Hamas masssausage traffickings were also known as Trip class submarineSante at Serassim Verra as 1865ampndash682ampndash831 is related to ballistic missiles While she viewed it friend of Halla equatorial weapons of Tuscany in France from vaccine homes to quotindividualquot among slaveryslaves such as artistual selling of factories were renamed English habit of twelve years                                                                                                                                             By the 1978 Russian TurkeyTurkist capital city ceased by farmers and the intention of navigation the ISBNs all encoding Transylvania International Organisation for Transition BankingAttiking others it is in the westernmost placed lines  This type of missile calculation maintains all greater proof was the 1990s as older adventures that never established a selfinterested case The newcomers were Prosecutors in child after the other weekend and capable function used                                                                                                                                                           Holding may be typically largely banned severish from sforked warhing tools and behave laws allowing the private jokes even through missile IIC control most notably each but no relatively larger success is not being reprinted and withdrawn into fortyordered cast and distribution                                                                                                                  Besides these markets notably a son of humor                                                                                                                 Sometimes more or only lowed quot80quot to force a suit for httpnewsbbccouk1sid9kcidweb9960219html 108214                            ltblockquotegt                                                                                                                                              The various disputes between Basic Mass and Council Conditioners  quotTitanistquot class streams and anarchism                                                                                                                       Internet traditions sprang east with Southern neighborhood systems are improved with Moatbreakers bold hot missiles its labor systems KCD numbered former ISBNMASspeaker attacks quotM3 5quot which are saved as the ballistic misely known and most functional factories  Establishment begins for some range of start rail years as dealing with 161 or 18950 million USD2 and covert all carbonate functions for example 7093 higher individuals and on missiles This might need not know against sexual video capita playing pointing degrees between silocalfed greater valous consumptions in the US header can be seen in collectivist                                                                                                                                 See also                                                                   Figure 5 Generated Wikipedia data
14                                                                                BritishLondon Bridge                                                      AntiTalmot TouchTucker novice                                            List of cambridge capital                                                  Elon Haven                                                                 USS Otaro Screamed Its                                                 Detroit Library                                                            Belgium Sea                                                                Tularan BellTurnbiller Squobil                                            Suntanal vocalistProsopyo                                                 Winkenpea                                                                  Milenton Streat                                                            Raiebin                                                                    Est Altar Macinton                                                         Military mass missileS3                                                   Organization of the Asian American state districtumbali landmarks        ISO                                                                        NFL                                                                        American AntiCapitalismMajor independent ITUUS singles                  London roleplaying gamePreRomanian Civil War                          YokukhavNaUnMurantano Kaufmann  SijoneGrafittsforbiel                 Neao trolleyne and deadweight drug                                         B45 BQB9  de red take painting is deployed larger than quanta submarine Susconfiction of advocate                                                  List of major swandarms                                                    CategoryItalo sales towns entertained by the ICBMs of SkinnerKnighting 707 killed by capital                                                                                                                                            Midple planetParishment of the value                                 Image2000JPGrightthumbIt tunneled nuclease at this bass AH OlampSāwflgin hhlgbying yoostallo eruptuals with low immigrantsshelted atkins and their atapping bugs                                                                                                                                          See also Iranian indigenous Flight Intercontinental Organization                                                                                           Pioneers                                                                                                                                                    Tended to be the results characteristic of warehoused labour share to control all these in the rational framing                                                                                                                                Gentiles                                                                    placeorline                                                               Footer names derive the topic class gt which he liked to deal without any of the parties Iquot by Alfred Hustin and Frank Henry and manufacturerhttpanciermscnituk IATB perspective was expected to be classified by the Straight Road of Buckning in 2003 Summer Olympic Touchbottom all minute                                                                                                                                                              Performance                                                                 ImageIare 300jpgleftthumb325pxIntercontinental file shortly after referring to his landmaster Sidney Goodwordd                                                                                                                      Italo                                                                          Chicago ballistic parksnonmonth in eastern Italy is a ItaloChina parent communist annual production began in May 1915                                                                                                         An ICBM the gurt and land has registered 155 billion in US and August 1688 and makes sure the UStransplantation disbanded backwards in the County by authorizing disputes that tend to carry over this peninsula                      Current malasses 25 decks and counterpoint culture that were impure between  systems                                                                          L14  194  100 000 kms                                                      Figure 6 Generated Wikipedia data cotd
15                                                                                httpwwwsibeoorgnetspacestainteologyhtm The design of thinker Wikipedia serverrouted website                                                          httpbalearburnededubooks1978taptrpiocscafac249cahtml Nationstate Player on the 13                                                                                                                                            Linux                                                                                                                                                       CategoryItalo Succeeded bystablesGeneral                                  CategoryItaloSaxon singers                                                CategoryAviation                                                                                                                                           deItalo                                                                    esGeotnia slago                                                            ja㤥㗁㜪ҍͷᔫ                                                                 koഇ㙆㮙 ਬ                                                                 nlRodenbaueri                                                              plMain Ages                                                                ptIbanez Heights                                                           ruМлкракянѳелолכуциянсьния агморелиа                                       simpleInternational Maritime Commission                                    skICBM                                                                     suOl aal Mitnet                                                            fiItalo Colombonasto Warkō                                                 svOpin den mailclimbere panczeur                                          zh	text                                                               revision                                                                   page                                                                         page                                                                            titleItaloII articletitle                                                 id14848id                                                                  revision                                                                        id15912447id                                                               timestamp20041119T194739Ztimestamp                                     contributor                                                                     usernameEl andresusername                                                  id509991id                                                               contributor                                                                  minor                                                                        commentdisambiguation from List of ISBN newsletters see ISO sitecomment                                                                           text xmlspacepreserveREDIRECT Quartic war                      text                                                                             revision                                                                   page                                                                         page                                                                            titleICSMtitle                                                             id14939id                                                                  revision                                                                        id42109942id                                                               timestamp20060228T172202Ztimestamp                                     contributor                                                                     usernameDtelclanusername                                                   id26id                                                                   contributor                                                                  minor                                                                        comment Possible catheterman comment                                   text xmlspacepreserveImageIsaacorgicehtml Independent national stage developmentShatting and Catalogue standardering in the IRBMs                                                                                       Up2000 they called the SC 4220 system he was swalloped early in Calvino or since each trial mentioned based on Balbovs new singlejargetbitoriann guessFigure 7 Generated Wikipedia data cotd
16 selfacharged versions Mt Costall Leyton was the two largest calashia at destored universities all fleeted with the customary calfed clipper                                                                                         His way to take in this literature called ICBMsAN a Softvalue speed Astronomical Classification Railway                                                                                                                              LACN645 Snowshore val nominated  made missile submandrationcontinental missiles steam musicians not of each club having on the ball and procedure at the last century                                                                                                                                                  Another communistic stark quotI submarinequot is buildingcorruptable a della missile missile than the Royal Society Society 12258 quotGlide sun wag lubrician They stay numerous capitalists and gas masks more widely interested This scheme has declarations before the certain emerging factories compelled by labour allowed to produce                                                                                                                    In the United States there is no hard resort in computation significantly                                                                                     In 1868 the Italo Capital Territories Unit started to the Continental Railway Centre  was called UC or two of his usage before being written by other students against the electiveballistic missiles deployment Steam is still quot20 to Nachtquot and Fia Citation Quantity Logos since 1967 They pass a Brigade managementQuarrystated missile system resolution taunting out of about 175 million Lochroom RiverTri                                                                                                            Alien from 1985 to 1999 it was an English and Network struggling basedal with the Lombardo capital in Silvio and Murray and heavily built in subparties address to 11188 Their forces gained prisoners to stalked a last missile mobili site                                                                                                                                                            Spanning civilization is quanting Software Societys ballistic missile  The same as antiintellectual anthropology continued in Southern Italy in 1914 and the French Confederation of Parliaments rapid favourable rise that began settled in March 20041983ampnbsp49                                                                                                                      In 1904 the Court began a British backed into a SR1 missile of trial ship in the Municipal Eightime CalendarAsiatic regime including Benjamin Tudor TurnerArthur Ravis and Abrahams LiberationCanton Olombus There was still land factory most turned up before lacking closers to the sitting shed backwards in primary science                                                                                                                              Weights and resolutions                                                     ImageSpanish 300 Protectionald landballi110svgsmall capital surface computer                                                                             ImageClaudiusjpg345pxrightOlympiad concert of Calvino and Eastern Calvino Mughal Ralvaldens above at the beginning strike the substrated roles of rich intellectual property visualizing the entire system but this missiles suggest that accounting differs between a giving train sleepwithdrawn or the dinosaur in and aucting                                                                                                                                    Internationally                                                           mainUnmanned Justice Address                                                                                                                               The ICBM created a the significant land railway called quotMGallipottequot and it needed stopped benzafkMacdonalical Sciences                                                                                               Electros appeared to be the Soviet Unions quotfirstquot vehicle from 2500 selling officials DORLAN STM331  by missilence illustrations with quotRajquot the Tunnel Hall of America an entity upon IL pages so missiles must try with a trademark must develop the land allowing traffic mass to a very few minutemen The missiles market is slow much easier is represented by GMMAz of BSM Software the utility of scaleout scale pime racks are normally crumbled aboutFigure 8 Generated Wikipedia data cotd
174 Handwriting Prediction
To test whether the prediction network could also be used to generate convincing
realvalued sequences we applied it to online handwriting data  online in this
context means that the writing is recorded as a sequence of pentip locations
as opposed to oine handwriting where only the page images are available
Online handwriting is an attractive choice for sequence generation due to its
low dimensionality two real numbers per data point and ease of visualisation
All the data used for this paper were taken from the IAM online handwriting
database IAMOnDB 21 IAMOnDB consists of handwritten lines collected
from 221 dierent writers using a smart whiteboard The writers were asked to
write forms from the LancasterOsloBergen text corpus 19 and the position
of their pen was tracked using an infrared device in the corner of the board
Samples from the training data are shown in Fig 9 The original input data
consists of the xandypen coordinates and the points in the sequence when
the pen is lifted o the whiteboard Recording errors in the xydata was
corrected by interpolating to ll in for missing readings and removing steps
whose length exceeded a certain threshold Beyond that no preprocessing was
used and the network was trained to predict the xycoordinates and the end
ofstroke markers one point at a time This contrasts with most approaches to
handwriting recognition and synthesis which rely on sophisticated preprocessing
and featureextraction techniques We eschewed such techniques because they
tend to reduce the variation in the data eg by normalising the character size
slant skew and soon which we wanted the network to model Predicting the
pen traces one point at a time gives the network maximum exibility to invent
novel handwriting but also requires a lot of memory with the average letter
occupying more than 25 timesteps and the average line occupying around 700
Predicting delayed strokes such as dots for is or crosses for ts that are added
after the rest of the word has been written is especially demanding
IAMOnDB is divided into a training set two validation sets and a test
set containing respectively 5364 1438 1518 and 3859 handwritten lines taken
from 775 192 216 and 544 forms For our experiments each line was treated
as a separate sequence meaning that possible dependencies between successive
lines were ignored In order to maximise the amount of training data we used
the training set test set and the larger of the validation sets for training and
the smaller validation set for earlystopping The lack of independent test set
means that the recorded results may be somewhat overt on the validation set
however the validation results are of secondary importance since no benchmark
results exist and the main goal was to generate convincinglooking handwriting
The principal challenge in applying the prediction network to online hand
writing data was determining a predictive distribution suitable for realvalued
inputs The following section describes how this was done
18Figure 9 Training samples from the IAM online handwriting database
Notice the wide range of writing styles the variation in line angle and character
sizes and the writing and recording errors such as the scribbled out letters in
the rst line and the repeated word in the nal line
41 Mixture Density Outputs
The idea of mixture density networks 2 3 is to use the outputs of a neural
network to parameterise a mixture distribution A subset of the outputs are
used to dene the mixture weights while the remaining outputs are used to
parameterise the individual mixture components The mixture weight outputs
are normalised with a softmax function to ensure they form a valid discrete dis
tribution and the other outputs are passed through suitable functions to keep
their values within meaningful range for example the exponential function is
typically applied to outputs used as scale parameters which must be positive
Mixture density network are trained by maximising the log probability den
sity of the targets under the induced distributions Note that the densities are
normalised up to a xed constant and are therefore straightforward to dier
entiate and pick unbiased sample from in contrast with restricted Boltzmann
machines 14 and other undirected models
Mixture density outputs can also be used with recurrent neural networks 28
In this case the output distribution is conditioned not only on the current input
but on the history of previous inputs Intuitively the number of components is
the number of choices the network has for the next output given the inputs so
far
For the handwriting experiments in this paper the basic RNN architecture
and update equations remain unchanged from Section 2 Each input vector xt
consists of a realvalued pair x1x2that denes the pen oset from the previous
19input along with a binary x3that has value 1 if the vector ends a stroke that
is if the pen was lifted o the board before the next vector was recorded and
value 0 otherwise A mixture of bivariate Gaussians was used to predict x1
andx2 while a Bernoulli distribution was used for x3 Each output vector yt
therefore consists of the end of stroke probability e along with a set of means
j standard deviations j correlations jand mixture weights jfor theM
mixture components That is
xt2RRf01g 15
yt
etfj
tj
tj
tj
tgM
j1
16
Note that the mean and standard deviation are two dimensional vectors whereas
the component weight correlation and endofstroke probability are scalar The
vectorsytare obtained from the network outputs  yt where
yt
etfwj
tj
tj
tj
tgM
j1
byNX
n1Whnyhn
t 17
as follows
et1
1  exp etet201 18
j
texp
j
t
PM
j01exp
j0
t j
t201X
jj
t 1 19
j
t j
t j
t2R 20
j
t exp
j
t
j
t0 21
j
ttanhj
t  j
t211 22
The probability density Pr xt1jyt of the next input xt1given the output
vectorytis dened as follows
Prxt1jyt MX
j1j
tNxt1jj
tj
tj
t
et if xt13 1
1etotherwise23
where
Nxj  1
212p
12expZ
212
24
with
Zx112
2
1x222
2
22x11x22
1225
20This can be substituted into Eq 6 to determine the sequence loss up to
a constant that depends only on the quantisation of the data and does not
inuence network training
Lx TX
t1log0
X
jj
tNxt1jj
tj
tj
t1
A
loget if xt13 1
log1et otherwise
26
The derivative of the loss with respect to the endofstroke outputs is straight
forward
Lx
et xt13et 27
The derivatives with respect to the mixture density outputs can be found by
rst dening the component responsibilities j
t
j
tj
tNxt1jj
tj
tj
t 28
j
tj
tPM
j01j0
t29
Then observing that
Lx
j
tj
tj
t 30
Lx
j
tj
tj
tj
tlogNxt1jj
tj
tj
t
j
tj
tj
t31
where
logNxj 
1C
1x11
1x22
2
32
logNxj 
2C
2x22
2x11
1
33
logNxj 
1Cx11
1x11
1x22
2
1 34
logNxj 
2Cx22
2x22
2x11
1
1 35
logNxj 
x11x22
121CZ 36
withZdened as in Eq 25 and
C1
1237
Fig 10 illustrates the operation of a mixture density output layer applied to
online handwriting prediction
21Output Density
Figure 10 Mixture density outputs for handwriting prediction The
top heatmap shows the sequence of probability distributions for the predicted
pen locations as the word under is written The densities for successive
predictions are added together giving high values where the distributions
overlap
Two types of prediction are visible from the density map the small
blobs that spell out the letters are the predictions as the strokes are being
written the three large blobs are the predictions at the ends of the strokes for
the rst point in the next stroke The endofstroke predictions have much
higher variance because the pen position was not recorded when it was o the
whiteboard and hence there may be a large distance between the end of one
stroke and the start of the next
The bottom heatmap shows the mixture component weights during the
same sequence The stroke ends are also visible here with the most active
components switching o in three places and other components switching on
evidently endofstroke predictions use a dierent set of mixture components
from instroke predictions
2242 Experiments
Each point in the data sequences consisted of three numbers the xandyoset
from the previous point and the binary endofstroke feature The network
input layer was therefore size 3 The coordinate osets were normalised to
mean 0 std dev 1 over the training set 20 mixture components were used
to model the osets giving a total of 120 mixture parameters per timestep
20 weights 40 means 40 standard deviations and 20 correlations A further
parameter was used to model the endofstroke probability giving an output
layer of size 121 Two network architectures were compared for the hidden
layers one with three hidden layers each consisting of 400 LSTM cells and one
with a single hidden layer of 900 LSTM cells Both networks had around 34M
weights The three layer network was retrained with adaptive weight noise 8
with all std devs initialised to 0075 Training with xed variance weight noise
proved ineective probably because it prevented the mixture density layer from
using precisely specied weights
The networks were trained with rmsprop  a form of stochastic gradient de
scent where the gradients are divided by a running average of their recent mag
nitude 32 Dene iLx
wiwherewiis network weight i The weight update
equations were
nini 12
i 38
gigi 1i 39
iiijip
nig2
ik40
wiwi  i 41
with the following parameters
 095 42
i 09 43
j 00001 44
k 00001 45
The output derivativesLx
ytwere clipped in the range  100100 and the
LSTM derivates were clipped in the range  1010 Clipping the output gradi
ents proved vital for numerical stability even so the networks sometimes had
numerical problems late on in training after they had started overtting on the
training data
Table 3 shows that the three layer network had an average persequence loss
153 nats lower than the one layer net However the sumsquarederror was
slightly lower for the single layer network the use of adaptive weight noise
reduced the loss by another 167 nats relative to the unregularised three layer
network but did not signicantly change the sumsquared error The adaptive
weight noise network appeared to generate the best samples
23Table 3 Handwriting Prediction Results All results recorded on the val
idation set LogLoss is the mean value of Lx in nats SSE is the mean
sumsquarederror per data point
Network Regularisation LogLoss SSE
1 layer none 10257 040
3 layer none 10410 041
3 layer adaptive weight noise 10577 041
43 Samples
Fig 11 shows handwriting samples generated by the prediction network The
network has clearly learned to model strokes letters and even short words es
pecially common ones such as of and the It also appears to have learned a
basic character level language models since the words it invents eald bryoes
lenrest look somewhat plausible in English Given that the average character
occupies more than 25 timesteps this again demonstrates the networks ability
to generate coherent longrange structures
5 Handwriting Synthesis
Handwriting synthesis is the generation of handwriting for a given text Clearly
the prediction networks we have described so far are unable to do this since
there is no way to constrain which letters the network writes This section de
scribes an augmentation that allows a prediction network to generate data se
quences conditioned on some highlevel annotation sequence a character string
in the case of handwriting synthesis The resulting sequences are suciently
convincing that they often cannot be distinguished from real handwriting Fur
thermore this realism is achieved without sacricing the diversity in writing
style demonstrated in the previous section
The main challenge in conditioning the predictions on the text is that the two
sequences are of very dierent lengths the pen trace being on average twenty
ve times as long as the text and the alignment between them is unknown until
the data is generated This is because the number of coordinates used to write
each character varies greatly according to style size pen speed etc One neural
network model able to make sequential predictions based on two sequences of
dierent length and unknown alignment is the RNN transducer 9 However
preliminary experiments on handwriting synthesis with RNN transducers were
not encouraging A possible explanation is that the transducer uses two sepa
rate RNNs to process the two sequences then combines their outputs to make
decisions when it is usually more desirable to make all the information avail
able to single network This work proposes an alternative model where a soft
window is convolved with the text string and fed in as an extra input to the
prediction network The parameters of the window are output by the network
24Figure 11 Online handwriting samples generated by the prediction
network All samples are 700 timesteps long
25at the same time as it makes the predictions so that it dynamically determines
an alignment between the text and the pen locations Put simply it learns to
decide which character to write next
51 Synthesis Network
Fig 12 illustrates the network architecture used for handwriting synthesis As
with the prediction network the hidden layers are stacked on top of each other
each feeding up to the layer above and there are skip connections from the
inputs to all hidden layers and from all hidden layers to the outputs The
dierence is the added input from the character sequence mediated by the
window layer
Given a length Ucharacter sequence cand a length Tdata sequence x the
soft window wtintocat timestep t1tT is dened by the following
discrete convolution with a mixture of KGaussian functions
tu KX
k1k
texp
k
t
k
tu2
46
wtUX
u1tucu 47
wheretu is the window weight ofcuat timestep t Intuitively the tparam
eters control the location of the window the tparameters control the width of
the window and the tparameters control the importance of the window within
the mixture The size of the soft window vectors is the same as the size of the
character vectors cuassuming a onehot encoding this will be the number of
characters in the alphabet Note that the window mixture is not normalised
and hence does not determine a probability distribution however the window
weighttu can be loosely interpreted as the networks belief that it is writ
ing character cuat timet Fig 13 shows the alignment implied by the window
weights during a training sequence
The size 3Kvectorpof window parameters is determined as follows by the
outputs of the rst hidden layer of the network
ttt Wh1ph1
tbp 48
t exp t 49
t exp
t
50
tt1 exp t 51
Note that the location parameters tare dened as osets from the previous
locationsct1 and that the size of the oset is constrained to be greater than
zero Intuitively this means that network learns how far to slide each window
at each step rather than an absolute location Using osets was essential to
getting the network to align the text with the pen trace
26Inputs
CharactersHidden 1WindowHidden 2OutputsFigure 12 Synthesis Network Architecture Circles represent layers solid
lines represent connections and dashed lines represent predictions The topology
is similar to the prediction network in Fig 1 except that extra input from the
character sequence c is presented to the hidden layers via the window layer
with a delay in the connection to the rst hidden layer to avoid a cycle in the
graph
27Thought that the muster fromFigure 13 Window weights during a handwriting synthesis sequence
Each point on the map shows the value of tu wheretindexes the pen trace
along the horizontal axis and uindexes the text character along the vertical axis
The bright line is the alignment chosen by the network between the characters
and the writing Notice that the line spreads out at the boundaries between
characters this means the network receives information about next and previous
letters as it makes transitions which helps guide its predictions
28Thewtvectors are passed to the second and third hidden layers at time t
and the rst hidden layer at time t1 to avoid creating a cycle in the processing
graph The update equations for the hidden layers are
h1
tH
Wih1xtWh1h1h1
t1Wwh1wt1b1
h
52
hn
tH
WihnxtWhn1hnhn1
tWhnhnhn
t1Wwhnwtbn
h
53
The equations for the output layer remain unchanged from Eqs 17 to 22
The sequence loss is
Lx log Pr xjc 54
where
Prxjc TY
t1Pr xt1jyt 55
Note thatytis now a function of cas well as x1t
The loss derivatives with respect to the outputs  etttttremain un
changed from Eqs 27 30 and 31 Given the loss derivativeLx
wtwith
respect to the size Wwindow vector wt obtained by backpropagating the out
put derivatives through the computation graph in Fig 12 the derivatives with
respect to the window parameters are as follows
ktu defk
texp
k
t
k
tu2WX
j1Lx
wj
tcj
u 56
Lx
k
tUX
u1ktu  57
Lx
k
tk
tUX
u1ktu k
tu258
Lx
k
tLx
k
t1 2k
tUX
u1ktu uk
t 59
Lx
k
t exp
k
tLx
k
t60
Fig 14 illustrates the operation of a mixture density output layer applied to
handwriting synthesis
52 Experiments
The synthesis network was applied to the same input data as the handwriting
prediction network in the previous section The characterlevel transcriptions
from the IAMOnDB were now used to dene the character sequences c The full
transcriptions contain 80 distinct characters capital letters lower case letters
digits and punctuation However we used only a subset of 57 with all the
29Synthesis Output Density
Figure 14 Mixture density outputs for handwriting synthesis The top
heatmap shows the predictive distributions for the pen locations the bottom
heatmap shows the mixture component weights Comparison with Fig 10 indi
cates that the synthesis network makes more precise predictions with smaller
density blobs than the predictiononly network especially at the ends of strokes
where the synthesis network has the advantage of knowing which letter comes
next
30Table 4 Handwriting Synthesis Results All results recorded on the val
idation set LogLoss is the mean value of Lx in nats SSE is the mean
sumsquarederror per data point
Regularisation LogLoss SSE
none 10969 023
adaptive weight noise 11282 023
digits and most of the punctuation characters replaced with a generic non
letter label2
The network architecture was as similar as possible to the best prediction
network three hidden layers of 400 LSTM cells each 20 bivariate Gaussian
mixture components at the output layer and a size 3 input layer The character
sequence was encoded with onehot vectors and hence the window vectors were
size 57 A mixture of 10 Gaussian functions was used for the window parameters
requiring a size 30 parameter vector The total number of weights was increased
to approximately 37M
The network was trained with rmsprop using the same parameters as in
the previous section The network was retrained with adaptive weight noise
initial standard deviation 0075 and the output and LSTM gradients were again
clipped in the range  100100 and 1010 respectively
Table 4 shows that adaptive weight noise gave a considerable improvement
in logloss around 313 nats but no signicant change in sumsquared error
The regularised network appears to generate slightly more realistic sequences
although the dierence is hard to discern by eye Both networks performed
considerably better than the best prediction network In particular the sum
squarederror was reduced by 44 This is likely due in large part to the im
proved predictions at the ends of strokes where the error is largest
53 Unbiased Sampling
Given c an unbiased sample can be picked from Pr xjc by iteratively drawing
xt1from Pr xt1jyt just as for the prediction network The only dierence is
that we must also decide when the synthesis network has nished writing the text
and should stop making any future decisions To do this we use the following
heuristic as soon as tU 1tu81uUthe current input xtis
dened as the end of the sequence and sampling ends Examples of unbiased
synthesis samples are shown in Fig 15 These and all subsequent gures were
generated using the synthesis network retrained with adaptive weight noise
Notice how stylistic traits such as character size slant cursiveness etc vary
2This was an oversight however it led to the interesting result that when the text contains
a nonletter the network must select a digits or punctuation mark to generate Sometimes
the character can be be inferred from the context eg the apostrophe in cant otherwise
it is chosen at random
31widely between the samples but remain moreorless consistent within them
This suggests that the network identies the traits early on in the sequence
then remembers them until the end By looking through enough samples for a
given text it appears to be possible to nd virtually any combination of stylistic
traits which suggests that the network models them independently both from
each other and from the text
Blind taste tests carried out by the author during presentations suggest
that at least some unbiased samples cannot be distinguished from real hand
writing by the human eye Nonetheless the network does make mistakes we
would not expect a human writer to make often involving missing confused
or garbled letters3 this suggests that the network sometimes has trouble de
termining the alignment between the characters and the trace The number of
mistakes increases markedly when less common words or phrases are included
in the character sequence Presumably this is because the network learns an
implicit characterlevel language model from the training set that gets confused
when rare or unknown transitions occur
54 Biased Sampling
One problem with unbiased samples is that they tend to be dicult to read
partly because real handwriting is dicult to read and partly because the
network is an imperfect model Intuitively we would expect the network to
give higher probability to good handwriting because it tends to be smoother
and more predictable than bad handwriting If this is true we should aim to
output more probable elements of Pr xjc if we want the samples to be easier to
read A principled search for high probability samples could lead to a dicult
inference problem as the probability of every output depends on all previous
outputs However a simple heuristic where the sampler is biased towards more
probable predictions at each step independently generally gives good results
Dene the probability bias bas a real number greater than or equal to zero
Before drawing a sample from Pr xt1jyt each standard deviation j
tin the
Gaussian mixture is recalculated from Eq 21 to
j
t exp
j
tb
61
and each mixture weight is recalculated from Eq 19 to
j
texp
j
t1 b
PM
j01exp
j0
t1 b 62
This articially reduces the variance in both the choice of component from the
mixture and in the distribution of the component itself When b 0 unbiased
sampling is recovered and as b1 the variance in the sampling disappears
3We expect humans to make mistakes like misspelling temperament as temperement as
the second writer in Fig 15 seems to have done
32Figure 15 Real and generated handwriting  The top line in each block is
real the rest are unbiased samples from the synthesis network The two texts
are from the validation set and were not seen during training
33and the network always outputs the mode of the most probable component in
the mixture which is not necessarily the mode of the mixture but at least a
reasonable approximation Fig 16 shows the eect of progressively increasing
the bias and Fig 17 shows samples generated with a low bias for the same texts
as Fig 15
55 Primed Sampling
Another reason to constrain the sampling would be to generate handwriting
in the style of a particular writer rather than in a randomly selected style
The easiest way to do this would be to retrain it on that writer only But
even without retraining it is possible to mimic a particular style by priming
the network with a real sequence then generating an extension with the real
sequence still in the networks memory This can be achieved for a real xcand
a synthesis character string sby setting the character sequence to c0cs
and clamping the data inputs to xfor the rst Ttimesteps then sampling
as usual until the sequence ends Examples of primed samples are shown in
Figs 18 and 19 The fact that priming works proves that the network is able to
remember stylistic features identied earlier on in the sequence This technique
appears to work better for sequences in the training data than those the network
has never seen
Primed sampling and reduced variance sampling can also be combined As
shown in Figs 20 and 21 this tends to produce samples in a cleaned up version
of the priming style with overall stylistic traits such as slant and cursiveness
retained but the strokes appearing smoother and more regular A possible
application would be the articial enhancement of poor handwriting
6 Conclusions and Future Work
This paper has demonstrated the ability of Long ShortTerm Memory recur
rent neural networks to generate both discrete and realvalued sequences with
complex longrange structure using nextstep prediction It has also introduced
a novel convolutional mechanism that allows a recurrent network to condition
its predictions on an auxiliary annotation sequence and used this approach to
synthesise diverse and realistic samples of online handwriting Furthermore it
has shown how these samples can be biased towards greater legibility and how
they can be modelled on the style of a particular writer
Several directions for future work suggest themselves One is the applica
tion of the network to speech synthesis which is likely to be more challenging
than handwriting synthesis due to the greater dimensionality of the data points
Another is to gain a better insight into the internal representation of the data
and to use this to manipulate the sample distribution directly It would also
be interesting to develop a mechanism to automatically extract highlevel an
notations from sequence data In the case of handwriting this could allow for
34Figure 16 Samples biased towards higher probability The probability
biasesbare shown at the left As the bias increases the diversity decreases and
the samples tend towards a kind of average handwriting which is extremely
regular and easy to read easier in fact than most of the real handwriting in the
training set Note that even when the variance disappears the same letter is
not written the same way at dierent points in a sequence for examples the es
in exactly the same the ls in until they all look because the predictions
are still inuenced by the previous outputs If you look closely you can see that
the last three lines are not quite exactly the same
35Figure 17 A slight bias The top line in each block is real The rest are
samples from the synthesis network with a probability bias of 015 which seems
to give a good balance between diversity and legibility
36Figure 18 Samples primed with real sequences The priming sequences
drawn from the training set are shown at the top of each block None of the
lines in the sampled text exist in the training set The samples were selected
for legibility
37Figure 19 Samples primed with real sequences cotd
38Figure 20 Samples primed with real sequences and biased towards
higher probability The priming sequences are at the top of the blocks The
probability bias was 1 None of the lines in the sampled text exist in the training
set
39Figure 21 Samples primed with real sequences and biased towards
higher probability cotd
40more nuanced annotations than just text for example stylistic features dierent
forms of the same letter information about stroke order and so on
Acknowledgements
Thanks to Yichuan Tang Ilya Sutskever Navdeep Jaitly Georey Hinton and
other colleagues at the University of Toronto for numerous useful comments
and suggestions This work was supported by a Global Scholarship from the
Canadian Institute for Advanced Research
References
1 Y Bengio P Simard and P Frasconi Learning longterm dependencies
with gradient descent is dicult IEEE Transactions on Neural Networks 
52157166 March 1994
2 C Bishop Mixture density networks Technical report 1994
3 C Bishop Neural Networks for Pattern Recognition  Oxford University
Press Inc 1995
4 N BoulangerLewandowski Y Bengio and P Vincent Modeling tempo
ral dependencies in highdimensional sequences Application to polyphonic
music generation and transcription In Proceedings of the Twentynine In
ternational Conference on Machine Learning ICML12  2012
5 J G Cleary Ian and I H Witten Data compression using adaptive cod
ing and partial string matching IEEE Transactions on Communications 
32396402 1984
6 D Eck and J Schmidhuber A rst look at music composition using lstm
recurrent neural networks Technical report IDSIA USISUPSI Instituto
Dalle Molle
7 F Gers N Schraudolph and J Schmidhuber Learning precise timing
with LSTM recurrent networks Journal of Machine Learning Research 
3115143 2002
8 A Graves Practical variational inference for neural networks In Advances
in Neural Information Processing Systems  volume 24 pages 23482356
2011
9 A Graves Sequence transduction with recurrent neural networks In ICML
Representation Learning Worksop  2012
10 A Graves A Mohamed and G Hinton Speech recognition with deep
recurrent neural networks In Proc ICASSP  2013
4111 A Graves and J Schmidhuber Framewise phoneme classication with bidi
rectional LSTM and other neural network architectures Neural Networks 
18602610 2005
12 A Graves and J Schmidhuber Oine handwriting recognition with multi
dimensional recurrent neural networks In Advances in Neural Information
Processing Systems  volume 21 2008
13 P D Gr unwald The Minimum Description Length Principle Adaptive
Computation and Machine Learning  The MIT Press 2007
14 G Hinton A Practical Guide to Training Restricted Boltzmann Machines
Technical report 2010
15 S Hochreiter Y Bengio P Frasconi and J Schmidhuber Gradient Flow
in Recurrent Nets the Diculty of Learning Longterm Dependencies
In S C Kremer and J F Kolen editors A Field Guide to Dynamical
Recurrent Neural Networks  2001
16 S Hochreiter and J Schmidhuber Long ShortTerm Memory Neural
Computation  9817351780 1997
17 M Hutter The Human Knowledge Compression Contest 2012
18 KC Jim C Giles and B Horne An analysis of noise in recurrent neural
networks convergence and generalization Neural Networks IEEE Trans
actions on  761424 1438 1996
19 S Johansson R Atwell R Garside and G Leech The tagged LOB corpus
users manual Norwegian Computing Centre for the Humanities 1986
20 B Knoll and N de Freitas A machine learning perspective on predictive
coding with paq CoRR  abs11083298 2011
21 M Liwicki and H Bunke IAMOnDB  an online English sentence
database acquired from handwritten text on a whiteboard In Proc 8th
Int Conf on Document Analysis and Recognition  volume 2 pages 956
961 2005
22 M P Marcus B Santorini and M A Marcinkiewicz Building a large
annotated corpus of english The penn treebank COMPUTATIONAL
LINGUISTICS  192313330 1993
23 T Mikolov Statistical Language Models based on Neural Networks  PhD
thesis Brno University of Technology 2012
24 T Mikolov I Sutskever A Deoras H Le S Kombrink and J Cernocky
Subword language modeling with neural networks Technical report Un
published Manuscript 2012
4225 A Mnih and G Hinton A Scalable Hierarchical Distributed Language
Model In Advances in Neural Information Processing Systems  volume 21
2008
26 A Mnih and Y W Teh A fast and simple algorithm for training neural
probabilistic language models In Proceedings of the 29th International
Conference on Machine Learning  pages 17511758 2012
27 T N Sainath A Mohamed B Kingsbury and B Ramabhadran Low
rank matrix factorization for deep neural network training with high
dimensional output targets In Proc ICASSP  2013
28 M Schuster Better generative models for sequential data problems Bidi
rectional recurrent mixture density networks pages 589595 The MIT
Press 1999
29 I Sutskever G E Hinton and G W Taylor The recurrent temporal
restricted boltzmann machine pages 16011608 2008
30 I Sutskever J Martens and G Hinton Generating text with recurrent
neural networks In ICML  2011
31 G W Taylor and G E Hinton Factored conditional restricted boltzmann
machines for modeling motion style In Proc 26th Annual International
Conference on Machine Learning  pages 10251032 2009
32 T Tieleman and G Hinton Lecture 65  rmsprop Divide the gradient by
a running average of its recent magnitude 2012
33 R Williams and D Zipser Gradientbased learning algorithms for recur
rent networks and their computational complexity In Backpropagation
Theory Architectures and Applications  pages 433486 1995
43
  FlashAttention  Fast and MemoryEﬃcient Exact Attention
with IOAwareness
Tri Daoy Daniel Y Fuy Stefano Ermony Atri Rudraz and Christopher Réy
yDepartment of Computer Science Stanford University
zDepartment of Computer Science and Engineering University at Buﬀalo SUNY
triddanfucsstanfordedu ermonstanfordedu atribuffaloedu 
chrismrecsstanfordedu
June 24 2022
Abstract
Transformers are slow and memoryhungry on long sequences since the time and memory complexity
of selfattention are quadratic in sequence length Approximate attention methods have attempted
to address this problem by trading oﬀ model quality to reduce the compute complexity but often do
not achieve wallclock speedup We argue that a missing principle is making attention algorithms IO
awareaccounting for reads and writes between levels of GPU memory We propose FlashAttention 
an IOaware exact attention algorithm that uses tiling to reduce the number of memory readswrites
between GPU high bandwidth memory HBM and GPU onchip SRAM We analyze the IO complexity
ofFlashAttention  showing that it requires fewer HBM accesses than standard attention and is
optimal for a range of SRAM sizes We also extend FlashAttention to blocksparse attention yielding
an approximate attention algorithm that is faster than any existing approximate attention method
FlashAttention trains Transformers faster than existing baselines 15 endtoend wallclock speedup
on BERTlarge seq length 512 compared to the MLPerf 11 training speed record 3 speedup on
GPT2 seq length 1K and 24 speedup on longrange arena seq length 1K4K FlashAttention
and blocksparse FlashAttention enable longer context in Transformers yielding higher quality models
07 better perplexity on GPT2 and 64 points of lift on longdocument classiﬁcation and entirely new
capabilities the ﬁrst Transformers to achieve betterthanchance performance on the PathX challenge
seq length 16K 614 accuracy and Path256 seq length 64K 631 accuracy
1 Introduction
Transformer models  82 have emerged as the most widely used architecture in applications such as natural
language processing and image classiﬁcation Transformers have grown larger  5 and deeper  83 but
equipping them with longer context remains diﬃcult  80 since the selfattention module at their heart
has time and memory complexity quadratic in sequence length An important question is whether making
attention faster and more memoryeﬃcient can help Transformer models address their runtime and memory
challenges for long sequences
Many approximate attention methods have aimed to reduce the compute and memory requirements of
attention These methods range from sparseapproximation  5174 to lowrank approximation  125084
and their combinations  3992 Although these methods reduce the compute requirements to linear or
nearlinear in sequence length many of them do not display wallclock speedup against standard attention
and have not gained wide adoption One main reason is that they focus on FLOP reduction which may not
correlate with wallclock speed and tend to ignore overheads from memory access IO
In this paper we argue that a missing principle is making attention algorithms IOaware 1that is
carefully accounting for reads and writes to diﬀerent levels of fast and slow memory eg between fast GPU
onchip SRAM and relatively slow GPU high bandwidth memory or HBM  45 Figure 1 left On modern
1arXiv220514135v2  csLG  23 Jun 2022FlashAttentionMemory Hierarchy with
Bandwidth  Memory SizeAttention on GPT2
FlashAttention PyTorchTime ms
MatmulMaskSoftmaxDropoutMatmul
Fused
KernelQ N x d V N X dKT d x N
QKT N x N
smQ KTV N x dOuter Loop
Copy Block to SRAM
CopyOuter Loop
CopyInner LoopCompute Block
on SRAM
Output to HBM
Inner LoopInner LoopOuter Loop
GPU
SRAM
GPU
HBM
Main Memory
CPU DRAMSRAM  19 TBs 20 MB
HBM 15 TBs 40 GB
DRAM  128 GBs
                1 TB
051015Figure 1 Left FlashAttention uses tiling to prevent materialization of the large 𝑁𝑁attention matrix
dotted box on relatively slow GPU HBM In the outer loop red arrows FlashAttention loops through
blocks of the KandVmatrices and loads them to fast onchip SRAM In each block FlashAttention
loops over blocks of Qmatrix blue arrows loading them to SRAM and writing the output of the attention
computation back to HBM RightSpeedup over the PyTorch implementation of attention on GPT2
FlashAttention does not read and write the large 𝑁𝑁attention matrix to HBM resulting in an 76 
speedup on the attention computation
GPUs compute speed has outpaced memory speed  616263 and most operations in Transformers are
bottlenecked by memory accesses  43 IOaware algorithms have been critical for similar memorybound
operations when reading and writing data can account for a large portion of the runtimesuch as database
joins 71 image processing  70 numerical linear algebra  4 and more  4085 However common Python
interfaces to deep learning such as PyTorch and Tensorﬂow do not allow ﬁnegrained control of memory
access
We propose FlashAttention  a new attention algorithm that computes exact attention with far fewer
memory accesses Our main goal is to avoid reading and writing the attention matrix to and from HBM
This requires i computing the softmax reduction without access to the whole input ii not storing the large
intermediate attention matrix for the backward pass We apply two wellestablished techniques to address
these challenges i We restructure the attention computation to split the input into blocks and make several
passes over input blocks thus incrementally performing the softmax reduction also known as tiling ii We
store the softmax normalization factor from the forward pass to quickly recompute attention onchip in the
backward pass which is faster than the standard approach of reading the intermediate attention matrix from
HBM We implement FlashAttention in CUDA to achieve ﬁnegrained control over memory access and
fuse all the attention operations into one GPU kernel Even with the increased FLOPs due to recomputation
our algorithm both runs faster up to 76x on GPT2  67 Figure 1 right and uses less memory linear
in sequence lengththan standard attention thanks to the massively reduced amount of HBM access
We analyze the IO complexity  1 ofFlashAttention  proving that it requires 𝑂¹𝑁2𝑑2𝑀1ºHBM
accesses where 𝑑is the head dimension and 𝑀is the size of SRAM as compared to Ω¹𝑁𝑑𝑁2ºof standard
attention For typical values of 𝑑and𝑀FlashAttention requires many times fewer HBM accesses
compared to standard attention up to 9 fewer as shown in Fig 2 Moreover we provide a lower bound
showing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over
all SRAM sizes
We also show that FlashAttention can serve as a useful primitive for realizing the potential of
approximate attention algorithms by overcoming their issues with memory access overhead As a proof of
concept we implement blocksparse FlashAttention  a sparse attention algorithm that is 24 faster than
evenFlashAttention  scaling up to sequence length of 64k We prove that blocksparse FlashAttention
has better IO complexity than FlashAttention by a factor proportional to the sparsity ratio We discuss
further extensions to other operations attention on multiGPU kernel regression blocksparse matrix
2multiply in Section 5 We opensource FlashAttention to make it easier to build on this primitive1
We empirically validate that FlashAttention speeds up model training and improves model quality by
modeling longer context We also benchmark the runtime and memory footprint of FlashAttention and
blocksparse FlashAttention compared to prior attention implementations
Faster Model Training FlashAttention trains Transformer models faster in wallclock time We
train BERTlarge seq length 512 15 faster than the training speed record in MLPerf 11  58 GPT2
seq length 1K 3 faster than baseline implementations from HuggingFace  87 and MegatronLM  77
and longrange arena seq length 1K4K 24 faster than baselines
Higher Quality Models FlashAttention scales Transformers to longer sequences which improves
their quality and enables new capabilities We observe a 07 improvement in perplexity on GPT2 and
64 points of lift from modeling longer sequences on longdocument classiﬁcation 13 FlashAttention
enables the ﬁrst Transformer that can achieve betterthanchance performance on the PathX  80 challenge
solely from using a longer sequence length 16K Blocksparse FlashAttention enables a Transformer
to scale to even longer sequences 64K resulting in the ﬁrst model that can achieve betterthanchance
performance on Path256
Benchmarking Attention FlashAttention is up to 3faster than the standard attention implemen
tation across common sequence lengths from 128 to 2K and scales up to 64K Up to sequence length of 512
FlashAttention is both faster and more memoryeﬃcient than any existing attention method whereas
for sequence length beyond 1K some approximate attention methods eg Linformer start to become
faster On the other hand blocksparse FlashAttention is faster than all existing approximate attention
methods that we know of
2 Background
We provide some background on the performance characteristics of common deep learning operations on
modern hardware GPUs We also describe the standard implementation of attention
21 Hardware Performance
We focus here on GPUs Performance on other hardware accelerators are similar 46 48
GPU Memory Hierarchy The GPU memory hierarchy Fig 1 left comprises multiple forms of
memory of diﬀerent sizes and speeds with smaller memory being faster As an example the A100 GPU
has 4080GB of high bandwidth memory HBM with bandwidth 1520TBs and 192KB of onchip SRAM
per each of 108 streaming multiprocessors with bandwidth estimated around 19TBs  4445 The onchip
SRAM is an order of magnitude faster than HBM but many orders of magnitude smaller in size As compute
has gotten faster relative to memory speed  616263 operations are increasingly bottlenecked by memory
HBM accesses Thus exploiting fast SRAM becomes more important
Execution Model GPUs have a massive number of threads to execute an operation called a kernel
Each kernel loads inputs from HBM to registers and SRAM computes then writes outputs to HBM
Performance characteristics Depending on the balance of computation and memory accesses op
erations can be classiﬁed as either computebound or memorybound This is commonly measured by the
arithmetic intensity 85 which is the number of arithmetic operations per byte of memory access
1Computebound the time taken by the operation is determined by how many arithmetic operations there
are while time accessing HBM is much smaller Typical examples are matrix multiply with large inner
dimension and convolution with large number of channels
2Memorybound the time taken by the operation is determined by the number of memory accesses while
time spent in computation is much smaller Examples include most other operations elementwise eg
activation dropout and reduction eg sum softmax batch norm layer norm
Kernel fusion The most common approach to accelerate memorybound operations is kernel fusion if
there are multiple operations applied to the same input the input can be loaded once from HBM instead of
multiple times for each operation Compilers can automatically fuse many elementwise operations  536575
1FlashAttention code is available at httpsgithubcomHazyResearchflashattention
3However in the context of model training the intermediate values still need to be written to HBM to save
for the backward pass reducing the eﬀectiveness of naive kernel fusion
22 Standard Attention Implementation
Given input sequences QKV2R𝑁𝑑where𝑁is the sequence length and 𝑑is the head dimension we want
to compute the attention output O2R𝑁𝑑
SQK2R𝑁𝑁Psoftmax¹Sº2R𝑁𝑁OPV2R𝑁𝑑
where softmax is applied rowwise
Standard attention implementations materialize the matrices SandPto HBM which takes 𝑂¹𝑁2ºmemory
Often𝑁𝑑eg for GPT2 𝑁1024and𝑑64 We describe the standard attention implementation
in Algorithm 0 As some or most of the operations are memorybound eg softmax the large number of
memory accesses translates to slow wallclock time
This problem is exacerbated by other elementwise operations applied to the attention matrix such as
masking applied to Sor dropout applied to P As a result there have been many attempts to fuse several
elementwise operations such as fusing masking with softmax 77
In Section 32 we will show that the standard attention implementation performs HBM accesses quadratic
in the sequence length 𝑁 We also compare the number of FLOPs and number of HBM accesses of standard
attention and of our method  FlashAttention 
Algorithm 0 Standard Attention Implementation
Require Matrices QKV2R𝑁𝑑in HBM
1Load QKby blocks from HBM compute SQK write Sto HBM
2Read Sfrom HBM compute Psoftmax¹Sº write Pto HBM
3Load PandVby blocks from HBM compute OPV write Oto HBM
4Return O
3FlashAttention  Algorithm Analysis and Extensions
We show how to compute exact attention with fewer HBM readswrites and without storing large intermediate
matrices for the backward pass This yields an attention algorithm that is both memory eﬃcient and faster in
wallclock time We analyze its IO complexity showing that our method requires much fewer HBM accesses
compared to standard attention We further show that FlashAttention can serve as a useful primitive by
extending it to handle blocksparse attention
We focus here on the forward pass for ease of exposition Appendix B contains details for the backward
31 An Eﬃcient Attention Algorithm With Tiling and Recomputation
Given the inputs QKV2R𝑁𝑑in HBM we aim to compute the attention output O2R𝑁𝑑and write it to
HBM Our goal is to reduce the amount of HBM accesses to subquadratic in 𝑁
We apply two established techniques tiling recomputation to overcome the technical challenge of
computing exact attention in subquadratic HBM accesses We describe this in Algorithm 1 The main idea
is that we split the inputs QKVinto blocks load them from slow HBM to fast SRAM then compute the
attention output with respect to those blocks By scaling the output of each block by the right normalization
factor before adding them up we get the correct result at the end
Tiling We compute attention by blocks Softmax couples columns of K so we decompose the large
softmax with scaling 51 60 66 For numerical stability the softmax of vector 𝑥2R𝐵is computed as
𝑚¹𝑥ºmax
𝑖𝑥𝑖 𝑓¹𝑥º
𝑒𝑥1𝑚¹𝑥º 𝑒𝑥𝐵𝑚¹𝑥º
 ℓ¹𝑥º
𝑖𝑓¹𝑥º𝑖softmax¹𝑥º𝑓¹𝑥º
ℓ¹𝑥º
4For vectors 𝑥¹1º𝑥¹2º2R𝐵 we can decompose the softmax of the concatenated 𝑥
𝑥¹1º𝑥¹2º
2R2𝐵as
𝑚¹𝑥º𝑚¹
𝑥¹1º𝑥¹2º
ºmax¹𝑚¹𝑥¹1ºº𝑚¹𝑥¹2ººº 𝑓¹𝑥ºh
𝑒𝑚¹𝑥¹1ºº𝑚¹𝑥º𝑓¹𝑥¹1ºº𝑒𝑚¹𝑥¹2ºº𝑚¹𝑥º𝑓¹𝑥¹2ººi

ℓ¹𝑥ºℓ¹
𝑥¹1º𝑥¹2º
º𝑒𝑚¹𝑥¹1ºº𝑚¹𝑥ºℓ¹𝑥¹1ºº𝑒𝑚¹𝑥¹2ºº𝑚¹𝑥ºℓ¹𝑥¹2ººsoftmax¹𝑥º𝑓¹𝑥º
ℓ¹𝑥º
Therefore if we keep track of some extra statistics  𝑚¹𝑥ºℓ¹𝑥º we can compute softmax one block at a time2
We thus split the inputs QKVinto blocks Algorithm 1 line 3 compute the softmax values along with
extra statistics Algorithm 1 line 10 and combine the results Algorithm 1 line 12
Recomputation One of our goals is to not store 𝑂¹𝑁2ºintermediate values for the backward pass The
backward pass typically requires the matrices SP2R𝑁𝑁to compute the gradients with respect to QKV
However by storing the output Oand the softmax normalization statistics ¹𝑚ℓº we can recompute the
attention matrix SandPeasily in the backward pass from blocks of QKVin SRAM This can be seen as a
form of selective gradient checkpointing  1034 While gradient checkpointing has been suggested to reduce
the maximum amount of memory required  66 all implementations that we know oﬀ have to trade speed
for memory In contrast even with more FLOPs our recomputation speeds up the backward pass due to
reduced HBM accesses Fig 2 The full backward pass description is in Appendix B
Implementation details Kernel fusion Tiling enables us to implement our algorithm in one
CUDA kernel loading input from HBM performing all the computation steps matrix multiply softmax
optionally masking and dropout matrix multiply then write the result back to HBM masking and dropout
in Appendix B This avoids repeatedly reading and writing of inputs and outputs from and to HBM
Algorithm 1 FlashAttention
Require Matrices QKV2R𝑁𝑑in HBM onchip SRAM of size 𝑀
1Set block sizes 𝐵𝑐𝑀
4𝑑
𝐵𝑟min𝑀
4𝑑
𝑑
2Initialize O¹0º𝑁𝑑2R𝑁𝑑ℓ¹0º𝑁2R𝑁𝑚¹1º𝑁2R𝑁in HBM
3Divide Qinto𝑇𝑟l
𝑁
𝐵𝑟m
blocks Q1Q𝑇𝑟of size𝐵𝑟𝑑each and divide KVin to𝑇𝑐l
𝑁
𝐵𝑐m
blocks
K1K𝑇𝑐andV1V𝑇𝑐 of size𝐵𝑐𝑑each
4Divide Ointo𝑇𝑟blocks O𝑖O𝑇𝑟of size𝐵𝑟𝑑each divide ℓinto𝑇𝑟blocksℓ𝑖ℓ𝑇𝑟of size𝐵𝑟each
divide𝑚into𝑇𝑟blocks𝑚1𝑚𝑇𝑟of size𝐵𝑟each
5for1𝑗𝑇𝑐do
6Load K𝑗V𝑗from HBM to onchip SRAM
7for1𝑖𝑇𝑟do
8Load Q𝑖O𝑖ℓ𝑖𝑚𝑖from HBM to onchip SRAM
9On chip compute S𝑖𝑗Q𝑖K𝑇
𝑗2R𝐵𝑟𝐵𝑐
10On chip compute 𝑚𝑖𝑗rowmax¹S𝑖𝑗º 2R𝐵𝑟P𝑖𝑗exp¹S𝑖𝑗𝑚𝑖𝑗º 2R𝐵𝑟𝐵𝑐pointwise ℓ𝑖𝑗
rowsum¹P𝑖𝑗º2R𝐵𝑟
11On chip compute 𝑚new
𝑖max¹𝑚𝑖𝑚𝑖𝑗º2R𝐵𝑟ℓnew
𝑖𝑒𝑚𝑖𝑚new
𝑖ℓ𝑖𝑒𝑚𝑖𝑗𝑚new
𝑖ℓ𝑖𝑗2R𝐵𝑟
12Write O𝑖 diag¹ℓnew
𝑖º1¹diag¹ℓ𝑖º𝑒𝑚𝑖𝑚new
𝑖O𝑖𝑒𝑚𝑖𝑗𝑚new
𝑖P𝑖𝑗V𝑗ºto HBM
13Writeℓ𝑖 ℓnew
𝑖𝑚𝑖 𝑚new
𝑖to HBM
14end for
15end for
16Return O
We show FlashAttention s correctness runtime and memory requirement proof in Appendix C
Theorem 1 Algorithm 1 returns Osoftmax¹QKºVwith𝑂¹𝑁2𝑑ºFLOPs and requires 𝑂¹𝑁ºadditional
memory beyond inputs and output
32 Analysis IO Complexity of FlashAttention
We analyze the IO complexity of FlashAttention  showing signiﬁcant reduction in HBM accesses compared
to standard attention We also provide a lower bound proving that no exact attention algorithm can
2This style of aggregation is called algebraic aggregation 33
5Attention Standard FlashAttention
GFLOPs 666 752
HBM RW GB 403 44
Runtime ms 417 73
Sparsity Speedup
 NonZero Blocks20 6050100150Fwd  Bwd msEﬀect of Block Size
Block Size64128 256 512Fwd Runtime ms
6
2HBM Accesses GBDense
FlashAttention
BlockSparse
FlashAttention246
RuntimeHBMAccesses
Figure 2 Left Forward  backward runtime of standard attention and FlashAttention for GPT2 medium
seq length 1024 head dim 64 16 heads batch size 64 on A100 GPU HBM access is the primary factor aﬀecting
runtime Middle Forward runtime of FlashAttention seq length 1024 head dim 64 16 heads batch size 64 on
A100 GPU Fewer HBM accesses result in faster runtime up to a point Right The runtime for seq length 4K of
blocksparse FlashAttention is faster than FlashAttention by a factor proportional to the sparsity
asymptotically improve on HBM accesses over all SRAM sizes Proofs are in Appendix C
Theorem2 Let𝑁be the sequence length 𝑑be the head dimension and 𝑀be size of SRAM with 𝑑𝑀𝑁𝑑
Standard attention Algorithm 0 requires Θ¹𝑁𝑑𝑁2ºHBM accesses while FlashAttention Algorithm 1
requiresΘ¹𝑁2𝑑2𝑀1ºHBM accesses
For typical values of 𝑑64128 and 𝑀around 100KB 𝑑2is many times smaller than 𝑀 and thus
FlashAttention requires many times fewer HBM accesses than standard implementation This leads to
both faster execution and lower memory footprint which we validate in Section 43
The main idea of the proof is that given the SRAM size of 𝑀 we can load blocks of KVof sizeΘ¹𝑀ºeach
Algorithm 1 line 6 For each block of KandV we iterate over all blocks of QAlgorithm 1 line 8 to compute
the intermediate values resulting in Θ¹𝑁𝑑𝑀1ºpasses over Q Each pass loads Θ¹𝑁𝑑ºelements which
amounts to Θ¹𝑁2𝑑2𝑀1ºHBM accesses We similarly prove that the backward pass of standard attention
requiresΘ¹𝑁𝑑𝑁2ºHBM accesses while the backward pass of FlashAttention requiresΘ¹𝑁2𝑑2𝑀1º
HBM accesses Appendix B
We prove a lowerbound one cannot asymptotically improve on the number of HBM accesses for all
values of𝑀the SRAM size when computing exact attention
Proposition 3 Let𝑁be the sequence length 𝑑be the head dimension and 𝑀be size of SRAM with
𝑑𝑀𝑁𝑑 There does not exist an algorithm to compute exact attention with 𝑜¹𝑁2𝑑2𝑀1ºHBM accesses
for all𝑀in the range𝑑𝑁𝑑¼
The proof relies on the fact that for 𝑀 Θ¹𝑁𝑑ºany algorithm must perform Ω¹𝑁2𝑑2𝑀1º Ω¹𝑁𝑑º
HBM accesses This type of lower bound over a subrange of 𝑀is common in the streaming algorithms
literature  88 We leave proving parameterized complexity  27 lower bounds in terms of 𝑀as exciting future
work
We validate that the number of HBM accesses is the main determining factor of attention runtime
In Fig 2 left we see that even though FlashAttention has higher FLOP count compared to standard
attention due to recomputation in the backward pass it has much fewer HBM accesses resulting in much
faster runtime In Fig 2 middle we vary the block size 𝐵𝑐ofFlashAttention  which results in diﬀerent
amounts of HBM accesses and measure the runtime of the forward pass As block size increases the number
of HBM accesses decreases as we make fewer passes over the input and runtime decreases For large enough
block size beyond 256 the runtime is then bottlenecked by other factors eg arithmetic operations
Moreover larger block size will not ﬁt into the small SRAM size
33 Extension BlockSparse FlashAttention
We extend FlashAttention to approximate attention we propose blocksparse FlashAttention  whose
IO complexity is smaller than FlashAttention by a factor proportional to the sparsity
Given inputs QKV2R𝑁𝑑and a mask matrix M2f01g𝑁𝑁 we want to compute
SQK2R𝑁𝑁Psoftmax¹S𝟙Mº2R𝑁𝑁OPV2R𝑁𝑑
where¹S𝟙Mº𝑘𝑙S𝑘𝑙ifM𝑘𝑙1and1ifM𝑘𝑙0 We require Mto have block form for some block sizes
𝐵𝑟𝐵𝑐 for all𝑘𝑙M𝑘𝑙M𝑖𝑗with𝑖b𝑘𝐵𝑟c𝑗b𝑙𝐵𝑐cfor some M2f01g𝑁𝐵𝑟𝑁𝐵𝑐
6Given a predeﬁned block sparsity mask M2f01g𝑁𝐵𝑟𝑁𝐵𝑐we can easily adapt Algorithm 1 to only
compute the nonzero blocks of the attention matrix The algorithm is identical to Algorithm 1 except we
skip zero blocks We reproduce the algorithm description in Algorithm 5 in Appendix B
We also analyze the IO complexity of blocksparse FlashAttention 
Proposition 4 Let𝑁be the sequence length 𝑑be the head dimension and 𝑀be size of SRAM with
𝑑𝑀𝑁𝑑 Blocksparse FlashAttention Algorithm 5 requires Θ¹𝑁𝑑𝑁2𝑑2𝑀1𝑠ºHBM accesses
where𝑠is the fraction of nonzero blocks in the blocksparsity mask
We see that applying blocksparsity yields a direct improvement by the sparsity to the larger term in the
IO complexity For large sequence lengths 𝑁𝑠is often set to 𝑁1211 or𝑁1log𝑁31792 resulting
inΘ¹𝑁p
𝑁ºorΘ¹𝑁log𝑁ºIO complexity For downstream experiments we use the ﬁxed butterﬂy sparsity
pattern 17 which has been shown to be able to approximate arbitrary sparsity 16
In Fig 2 right we validate that as the sparsity increases the runtime of blocksparse FlashAttention
improves proportionally On the LRA benchmark blocksparse FlashAttention achieves 28speedup
while performing on par with standard attention Section 4
4 Experiments
We evaluate the impact of using FlashAttention to train Transformer models We validate two claims
about training time and model accuracy and report attention runtime and memory benchmarks
Training Speed FlashAttention outperforms the MLPerf 11  58 speed record for BERT by 15 and
speeds up GPT2 up to 3 over HuggingFace  87 and 18over Megatron  77 over standard Transformers
FlashAttention speeds up the longrange arena LRA benchmark 24 
Quality FlashAttention scales Transformers to longer sequences yielding higher quality FlashAt
tention trains GPT2 with context length 4K faster than Megatron trains GPT2 with context length
1K while achieving 07 better perplexity Modeling longer sequences yields 64 points of lift on two long
document classiﬁcation tasks Finally FlashAttention yields the ﬁrst Transformer that can achieve
betterthanrandom performance on the challenging PathX task sequence length 16K and blocksparse
FlashAttention yields the ﬁrst sequence model that we know of that can achieve betterthanrandom
performance on Path256 sequence length 64K
Benchmarking Attention We measure the runtime and memory performance of FlashAttention
and blocksparse FlashAttention based on sequence length We conﬁrm that the memory footprint
ofFlashAttention scales linearly with seq length and is up to 3 faster than standard attention for
common seq lengths up to 2K We conﬁrm that runtime of blocksparse FlashAttention scales linearly
in seq length and is faster than all existing approximate attention baselines
Additional experiment details are in Appendix E
41 Faster Models with FlashAttention
BERT FlashAttention yields the fastest singlenode BERT training speed that we know of We train a
BERTlarge  22 model with FlashAttention on Wikipedia Table 1 compares our training time to the
implementation from Nvidia that set the training speed record for MLPerf 11  58 Our implementation is
15 faster
Table 1 Training time of BERTlarge starting from the same initialization provided by the MLPerf benchmark to
reach the target accuracy of 720 on masked language modeling Averaged over 10 runs on 8 A100 GPUs
BERT Implementation Training time minutes
Nvidia MLPerf 11 58 20015
FlashAttention ours 17414
GPT2 FlashAttention yieldsfastertrainingtimesforGPT2 67onthelargeOpenWebtextdataset 32
than the widely used HuggingFace  87 and MegatronLM  77 implementations Table 2 shows up to 3 end
toend speedup compared to Huggingface and 17 speedup compared to MegatronLM FlashAttention
7achieves the same perplexity as the other two implementations as we do not change the model deﬁnition
Appendix E includes plots of the validation perplexity throughout training conﬁrming that FlashAttention
is as numerically stable as the baselines and produces the same training  validation curves
Table 2 GPT2 small and medium using FlashAttention achieve up to 3speed up compared to Huggingface
implementation and up to 17 compared to MegatronLM Training time reported on 8 A100s GPUs
Model implementations OpenWebText ppl Training time speedup
GPT2 small  Huggingface 87 182 95 days 10 
GPT2 small  MegatronLM 77 182 47 days 20 
GPT2 small  FlashAttention 182 27 days 35
GPT2 medium  Huggingface 87 142 210 days 10 
GPT2 medium  MegatronLM 77 143 115 days 18 
GPT2 medium  FlashAttention 143 69 days 30
Longrange Arena We compare vanilla Transformer with either standard implementation or FlashAt
tention  on the longrange arena LRA  80 benchmark We measure accuracy throughput and training
time of all models Each task has a diﬀerent sequence length varying between 1024 and 4096 We follow the
implementation and experimental setting in Tay et al 80and Xiong et al 903Table 3 shows that FlashAt
tention achieves up 24speedup compared to standard attention Blocksparse FlashAttention is
faster than all of the approximate attention methods that we have tested
Table 3 The performance of standard attention FlashAttention  blocksparse FlashAttention  and approximate
attention baselines on the LongRangeArena benchmarks
Models ListOps Text Retrieval Image Pathﬁnder AvgSpeedup
Transformer 360 636 816 423 727 593 
FlashAttention 376 639 814 435 727 598 24
Blocksparse FlashAttention 370 630 813 436 733 596 28
Linformer 84 356 559 777 378 676 549 25
Linear Attention 50 388 632 807 426 725 596 23
Performer 12 368 636 822 421 699 589 18
Local Attention 80 361 602 767 406 666 560 17
Reformer 51 365 638 785 396 694 576 13
Smyrf 19 361 641 790 396 705 579 17
42 Better Models with Longer Sequences
Language Modeling with Long Context The runtime and memoryeﬃciency of FlashAttention
allow us to increase the context length of GPT2 by 4 while still running faster than the optimized
implementation from MegatronLM Table 4 shows that that GPT2 with FlashAttention and context
length 4K is still 30 faster than GPT2 from Megatron with context length 1K while achieving 07 better
perplexity
Table 4 GPT2 small with FlashAttention  with 4larger context length compared to MegatronLM is still 30
faster while achieving 07 better perplexity Training time on 8 A100 GPUs is reported
Model implementations Context length OpenWebText ppl Training time speedup
GPT2 small  MegatronLM 1k 182 47 days 10 
GPT2 small  FlashAttention 1k 182 27 days 17
GPT2 small  FlashAttention 2k 176 30 days 16 
GPT2 small  FlashAttention 4k 175 36 days 13
Long Document Classiﬁcation Training Transformers with longer sequences with FlashAttention
improves performance on the MIMICIII  47 and ECtHR  67 datasets MIMICIII contains intensive care
unit patient discharge summaries each annotated with multiple labels ECtHR contains legal cases from the
3LRA accuracy results are known to be highly dependent on the tuning procedure  90 Our reproduced baselines perform
better than as reported in the original comparison 80
8Attention Memory Usage
Sequence LengthAttention Runtime Fwd Pass  Bwd Pass
Sequence LengthRuntime ms
Memory Footprint GB256 8K 16K 32K 64K 128 256 512 1024 2048 4096101102
1020
FlashAttention
BlockSparse FlashAttentionPyTorch Attention
Megatron AttentionLinformer Attention
OpenAI Sparse Attention8192100Crossover Points
20x2xFigure 3 Leftruntime of forward pass  backward pass Rightattention memory usage
European Court of Human Rights each of which is mapped to articles of the Convention of Human Rights
that were allegedly violaged Both of these datasets contain very long text documents the average number of
tokens in MIMIC is 2395 tokens and the longest document contains 14562 tokens while the average and
longest numbers in ECtHR are 2197 and 49392 respectively We evaluate lift from increasing the sequence
length of a pretrained RoBERTa model 56 we repeat the positional embeddings as in Beltagy et al 3
Table 5 shows that sequence length 16K outperforms length 512 by 43 points on MIMIC and that length
8K outperforms length 512 by 85 points on ECtHR The discrepancies may be due to subtle distribution
shifts MIMICIII contains specialized medical text and thus may be more susceptible to a distribution shift
in the document length whereas ECtHR contains general language
Table 5 Long Document performance mi
cro𝐹1 at diﬀerent sequence lengths using
FlashAttention 
512 1024 2048 4096 8192 16384
MIMICIII 47 528 507 517 546 564 571
ECtHR 6 722 743 771 786 807792Table 6 We report the ﬁrst Transformer
model that can achieve nonrandom perfor
mance on PathX and Path256
Model PathX Path256
Transformer 7 7
Linformer 84 7 7
Linear Attention 50 7 7
Performer 12 7 7
Local Attention 80 7 7
Reformer 51 7 7
SMYRF 19 7 7
FlashAttention 614 7
Blocksparse FlashAttention 560 631
PathX and Path256 The PathX and Path256 benchmarks are challenging tasks from the longrange
arena benchmark designed to test long context The task is to classify whether two points in a black and
white 128128 or 256256 image have a path connecting them and the images are fed to the transformer
one pixel at a time In prior work all transformer models have either run out of memory or only achieved
random performance  80 There has been a search for alternative architectures that can model such long
context  37 We present here the ﬁrst result of Transformer models being able to solve PathX and Path256
Table 6 We pretrain a transformer on Path64 and then transfer to PathX by spatially interpolating
the positional embeddings FlashAttention achieves 614 accuracy on PathX Additionally blocksparse
FlashAttention enables the Transformers to scale to sequence length 64K achieving 631 accuracy4on
Path256
43 Benchmarking Attention
We vary sequence length and measure runtime and memory usage of FlashAttention and blocksparse
FlashAttention against various attention baselines on one A100 GPU with 40 GB HBM with dropout and
a padding mask We compare against reference implementations for exact attention approximate attention
and sparse attention We report a subset of baselines in the main body Appendix E contains more baselines
and full details
4Path256 requires longer sequences but has relatively shorter paths than PathX so it is easier to obtain a higher accuracy
9Runtime Figure 3 left reports the runtime in milliseconds of the forward  backward pass of FlashAt
tention and blocksparse FlashAttention compared to the baselines in exact approximate and sparse
attention exact numbers in Appendix E Runtime grows quadratically with sequence length but FlashAt
tention runs signiﬁcantly faster than exact attention baselines up to 3 faster than the PyTorch
implementation The runtimes of many approximatesparse attention mechanisms grow linearly with se
quence length but FlashAttention still runs faster than approximate and sparse attention for short
sequences due to fewer memory accesses The approximate attention runtimes begin to cross over with
FlashAttention at sequences between 512 and 1024 On the other hand blocksparse FlashAttention
is faster than all implementations of exact sparse and approximate attention that we know of across all
sequence lengths
Memory Footprint Figure 3 right shows the memory footprint of FlashAttention and blocksparse
FlashAttention compared to various exact approximate and sparse attention baselines FlashAttention
and blocksparse FlashAttention have the same memory footprint which grows linearly with sequence
length FlashAttention is up to 20more memory eﬃcient than exact attention baselines and is more
memoryeﬃcient than the approximate attention baselines All other algorithms except for Linformer run
out of memory on an A100 GPU before 64K and FlashAttention is still 2more eﬃcient than Linformer
5 Limitations and Future Directions
We discuss limitations of our approach and future directions Related work is given in Appendix A
Compiling to CUDA Our current approach to building IOaware implementations of attention requires
writing a new CUDA kernel for each new attention implementation This requires writing the attention
algorithm in a considerably lowerlevel language than PyTorch and requires signiﬁcant engineering eﬀort
Implementations may also not be transferrable across GPU architectures These limitations suggest the
need for a method that supports writing attention algorithms in a highlevel language eg PyTorch and
compiling to IOaware implementations in CUDAsimilar to eﬀorts such as Halide in image processing  70
IOAware Deep Learning We believe that the IOaware approach can extend beyond attention
Attention is the most memoryintensive computation in Transformers but every layer in a deep network
touches GPU HBM We hope our work inspires IOaware implementations of additional modules We discuss
these potential extensions in Appendix D
MultiGPU IOAware Methods Our IOaware implementation of attention is optimal within con
stants for computing attention on a single GPU However the attention computation may be parallelizable
across multiple GPUs  72 Using multiple GPUs adds an additional layer to IO analysisaccounting for
data transfer between GPUs We hope our work inspires future work in this direction
Acknowledgments
Our implementation uses Apexs FMHA code  httpsgithubcomNVIDIAapextreemasterapex
contribcsrcfmha  as a starting point We thank YoungJun Ko for the indepth explanation of his FMHA
implementation and for his thoughtful answers to our questions about CUDA We thank Sabri Eyuboglu
Megan Leszczynski Laurel Orr Yuhuai Wu Beidi Chen and Xun Huang for their constructive feedback and
suggestions on early drafts of the paper We thank Markus Rabe and Charles Staats for helpful discussion of
their attention algorithm
We gratefully acknowledge the support of NIH under No U54EB020405 Mobilize NSF under Nos
CCF1763315 Beyond Sparsity CCF1563078 Volume to Velocity and 1937301 RTML ARL under
No W911NF2120251 Interactive HumanAI Teaming ONR under No N000141712266 Unifying Weak
Supervision ONR N000142012480 Understanding and Applying NonEuclidean Geometry in Machine
Learning N000142012275 NEPTUNE NXP Xilinx LETICEA Intel IBM Microsoft NEC Toshiba
TSMC ARM Hitachi BASF Accenture Ericsson Qualcomm Analog Devices Google Cloud Salesforce
Total the HAIGCP  HAIAzure Cloud Credits for Research program the Stanford Data Science Initiative
SDSI Department of Defense DoD through the National Defense Science and Engineering Graduate
Fellowship NDSEG Program and members of the Stanford DAWN project Facebook Google and
VMWare The US Government is authorized to reproduce and distribute reprints for Governmental purposes
10notwithstanding any copyright notation thereon Any opinions ﬁndings and conclusions or recommendations
expressed in this material are those of the authors and do not necessarily reﬂect the views policies or
endorsements either expressed or implied of NIH ONR or the US Government Atri Rudras research is
supported by NSF grant CCF1763481
References
1Alok Aggarwal and S Vitter Jeﬀrey The inputoutput complexity of sorting and related problems
Communications of the ACM  31911161127 1988
2Irwan Bello LambdaNetworks Modeling longrange interactions without attention arXiv preprint
arXiv210208602  2021
3Iz Beltagy Matthew E Peters and Arman Cohan Longformer The longdocument transformer arXiv
preprint arXiv200405150  2020
4L Susan Blackford Antoine Petitet Roldan Pozo Karin Remington R Clint Whaley James Demmel
Jack Dongarra Iain Duﬀ Sven Hammarling Greg Henry et al An updated set of basic linear algebra
subprograms blas ACM Transactions on Mathematical Software  282135151 2002
5Tom Brown Benjamin Mann Nick Ryder Melanie Subbiah Jared D Kaplan Prafulla Dhariwal Arvind
Neelakantan Pranav Shyam Girish Sastry Amanda Askell et al Language models are fewshot learners
Advances in neural information processing systems  3318771901 2020
6Ilias Chalkidis Ion Androutsopoulos and Nikolaos Aletras Neural legal judgment prediction in English
InProceedings of the 57th Annual Meeting of the Association for Computational Linguistics  pages
43174323 Florence Italy 2019 Association for Computational Linguistics doi 1018653v1P191424
URL httpswwwaclweborganthologyP191424 
7Ilias Chalkidis Manos Fergadiotis Dimitrios Tsarapatsanis Nikolaos Aletras Ion Androutsopoulos and
Prodromos Malakasiotis Paragraphlevel rationale extraction through regularization A case study on
european court of human rights cases In Proceedings of the Annual Conference of the North American
Chapter of the Association for Computational Linguistics  Mexico City Mexico 2021 Association for
Computational Linguistics
8Benjamin Charlier Jean Feydy Joan Alexis Glaunès FrançoisDavid Collin and Ghislain Durif Kernel
operations on the gpu with autodiﬀ without memory overﬂows Journal of Machine Learning Research 
227416 2021 URL httpjmlrorgpapersv2220275html 
9Beidi Chen Tri Dao Eric Winsor Zhao Song Atri Rudra and Christopher Ré Scatterbrain Unifying
sparse and lowrank attention In Advances in Neural Information Processing Systems NeurIPS  2021
10Tianqi Chen Bing Xu Chiyuan Zhang and Carlos Guestrin Training deep nets with sublinear memory
costarXiv preprint arXiv160406174  2016
11Rewon Child Scott Gray Alec Radford and Ilya Sutskever Generating long sequences with sparse
transformers arXiv preprint arXiv190410509  2019
12Krzysztof Marcin Choromanski Valerii Likhosherstov David Dohan Xingyou Song Andreea Gane
Tamas Sarlos Peter Hawkins Jared Quincy Davis Afroz Mohiuddin Lukasz Kaiser et al Rethinking
attention with performers In International Conference on Learning Representations ICLR  2020
13Xiang Dai Ilias Chalkidis Sune Darkner and Desmond Elliott Revisiting transformerbased models for
long document classiﬁcation arXiv preprint arXiv220406683  2022
14Zihang Dai Zhilin Yang Yiming Yang Jaime G Carbonell Quoc Le and Ruslan Salakhutdinov
TransformerXL Attentive language models beyond a ﬁxedlength context In Proceedings of the 57th
Annual Meeting of the Association for Computational Linguistics  pages 29782988 2019
1115Tri Dao Albert Gu Matthew Eichhorn Atri Rudra and Christopher Ré Learning fast algorithms
for linear transforms using butterﬂy factorizations In International Conference on Machine Learning
ICML 2019
16Tri Dao Nimit Sohoni Albert Gu Matthew Eichhorn Amit Blonder Megan Leszczynski Atri Rudra
and Christopher Ré Kaleidoscope An eﬃcient learnable representation for all structured linear maps
InInternational Conference on Learning Representations ICLR  2020
17Tri Dao Beidi Chen Kaizhao Liang Jiaming Yang Zhao Song Atri Rudra and Christopher Ré
Pixelated butterﬂy Simple and eﬃcient sparse training for neural network models In International
Conference on Learning Representations ICLR  2022
18Tri Dao Beidi Chen Nimit Sohoni Arjun Desai Michael Poli Jessica Grogan Alexander Liu Aniruddh
Rao Atri Rudra and Christopher Ré Monarch Expressive structured matrices for eﬃcient and accurate
training In International Conference on Machine Learning ICML  2022
19Giannis Daras Nikita Kitaev Augustus Odena and Alexandros G Dimakis Smyrfeﬃcient attention
using asymmetric clustering Advances in Neural Information Processing Systems  3364766489 2020
20Christopher De Sa Albert Gu Rohan Puttagunta Christopher Ré and Atri Rudra A twopronged
progress in structured dense matrix vector multiplication In Proceedings of the TwentyNinth Annual
ACMSIAM Symposium on Discrete Algorithms  pages 10601079 SIAM 2018
21Peter J Denning The working set model for program behavior Communications of the ACM  115
323333 1968
22Jacob Devlin MingWei Chang Kenton Lee and Kristina Toutanova BERT Pretraining of deep
bidirectional transformers for language understanding 2019
23Xin Dong Shangyu Chen and Sinno Jialin Pan Learning to prune deep neural networks via layerwise
optimal brain surgeon arXiv preprint arXiv170507565  2017
24Alexey Dosovitskiy Lucas Beyer Alexander Kolesnikov Dirk Weissenborn Xiaohua Zhai Thomas
Unterthiner Mostafa Dehghani Matthias Minderer Georg Heigold Sylvain Gelly et al An image is
worth 16x16 words Transformers for image recognition at scale In International Conference on Learning
Representations  2020
25Y Eidelman and I Gohberg On a new class of structured matrices Integral Equations and Operator
Theory 343293324 1999
26Jean Feydy Joan Glaunès Benjamin Charlier and Michael Bronstein Fast geometric learning with
symbolic matrices Advances in Neural Information Processing Systems  33 2020
27 Jörg Flum and Martin Grohe Parameterized Complexity Theory  Springer 2006
28Jonathan Frankle and Michael Carbin The lottery ticket hypothesis Finding sparse trainable neural
networks In International Conference on Learning Representations  2018
29Jonathan Frankle Gintare Karolina Dziugaite Daniel M Roy and Michael Carbin Stabilizing the
lottery ticket hypothesis arXiv preprint arXiv190301611  2019
30Jonathan Frankle Gintare Karolina Dziugaite Daniel Roy and Michael Carbin Linear mode connectivity
and the lottery ticket hypothesis In International Conference on Machine Learning  pages 32593269
PMLR 2020
31Karan Goel Albert Gu Chris Donahue and Christopher Ré Its raw audio generation with statespace
models In International Conference on Machine Learning ICML  2022
32 Aaron Gokaslan Vanya Cohen Pavlick Ellie and Stefanie Tellex Openwebtext corpus 2019
1233Jim Gray Surajit Chaudhuri Adam Bosworth Andrew Layman Don Reichart Murali Venkatrao
Frank Pellow and Hamid Pirahesh Data cube A relational aggregation operator generalizing groupby
crosstab and subtotals Data mining and knowledge discovery  112953 1997
34Andreas Griewank and Andrea Walther Evaluating derivatives principles and techniques of algorithmic
diﬀerentiation  SIAM 2008
35Albert Gu Tri Dao Stefano Ermon Atri Rudra and Christopher Ré Hippo Recurrent memory with
optimal polynomial projections In Advances in neural information processing systems NeurIPS  2020
36Albert Gu Isys Johnson Karan Goel Khaled Saab Tri Dao Atri Rudra and Christopher Ré Combining
recurrent convolutional and continuoustime models with linear state space layers Advances in Neural
Information Processing Systems  34 2021
37Albert Gu Karan Goel and Christopher Ré Eﬃciently modeling long sequences with structured state
spaces In The International Conference on Learning Representations ICLR  2022
38Song Han Jeﬀ Pool John Tran and William J Dally Learning both weights and connections for eﬃcient
neural networks arXiv preprint arXiv150602626  2015
39Song Han Huizi Mao and William J Dally Deep compression Compressing deep neural networks
with pruning trained quantization and huﬀman coding In International Conference on Learning
Representations  2016
40John Hennessy and David Patterson Memory hierarchy design Computer Architecture A Quantitative
Approach  pages 390525 2003
41 Sara Hooker The hardware lottery arXiv preprint arXiv200906489  2020
42Weizhe Hua Zihang Dai Hanxiao Liu and Quoc V Le Transformer quality in linear time arXiv
preprint arXiv220210447  2022
43Andrei Ivanov Nikoli Dryden Tal BenNun Shigang Li and Torsten Hoeﬂer Data movement is all
you need A case study on optimizing transformers Proceedings of Machine Learning and Systems  3
711732 2021
44Zhe Jia and Peter Van Sandt Dissecting the Ampere GPU architecture via microbenchmarking GPU
Technology Conference 2021
45Zhe Jia Marco Maggioni Benjamin Staiger and Daniele P Scarpazza Dissecting the nvidia Volta GPU
architecture via microbenchmarking arXiv preprint arXiv180406826  2018
46Zhe Jia Blake Tillman Marco Maggioni and Daniele Paolo Scarpazza Dissecting the graphcore IPU
architecture via microbenchmarking arXiv preprint arXiv191203413  2019
47Alistair EW Johnson Tom J Pollard Lu Shen Liwei H Lehman Mengling Feng Mohammad Ghassemi
Benjamin Moody Peter Szolovits Leo Anthony Celi and Roger G Mark Mimiciii a freely accessible
critical care database Scientiﬁc data  3119 2016
48Norman P Jouppi Cliﬀ Young Nishant Patil David Patterson Gaurav Agrawal Raminder Bajwa Sarah
Bates Suresh Bhatia Nan Boden Al Borchers et al Indatacenter performance analysis of a tensor
processing unit In Proceedings of the 44th annual international symposium on computer architecture 
pages 112 2017
49Thomas Kailath SunYuan Kung and Martin Morf Displacement ranks of matrices and linear equations
Journal of Mathematical Analysis and Applications  682395407 1979
50Angelos Katharopoulos Apoorv Vyas Nikolaos Pappas and François Fleuret Transformers are RNNs
Fast autoregressive transformers with linear attention In International Conference on Machine Learning 
pages 51565165 PMLR 2020
1351Nikita Kitaev Łukasz Kaiser and Anselm Levskaya Reformer The eﬃcient transformer In The
International Conference on Machine Learning ICML  2020
52Zhenzhong Lan Mingda Chen Sebastian Goodman Kevin Gimpel Piyush Sharma and Radu Soricut
Albert A lite BEDRT for selfsupervised learning of language representations In The International
Conference on Learning Representations ICLR  2020
53Mingzhen Li Yi Liu Xiaoyan Liu Qingxiao Sun Xin You Hailong Yang Zhongzhi Luan Lin Gan
Guangwen Yang and Depei Qian The deep learning compiler A comprehensive survey IEEE
Transactions on Parallel and Distributed Systems  323708727 2020
54Valerii Likhosherstov Krzysztof Choromanski Jared Davis Xingyou Song and Adrian Weller Sublinear
memory How to make performers slim arXiv preprint arXiv201211346  2020
55Ji Lin Yongming Rao Jiwen Lu and Jie Zhou Runtime neural pruning In I Guyon U V Luxburg
S Bengio H Wallach R Fergus S Vishwanathan and R Garnett editors Advances in Neural
Information Processing Systems  volume 30 Curran Associates Inc 2017
56Yinhan Liu Myle Ott Naman Goyal Jingfei Du Mandar Joshi Danqi Chen Omer Levy Mike Lewis
Luke Zettlemoyer and Veselin Stoyanov Roberta A robustly optimized bert pretraining approach
arXiv preprint arXiv190711692  2019
57Xuezhe Ma Xiang Kong Sinong Wang Chunting Zhou Jonathan May Hao Ma and Luke Zettlemoyer
Luna Linear uniﬁed nested attention Advances in Neural Information Processing Systems  34 2021
58Peter Mattson Christine Cheng Gregory Diamos Cody Coleman Paulius Micikevicius David Patterson
Hanlin Tang GuYeon Wei Peter Bailis Victor Bittorf et al Mlperf training benchmark Proceedings
of Machine Learning and Systems  2336349 2020
59Frank McSherry Michael Isard and Derek G Murray Scalability but at what fCOSTg In15th
Workshop on Hot Topics in Operating Systems HotOS XV  2015
60Maxim Milakov and Natalia Gimelshein Online normalizer calculation for softmax arXiv preprint
arXiv180502867  2018
61 NVIDIA Nvidia Tesla V100 GPU architecture 2017
62 NVIDIA Nvidia A100 tensor core GPU architecture 2020
63 NVIDIA Nvidia H100 tensor core GPU architecture 2022
64D Stott Parker Random butterﬂy transformations with applications in computational linear algebra
1995
65Adam Paszke Sam Gross Francisco Massa Adam Lerer James Bradbury Gregory Chanan Trevor
Killeen Zeming Lin Natalia Gimelshein Luca Antiga et al Pytorch An imperative style high
performance deep learning library Advances in neural information processing systems  32 2019
66Markus N Rabe and Charles Staats Selfattention does not need 𝑂¹𝑛2ºmemory arXiv preprint
arXiv211205682  2021
67Alec Radford Jeﬀrey Wu Rewon Child David Luan Dario Amodei Ilya Sutskever et al Language
models are unsupervised multitask learners OpenAI blog  189 2019
68Jack Rae and Ali Razavi Do transformers need deep longrange memory In Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics  Online July 2020 Association for
Computational Linguistics URL httpswwwaclweborganthology2020aclmain672 
69Jack W Rae Anna Potapenko Siddhant M Jayakumar and Timothy P Lillicrap Compressive trans
formers for longrange sequence modelling In The International Conference on Learning Representations
ICLR 2020
1470Jonathan RaganKelley Connelly Barnes Andrew Adams Sylvain Paris Frédo Durand and Saman
Amarasinghe Halide a language and compiler for optimizing parallelism locality and recomputation in
image processing pipelines Acm Sigplan Notices  486519530 2013
71Raghu Ramakrishnan Johannes Gehrke and Johannes Gehrke Database management systems  volume 3
McGrawHill New York 2003
72Benjamin Recht and Christopher Ré Parallel stochastic gradient algorithms for largescale matrix
completion Mathematical Programming Computation  52201226 2013
73Hongyu Ren Hanjun Dai Zihang Dai Mengjiao Yang Jure Leskovec Dale Schuurmans and Bo Dai
Combiner Full attention transformer with sparse computation cost Advances in Neural Information
Processing Systems  34 2021
74Aurko Roy Mohammad Saﬀar Ashish Vaswani and David Grangier Eﬃcient contentbased sparse
attention with routing transformers Transactions of the Association for Computational Linguistics  9
5368 2021
75 Amit Sabne XLA Compiling machine learning for peak performance 2020
76Victor Sanh Thomas Wolf and Alexander M Rush Movement pruning Adaptive sparsity by ﬁnetuning
arXiv preprint arXiv200507683  2020
77Mohammad Shoeybi Mostofa Patwary Raul Puri Patrick LeGresley Jared Casper and Bryan Catanzaro
MegatronLM Training multibillion parameter language models using model parallelism arXiv preprint
arXiv190908053  2019
78Vikas Sindhwani Tara Sainath and Sanjiv Kumar Structured transforms for smallfootprint deep
learning In Advances in Neural Information Processing Systems  pages 30883096 2015
79Sainbayar Sukhbaatar Edouard Grave Piotr Bojanowski and Armand Joulin Adaptive attention span
in transformers In Proceedings of the Annual Meeting of the Association for Computational Linguistics 
2019
80Yi Tay Mostafa Dehghani Samira Abnar Yikang Shen Dara Bahri Philip Pham Jinfeng Rao Liu
Yang Sebastian Ruder and Donald Metzler Long range arena A benchmark for eﬃcient transformers
InInternational Conference on Learning Representations  2020
81Yi Tay Mostafa Dehghani Dara Bahri and Donald Metzler Eﬃcient transformers A survey arXiv
preprint arXiv200906732  2020
82Ashish Vaswani Noam Shazeer Niki Parmar Jakob Uszkoreit Llion Jones Aidan N Gomez Łukasz
Kaiser and Illia Polosukhin Attention is all you need Advances in neural information processing
systems 30 2017
83Hongyu Wang Shuming Ma Li Dong Shaohan Huang Dongdong Zhang and Furu Wei Deepnet
Scaling transformers to 1000 layers arXiv preprint arXiv220300555  2022
84Sinong Wang Belinda Z Li Madian Khabsa Han Fang and Hao Ma Linformer Selfattention with
linear complexity arXiv preprint arXiv200604768  2020
85Samuel Williams Andrew Waterman and David Patterson Rooﬂine an insightful visual performance
model for multicore architectures Communications of the ACM  5246576 2009
86Michael E Wolf and Monica S Lam A data locality optimizing algorithm In Proceedings of the ACM
SIGPLAN 1991 conference on Programming language design and implementation  pages 3044 1991
1587Thomas Wolf Lysandre Debut Victor Sanh Julien Chaumond Clement Delangue Anthony Moi Pierric
Cistac Tim Rault Rémi Louf Morgan Funtowicz Joe Davison Sam Shleifer Patrick von Platen
Clara Ma Yacine Jernite Julien Plu Canwen Xu Teven Le Scao Sylvain Gugger Mariama Drame
Quentin Lhoest and Alexander M Rush Transformers Stateoftheart natural language processing
InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing System
Demonstrations  pages 3845 Online October 2020 Association for Computational Linguistics URL
httpswwwaclweborganthology2020emnlpdemos6 
88David P Woodruﬀ Optimal space lower bounds for all frequency moments In SODA volume 4 pages
167175 Citeseer 2004
89Felix Wu Angela Fan Alexei Baevski Yann N Dauphin and Michael Auli Pay less attention with
lightweight and dynamic convolutions In The International Conference on Learning Representations
ICLR 2019
90Yunyang Xiong Zhanpeng Zeng Rudrasis Chakraborty Mingxing Tan Glenn Fung Yin Li and Vikas
Singh Nyströmformer A nystömbased algorithm for approximating selfattention In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence AAAI Conference on Artiﬁcial Intelligence  volume 35
page 14138 2021
91Li Yuan Yunpeng Chen Tao Wang Weihao Yu Yujun Shi ZiHang Jiang Francis EH Tay Jiashi Feng
and Shuicheng Yan Tokenstotoken vit Training vision transformers from scratch on imagenet In
Proceedings of the IEEECVF International Conference on Computer Vision  pages 558567 2021
92Manzil Zaheer Guru Guruganesh Kumar Avinava Dubey Joshua Ainslie Chris Alberti Santiago
Ontanon Philip Pham Anirudh Ravula Qifan Wang Li Yang et al Big bird Transformers for longer
sequences Advances in Neural Information Processing Systems  33 2020
93Shuangfei Zhai Walter Talbott Nitish Srivastava Chen Huang Hanlin Goh Ruixiang Zhang and Josh
Susskind An attention free transformer arXiv preprint arXiv210514103  2021
94Chen Zhu Wei Ping Chaowei Xiao Mohammad Shoeybi Tom Goldstein Anima Anandkumar and
Bryan Catanzaro Longshort transformer Eﬃcient transformers for language and vision Advances in
Neural Information Processing Systems  34 2021
16A Related Work
IOAware Runtime Optimization The broad concept of optimizing for reading and writing to fastslow
memory has a long history in computer science and has been known by many names We draw the most
direct connection to the literature of analyzing IO complexity in this work  1 but concepts of memory
hierarchies are fundamental and has appeared in many forms from the working set model  21 to data
locality  86 to the Rooﬂine model of arithmetic intensity  85 to analyses of scalability  59 to standard
textbook treatments of computer architecture  40 We hope that this work encourages the community to
adopt these ideas in more parts of the deep learning stack
Eﬃcient ML Models with Structured Matrices Matrix multiply is the core computational bottle
neck of most machine learning models To reduce the computational complexity there have been numerous
approaches to learn over a more eﬃcient set of matrices These matrices are called structured matrices  which
have subquadratic  𝑜¹𝑛2ºfor dimension 𝑛𝑛 number of parameters and runtime Most common examples
of structured matrices are sparse and lowrank matrices along with fast transforms commonly encountered
in signal processing Fourier Chebyshev sinecosine orthogonal polynomials There have been several
more general classes of structured matrices proposed in machine learning Toeplitzlike  78 lowdisplacement
rank 49 quasiseparable  25 The butterﬂy pattern we use for our blocksparse attention is motivated
by the fact that butterﬂy matrices  1564 and their products have been shown to be able to express any
structured matrices with almost optimal runtime and number of parameters  1620 However even though
structured matrices are eﬃcient in theory they have not seen wide adoption since it is hard to translate their
eﬃciency to wallclock speedup since dense unconstrained matrix multiply has very optimize implementation
a phenomenon known as the hardware lottery  41 Extensions of butterﬂy matrices  1718 aimed to make
butterﬂy matrices more hardwarefriendly
Sparse Training Our blocksparse FlashAttention can be seen as a step towards making sparse model
training more eﬃcient Sparse models have seen success in compressing models for inference pruning by
sparsifyingtheweightmatrices 2338395576 Formodeltraining thelotteryticketshypothesis 282930
suggests that there are a set of small subnetworks derived from a larger dense network that performs as
well as the original dense network Out blocksparse FlashAttention can also be seen as a ﬁxed lottery
ticket in the context of attention we ﬁx the sparsity pattern to be the butterﬂy pattern through training
and observe that it performs almost as well as the dense FlashAttention on the Longrange Arena tasks
Eﬃcient Transformer Transformerbased models have become the most widelyused architecture in
natural language processing  22 and computer vision  2491 However one of their computational bottlenecks
is that their time and memory scales quadratic in the sequence length There are numerous approaches to
overcome this bottleneck including approximation with hashing ie sparse such as Reformer  51 and
Smyrf 19 and with lowrank approximation such as Performer  1254 One can even combine sparse and
lowrank approximation for better accuracy eg Longformer  3 BigBird  92 Scatterbrain  9 Longshort
transformer  94 Combiner  73 Other approaches include compressing along the sequence dimension to
attend to multiple tokens at once  52577989 One can also attend over the states from previous sequences
to help lengthen the context eg TransformerXL  14 and Compressive Transformer  69 We recommend
the survey 81 for more details
There are several lines of work on developing other modules instead of attention to model longer context
HiPPO  35 and its extensions most notably S4  313637 projects the history on a polynomial basis
allowing accurate reconstruction of the history through statespace models They combine the strengths of
CNNs eﬃcient training RNNs eﬃcient inference and continuous models robust to change in sampling
rates LambdaNetworks  2 AFT  93 and FLASH  42 are other attempts at replacing attention in the
context of image classiﬁcation and language modeling
B Algorithm Details
We ﬁrst derive the forward and backward passes of attention and show that they can be computed in a
memoryeﬃcient manner requiring extra memory linear instead of quadratic in the sequence length Though
they reduce the amount of extra memory required naively they still incur quadratic HBM accesses resulting
in slower execution speed We describe the FlashAttention algorithm to implement both the forward
17and the backward passes on GPUs that reduces HBM accesses leading to both faster runtime and smaller
memory footprint
B1 Memoryeﬃcient forward pass
The main challenge in making attention memoryeﬃcient is the softmax that couples the columns of Kand
columns of V Our approach is to compute the softmax normalization constant separately to decouple the
columns This technique  60 has been used in the literature  5166 to show that attention computation
does not need quadratic extramemory though the number of HBM accesses is still quadratic resulting in
slow runtime
For simplicity we omit here the maxshifting step during softmax The full algorithm in Appendix B3
contains all the steps
Recall that given input sequences QKV2R𝑁𝑑 we want to compute the attention output O2R𝑁𝑑
SQK2R𝑁𝑁Psoftmax¹Sº2R𝑁𝑁OPV2R𝑁𝑑
We have that 𝑆𝑖𝑗𝑞𝑇
𝑖𝑘𝑗where𝑞𝑖and𝑘𝑗are the𝑖th and𝑗th columns of QandKrespectively Deﬁne
the normalization constants of softmax
𝐿𝑖
𝑗𝑒𝑞𝑇
𝑖𝑘𝑗 1
Let𝑣𝑗be the𝑗th column of V then the𝑖th columns of the output is
𝑜𝑖𝑃𝑖V
𝑗𝑃𝑖𝑗𝑣𝑗
𝑗𝑒𝑞𝑇
𝑖𝑘𝑗
𝐿𝑖𝑣𝑗 2
We see that once 𝐿𝑖is computed we can compute 𝑜𝑖without extra memory by repeatedly summing
𝑒𝑞𝑇
𝑖𝑘𝑗
𝐿𝑖𝑣𝑗 Therefore the forward pass can be computed with 𝑂¹𝑛ºextra memory
1 Compute 𝐿𝑖for all𝑖according to Eq 1 which takes 𝑂¹𝑛ºextra memory
2 Compute 𝑜𝑖for all𝑖according to Eq 2 which takes 𝑂¹𝑑ºextra memory
B2 Memoryeﬃcient backward pass
We derive the backward pass of attention and show that it can also be computed with linear memory Rabe
and Staats 66suggests that the backward pass can be done without quadratic extra memory by applying
gradient checkpointing to the memoryeﬃcient forward pass We instead derive the backward pass explicitly
and show how it can be computed in a memoryeﬃcient manner
Suppose that there is a scalar loss function 𝜙 and let the output gradient be dO2R𝑛𝑑where dOdenotes
𝜙
O We want to compute the input gradients dQdKdV2R𝑛𝑑where dQdKdVdenote𝜙
Q𝜙
K𝜙
V
respectively
The gradient dVis easy to see Applying reversemode autodiﬀ by hand aka the chain rule we obtain
in matrix notation dVP𝑇dO Thus
𝑑𝑣𝑗
𝑖𝑃𝑖𝑗𝑑𝑜𝑖
𝑖𝑒𝑞𝑇
𝑖𝑘𝑗
𝐿𝑖𝑑𝑜𝑖 3
Since we already computed 𝐿𝑖𝑑𝑣𝑗can be computed without extra memory by repeated summing
The gradients dQanddKare a little more complicated We go through the gradients dPanddSﬁrst
From Eq 2 we have that dPdOV𝑇 and so
𝑑𝑃𝑖𝑗𝑑𝑜𝑇
𝑖𝑣𝑗
Recall that 𝑃𝑖softmax¹𝑆𝑖º Using the fact that the Jacobian of 𝑦softmax¹𝑥ºisdiag¹𝑦º𝑦𝑦𝑇 we
have that
𝑑𝑆𝑖¹diag¹𝑃𝑖º𝑃𝑖𝑃𝑇
𝑖º𝑑𝑃𝑖𝑃𝑖𝑑𝑃𝑖¹𝑃𝑇
𝑖𝑑𝑃𝑖º𝑃𝑖
18wheredenotes pointwise multiplication
Deﬁne
𝐷𝑖𝑃𝑇
𝑖𝑑𝑃𝑖
𝑗𝑒𝑞𝑇
𝑖𝑘𝑗
𝐿𝑖𝑑𝑜𝑇
𝑖𝑣𝑗𝑑𝑜𝑇
𝑖
𝑗𝑒𝑞
𝑖𝑘𝑗
𝐿𝑖𝑣𝑗𝑑𝑜𝑇
𝑖𝑜𝑖 4
then
𝑑𝑆𝑖𝑃𝑖𝑑𝑃𝑖𝐷𝑖𝑃𝑖
Hence
𝑑𝑆𝑖𝑗𝑃𝑖𝑗𝑑𝑃𝑖𝑗𝐷𝑖𝑃𝑖𝑗𝑃𝑖𝑗¹𝑑𝑃𝑖𝑗𝐷𝑖º
Now we can get the gradients dQanddK Recall that 𝑆𝑖𝑗𝑞𝑇
𝑖𝑘𝑗 so
𝑑𝑞𝑖
𝑗𝑑𝑆𝑖𝑗𝑘𝑗
𝑗𝑃𝑖𝑗¹𝑑𝑃𝑖𝑗𝐷𝑖º𝑘𝑗
𝑗𝑒𝑞𝑇
𝑖𝑘𝑗
𝐿𝑖¹𝑑𝑜𝑇
𝑖𝑣𝑗𝐷𝑖º𝑘𝑗 5
Similarly
𝑑𝑘𝑗
𝑖𝑑𝑆𝑖𝑗𝑞𝑖
𝑖𝑃𝑖𝑗¹𝑑𝑃𝑖𝑗𝐷𝑖º𝑞𝑖
𝑖𝑒𝑞𝑇
𝑖𝑘𝑗
𝐿𝑖¹𝑑𝑜𝑇
𝑖𝑣𝑗𝐷𝑖º𝑞𝑖 6
Therefore the backward pass can also be computed with 𝑂¹𝑛ºextra memory
1 Compute 𝑑𝑣𝑗for all𝑗according to Eq 3 which takes 𝑂¹𝑑ºextra memory
2 Compute 𝐷𝑖for all𝑖according to Eq 4 which takes 𝑂¹𝑛ºextra memory
3 Compute 𝑑𝑞𝑖for all𝑖according to Eq 5 which takes 𝑂¹𝑑ºextra memory
4 Compute 𝑑𝑘𝑗for all𝑗according to Eq 6 which takes 𝑂¹𝑑ºextra memory
B3 FlashAttention  Forward Pass
We describe the full details of FlashAttention forward pass Given input sequences QKV2R𝑁𝑑 we
want to compute the attention output O2R𝑁𝑑
S𝜏QK2R𝑁𝑁Smaskedmask¹𝑆º2R𝑁𝑁Psoftmax¹Smaskedº2R𝑁𝑁
Pdroppeddropout¹P𝑝dropºOPdroppedV2R𝑁𝑑
where𝜏2Ris some softmax scaling typically1p
𝑑maskis some masking function that sets some entries of
the input to1and keep other entries the same eg key padding mask when sequences in the batch dont
have the same lengths and are padded and dropout¹𝑥𝑝ºapplies dropout to 𝑥elementwise ie output𝑥
1𝑝
with probability 1𝑝and output 0 with probability 𝑝for each element 𝑥
The full algorithm is in Algorithm 2 We save the output O the softmax statistics ℓand𝑚 and the
pseudorandom number generator state Rfor the backward pass
19Algorithm 2 FlashAttention Forward Pass
Require Matrices QKV2R𝑁𝑑in HBM onchip SRAM of size 𝑀 softmax scaling constant 𝜏2R
masking function mask dropout probability 𝑝drop
1Initialize the pseudorandom number generator state Rand save to HBM
2Set block sizes 𝐵𝑐𝑀
4𝑑
𝐵𝑟min𝑀
4𝑑
𝑑
3Initialize O¹0º𝑁𝑑2R𝑁𝑑ℓ¹0º𝑁2R𝑁𝑚¹1º𝑁2R𝑁in HBM
4Divide Qinto𝑇𝑟l
𝑁
𝐵𝑟m
blocks Q1Q𝑇𝑟of size𝐵𝑟𝑑each and divide KVin to𝑇𝑐l
𝑁
𝐵𝑐m
blocks
K1K𝑇𝑐andV1V𝑇𝑐 of size𝐵𝑐𝑑each
5Divide Ointo𝑇𝑟blocks O𝑖O𝑇𝑟of size𝐵𝑟𝑑each divide ℓinto𝑇𝑟blocksℓ𝑖ℓ𝑇𝑟of size𝐵𝑟each
divide𝑚into𝑇𝑟blocks𝑚1𝑚𝑇𝑟of size𝐵𝑟each
6for1𝑗𝑇𝑐do
7Load K𝑗V𝑗from HBM to onchip SRAM
8for1𝑖𝑇𝑟do
9Load Q𝑖O𝑖ℓ𝑖𝑚𝑖from HBM to onchip SRAM
10On chip compute S𝑖𝑗𝜏Q𝑖K𝑇
𝑗2R𝐵𝑟𝐵𝑐
11On chip compute Smasked
𝑖𝑗mask¹S𝑖𝑗º
12On chip compute 𝑚𝑖𝑗rowmax¹Smasked
𝑖𝑗º2R𝐵𝑟P𝑖𝑗exp¹Smasked
𝑖𝑗𝑚𝑖𝑗º2R𝐵𝑟𝐵𝑐pointwise
ℓ𝑖𝑗rowsum¹P𝑖𝑗º2R𝐵𝑟
13On chip compute 𝑚new
𝑖max¹𝑚𝑖𝑚𝑖𝑗º2R𝐵𝑟ℓnew
𝑖𝑒𝑚𝑖𝑚new
𝑖ℓ𝑖𝑒𝑚𝑖𝑗𝑚new
𝑖ℓ𝑖𝑗2R𝐵𝑟
14On chip compute Pdropped
𝑖𝑗dropout¹P𝑖𝑗𝑝dropº
15Write O𝑖 diag¹ℓnew
𝑖º1¹diag¹ℓ𝑖º𝑒𝑚𝑖𝑚new
𝑖O𝑖𝑒𝑚𝑖𝑗𝑚new
𝑖Pdropped
𝑖𝑗V𝑗ºto HBM
16Writeℓ𝑖 ℓnew
𝑖𝑚𝑖 𝑚new
𝑖to HBM
17end for
18end for
19Return Oℓ𝑚R
B4 FlashAttention  Backward Pass
We describe the full details of FlashAttention backward pass Given input sequences QKV2R𝑁𝑑 the
output O2R𝑁𝑑 and the output gradient dO we want to compute the input gradients dQdKdV2R𝑁𝑑
We ﬁrst describe the standard attention backward pass in Algorithm 3 for completeness
Algorithm 3 Standard Attention Backward Pass
Require Matrices QKVdO2R𝑁𝑑P2R𝑁𝑁in HBM
1Load PdOby blocks from HBM compute dVPdO2R𝑁𝑑 write dVto HBM
2Load dOVby blocks from HBM compute dPdOV2R𝑁𝑁 write dPto HBM
3Read PdPfrom HBM compute dS2R𝑁𝑁where𝑑𝑆𝑖𝑗𝑃𝑖𝑗¹𝑑𝑃𝑖𝑗Í
𝑙𝑃𝑖𝑙𝑑𝑃𝑖𝑙º write dSto HBM
4Load dSandKby blocks from HBM compute dQdSK write dQto HBM
5Load dSandQby blocks from HBM compute dKdSQ write dKto HBM
6Return dQdKdV
We now make two observations about FlashAttention backward pass
1We do not need to store the dropout mask of size 𝑂¹𝑁2ºfrom the forward pass Instead we can save
the pseudorandom number generator states from the forward pass and regenerate the dropout mask
in the backward pass This allows us to only use 𝑂¹𝑁ºextra memory
2When computing the softmax gradient we use Eq 4 to compute 𝐷𝑖𝑃
𝑖𝑑𝑃𝑖without reducing over
𝑃𝑖and𝑑𝑃𝑖of size𝑁they might not ﬁt into SRAM Instead we can rewrite 𝐷𝑖𝑑𝑜
𝑖𝑜𝑖and compute
the dot product between vectors of size 𝑑
20The full FlashAttention backward pass algorithm is in Algorithm 4 Conceptually it is just a block
version of the derivation in Appendix B2
Algorithm 4 FlashAttention Backward Pass
Require Matrices QKVOdO2R𝑁𝑑in HBM vectors ℓ𝑚2R𝑁in HBM onchip SRAM of size 𝑀
softmax scaling constant 𝜏2R masking function mask dropout probability 𝑝drop pseudorandom
number generator state Rfrom the forward pass
1Set the pseudorandom number generator state to R
2Set block sizes 𝐵𝑐𝑀
4𝑑
𝐵𝑟min𝑀
4𝑑
𝑑
3Divide Qinto𝑇𝑟l
𝑁
𝐵𝑟m
blocks Q1Q𝑇𝑟of size𝐵𝑟𝑑each and divide KVin to𝑇𝑐l
𝑁
𝐵𝑐m
blocks
K1K𝑇𝑐andV1V𝑇𝑐 of size𝐵𝑐𝑑each
4Divide Ointo𝑇𝑟blocks O𝑖O𝑇𝑟of size𝐵𝑟𝑑each divide dOinto𝑇𝑟blocks dO𝑖dO𝑇𝑟of size
𝐵𝑟𝑑each divide ℓinto𝑇𝑟blocksℓ𝑖ℓ𝑇𝑟of size𝐵𝑟each divide 𝑚into𝑇𝑟blocks𝑚1𝑚𝑇𝑟of size
𝐵𝑟each
5Initialize dQ¹0º𝑁𝑑in HBM and divide it into 𝑇𝑟blocks dQ1dQ𝑇𝑟of size𝐵𝑟𝑑each Initialize
dK¹0º𝑁𝑑dV¹0º𝑁𝑑in HBM and divide dKdVin to𝑇𝑐blocks dK1dK𝑇𝑐anddV1dV𝑇𝑐
of size𝐵𝑐𝑑each
6for1𝑗𝑇𝑐do
7Load K𝑗V𝑗from HBM to onchip SRAM
8Initialize dK𝑗¹0º𝐵𝑐𝑑dV𝑗¹0º𝐵𝑐𝑑on SRAM
9for1𝑖𝑇𝑟do
10Load Q𝑖O𝑖dO𝑖dQ𝑖ℓ𝑖𝑚𝑖from HBM to onchip SRAM
11On chip compute S𝑖𝑗𝜏Q𝑖K𝑇
𝑗2R𝐵𝑟𝐵𝑐
12On chip compute Smasked
𝑖𝑗mask¹S𝑖𝑗º
13On chip compute P𝑖𝑗diag¹𝑙𝑖º1exp¹Smasked
𝑖𝑗𝑚𝑖º2R𝐵𝑟𝐵𝑐
14On chip compute dropout mask Z𝑖𝑗2R𝐵𝑟𝐵𝑐where each entry has value1
1𝑝dropwith probability
1𝑝dropand value 0 with probability 𝑝drop
15On chip compute Pdropped
𝑖𝑗P𝑖𝑗Z𝑖𝑗pointwise multiply
16On chip compute dV𝑗 dV𝑗¹Pdropped
𝑖𝑗ºdO𝑖2R𝐵𝑐𝑑
17On chip compute dPdropped
𝑖𝑗dO𝑖V
𝑗2R𝐵𝑟𝐵𝑐
18On chip compute dP𝑖𝑗dPdropped
𝑖𝑗Z𝑖𝑗pointwise multiply
19On chip compute 𝐷𝑖rowsum¹dO𝑖O𝑖º2R𝐵𝑟
20On chip compute dS𝑖𝑗P𝑖𝑗¹dP𝑖𝑗𝐷𝑖º2R𝐵𝑟𝐵𝑐
21Write dQ𝑖 dQ𝑖𝜏dS𝑖𝑗K𝑗2R𝐵𝑟𝑑to HBM
22On chip compute dK𝑗 dK𝑗𝜏dS
𝑖𝑗Q𝑖2R𝐵𝑐𝑑
23end for
24Write dK𝑗 dK𝑗dV𝑗 dV𝑗to HBM
25end for
26Return dQdKdV
We see that similar to the forward pass the backward pass performs 𝑂¹𝑁2ºFLOPs and only requires
𝑂¹𝑁ºextra memory beyond inputs output output gradient and input gradients
We analyze the IOcomplexity of the backward pass similar to the forward pass Theorem 2
Theorem5 Let𝑁be the sequence length 𝑑be the head dimension and 𝑀be size of SRAM with 𝑑𝑀𝑁𝑑
Standard attention Algorithm 0 backward pass requires Θ¹𝑁𝑑𝑁2ºHBM accesses while FlashAttention
backward pass Algorithm 4 requires Θ¹𝑁2𝑑2𝑀1ºHBM accesses
The proof is in Appendix C
21B5 Comparison with Rabe and Staats 66
We describe here some similarities and diﬀerences between our FlashAttention algorithm and the algorithm
of Rabe and Staats 66
Conceptually both FlashAttention and Rabe and Staats 66operate on blocks of the attention matrix
using the wellestablished technique of tiling or softmax scaling  5160 To reduce the memory footprint
both methods avoid storing the large attention matrix in the forward pass and recompute it in the backward
pass
The ﬁrst major diﬀerence is that Rabe and Staats 66focuses on the reducing the total memory footprint
maximum amount of GPU memory required while FlashAttention focuses on reducing memory accesses
the number of memory readswrites As mentioned in Section 2 the amount of memory access is the
primary determining factor of runtime Reducing memory accesses also necessarily reduces the total amount
of memory required eg if an operation incurs 𝐴memory accesses then its total memory requirement is at
most𝐴 As a result FlashAttention is faster than standard attention 24  while Rabe and Staats 66
is around the same speed or slightly slower than standard attention In terms of total memory required both
methods oﬀer substantial memory saving
The second diﬀerence between the two methods is the way information is summarized from each block
to pass to the next block Rabe and Staats 66summarizes each block with its temporary output along
with the softmax normalization statistics At the end of the forward pass the temporary outputs of all the
blocks are combined using the statistics to produce the ﬁnal output FlashAttention instead incrementally
updates the output Algorithm 1 line 12 after processing each block so only one copy of the output is needed
instead of𝐾copies for𝐾blocks This means that FlashAttention has smaller total memory requirement
compared to Rabe and Staats 66
The ﬁnal major diﬀerence is the way the backward pass is computed Rabe and Staats 66uses gradient
checkpointing to recompute the attention matrix and the temporary output of each block FlashAttention
instead simpliﬁes the backward pass analytically Appendices B2 and B4 It only recomputes the attention
matrix and does not recompute the temporary output of each block This reduces the memory requirement
for the backward pass and yields speedup
C Proofs
Proof of Theorem 1 We ﬁrst count the number of FLOPs and extra memory required
The dominating FLOPs are from matrix multiplication In the inner loop Algorithm 1 line 9 we
compute Q𝑖K
𝑗2R𝐵𝑟𝐵𝑐forQ𝑖2R𝐵𝑟𝑑andK𝑗2R𝐵𝑐𝑑 which takes 𝑂¹𝐵𝑟𝐵𝑐𝑑ºFLOPs We also compute
Algorithm 1 line 12 P𝑖𝑗V𝑗2R𝐵𝑟𝑑forP𝑖𝑗2R𝐵𝑟𝐵𝑐andV𝑗2R𝐵𝑐𝑑 which takes 𝑂¹𝐵𝑟𝐵𝑐𝑑ºFLOPs We
execute the inner loops 𝑇𝑐𝑇𝑟l
𝑁
𝐵𝑐ml
𝑁
𝐵𝑟m
times Therefore the total number of FLOPs is
𝑂𝑁2
𝐵𝑐𝐵𝑟𝐵𝑟𝐵𝑐𝑑
𝑂¹𝑁2𝑑º
In terms of extra memory required we see that we need 𝑂¹𝑁ºmemory to store the statistics ¹ℓ𝑚º
We now prove the algorithms correctness by induction on 𝑗for0𝑗𝑇𝑐 Let K𝑗2R𝑗𝐵𝑐𝑑be the
ﬁrst𝑗𝐵𝑐rows of K and similarly V𝑗2R𝑗𝐵𝑐𝑑the the ﬁrst 𝑗𝐵𝑐rows of V Let S𝑗QK
𝑗2R𝑁𝑗𝐵𝑐 and
P𝑗softmax¹S𝑗º2R𝑁𝑗𝐵𝑐softmax applied rowwise Let 𝑚𝑗ℓ¹𝑗ºO¹𝑗ºbe the values of 𝑚ℓOin HBM
after the𝑗th iteration of the outer loop Algorithm 1 line 5 Note that these values of 𝑚ℓOare updated
after each iteration of the outer loop We want to show that after the 𝑗th iteration of the outer loop we
have computed in HBM
𝑚¹𝑗ºrowmax¹S𝑗º2R𝑁 ℓ¹𝑗ºrowsum¹exp¹S𝑗𝑚¹𝑗ººº2R𝑁O¹𝑗ºP𝑗V𝑗2R𝑁𝑑
Based on our initialization Algorithm 1 line 2 this claim is true for 𝑗0ie before the any iteration
of the outer loop is executed Suppose that the claim holds for some 𝑗0𝑇𝑐1 We want to show that
the claim also holds for 𝑗1 Indeed when we update the statistics in the inner loop Algorithm 1 line 10
22on the¹𝑗1ºth iteration of the outer loop we update 𝑚¹𝑗1ºmax¹𝑚¹𝑗º𝑚ºwhere 𝑚2R𝑁is the rowmax
ofS𝑗𝑗1 the slice of Sfrom column 𝑗𝐵𝑐to column¹𝑗1º𝐵𝑐1 This implies that
𝑚¹𝑗1ºrowmax¹S𝑗1º2R𝑁
Similarly we update
ℓ¹𝑗1º𝑒𝑚¹𝑗º𝑚¹𝑗1ºℓ¹𝑗º𝑒𝑚𝑚¹𝑗1ºℓ
where ℓrowsum¹exp¹S𝑗𝑗1𝑚ºº2R𝑁 By the same algebraic manipulation in Section 31 we obtain
ℓ¹𝑗1ºrowsum¹exp¹S𝑗1𝑚¹𝑗1ººº2R𝑁
LetV𝑗𝑗1be the slice of Vfrom column 𝑗𝐵𝑐to column¹𝑗1º𝐵𝑐1 we also update
O¹𝑗1ºdiag¹ℓ¹𝑗1ºº1¹diag¹ℓ¹𝑗ºº𝑒𝑚¹𝑗º𝑚¹𝑗1ºO¹𝑗º𝑒𝑚𝑚¹𝑗1ºexp¹S𝑗𝑗1𝑚ºV𝑗𝑗1º
diag¹ℓ¹𝑗1ºº1¹diag¹ℓ¹𝑗ºº𝑒𝑚¹𝑗º𝑚¹𝑗1ºP𝑗V𝑗𝑒𝑚¹𝑗1ºexp¹S𝑗𝑗1ºV𝑗𝑗1º
diag¹ℓ¹𝑗1ºº1¹diag¹ℓ¹𝑗ºº𝑒𝑚¹𝑗º𝑚¹𝑗1ºdiag¹ℓ¹𝑗ººexp¹S𝑗𝑚¹𝑗ººV𝑗𝑒𝑚¹𝑗1ºexp¹S𝑗𝑗1ºV𝑗𝑗1º
diag¹ℓ¹𝑗1ºº1¹𝑒𝑚¹𝑗1ºexp¹S𝑗ºV𝑗𝑒𝑚¹𝑗1ºexp¹S𝑗𝑗1ºV𝑗𝑗1º
diag¹ℓ¹𝑗1ºº1¹exp¹S𝑗𝑚¹𝑗1ººV𝑗exp¹S𝑗𝑗1𝑚¹𝑗1ººV𝑗𝑗1º
diag¹ℓ¹𝑗1ºº1
expS𝑗S𝑗𝑗1
𝑚¹𝑗1ºV𝑗
V𝑗𝑗1
softmax¹S𝑗1ºV𝑗1
We then see that the claim is also true for 𝑗1 By induction the claim is true for all 𝑗0𝑇𝑐
When𝑗𝑇𝑐 we conclude that the ﬁnal value of Oin HBM is softmax¹SºVsoftmax¹QKºV

Proof of Theorem 2 We ﬁrst analyze the IO complexity of standard attention implementation The inputs
QKV2R𝑁𝑑reside in HBM and the at the end of the algorithm the output O2R𝑁𝑑is written to HBM
In the ﬁrst step of computing the matrix multiply SQK the inputs QKare read from HBM and the
output S2R𝑁𝑁is written to HBM Algorithm 0 line 1 This incurs Θ¹𝑁𝑑𝑁2ºHBM accesses
In the second step of computing Psoftmax¹Sº the input Sis read from HBM and the output Pis
written to HBM Algorithm 0 line 2 This incurs Θ¹𝑁2ºHBM accesses
In the last step of computing OPV the inputs PVare read from global memory and the output Ois
written to HBM Algorithm 0 line 3 This incurs Θ¹𝑁𝑑𝑁2ºHBM accesses
Overall standard attention implementation requires Θ¹𝑁𝑑𝑁2ºglobal memory accesses
We now analyze the IO complexity of streaming attention
Following Algorithm 1 we see that each element of KandVis loaded from HBM once Algorithm 1
line 6 We make 𝑇𝑐passes over QandO each pass loading all of Qand all of Oto HBM Algorithm 1
line 8 Therefore the number of HBM accesses is Θ¹𝑁𝑑𝑁𝑑𝑇𝑐ºΘ¹𝑁𝑑𝑇𝑐º
We derive the conditions on the block sizes 𝐵𝑐and𝐵𝑟 We need the blocks K𝑗andV𝑗of size𝐵𝑐𝑑to ﬁt
into onchip memory which translates to
𝐵𝑐𝑑𝑂¹𝑀º𝐵𝑐𝑂𝑀
𝑑

Similarly we need the blocks Q𝑖O𝑖of size𝐵𝑟𝑑to ﬁt into onchip memory which translates to
𝐵𝑟𝑑𝑂¹𝑀º𝐵𝑟𝑂𝑀
𝑑

Finally we need the block S𝑖𝑗of size𝐵𝑟𝐵𝑐to ﬁt into onchip memory which translates to
𝐵𝑟𝐵𝑐𝑂¹𝑀º
23We therefore set
𝐵𝑐Θ𝑀
𝑑
 𝐵𝑟Θ
min𝑀
𝑑𝑀
𝐵𝑐
Θ
min𝑀
𝑑𝑑

We then have
𝑇𝑐𝑁
𝐵𝑐Θ𝑁𝑑
𝑀

As a result the number of HBM accesses is
Θ¹𝑁𝑑𝑇𝑐ºΘ𝑁2𝑑2
𝑀


Proof of Proposition 3 For contradiction suppose that there exists an algorithm that computes exact
attention where the number for HBM access for all 𝑀2𝑑𝑁𝑑¼is
𝑜𝑁2𝑑2
𝑀

In the regime of 𝑀Θ¹𝑁𝑑º this results in the number of HBM accesses
𝑜𝑁2𝑑2
𝑁𝑑
𝑜¹𝑁𝑑º
However the input to attention matrices QKV and the output Ohave size𝑁𝑑and they start out being
in HBM so if the algorithm computes exact attention it must incur at least Ω¹𝑁𝑑ºHBM accesses This is a
contradiction 
Proof of Theorem 5 The IO complexity of the attention backward is very similar to the IO complexity of
the attention forward Theorem 2 Here we provide a sketch of the proof
We ﬁrst analyze the IO complexity of standard attention backward pass The inputs QKVdO2R𝑁𝑑
reside in HBM and the at the end of the algorithm the outputs dQdKdV2R𝑁𝑑are written to HBM
At each step of the standard attention backward pass one needs to load inputs of size 𝑁𝑑or𝑁2from
HBM and needs to write the outputs of size 𝑁2or𝑁𝑑to HBM This incurs Θ¹𝑁𝑑𝑁2ºHBM accesses
We now analyze the IO complexity of FlashAttention backward pass
Similar to Theorem 2 we see that each element of KandVis loaded from HBM once Each element of
dKanddVis only written to HBM once We make 𝑇𝑐passes over QOdO each pass loading all of QOdO
to HBM We also make 𝑇𝑐passes over dQ each pass readingwriting all of dQfromto HBM Therefore the
number of HBM accesses is Θ¹𝑁𝑑𝑁𝑑𝑇𝑐ºΘ¹𝑁𝑑𝑇𝑐º
As in the proof of Theorem 2 the constraints on the block sizes are that
𝐵𝑐Θ𝑀
𝑑
 𝐵𝑟Θ
min𝑀
𝑑𝑑

We then have
𝑇𝑐𝑁
𝐵𝑐Θ𝑁𝑑
𝑀

As a result the number of HBM accesses is
Θ¹𝑁𝑑𝑇𝑐ºΘ𝑁2𝑑2
𝑀


24Algorithm 5 BlockSparse FlashAttention Forward Pass
Require Matrices QKV2R𝑁𝑑in HBM onchip SRAM of size 𝑀 softmax scaling constant 𝜏2R
masking function mask dropout probability 𝑝drop block sizes 𝐵𝑐𝑀
4𝑑
𝐵𝑟min𝑀
4𝑑
𝑑 block
sparsity mask 𝑀2f01g𝑁𝐵𝑟𝑁𝐵𝑐
1Initialize the pseudorandom number generator state Rand save to HBM
2Initialize O¹0º𝑁𝑑2R𝑁𝑑ℓ¹0º𝑁2R𝑁𝑚¹1º𝑁2R𝑁in HBM
3Divide Qinto𝑇𝑟l
𝑁
𝐵𝑟m
blocks Q1Q𝑇𝑟of size𝐵𝑟𝑑each and divide KVin to𝑇𝑐l
𝑁
𝐵𝑐m
blocks
K1K𝑇𝑐andV1V𝑇𝑐 of size𝐵𝑐𝑑each
4Divide Ointo𝑇𝑟blocks O𝑖O𝑇𝑟of size𝐵𝑟𝑑each divide ℓinto𝑇𝑟blocksℓ𝑖ℓ𝑇𝑟of size𝐵𝑟each
divide𝑚into𝑇𝑟blocks𝑚1𝑚𝑇𝑟of size𝐵𝑟each
5for1𝑗𝑇𝑐do
6Load K𝑗V𝑗from HBM to onchip SRAM
7for1𝑖𝑇𝑟do
8if𝑀𝑖𝑗0then
9 Load Q𝑖O𝑖ℓ𝑖𝑚𝑖from HBM to onchip SRAM
10 On chip compute S𝑖𝑗𝜏Q𝑖K𝑇
𝑗2R𝐵𝑟𝐵𝑐
11 On chip compute Smasked
𝑖𝑗mask¹S𝑖𝑗º
12 On chip compute 𝑚𝑖𝑗rowmax¹Smasked
𝑖𝑗º2R𝐵𝑟P𝑖𝑗exp¹Smasked
𝑖𝑗𝑚𝑖𝑗º2R𝐵𝑟𝐵𝑐pointwise
ℓ𝑖𝑗rowsum¹P𝑖𝑗º2R𝐵𝑟
13 On chip compute 𝑚new
𝑖max¹𝑚𝑖𝑚𝑖𝑗º2R𝐵𝑟ℓnew
𝑖𝑒𝑚𝑖𝑚new
𝑖ℓ𝑖𝑒𝑚𝑖𝑗𝑚new
𝑖ℓ𝑖𝑗2R𝐵𝑟
14 On chip compute Pdropped
𝑖𝑗dropout¹P𝑖𝑗𝑝dropº
15 Write O𝑖 diag¹ℓnew
𝑖º1¹diag¹ℓ𝑖º𝑒𝑚𝑖𝑚new
𝑖O𝑖𝑒𝑚𝑖𝑗𝑚new
𝑖Pdropped
𝑖𝑗V𝑗ºto HBM
16 Writeℓ𝑖 ℓnew
𝑖𝑚𝑖 𝑚new
𝑖to HBM
17end if
18end for
19end for
20Return Oℓ𝑚R
D Extension Details
D1 Blocksparse FlashAttention
We describe the full blocksparse FlashAttention algorithm in Algorithm 5 The algorithm is identical
to Algorithm 2 except that we skip zero blocks
We prove the IOcomplexity of blocksparse FlashAttention 
Proof of Proposition 4 The proof is very similar to the proof of Theorem 2 For the blocksparse case notice
that we only need to load blocks corresponding to nonzero blocks As a result the number of HBM accesses
are scaled by 𝑠 the fraction of nonzero blocks in the blocksparsity mask However for small values of 𝑠 we
would still need to write the result O2R𝑁𝑑 Therefore the number of HBM accesses is
Θ
𝑁𝑑𝑁2𝑑2
𝑀𝑠


D2 Potential Extensions
We discuss here a few potential extensions of the IOaware approach to speed up deep learning training
MultiGPU Attention Large language models are trained on hundreds or thousands of GPUs and
one typically splits the attention computation between 48 GPUs on the same node  77 This introduces
another level of memory hierarchy beside GPU SRAM and GPU HBM we also have the HBM of other
25GPUs For very long sequences the diﬀerent GPUs on the same node can cooperate to compute attention by
taking into account the asymmetry of diﬀerent levels of memory hierarchy
Sparse MLP layers Typical dense MLP layers are computebound and not memorybound To improve
their eﬃciency MLP layers with sparse weight matrices can be used  17 However many sparse MLP layers
are instead memorybound and their speedup is often not proportional to the sparsity We believe that an
IOaware implementation can alleviate this issue and realize the beneﬁts of sparsity We are excited about
future work in this direction to reduce the computational requirement of large models and improve their
wallblock runtime
Kernel machine learning Our approach in FlashAttention relies on the fact that the 𝑁𝑁
attention matrix is a function of a lowrank matrix QKof rank𝑑𝑁 As a result we can repeatedly
load the inputs QKand recompute the block of the attention matrix that we need signiﬁcantly reducing
HBM access As similar scenario happens in kernel machine learning each element 𝐾𝑖𝑗of the𝑁𝑁kernel
matrix Kis a function of two vectors of size 𝑑𝑁 as it measures the similarity between two datapoints 𝑥𝑖
and𝑥𝑗 The KeOps library  826 is a successful example of how reducing memory readswrites can speed up
kernel operations We hope that this will motivate kernel methods that focus more on reducing IOs instead
of just FLOPs
E Full Experimental Results
E1 BERT
We train BERTlarge following the training procedure and hyperparameters of the reference MLPerf 11
implementation In particular we use the LAMB optimizer with learning rate 375e3 with batch size 448
trained for at most 7100 steps The training is stopped once the validation accuracy for masked language
modeling reaches the target 720 and the wallclock runtime is measured We train with FP16 precision
using Apex AMP with O2 optimization level
We compare our results with the reported training speed from Nvidia that was submitted to MLPerf 11
Table 1
We use the same train  validation data split provided by MLPerf 11 reference implementation In
particular we evaluate on the same 10000 validation examples as the baseline from Nvidia
We train the model on 8 A10080GB GPUs Each training run takes between 16 and 19 minutes and we
average the results of 10 runs
E2 GPT2
We use the standard implementations of GPT2  67 from Huggingface transformers library and from
Nvidias MegatronLM repo We follow the training recipe of the MegatronLM repo
We use an eﬀective batch size of 512 and use gradient accumulation to ﬁt into available GPU memory
We use the AdamW optimizer with learning rate 6e4 for GPT2 small and 15e4 for GPT2 medium and
weight decay of 01 All models are trained with the same hyperparameters for 400K steps We run all
implementations with mixedprecision training PyTorch AMP
We use the Openwebtext dataset with the GPT2 BPE tokenizer We randomly select 05 of the dataset
as the validation set with the rest being used as training set This random selection of validation set is done
once and all models are evaluated on the same validation set
We train the model on 8 A10040GB GPUs and we measure the wallclock training time Training
GPT2 small takes between 2795 days and training GPT2 medium takes between 69210 days Table 2
In Fig 4 we plot of the validation perplexity throughout training of GPT2 smallmedium using either
HuggingFace implementation or our FlashAttention implementation We see that FlashAttention be
haves the same as the baseline implementation and the validation perplexity curves of the two implementations
almost lie on top of each other
Long Document Classiﬁcation For MIMICIII and ECtHR we follow the hyperparameters of Dai et al
13
26100k 200k 300k
Training steps1015202530Val perplexityGPT2small HuggingFace
GPT2small FlashAttention
GPT2medium HuggingFace
GPT2medium FlashAttentionFigure 4 Validation perplexity of GPT2 smallmedium using two implementations We conﬁrm that
FlashAttention yields the same validation curves as the baseline implementation from HuggingFace
E3 LRA details
We follow the hyperparameters from the Longrange arena paper  80 the Longrange arena repo  https
githubcomgoogleresearchlongrangearena  and the Nyströmformer reproduction  90 To be
generous to the baseline methods if we are unable to reproduce the performance of any baseline for any of
the ﬁve tasks we report the better performance from Tay et al 80or Xiong et al 90for that baseline on
that task
After hyperparameter tuning almost all of the attention methods achieve similar accuracy on all of the
ﬁve LRA tasks
We run all methods with mixedprecision training except for Performer not stable with mixed precision
and Local Attention implementation does not support FP16
To calculate the overall wallclocktime speedup we take the geometric mean of the wallclocktime speedup
of each of the ﬁve tasks
PathX For PathX and Path256 we follow the hyperparameters from the PathFinder32 experiments
from the longrange arena paper 80 For both we ﬁrst pretrain a model on Path64 We take the checkpoint
after 200 epochs upsample its positional embedding we duplicate the positional embeddings gridwise in
space and ﬁnetune it on the downstream task for 200 epochs with one epoch of linear warmup and cosine
decay of the learning rate For PathX we take the best performing checkpoint according to val accuracy
and additionally ﬁnetune it for 200 epochs with the same warmup and learning rate this adds roughly 4
points of accuracy to FlashAttention for PathX but the model starts overﬁtting afterwards
E4 Comparison with Apex FMHA
We compare our methodimplementation with Apex FMHA  httpsgithubcomNVIDIAapextree
masterapexcontribcsrcfmha 
When we started this project Apex FMHA was the fastest implementation of attention that we knew
of tailored for short sequences of length at most 512 In fact almost all MLPerf submissions for BERT
training benchmark running on Nvidia GPUs use FMHA for their model code as of MLPerf 11  58 Since
27Table 7 Runtime ms of FlashAttention compared to FMHA by sequence length with masking and dropout
measured on an A100SXM440GB GPU Batch size 64 16 heads head dimension 64 ie BERTlarge size
Attention Method 128 256 512
Apex FMHA forward 010 029 114
FlashAttention forward 008 022 081
Apex FMHA backward 017 052 181
FlashAttention backward 020 053 200
Apex FMHA forward  backward 027081 295
FlashAttention forward  backward 028 075 281
FMHA targets BERT models it only supports head dimension 64 and only runs on A100 GPUs FMHA
fuses the attention computation dropout¹softmax¹mask¹QKºººVinto one CUDA kernel In the forward
pass it stores the attention matrix softmax¹mask¹QK𝑇ººto HBM to be used in gradient computation As a
result it does not oﬀer substantial memory saving though for shorter sequences memory footprint is often
not a primary concern
We use FMHA code as a starting point and apply two wellestablished techniques tiling and recomputa
tion to deal with long sequences and to save memory as mentioned in Section 3 As a result we can support
much longer sequences eg up to length 64K We also support more head dimensions 16 32 64 128 and
broader GPU types all Turing and Ampere GPUs at the time of writing
In Table 7 we compare the performance of FlashAttention and Apex FMHA for short sequences as
FMHA only supports sequence length at most 512 Generally FlashAttention is slightly faster than
FMHA in the forward pass and slightly slower than FMHA in the backward pass This is because we do not
store the attention matrix in the forward pass and recompute it in the backward pass Compared to FMHA
the overall runtime of FlashAttention is about 4 slower for sequence length 128 8 faster for sequence
length 256 and 5 faster for sequence length 512
E5 Speedup On Diﬀerent Hardware and Conﬁgurations
Speedup varies between diﬀerent types of GPU types and generations depending on HBM bandwidth and
SRAM size In this section we proﬁle FlashAttention speedup on diﬀerent GPUs and conﬁgurations
Figure 5 Speedup over standard PyTorch attention at diﬀerent sequence lengths on A100
A100Figure 5 shows speedup on an A100 GPU with batch size 8 head dimension 64 and 12 attention
heads across diﬀerent sequence lengths We generally see 24 speedup and we see more speedup when
using dropout and masking due to kernel fusion
28Figure 6 Speedup over standard PyTorch attention at diﬀerent sequence lengths on A100 with head
dimension 128
A100 Head Dimension 128 Speedup also changes when we increase the head dimension Each block
requires more memory so we need to use smaller block sizes to ﬁt into SRAM Figure 6 shows speedup with
head dimension 128 on an A100 batch size 16 12 heads We see less speedup overallbut we can still see
signiﬁcant speedup up to 3  with a causal mask where half the blocks are masked out
Figure 7 Speedup over standard PyTorch attention at diﬀerent sequence lengths on RTX 3090
RTX 3090 Figure 7 shows speedup on an RTX 3090 GPU Here we use batch size 12 with 12 attention
heads We observe slightly higher speedups on the RTX 3090 between 2545  since the memory bandwidth
on an RTX 3090 is lower than on an A100 roughly 900 GBs vs 15 TBs
T4Figure 8 shows speedup on a T4 GPU T4 SRAM is smaller than A100 so we need to make the block
sizes smaller in FlashAttention  As a result we observe less speedup on T4 which matches the IO
complexity analysis in Section 32 T4 GPUs are commonly used for inference so we also report speedup on
the forward pass only
29Figure 8 Speedup over standard PyTorch attention at diﬀerent sequence lengths on T4 TopCombined
forward pass  backward pass Bottom Forward pass only
E6 Full Benchmarking Results
We report the full benchmarking results and experimental details on A100
Baselines We compare against reference implementations for exact attention from PyTorchHuggingFace
and Megatron approximate attention and sparse attention For approximate attention we compare against
reference implementations of Reformer  51 Local Attention  68 Linformer Attention  84 Smyrf  19 and
LongShortFormer LSFormer  94 For sparse attention we compare against reference implementations of
BlockSparse Attention form OpenAI  11 Longformer 3 and BigBird Attention  92 For the approximate
and sparse attention we use a compression ratio of 18 or a compressed sequence length of 256 whichever is
smaller
Setup We measure runtime and memory usage of the attention computation with 8 heads of dimension 64
and batch size 16 on a machine with one A100 GPU with 40 GB of GPU HBM We vary sequence length
in our experiments We compute attention on random vectors for QK and Vwe do not measure the
projection from the hidden layer For dropout we use dropout 01 for masking we use a padding mask
with uniformlyrandom mask lengths between the total sequence length and the total sequence length minus
20 To measure runtime we take the average of 100 measurements of the attention call We only measure
memory footprint once since it does not vary between runs
30Table 8 Pointers to results tables
Dropout Masking Pass Table
Yes Yes Forward Table 9
Yes Yes Backward Table 10
Yes Yes Combined Table 11
No Yes Forward Table 12
No Yes Backward Table 13
No Yes Combined Table 14
Yes No Forward Table 15
Yes No Backward Table 16
Yes No Combined Table 17
No No Forward Table 18
No No Backward Table 19
No No Combined Table 20
No No Memory Usage Combined Table 21
Table 9 Forward pass runtime ms of various exactapproximatesparse attention mechanisms by sequence length
with dropout and masking  Best in bold second best underlined 
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 036 034 078 254 933 3633    
Megatron 040 040 110 365 1619     
Reformer 203 315 567 1102 2259 4614 9738 21213  
Local Attention 083 086 101 220 713 1432 2860 5779 11767 
Linformer 067 052 069 071 165 318 615 1216 2417 5239
Smyrf 227 234 391 744 1471 2922 5827 11641  
LSformer 118 127 134 338 1140 2255 4495 8976 17966 
Block Sparse 112 111 213 277 695 2091    
Longformer 122 114 108 195 572 1298    
BigBird 113 112 112 177 603 1368    
FlashAttention 004 006 021 082 285 1041 4174 16719 67076 268235
BlockSparse FlashAttention 006 006 006 012 044 086 170 329 655 1334
We report timing results on the forward pass backward pass and combined forward  backward pass
We measure each method with and without dropout masking or bothexcept for Block Sparse Longformer
and BigBird These methods did not successfully run the backward pass with masking due to a bug in
external libraries so we measured them without masking to be generous We use FP16 for all measurements
except for Local Attention whose implementation only supports FP32
For each baseline we increase sequence length until it runs out of memory on the GPU except for the
following exceptions The Megatron implementation does not support sequence lengths longer than 2048
BlockSparse OpenAI does not support sequence lengths longer than 4096 Longformer and BigBird do not
support sequence lengths longer than 8092
We measure memory usage on the combined forward  backward pass without dropout or masking
Results Table 8 summarizes all the experimental conﬁgurations and contains pointers to the results tables
31Table 10 Backward pass runtime ms of various exactapproximatesparse attention mechanisms by sequence length
with dropout and masking  Best in bold second best underlined 
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 037 049 166 581 2232 8767    
Megatron 035 032 077 242 843     
Reformer 237 459 891 1768 3513 7005 14001   
Local Attention 055 062 149 403 1378 2761 5520 11027 22140 
Linformer 089 080 081 093 248 475 929 1827 3653 
Smyrf 141 283 543 1072 2125 4231 8448 16895  
LSformer 175 176 301 750 2007 3908 7639 15082  
Block Sparse 129 128 218 304 727 2116    
Longformer 127 131 129 204 524 1074 2595   
BigBird 133 128 132 181 555 1144 2745   
FlashAttention 030 026 068 202 684 2689 10570 41896 166689 666044
BlockSparse FlashAttention 030 027 029 059 150 294 582 1185 2398 4761
Table 11 Forward pass  backward pass runtime ms of various exactapproximatesparse attention mechanisms by
sequence length with dropout and masking  Best in bold second best underlined 
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 084 086 235 829 3175 12419    
Megatron 087 089 133 421 1650     
Reformer 430 776 1460 2874 5779 11634 23757   
Local Attention 140 160 206 606 2094 4201 8408 16848 33945 
Linformer 157 149 155 160 419 804 1571 3092 6147 
Smyrf 341 508 935 1818 3603 7168 14304 28587  
LSformer 308 310 426 1090 3159 6172 12151 24118  
Block Sparse 254 252 371 544 1329 3919    
Longformer 247 249 251 310 1039 2249 6044   
BigBird 251 249 252 340 1097 2389 6328   
FlashAttention 043 041 095 255 956 3749 14775 58661 233911 934130
BlockSparse FlashAttention 044 044 045 089 195 412 764 1660 3273 6411
Table 12 Forward pass runtime ms of various exactapproximatesparse attention mechanisms by sequence length
with masking  Best in bold second best underlined 
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 030 030 063 193 708 2745 11290   
Megatron 045 041 043 152 580     
Reformer 187 300 537 1043 2140 4383 9280 20324  
Local Attention 070 081 102 209 664 1334 2677 5402 11011 
Linformer 063 050 067 065 136 260 504 992 1969 4347
Smyrf 238 232 376 716 1414 2809 5598 11173  
LSformer 122 129 144 328 1099 2172 4329 8632 17276 
Block Sparse 096 104 166 216 541 1615    
Longformer 099 098 099 156 479 1107 3298   
BigBird 096 102 102 148 505 1159 3416   
FlashAttention 003 004 017 068 228 840 3355 13414 53750 215088
BlockSparse FlashAttention 005 004 005 011 035 068 133 254 534 1073
Table 13 Backward pass runtime ms of various exactapproximatesparse attention mechanisms by sequence length
with masking  Best in bold second best underlined 
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 044 046 153 533 2034 7987    
Megatron 029 031 065 195 649     
Reformer 231 447 868 1720 3414 6809 13602   
Local Attention 051 062 130 381 1333 2672 5341 10682 21415 
Linformer 076 081 094 087 224 425 835 1638 3267 7211
Smyrf 134 277 530 1046 2073 4127 8241 16486  
LSformer 166 161 309 742 1968 3835 7492 14786  
Block Sparse 124 125 204 291 678 1967    
Longformer 127 123 124 185 499 1021 2489   
BigBird 143 150 144 169 525 1086 2626   
FlashAttention 021 022 062 184 577 2225 8621 33891 134391 536109
BlockSparse FlashAttention 022 022 026 057 155 313 598 1221 2349 4785
32Table 14 Forward pass  backward pass runtime ms of various exactapproximatesparse attention mechanisms by
sequence length with masking  Best in bold second best underlined 
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 080 081 208 723 2751 10758    
Megatron 081 083 109 336 1239     
Reformer 416 746 1406 2768 5566 11215 22937   
Local Attention 139 168 208 583 2004 4016 8044 16135 32511 
Linformer 151 142 156 167 367 699 1363 2677 5336 11756
Smyrf 338 493 907 1766 3494 6955 13872 27741  
LSformer 308 310 426 1090 3159 6172 12151 24118  
Block Sparse 239 240 331 502 1225 3594    
Longformer 236 234 238 294 983 2135 5812   
BigBird 235 235 237 325 1036 2257 6063   
FlashAttention 032 030 083 237 795 3077 11998 47365 188343 751301
BlockSparse FlashAttention 034 034 036 069 185 389 716 1485 3046 6003
Table 15 Forward pass runtime ms of various exactapproximatesparse attention mechanisms by sequence length
with dropout  Best in bold second best underlined 
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 026 024 057 180 656 2534    
Megatron 027 027 056 188 656     
Reformer 183 296 531 1033 2119 4342 9196 20134  
Local Attention 051 060 078 201 623 1252 2507 5050 10218 
Linformer 047 037 049 052 137 265 512 1013 2025 4416
Smyrf 212 201 315 597 1183 2336 4648 9272  
LSformer 128 133 151 339 1140 2254 4496 8985 17973 
Block Sparse 103 100 172 239 596 1788    
Longformer 102 103 103 173 510 1163 3422   
BigBird 099 103 101 158 536 1227 3556   
FlashAttention 010 010 022 083 281 1038 4163 16701 66874 267811
BlockSparse FlashAttention 054 051 068 061 067 110 189 371 718 1441
Table 16 Backward pass runtime ms of various exactapproximatesparse attention mechanisms by sequence length
with dropout  Best in bold second best underlined 
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 044 035 090 294 1077 4167    
Megatron 028 033 092 294 1080     
Reformer 224 434 839 1662 3302 6577 13152   
Local Attention 051 058 141 371 1296 2598 5194 10372 20778 
Linformer 084 074 079 085 228 437 866 1702 3378 
Smyrf 127 256 490 966 1916 3813 7617 15239  
LSformer 167 177 303 752 2010 3913 7635 15083  
Block Sparse 127 136 215 304 727 2118    
Longformer 128 134 138 198 524 1074 2595   
BigBird 148 147 150 181 557 1138 2743   
FlashAttention 015 018 058 186 650 2621 10427 41610 166192 664301
BlockSparse FlashAttention 017 017 017 040 110 204 443 933 1828 3731
Table 17 Forward pass  backward pass runtime ms of various exactapproximatesparse attention mechanisms by
sequence length with dropout  Best in bold second best underlined 
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 066 067 143 482 1747 6729    
Megatron 088 090 149 473 1741     
Reformer 406 728 1368 2698 5427 10939 22380   
Local Attention 109 140 199 561 1923 3862 7730 15463 31112 
Linformer 131 121 130 139 373 715 1405 2769 5500 
Smyrf 300 437 805 1566 3104 6164 12304 24565  
LSformer 307 317 431 1089 3154 6178 12156 24094  
Block Sparse 254 252 371 544 1329 3919    
Longformer 247 249 251 310 1039 2249 6044   
BigBird 251 249 252 340 1097 2389 6328   
FlashAttention 035 036 080 252 916 3670 14613 58345 233201 932363
BlockSparse FlashAttention 091 083 094 092 183 350 702 1356 2671 5392
33Table 18 Forward pass runtime ms of various exactapproximatesparse attention mechanisms by sequence length
Best in bold second best underlined 
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 021 022 043 127 432 1647 6777   
Megatron 024 026 042 133 428     
Reformer 177 282 501 974 2003 4111 8739 19240  
Local Attention 048 057 080 190 576 1156 2313 4665 9474 
Linformer 046 036 045 050 109 209 401 790 1570 3540
Smyrf 194 196 301 569 1126 2223 4421 8822  
LSformer 121 134 134 331 1101 2171 4327 8632 17285 
Block Sparse 096 104 166 216 541 1615    
Longformer 099 098 099 156 479 1107 3298   
BigBird 096 102 102 148 505 1159 3416   
FlashAttention 008 009 018 068 240 842 3354 13403 53595 214705
BlockSparse FlashAttention 056 052 063 065 061 096 169 302 569 1177
Table 19 Backward pass runtime ms of various exactapproximatesparse attention mechanisms by sequence length
Best in bold second best underlined 
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 026 029 078 244 882 3387    
Megatron 029 030 080 259 886     
Reformer 218 421 814 1612 3202 6384 12760   
Local Attention 051 064 128 360 1252 2508 5022 10023 20066 
Linformer 069 076 069 080 204 388 767 1504 3011 6315
Smyrf 124 249 477 942 1865 3712 7415 14835  
LSformer 168 161 302 740 1972 3827 7489 14799  
Block Sparse 124 125 204 291 678 1967    
Longformer 127 123 124 185 499 1021 2489   
BigBird 143 150 144 169 525 1086 2626   
FlashAttention 011 016 052 162 545 2157 8475 33600 133856 534319
BlockSparse FlashAttention 011 012 016 038 120 234 469 910 1874 3704
Table 20 Forward pass  backward pass runtime ms of various exactapproximatesparse attention mechanisms by
sequence length Best in bold second best underlined 
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 067 070 118 367 1322 5044    
Megatron 074 065 123 380 1321     
Reformer 393 701 1315 2589 5209 10500 21513   
Local Attention 109 127 199 538 1832 3677 7367 14729 29635 
Linformer 131 125 130 129 320 610 1193 2339 4672 10052
Smyrf 298 423 778 1512 2996 5945 11860 23702  
LSformer 303 305 426 1070 3077 6015 11833 23494  
Block Sparse 239 240 331 502 1225 3594    
Longformer 236 234 238 294 983 2135 5812   
BigBird 235 235 237 325 1036 2257 6063   
FlashAttention 031 031 073 229 764 3009 11850 47051 187608 749285
BlockSparse FlashAttention 074 077 082 088 171 321 656 1260 2493 5039
Table 21 Memory usage MB of various exactapproximatesparse attention mechanisms by sequence length Best
inbold second best underlined 
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 36 104 336 1184 4416 17024    
Megatron 36 104 336 1184 4416     
Reformer 377 754 1508 3016 6033 12067 24134   
Local Attention 53 110 232 592 1696 3392 6784 13568 27136 
Linformer 25 52 114 287 832 1652 3292 6572 13132 26252
Smyrf 217 434 868 1737 3474 6947 13894 27788  
LSformer 72 152 333 796 2540 5068 10125 20240  
Block Sparse 33 82 228 408 910 2401    
Longformer 30 61 124 277 681 1370 2748   
BigBird 33 66 131 294 708 1431 2872   
FlashAttention 22 44 104 209 418 836 1672 3344 6688 13376
BlockSparse FlashAttention 22 44 104 209 418 836 1672 3344 6690 13384
34
  Generating Long Sequences with Sparse Transformers
Rewon Child1Scott Gray1Alec Radford1Ilya Sutskever1
Abstract
Transformers are powerful sequence models but
require time and memory that grows quadrati
cally with the sequence length In this paper we
introduce sparse factorizations of the attention
matrix which reduce this to Onpn We also
introduce a a variation on architecture and initial
ization to train deeper networks b the recompu
tation of attention matrices to save memory and
c fast attention kernels for training We call net
works with these changes Sparse Transformers
and show they can model sequences tens of thou
sands of timesteps long using hundreds of layers
We use the same architecture to model images
audio and text from raw bytes setting a new state
of the art for density modeling of Enwik8 CIFAR
10 and ImageNet64 We generate unconditional
samples that demonstrate global coherence and
great diversity and show it is possible in principle
to use selfattention to model sequences of length
one million or more
1 Introduction
Estimating complex highdimensional data distributions is
a central problem in unsupervised learning as many down
stream applications of interest involve generation of text
images audio and other data Additionally it is believed to
be a key component of unsupervised representation learning
Recently neural autoregressive models have achieved im
pressive results in this domain achieving stateoftheart in
modeling natural language Jozefowicz et al 2016 Rad
ford et al 2018 Dai et al 2018 raw audio Van Den Oord
et al 2016 Mehri et al 2016 and images Oord et al
2016 Menick  Kalchbrenner 2018 Salimans et al
2017 Reed et al 2017 Chen et al 2017
These methods decompose a joint probability distribution
into a product of conditional ones Modeling these condi
tional distributions is extremely challenging however as
they contain many complex longrange dependencies and
require a suitably expressive model architecture to learn
them
Architectures based off CNNs Oord et al 2016 have made
Figure 1 Unconditional samples from our neural autoregressive
model on ImageNet 64 and a classical music dataset We used the
same selfattention based architecture for audio images and text
The samples above were generated with softmax temperature 10
and had lengths 12288 and 65536 Audio samples be listened to at
httpsopenaicomblogsparsetransformer
great progress in this direction but require signiﬁcant depth
to expand their receptive ﬁeld To address this WaveNet
Van Den Oord et al 2016 introduced dilated convolutions
which allowed the network to model longrange dependen
cies in a logarithmic number of layers
Separately the Transformer Vaswani et al 2017 has been
shown to excel on many natural language tasks which may
be in part due to its ability to model arbitrary dependencies
in a constant number of layers As each selfattention layer
has a global receptive ﬁeld the network can allocate rep
resentational capacity to the input regions for which it isarXiv190410509v1  csLG  23 Apr 2019Generating Long Sequences with Sparse Transformers
most useful Thus the architecture may be more ﬂexible
at generating diverse data types than networks with ﬁxed
connectivity patterns
However the memory and computational requirements of
such networks grows quadratically with sequence length
which excludes their use on long sequences
The main contribution of this work is to introduce several
sparse factorizations of the attention matrix which scale
asOnppnwith the sequence length without sacriﬁcing
performance These work by separating the full attention
computation into several faster attention operations which
when combined can approximate the dense attention oper
ation We use this to apply selfattention to sequences of
unprecedented length
Additionally we introduce several other changes to the
Transformer including
A restructured residual block and weight initialization
to improve training of very deep networks
A set of sparse attention kernels which efﬁciently com
pute subsets of the attention matrix
Recomputation of attention weights during the back
wards pass to reduce memory usage
We empirically validate that models augmented in this man
ner can achieve stateoftheart compression and generation
of natural language raw audio and natural images The
simplicity of the architecture leads us to believe it may be
useful for many problems of interest
2 Related Work
The most related work involves other techniques for scaling
up autoregressive generative models For images Reed
et al 2017 models conditional independence between the
pixels in order to generate many locations in parallel and
Menick  Kalchbrenner 2018 imposes an ordering and
multiscale upsampling procedure to generate high ﬁdelity
samples Parmar et al 2018 uses blocks of local attention
to apply Transformers to images For text Dai et al 2018
introduces a state reuse memory for modeling longterm
dependencies And for audio in addition to Van Den Oord
et al 2016 Mehri et al 2016 used a hierarchical struc
ture and RNNs of varying clockrates to use long contexts
during inference similar to Koutnik et al 2014 Huang
et al 2018 apply Transformers to MIDI generation with
an efﬁcient relative attention
Our work is simpler than many of the techniques above and
can be applied equally across images text and audio Many
of the above techniques are orthogonal to ours moreover
and could be used in conjunction with oursOutside of generative modeling there are several works
relevant to improving the efﬁciency of attention based off
chunking Chiu  Raffel 2017 or using ﬁxed length repre
sentations Britz et al 2017 Other works have investigated
attention with multiple hops such as Sukhbaatar et al
2015 and Gehring et al 2017
It is worth noting that the Gated Pixel CNN Oord et al
2016 and WaveNet Van Den Oord et al 2016 use multi
plicative interactions in their networks which are related to
selfattention
3 Background
We consider the task of autoregressive sequence gener
ation where the joint probability of a sequence x
fx1x2x ngis modeled as the product of conditional
probability distributions and parameterized by a network 
px nY
i1pxijx1x i1 1
We treat images text and audio as a sequence of discrete
tokens typically raw bytes The network takes in the se
quence of tokens and outputs a categorical distribution over
thevpossible values of the next token using the softmax
function where vis the size of the vocabulary  The training
objective is to maximize the logprobability of the data with
respect to
A simple and powerful choice for model is a Transformer
Vaswani et al 2017 in decoderonly mode as demon
strated by Radford et al 2018 and Liu et al 2018 These
models transform the input sequence with blocks of mul
tihead selfattention over the entire sequence followed by
dense transformations over each sequence element The self
attention portion of the network must compute nweightings
for each ofnelements however which can quickly become
intractable as the sequence length grows
In the following sections we describe our modiﬁcations to
the Transformer architecture which make it more suitable
for modeling long sequences
4 Factorized SelfAttention
Sparse Transformers separate the full selfattention opera
tion across several steps of attention as visualized in Figure
3b and 3c To motivate our approach we ﬁrst perform
a qualitative assessment of attention patterns learned by a
standard Transformer on an image datasetGenerating Long Sequences with Sparse Transformers
Figure 2 Learned attention patterns from a 128layer network on CIFAR10 trained with full attention White highlights denote attention
weights for a head while generating a given pixel and black denotes the autoregressive mask Layers are able to learn a variety of
specialized sparse structures which may explain their ability to adapt to different domains a Many early layers in the network learn
locally connected patterns which resemble convolution b In layers 19 and 20 the network learned to split the attention across a
row attention and column attention effectively factorizing the global attention calculation c Several attention layers showed global
datadependent access patterns d Typical layers in layers 64128 exhibited high sparsity with positions activating rarely and only for
speciﬁc input patterns
a Transformer
 b Sparse Transformer strided
 c Sparse Transformer ﬁxed
Figure 3 Two 2d factorized attention schemes we evaluated in comparison to the full attention of a standard Transformer a The top
row indicates for an example 6x6 image which positions two attention heads receive as input when computing a given output The
bottom row shows the connectivity matrix not to scale between all such outputs rows and inputs columns Sparsity in the connectivity
matrix can lead to signiﬁcantly faster computation In b and c full connectivity between elements is preserved when the two heads are
computed sequentially We tested whether such factorizations could match in performance the rich connectivity patterns of Figure 2Generating Long Sequences with Sparse Transformers
41 Qualitative assessment of learned attention
patterns
We visualized the attention patterns learned by a 128layer
selfattention network on CIFAR10 and present several
examples in Figure 2 Visual inspection showed that most
layers had sparse attention patterns across most data points
suggesting that some form of sparsity could be introduced
without signiﬁcantly affecting performance Several layers
Figure 2c clearly exhibited global patterns however and
others exhibited datadependent sparsity Figure 2d both
of which would be impacted by introducing a predetermined
sparsity pattern into all of the attention matrices
In this paper we restricted our investigation to a class of
sparse attention patterns that have connectivity between all
positions over several steps of attention These methods can
be more efﬁcient than full attention while still providing
global context to any given position We aimed to empiri
cally validate the performance of these factorized patterns
on a range of tasks given that they are unable to learn the
exact same mappings as those in Figure 2 We present the
formulation of factorized attention below
42 Factorized selfattention
A selfattention layer maps a matrix of input embeddings
Xto an output matrix and is parameterized by a connectiv
ity patternSfS1S ng whereSidenotes the set of
indices of the input vectors to which the ith output vector
attends The output vector is a weighted sum of transforma
tions of the input vectors
AttendXS 
axiSi
i2f1ng2
axiSi  softmax 
WqxiKT
Sip
d
VSi 3
KSi
Wkxj
j2SiVSi
Wvxj
j2Si4
HereWqWk andWvrepresent the weight matrices which
transform a given xiinto a query key orvalue  anddis
the inner dimension of the queries and keys The output at
each position is a sum of the values weighted by the scaled
dotproduct similarity of the keys and queries
Full selfattention for autoregressive models deﬁnes Si
fjjig allowing every element to attend to all previous
positions and its own position
Factorized selfattention instead has pseparate attention
heads where the mth head deﬁnes a subset of the indices
Am
i fjjigand letsSiAm
i We are
chieﬂy interested in efﬁcient choices for the subset A where
jAm
ijppnAdditionally for the time being we consider valid choices
ofA where all input positions are connected to all future
output positions across the psteps of attention
For everyjipair we set every Asuch thatican attend
tojthrough a path of locations with maximum length p 1
Speciﬁcally if jabci is the path of indices then
j2A1
aa2A2
bb2A3
c and so forth
These two criteria allow us keep the ability of Transformers
to propagate signals from arbitrary input positions to arbi
trary output positions in a constant number of steps while
reducing the total effective computation to Onppn We
also note that softening the validity criterion for instance
having a series of only locally connected layers may be a
useful inductive bias for certain domains
In this work we explore two factorizations for p 2 which
we describe in the following section though we note that
the same techniques can be easily extended to higher dimen
sions
43 Twodimensional factorized attention
A natural approach to deﬁning a factorized attention pattern
in two dimensions is to have one head attend to the previous
llocations and the other head attend to every lth location
wherelis the stride and chosen to be close topn a method
we call strided attention
FormallyA1
iftt 1igfort max0il
andA2
ifj ij modl 0g This pattern can be
visualized in Figure 3b
This formulation is convenient if the data naturally has a
structure that aligns with the stride like images or some
types of music For data without a periodic structure like
text however we ﬁnd that the network can fail to properly
route information with the strided pattern as spatial coor
dinates for an element do not necessarily correlate with the
positions where the element may be most relevant in the
future
In those cases we instead use a ﬁxed attention pattern Fig
ure 3c where speciﬁc cells summarize previous locations
and propagate that information to all future cells
FormallyA1
ifj bjlcbilcg where the brackets
denote the ﬂoor operation and A2
ifjjmodl2
ftt 1lg wheretlcandcis a hyperparameter
Concretely if the stride is 128 and c 8 then all future
positions greater than 128 can attend to positions 120128
all positions greater than 256 can attend to 248256 and so
forth
A ﬁxedattention pattern with c 1limits the expressivity
of the network signiﬁcantly as many representations inGenerating Long Sequences with Sparse Transformers
the network are only used for one block whereas a small
number of locations are used by all blocks We instead
found choosing c2f81632gfor typical values of l2
f128256gto perform well although it should be noted that
this increases the computational cost of this method by cin
comparison to the strided attention
Additionally we found that when using multiple heads
having them attend to distinct subblocks of length cwithin
the block of size lwas preferable to having them attend to
the same subblock
In the subsequent section we describe how to incorporate
factorized attention into the Sparse Transformer architec
ture
5 Sparse Transformer
Here we fully describe the Sparse Transformer architecture
which is a modiﬁed version of the Transformer Vaswani
et al 2017
51 Factorized attention heads
Standard dense attention simply performs a linear transfor
mation of the attend function deﬁned in Equation 2
attentionX WpattendXS 5
whereWpdenotes the postattention weight matrix The
simplest technique for integrating factorized selfattention
is to use one attention type per residual block and interleave
them sequentially or at a ratio determined as a hyperparam
eter
attentionX WpattendXArmodp6
Hereris the index of the current residual block and pis the
number of factorized attention heads
A second approach is to have a single head attend to the
locations of the pixels that both factorized heads would
attend to which we call a merged head
attentionX WpattendXp
m1Am 7
This is slightly more computationally intensive but only
by a constant factor A third approach is to use multihead
attention Vaswani et al 2017 where nhattention products
are computed in parallel then concatenated along the feature
dimension
attentionX Wp
attendXAi
i2f1n hg8
embed
linearsoftmaxnorm
norm
normdropout
dropoutattention
feedforward   Figure 4 Diagram depicting one residual block of the Sparse Trans
former The shaded background indicates tensors which are check
pointed Chen et al 2016 and stored in GPU memory The other
tensors including the attention weights and feedforward network
activations are recomputed during the calculation of gradients
reducing memory usage substantially
Here theAcan be the separate attention patterns the
merged patterns or interleaved as in Eq 2 Also the di
mensions of the weight matrices inside the attend function
are reduced by a factor of 1nh such that the number of
parameters are invariant across values of nh
We typically ﬁnd multiple heads to work well though for
extremely long sequences where the attention dominates the
computation time it is more worthwhile to perform them
one at a time and sequentially
52 Scaling to hundreds of layers
We found that Transformers were difﬁcult to train with
many layers as noted by AlRfou et al 2018 Instead
of incorporating auxillary losses we adopted the followingGenerating Long Sequences with Sparse Transformers
architectural changes
First we use the preactivation residual block of He et al
2016 deﬁning a network of Nlayers in the following way
H0 embedXW e 9
HkHk1 resblock Hk1 10
y softmaxnorm HNWout 11
where embed is a function we describe in the next section
Woutis a weight matrix and resblockhnormalizes the
input to the attention block and a positionwise feedforward
network in the following way
aH  dropoutattentionnorm H 12
bH  dropoutnorm HaH 13
resblockH aH bH 14
Thenorm function denotes Layer Normalization Ba et al
2016 and x W2fW1xb1 b2 Our choice of
fis the Gaussian Error Linear Unit Hendrycks  Gimpel
2016fX Xsigmoid1702X as used in Rad
ford et al 2018 The output dimension of W1is 40 times
the input dimension unless otherwise noted
Observe that HNis the sum of Napplications of functions
aandb and thus each function block receives a gradient
directly from the output layer  We scale the initialization
ofW2andWpin Eq 5 by1p
2Nto keep the ratio of input
embedding scale to residual block scale invariant across
values ofN
53 Modeling diverse data types
In addition to the embedding of input symbols positional
embeddings are typically used in Transformers and other
locationagnostic architectures to encode the spatial relation
ships of data Gehring et al 2017 Parmar et al 2018
We found using learned embeddings which either encoded
the structure of the data or the factorized attention patterns
were important for performance of our models
We added either nembddata ornembdattn embed
dings to each input location where ddata refers to the num
ber of dimensions of the data and dattnis the number of
dimensions of the factorized attention If xiis the onehot
encodedith element in the sequence and oj
irepresents
the onehot encoded position of xiin thejth dimension
1jnemb then
embedXW e 0
xiWenembX
j1oj
iWj1
A
xi2X15For images we used data embeddings where ddata 3
for the row column and channel location of each input
byte For text and audio we used twodimensional attention
embeddings where dattn 2and the index corresponds to
each positions row and column index in a matrix of width
equal to the stride
54 Saving memory by recomputing attention weights
Gradient checkpointing has been shown to be effective in
reducing the memory requirements of training deep neural
networks Chen et al 2016 Gruslys et al 2016 It is
worth noting however that this technique is particularly
effective for selfattention layers when long sequences are
processed as memory usage is high for these layers relative
to the cost of computing them
Using recomputation alone we are able to train dense atten
tion networks with hundreds of layers on sequence lengths
of 16384 which would be infeasible on modern hardware
otherwise
In our experiments we recompute the attention and feed
forward blocks during the backwards pass To simplify
our implementation we do not apply dropout within the
attention blocks as in Vaswani et al 2017 and instead
only apply it at the end of each residual addition as seen in
Figure 4
55 Efﬁcient blocksparse attention kernels
The sparse attention masks in 3b and 3c can be efﬁciently
computed by slicing out subblocks from the query key and
value matrices and computing the product in blocks Atten
tion over a local window can be computed asis whereas
attention with a stride of kcan be computed by transposing
the matrix and computing a local window Fixed attention
positions can be aggregated and computed in blocks
In order to ease experimentation we implemented a set of
GPU kernels which efﬁciently perform these operations
The softmax operation is fused into a single kernel and
also uses registers to eliminate loading the input data more
than once allowing it to run at the same speed as a simple
nonlinearity The upper triangle of the attention matrix
is never computed moreover removing the need for the
negative bias term of Vaswani et al 2017 and halving the
number of operations to be performed
56 Mixedprecision training
We store network weights in singleprecision ﬂoatingpoint
but otherwise compute network activations and gradients in
halfprecision as in Micikevicius et al 2017 This acceler
ates our training due to the usage of Tensor Core operations
on the V100 GPU During the gradient calculation we useGenerating Long Sequences with Sparse Transformers
Figure 5 Unconditional samples from ImageNet 64x64 generated with an unmodiﬁed softmax temperature of 10 We are able to learn
longrange dependencies directly from pixels without using a multiscale architecture
dynamic loss scaling to reduce numerical underﬂow and
we communicate halfprecision gradients when averaging
across multiple GPUs When sampling we cast the queries
and keys to singleprecision as the querykey product can
sometimes overﬂow the max value of halfprecision
6 Training
We use the Adam optimizer with a linear warmup of 5000
iterations and a gradient clipping of 10 both of which we
found important for model stability We use a weight decay
penalty of 001 We annealed the learning rate according to
a cosine decay as in Radford et al 2018 We train on 8
V100 GPUs unless otherwise noted
All embeddings are of a constant dimension d usually one
off2565121024g By default all linear transforms are to
the same dimension with the exception of the feedforward
network which projects the input to 4d unless we use
halfsize transformations where it is 2d Additionally
sometimes we halve the size of the query and key transfor
mations
We initialize the token embedding WefromN00125p
dand
the position embeddings from N00125pdnemb Within the
attention and feedforward components all biases are initialized to 0 and all weights are initialized from N00125pdin
wheredinis the fanin dimension The weight matrix for
the output logits was initialized to 0
7 Experiments
We empirically test our architecture on density modeling
tasks including natural images text and raw audio A
summary of the results is available in Table 1 We found
that in addition to running signiﬁcantly faster than full
attention sparse patterns also converged to lower error as
shown in Table 2 This may point to a useful inductive bias
from the sparsity patterns we introduced or an underlying
optimization issue with full attention
71 CIFAR10
We train strided Sparse Transformers on CIFAR10 images
represented as sequences of 3072 bytes Models have 2
heads 128 layers d 256 halfsize feedforward network
and querykey projections and are trained for 120 epochs
with a learning rate of 000035 and a dropout rate of 025
until validation error stops decreasing
We use 48000 examples for training and 2000 examples for
validation evaluating the performance of our best models onGenerating Long Sequences with Sparse Transformers
Table 1 Summary of our ﬁndings for density modeling tasks Re
sults are reported in bits per byte which is equivalent to bits per
dim for image tasks M refers to millions of parameters
Model Bits per byte
CIFAR10
PixelCNN Oord et al 2016 303
PixelCNN Salimans et al 2017 292
Image Transformer Parmar et al 2018 290
PixelSNAIL Chen et al 2017 285
Sparse Transformer 59M strided 280
Enwik8
Deeper SelfAttention AlRfou et al 2018 106
TransformerXL 88M Dai et al 2018 103
TransformerXL 277M Dai et al 2018 099
Sparse Transformer 95M ﬁxed 099
ImageNet 64x64
PixelCNN Oord et al 2016 357
Parallel Multiscale Reed et al 2017 37
Glow Kingma  Dhariwal 2018 381
SPN 150M Menick  Kalchbrenner 2018 352
Sparse Transformer 152M strided 344
Classical music 5 seconds at 12 kHz
Sparse Transformer 152M strided 197
the test set The model achieves 280 bits per dim  2798
0004over seeds 1 2 3 versus the previous 285state of
the art Chen et al 2017 We also compare performance of
different attention patterns in Table 2 The strided attention
reaches the lowest error in the shortest amount of time
surpassing the error of dense attention at 282 bits per dim
72 Text
In order to assess Sparse Transformers on datasets without
a strong twodimensional structure we trained models on
the EnWik8 dataset which represents the ﬁrst 108bytes
of Wikipedia and contains a great degree of variability in
periodic structure We trained with a context length of
12288 which is longer than previous approaches
We trained on the ﬁrst 90 million tokens and reserved the last
10 million for validation and test We used 30layer ﬁxed
Sparse Transformers with 8 heads d 512 and a dropout
rate of 040 We trained for 80 epochs until validation loss
stopped decreasing We used a stride of 128 c 32  and
merged the factorized attention heads
Our best model reached 099 bits per dim  09920001
over seeds 1 2 3 surpassing the 103 stateoftheart for
a similarlysized TransformerXL Dai et al 2018 and
matching the 099 of a model trained with more than doubleTable 2 Sparse patterns showed increased speed and also better
loss on the datasets where we could compare both which may
point to a useful inductive bias in the patterns we learned or an
underlying optimization issue with full attention
Model Bits per byte TimeIter
Enwik8 12288 context
Dense Attention 100 131
Sparse Transformer Fixed 099 055
Sparse Transformer Strided 113 035
CIFAR10 3072 context
Dense Attention 282 054
Sparse Transformer Fixed 285 047
Sparse Transformer Strided 280 038
Table 3 We observe increased compression of Enwik8 with longer
contexts suggesting the Sparse Transformer can effectively incor
porate longterm dependencies
Minimum context length during evaluation Bits per byte
6144 tokens 09952
9216 tokens 09936
10752 tokens 09932
11904 tokens 09930
12096 tokens 09922
12160 tokens 09908
the number of parameters Strided attention failed to do well
on this dataset whereas ﬁxed patterns were able to recover
and surpass the performance of dense attention as listed in
Table 2
Additionally during evaluation of the test set we modiﬁed
the minimum context length the network could use by evalu
ating fewer tokens in parallel We saw monotonic increases
in performance with more tokens used up to 12160 out
of the 12288 tokens used for training see Table 3 which
suggests the network is effectively incorporating longterm
dependencies
73 ImageNet 64x64
In order to test the ability of the model to learn long range
dependencies and scale to a large dataset we train on the
version of downsampled ImageNet released by Oord et al
2016 and evaluate on the validation set We used a 48 layer
strided Sparse Transformer with 16 attention heads and d
 512 totaling 152 million parameters We used a stride
of 128 a dropout of 001 and trained for 70 epochs which
took 7 days on 64 V100 GPUs
Our model achieves a loss of 344 bits per dim 3437 across
1 run in comparison to the previous 352 Menick  Kalch
brenner 2018Generating Long Sequences with Sparse Transformers
Additionally we generate unconditional samples Figure
5 at an unmodiﬁed softmax temperature of 10 from the
model and from one trained with twice the layers 300M
parameters total We include here samples from the 300M
parameter model On visual assessment we ﬁnd no artifacts
from the sparsity patterns and see evidence of longterm
structure in most images
74 Classical music from raw audio
To test the extent to which Sparse Transformers are able
to scale to very long contexts we trained models on the
classical music dataset released by Dieleman et al 2018
As details of the dataset processing are unavailable we omit
any direct comparison to other work and instead study what
size of Sparse Transformer we can train with increasing
context size For each sequence length we attempted to
train the largest model which could entirely ﬁt into 16GB
V100 accelerators without model parallelism
Overall we found that increasing the sequence length by a
factor of 4 requires a reduction in model capacity of approx
imately 4p
4  8  Thus we found we could use factorized
selfattention on sequences over 1 million timesteps long
albeit with extremely few parameters 3 million
Samples are available for sequences of length 65536 which
correspond to around 5 seconds of generated audio at 12kHz
The samples clearly demonstrate global coherence over the
sampled period and exhibit a variety of play styles and
tones swapping from rhythmic playing to forceful To
listen to samples visit httpsopenaicomblog
sparsetransformer  Sample quality quickly de
grades for greater sequence lengths due to reduced model
capacity
Table 4 Performance of a strided Sparse Transformer on a classical
audio dataset  law encoded at 12 kHz as a function of sequence
length and model size
Sequence length Parameters Bits per byte
65536 152M 197
262144 25M 217
1048576 3M 299
8 Conclusion
We introduced Sparse Transformers and showed they attain
equivalent or better performance on density modeling of
long sequences than standard Transformers while requiring
signiﬁcantly fewer operations This performance is state
oftheart in images and text and is easily adaptable to raw
audio The model demonstrates usage of longterm context
and generates globally coherent samples9 Acknowledgements
We would like to thank Ashish Vaswani for insightful dis
cussions during the genesis of the project We also thank
Joshua Meier and Mark Chen for helpful discussions and
Johannes Otterbach Prafulla Dhariwal and David Luan for
feedback on drafts of this paper
References
AlRfou R Choe D Constant N Guo M and Jones
L Characterlevel language modeling with deeper self
attention arXiv preprint arXiv180804444  2018
Ba J L Kiros J R and Hinton G E Layer normalization
arXiv preprint arXiv160706450  2016
Britz D Guan M Y  and Luong MT Efﬁcient attention
using a ﬁxedsize memory representation arXiv preprint
arXiv170700110  2017
Chen T Xu B Zhang C and Guestrin C Training
deep nets with sublinear memory cost arXiv preprint
arXiv160406174  2016
Chen X Mishra N Rohaninejad M and Abbeel P
Pixelsnail An improved autoregressive generative model
arXiv preprint arXiv171209763  2017
Chiu CC and Raffel C Monotonic chunkwise attention
arXiv preprint arXiv171205382  2017
Dai Z Yang Z Yang Y  Cohen W W Carbonell J Le
Q V  and Salakhutdinov R Transformerxl Language
modeling with longerterm dependency 2018
Dieleman S van den Oord A and Simonyan K The chal
lenge of realistic music generation modelling raw audio
at scale In Advances in Neural Information Processing
Systems  pp 80008010 2018
Gehring J Auli M Grangier D Yarats D and Dauphin
Y  N Convolutional sequence to sequence learning arXiv
preprint arXiv170503122  2017
Gruslys A Munos R Danihelka I Lanctot M and
Graves A Memoryefﬁcient backpropagation through
time In Advances in Neural Information Processing
Systems  pp 41254133 2016
He K Zhang X Ren S and Sun J Identity mappings in
deep residual networks arXiv preprint arXiv160305027 
2016
Hendrycks D and Gimpel K Bridging nonlinearities and
stochastic regularizers with gaussian error linear units
arXiv preprint arXiv160608415  2016Generating Long Sequences with Sparse Transformers
Huang CZ A Vaswani A Uszkoreit J Shazeer N
Hawthorne C Dai A M Hoffman M D and Eck
D An improved relative selfattention mechanism for
transformer with application to music generation arXiv
preprint arXiv180904281  2018
Jozefowicz R Vinyals O Schuster M Shazeer N and
Wu Y  Exploring the limits of language modeling arXiv
preprint arXiv160202410  2016
Kingma D P and Dhariwal P Glow Generative ﬂow
with invertible 1x1 convolutions In Advances in Neural
Information Processing Systems  pp 1023610245 2018
Koutnik J Greff K Gomez F and Schmidhuber J A
clockwork rnn arXiv preprint arXiv14023511  2014
Liu P J Saleh M Pot E Goodrich B Sepa
ssi R Kaiser L and Shazeer N Generating
wikipedia by summarizing long sequences arXiv preprint
arXiv180110198  2018
Mehri S Kumar K Gulrajani I Kumar R Jain S
Sotelo J Courville A and Bengio Y  Samplernn An
unconditional endtoend neural audio generation model
arXiv preprint arXiv161207837  2016
Menick J and Kalchbrenner N Generating high ﬁdelity im
ages with subscale pixel networks and multidimensional
upscaling arXiv preprint arXiv181201608  2018
Micikevicius P Narang S Alben J Diamos G Elsen
E Garcia D Ginsburg B Houston M Kuchaev O
Venkatesh G et al Mixed precision training arXiv
preprint arXiv171003740  2017
Oord A v d Kalchbrenner N and Kavukcuoglu
K Pixel recurrent neural networks arXiv preprint
arXiv160106759  2016
Parmar N Vaswani A Uszkoreit J Kaiser Ł Shazeer
N and Ku A Image transformer arXiv preprint
arXiv180205751  2018
Radford A Narasimhan K Salimans T and Sutskever
I Improving language understanding by genera
tive pretraining URL httpss3uswest2 ama
zonaws comopenaiassetsresearchcoverslanguage
unsupervisedlanguage understanding paper pdf  2018
Reed S Oord A v d Kalchbrenner N Colmenarejo
S G Wang Z Belov D and de Freitas N Paral
lel multiscale autoregressive density estimation arXiv
preprint arXiv170303664  2017
Salimans T Karpathy A Chen X and Kingma D P
Pixelcnn Improving the pixelcnn with discretized lo
gistic mixture likelihood and other modiﬁcations arXiv
preprint arXiv170105517  2017Sukhbaatar S Weston J Fergus R et al Endtoend
memory networks In Advances in neural information
processing systems  pp 24402448 2015
Van Den Oord A Dieleman S Zen H Simonyan K
Vinyals O Graves A Kalchbrenner N Senior A and
Kavukcuoglu K Wavenet A generative model for raw
audio CoRR abs160903499  2016
Vaswani A Shazeer N Parmar N Uszkoreit J Jones
L Gomez A N Kaiser Ł and Polosukhin I Atten
tion is all you need In Advances in Neural Information
Processing Systems  pp 59986008 2017
  xLSTM Extended Long ShortTerm Memory
Maximilian Beck12Korbinian Pöppel12Markus Spanring1
Andreas Auer12Oleksandra Prudnikova1Michael Kopp
Günter Klambauer12Johannes Brandstetter123Sepp Hochreiter123
Equal contribution
1ELLIS Unit LIT AI Lab Institute for Machine Learning JKU Linz Austria
2NXAI Lab Linz Austria3NXAI GmbH Linz Austria
Abstract
In the 1990s the constant error carousel and gating were introduced as the central
ideas of the Long ShortTerm Memory LSTM Since then LSTMs have stood
the test of time and contributed to numerous deep learning success stories in
particular they constituted the first Large Language Models LLMs However
the advent of the Transformer technology with parallelizable selfattention at its
core marked the dawn of a new era outpacing LSTMs at scale We now raise a
simple question How far do we get in language modeling when scaling LSTMs to
billions of parameters leveraging the latest techniques from modern LLMs but
mitigating known limitations of LSTMs Firstly we introduce exponential gating
with appropriate normalization and stabilization techniques Secondly we modify
the LSTM memory structure obtaining i sLSTM with a scalar memory a scalar
update and new memory mixing ii mLSTM that is fully parallelizable with a
matrix memory and a covariance update rule Integrating these LSTM extensions
into residual block backbones yields xLSTM blocks that are then residually stacked
into xLSTM architectures Exponential gating and modified memory structures
boost xLSTM capabilities to perform favorably when compared to stateoftheart
Transformers and State Space Models both in performance and scaling
Memory Cells
 Constant Error Carousel
 Sigmoid Gating
 Recurrent Inference
 Recurrent TrainingsLSTM
 New Memory MixingMemory Cells xLSTM Blocks xLSTM
mLSTM
 Exponential Gating
 Parallel Training
 Covariance Update Rule Matrix MemoryLSTM
 Exponential Gating
Figure 1 The extended LSTM xLSTM family From left to right 1 The original LSTM memory
cell with constant error carousel and gating 2 New sLSTM and mLSTM memory cells that introduce
exponential gating sLSTM offers a new memory mixing technique mLSTM is fully parallelizable
with a novel matrix memory cell state and new covariance update rule 3 mLSTM and sLSTM in
residual blocks yield xLSTM blocks 4 Stacked xLSTM blocks give an xLSTM architecturearXiv240504517v1  csLG  7 May 20241 Introduction
The Long ShortTerm Memory LSTM ideas Hochreiter 1991 Hochreiter  Schmidhuber
1997ba ie the constant error carousel and gating were introduced to overcome the vanishing
gradient problem of recurrent neural networks Hochreiter 1991 Hochreiter et al 2000
ctftct1itzt h totψct 1
The constant error carousel is the additive update of the cell state ct1green by cell inputs ztand
moderated by sigmoid gates blue The input gate itand the forget gate ftcontrol this update while
the output gate otcontrols the output of the memory cell ie the hidden state ht The cell state is
normalized or squashed by ψand then output gating gives the hidden state
LSTMs have been successfully applied to various domains Hochreiter et al 2001 2007 Schmid
huber 2015 and prevailed over text generation until the dawn of Transformers in 2017 Vaswani
et al 2017 The effectiveness of LSTMs has been demonstrated at numerous sequencerelated tasks
such as generating text Graves 2013 Karpathy 2015 generating handwritings Graves 2013
sequencetosequence translation Sutskever et al 2014 evaluating computer programs Zaremba
 Sutskever 2014 generating image captions Karpathy  FeiFei 2015 Hossain et al 2019
generating source code Karpathy 2015 rainfallrunoff modeling Kratzert et al 2018 2019
or hydrological models for flooding warnings Nearing et al 2024 In reinforcement learning
LSTMs are the best performing sequence models eg the AlphaStar model for StarCraft II Vinyals
et al 2017 the OpenAI Five model for Dota 2 Karpathy 2019 and models of the magnetic
controller for nuclear fusion Degrave et al 2022 LSTMs excel at learning abstractions ie adeptly
extracting semantic information and storing it in their memory cells Karpathy 2015 which for
example became evident by number and syntax neurons Lakretz et al 2019 linguistic neurons Bau
et al 2019 and sentiment neurons Radford et al 2017 LSTMs are still used in highly relevant
applications Degrave et al 2022 Nearing et al 2024 and have stood the test of time
Figure 2 LSTM limitations Left Nearest Neighbor Search
problem in terms of mean squared error MSE Given a
reference vector a sequence is scanned sequentially for the
most similar vector with the objective to return its attached
value at sequence end LSTM struggles to revise a stored
value when a more similar vector is found Our new xLSTM
overcomes this limitation by exponential gating Right  Rare
Token Prediction The perplexity PPL of token predic
tion on Wikitext103 in buckets of token frequency LSTM
performs worse on predicting rare tokens because of its lim
ited storage capacities whereas our new xLSTM solves this
problem via a matrix memoryDespite their tremendous successes
LSTMs have three main limitations
i Inability to revise storage deci
sions We exemplify this limitation
via the Nearest Neighbor Search prob
lem see also Appendix B With a ref
erence vector given a sequence must
be scanned sequentially for the most
similar vector in order to provide its
attached value at sequence end The
left panel of Figure 2 shows the mean
squared error at this task LSTM strug
gles to revise a stored value when a
more similar vector is found while
our new xLSTM remediates this limi
tation by exponential gating ii Lim
ited storage capacities ie informa
tion must be compressed into scalar
cell states We exemplify this limita
tion via Rare Token Prediction  In the
right panel of Figure 2 the perplex
ity of token prediction on Wikitext
103 Merity et al 2017 is given for
buckets of different token frequency
LSTM performs worse on rare tokens because of its limited storage capacities Our new xLSTM
solves this problem by a matrix memory iii Lack of parallelizability due to memory mixing ie
the hiddenhidden connections between hidden states from one time step to the next which enforce
sequential processing
These limitations of LSTM have paved the way for the emergence of Transformers Vaswani et al
2017 in language modeling What performances can we achieve in language modeling when
overcoming these limitations and scaling LSTMs to the size of current Large Language Models
22 Extended Long ShortTerm Memory
To overcome the LSTM limitations Extended Long ShortTerm Memory xLSTM introduces two
main modifications to the LSTM idea of Equation 1 Those modifications  exponential gating
and novel memory structures  enrich the LSTM family by two members i the new sLSTM see
Section 22 with a scalar memory a scalar update and memory mixing and ii the new mLSTM
see Section 23 with a matrix memory and a covariance outer product update rule which is fully
parallelizable Both sLSTM and mLSTM enhance the LSTM through exponential gating To enable
parallelization the mLSTM abandons memory mixing ie the hiddenhidden recurrent connections
Both mLSTM and sLSTM can be extended to multiple memory cells where sLSTM features memory
mixing across cells Further the sLSTM can have multiple heads without memory mixing across the
heads but only memory mixing across cells within each head This introduction of heads for sLSTM
together with exponential gating establishes a new way of memory mixing For mLSTM multiple
heads and multiple cells are equivalent
Integrating these new LSTM variants into residual block modules results in xLSTM blocks see
Section 24 Residually stacking those xLSTM blocks in architectures provides xLSTM architectures
see Section 24 See Figure 1 for the xLSTM architecture with its components
21 Review of the Long ShortTerm Memory
The original LSTM idea Hochreiter 1991 Hochreiter  Schmidhuber 1997ba introduced the
scalar memory cell as a central processing and storage unit that avoids vanishing gradients Hochreiter
1991 Hochreiter et al 2000 through the constant error carousel the cell state update The memory
cell contains three gates input output and forget gate The forget gate has been introduced by Gers
et al 2000 The LSTM memory cell update rules at time step tare
ctftct1itzt cell state 2
htotht htψ
ct
hidden state 3
ztφzt ztw
zxtrzht1bz cell input 4
itσit
 itw
ixtriht1bi input gate 5
ftσ
ft
 ftw
fxtrfht1bf forget gate 6
otσ ot  otw
oxtroht1bo output gate 7
The weight vectors wzwiwf andwocorrespond to the input weight vectors between inputs xt
and cell input input gate forget gate and output gate respectively The weights rzrirf and ro
correspond to the recurrent weights between hidden state ht1and cell input input gate forget gate
and output gate respectively bzbibf and boare the corresponding bias terms φandψare the
cell input and hidden state activation functions typically tanh ψis used to normalize or squash
the cell state which would be unbounded otherwise All gate activation functions are sigmoid ie
σx  1 1  exp x In later formulations multiple memory cells were combined in a vector
which allows the usage of recurrent weight matrices to mix the cell outputs of memory cells Greff
et al 2015 for more details see Appendix A1 Ablation studies showed that all components of the
memory cell are crucial Greff et al 2015
22 sLSTM
To empower LSTMs with the ability to revise storage decisions we introduce exponential gates
red together with normalization and stabilization In particular input and forget gates can have
exponential activation functions For normalization we introduce a normalizer state that sums up the
product of input gate times all future forget gates
3The sLSTM forward pass is
ctftct1itzt cell state 8
ntftnt1it normalizer state 9
htotht htctnt hidden state 10
ztφzt ztw
zxtrzht1bz cell input 11
itexpit
 itw
ixtriht1bi input gate 12
ftσ
ft
ORexp
ft
ftw
fxtrfht1bf forget gate 13
otσ ot  otw
oxtroht1bo output gate 14
We broadcast the original LSTM gating techniques ie input andor hiddendependent gating plus
bias term to the new architectures Exponential activation functions can lead to large values that cause
overflows Therefore we stabilize gates with an additional state mtMilakov  Gimelshein 2018 
mt max
logft mt1logit
stabilizer state 15
i
texp
log i tmt
exp
itmt
stabil input gate 16
f
texp
log f t mt1mt
stabil forget gate 17
We show in Appendix A2 that replacing ftbyf
tanditbyi
tin the forward pass does neither change
the output of the whole network nor the derivatives of the loss with respect to the parameters
New Memory Mixing sLSTM can have multiple memory cells like the original LSTM see
Appendix A2 Multiple memory cells enable memory mixing via recurrent connections RzRi
RfRofrom hidden state vector hto memory cell input zand the gates ifo respectively A new
aspect in memory mixing is the effect of exponential gating The new sLSTM can have multiple
heads with memory mixing within each head but not across heads The introduction of heads for
sLSTM together with exponential gating establishes a new way of memory mixing
23 mLSTM
To enhance storage capacities of LSTMs we increase the LSTM memory cell from a scalar cRto
a matrix CRdd Hence retrieval is performed via a matrix multiplication At time t we want to
store a pair of vectors the key ktRdand the value vtRdwe use the Transformer terminology
Later at time tτ the value vtshould be retrieved by a query vector qtτRd This is the setting
of Bidirectional Associative Memories BAMs Kohonen 1972 Anderson 1972 Nakano 1972
Anderson et al 1977 The covariance update rule Sejnowski 1977 Dayan  Willshaw 1991 for
storing a keyvalue pair is
CtCt1vtk
t 18
We assume a layernorm before projecting inputs to keys and values therefore they have zero mean
The covariance update rule is optimal Dayan  Willshaw 1991 for a maximal separability of
retrieved binary vectors which is equivalent to a maximal signalnoise ratio Higher separability is
possible when limiting retrieval to pairwise interactions and conceding quadratic complexity like
attention Krotov  Hopfield 2016 2017 Ramsauer et al 2021 The covariance update rule is
equivalent to Fast Weight Programmers Schmidhuber 1992 Schlag et al 2021 which have later
been equipped with a constant decay rate multiplied to Ct1and a constant learning rate multiplied
tovtk
tBa et al 2016a In this spirit we integrate the covariance update rule into the LSTM
framework where the forget gate corresponds to decay rate and the input gate to the learning rate
while the output gate scales the retrieved vector
For this matrix memory the normalizer state is the weighted sum of key vectors where each key
vector is weighted by the input gate and all future forget gates Again the normalizer state keeps
4record of the strength of the gates Since the dot product between query and normalizer state can
be close to zero we use the absolute value of this dot product and lower bound it by a threshold
typically 10 as done previously Sun et al 2023 The mLSTM forward pass is
CtftCt1itvtk
t cell state 19
ntftnt1itkt normalizer state 20
htotht htCtqtmaxnn
tqt1o
hidden state 21
qtWqxtbq query input 22
kt1
dWkxtbk key input 23
vtWvxtbv value input 24
itexpit
 itw
ixtbi input gate 25
ftσ
ft
ORexp
ft
ftw
fxtbf forget gate 26
otσot otWoxtbo output gate 27
mLSTM can have multiple memory cells like the original LSTM For mLSTM multiple heads and
multiple cells are equivalent as there is no memory mixing In order to stabilize the exponential gates
of mLSTM we use the same stabilization techniques as for sLSTM see Equation 15 Since the
mLSTM has no memory mixing this recurrence can be reformulated in a parallel version For more
details we refer to Appendix A3
24 xLSTM Architecture
Figure 3 xLSTM blocks Left A residual sLSTM block
with post upprojection like Transformers The input is fed
into an sLSTM  with an optional convolution  followed
by a gated MLP Right  A residual mLSTM block with
pre upprojection like State Space models mLSTM is
wrapped inside two MLPs via a convolution a learnable
skip connection and an output gate that acts component
wise See Figure 9 and Figure 10 in the appendix for detailsxLSTM Blocks An xLSTM block
should nonlinearly summarize the
past in a highdimensional space to
better separate different histories or
contexts Separating histories is the
prerequisite to correctly predict the
next sequence element such as the
next token We resort to Covers
Theorem Cover 1965 which states
that in a higher dimensional space
nonlinearly embedded patterns can
more likely be linearly separated
than in the original space We con
sider two residual block architectures
i A residual block with post up
projection like Transformers which
nonlinearly summarizes the past in
the original space then linearly maps
into a highdimensional space applies
a nonlinear activation function and
linearly maps back to the original
space see left panel of Figure 3 and third column in Figure 1 A more detailed version is de
picted in Figure 9 in the appendix ii A residual block with pre upprojection like State Space
Models which linearly maps to a highdimensional space nonlinearly summarizes the past in the
highdimensional space and then linearly maps back to the original space For an xLSTM block
containing an sLSTM we mostly use the post upprojection block For an xLSTM block containing
an mLSTM we use the pre upprojection block since the memory capacity becomes larger in the
highdimensional space See left panel of Figure 3 and third column in Figure 1 or Figure 9 in the
appendix for more details
5xLSTM Architecture An xLSTM architecture is constructed by residually stacking build
ing blocks Srivastava et al 2015 He et al 2016 We rely on the most commonly used pre
LayerNorm Ba et al 2016b residual backbones as used in contemporary Large Language Models
See last column in Figure 1
25 Memory and Speed Considerations
Contrary to Transformers xLSTM networks have a linear computation and a constant memory
complexity with respect to the sequence length Since the xLSTM memory is compressive it is well
suited for industrial applications and implementations on the edge
The memory of mLSTM does not require parameters but is computationally expensive through its dd
matrix memory and ddupdate We trade off memory capacity against computational complexity
Nevertheless the computations can be done in parallel on GPUs therefore these computations have
only a minor effect on the wall clock time
While mLSTM is parallelizable analog to FlashAttention Dao et al 2022 Dao 2024 or GLA Yang
et al 2023 sLSTM is not parallelizable due to the memory mixing hiddenhidden connections
However we developed a fast CUDA implementation with GPU memory optimizations to the register
level which is typically less than two times slower than mLSTM
3 Related Work
Linear Attention Several methods have been suggested to overcome the quadratic complexity
in terms of context length of the Transformer and make attention linear in the context length The
Synthesizer learns synthetic attention weights without tokentoken interactions Tay et al 2020
Linformer realizes selfattention by a lowrank matrix and even linearly approximates it Wang
et al 2020 Linear Transformer linearizes the attention mechanism Katharopoulos et al 2020
Performer linearly approximates the attention softmax by positive orthogonal random features
approach Choromanski et al 2021 Attention has been replaced by fast long convolutions in the
Structured Global Convolution SGConv Li et al 2022 and the Hyena Hierarchy Poli et al 2023
State Space Models Recently State Space Models SSMs became very popular since they are
linear in the context length and show promising performance compared to Transformers One of the
first proposed models was Structured State Space sequence model S4 Gu et al 2021 followed by
Diagonal State Space DSS model Gupta et al 2022 Gated State Space GSS models Mehta
et al 2022 S5 model Smith et al 2022 Bidirectional Gated SSM BiGS Wang et al 2022 H3
model Fu et al 2023 and Mamba Gu  Dao 2023
Recurrent Neural Networks Recurrent Neural Networks RNNs have been suggested to replace
Transformer and attention due to their linearity in the context length RNNs with Deep Linear
Recurrent Units LRUs showed promising results for language modeling Orvieto et al 2023 De
et al 2024 as did Hierarchically Gated Linear RNN HGRN Qin et al 2023 and HGRN2 Qin
et al 2024 A wellknown RNN approach to large language modeling is RWKV Peng et al 2023
2024 showcasing competitive performance to Transformers
Gating One of the key ideas of LSTM is gating which was rediscovered and reinterpreted in many
recent approaches Gating was used in HGRN Qin et al 2023 HGRN2 Qin et al 2024 Gated
Linear Attention GLA Yang et al 2023 Gated State Space GSS models Mehta et al 2022
Bidirectional Gated SSM BiGS Wang et al 2022 Moving Average Equipped Gated Attention
MEGA Ma et al 2022 RWKV Peng et al 2023 and Mamba Gu  Dao 2023
Covariance Update Rule To enhance storage capacities we equipped the mLSTM cell with
a matrix memory with a covariance update rule Other methods which build on such an update
mechanism are Fast Weight Programmers Schmidhuber 1992 Schlag et al 2021 RWKV5 and
RWKV6 Peng et al 2024 Retention Sun et al 2023 Linear Transformer Katharopoulos et al
2020 and HGRN2 Qin et al 2024
6Most Related Conceptually the closest models to xLSTM are Retention Sun et al 2023
RWKV Peng et al 2023 2024 and HGRN2 Qin et al 2024 These models share the con
cepts matrix memory andor gating However in contrast to the new sLSTM these approaches do
not allow memory mixing Memory mixing enables to solve state tracking problems and therefore
LSTMs are more expressive than State Space Models SSMs and Transformers Merrill et al 2024
Delétang et al 2023 State tracking is required to evaluate code or to track entities in a long narrative
Residually Stacking Architectures Like almost all contemporary large deep learning models
xLSTM architectures are constructed by residually stacking building blocks Srivastava et al 2015
He et al 2016 This construction enabled deep convolutional networks He et al 2016 and
Transformers Vaswani et al 2017 Transformers are the ultimate force behind Large Language
Models LLMs like GPT3 Brown et al 2020 ChatGPT Schulman et al 2022 GPT4 Achiam
et al 2023 MegatronLM Shoeybi et al 2019 Gopher Rae et al 2021 ERNIE 30 Titan Wang
et al 2021 GLaM Du et al 2021 Chinese M6 Lin et al 2021 mutilingual AlexaTM
20B Soltan et al 2022 OPT Zhang et al 2022 Chinchilla Hoffmann et al 2022 BLOOM Scao
et al 2022 GLM130B Zeng et al 2022 LaMDA Thoppilan et al 2022 PaLM Chowdhery
et al 2022 Llama Touvron et al 2023 Gemini Google 2023 Reid et al 2024
4 Experiments
In this section we experimentally evaluate xLSTM and compare it to existing methods with a focus on
language modeling We investigate xLSTMs specific capabilities on synthetic tasks in Section 41 In
Section 42 we compare the validation set perplexity of various current language modeling methods
that were trained on 15B tokens from SlimPajama Soboleva et al 2023 On the same dataset we
perform ablation studies for xLSTM Then we assess the scaling behavior of the different methods
analogous to Kaplan et al 2020 and Brown et al 2020 In Section 43 we conduct a more
thorough language modeling experiment We compare xLSTM and the best performing methods from
Section 42 after being trained on 300B tokens from SlimPajama Soboleva et al 2023 First we
assess how well the methods perform in extrapolating to longer contexts secondly we test the methods
via validation perplexity and performance on downstream tasks Sutawika et al 2024 thirdly we
evaluate the methods on 571 text domains of the PALOMA language benchmark dataset Magnusson
et al 2023 fourthly we again assess the scaling behavior of the different methods but now with 20
times more training data
For all experiments we use the notation xLSTM ab for the ratio abof mLSTMbased versus
sLSTMbased xLSTM blocks For example xLSTM71 means that out of eight blocks seven are
mLSTMbased blocks and one is an sLSTMbased block For a common total block number of 48
this translates to 6 sLSTMbased blocks and 42 mLSTMbased blocks Further for all experiments
we use pre and post upprojection blocks for mLSTM and sLSTM respectively
41 Synthetic Tasks and Long Range Arena
First we test the effectiveness of xLSTMs new exponential gating with memory mixing on formal
languages Delétang et al 2023 Then we assess the effectiveness of xLSTMs new matrix memory
on the MultiQuery Associative Recall task Arora et al 2023 Finally xLSTMs performance at
processing long sequences in the Long Range Arena is evaluated Tay et al 2021
Test of xLSTMs Exponential Gating with Memory Mixing We test xLSTMs new exponential
gating with memory mixing which should enable it to solve state tracking problems Merrill
et al 2024 Merrill  Sabharwal 2023 We implement and extend the formal language tasks
from Delétang et al 2023 to enable multilength training for length extrapolation For a detailed
description of all tasks and extended results see Appendix B11 We compare xLSTM to other
methods including Transformers State Space Models and Recurrent Neural Networks The accuracy
of the tested methods is evaluated on those tokens relevant to the task The accuracy is scaled between
0 random and 1 perfect We compare 2block architectures of the following methods on these
tasks xLSTM01 ie only sLSTM xLSTM10 ie only mLSTM xLSTM11 Llama
Mamba RWKV  Retention Hyena LSTM and LSTM in Transformer blocks LSTM Block The
results of this experiment are shown in Figure 4 Models such as Transformers or State Space Models
without memory mixing no state tracking cannot solve eg regular grammars like the parity task
7Bucket SortMissing
DuplicateMod
Arithmetic
w BracketsSolve
Equation Cycle Nav Even PairsMod
Arithmetic
wo Brackets Parity MajorityMajority
Count
Llama
Mamba
Retention
Hyena
RWKV4
RWKV5
RWKV6
LSTM
Block
LSTM
xLSTM01
xLSTM10
xLSTM11092
002008
00002
00002
00004
00110
00003
00003
001037
001013
00
069
00015
00004
001005
002086
00410
00005
002013
002069
001045
003
013
001003
00003
00003
00005
001051
007004
00005
001036
00012
001
03
002006
002005
00002
00006
001093
007004
00004
00036
001018
002
054
00021
001006
00007
00013
0010
00007
00006
00063
00013
00
049
004015
001008
00008
00026
00510
00015
002006
003073
001034
003
096
00023
006009
001009
002031
01410
00016
00022
012076
001024
001
099
00015
00076
0005
005097
00310
00091
00910
00058
002027
00
094
00102
00072
004038
005093
00710
0010
0010
00082
002033
00
084
008023
001057
009055
00910
0010
0010
0010
00075
002022
00
097
00033
022003
00003
001086
00110
00004
00004
001074
001046
00
07
02102
001015
006024
00408
00310
0006
0410
00064
00405
00Context SentsitiveDeterministic
Context Free RegularFigure 4 Test of xLSTMs exponential gating with memory mixing Results are given by the scaled
accuracy of different models at solving formal language tasks of which some require state tracking
The different tasks are grouped by the Chomsky hierarchy
This result is in agreement with findings that Transformers and State Space models are fundamentally
less powerful than RNNs Merrill et al 2024 Merrill  Sabharwal 2023 Delétang et al 2023
Test of xLSTMs Memory Capacities on Associative Recall Tasks In this experiment we test
xLSTMs new matrix memory in terms of the memory capacity on the MultiQuery Associative
Recall task Arora et al 2023 For each sequence keyvalue pairs are randomly chosen from a
large vocabulary which must be memorized for later retrieval To enhance the difficulty of the
original task we increase the number of keyvalue pairs up to 256 and extend the context length up
to 2048 Thus we have broader tests for the memory capacities of different models We compare
2block architectures of Llama Mamba RWKV5 RWKV6 xLSTM11 and xLSTM10 The
models are evaluated by the accuracy at recalling the pairs Since Transformers eg Llama have
a memory that is exponential in the coding dimension Ramsauer et al 2021 they constitute the
gold standard at this task Results are shown in Figure 5 xLSTM11 performs best among all
nonTransformer models also for small models Interestingly the sLSTM block does not diminish
the memory capacity but rather leverages it which becomes evident at the most difficult task with 256
keyvalue pairs Additional results are presented in Appendix B12 where extrapolation analyses
indicate that xLSTMs enhanced memory capacities also pertain when extrapolating to contexts that
are longer than those seen during training
32 64 128 256 512
Model Dim000025050075100Accuracy
KV Pairs  48
32 64 128 256 512
Model Dim
KV Pairs  96
32 64 128 256 512
Model Dim
KV Pairs  256Llama Mamba RWKV5 RWKV6 xLSTM10 xLSTM11
Figure 5 Test of memory capacities of different models at the MultiQuery Associative Recall task
with context length 2048 Each panel is dedicated to a different number of keyvalue pairs The
xaxis displays the model size and the yaxis the validation accuracy
8Test of xLSTMs Long Context Capabilities on Long Range Arena To assess xLSTMs per
formance on long sequences and large contexts we compare different methods on the Long Range
Arena Tay et al 2021 xLSTM demonstrates consistent strong performance on all of the tasks
suggesting that the xLSTM architecture is remarkably efficient in handling different aspects of long
context problems For more details see Appendix B13
42 Method Comparison and Ablation Study
The main question of this paper is what can we achieve in language modeling when scaling up the
new LSTM variants Therefore we train xLSTMs Transformers State Space Models and other
methods on 15B tokens from SlimPajama in an autoregressive language modeling setting We
compare the trained models on the validation set Finally we perform ablation studies for xLSTM
ModelParams
MSlimPajama
15B ppl 
GPT3 356 1426
Llama 407 1425
H3 420 1823
Mamba 423 1370
Hyena 435 1759
RWKV4 430 1562
RWKV5 456 1653
RWKV6 442 1740
RetNet 431 1623
HGRN 411 2183
GLA 412 1956
HGRN2 411 1677
xLSTM10 409 1343
xLSTM71 408 1348
Table 1 Method comparison on next
token prediction when trained on 15B
tokens from SlimPajama Best valida
tion perplexities within model classes ie
Transformers LSTMs SSMs RNNs and
linear Transformers are underlined and
overall best is in bold For each model
class the best performing methods are
later used in Section 43 for LLM training
xLSTMs with new memory xLSTM10
and xLSTM71 perform bestComparing xLSTM to Other Methods For com
parison we train models on 15B tokens from SlimPa
jama Soboleva et al 2023 The trained models
are evaluated by their perplexity on the validation
set We compare the following methods xLSTM
our new method GPT3 Transformer Brown et al
2020 Llama Transformer Touvron et al 2023 H3
SSM Fu et al 2023 Mamba SSM Gu  Dao
2023 RWKV4 RNN Peng et al 2023 RWKV5
RNN Peng et al 2024 RWKV6 RNN Peng et al
2024 GLA linear Transformer Yang et al 2023
HGRN RNN Qin et al 2023 HGRN2 RNN Qin
et al 2024 RetNet linear Transformer Sun et al
2023 Hyena linear Transformer Poli et al 2023
xLSTM10 and xLSTM71 see Section 4 The
models were trained with mixed precision except
RWKV5 RWKV6 GLA HGRN HGRN2 where
mixedprecision training was not supported by the ref
erence implementation We categorize the methods into
a Transformers b State Space Models SSMs and
c Recurrent Neural Networks RNNs together with lin
ear Transformers Linear Transformers are linear meth
ods that substitute the Transformer attention mechanism
The models match a GPT3 model with 350M param
eters in size ie embedding dim 1024 and 24 residual
blocks Only GPT3 uses shared weights for token and
output embeddings therefore has fewer parameters The
results in Table 1 show that xLSTM outperforms all ex
isting methods in validation perplexity For details see
Appendix B2 Figure 6 shows the scaling behaviour
for this experiment indicating that xLSTM will also
perform favorably for larger models
Ablation Studies Table 1 and Figure 6 demonstrate that xLSTM achieves excellent results at
language modeling when being trained on 15B tokens from SlimPajama Thus it is only natural
to ask which of the elements of xLSTM is responsible for the improvements over vanilla LSTM
performances evoking an ablation study of the individual new xLSTM components For doing
so we morph a vanilla LSTM architecture stepbystep into an xLSTM architecture First we
integrate LSTM layers into preLayerNorm residual backbones second we extend this to a post
upprojection block then we add exponential gating and finally the matrix memory The results are
shown in Table 2 top The ablation studies attribute the strong performance improvement to both
the exponential gating and the matrix memory Additionally since gating is an everoccuring topic
in RNNs and State Space Models we ablate different gating mechanisms In Table 2 bottom we
conclude that having each gate learnable and influenced by the input has an incremental positive
effect Additional studies on the individual backbone components are discussed in Appendix B2
902 04 10 14 27
Number of Parameters 10910111213141516171819Validation Perplexity
Llama
Mamba
RWKV4
xLSTM71
xLSTM10
15B T okensFigure 6  Method comparison
on next token prediction when
trained on 15B tokens from
SlimPajama Performance mea
sure in validation perplexity for
the best methods of each model
class see Table 1 are reported
The performance degradation of
xLSTM71 at 27B is due to
initially slower training conver
gence that leads to an especially
undertrained model xLSTM is
the best method at all sizes
Ablation studies on the new xLSTM components
Model ModificationExponential
GatingMatrix
MemoryParams
MSlimPajama
15B ppl 
LSTMVanilla MultiLayer LSTM   6078 241786
Adding Resnet Backbone   5061 3546
Adding UpProjection Backbone   5059 2601
xLSTM01 Adding Exponential Gating   4273 1770
xLSTM71 Adding Matrix Memory   4084 1348
Ablation studies on different gating techniques
Learnable GatesForget Gate Input GateSlimPajama
15B ppl  Input
DependentLearnable
BiasBias
InitInput
DependentLearnable
BiasBias
Init
No Gates      0 NaN
No Gates   36   0 1395
Forget Gate   36   0 1358
Input Gate   36   N001 1369
Forget Gate Bias   36   0 1376
Forget  Input Gate Bias   36   N001 1373
Forget Gate  Input Gate Bias   36   N001 1355
Forget Gate  Input Gate   36   N001 1343
Table 2 Ablation studies Top Ablation studies on the new xLSTM components contributing
the strong performance improvement of xLSTM over vanilla LSTM to both the exponential gating
and the matrix memory Bottom Ablation studies on different gating techniques We consider an
xLSTM10 with sigmoid forget gate and exponential input gate Bias initialization means that the
forget gate is set to one 36indicates that values are taken equidistant in the respective interval and
N001that values are randomly chosen from a Gaussian with mean 0and std 01 PPL denotes
validation perplexity The first two lines correspond to models similar to linearized attention line
four to Retention line five to RWKV5 and line six to RWKV6 Dependencies of the gates on the
input lead to better performance
1043 xLSTM as Large Language Model
We culminate this study in largescale language modeling experiments testing the potential of
xLSTM as an LLM We therefore increase the amount of training data and train on 300B tokens from
SlimPajama The same number of tokens is used in eg Mamba Gu  Dao 2023 and Griffin De
et al 2024 We compare xLSTM RWKV4 Llama and Mamba which were selected as the
bestperforming methods in their respective method classes in the model comparison in Section 42
We train different model sizes 125M 350M 760M 13B test all models for length extrapolation
capabilities and evaluate their performance on the validation set We assess their performance on
downstream tasks test their performance in language modeling on 471 text domains of the PALOMA
benchmark and finally investigate their scaling law behavior
Sequence Length Extrapolation First we test the sequence length extrapolation for 13Bsized
large models of xLSTM RWKV4 Llama and Mamba All models are trained on context length
2048 and then tested for context lengths up to 16384 See Figure 7 for the results In contrast to
other methods xLSTM models maintain low perplexities for longer contexts
ModelSlimPajama
300B ppl 
at 16k
Llama 33783
Mamba 1400
RWKV4 1375
xLSTM71 892
xLSTM10 901
Figure 7 Sequence extrapolation in language modeling This is a comparison of 13Bsized large
models of xLSTM RWKV4 Llama and Mamba at next token prediction on the SlimPajama
validation set after training on 300B tokens from SlimPajama Models are trained with context length
2048 and then tested for context lengths up to 16384 Left Token perplexities evaluated at different
context lengths In contrast to other methods xLSTM models remain at low perplexities for longer
contexts Right Prediction quality when extrapolating to long context sizes in terms of validation
perplexity PPL xLSTM yields the best PPL values best in bold second best underlined
Validation Perplexity and Downstream Tasks Secondly for all model sizes we evaluate the
performance of xLSTM RWKV4 Llama and Mamba models on the SlimPajama validation set for
next token prediction and on downstream tasks that measure common sense reasoning The third
column of Table 3 lists the validation set perplexities of different methods Both xLSTM10 and
xLSTM71 are the best models for all model sizes with respect to the validation set perplexity The
other columns of Table 3 provide the performance on downstream tasks In the vast majority of tasks
and across all model sizes xLSTM is the best method  only on the ARC task Mamba is in some
cases the best method For details see Appendix B3
Performance on PALOMA Language Tasks Thirdly for all model sizes we test the next token
prediction performance of xLSTM RWKV4 Llama and Mamba models on PALOMA language
tasks Magnusson et al 2023 We measure the performance by the perplexity for next token
prediction on 571 text domains which range from nytimescom to rdepression on Reddit Table 4
shows token prediction perplexity grouped into language modeling first seven columns and fine
grained domain benchmarks last 5 columns xLSTM10 performs better than xLSTM71 on
these language tasks xLSTM10 has in 568 out of 571 995 text domains a lower perplexity
than Mamba in 486 out of 571 851 a lower perplexity than Llama in 570 out of 571 998 a
lower perplexity than RWKV4 For details see Appendix B3
11ModelParams
MSlimPajama
300B ppl LAMBADA
pplLAMBADA
accHellaSwag
accPIQA
accARCE
accARCC
accWinoGrande
accAverage
acc125MRWKV4 1694 1666 5472 2377 3403 6600 4794 2406 5091 4112
Llama 1622 1589 3921 3154 3409 6545 4533 2363 5067 4178
Mamba 1678 1508 2776 3414 3647 6676 4886 2440 5114 4363
xLSTM10 1638 1463 2598 3652 3674 6561 4781 2483 5185 4389
xLSTM71 1637 1460 2659 3608 3675 6687 4832 2526 5170 4416350MRWKV4 4305 1262 2157 3662 4247 6942 5446 2543 5122 4660
Llama 4066 1219 1573 4419 4445 6915 5223 2628 5359 4832
Mamba 4231 1164 1283 4624 4755 6970 5547 2756 5430 5014
xLSTM10 4093 1131 1149 4933 4806 6959 5572 2662 5438 5062
xLSTM71 4084 1137 1211 4774 4789 7116 5661 2782 5328 5075760MRWKV4 8910 1055 1098 4743 5229 7269 5884 2884 5541 5258
Llama 8341 1060 990 5141 5216 7095 5648 2875 5667 5274
Mamba 8705 1024 924 5084 5397 7116 6044 2978 5699 5386
xLSTM10 8404 986 809 5478 5572 7269 6275 3259 5817 5612
xLSTM71 8397 991 807 5527 5612 7274 6136 2961 5643 552613BRWKV4 15152 983 984 4978 5620 7470 6183 3063 5556 5478
Llama 14204 944 723 5744 5781 7312 6279 3174 5904 5699
Mamba 14753 914 741 5564 6045 7443 6612 3370 6014 5841
xLSTM10 14226 889 686 5783 6091 7459 6431 3259 6062 5848
xLSTM71 14201 900 704 5669 6026 7492 6511 3234 5927 5810
Table 3 Validation set perplexity and downstream tasks Comparison of xLSTM RWKV4 Llama
and Mamba on the validation set at next token prediction and on downstream tasks after training on
300B tokens from SlimPajama Model sizes are 125M 250M 760M and 13B The first column
shows the methods and the second the actual number of parameters The third column lists the
validation set perplexities while the remaining columns show the performance on downstream tasks
Best model per model size is depicted bold and the second best is underlined In the vast majority of
tasks and across all model sizes xLSTM is the best method  only on the ARC task Mamba is in
some cases the best method xLSTM10 and xLSTM71 are the two best models with respect to
validation set perplexity
ModelParams
MC4MC4
ENWikitext
103Penn
TreebankRed
PajamaRefined
WebDolmaM2D2
S2ORCM2D2
WikipediaC4
DomainsDolma
SubredditsDolma
CodingAverage125MRWKV4 1694 2625 2233 2918 3845 899 3247 1704 2386 2142 2268 3708 512 2374
Llama 1622 2464 1723 2316 3156 826 2915 1510 1971 2041 2145 3673 361 2092
Mamba 1678 2312 1704 2249 3063 796 2773 1460 1938 1936 2014 3432 377 2005
xLSTM10 1638 2254 1632 2198 3047 780 2721 1435 1902 1904 1965 3415 364 1968
xLSTM71 1637 2239 1613 2147 3001 775 2691 1413 186 1884 1952 339 359 1944350MRWKV4 4305 1955 1582 1964 2758 697 2428 1294 1759 1596 1698 2940 390 1755
Llama 4066 1838 1328 1641 2182 656 2209 1176 1505 1525 1599 2830 312 1567
Mamba 4231 1733 1305 1611 2224 634 2104 1142 1483 1453 1516 2702 320 1519
xLSTM10 4093 1701 1255 1517 2251 620 2066 1116 1444 1427 1485 2670 308 1488
xLSTM71 4084 1698 1268 1543 2186 623 2070 1122 1462 1430 1485 2661 311 1488760MRWKV4 8910 1551 1276 1484 2139 591 1928 1070 1427 1304 1368 2422 332 1408
Llama 8341 1575 1159 1347 1833 582 1904 1033 1300 1305 1376 2480 290 1349
Mamba 8705 1508 1154 1347 1934 569 1843 1015 1305 1262 1325 2394 299 1330
xLSTM10 8404 1460 1103 1261 1774 552 1787 985 1250 1220 1281 2346 287 1276
xLSTM71 8397 1472 1111 1268 1761 555 1801 987 1259 1225 1289 2343 288 128013BRWKV4 15152 1451 1204 1373 1937 562 1825 1011 1346 1210 1287 2285 325 1318
Llama 14204 1393 1044 1174 1592 529 1703 935 1161 1153 1224 2263 274 1204
Mamba 14753 1335 1040 1176 1665 521 1650 917 1173 1118 1183 2143 283 1184
xLSTM10 14226 1313 1009 1141 1592 510 1625 901 1143 1095 1160 2129 273 1158
xLSTM71 14201 1331 1021 1132 1600 516 1648 911 1161 1110 1176 2150 275 1169
Table 4 Performance on PALOMA Language Modeling Tasks Comparison of xLSTM RWKV4
Llama and Mamba by the perplexity of next token prediction on the PALOMA language benchmark
after training on 300B tokens from SlimPajama Model sizes are 125M 250M 760M and 13B
The second column shows the actual number of parameters The 571 text domains are grouped into
language modeling next seven columns and finegrained domain benchmarks further 5 columns
The last column shows the average perplexity across all of these tasks Best model per model size is
given in bold and the second best is underlined xLSTM yields the best performance
12Scaling Laws Fourthly we assess the powerlaw scaling behavior which allows to extrapolate the
performance to larger model sizes Kaplan et al 2020 Brown et al 2020 Figure 8 presents the
scaling behavior All models share a similar scaling behavior but with different offsets RWKV4
performs worst followed by Llama and Mamba xLSTM is better than Mamba with a similar margin
to Mamba as Mamba has to Llama The scaling behavior indicates that for larger models xLSTM
will continue to perform favourable compared to Transformers and StateSpace models
02 04 10 14
Number of Parameters 10991011121314151617Validation Perplexity
Llama
Mamba
RWKV4
xLSTM71
xLSTM10
300B T okens
Figure 8 Scaling laws Next token prediction perplexity of xLSTM RWKV4 Llama and Mamba
on the SlimPajama validation set when trained on 300B tokens from SlimPajama Model sizes are
125M 350M 760M and 13B Best models for each model class see Table 1 were selected The
scaling laws indicate that for larger models xLSTM will perform well too
5 Limitations
i In contrast to mLSTM memory mixing of the sLSTM prohibits parallelizable operations and
therefore does not allow a fast parallel implementation Nevertheless we developed a fast CUDA ker
nel for sLSTM which is currently around 15 times slower than our parallel mLSTM implementation
ii The CUDA kernels for mLSTM are not optimized and therefore the current implementation is
about 4 times slower than FlashAttention or the scan used in Mamba Faster CUDA kernels could be
obtained in the vein of FlashAttention iii The matrix memory of mLSTM has high computation
complexity since ddmatrices must be processed Still the memory update and retrieval does not
use parameters and can be parallelized using standard matrix operations therefore the wall clock
time overhead due to the complex memory is minor iv The initialization of the forget gates must be
chosen carefully v Since the matrix memory is independent of the sequence length increasing the
sequence length might overload the memory for longer context sizes Still this does not appear to be
a limitation for contexts up to 16k see Section 43 vi Due to the expensive computational load for
large language experiments we did neither fully optimize the architecture nor the hyperparameters
especially for larger xLSTM architectures We anticipate that an extensive optimization process is
needed for xLSTM to reach its full potential
136 Conclusion
We have partly answered our simple question How far do we get in language modeling when scaling
LSTM to billions of parameters So far we can answer At least as far as current technologies
like Transformers or State Space Models We have enhanced LSTM to xLSTM by exponential
gating with memory mixing and a new memory structure xLSTM models perform favorably on
language modeling when compared to stateoftheart methods like Transformers and State Space
Models The scaling laws indicate that larger xLSTM models will be serious competitors to current
Large Language Models that are built with the Transformer technology xLSTM has the potential to
considerably impact other deep learning fields like Reinforcement Learning Time Series Prediction
or the modeling of physical systems
Acknowledgements
We thank Sebastian Lehner Daniel Klotz Thomas Adler Matthias Dellago Gerald Gutenbrunner
Fabian Paischer Vihang Patil Niklas Schmidinger Benedikt Alkin Kajetan Schweighofer Anna
Zimmel Lukas Aichberger Lukas Hauzenberger Bernhard Schäfl Johannes Lehner for helpful
discussions and feedback
References
J Achiam S Adler S Agarwal et al GPT4 technical report ArXiv  230308774 2023
J Anderson J Silverstein S Ritz and R Jones Distinctive features categorical perception and
probability learning Some applications of a neural model Psychological Review  84413451
1977 doi 1010370033295X845413
J A Anderson A simple neural network generating an interactive memory Mathematical Bio
sciences  14 1972 doi 1010160025556472900752
S Arora S Eyuboglu A Timalsina I Johnson M Poli J Zou A Rudra and C Ré Zoology
Measuring and improving recall in efficient language models ArXiv  231204927 2023
J Ba G E Hinton V  Mnih J Z Leibo and C Ionescu Using fast weights to attend to the recent
past In D D Lee M Sugiyama U V  Luxburg I Guyon and R Garnett eds Advances in
Neural Information Processing Systems 29  pp 43314339 Curran Associates Inc 2016a
J Ba J R Kiros and G Hinton Layer normalization ArXiv  160706450 2016b
A Bau Y  Belinkov H Sajjad N Durrani F Dalvi and J Glass Identifying and controlling
important neurons in neural machine translation In International Conference on Learning Repre
sentations ICLR  2019 URL httpsopenreviewnetforumidH1zPsR5KX 
Y  Bisk R Zellers R LeBras J Gao and Y  Choi Piqa Reasoning about physical commonsense in
natural language In AAAI Conference on Artificial Intelligence  volume 34 pp 74327439 2020
S L Blodgett L Green and B OConnor Demographic dialectal variation in social media A case
study of AfricanAmerican English In Conference on Empirical Methods in Natural Language
Processing  pp 11191130 2016 doi 1018653v1D161120
T Brown B Mann N Ryder et al Language models are fewshot learners In H Larochelle
M Ranzato R Hadsell MF Balcan and H Lin eds Advances in Neural Information Processing
Systems  volume 33 pp 18771901 Curran Associates Inc 2020
K M Choromanski V  Likhosherstov D Dohan X Song A Gane T Sarlós P Hawkins J Q
Davis A Mohiuddin L Kaiser D B Belanger L J Colwell and A Weller Rethinking
attention with performers In 9th International Conference on Learning Representations ICLR 
OpenReviewnet 2021 URL httpsopenreviewnetforumidUa6zuk0WRH 
A Chowdhery S Narang J Devlin et al PaLM scaling language modeling with pathways ArXiv 
220402311 2022
14A Chronopoulou M Peters and J Dodge Efficient hierarchical domain adaptation for pretrained lan
guage models In Conference of the North American Chapter of the Association for Computational
Linguistics  pp 13361351 2022 doi 1018653v12022naaclmain96
P Clark I Cowhey O Etzioni T Khot A Sabharwal C Schoenick and O Tafjord Think you have
solved question answering Try ARC the AI2 reasoning challenge ArXiv  180305457 2018
T M Cover Geometrical and statistical properties of systems of linear inequalities with applications
in pattern recognition Electronic Computers IEEE Transactions on  EC143326334 1965
T Dao Flashattention2 Faster attention with better parallelism and work partitioning In In
ternational Conference on Learning Representations ICLR  volume 12 2024 URL https
openreviewnetforumidmZn2Xyh9Ec 
T Dao D Y  Fu S Ermon A Rudra and C Ré Flashattention Fast and memoryefficient exact
attention with IOawareness In A H Oh A Agarwal D Belgrave and K Cho eds Advances
in Neural Information Processing Systems NeurIPS  2022 URL httpsopenreviewnet
forumidH4DqfPSibmx 
P Dayan and D J Willshaw Optimising synaptic learning rules in linear associative memories
Biological Cybernetics  65 1991 doi 101007bf00206223
S De S L Smith A Fernando A Botev G CristianMuraru A Gu R Haroun L Berrada
Y  Chen S Srinivasan G Desjardins A Doucet D Budden Y  W Teh R Pascanu N DeFreitas
and C Gulcehre Griffin Mixing gated linear recurrences with local attention for efficient language
models ArXiv  240219427 2024
J Degrave F Felici J Buchli et al Magnetic control of tokamak plasmas through deep reinforcement
learning Nature  602414419 2022 doi 101038s41586021043019
G Delétang A Ruoss J GrauMoya T Genewein L K Wenliang E Catt C Cundy M Hutter
S Legg J Veness and P A Ortega Neural networks and the Chomsky hierarchy In International
Conference on Learning Representations ICLR  volume 11 2023 URL httpsopenreview
netforumidWbxHAzkeQcn 
N Du Y  Huang A M Dai et al GLaM efficient scaling of language models with mixtureof
experts ArXiv  211206905 2021
D Y  Fu T Dao K K Saab A W Thomas A Rudra and C Re Hungry hungry hippos Towards
language modeling with state space models In The Eleventh International Conference on Learning
Representations  2023 URL httpsopenreviewnetforumidCOZDy0WYGg 
L Gao S Biderman S Black L Golding T Hoppe C Foster J Phang H He A Thite
N Nabeshima S Presser and C Leahy The Pile An 800gb dataset of diverse text for lan
guage modeling ArXiv  210100027 2021
F A Gers J Schmidhuber and F Cummins Learning to forget Continual prediction with LSTM
Neural Compututation  121024512471 2000
Gemini Team Google Gemini A family of highly capable multimodal models ArXiv  231211805
2023
A Graves Generating sequences with recurrent neural networks ArXiv  13080850 2013
S Greenbaum and G Nelson The international corpus of English ICE project World Englishes 
151315 1996
K Greff R K Srivastava J Koutník B R Steunebrink and J Schmidhuber LSTM A search
space odyssey ArXiv  150304069 2015
A Gu and T Dao Mamba Lineartime sequence modeling with selective state spaces ArXiv 
231200752 2023
A Gu K Goel and C Ré Efficiently modeling long sequences with structured state spaces ArXiv 
211100396 2021
15A Gupta A Gu and J Berant Diagonal state spaces are as effective as structured state spaces
ArXiv  220314343 2022
K He X Zhang S Ren and J Sun Deep residual learning for image recognition In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition CVPR  pp 770778 2016
S Hochreiter Untersuchungen zu dynamischen neuronalen Netzen Masters thesis Technische
Universität München 1991
S Hochreiter and J Schmidhuber Long shortterm memory Neural Computation  9817351780
1997a
S Hochreiter and J Schmidhuber LSTM can solve hard long time lag problems In M C Mozer
M I Jordan and T Petsche eds Advances in Neural Information Processing Systems NeurIPS 
volume 9 pp 473479 MIT Press Cambridge MA 1997b
S Hochreiter Y  Bengio P Frasconi and J Schmidhuber Gradient flow in recurrent nets the
difficulty of learning longterm dependencies In J Kolen and S Kremer eds A Field Guide to
Dynamical Recurrent Networks  IEEE 2000
S Hochreiter A Steven Younger and Peter R Conwell Learning to learn using gradient descent
In G Dorffner H Bischof and K Hornik eds Proc Int Conf on Artificial Neural Networks
ICANN 2001  pp 8794 Springer 2001
S Hochreiter M Heusel and K Obermayer Fast modelbased protein homology detection without
alignment Bioinformatics  231417281736 2007
J Hoffmann S Borgeaud A Mensch et al Training computeoptimal large language models ArXiv 
220315556 2022
M D Hossain F Sohel M F Shiratuddin and H Laga A comprehensive survey of deep learning
for image captioning ACM Computing Surveys CSUR  516118 2019
J Kaplan S McCandlish T Henighan T B Brown B Chess R Child S Gray A Radford J Wu
and D Amodei Scaling laws for neural language models ArXiv  200108361 2020
A Karpathy The unreasonable effectiveness of recurrent neural networks
httpkarpathygithubio20150521rnneffectiveness 2015
A Karpathy OpenAI Five defeats Dota 2 world champions httpsopenaicomresearchopenaifive
defeatsdota2worldchampions 2019
A Karpathy and L FeiFei Deep visualsemantic alignments for generating image descriptions In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition CVPR  pp
31283137 2015
A Katharopoulos A Vyas N Pappas and F Fleuret Transformers are RNNs Fast autoregressive
transformers with linear attention In E H Daumé III and A Singh eds International Conference
on Machine Learning ICML  volume 119 of Proceedings of Machine Learning Research  pp
51565165 PMLR 2020
T Katsch GateLoop Fully datacontrolled linear recurrence for sequence modeling ArXiv 
231101927 2023
D Kocetkov R Li L BenAllal J Li C Mou C Mu nozFerrandis Y  Jernite M Mitchell
S Hughes T Wolf D Bahdanau L vonWerra and H deVries The Stack 3 TB of permissively
licensed source code ArXiv  221115533 2022
T Kohonen Correlation matrix memories IEEE Transactions on Computers  C214 1972 doi
101109tc19725008975
F Kratzert D Klotz C Brenner K Schulz and M Herrnegger Rainfallrunoff modelling using long
shortterm memory LSTM networks Hydrology and Earth System Sciences  221160056022
2018
16F Kratzert D Klotz G Shalev G Klambauer S Hochreiter and G Nearing Benchmarking a
catchmentaware long shortterm memory network LSTM for largescale hydrological modeling
ArXiv  190708456 2019
A Krizhevsky Learning multiple layers of features from tiny images Masters thesis Deptartment
of Computer Science University of Toronto 2009
D Krotov and J J Hopfield Dense associative memory for pattern recognition In D D Lee
M Sugiyama U V  Luxburg I Guyon and R Garnett eds Advances in Neural Information
Processing Systems  pp 11721180 Curran Associates Inc 2016
D Krotov and J J Hopfield Dense associative memory is robust to adversarial inputs ArXiv 
170100939 2017
Y  Lakretz G Kruszewski T Desbordes D Hupkes S Dehaene and M Baroni The emergence of
number and syntax units in LSTM language models In J Burstein C Doran and T Solorio eds
Conference of the North American Chapter of the Association for Computational Linguistics  pp
1120 Association for Computational Linguistics 2019 doi 1018653v1N191002
Y  Li T Cai Y  Zhang D Chen and D Dey What makes convolutional models great on long
sequence modeling ArXiv  221009298 2022
P Liang R Bommasani T Lee et al Holistic evaluation of language models Annals of the New
York Academy of Sciences  1525140146 2023
J Lin R Men A Yang C Zhou M Ding Y  Zhang P Wang A Wang L Jiang X Jia J Zhang
J Zhang X Zou Z Li X Deng J Liu J Xue H Zhou J Ma j Yu Y  Li W Lin J Zhou
J Tang and H Yang M6 A Chinese multimodal pretrainer ArXiv  210300823 2021
D Linsley J Kim V  Veerabadran C Windolf and T Serre Learning longrange spatial dependen
cies with horizontal gated recurrent units Advances in Neural Information Processing Systems
NeurIPS  31 2018
I Loshchilov and F Hutter Decoupled weight decay regularization In International Confer
ence on Learning Representations ICLR  2019 URL httpsopenreviewnetforumid
Bkg6RiCqY7 
X Ma C Zhou X Kong J He L Gui G Neubig J May and L Zettlemoyer Mega Moving
average equipped gated attention ArXiv  220910655 2022
A L Maas R E Daly P T Pham D Huang A Y  Ng and C Potts Learning word vectors
for sentiment analysis In Annual Meeting of the Association for Computational Linguistics 
volume 49 pp 142150 2011
I Magnusson A Bhagia V  Hofmann et al Paloma A benchmark for evaluating language model
fitArXiv  231210523 2023
H Mehta A Gupta A Cutkosky and B Neyshabur Long range language modeling via gated state
spaces ArXiv  220613947 2022
S Merity C Xiong J Bradbury and R Socher Pointer sentinel mixture models In Interna
tional Conference on Learning Representations ICRL  2017 URL httpsopenreviewnet
forumidByj72udxe 
W Merrill and A Sabharwal The parallelism tradeoff Limitations of logprecision transformers
Transactions of the Association for Computational Linguistics  11531545 2023 doi 101162
tacl_a_00562
W Merrill J Petty and A Sabharwal The illusion of state in statespace models ArXiv  240408819
2024
M Milakov and N Gimelshein Online normalizer calculation for softmax ArXiv  180502867
2018
17K Nakano Associatron  a model of associative memory IEEE Transactions on Systems Man and
Cybernetics  SMC23380388 1972 doi 101109TSMC19724309133
G Nearing D Cohen V  Dube M Gauch O Gilon S Harrigan A Hassidim D Klotz F Kratzert
A Metzger S Nevo F Pappenberger C Prudhomme G Shalev S Shenzis T Y  Tekalign
D Weitzner and Y  M B Kosko Global prediction of extreme floods in ungauged watersheds
Nature  627559563 2024 doi 101038s41586024071451
C Olsson N Elhage N Nanda et al Incontext learning and induction heads ArXiv  220911895
2022
A Orvieto S L Smith A Gu A Fernando C Gulcehre R Pascanu and S De Resurrecting
recurrent neural networks for long sequences In Proceedings of the 40th International Conference
on Machine Learning ICML  JMLRorg 2023 doi 10555536184083619518
A Papasavva S Zannettou E DeCristofaro G Stringhini and J Blackburn Raiders of the lost
KeK 35 years of augmented 4chan posts from the politically incorrect board In International
AAAI Conference on Web and Social Media ICWSM  volume 14 pp 885894 2020
D Paperno G Kruszewski A Lazaridou NQ Pham R Bernardi S Pezzelle M Baroni Gemma
G Boleda and R Fernández The LAMBADA dataset Word prediction requiring a broad
discourse context In Annual Meeting of the Association for Computational Linguistics  volume 1
pp 15251534 2016
G Penedo Q Malartic D Hesslow R Cojocaru A Cappelli H Alobeidli B Pannier E Al
mazrouei and J Launay The RefinedWeb dataset for Falcon LLM Outperforming curated corpora
with web data and web data only ArXiv  230601116 2023
B Peng E Alcaide Q Anthony et al RWKV Reinventing RNNs for the transformer era ArXiv 
230513048 2023
B Peng D Goldstein Q Anthony et al Eagle and Finch RWKV with matrixvalued states and
dynamic recurrence ArXiv  240405892 2024
M Poli S Massaroli E Nguyen D Y  Fu T Dao S Baccus Y  Bengio S Ermon and C Ré Hyena
hierarchy Towards larger convolutional language models In Proceedings of the 40th International
Conference on Machine Learning ICML  JMLRorg 2023 doi 10555536184083619572
M Poli A W Thomas E Nguyen P Ponnusamy B Deiseroth K Kersting T Suzuki B Hie S Er
mon C Ré C Zhang and S Massaroli Mechanistic design and scaling of hybrid architectures
ArXiv  240317844 2024
Z Qin S Yang and Y  Zhong Hierarchically gated recurrent neural network for sequence modeling
InAdvances in Neural Information Processing Systems NeurIPS  volume 37 2023 URL
httpsopenreviewnetforumidP1TCHxJwLB 
Z Qin S Yang W Sun X Shen D Li W Sun and Y  Zhong HGRN2 Gated linear RNNs with
state expansion ArXiv  240407904 2024
D R Radev P Muthukrishnan and V  Qazvinian The ACL anthology network corpus In Workshop
on Text and Citation Analysis for Scholarly Digital Libraries NLPIR4DL  pp 5461 Association
for Computational Linguistics 2009
A Radford R Jozefowicz and I Sutskever Learning to generate reviews and discovering sentiment
ArXiv  170401444 2017
A Radford J Wu R Child D Luan D Amodei and I Sutskever Language models are unsupervised
multitask learners httpsopenaicomindexbetterlanguagemodels  2019
J W Rae S Borgeaud T Cai et al Scaling language models Methods analysis  insights from
training Gopher ArXiv  211211446 2021
C Raffel N Shazeer A Roberts K Lee S Narang M Matena Y  Zhou W Li and P J Liu
Exploring the limits of transfer learning with a unified texttotext transformer ArXiv  191010683
2019
18H Ramsauer B Schäfl J Lehner P Seidl M Widrich L Gruber M Holzleitner M Pavlovi c
G K Sandve V  Greiff D Kreil M Kopp G Klambauer J Brandstetter and S Hochreiter
Hopfield networks is all you need In International Conference on Learning Representations
ICLR  OpenReview 2021
M Reid V  Zhong S Gururangan and L Zettlemoyer M2D2 A massively multidomain language
modeling dataset In Conference on Empirical Methods in Natural Language Processing  pp
964975 2022
M Reid N Savinov D Teplyashin et al Gemini 15 Unlocking multimodal understanding across
millions of tokens of context ArXiv  240305530 2024
M H Ribeiro J Blackburn B Bradlyn E DeCristofaro G Stringhini S Long S Greenberg and
S Zannettou The evolution of the manosphere across the web In Proceedings of the international
AAAI conference on web and social media  volume 15 pp 196207 2021
K Sakaguchi R L Bras C Bhagavatula and Y  Choi Winogrande An adversarial winograd
schema challenge at scale Communications of the ACM  64999106 2021
T L Scao A Fan C Akiki et al BLOOM A 176Bparameter openaccess multilingual language
model ArXiv  221105100 2022
I Schlag K Irie and J Schmidhuber Linear transformers are secretly fast weight programmers
In M Meila and T Zhang eds Proceedings of the 38th International Conference on Machine
Learning ICML  volume 139 of Proceedings of Machine Learning Research  pp 93559366
PMLR 2021
J Schmidhuber Learning to control fastweight memories An alternative to recurrent nets Neural
Computation  41131139 1992
J Schmidhuber Deep learning in neural networks An overview Neural Networks  6185117 2015
doi 101016jneunet201409003
J Schulman B Zoph C Kim J Hilton et al ChatGPT Optimizing language models for dialogue
httpsopenaicomblogchatgpt 2022 OpenAI Research
T J Sejnowski Storing covariance with nonlinearly interacting neurons Journal of Mathematical
Biology  4 1977 doi 101007BF00275079
M Shoeybi M Patwary R Puri P LeGresley J Casper and B Catanzaro MegatronLM Training
multibillion parameter language models using model parallelism ArXiv  190908053 2019
J T H Smith A Warrington and S W Linderman Simplified state space layers for sequence
modeling ArXiv  220804933 2022
D Soboleva F AlKhateeb R Myers J R Steeves J Hestness and N Dey SlimPajama A 627B
token cleaned and deduplicated version of RedPajama httpswwwcerebrasnetblog
slimpajamaa627btokencleanedanddeduplicatedversionofredpajama 
2023 URL httpshuggingfacecodatasetscerebrasSlimPajama627B 
L Soldaini R Kinney A Bhagia et al Dolma an open corpus of three trillion tokens for language
model pretraining research ArXiv  230601116 2023
S Soltan S Ananthakrishnan J FitzGerald R Gupta W Hamza H Khan C Peris S Rawls
A Rosenbaum A Rumshisky C S Prakash M Sridhar F Triefenbach A Verma G Tur and
P Natarajan AlexaTM 20B Fewshot learning using a largescale multilingual Seq2Seq model
ArXiv  220801448 2022
R K Srivastava K Greff and J Schmidhuber Training very deep networks In C Cortes
N Lawrence D Lee M Sugiyama and R Garnett eds Advances in Neural Information
Processing Systems NeurIPS  volume 28 Curran Associates Inc 2015
Y  Sun L Dong S Huang S Ma Y  Xia J Xue J Wang and F Wei Retentive network A
successor to transformer for large language models ArXiv  230708621 2023
19L Sutawika L Gao H Schoelkopf et al EleutherAIlmevaluationharness Major refactor 2023
L Sutawika H Schoelkopf L Gao B Abbasi S Biderman J Tow B fattori C Lovering
farzanehnakhaee70 J Phang A Thite Fazz T Wang N Muennighoff Aflah sdtblck nopperl
gakada tttyuntian researcher2 Chris J Etxaniz H A Lee Z Kasner Khalid J Hsu A Kanekar
P S Ammanamanchi V  Boykis and AndyZwei EleutherAIlmevaluationharness 2024
I Sutskever O Vinyals and Q V  V  Le Sequence to sequence learning with neural networks In
Z Ghahramani M Welling C Cortes N D Lawrence and K Q Weinberger eds Advances in
Neural Information Processing Systems 27 NIPS13  pp 31043112 Curran Associates Inc
2014
Y  Tay D Bahri D Metzler DC Juan Z Zhao and C Zheng Synthesizer Rethinking self
attention in transformer models ArXiv  200500743 2020
Y  Tay M Dehghani S Abnar Y  Shen D Bahri P Pham J Rao L Yang S Ruder and D Metzler
Long range arena A benchmark for efficient transformers In International Conference on Learning
Representations ICRL  2021 URL httpsopenreviewnetforumidqVyeWgrC2k 
R Thoppilan D deFreitas J Hall et al LaMDA Language models for dialog applications ArXiv 
220108239 2022
TogetherComputer Redpajama an open dataset for training large language models 2023 URL
httpsgithubcomtogethercomputerRedPajamaData 
H Touvron T Lavril G Izacard X Martinet MA Lachaux T Lacroix B Rozière N Goyal
E Hambro F Azhar A Rodriguez A Joulin E Grave and G Lample Llama Open and efficient
foundation language models ArXiv  23021397 2023
D Vadas and J R Curran Parsing noun phrases in the Penn Treebank Computational Linguistics 
374753809 2011
A Vaswani N Shazeer N Parmar J Uszkoreit L Jones A N Gomez L Kaiser and I Polosukhin
Attention is all you need In Advances in Neural Information Processing Systems NeurIPS 
volume 30 pp 59986008 Curran Associates Inc 2017
O Vinyals T Ewalds S Bartunov et al Starcraft II A new challenge for reinforcement learning
ArXiv  170804782 2017
J Wang J N Yan A Gu and A M Rush Pretraining without attention ArXiv  221210544 2022
S Wang B Z Li M Khabsa H Fang and H Ma Linformer Selfattention with linear complexity
ArXiv  200604768 2020
S Wang Y  Sun Y  Xiang et al ERNIE 30 Titan Exploring largerscale knowledge enhanced
pretraining for language understanding and generation ArXiv  211212731 2021
Y  Wu and K He Group normalization In Proceedings of the European conference on computer
vision ECCV  pp 319 2018
L Xue N Constant A Roberts M Kale R AlRfou A Siddhant A Barua and C Raffel
mT5 A massively multilingual pretrained texttotext transformer In Conference of the North
American Chapter of the Association for Computational Linguistics  pp 483498 2021 doi
1018653v12021naaclmain41
S Yang and Y  Zhang FLA A Tritonbased library for hardwareefficient implementa
tions of linear attention mechanism 2024 URL httpsgithubcomsustcsonglin
flashlinearattention 
S Yang B Wang Y  Shen R Panda and Y  Kim Gated linear attention transformers with hardware
efficient training ArXiv  231206635 2023
S Zannettou B Bradlyn E DeCristofaro H Kwak M Sirivianos G Stringini and J Blackburn
What is Gab A bastion of free speech or an altright echo chamber In The Web Conference  pp
10071014 2018 doi 10114531845583191531
20W Zaremba and I Sutskever Learning to execute ArXiv  14104615 2014
R Zellers A Holtzman Y  Bisk A Farhadi and Y  Choi HellaSwag Can a machine really
finish your sentence In Annual Meeting of the Association for Computational Linguistics  pp
47914800 2019
A Zeng X Liu Z Du et al GLM130B An open bilingual pretrained model ArXiv  221002414
2022
S Zhang S Roller N Goyal M Artetxe M Chen S Chen C Dewan M Diab X Li X V  Lin
T Mihaylov M Ott S Shleifer K Shuster D Simig P S Koura A Sridhar T Wang and
L Zettlemoyer OPT Open pretrained transformer language models ArXiv  220501068 2022
21Contents
A Extended Long ShortTerm Memory 23
A1 Vanilla Long ShortTerm Memory Formulation Vector Notation           23
A2 sLSTM                                         23
A3 mLSTM                                         25
A4 Detailed Block Structure                                29
B Experiments 31
B1 Synthetic Tasks and Long Range Arena                         31
B11 Test of xLSTMs Exponential Gating with Memory Mixing          31
B12 Test of xLSTMs Memory Capacities on Associative Recall Tasks      34
B13 Test of xLSTMs Long Range Capabilities on the Long Range Arena    36
B2 Method Comparison and Ablation Study on SlimPajama 15B            40
B3 xLSTM Large Language Models  SlimPajama300B                 42
C Detailed Results on PALOMA Language Model Evaluation 44
22A Extended Long ShortTerm Memory
A1 Vanilla Long ShortTerm Memory Formulation Vector Notation
The vanilla LSTM memory cell update rules Greff et al 2015 at time step textend the scalar cell
state formulation to a vector of cell states
ctftct1itzt cell state 28
htotht htψ
ct
hidden state 29
ztφzt ztWzxtRzht1bz cell input 30
itσ
it
 itWixtRiht1bi input gate 31
ftσ
ft
 ftWfxtRfht1bf forget gate 32
otσot otWoxtRoht1bo output gate 33
The matrices WzWiWf andWocorrespond to the input weights between inputs xtand cell
input input gate forget gate and output gate respectively The matrices RzRiRf andRo
correspond to the recurrent weights between hidden state ht1and cell input input gate forget gate
and output gate respectively bzbibf andboare the corresponding bias vectors φandψare the
cell input and hidden state activation functions typically tanh ψis used to normalize or squash the
cell state which would be unbounded otherwise
A2 sLSTM
Similar to the LSTM in Section A1 also the sLSTM can be vectorized to multiple cells
ctftct1itzt cell state 34
ntftnt1it normalizer state 35
htotht htctn1
t hidden state 36
ztφzt ztWzxtRzht1bz cell input 37
itexp
it
 itWixtRiht1bi input gate 38
ftexp
ft
ORσ
ft
 ftWfxtRfht1bf forget gate 39
otσot otWoxtRoht1bo output gate 40
Here the cell input activation function φistanh  the hidden state activation function is the identity
φhelps stabilizing the recurrence
Considering external gradient contribution δext
htfrom subsequent layers and recurrent gradient contri
bution δR
htfrom gradients from future states flowing over the cell interaction matrix R we obtain the
recursive backward pass of sLSTM where δaindicates gradients with respect to parameter  internal
variable a
23δhtδext
htδR
ht41
δct1ftδctot1nt11δht1 42
δnt1ftδntotct1n2
t1δht1 43
δftf
tct1δctf
tnt1δnt 44
δiti
tztδcti
tδnt 45
δztitφztδct 46
δoto
tctn1
tδht 47
δxtX
gfizoW
gδgt 48
δR
ht1X
gfizoR
gδgt 49
δ
RgX
tht1δ
gt g ifzo 50
δ
WgX
txtδ
gt g ifzo 51
with the derivatives of the respective gate activation function i
t expit  exp it ito
t
σot andf
tσftorf
tftdepending on the forget gate activation φzis the derivative of
the cell input activation function φz
The matrices RzRiRfRoare blockdiagonal which is analogous to multiple heads in the
mLSTM This way the parameters reduce to d2Nh where Nhis the number of heads limiting the
cell interactions to individual heads This parameter efficient formulation of cell interactions together
with the exponential gating is called the new memory mixing Finally to stabilize the backward
pass we clip the magnitude of δR
htto10 as a means to prohibit exploding gradients for long context
lengths
Proof of Equivalence for sLSTM Stabilized Version The stabilization state m see Equation 15
in the main paper has no gradient and hence does not influence the other gradients We go back to
the scalar version Equation 8 here for simplicity We redefine cs
tandns
tas stabilized cell and
normalizer states
ctcs
texp
mt
52
ntns
texp
mt
53
Inserting Equation 15 into Equation 8 yields
hs
tcs
tns
t 54
exp
log f t mt1mt
cs
t1 exp
log i tmt
zt
exp
log f t mt1mt
ns
t1 exp
log i tmt 55
exp
log f t mt1
cs
t1 exp log i tzt
exp
log f t mt1
ns
t1 exp log i t56
exp log f tct1 exp log i tzt
exp log f tnt1 exp log i t57
ftct1 itzt
ftnt1 itctntht 58
24Therefore since the loss solely depends on ht theres no dependency on mt and consequently
no gradient exists for this stabilization state Note that mtcan be chosen arbitrarily We choose
mt max log  ft mt1log it which stabilizes the exponential function One can even find
mt such that the normalizer state ntcan be eliminated but this version was experimentally found to
be numerically unstable in the backward pass
A3 mLSTM
Throughout this section 1RTdenotes a column vector of ones and 1R1Ta row vector of
ones where Tis the dimension of this vector
Recurrent mLSTM Backward Pass The recurrent formulation of the mLSTM cell in Equation 19
yields the following backward pass recurrence where δaindicates gradients with respect to parameter
or internal variable aandδext
htdenotes gradients from subsequent layers
δ
htotδext
ht59
δCt1 ftδCtqt1δ
ht1
maxn
t1qt11	 60
δnt1 ftδntq
t1C
t1δht1
maxn
t1qt11	2Ω
n
t1qt1
qt1 61
δ
vt itk
tδ
Ct62
δ
kt it
v
tδCtδ
nt
63
δqtC
tδht
maxn
tqt1	q
tC
tδht
maxn
tqt1	2Ω
n
tqt
nt 64
δxtX
gqkvW
gδgt 65
δ
WgX
txtδ
gt g  q k v 66
δbgX
tδgt g  q k v 67
δft
1
Ct1δCt1
11nt1δnt
γ
ft
68
δit
1
vtk
t
δCt1
11nt1δnt
exp
ft
69
δothtσ otδht 70
andΩ z  Θ  z1Θ z1Θ zbeing the Heaviside step function γzis either σz
orexp z depending on the forget gate activation
Parallel mLSTM Forward Pass The mLSTM recurrence in Equations 1927 can be reformulated
in a parallel form which is used to speed up training After training we can still use the recurrent
formulation for fast text generation
Instead of processing each input xtRdat time step tsequentially the parallel version processes
all timesteps of a full sequence XRTdat once where Tis the sequence length and dis the
head dimension We present the forward pass of the mLSTM for a single head and drop the head
dimension for simplicity
25LetfRTbe the forget gate preactivations and iRTbe the input gate preactivations for a full
sequence We construct the forget gate activation matrix FRTTby
Fij

0 forj  i
1 forjiQi
kj1σ
fk
forj  i 71
and the input gate preactivation matrix IRTTby
Iij0forj  i
ijforij 72
By applying the elementwise exponential input gate activation function naively we obtain the
unstabilized gate activation matrix DRTTas
DFexpI 73
In order to avoid overflow due to the exponential function we apply the same stabilization as in the
recurrent sLSTM see Equation 15 In the parallel formulation of the mLSTM we get a numerically
stable gate activation matrix DRTTby taking the logarithm of Delementwise and subtracting
the rowwise maximum value of Dfrom each element
eD log D log
FexpI
 log FI 74
D exp eDmaxeD 75
Given the queries keys and values QKVRTd for a full sequence we can compute all hidden
preactivation states eHRTdin parallel for the unstabilized version by
eHC V withCeC
max
PT
j1eCij1andeCQK

dD 76
Note that we extract the1
dfactor for Kexplicitly here and further on For the stabilized version
this yields
eHC V withCeC
max
PT
j1eC
ijexpmaxeDandeCQK

dD77
where for both versions the hidden preactivation states eHare identical
With the output gate preactivations eORTdwe can compute the hidden states HRTdfor all
timesteps by applying the output gate in parallel for each timestep elementwise
HσeOeH 78
This gives the parallel forward pass of the mLSTM for a full input sequence XRTd
Parallel mLSTM Backward Pass We present the backward pass of the mLSTM for the stabilized
version only For completeness we summarize the forward pass in the stabilized version before we
present the backward pass
Given the forget gate matrix FRTT the logarithm of the forget gate matrix F log FRTT
and the input gate matrix IRTTas introduced above together with the queries keys and values
26QKVRTd we can write the forward pass of the mLSTM in the stabilized version as
eDFI 79
m max
jeDij rowwise maximum 80
D exp eDm1 81
eCQK

dD82
bTX
j1eC
ijeC1 rowwise sum 83
n max  bexpm 84
CeC
n11
85
eHC V 86
With this forward pass we can compute the gradients δafor all intermediate and input variables to the
mLSTM forward pass in the backward pass We denote the gradient with respect to variable aasδa
Given the output gradient δeHRTdwe can compute the backward pass for the intermediate
gradients as
δ
CVδ
eH87
δn
eC
n21
δC
1 88

eCδC
1
n289
δbsignnδn1ifbexpm
0otherwise90
δeCC
n11
δC columnwise broadcast 91
δ
eCb1δ
b columnwise broadcast 92
δeCδeCCδeCB93
δDQK

dδeC 94
δeD exp eDmδDDδD 95
We do not compute the gradients for mas they cancel out see the proof in the recurrent sLSTM
With these intermediate gradients the gradients for the logarithmic forget gate matrix δFRTT
the input gate matrix δIRTT and the queries keys and values δQ δK δVRTdare given by
δFδeD96
δIδeD97
δQ
DδeCK
d98
δK
DδeCQ
d99
δVCδeH100
Having computed the gradients for the logarithmic forget gate matrix δF we can compute the
gradients for the forget gate preactivations δf
δf1 δf2  δ fTRT
27Recall the logarithmic forget gate matrix F log Fis computed by
Fij log Fij

 forj  i
0 forjiPi
kj1logσ
fk
z
fkPi
kj1fkforj  i 101
With the substitution f log σfwe compute the gradients for the logarithmic forget gate activations
δf
δf1 δf2  δfTRTas
δfkk1X
j1TX
ik1
δF
ij 102
δfkσfkδfk 103
where the last equation makes use of the following
d
dxlogσx 1  exp x1expx1
expx
1  exp x1
1  exp x
σx104
Finally we compute the input gate preactivations gradients δi
δi1 δi2  δ iSRTas the
columnwise sum over the rows of the input gate matrix δI
δikTX
i1δIik 105
This completes the backward pass of the parallel mLSTM for a full input sequence XRTd
28A4 Detailed Block Structure
PF¾
NH4 NH4 NH4f z o
Conv4Swishi
NH4NH4 NH4 NH4 NH4sLSTMGN
LNPF PF 3434GeLU
Figure 9 Schematic representation of an sLSTM Block  post upprojection Embedded in a pre
LayerNorm residual structure the input is optionally passed through a causal convolution of window
size4that includes a Swish activation for input and forget gates Then for all input forget and output
gates ifo and the cell update zthe input is fed through a blockdiagonal linear layer with four
diagonal blocks or heads These diagonal blocks coincide with the recurrent gate preactivations
from the last hidden state which corresponds to an sLSTM with four heads depicted with the circular
arrows The resulting hidden state goes through a GroupNorm layer Wu  He 2018  a headwise
LayerNorm for each of the four heads Finally the output is up and downprojected using a gated
MLP with GeLU activation function and projection factor 43to match parameters
29PF2PF½
PF2NH4
BS4 BS4 BS4
Conv4LSkipSwish
Swishq k vi fmLSTMGN
LNFigure 10 Schematic representation of an mLSTM block  pre upprojection Embedded in a
preLayerNorm residual structure the input is upprojected first with projection factor 2 once for
an externalized output gate and once as input for the mLSTM cells The mLSTM cell input is
dimensionwise causally convolved kernel size 4 before entering a learnable skip connection We
obtain input qandkvia blockdiagonal projection matrices of block size 4 The values vare fed
directly skipping the convolution part After the mLSTM sequence mixing outputs are normalized
via GroupNorm Wu  He 2018  a headwise layer norm for each of the four heads Finally the
learnable skip input is added and the result is gated componentwise with the external output gate
The output is downprojected
30B Experiments
Training Setup For all experiments we use Python1311 with PyTorch 2202 and CUDA 1213
on NVIDIA A100 GPUs
Nearest Neighbor Search Task For this auxiliary task we use randomly sampled feature vectors
of dimension 2 and unit norm The attached value is a uniformly distributed random number from
01 leading to inputs vectors of dimension 3 The first feature vector serves as search key with the
first value being ignored Then the model has to predict the value of the nearest neighbor so far in the
sequence We train on 8192 sequences of context length up to 64 uniformly sampled and validate
on 8192 different samples All models have two blocks and embedding dimension 128 We use a
dropout of 01 10 linear warmup steps and cosine decay to 1e7 for 100k total training steps We
sweep over learning rates 1e4 1e3 1e2 1e1 and 5 seeds each The reported values in Figure 2 are
mean values for the best learning rate and 99 confidence intervals Note that LSTM requires very
high learning rates whereas Transformers Llama perform best at the smallest learning rate The
xLSTM01 reaches similar performance across all learning rates
Wikitext103 Rare Token Prediction For this exemplary experiment on rare token prediction we
trained 125Msized models on Wikitext103 Merity et al 2017 All models have an embedding
dimension of 768 in a post upprojection structure of 12 residual blocks The Transformer model
Llama uses MultiHead Attention for what is called LSTM the MultiHead Attention is replaced by
an LSTM and the xLSTM10 contains mLSTM layers with matrix memory Models were trained
with maximum learning rate 1e3 4k steps linear warmup and cosine decay for in total 50k steps
using a batch size of 256 and context length of 512 We use the validation perplexity as a stopping
criterion and evaluate on the test set
B1 Synthetic Tasks and Long Range Arena
B11 Test of xLSTMs Exponential Gating with Memory Mixing
We evaluate xLSTM on a suite of formal language tasks to test its exponential gating and memory
mixing mechanism
Formal languages provide a framework to probe the generalization capabilities of models They allow
to specifically test different expressivity levels eg along the Chomsky hierarchy Typical language
model architectures do not necessarily fit perfectly in these hierarchies Delétang et al 2023 
nevertheless these languages allow to illustrate differences in generalization expressivity between
different architectures Our evaluation tasks are heavily based on the work of Delétang et al 2023
Experiment Setup The different formal language tasks in the experiment see individual tasks
description below encompass different levels of the Chomsky hierarchy as well as additional counting
and memoryfocused tasks We use different lengths per sample which allows us to validate in a
length extrapolation setting We train on a varying task length up to 40 The evaluation is done for
task lengths between 40 and 256 as we are only interested in the task generalization capabilities of
the models
In all experiments we use two blocks or layers for the pure LSTM for all models We compare
Llama Mamba Retention Hyena RWKV4 RWKV5 RWKV6 LSTM xLSTM01 xLSTM10
and xLSTM11 The sLSTM block is used without a convolution and with normal weight initializa
tion LSTM Block refers to an architecture where a vanilla LSTM is used instead of selfattention
inside a Transformer block
All models are trained with 3 different learning rates 1e2 1e3 1e4 each with two seeds
Batch size is 256  cosine annealing min lr 1e5 with 10 warmup steps is applied We use
AdamW Loshchilov  Hutter 2019 and a weight decay of 01 for training In each experiment we
train for 100k steps  the samples are generated randomly however all experiments are trained and
evaluated on the same samples
1httpspythonorg
2httpspytorchorg
3httpsdocsnvidiacomcudaarchive1210
31Odds FirstReverse
StringStack
Manipulation Repetition Set
Llama
Retention
RWKV4
Hyena
RWKV5
RWKV6
xLSTM01
Mamba
LSTM
Block
xLSTM01
xLSTM10
xLSTM11007
00006
00011
001008
00004
00
003
00011
00003
00002
00002
00
008
00012
00102
0001
0001
002
004
00015
00007
00007
00003
00
008
001009
001016
00016
00013
001
013
001011
00023
001015
001019
001
009
001014
003013
001009
001017
001
008
001013
002021
00015
001012
00
008
001017
002025
002015
001018
001
009
001014
003013
001009
001017
001
015
003022
002025
003028
00017
001
008
0002
001017
00009
00015
003Context
SentsitiveDeterministic
Context FreeFigure 11 Supplementary results given by scaled accuracy of different models at solving formal
language tasks Tasks are grouped by the Chomsky hierarchy
Additional Formal Language Results Figure 11 showcases supplementary results on formal
language task detailing tasks where no model attained a minimum scaled accuracy of 03 Although
no model achieves proper extrapolation of the task to a larger context length xLSTM performs best
among the evaluated models
Individual Task Description The majority of tasks are based on Delétang et al 2023 We
provide the vocabulary size Vand the random accuracy srand for accuracy scaling used in the
evaluation As we evaluate different task lengths each task has a padding token which is used to pad
the sequence to the given context length In Listing 1 there is an example for each task
Bucket Sort Given a string of tokens of a sorted alphabet compute the sorted string
V 11 srand1
V1
Cycle Nav Given a string of movement tokens  11 STAY compute the end position
of the agent with start position 0 The position must be computed modulo the maximum
position
V 9 srand1
V4
Even Pairs Given a binary string of aandbtokens compute whether the number of aband
bais even This task can be solved by checking if the first and last token of the string are
equal
V 3 srand 05
Majority Given a string of tokens compute the token that occurred most often in the
sequence
V 64 srand1
V1
Majority Count Given a string of tokens of an ordered alphabet Compute the count of
the token that occurred most often in the sequence If the count exceeds the vocab size the
highest vocab token should be outputted
V 64 srand1
V1
Missing Duplicate Given a string of tokens The string is repeated but one of the tokens is
masked in the repetition Output the token that is masked
V 11 srand1
V2
32Mod Arithmetic wo Brackets Calculate the result  modulo the max number  of the
arithmetic operations in the context The maximum number is the vocabulary size minus the
number of special tokens  PAD
V 10 srand1
V5
Mod Arithmetic w Brackets Calculate the result  modulo the maximum number  of
the arithmetic operations in the context The maximum number is vocabulary size minus the
number of special tokens  PAD
V 12 srand1
V7
Odds First An string of tokens t1 t2 t3 tnis given Output all tokens with and odd index
t1 t3  then the token with an even index  t2t4  Apart from that keep the ordering of
the initial string
V 12 srand1
V2
Parity Given a binary string of aandbtokens compute if the number of bs is even If
the number is even output aotherwise b This is equivalent to sequentially calculating the
halfadder sum
V 3 srand 05
Repetition Given a string of tokens  repeat it
V 12 srand1
V2
Reverse String Given a string of tokens  repeat it in reverse order
V 12 srand1
V2
Stack Manipulation An initial stack content is given followed by a sequence of push and
pop operations Compute the stack content after the operations
V 11 srand1
V3
2
SetGiven a string of tokens compute the ordered set of the tokens Keep the ordering so
that tokens that occurred first are also outputted first
V 128 srand1
V2
Solve Equation Given is an equation with the operators  number and an
unknown variable x Compute the value of the variable modulo the max number The
maximum number is vocabulary size minus the number of special tokens  PAD
ACT
V 14 srand1
V9
33Bucket Sort
Sequence 1 4 8 6 1 1 1 4 6 8
Cycle Nav
Sequence STAY 1 1 1 STAY 1 1 1 1 P3
Even Pairs
Sequence a b b a a b a b a a
Majority
Sequence 1 7 6 4 3 8 1 7 2 1
Majority Count
Sequence 1 7 6 4 4 8 1 7 2 2
Missing Duplicate
Sequence 4 8 6 2 5 4 8 6 2 MIS 5
Mod Arithmetic wo Braces
Sequence 0  4  0  2  4 PAD
Mod Arithmetic w Braces
Sequence    2    2     4  2    2
Odds First
Sequence 2 7 3 2 6 9 ACT 2 3 6 7 2 9
Parity
Sequence a b b a a b a b
Repetition
Sequence 2 4 8 6 2 ACT 2 4 8 6 2
Reverse String
Sequence 2 4 8 6 2 ACT 2 6 8 4 2
Stack Manipulation
Sequence ST1 ST1 ST3 POP POP PS3 PS3 ACT ST1 ST3 ST3
Set
Sequence 8 6 6 3 5 4 5 3 ACT 8 6 3 5 4
Solve Equation
Sequence    2  0    x    1    2 ACT 2
Listing 1 Examples of the formal language tasks Red tokens are evaluated for loss and accuracy
metrics but are padded for the input The tokens are illustrated in a way that allows easy semantic
interpretation for the given task  hence some tokens are represented by multiple characters
B12 Test of xLSTMs Memory Capacities on Associative Recall Tasks
We test the memory capacity of xLSTM with the MultiQuery Associative Recall task proposed by
Arora et al 2023 Figure 12 illustrates the basic task setup
Why MultiQuery Associative Recall for Memory Tests of LLM Architectures Associative
Recall AR the ability to retrieve a specific value information associated with a given key infor
mation constitutes a key capability for LLM to perform well Poli et al 2024 Arora et al 2023
Olsson et al 2022 Especially its quality of incontext learning seems to be strongly connected to
this capability Olsson et al 2022 Arora et al 2023 attribute performance gaps between early
nonTransformer and Transformer language models specifically to performance gaps in associative
recall They argue that prior AR evaluations fall short of capturing these differences and propose
MQAR which can show the AR performance differences that translate to performance differences
in language modeling performance Hence MQAR is especially suitable to analyze the memory
capacity of LLM Transformer eg Llama models can be seen as the gold standard for this task as
their memory is exponential in the coding dimension Ramsauer et al 2021
Experiment Setup There are two relevant variables that determine different experimental setups
1Context Length CL  Length of the sequence of one sample  this influences the distances
between the keyvalue definition and the recall 2 Number KeyValue Pairs KV  Influences how
many keyvalue pairs the model needs to keep track of The vocabulary size is always 8192
34In all experiments we use two blocks or layers for the pure LSTM for all models LSTM Block
model refers to an architecture where a vanilla LSTM is used instead of selfattention inside a
Transformer block
For each task setup we train each model with 4 different learning rates batch size  24 1e2
215e3 46e4 1e4 batch size 24 1e3 22e4 5e5 1e5 The batch size BS changes
depending on the context length CL CL64128 BS512 CL256 BS256 CL756 BS128
CL1024 BS96 CL2048 BS24 We vary the embedding dimension Model Dim between
different experiments  different numbers of heads are used accordingly For each experiment we
generate 100000 training samples validation 3000 samples and train for 64 epochs We apply
cosine annealing min lr 1e4 and 1e5 with 10 warmup steps We use AdamW Loshchilov 
Hutter 2019 and a weight decay of 01 for training
We conduct three different experiments
MQARExperiment 1 evaluates in the same fashion as Arora et al 2023 a vari
ety of models Llama Mamba Mamba noWT  ie without weight tying Reten
tion Hyena H3 RWKV4 RWKV5 RWKV6 LSTM LSTM Block xLSTM01
xLSTM10 and xLSTM11 on increasing task difficulty by increasing the context length
and number of keyvalue pairs simultaneously We benchmark three parameter settings
CLKV644128825616
MQARExperiment 2 increases the task difficulty notably and goes beyond previous
evaluations on this task We individually scale the context length CL756 1024 2048
and the keyvalue pairs KV48 96 256 and evaluate all combinations This experiment
especially probes the memory capacity because the number of keyvalue pairs is high
To reduce the computational burden we only evaluate models that perform flawlessly in
Experiment 1  additionally we evaluate Transformer only in the hardest setting CL2048
as sanity check because no performance decrease is expected
MQARExperiment 3 analyzes whether the AR capability learned on a certain context
length extrapolates to bigger context lengths For each KV setting of Experiment 2 we use
the models we select the 3 biggest model dimensions trained on CL2048 and evaluate
bigger context lengths CL4096 6144 8192
Extended Results The result of Experiment 1 can be found in Figure 13 In accordance to the
results of Arora et al 2023 H3 Hyena RWKV4 fail to solve the task with a smaller model
dimension In contrast xLSTM11 xLSTM10 Mamba RWKV5 and RWKV6 are able to solve
these settings for all model dimensions The comparison of xLSTM01 with both original LSTM
variants indicates that the exponential gating mechanism improves the AR capabilities of the model
However both fall short because of the reduced memory capacity compared to xLSTM11 and
xLSTM10
The results of Experiment 2 are presented in Figure 14 Scaling the context length has a low impact
on the performance of the models However while xLSTM11 and xLSTM10 show no clear
decay both RWKV variants slightly but consistently lose performance with increasing context
lengths The varying number of keyvalue pairs which mainly probes the memory capacity of the
nonTransformer models has a more notable impact across all models RWKV5 seems to outperform
RWKV6 The latter fails to learn the task at all in some KV256 settings Overall xLSTM11 is the
bestperforming nonTransformer model  suggesting that it provides enhanced memory capacity
also in long contexts
Figure 15 shows the extrapolation results from Experiment 3 For xLSTM11 xLSTM10 and
Mamba the model performance does not change in the extrapolation setting The RWKV models
especially RWKV5 degrade slightly with increasing context length xLSTM11 performs best as
it maintains its superior performance of Experiment 2
4The keys are distributed on the evaluation part of the sequence given a powerlaw distribution This is
motivated by similar structures in natural language text
35InputTargetKV  4  CL  18 Figure 12 Illustration of the MQAR task Color pairs represent keyvalue pairs keys have darker
shade The first part of the sequence defines the keyvalue pairs for the respective sample After that
the keys appear randomly according to a power law distribution4 Grey tokens in the input sequence
represent a zero token The target sequence contains the value after the respective key appearance
 the rest of the tokens are ignored for the accuracy and loss calculation The model must predict the
value tokens given the respective key
B13 Test of xLSTMs Long Range Capabilities on the Long Range Arena
We assess the performance of xLSTM across tasks in the Long Range Arena benchmark Tay et al
2021 examining its ability to effectively handle longer context lengths and diverse data types
Our experiments on Long Range Arena benchmark are composed of five tasks
Retrieval  The task is to predict if two documents have a citation link The dataset of text
documents is derived from the ACL Anthology Network Radev et al 2009
ListOps  This is a set of modular arithmetic tasks including brackets and lists of numbers
using the operations MINMAXMEDIAN andSUMMOD modular sum A particular example
isMAX 4 3 MIN 2 3  1 0 MEDIAN 1 5 8 9 2  5
Image  This task is based on a version of the CIFAR dataset Krizhevsky 2009 where
images are transformed to a sequence of pixels and this sequence has to be classified into the
usual CIFAR classes We test both a grayscale GImage and RGB RGBImage version
of this dataset as Orvieto et al 2023 uses colored images contrary to the standard setup
Pathfinder  The input for this task is a 32x32 grayscale image given as pixel sequence
with two dots and several curved lines on it The task is to predict if the two dots are
connected by any of the lines Linsley et al 2018
We omit the Text classification task Maas et al 2011 as the language modeling experiments already
test this kind of data and the PathfinderX version of Pathfinder 
Experiment Setup The architectures that are tested in this experiment comprise LLama Mamba
LSTM RWKV4 and xLSTM LSTM Block refers to an architecture where a vanilla LSTM is used
inside a post upprojection block like Transformer with attention replaced by LSTM For xLSTM
we choose the best performing of xLSTM01 or xLSTM10 on the validation set specifically the
former for the Image tasks and the latter for all other ones
We use the hyperparameter settings of the S5 model Smith et al 2022 and Linear Recurrent Unit
model Orvieto et al 2023 with additional hyperparamter search on learning rates and schedulers
for all models We use two different schedulers Linear Warmup Cosine Annealing and Linear
Warmup Cosine Annealing with Restarts Both learning rate schedulers were evaluated with learning
rates of 1e3 6e4 and 1e4 For the second scheduler the number of restarts  R is set to 3 The
model hyperparameters for each dataset are displayed in Table 5
Results Table 6 shows the result of experiments on the Long Range Arena benchmark xLSTM
demonstrates consistent strong performance on all of the tasks suggesting that the proposed architec
ture is remarkably efficient in handling different aspects of long context problems
36000204060810Transformer
Accuracy
Context Length  64
 Context Length  128
 Context Length  256
000204060810xLSTMFamily
Accuracy
000204060810Mamba
Accuracy
000204060810RWKV
Accuracy
32 64 128 256 512
Model Dim000204060810Others
Accuracy
32 64 128 256 512
Model Dim
32 64 128 256 512
Model Dim
Llama
Mamba
Mamaba noWTRWKV4
RWKV5
RWKV6Retention
Hyena
H3xLSTM01
xLSTM10
xLSTM11LSTM Block
LSTMFigure 13 Result of MQARExperiment 1 The columns show different task settings context length
and keyvalue pairs The rows group related models for better clarity The xaxis gives the model
size and the yaxis the validation accuracy
37000204060810Context Length  756
Accuracy
KeyValue Pairs  48
 KeyValue Pairs  96
 KeyValue Pairs  256
000204060810Context Length  1024
Accuracy
32 64 128 256 512
Model Dim000204060810Context Length  2048
Accuracy
32 64 128 256 512
Model Dim
32 64 128 256 512
Model Dim
Llama Mamba RWKV5 RWKV6 xLSTM10 xLSTM11Figure 14 Result of MQARExperiment 2 The columns and rows correspond to different numbers
of keyvalue pairs and the context length respectivly The xaxis gives the model size and the yaxis
the validation accuracy
Task BlocksEmbedding
DimBatch
SizeTraining
Steps
Retrieval 6 128 64 100k
ListOps 8 128 32 80k
Pathfinder 6 192 64 500k
GImage 6 512 64 180k
RGBImage 6 512 64 180k
Table 5 Long Range Arena model hyperparameters These are the model hyperparameters used in
each of the Long Range Arena tasks For each model we used the best learning rate and the better of
the two learning rate schedulers
38000025050075100Context Length  4096
Accuracy
KeyValue Pairs  48
 KeyValue Pairs  96
 KeyValue Pairs  256
000025050075100Context Length  6144
Accuracy
128 256 512
Model Dim000025050075100Context Length  8192
Accuracy
128 256 512
Model Dim
128 256 512
Model Dim
Mamba RWKV5 RWKV6 xLSTM10 xLSTM11Figure 15 Result of MQARExperiment 3 Extrapolation All evaluated models were trained on
context length 2048 and the number of keyvalue pairs given by the columns of the plot The rows
show the different context lengths used in the evaluation The xaxis gives the model size and the
yaxis the validation accuracy
Retrieval
accListOps
accPathfinder
accGImage
accRGBImage
accRanking
acc
Random Baseline 0500 0100 0500 0100 0100
Llama 0845 0379 0887 0541 0629 52
Mamba 0902 0325 0992 0689 0765 22
RWKV4 0898 0389 0914 0691 0757 30
LSTM X 0275 X 0675 0718 54
LSTM Block 0880 0495 X 0690 0756 34
xLSTM 0906 0411 0919 0695 0761 16
Table 6 Long Range Arena test accuracy Bold highlights the best performing model underlined
the second best X denotes models that fail to outperform random baselines xLSTM is the best of
xLSTM10 xLSTM01 based on validation dataset accuracy
39B2 Method Comparison and Ablation Study on SlimPajama 15B
General Training Procedure We tokenize our datasets using the HuggingFace GPT2 tokenizer
Radford et al 2019 Brown et al 20205and use this tokenizer for all models in this paper In
general we try to follow Brown et al 2020 for the general training setup ie we choose context
length 2048 and batch sizes 256 or 512 for our models We use the AdamW Loshchilov  Hutter
2019 optimizer with beta parameters  β1β209 095 and an epsilon parameter of 1e5 As
learning rate scheduler we use a linear warmup with 750 steps and cosine decay to 10 of the
peak learning rate We apply a weight decay of 01 to all our models and always exclude the token
embedding matrix from weight decay If not specified otherwise we do not tie the weights of
the token embedding and the language model head For parallelization we use PyTorch FSDP in
SHARD_GRAD_OP mode with mixed precision in bfloat16  where applicable For small models we
useNO_SHARD  We keep the weights in float32 and reduce the gradients across GPUs in float32 
We use torchcompile to speed up models except for Transformer models as their training curves
did not match the noncompiled versions For xLSTM71 we use positions 3 5 7 40 42 44 for
sLSTMbased blocks except for the 125M size where we use 3 20 this is actually a 111 ratio
Model EmbeddingDim Blocks HeadsHeadDimParams
MPeak LR
15BPeak LR
300B125MRWKV4 768 12  1694 3e3 6e4
Llama 768 12 12  64 1622 3e3 3e3
Mamba 768 24  1678 3e3 3e3
xLSTM 768 24 4  384 1638 1e3 15e3350MRWKV4 1024 24  4305 1e3 4e4
Llama 1024 24 16  64 4066 15e3 15e4
Mamba 1024 48  4231 15e3 15e3
xLSTM 1024 48 4  512 4093 1e3 75e4760MRWKV4 1536 24  8910 1e3 25e4
Llama 1536 24 16  96 8341 125e3 125e4
Mamba 1536 48  8705 125e3 125e3
xLSTM 1536 48 4  768 8404 9e4 625e413BRWKV4 2048 24  15152 1e3 2e4
Llama 2048 24 32  64 14204 1e3 1e3
Mamba 2048 48  14753 1e3 1e3
xLSTM 2048 48 4  1024 14226 9e4 5e427BRWKV4 2560 32  29848 8e4 
Llama 2560 32 32  80 27795 8e4 
Mamba 2560 64  28972 8e4 
xLSTM 2560 64 4  1280 27883 8e4 
Table 7 Peak learning rates and model dimensions for scaling law plots
Details on Comparison to Other Methods For the model comparison on 15B training tokens
of SlimPajama we train all models with context length 2048 and batch size 256 We use a peak
learning rate of 1e3 for all models for comparability The learning rate decays over 30k training
steps The models are compared after one epoch at training step 28170 As model implementations
we use the original repositories code for Mamba Gu  Dao 20236 RWKV5 RWKV6 Peng
et al 20247 For RWKV4 we use a cleaned and validated reimplementation based on the
original repo and kernels Peng et al 2023 For HGRN Qin et al 2023 GLA Yang et al
2023 HGRN2 Qin et al 2024 we use the a reimplementation by the authors of GLA Yang
5httpshuggingfacecodocstransformersenmodel_docgpt2
6httpsgithubcomstatespacesmamba
7httpsgithubcomBlinkDLRWKVLM
40et al 2023 Yang  Zhang 20248 For GPT3 and Llamalike Transformers we use our own
implementations based on PyTorch Note that for all xLSTMs Transformers Mamba and RWKV4
we use Mixed Precision training with bfloat16 and weights in float32 precision while resorting
to full bfloat16 precision weights and operations for all other models due to their custom kernels
that force one precision internally Following the general training procedure we use torchcompile
for all models except for models using the flashlinearattention Yang  Zhang 2024
library because of compilation problems
General Details on Ablation Studies We follow our general training procedure and train all
models with context length 2048 batch size 256 and peak learning rate 1e3 We report perplexity
values on the validation set
Additional Ablation Study on Matrix Memory As default block configuration we use the
mLSTM in the pre upprojection block see Figure 10 and the sLSTM in the post upprojection block
see Figure 9 In this experiment we study combination of mLSTM with different block variants
using the xLSTM10 architecture We compare the mLSTM in a post upprojection block see
Figure 3 and 9 with ReLU2activation function and nongated feedforward network to mLSTM
in a pre upprojection block with and without a dimensionwise causal convolution Table 8 shows
that the matrix memory benefits from the pre upprojection block structure and that the convolution
within this block is important
Model Details BlocksEmbedding
DimParams
MSlimPajama
15B ppl 
xLSTM10Post UpProjection Block ReLU2 24 1024 4304 1390
Pre UpProjection Block No Convolution 48 1024 4088 1541
Pre UpProjection Block With Convolution 48 1024 4093 1343
Table 8 Matrix Memory variants We study different configurations for the matrix memory Matrix
memory in the pre upprojection block performs best and gives xLSTM10 Notably it seems that
the dimensionwise causal convolution within the pre upprojection block is important
Details on new xLSTM Components Ablation Study In Table 2 top we show our modifications
to the vanilla LSTM that transform the vanilla LSTM into the xLSTM We start with a large
default PyTorch LSTM with 24 layers and 1536 hidden size Due to a lack of skipconnections
and LayerNorms vanilla LSTMs of this size are not trainable We then add skipconnections and
preLayerNorms before each LSTM layer corresponding to a residual architecture This enables
training for LSTMs at this scale Replacing every second LSTM layer by a nongated feedforward
network with GeLU activation function similar to Vaswani et al which corresponds to the post
upprojection backbone see Figure 3 further boosts performance Adding Exponential Gating to this
architecture yields the sLSTM as depicted in Figure 9 with another large performance improvement
Finally adding the best Matrix Memory variant found in Table 8 by replacing some sLSTM blocks
with the mLSTM see Figure 10 gives xLSTM71 with the best performance
Details on Gating Technique Ablation Study In Table 2 bottom we investigate the effect
of trainable and inputdependent gates for mLSTM The results show that in contrast to other
methods Katharopoulos et al 2020 Sun et al 2023 Qin et al 2023 Katsch 2023 Yang et al
2023 Qin et al 2024 Peng et al 2024 having the gates both learnable and input dependent gives
the best results
Details on Scaling Experiments We follow our general training procedure see paragraph above
and train all models including the 13B and 27B model sizes with context length 2048 and batch
size 256 We use the peak learning rates from Table 7
8httpsgithubcomsustcsonglinflashlinearattention
41B3 xLSTM Large Language Models  SlimPajama300B
General Training Procedure We use the same general training procedure as in Section B2 with
peak learning rates from Table 7 All models are trained with context length 2048 The 125M 350M
and 760M models are trained with batch size 256 for 600k training steps whereas the 13B models
are trained with batch size 512 for 300k training steps We keep the same learning rate scheduler
across all models
Details on Downstream Evaluation We use the LM Evaluation Harness from
EleutherAI Sutawika et al 2023 for evaluating the following tasks that measure common
sense reasoning LAMBADA OpenAI version in LM Evaluation Harness Paperno et al 2016
HellaSwag Zellers et al 2019 PIQA Bisk et al 2020 ARCchallenge ARCeasy Clark et al
2018 WinoGrande Sakaguchi et al 2021 This selection of downstream tasks is inspired by Gu
 Dao 2023
Following Gu  Dao 2023 we report accuracy for LAMADA WinoGrande PIQA and ARCeasy
and accuracy normalized by sequence length for HellaSwag and ARCchallenge
We evaluate all models in full float32  full bfloat16 andbfloat16 Mixed Precision with weights
infloat32  For each model we select the best value respectively
Details on PALOMA We use 16 out of the 18 data sources of the PALOMA dataset Magnusson
et al 2023 We use C4 Raffel et al 2019 MC4EN Xue et al 2021 Wikitext103 Merity
et al 2017 PennTreebank Vadas  Curran 2011 RedPajama TogetherComputer 2023 Fal
con Refinedweb Refined Web Penedo et al 2023 Dolma v15 Soldaini et al 2023 M2D2
S2ORC M2D2 Wikipedia Reid et al 2022 C4100Domains C4 Domains Chronopoulou et al
2022 Dolma100Subreddits Dolma Subreddits Soldaini et al 2023 Dolma100Programming
Languages Dolma Coding Soldaini et al 2023 Kocetkov et al 2022 TwitterAAE Blodgett
et al 2016 Liang et al 2023 Manosphere Corpus Ribeiro et al 2021 GAB Corpus Zannettou
et al 2018 4CHAN Corpus Papasavva et al 2020 We leave out ThePile Gao et al 2021 and
ICE Greenbaum  Nelson 1996 as they are not part of Palomas Huggingface dataset repository9
A detailed description of these datasets can be found in Magnusson et al 2023 Table 2 All models
are evaluated in bfloat16 Mixed Precision
Results on the data sources TwitterAAE Manosphere GAB and 4CHAN are reported in Table 9 and
for each individual dataset the results are given in Section C
ModelParams
MTwitter
AAEManosphere 4CHAN GAB125MRWKV4 1694 26580 3931 1848 5389
Llama 1622 27793 3298 1403 5645
Mamba 1678 25817 3214 1401 5158
xLSTM10 1638 24453 3145 1327 5100
xLSTM71 1637 24851 3090 1345 5025350MRWKV4 4305 21617 3025 1382 4225
Llama 4066 23109 2590 1149 4304
Mamba 4231 20288 2524 1160 4078
xLSTM10 4093 20061 2458 1120 3983
xLSTM71 4084 20625 2473 1131 3986760MRWKV4 8910 19527 2466 1200 3573
Llama 8341 20550 2269 1040 3768
Mamba 7932 18274 2258 1047 3625
xLSTM10 8404 17974 2166 1011 3533
xLSTM71 8397 18019 2178 1022 348913BRWKV4 15152 17487 2351 1134 3318
Llama 14204 19252 2067 967 3484
Mamba 14753 17138 2037 980 3201
xLSTM10 14226 16616 1994 964 3190
xLSTM71 14201 17136 2028 964 3217
Table 9 Perplexity values per domain
9httpshuggingfacecodatasetsallenaipaloma
42In order to evaluate the perplexity values on each data source we split the text documents into
sequences of length 2048 which corresponds to the pretraining context length of all models For
documents longer than 2048 tokens we split each document into nonoverlapping input sequences In
this case for the last input sequence we follow the LM Evaluation Harness and fill up the full 2048
token context window with previous tokens but compute the perplexity only on the remaining tokens
We compute the token perplexities per data source in Table 4 as the exponential of the negative
loglikelihoods per domain weighted by the number of tokens per domain in that data source as it is
defined in Magnusson et al 2023 Equation 1
43C Detailed Results on PALOMA Language Model Evaluation
We report the perplexity values on each of the 571 subdomains of PALOMA in Table 10 Note that
the aggregated perplexity values in Table 4 are not macro averages of the values shown in Table 10
Dataset Llama Mamba RWKV4 xLSTM71 xLSTM10
Params M 1420 1475 1515 1420 1423
4chan_meta_sep_val00000000 958 972 1137 953 955
4chan_meta_sep_val00000001 995 1006 1157 991 988
4chan_meta_sep_val00000002 942 953 1100 940 938
4chan_meta_sep_val00000003 978 993 1148 977 977
c4_100dom_val_100_wwwigncom 1622 1575 1710 1567 1543
c4_100dom_val_10_wwweventbritecom 1272 1233 1333 1230 1212
c4_100dom_val_11_linkspringercom 866 854 931 842 833
c4_100dom_val_12_wwwchicagotribunecom 1209 1160 1249 1155 1137
c4_100dom_val_13_wwwfoxnewscom 959 921 983 916 908
c4_100dom_val_14_wwwaljazeeracom 1097 1061 1131 1050 1040
c4_100dom_val_15_wwwdailymailcouk 1242 1197 1287 1185 1169
c4_100dom_val_16_wwwncbinlmnihgov 739 731 798 711 707
c4_100dom_val_17_wwwexpresscouk 1157 1104 1184 1099 1079
c4_100dom_val_18_enmwikipediaorg 928 895 952 889 880
c4_100dom_val_19_wwwcnetcom 1261 1223 1312 1209 1197
c4_100dom_val_1_wwwnytimescom 1313 1266 1404 1268 1244
c4_100dom_val_20_wwwtelegraphcouk 1371 1310 1428 1306 1288
c4_100dom_val_21_wwwtheatlanticcom 1470 1417 1554 1417 1397
c4_100dom_val_22_forumsmacrumorscom 1777 1734 1915 1722 1695
c4_100dom_val_23_wwworeillycom 1336 1299 1431 1302 1288
c4_100dom_val_24_wwwwashingtonpostcom 1206 1158 1298 1164 1141
c4_100dom_val_25_wwwzdnetcom 1322 1286 1380 1278 1261
c4_100dom_val_26_wwwfoxbusinesscom 932 903 958 892 881
c4_100dom_val_27_wwwreuterscom 1067 1013 1116 1013 997
c4_100dom_val_28_wwwibtimescouk 1136 1101 1171 1089 1076
c4_100dom_val_29_wwwrtcom 1359 1296 1424 1298 1274
c4_100dom_val_2_enwikipediaorg 1075 1045 1132 1032 1019
c4_100dom_val_30_wwwprwebcom 1118 1088 1192 1083 1065
c4_100dom_val_31_wwwdeviantartcom 2178 2105 2278 2100 2069
c4_100dom_val_32_wwwsicom 1149 1100 1192 1090 1076
c4_100dom_val_33_wwwbbccom 935 891 941 880 870
c4_100dom_val_34_githubcom 1157 1149 1294 1140 1128
c4_100dom_val_35_nypostcom 1431 1341 1529 1362 1331
c4_100dom_val_36_itunesapplecom 1649 1588 1715 1598 1569
c4_100dom_val_37_wwwinstructablescom 1675 1633 1773 1628 1597
c4_100dom_val_38_wwwyoutubecom 842 824 883 822 807
c4_100dom_val_39_wwwbookingcom 884 849 883 841 832
c4_100dom_val_40_wwwetsycom 1193 1166 1266 1152 1143
c4_100dom_val_41_wwwmarketwiredcom 766 747 788 733 727
c4_100dom_val_42_sitesgooglecom 1423 1381 1491 1368 1351
c4_100dom_val_43_wwwbaltimoresuncom 1157 1116 1196 1109 1095
c4_100dom_val_44_wwwagreatertowncom 1356 1294 1357 1277 1264
c4_100dom_val_45_wwwnprorg 1059 1030 1114 1019 1012
c4_100dom_val_46_wwwfoolcom 1103 1063 1135 1056 1042
c4_100dom_val_47_wwwtripadvisorcom 1580 1526 1626 1510 1493
c4_100dom_val_48_wwwbbccouk 1255 1210 1302 1200 1185
44Dataset Llama Mamba RWKV4 xLSTM71 xLSTM10
c4_100dom_val_49_listsw3org 1875 1824 1989 1805 1784
c4_100dom_val_4_wwwlatimescom 1188 1146 1240 1139 1124
c4_100dom_val_50_mashablecom 1244 1195 1285 1190 1176
c4_100dom_val_51_disneyparksmomspaneldisneygocom 1199 1129 1198 1116 1100
c4_100dom_val_52_wwwcnbccom 1065 1032 1099 1024 1010
c4_100dom_val_53_answerssapcom 2359 2309 2571 2299 2255
c4_100dom_val_54_homestarscom 1413 1370 1451 1365 1352
c4_100dom_val_55_wwwhindustantimescom 1213 1160 1274 1160 1137
c4_100dom_val_56_wwwreferencecom 1157 1104 1175 1092 1079
c4_100dom_val_57_wwwcitydatacom 1838 1794 1961 1773 1762
c4_100dom_val_58_mediumcom 1550 1509 1658 1518 1501
c4_100dom_val_59_appwiringdiagram 974 910 968 888 875
c4_100dom_val_5_wwwtheguardiancom 1478 1409 1547 1408 1386
c4_100dom_val_60_wwwcsmonitorcom 1535 1485 1592 1475 1457
c4_100dom_val_61_wwwadweekcom 1455 1395 1558 1409 1381
c4_100dom_val_62_docsmicrosoftcom 769 779 886 768 758
c4_100dom_val_63_wwwyahoocom 929 888 971 889 877
c4_100dom_val_64_wwwthesuncouk 1218 1166 1274 1159 1139
c4_100dom_val_65_wwwnydailynewscom 1215 1160 1261 1156 1136
c4_100dom_val_66_wwwdailystarcouk 1065 1017 1103 1009 992
c4_100dom_val_67_fineartamericacom 1206 1158 1229 1146 1136
c4_100dom_val_68_wwwkickstartercom 1385 1358 1538 1355 1338
c4_100dom_val_69_ukreuterscom 954 913 990 907 892
c4_100dom_val_6_wwwhuffpostcom 1345 1303 1396 1299 1283
c4_100dom_val_70_wwwinsiderpagescom 1324 1284 1355 1277 1264
c4_100dom_val_71_wwwinquisitrcom 1212 1158 1286 1171 1138
c4_100dom_val_72_listsdebianorg 1818 1781 1962 1767 1730
c4_100dom_val_73_wwwstraitstimescom 1151 1106 1191 1094 1079
c4_100dom_val_74_wwwcbsnewscom 1029 991 1060 982 972
c4_100dom_val_75_simplewikipediaorg 825 785 837 778 767
c4_100dom_val_76_deadlinecom 1475 1383 1548 1392 1351
c4_100dom_val_77_wwwandroidheadlinescom 1111 1074 1143 1072 1059
c4_100dom_val_78_wwwwiredcom 1442 1388 1514 1387 1368
c4_100dom_val_79_wwwbustlecom 1279 1233 1319 1225 1209
c4_100dom_val_7_patentsgooglecom 759 784 933 772 759
c4_100dom_val_80_premiumwpmudevorg 1686 1663 1813 1650 1629
c4_100dom_val_81_wwwlibrarythingcom 1436 1398 1542 1391 1375
c4_100dom_val_82_mailarchivesapacheorg 567 561 617 556 549
c4_100dom_val_83_scholarsdukeedu 872 843 903 832 821
c4_100dom_val_84_wwwglassdoorcom 1664 1597 1699 1600 1583
c4_100dom_val_85_wwwpcworldcom 1234 1195 1295 1190 1172
c4_100dom_val_86_wwwshutterstockcom 870 889 1075 862 852
c4_100dom_val_87_myemailconstantcontactcom 1459 1424 1532 1418 1398
c4_100dom_val_88_wwweventbritecouk 1447 1399 1489 1398 1379
c4_100dom_val_89_wwwfastcompanycom 1424 1375 1552 1382 1356
c4_100dom_val_8_wwwbusinessinsidercom 1097 1069 1135 1052 1046
c4_100dom_val_90_wwwfirstpostcom 1171 1124 1208 1112 1096
c4_100dom_val_91_wwwentrepreneurcom 1310 1268 1365 1272 1254
c4_100dom_val_92_wwwbreitbartcom 1347 1267 1429 1284 1256
c4_100dom_val_93_techcrunchcom 1420 1368 1518 1382 1358
c4_100dom_val_94_wwwnmecom 1412 1328 1506 1343 1312
c4_100dom_val_95_wwwndtvcom 1066 1026 1090 1010 1000
45Dataset Llama Mamba RWKV4 xLSTM71 xLSTM10
c4_100dom_val_96_financeyahoocom 996 955 1022 943 934
c4_100dom_val_97_archiveslibstatemaus 653 612 709 627 585
c4_100dom_val_98_wwwgsmarenacom 2321 2215 2452 2210 2176
c4_100dom_val_99_wwwlonelyplanetcom 1133 1092 1228 1084 1069
c4_100dom_val_9_wwwforbescom 1372 1331 1463 1334 1313
c4_en_val00000000 1434 1370 1487 1367 1346
c4_en_val00000001 1486 1428 1551 1421 1409
c4_en_val00000002 1529 1471 1595 1471 1451
c4_en_val00000003 1295 1228 1332 1223 1206
c4_en_val00000004 1256 1213 1327 1205 1187
c4_en_val00000005 1277 1235 1326 1232 1218
dolmav1_5_val_books 1300 1244 1364 1244 1227
dolmav1_5_val_commoncrawl 1686 1637 1800 1635 1610
dolmav1_5_val_pes2o 942 956 1125 941 929
dolmav1_5_val_reddit_uniform 2304 2197 2384 2205 2180
dolmav1_5_val_stack_uniform 230 233 253 230 229
dolmav1_5_val_wiki 1086 1048 1125 1041 1031
dolma_100_proglang_val_00_text 561 630 694 567 569
dolma_100_proglang_val_01_markdown 316 316 356 315 311
dolma_100_proglang_val_02_c 184 191 223 186 185
dolma_100_proglang_val_03_php 175 175 183 173 172
dolma_100_proglang_val_04_java 196 199 218 195 195
dolma_100_proglang_val_05_c 219 225 253 221 219
dolma_100_proglang_val_06_python 235 239 262 236 234
dolma_100_proglang_val_07_javascript 254 259 283 253 253
dolma_100_proglang_val_08_html 192 194 213 191 191
dolma_100_proglang_val_09_c 223 228 245 219 224
dolma_100_proglang_val_10_yaml 293 301 371 294 292
dolma_100_proglang_val_11_go 175 178 197 177 175
dolma_100_proglang_val_12_typescript 217 220 241 218 216
dolma_100_proglang_val_13_xml 244 250 278 246 248
dolma_100_proglang_val_14_css 225 225 234 221 220
dolma_100_proglang_val_15_jupyternb 157 160 175 158 158
dolma_100_proglang_val_16_rust 196 201 223 197 196
dolma_100_proglang_val_17_unity3dasset 401 417 456 410 405
dolma_100_proglang_val_18_gettextcatalog 284 287 353 286 283
dolma_100_proglang_val_19_ruby 241 244 270 239 238
dolma_100_proglang_val_20_vue 195 195 210 194 193
dolma_100_proglang_val_21_sql 218 223 246 217 216
dolma_100_proglang_val_22_swift 186 188 204 186 184
dolma_100_proglang_val_23_kotlin 205 207 229 207 204
dolma_100_proglang_val_24_scala 224 228 264 225 223
dolma_100_proglang_val_25_scss 226 227 238 224 224
dolma_100_proglang_val_26_tex 404 421 497 410 404
dolma_100_proglang_val_27_dart 179 182 201 180 178
dolma_100_proglang_val_28_kicad 257 279 386 268 267
dolma_100_proglang_val_29_shell 371 374 431 369 363
dolma_100_proglang_val_30_smali 138 139 145 138 137
dolma_100_proglang_val_31_lua 565 601 718 533 545
dolma_100_proglang_val_32_restructuredtext 401 405 466 397 392
dolma_100_proglang_val_33_perl 257 262 301 259 255
dolma_100_proglang_val_34_diff 287 295 343 289 286
46Dataset Llama Mamba RWKV4 xLSTM71 xLSTM10
dolma_100_proglang_val_35_ini 391 416 490 405 398
dolma_100_proglang_val_36_jsx 183 184 195 183 182
dolma_100_proglang_val_37_haskell 294 307 373 302 295
dolma_100_proglang_val_38_gnuplot 265 288 336 281 277
dolma_100_proglang_val_39_postscript 1909 1952 1956 1866 1864
dolma_100_proglang_val_40_groff 613 632 745 622 621
dolma_100_proglang_val_41_turtle 235 245 317 239 235
dolma_100_proglang_val_42_fortran 232 239 283 235 231
dolma_100_proglang_val_43_makefile 293 301 351 286 282
dolma_100_proglang_val_44_mathematica 1034 1134 1324 1049 1071
dolma_100_proglang_val_45_pascal 418 481 549 417 427
dolma_100_proglang_val_46_commonlisp 256 271 332 262 258
dolma_100_proglang_val_47_gas 249 273 359 257 253
dolma_100_proglang_val_48_vhdl 391 406 469 392 390
dolma_100_proglang_val_49_julia 325 336 405 330 326
dolma_100_proglang_val_50_edn 199 210 267 204 203
dolma_100_proglang_val_51_visualbasic 242 249 272 237 238
dolma_100_proglang_val_52_powershell 408 416 450 386 389
dolma_100_proglang_val_53_gcode 226 266 329 244 237
dolma_100_proglang_val_54_ocaml 306 329 422 319 313
dolma_100_proglang_val_55_javaserverp 210 211 231 206 209
dolma_100_proglang_val_56_solidity 409 441 528 405 410
dolma_100_proglang_val_57_graphvizdot 217 248 354 232 229
dolma_100_proglang_val_58_less 224 226 233 222 222
dolma_100_proglang_val_59_twig 181 181 191 180 179
dolma_100_proglang_val_60_asciidoc 533 550 684 543 534
dolma_100_proglang_val_61_groovy 212 215 241 213 211
dolma_100_proglang_val_62_llvm 226 240 325 231 223
dolma_100_proglang_val_63_hcl 252 256 296 252 248
dolma_100_proglang_val_64_htmlerb 210 209 223 208 207
dolma_100_proglang_val_65_erlang 284 298 387 288 285
dolma_100_proglang_val_66_elixir 293 299 358 291 290
dolma_100_proglang_val_67_eagle 535 690 1075 564 576
dolma_100_proglang_val_68_arduino 337 340 381 328 328
dolma_100_proglang_val_69_coffeescript 280 285 327 280 277
dolma_100_proglang_val_70_toml 776 762 844 753 758
dolma_100_proglang_val_71_cuda 215 221 256 219 216
dolma_100_proglang_val_72_nix 780 784 903 788 783
dolma_100_proglang_val_73_smalltalk 932 961 1260 947 920
dolma_100_proglang_val_74_cmake 187 186 202 184 181
dolma_100_proglang_val_75_actionscript 245 254 288 246 246
dolma_100_proglang_val_76_glsl 240 242 272 236 232
dolma_100_proglang_val_77_systemverilog 253 266 317 258 255
dolma_100_proglang_val_78_haxe 274 281 320 277 276
dolma_100_proglang_val_79_f 289 302 353 293 288
dolma_100_proglang_val_80_max 159 162 180 161 161
dolma_100_proglang_val_81_objectivec 218 219 240 217 216
dolma_100_proglang_val_82_standardml 357 405 479 381 377
dolma_100_proglang_val_83_dockerfile 408 417 437 401 405
dolma_100_proglang_val_84_emacslisp 383 383 444 380 372
dolma_100_proglang_val_85_scheme 278 286 340 284 277
dolma_100_proglang_val_86_clojure 318 330 400 326 317
47Dataset Llama Mamba RWKV4 xLSTM71 xLSTM10
dolma_100_proglang_val_87_handlebars 179 179 188 178 178
dolma_100_proglang_val_88_smarty 230 235 258 229 230
dolma_100_proglang_val_89_logos 237 258 298 246 244
dolma_100_proglang_val_90_stata 467 508 685 485 481
dolma_100_proglang_val_91_yacc 242 248 287 244 243
dolma_100_proglang_val_92_nimrod 275 287 363 281 277
dolma_100_proglang_val_93_tcl 300 316 395 307 302
dolma_100_proglang_val_94_viml 556 576 721 559 555
dolma_100_proglang_val_95_asp 179 179 190 177 177
dolma_100_proglang_val_96_protocolbuffer 132 131 138 131 132
dolma_100_proglang_val_97_r 280 292 366 286 281
dolma_100_proglang_val_98_cython 234 239 269 236 235
dolma_100_proglang_val_99_mediawiki 201 210 248 212 204
dolma_100_subreddits_val_00_AskReddit 2025 1929 2038 1928 1914
dolma_100_subreddits_val_01_politics 2208 2070 2207 2083 2061
dolma_100_subreddits_val_02_AmItheAsshole 2249 2130 2289 2160 2127
dolma_100_subreddits_val_03_worldnews 2257 2143 2277 2150 2123
dolma_100_subreddits_val_04_relationships 1864 1780 1889 1786 1767
dolma_100_subreddits_val_05_relationship_advice 1940 1853 1968 1863 1846
dolma_100_subreddits_val_06_news 2249 2125 2251 2149 2117
dolma_100_subreddits_val_07_leagueoflegends 3445 3241 3513 3246 3204
dolma_100_subreddits_val_08_todayilearned 2253 2130 2268 2128 2110
dolma_100_subreddits_val_09_TwoXChromosomes 2020 1916 2025 1920 1902
dolma_100_subreddits_val_10_personalfinance 1862 1765 1882 1773 1764
dolma_100_subreddits_val_11_changemyview 2002 1910 2050 1917 1899
dolma_100_subreddits_val_12_unpopularopinion 2339 2216 2363 2232 2204
dolma_100_subreddits_val_13_movies 2162 2052 2179 2064 2035
dolma_100_subreddits_val_14_Games 2226 2115 2252 2118 2087
dolma_100_subreddits_val_15_nba 2328 2193 2360 2210 2185
dolma_100_subreddits_val_16_pics 2184 2056 2182 2064 2047
dolma_100_subreddits_val_17_gaming 2445 2313 2461 2315 2286
dolma_100_subreddits_val_18_soccer 2338 2212 2361 2219 2203
dolma_100_subreddits_val_19_nfl 1986 1876 2017 1881 1862
dolma_100_subreddits_val_20_explainlikeimfive 1835 1721 1859 1732 1703
dolma_100_subreddits_val_21_conspiracy 2386 2253 2409 2267 2254
dolma_100_subreddits_val_22_atheism 2123 2018 2143 2023 2013
dolma_100_subreddits_val_23_AskMen 2000 1904 2011 1910 1894
dolma_100_subreddits_val_24_videos 2226 2124 2251 2129 2104
dolma_100_subreddits_val_25_sex 2113 2013 2130 2009 1998
dolma_100_subreddits_val_26_raisedbynarcissists 2207 2108 2248 2120 2102
dolma_100_subreddits_val_27_NoStupidQuestions 1966 1859 1987 1868 1852
dolma_100_subreddits_val_28_DestinyTheGame 3527 3358 3613 3378 3337
dolma_100_subreddits_val_29_anime 2321 2204 2346 2212 2177
dolma_100_subreddits_val_30_DnD 2822 2671 2878 2672 2639
dolma_100_subreddits_val_31_ukpolitics 2235 2119 2280 2131 2110
dolma_100_subreddits_val_32_funny 2078 1945 2070 1940 1923
dolma_100_subreddits_val_33_europe 2176 2059 2210 2072 2052
dolma_100_subreddits_val_34_canada 2244 2121 2244 2130 2109
dolma_100_subreddits_val_35_Christianity 1788 1702 1810 1704 1694
dolma_100_subreddits_val_36_SquaredCircle 2587 2431 2583 2434 2403
dolma_100_subreddits_val_37_AskWomen 1772 1681 1777 1685 1672
dolma_100_subreddits_val_38_legaladvice 1866 1775 1892 1774 1764
48Dataset Llama Mamba RWKV4 xLSTM71 xLSTM10
dolma_100_subreddits_val_39_JUSTNOMIL 2425 2316 2486 2332 2302
dolma_100_subreddits_val_40_technology 2339 2209 2352 2221 2195
dolma_100_subreddits_val_41_IAmA 1983 1883 1986 1871 1856
dolma_100_subreddits_val_42_wow 3126 2925 3144 2939 2882
dolma_100_subreddits_val_43_Parenting 2015 1911 2043 1930 1906
dolma_100_subreddits_val_44_exmormon 2312 2190 2344 2199 2184
dolma_100_subreddits_val_45_AdviceAnimals 2214 2096 2214 2098 2079
dolma_100_subreddits_val_46_childfree 2187 2085 2213 2089 2072
dolma_100_subreddits_val_47_unitedkingdom 2327 2200 2340 2200 2185
dolma_100_subreddits_val_48_ffxiv 3253 3079 3333 3101 3062
dolma_100_subreddits_val_49_dndnext 2967 2803 3053 2826 2763
dolma_100_subreddits_val_50_ADHD 2075 1983 2114 1995 1978
dolma_100_subreddits_val_51_loseit 1936 1839 1949 1852 1833
dolma_100_subreddits_val_52_asoiaf 2528 2399 2563 2394 2369
dolma_100_subreddits_val_53_BabyBumps 2096 1982 2111 1992 1976
dolma_100_subreddits_val_54_Advice 1917 1829 1935 1838 1819
dolma_100_subreddits_val_55_australia 2397 2251 2406 2261 2240
dolma_100_subreddits_val_56_CFB 2045 1941 2092 1949 1923
dolma_100_subreddits_val_57_offmychest 1963 1879 1977 1893 1877
dolma_100_subreddits_val_58_PublicFreakout 2596 2449 2602 2465 2439
dolma_100_subreddits_val_59_TrueOffMyChest 2153 2063 2170 2073 2054
dolma_100_subreddits_val_60_science 2044 1946 2064 1951 1938
dolma_100_subreddits_val_61_magicTCG 2882 2679 2894 2669 2638
dolma_100_subreddits_val_62_asktransgender 2072 1986 2107 1983 1962
dolma_100_subreddits_val_63_DotA2 3435 3238 3474 3257 3216
dolma_100_subreddits_val_64_neoliberal 2174 2059 2226 2064 2045
dolma_100_subreddits_val_65_whowouldwin 2918 2781 3008 2763 2730
dolma_100_subreddits_val_66_depression 1828 1752 1831 1750 1741
dolma_100_subreddits_val_67_WTF 2230 2118 2238 2117 2099
dolma_100_subreddits_val_68_pathofexile 4048 3859 4143 3875 3843
dolma_100_subreddits_val_69_PoliticalDiscussion 2001 1892 2016 1897 1882
dolma_100_subreddits_val_70_Libertarian 2297 2177 2315 2187 2175
dolma_100_subreddits_val_71_PurplePillDebate 2494 2366 2544 2385 2355
dolma_100_subreddits_val_72_Fitness 2157 2035 2148 2034 2011
dolma_100_subreddits_val_73_books 2112 2002 2131 2009 1982
dolma_100_subreddits_val_74_dogs 2013 1912 2032 1920 1892
dolma_100_subreddits_val_75_pcmasterrace 2373 2249 2402 2256 2221
dolma_100_subreddits_val_76_teenagers 1837 1635 1644 1556 1702
dolma_100_subreddits_val_77_stopdrinking 2108 2002 2119 2017 1998
dolma_100_subreddits_val_78_Overwatch 3047 2877 3113 2913 2857
dolma_100_subreddits_val_79_television 2397 2263 2405 2275 2249
dolma_100_subreddits_val_80_buildapc 2155 2022 2178 2029 1998
dolma_100_subreddits_val_81_askscience 1725 1639 1752 1634 1611
dolma_100_subreddits_val_82_programming 2366 2261 2404 2255 2224
dolma_100_subreddits_val_83_Guildwars2 3298 3117 3358 3139 3091
dolma_100_subreddits_val_84_cars 2257 2141 2273 2138 2115
dolma_100_subreddits_val_85_formula1 2385 2265 2409 2271 2249
dolma_100_subreddits_val_86_sysadmin 2423 2290 2441 2296 2264
dolma_100_subreddits_val_87_hockey 2146 2026 2174 2037 2020
dolma_100_subreddits_val_88_india 2415 2292 2442 2308 2268
dolma_100_subreddits_val_89_SubredditDrama 1914 1826 1963 1829 1812
dolma_100_subreddits_val_90_DMAcademy 2777 2631 2838 2641 2600
49Dataset Llama Mamba RWKV4 xLSTM71 xLSTM10
dolma_100_subreddits_val_91_dating_advice 2018 1927 2042 1940 1921
dolma_100_subreddits_val_92_Catholicism 1911 1822 1941 1817 1803
dolma_100_subreddits_val_93_Drugs 2450 2329 2474 2332 2312
dolma_100_subreddits_val_94_trees 2356 2238 2383 2241 2225
dolma_100_subreddits_val_95_boardgames 2269 2148 2313 2161 2138
dolma_100_subreddits_val_96_Conservative 2279 2153 2297 2168 2153
dolma_100_subreddits_val_97_Futurology 2355 2236 2377 2237 2217
dolma_100_subreddits_val_98_beyondthebump 2107 1989 2122 2008 1983
dolma_100_subreddits_val_99_weddingplanning 2011 1901 2033 1919 1896
falconrefinedweb_val00000000 1592 1546 1714 1537 1522
falconrefinedweb_val00000001 1849 1791 1989 1790 1771
falconrefinedweb_val00000002 1845 1790 1969 1791 1768
falconrefinedweb_val00000003 1675 1623 1792 1616 1589
falconrefinedweb_val00000004 1626 1566 1732 1573 1541
falconrefinedweb_val00000005 1541 1496 1656 1492 1474
gab_val00000000 3319 3055 3157 3073 3032
gab_val00000001 3564 3276 3396 3280 3263
gab_val00000002 3438 3168 3275 3180 3165
gab_val00000003 3486 3205 3326 3220 3200
gab_val00000004 3620 3335 3458 3342 3323
gab_val00000005 3346 3082 3188 3106 3072
gab_val00000006 3576 3277 3426 3304 3274
gab_val00000007 3554 3260 3376 3278 3241
gab_val00000008 3511 3203 3323 3225 3186
gab_val00000009 3413 3134 3236 3150 3130
m2d2_s2orc_unsplit_val_Art 2007 1980 2188 1978 1944
m2d2_s2orc_unsplit_val_Philosophy 1480 1482 1677 1469 1447
m2d2_s2orc_unsplit_val_astroph 1170 1170 1318 1152 1133
m2d2_s2orc_unsplit_val_astrophCO 1147 1149 1290 1137 1115
m2d2_s2orc_unsplit_val_astrophEP 1276 1273 1428 1260 1245
m2d2_s2orc_unsplit_val_astrophGA 1170 1170 1318 1152 1133
m2d2_s2orc_unsplit_val_astrophHE 1185 1177 1329 1162 1146
m2d2_s2orc_unsplit_val_astrophIM 1536 1533 1716 1521 1492
m2d2_s2orc_unsplit_val_astrophSR 1308 1308 1489 1286 1270
m2d2_s2orc_unsplit_val_astroph_l1 1536 1533 1716 1521 1492
m2d2_s2orc_unsplit_val_atomph 1274 1284 1444 1275 1253
m2d2_s2orc_unsplit_val_chemph 1320 1329 1522 1314 1297
m2d2_s2orc_unsplit_val_condmat 1167 1178 1337 1167 1150
m2d2_s2orc_unsplit_val_condmatdisnn 1254 1267 1428 1258 1238
m2d2_s2orc_unsplit_val_condmatmeshall 1124 1150 1319 1130 1110
m2d2_s2orc_unsplit_val_condmatmtrlsci 1219 1233 1409 1218 1191
m2d2_s2orc_unsplit_val_condmatother 1187 1196 1355 1183 1165
m2d2_s2orc_unsplit_val_condmatquantgas 1167 1178 1337 1167 1150
m2d2_s2orc_unsplit_val_condmatsoft 1218 1223 1393 1218 1202
m2d2_s2orc_unsplit_val_condmatstatmech 1203 1214 1360 1208 1189
m2d2_s2orc_unsplit_val_condmatstrel 1039 1050 1198 1041 1022
m2d2_s2orc_unsplit_val_condmatsuprcon 1157 1166 1313 1153 1130
m2d2_s2orc_unsplit_val_condmat_l1 1254 1267 1428 1258 1238
m2d2_s2orc_unsplit_val_csAI 1171 1209 1420 1201 1179
m2d2_s2orc_unsplit_val_csAR 1309 1336 1530 1318 1299
m2d2_s2orc_unsplit_val_csCC 845 881 1046 870 854
m2d2_s2orc_unsplit_val_csCE 1321 1331 1501 1318 1302
50Dataset Llama Mamba RWKV4 xLSTM71 xLSTM10
m2d2_s2orc_unsplit_val_csCG 839 868 1012 859 847
m2d2_s2orc_unsplit_val_csCL 1466 1475 1696 1470 1447
m2d2_s2orc_unsplit_val_csCR 1463 1486 1672 1474 1456
m2d2_s2orc_unsplit_val_csCV 1268 1278 1438 1266 1249
m2d2_s2orc_unsplit_val_csCY 1601 1593 1752 1584 1567
m2d2_s2orc_unsplit_val_csDB 1186 1235 1466 1227 1203
m2d2_s2orc_unsplit_val_csDC 1360 1402 1620 1379 1356
m2d2_s2orc_unsplit_val_csDL 1467 1483 1705 1475 1450
m2d2_s2orc_unsplit_val_csDM 811 838 984 827 814
m2d2_s2orc_unsplit_val_csDS 963 999 1176 988 969
m2d2_s2orc_unsplit_val_csET 1480 1495 1700 1489 1467
m2d2_s2orc_unsplit_val_csFL 951 984 1164 974 957
m2d2_s2orc_unsplit_val_csGL 1651 1643 1818 1638 1621
m2d2_s2orc_unsplit_val_csGR 1345 1360 1553 1354 1329
m2d2_s2orc_unsplit_val_csGT 925 959 1134 949 929
m2d2_s2orc_unsplit_val_csHC 1676 1693 1908 1684 1666
m2d2_s2orc_unsplit_val_csIR 1330 1346 1526 1331 1321
m2d2_s2orc_unsplit_val_csLG 1039 1052 1214 1044 1027
m2d2_s2orc_unsplit_val_csLO 975 1023 1250 1003 981
m2d2_s2orc_unsplit_val_csMA 1124 1165 1410 1141 1119
m2d2_s2orc_unsplit_val_csMM 1312 1340 1529 1325 1303
m2d2_s2orc_unsplit_val_csMS 1398 1414 1627 1411 1389
m2d2_s2orc_unsplit_val_csNA 1053 1080 1252 1071 1047
m2d2_s2orc_unsplit_val_csNE 1376 1400 1610 1389 1364
m2d2_s2orc_unsplit_val_csNI 1000 1022 1161 1004 993
m2d2_s2orc_unsplit_val_csOH 1524 1543 1762 1534 1510
m2d2_s2orc_unsplit_val_csOS 1461 1493 1735 1480 1453
m2d2_s2orc_unsplit_val_csPF 1260 1282 1471 1270 1248
m2d2_s2orc_unsplit_val_csPL 1543 1574 1858 1565 1540
m2d2_s2orc_unsplit_val_csRO 1304 1319 1495 1312 1287
m2d2_s2orc_unsplit_val_csSC 1110 1142 1333 1130 1110
m2d2_s2orc_unsplit_val_csSD 1327 1342 1526 1336 1313
m2d2_s2orc_unsplit_val_csSE 1772 1347 1546 1340 1321
m2d2_s2orc_unsplit_val_csSI 1203 1225 1403 1219 1199
m2d2_s2orc_unsplit_val_csSY 1140 1179 1351 1163 1139
m2d2_s2orc_unsplit_val_cs_l1 839 868 1012 859 847
m2d2_s2orc_unsplit_val_econEM 1162 1176 1373 1168 1141
m2d2_s2orc_unsplit_val_econTH 975 1016 1199 999 988
m2d2_s2orc_unsplit_val_econ_l1 975 1016 1199 999 988
m2d2_s2orc_unsplit_val_eessAS 1205 1214 1388 1209 1188
m2d2_s2orc_unsplit_val_eessIV 1377 1389 1571 1376 1354
m2d2_s2orc_unsplit_val_eessSP 1129 1145 1294 1128 1113
m2d2_s2orc_unsplit_val_eess_l1 1377 1389 1571 1376 1354
m2d2_s2orc_unsplit_val_grqc 1284 1299 1468 1284 1271
m2d2_s2orc_unsplit_val_hepex 1047 1037 1161 1013 996
m2d2_s2orc_unsplit_val_heplat 1313 1310 1457 1302 1280
m2d2_s2orc_unsplit_val_hepph 1167 1181 1338 1166 1145
m2d2_s2orc_unsplit_val_hepth 1146 1149 1271 1140 1124
m2d2_s2orc_unsplit_val_mathAC 708 737 871 726 713
m2d2_s2orc_unsplit_val_mathAG 889 927 1105 916 895
m2d2_s2orc_unsplit_val_mathAP 935 953 1090 941 935
m2d2_s2orc_unsplit_val_mathAT 857 877 1016 872 853
51Dataset Llama Mamba RWKV4 xLSTM71 xLSTM10
m2d2_s2orc_unsplit_val_mathCA 918 949 1101 936 930
m2d2_s2orc_unsplit_val_mathCO 699 733 869 721 708
m2d2_s2orc_unsplit_val_mathCT 978 1020 1204 1012 991
m2d2_s2orc_unsplit_val_mathCV 781 807 936 799 787
m2d2_s2orc_unsplit_val_mathDG 796 818 950 808 798
m2d2_s2orc_unsplit_val_mathDS 788 812 961 808 796
m2d2_s2orc_unsplit_val_mathFA 771 796 935 788 781
m2d2_s2orc_unsplit_val_mathGM 785 815 957 807 793
m2d2_s2orc_unsplit_val_mathGN 627 656 782 645 638
m2d2_s2orc_unsplit_val_mathGR 739 766 900 751 741
m2d2_s2orc_unsplit_val_mathGT 747 771 927 762 747
m2d2_s2orc_unsplit_val_mathHO 1452 1470 1652 1451 1431
m2d2_s2orc_unsplit_val_mathKT 754 780 914 770 758
m2d2_s2orc_unsplit_val_mathLO 984 1041 1253 1013 1003
m2d2_s2orc_unsplit_val_mathMG 825 853 999 842 826
m2d2_s2orc_unsplit_val_mathNA 985 1005 1166 995 983
m2d2_s2orc_unsplit_val_mathNT 826 851 992 843 831
m2d2_s2orc_unsplit_val_mathOA 721 755 907 747 732
m2d2_s2orc_unsplit_val_mathOC 970 1001 1162 985 969
m2d2_s2orc_unsplit_val_mathPR 891 920 1058 904 899
m2d2_s2orc_unsplit_val_mathQA 809 840 993 828 816
m2d2_s2orc_unsplit_val_mathRA 718 744 875 739 727
m2d2_s2orc_unsplit_val_mathRT 839 871 1033 865 849
m2d2_s2orc_unsplit_val_mathSG 863 888 1036 876 859
m2d2_s2orc_unsplit_val_mathSP 939 965 1127 952 937
m2d2_s2orc_unsplit_val_math_l1 781 807 936 799 787
m2d2_s2orc_unsplit_val_nlinAO 1182 1201 1377 1190 1175
m2d2_s2orc_unsplit_val_nlinCD 1273 1291 1488 1287 1260
m2d2_s2orc_unsplit_val_nlinCG 1243 1275 1488 1261 1244
m2d2_s2orc_unsplit_val_nlinPS 1129 1144 1286 1139 1122
m2d2_s2orc_unsplit_val_nlinSI 944 981 1128 964 951
m2d2_s2orc_unsplit_val_nlin_l1 1243 1275 1488 1261 1244
m2d2_s2orc_unsplit_val_nuclex 1302 1294 1461 1285 1263
m2d2_s2orc_unsplit_val_nuclth 1165 1178 1343 1168 1148
m2d2_s2orc_unsplit_val_physicsaccph 1375 1401 1617 1374 1358
m2d2_s2orc_unsplit_val_physicsaoph 1392 1404 1591 1389 1368
m2d2_s2orc_unsplit_val_physicsappph 1370 1381 1554 1362 1343
m2d2_s2orc_unsplit_val_physicsatmclus 1300 1313 1511 1300 1274
m2d2_s2orc_unsplit_val_physicsatomph 1274 1284 1444 1275 1253
m2d2_s2orc_unsplit_val_physicsbioph 1330 1342 1526 1332 1308
m2d2_s2orc_unsplit_val_physicschemph 1320 1329 1522 1314 1297
m2d2_s2orc_unsplit_val_physicsclassph 1101 1127 1285 1112 1094
m2d2_s2orc_unsplit_val_physicscompph 1123 1137 1288 1126 1108
m2d2_s2orc_unsplit_val_physicsdataan 1318 1333 1497 1325 1300
m2d2_s2orc_unsplit_val_physicsedph 1221 1233 1388 1218 1203
m2d2_s2orc_unsplit_val_physicsfludyn 1181 1199 1373 1181 1164
m2d2_s2orc_unsplit_val_physicsgenph 1415 1439 1676 1418 1403
m2d2_s2orc_unsplit_val_physicsgeoph 1475 1486 1681 1471 1457
m2d2_s2orc_unsplit_val_physicshistph 1557 1543 1697 1540 1518
m2d2_s2orc_unsplit_val_physicsinsdet 1401 1416 1614 1407 1379
m2d2_s2orc_unsplit_val_physicsmedph 1434 1446 1650 1429 1409
m2d2_s2orc_unsplit_val_physicsoptics 1274 1294 1464 1280 1254
52Dataset Llama Mamba RWKV4 xLSTM71 xLSTM10
m2d2_s2orc_unsplit_val_physicsplasmph 1365 1381 1577 1369 1344
m2d2_s2orc_unsplit_val_physicspopph 1380 1367 1517 1360 1341
m2d2_s2orc_unsplit_val_physicssocph 1279 1297 1480 1283 1266
m2d2_s2orc_unsplit_val_physicsspaceph 1300 1309 1477 1294 1276
m2d2_s2orc_unsplit_val_physics_l1 1557 1543 1697 1540 1518
m2d2_s2orc_unsplit_val_plasmph 1365 1381 1577 1369 1344
m2d2_s2orc_unsplit_val_qbio 1369 1387 1575 1375 1350
m2d2_s2orc_unsplit_val_qbioBM 1328 1352 1572 1341 1319
m2d2_s2orc_unsplit_val_qbioCB 1206 1234 1421 1219 1197
m2d2_s2orc_unsplit_val_qbioGN 1321 1140 1274 1132 1116
m2d2_s2orc_unsplit_val_qbioMN 1196 1195 1336 1190 1170
m2d2_s2orc_unsplit_val_qbioNC 1369 1387 1575 1375 1350
m2d2_s2orc_unsplit_val_qbioOT 1490 1494 1716 1492 1473
m2d2_s2orc_unsplit_val_qbioPE 1257 1271 1462 1269 1241
m2d2_s2orc_unsplit_val_qbioQM 1249 1269 1444 1256 1240
m2d2_s2orc_unsplit_val_qbioSC 1368 1385 1560 1375 1353
m2d2_s2orc_unsplit_val_qbioTO 1349 1353 1532 1348 1333
m2d2_s2orc_unsplit_val_qbio_l1 1369 1387 1575 1375 1350
m2d2_s2orc_unsplit_val_qfinCP 1137 1161 1336 1141 1128
m2d2_s2orc_unsplit_val_qfinEC 1172 1189 1377 1177 1163
m2d2_s2orc_unsplit_val_qfinGN 1379 1391 1573 1383 1361
m2d2_s2orc_unsplit_val_qfinMF 991 1021 1192 1004 990
m2d2_s2orc_unsplit_val_qfinPM 1100 1131 1314 1114 1094
m2d2_s2orc_unsplit_val_qfinPR 1587 925 1037 920 903
m2d2_s2orc_unsplit_val_qfinRM 1135 1149 1308 1141 1122
m2d2_s2orc_unsplit_val_qfinST 1243 1246 1418 1243 1226
m2d2_s2orc_unsplit_val_qfinTR 1279 1314 1532 1289 1274
m2d2_s2orc_unsplit_val_qfin_l1 1379 1391 1573 1383 1361
m2d2_s2orc_unsplit_val_quantph 1118 1144 1318 1132 1111
m2d2_s2orc_unsplit_val_statAP 1337 1356 1552 1342 1315
m2d2_s2orc_unsplit_val_statCO 1307 1256 1442 1246 1224
m2d2_s2orc_unsplit_val_statME 1109 1126 1291 1111 1087
m2d2_s2orc_unsplit_val_statML 1113 1139 1329 1123 1106
m2d2_s2orc_unsplit_val_statOT 1131 1155 1328 1145 1124
m2d2_s2orc_unsplit_val_stat_l1 1307 1256 1442 1246 1224
m2d2_s2orc_unsplit_val_suprcon 1157 1166 1313 1153 1130
m2d2_wikipedia_unsplit_val_Culture_and_the_arts 1230 1190 1282 1178 1166
m2d2_wikipedia_unsplit_val_Culture_and_the_arts__Culture_and_Humanities 1213 1174 1282 1163 1148
m2d2_wikipedia_unsplit_val_Culture_and_the_arts__Games_and_Toys 1406 1386 1517 1379 1357
m2d2_wikipedia_unsplit_val_Culture_and_the_arts__Mass_media 1216 1180 1274 1179 1155
m2d2_wikipedia_unsplit_val_Culture_and_the_arts__Performing_arts 1175 1125 1203 1117 1103
m2d2_wikipedia_unsplit_val_Culture_and_the_arts__Sports_and_Recreation 1001 963 1036 958 954
m2d2_wikipedia_unsplit_val_Culture_and_the_arts__The_arts_and_Entertainment 1213 1185 1283 1173 1158
m2d2_wikipedia_unsplit_val_Culture_and_the_arts__Visual_arts 1236 1209 1305 1199 1187
m2d2_wikipedia_unsplit_val_General_referece 1180 1146 1243 1146 1130
m2d2_wikipedia_unsplit_val_General_referece__Further_research_tools_and_topics 1052 1020 1096 1012 999
m2d2_wikipedia_unsplit_val_General_referece__Reference_works 1180 1146 1243 1146 1130
m2d2_wikipedia_unsplit_val_Health_and_fitness 1075 1047 1114 1037 1030
m2d2_wikipedia_unsplit_val_Health_and_fitness__Exercise 964 929 995 927 916
m2d2_wikipedia_unsplit_val_Health_and_fitness__Health_science 1010 980 1043 971 956
m2d2_wikipedia_unsplit_val_Health_and_fitness__Human_medicine 914 883 959 863 854
m2d2_wikipedia_unsplit_val_Health_and_fitness__Nutrition 891 868 940 861 847
53Dataset Llama Mamba RWKV4 xLSTM71 xLSTM10
m2d2_wikipedia_unsplit_val_Health_and_fitness__Public_health 1075 1047 1114 1037 1030
m2d2_wikipedia_unsplit_val_Health_and_fitness__Self_care 1291 1249 1361 1242 1228
m2d2_wikipedia_unsplit_val_History_and_events 1365 1329 1448 1320 1300
m2d2_wikipedia_unsplit_val_History_and_events__By_continent 1177 1144 1236 1136 1126
m2d2_wikipedia_unsplit_val_History_and_events__By_period 1278 1241 1346 1237 1212
m2d2_wikipedia_unsplit_val_History_and_events__By_region 1236 1188 1287 1179 1164
m2d2_wikipedia_unsplit_val_Human_activites 1243 1203 1298 1195 1181
m2d2_wikipedia_unsplit_val_Human_activites__Human_activities 1243 1203 1298 1195 1181
m2d2_wikipedia_unsplit_val_Human_activites__Impact_of_human_activity 1247 1205 1312 1200 1182
m2d2_wikipedia_unsplit_val_Mathematics_and_logic 1290 1251 1379 1248 1229
m2d2_wikipedia_unsplit_val_Mathematics_and_logic__Fields_of_mathematics 824 826 937 828 806
m2d2_wikipedia_unsplit_val_Mathematics_and_logic__Logic 1321 1287 1390 1285 1267
m2d2_wikipedia_unsplit_val_Mathematics_and_logic__Mathematics 1290 1251 1379 1248 1229
m2d2_wikipedia_unsplit_val_Natural_and_physical_sciences 919 822 881 797 796
m2d2_wikipedia_unsplit_val_Natural_and_physical_sciences__Biology 1097 1070 1153 1064 1051
m2d2_wikipedia_unsplit_val_Natural_and_physical_sciences__Earth_sciences 1169 1136 1228 1122 1105
m2d2_wikipedia_unsplit_val_Natural_and_physical_sciences__Nature 1043 1011 1095 1000 982
m2d2_wikipedia_unsplit_val_Natural_and_physical_sciences__Physical_sciences 1148 1109 1193 1098 1090
m2d2_wikipedia_unsplit_val_Philosophy_and_thinking 1183 1172 1304 1160 1145
m2d2_wikipedia_unsplit_val_Philosophy_and_thinking__Philosophy 1200 1161 1266 1157 1143
m2d2_wikipedia_unsplit_val_Philosophy_and_thinking__Thinking 1094 1061 1134 1056 1042
m2d2_wikipedia_unsplit_val_Religion_and_belief_systems 1281 1245 1344 1238 1219
m2d2_wikipedia_unsplit_val_Religion_and_belief_systems__Allah 1111 1080 1166 1071 1058
m2d2_wikipedia_unsplit_val_Religion_and_belief_systems__Belief_systems 1146 1106 1186 1095 1085
m2d2_wikipedia_unsplit_val_Religion_and_belief_systems__Major_beliefs_of_the_world 1238 1203 1294 1191 1179
m2d2_wikipedia_unsplit_val_Society_and_social_sciences 1053 1024 1103 1016 1005
m2d2_wikipedia_unsplit_val_Society_and_social_sciences__Social_sciences 1047 1016 1095 1014 1004
m2d2_wikipedia_unsplit_val_Society_and_social_sciences__Society 1248 1213 1302 1207 1193
m2d2_wikipedia_unsplit_val_Technology_and_applied_sciences 851 818 866 793 788
m2d2_wikipedia_unsplit_val_Technology_and_applied_sciences__Agriculture 1245 1207 1300 1203 1188
m2d2_wikipedia_unsplit_val_Technology_and_applied_sciences__Computing 1362 1323 1456 1318 1297
m2d2_wikipedia_unsplit_val_Technology_and_applied_sciences__Engineering 1300 1272 1387 1264 1243
m2d2_wikipedia_unsplit_val_Technology_and_applied_sciences__Transport 1434 1390 1520 1394 1373
manosphere_meta_sep_val_avfm 1942 1927 2188 1964 1918
manosphere_meta_sep_val_incels 1126 1218 2140 1151 1129
manosphere_meta_sep_val_mgtow 2483 2427 2750 2412 2380
manosphere_meta_sep_val_pua_forum 2422 2385 2652 2386 2352
manosphere_meta_sep_val_red_pill_talk 3459 3390 3726 3390 3327
manosphere_meta_sep_val_reddit 2063 1978 2110 1994 1958
manosphere_meta_sep_val_rooshv 2246 2217 2478 2201 2169
manosphere_meta_sep_val_the_attraction 2085 2057 2317 2057 2020
mc4_val00000000 835 841 1002 823 815
mc4_val00000001 1217 1197 1358 1174 1164
mc4_val00000002 996 1006 1196 986 967
mc4_val00000003 1138 1129 1277 1112 1100
mc4_val00000004 1196 1164 1303 1150 1135
ptb_val 1592 1665 1937 1600 1592
redpajama_val_arxiv 515 528 578 512 509
redpajama_val_books 1291 1271 1360 1261 1250
redpajama_val_c4 1301 1251 1355 1249 1227
redpajama_val_commoncrawl 1090 1056 1170 1052 1035
redpajama_val_github 166 166 175 165 164
54Dataset Llama Mamba RWKV4 xLSTM71 xLSTM10
redpajama_val_stackexchange 373 372 403 368 363
redpajama_val_wikipedia 464 438 468 435 429
twitterAAE_HELM_fixed_val_AA 34698 30279 31030 30165 28997
twitterAAE_HELM_fixed_val_white 11862 10734 10913 10765 10513
wikitext_103_val 1174 1176 1373 1132 1141
Table 10 PPL Evaluations For the 13B sized models trained on 300B SlimPajama tokens these are
the detailed evaluation results on the respective validation datasets
55
  Autoregressive Model Beats Diffusion Llama for
Scalable Image Generation
Peize Sun1Yi Jiang2Shoufa Chen1Shilong Zhang1Bingyue Peng2
Ping Luo1Zehuan Yuan2
1The University of Hong Kong2ByteDance
Codes and models httpsgithubcomFoundationVisionLlamaGen
Figure 1 Image generation with vanilla autoregressive models  We show samples from our
classconditional image top row and textconditional image bottom row generation models
Abstract
We introduce LlamaGen a new family of image generation models that apply origi
nal nexttoken prediction paradigm of large language models to visual generation
domain It is an affirmative answer to whether vanilla autoregressive models eg
Llama without inductive biases on visual signals can achieve stateoftheart image
generation performance if scaling properly We reexamine design spaces of image
tokenizers scalability properties of image generation models and their training
data quality The outcome of this exploration consists of 1 An image tokenizer
with downsample ratio of 16 reconstruction quality of 094 rFID and codebook
usage of 97 on ImageNet benchmark 2 A series of classconditional image
generation models ranging from 111M to 31B parameters achieving 218 FID on
ImageNet 256 256 benchmarks outperforming the popular diffusion models such
as LDM DiT 3 A textconditional image generation model with 775M parame
ters from twostage training on LAIONCOCO and high aesthetics quality images
demonstrating competitive performance of visual quality and text alignment 4
We verify the effectiveness of LLM serving frameworks in optimizing the inference
speed of image generation models and achieve 326  414 speedup We release
all models and codes to facilitate opensource community of visual generation and
multimodal foundation models
 Corresponding authors  project leadarXiv240606525v1  csCV  10 Jun 20241 Introduction
Built upon autoregressive models large language models LLMs Vaswani et al 2017 Devlin et al
2018 Radford et al 2018 Raffel et al 2020 Radford et al 2019 Brown et al 2020 Zhang et al
2022 generate the text by predicting the next token in a sequence This nexttoken prediction
paradigm presents unprecedented capabilities in solving language tasks in a humanlike conversational
manner Ouyang et al 2022 OpenAI 2022 2023b Google 2023 Anthropic 2023 Workshop et al
2022 Touvron et al 2023ab Bai et al 2023a Yang et al 2023 Team 2023 Bi et al 2024 and
incredible scalability Kaplan et al 2020 Henighan et al 2020 Hoffmann et al 2022 Wei et al 2022
Alabdulmohsin et al 2022 Chowdhery et al 2023 Anil et al 2023 demonstrating a promising path
toward generalpurpose artificial intelligence models
Witnessed the scalability of autoregressive models on large language models pioneering works
attempt to explore autoregressive models in image generation for example VQV AE Van Den Oord
et al 2017 Razavi et al 2019 VQGAN Esser et al 2021 Lee et al 2022 DALLE Ramesh et al
2021 Parti Yu et al 2021 2022 They introduce image tokenizers to convert continuous images to
discrete tokens and apply autoregressive models to generate image tokens in the way of nexttoken
prediction They demonstrate strong performance among their contemporaries Brock et al 2018
Ho et al 2020 Dhariwal  Nichol 2021 in the year before 2022 However their opensource
communities are not well developed which largely limits their further improvements
At the same period another image generation method diffusion models Song  Ermon 2019 Ho
et al 2020 Song et al 2020 Dhariwal  Nichol 2021 Nichol et al 2021 Lu et al 2022a Ho
et al 2022a Ho  Salimans 2022 Rombach et al 2022 Ramesh et al 2022 Saharia et al 2022
Rombach et al 2022 develop rapidly Along with their opensource communities they dominate
the field of visual generation up to today However diffusion models share distinct paradigms with
autoregressive language models which poses a huge challenge to building a unified model between
language and vision
In this work we are committed to pushing the envelope of autoregressive models on image generation
further continuing its research methodology and contributing to opensource community Reviewing
the literature on image generation in the year before 2024 we identify three keys to existing advanced
models Peebles  Xie 2023 Podell et al 2023 Xue et al 2023 Chen et al 2023bc Betker et al
2023 Li et al 2024 Esser et al 2024 1 welldesigned image compressors 2 scalable image
generation models and 3 highquality training data Motivated by this we reexamine the designs of
image tokenizers image compressors for autoregressive models the scalability properties of image
generation models and the effects of training data
Towards a potential unified model between language and vision our design is reducing the inductive
biases on visual signals and adopting the same architecture as LLM This belongs to a different re
search philosophy with recent works Chang et al 2022 Yu et al 2023b Tian et al 2024 that modify
the architectures under the guidance of visionoriented designs For example MaskGIT Chang et al
2022 MAGVIT Yu et al 2023ab adopt the masked image modeling strategy V AR Tian et al
2024 uses hierarchical multiscale property Although they have succeeded in achieving leading
image generation performance and even better than diffusion models it is still not clear whether
the original language model architectures are capable of this Instead our work reveals that vanilla
autoregressive models that apply the exactly same nexttoken prediction as language models are
also able to achieve stateoftheart image generation performance As a bonus we can leverage the
techniques Dao et al 2022 Rasley et al 2020 Shoeybi et al 2019 Zhao et al 2023 Kwon et al
2023 Chen et al 2023a Dettmers 2022 developed in LLM community to optimize the training
recipes and inference speeds of our models
In summary our contributions to the community include
1Image tokenizer An image tokenizer with downsample ratio of 16 achieves reconstruction
quality of 094 rFID and codebook usage of 97 on ImageNet benchmark With the downsample
ratio of 8 our tokenizer is competitive or even better than continuous V AE Rombach et al
2022 Podell et al 2023 OpenAI 2023a used in diffusion models This shows that discrete
representation in image tokenizers is no longer the bottleneck of the image reconstruction
2Scalable image generation model A series of classconditional image generation models
ranging from 111M to 31B parameters are developed based on Llama architecture Touvron
2et al 2023ab The largest model realizes 218 FID on ImageNet 256 256 benchmarks
outperforming the popular diffusion models such as LDM Rombach et al 2022 DiT Peebles
 Xie 2023 This shows that vanilla autoregressive models without inductive biases on visual
signals can serve as the basis of image generation systems
3Hiqhquality training data A textconditional image generation model with 775M parameters
is firstly trained on a 50M subset of LAIONCOCO LAION 2022 and then finetuned on
10M internal high aesthetics quality images It demonstrates competitive performance of visual
quality and text alignment
4Optimized inference speed We adopt vLLM Kwon et al 2023 one of the most popular
LLM serving frameworks to optimize the inference speed of our image generation models and
remarkable 326  414 speedup is achieved
We release all models and codes to facilitate the opensource community of visual generation and
multimodal foundation models It is worth noticing that our released models are still behind stateof
theart visual generation models based on diffusion models AlphaVLLM 2024 Esser et al 2024
Brooks et al 2024 When more training data and computation resources are available in the future
largescale ARbased visual generation models eg above 7B parameters will be explored
2 Autoregressive Models for Image Generation
21 Overview
Firstly image pixels xRHW3are quantized into qQhwdiscrete tokens by the image
tokenizer Van Den Oord et al 2017 Esser et al 2021 Yu et al 2021 where hHpwWp 
pis downsample ratio of the image tokenizer qijis indices of the image codebook Then these
image tokens are reshaped to a sequence of hwtokens in raster scan ordering and used to train
Transformer Vaswani et al 2017based autoregressive models
During image generation image tokens q1 q2     q hware generated by autoregressive mod
els Radford et al 2018 2019 Brown et al 2020 Touvron et al 2023a in the way of nexttoken
predictionQhw
t1pqtqt c where cis class label embedding or text embedding Finally these
image tokens are converted to image pixels by the image tokenizer decoder
22 Image Tokenizer
QuantizedAutoencoder architecture We use the same architecture as VQGAN Esser et al
2021 encoderquantizerdecoder The encoder and the decoder are ConvNet with downsample ratio
p The quantizer contains a codebook ZRKCwithKlearnable vectors The encoder projects
image pixels xto the feature map f The quantization process maps each vector fijin the feature
map to the code index qijof its nearest vector zijin the codebook During decoding the code
index qijis remapped to the feature vector zijand the decoder converts these feature vectors
back to the image pixels ˆx
The codebook has critical effects on image tokenization performance Following Yu et al 2021 we
useℓ2normalization to codebook vectors low codebook vector dimension C and large codebook
sizeK These designs significantly improve reconstruction quality and codebook usage More details
will be discussed in experiments
Training losses Since quantization is a nondifferentiable operation a straightthrough gradient
estimator Bengio et al 2013 is used to preserve the gradient from the decoder to the encoder
zsgzf fsgis stopgradient operation For codebook learning LVQsgfz2
2
βfsgz2
2 where the second term is commitment loss Van Den Oord et al 2017 to force feature
vectors extracted from the encoder to be close to codebook vectors βis commitment loss weight
For simplicity we dont add entropy loss Yu et al 2023a Chang et al 2022 in codebook learning
For image reconstruction training LAEℓ2xˆxLPxˆxλGLGˆx where ℓ2is a reconstruction
loss on image pixels LPis a perceptual loss from LPIPS Zhang et al 2018 LGis an adversarial
loss from a PatchGAN Isola et al 2017 discriminator trained at the same time with the image
tokenizer and λGis adversarial loss weight
3Model Parameters Layers Hidden Size Heads
LlamaGenB 111M 12 768 12
LlamaGenL 343M 24 1024 16
LlamaGenXL 775M 36 1280 20
LlamaGenXXL 14B 48 1536 24
LlamaGen3B 31B 24 3200 32
Table 1 Model sizes and architecture configurations of LlamaGen The configurations are
following previous works Radford et al 2019 Touvron et al 2023a OpenLMResearch 2023
23 Image Generation by Autoregressive Models
Llama architecture Our model architecture is largely based on Llama Touvron et al 2023ab
applying prenormalization using RMSNorm Zhang  Sennrich 2019 SwiGLU activation func
tion Shazeer 2020 and rotary positional embeddings Su et al 2024 Specifically we use 2D RoPE
in at each layer of our model following the implementation of Lu et al 2023 Fang et al 2023 We
do not use the technique of AdaLN Peebles  Xie 2023 to keep our structure the same as LLM
Classconditional image generation The class embedding is indexed from a set of learnable
embeddings Peebles  Xie 2023 Esser et al 2021 and is used as the prefilling token embedding
Starting from this token embedding the model generates the sequence of image tokens by nexttoken
prediction way and stops at the location of the predefined maximum length
Textconditional image generation To integrate the text condition into autoregressive models we
use FLANT5 XL Chung et al 2024 as the text encoder the encoded text feature is projected by an
additional MLP Chen et al 2023bc and is used as prefilling token embedding in autoregressive
models We note that this design is not an ultimate design for multimodal foundation models where
a unified vocabulary is established between language and vision Lu et al 2023 Team et al 2023
We leave it for future research
Classifierfree guidance Developed in the diffusion model community classifierfree guidance Ho
 Salimans 2022 is wellknown for its improving visual quality and textimage alignment We adopt
it in our models During training the conditional is randomly dropped and is replaced by a null
unconditional embedding Peebles  Xie 2023 Chen et al 2023b In inference for each token its
logitℓgis formed by ℓgℓusℓcℓu where ℓcis conditional logit ℓuis unconditional logit
andsis scale of the classifierfree guidance
It is worth noting that all design choices discussed so far are largely inspired by previous works for
example image tokenizer is borrowed from Rombach et al 2022 Yu et al 2021 image generation
is from Peebles  Xie 2023 Chen et al 2023b Esser et al 2021 A large portion of these techniques
are well studied in diffusion models but little in AR models Our work adapts these advanced designs
collectively to ARbased visual generation models
24 Scale Up
Our model architecture is almost the same as Llama which allows us to seamlessly adopt optimization
techniques Zhang  Sennrich 2019 Shazeer 2020 Su et al 2024 and training recipes Dao et al
2022 Rasley et al 2020 Shoeybi et al 2019 in LLM community As shown in Table 1 we scale the
model size up to 31B parameters in this work All models are implemented with PyTorch 2 Ansel
et al 2024 and trained on 80GB A100 GPUs For training the models with parameters below 14B
we directly use DDP otherwise we adopt PyTorch FSDP Zhao et al 2023 to optimize GPU memory
usage
25 Serving
Autoregressive models have always suffered from its low inference speed With the rapid development
of large language models advanced inference techniques Kwon et al 2023 Chen et al 2023a
Dettmers 2022 are proposed in the LLM community to optimize the inference speed
4Similar to training inference techniques developed in the LLM community can also be adopted to
optimize our models We verify the effectiveness of vLLM Kwon et al 2023 one of the most
popular LLM serving frameworks on our image generation methods As shown in Table 7 326 
414 speedup is achieved compared to the baseline setting
3 Experiments
31 Image Tokenizer
Training setup The training is on ImageNet Deng et al 2009 train set using the resolution of
256256 and random crop data augmentation The image tokenizer model size is 72M and 70M
when the downsample ratio is 16 and 8 respectively All models are trained with the same settings
constant learning rate of 104 AdamW optimizer with β1 09β2 095 weight decay  005
batch size of 128 and training epochs of 40 For the training losses commitment loss weight is 025
and adversarial loss weight is 05 The adversarial loss is enabled after 20k training iterations
Evaluation metrics We use the popular ImageNet benchmark under the image resolution of 256
256 The image reconstruction quality is measured by rFID reconstructionFID on 256 256
ImageNet 50k validation set The codebook usage is calculated as the percentage of used codes in the
queue of size 65536 over the whole codebook size We also report PSNR and SSIM as the metrics of
reconstruction quality following SDXL Podell et al 2023
dim rFIDPSNRSSIMusage
256 921 1832 0575 029
32 322 1998 0646 209
8 219 2079 0675 970
4 988 1939 0593 820
aCodebook vector dimension Lower vector di
mension from 256 to 8 improves both reconstruction
quality and codebook usage significantlysize rFIDPSNRSSIMusage
4096 302 1999 0643 1000
8192 291 2041 0654 750
16384 219 2079 0675 970
32768 226 2059 0663 850
bCodebook size Larger codebook size from 4096
to 16384 benefits to the overall performance of image
tokenizers
Table 2 Ablation studies on codebook designs in image tokenizers  The evaluations are on
256256 ImageNet 50k validation set The default setting is codebook vector dimension is 8
codebook size is 16384 downsample ratio is 16
ratio img size tokens size rFIDPSNRSSIMusage
256 256 16 16 219 2079 0675 970
16 384 576 24 24 094 2194 0726 970
512 1024 32 32 070 2303 0772 970
256 1024 32 32 059 2445 0813 976
8 384 2304 48 48 037 2563 0852 976
512 4096 64 64 039 2698 0888 976
Table 3 Number of tokens to represent the image The number of tokens depends on downsample
ratio and input image size The reconstructed image is always resized to 256 256 when evaluating
on ImageNet 50k validation set The default setting is codebook vector dimension is 8 codebook
size is 16384
Effect of image codebook designs As shown in Table 2 when the codebook vector dimension is
reduced from 256 to 32 to 8 much better reconstruction quality and codebook usage are consistently
achieved For codebook size a larger size from 4096 to 16384 benefits the overall performance
These observations are consistent with previous works Yu et al 2021 2023b
Effect of number of tokens to represent the image Table 3 studies the effect of image token
number on image reconstruction quality Using the same image tokenizer for example downsample
ratio as 16 representing an image with only 256 tokens 16 16 is not sufficient for good reconstruc
tion quality and increasing the number of tokens to 576 24 24 could largely improve the image
quality from 243 to 099 rFID
5ratio method dim sizeImageNet COCO
rFIDPSNRSSIMrFIDPSNRSSIM
16VQGAN 256 1024 830 1951 0614 1695 1908 0613
VQGAN 256 16384 499 2000 0629 1229 1957 0630
MaskGIT 256 1024 228     
Ours 8 16384 219 2079 0675 811 2042 0678
8VQGANoim4 256 144 2263 0737 658 22289 0744
VQGANoim4 16384 119 2338 0762 589 2308 0771
ViTVQGAN 32 8192 128     
Ours 8 16384 059 2445 0813 419 2420 0822
8SDV AEukn4  074 2568 0820 445 2541 0831
SDXLV AEukn4  068 2604 0834 407 2576 0845
OAIDecoderukn4  081 2443 0786 459 2419 0800
Table 4 Comparisons with other image tokenizers The evaluations are on 256 256 ImageNet
50k validation set and COCO 5k val2017 set All models are trained on ImageNet except oim is
on OpenImage ukn is unknown training data
Comparisons with other image tokenizers We compare with other image tokenizers including
VQGAN Esser et al 2021 MaskGIT Chang et al 2022 ViTVQGAN Yu et al 2021 As shown
in Table 4 our tokenizer outperforms previous image tokenizers We also evaluate our tokenizer on
COCO val2017 Lin et al 2014 of 256 256 image resolution to verify the image reconstruction
quality since COCO images contain more complex scenes The comparison results are consistent
with those in ImageNet validation set This shows our tokenizer is a generalizable image tokenizer
for both objectcentric and scenecentric images
Importantly our tokenizer is competitive to continuous latent space representation such as SD
V AE Rombach et al 2022 SDXL V AE Podell et al 2023 and Consistency Decoder from
OpenAI OpenAI 2023a which are widely used in diffusion models This shows that discrete
representation in the image tokenizer is no longer the bottleneck of the image reconstruction
32 Classconditional Image Generation
Training setup Our benchmark is the popular 256 256 ImageNet All models are trained with
the similar settings base learning rate of 104per 256 batch size AdamW optimizer with β1 09
β2 095weight decay  005 gradient clipping of 10 The dropout is always 01 for input
token embedding attention module and FFN module The class condition embedding dropout for
classifierfree guidance is 01
Precomputing image codes To accelerate the model training we use the image tokenizer to precom
pute image codes before training To achieve the similar effect of random crop data augmentation
we extract image codes of ten crops of the original image During training we randomly select one
copy code from the ten augmentations
Evaluation metrics We use Fréchet inception distance FID Heusel et al 2017 as the main
metric We also report Inception Score IS Salimans et al 2016 sFID Nash et al 2021 and
PrecisionRecall Kynkäänniemi et al 2019 as secondary metrics All evaluations are implemented
using ADMs TensorFlow scripts Dhariwal  Nichol 2021 for fair comparisons
Effect of image tokens Although increasing the image tokens brings better image reconstruction
quality it is not strongly correlated to image generation quality As shown in Table 5 when the model
parameter is smaller than 1B 256 16 16 tokens bring better image generation performance than
576 24 24 This shows the synergistic effect of scaling up model parameters and token numbers
Nevertheless fewer image tokens would limit the image generation performance for example 256
1616 tokens limit the FID at 306 FID while 576 24 24 could further improve the FID to a
lower value
6image token model FID IS Precision Recall
image size 256 256
tokens 256 16 16
rFID 219B 869 12443 078 046
L 421 20000 082 050
XL 339 22708 081 054
XXL 309 25360 082 052
3B 306 27971 084 053
image size 384 384
tokens 576 24 24
rFID 094B 1289 9244 073 048
L 501 16731 078 052
XL 342 20293 079 056
XXL 289 23621 080 056
3B 261 25190 080 056
Table 5 The effect of image tokens on image generation The generated image is always resized to
256256 when evaluating on ImageNet benchmark We compare all models after training 50 epochs
The inference setting is cfg  175 topk  0 all topp  10 temperature  10 for all experiments
a without classifierfree guidance
 b with classifierfree guidance
Figure 2 Scaling model size We show FID of 256 256 ImageNet benchmark over training epochs
Scaling model size brings consistent improvement on FID during the whole training process More
detailed evaluation metrics are in Appendix
a classifierfree guidance
 b topk sampling
Figure 3 The effect of sampling configuration We show FID and Inception Score of 256 256
ImageNet benchmark over different sampling configurations The model is LlamaGenL and the
default setting is cfg  20 topk  0 all topp  10 temperature  10
Effect of model size We train our models across five model sizes B L XL XXL 3B and evaluate
their performance with and without classifierfree guidance Figure 2 illustrates how FID changes as
both the model sizes and the training epochs increase Notable improvements in FID are observed
when scaling the model from LlamaGenB to LlamaGenXXL Further scaling to 3B yields only
marginal improvements A plausible explanation for this phenomenon could be the limitation in
dataset size ImageNet Deng et al 2009 comprises approximately only 1 million images expanding
the dataset or using stronger data augmentation could potentially lead to further improvements
7Type Model Para FID IS Precision Recall
GANBigGAN Brock et al 2018 112M 695 2245 089 038
GigaGAN Kang et al 2023 569M 345 2255 084 061
StyleGanXL Sauer et al 2022 166M 230 2651 078 053
DiffusionADM Dhariwal  Nichol 2021 554M 1094 1010 069 063
CDM Ho et al 2022b  488 1587  
LDM4 Rombach et al 2022 400M 360 2477  
DiTXL2 Peebles  Xie 2023 675M 227 2782 083 057
MaskMaskGIT Chang et al 2022 227M 618 1821 080 051
MaskGITre Chang et al 2022 227M 402 3556  
ARVQGAN Esser et al 2021 227M 1865 804 078 026
VQGAN Esser et al 2021 14B 1578 743  
VQGANre Esser et al 2021 14B 520 2803  
ViTVQGAN Yu et al 2021 17B 417 1751  
ViTVQGANre Yu et al 2021 17B 304 2274  
RQTran Lee et al 2022 38B 755 1340  
RQTranre Lee et al 2022 38B 380 3237  
ARLlamaGenB cfg200 111M 546 19361 083 045
LlamaGenL cfg200 343M 307 25606 083 052
LlamaGenXL cfg175 775M 262 24408 080 057
LlamaGenXXL cfg175 14B 234 25390 080 059
LlamaGen3B cfg165 31B 218 26333 081 058
LlamaGen3B cfg175 31B 232 28010 082 056
LlamaGen3B cfg200 31B 281 31159 084 054
Table 6 Model comparisons on classconditional ImageNet 256 256 benchmark  Metrics include
Fréchet inception distance FID inception score IS precision and recall   or  indicate lower
or higher values are better re means using rejection sampling cfg means using classifierfree
guidance More detailed results are in Appendix
Effect of classifierfree guidance CFG First as shown in Figure 2 using classifierfree guidance
can significantly enhance the visual quality across all model sizes Moreover Figure 3a illustrates
that the model achieves optimal FID at CFG  20 and further increasing CFG would deteriorate FID
which is consistent with previous findings Dhariwal  Nichol 2021 Additionally the increment in
CFG results in a tradeoff between diversity and fidelity as evidenced by increased precision and
decreased recall demonstrated in Table 10
Effect of topk sampling As shown in Figure 3b a small topk value is not beneficial for FID
and IS Increasing topk continuously improves FID but decreases IS which trades off fidelity for
diversity We observe a similar trend when changing the parameter of topp and temperature in
sampling Since FID is our main metric we use maximum value as the default topk value which is
the whole codebook size
Comparisons with other image generation methods In Table 6 we compare with popular image
generation models including GAN Brock et al 2018 Kang et al 2023 Sauer et al 2022 Diffusion
models Dhariwal  Nichol 2021 Ho et al 2022b Rombach et al 2022 Peebles  Xie 2023
and maskedprediction models Chang et al 2022 Our models exhibit competitive performance
in all metrics of FID IS Precision and Recall Notably our 3B model outperforms the popular
diffusion models LDM Rombach et al 2022 DiT Peebles  Xie 2023 This shows that vanilla
autoregressive models can serve as the basis of advanced image generation systems
When comparing with autoregressive models Esser et al 2021 Yu et al 2021 Lee et al 2022 our
model outperforms all previous models at different levels of model parameters This benefits from
better designs of image tokenizers and better scalability of image generation models We hope our
simple and effective implementation will serve as a solid baseline and help facilitate future research
in autoregressive models for image generations
8A furry black bear standing in a rocky weedy area in the wild
A kitchen that is in the process of having the floors done
A cutting board topped with bread meat and vegetables
Stage I
Stage II
a big purple bus parked in a parking spot
Figure 4 Visualization of twostage training of textconditional image generation models
Comparisons of generated images by models after stage I training and stage II training The text
prompts are from COCOPrompts
33 Textconditional Image Generation
Training setup We adopt a twostage training strategy In stage I the model is trained on a
50M subset of LAIONCOCO LAION 2022 with the image resolution 256 256 In Stage II
the model is finetuned on 10M internal high aesthetic quality images with the image resolution
512512 Examples of training data are shown in the Appendix The maximum length of text token
embedding is set to 120 and left padding is used to enable batch processing The text condition
embedding dropout for classifierfree guidance is 01 All models are trained with similar settings
model parameters of 775M base learning rate of 104per 256 batch size AdamW optimizer with
β1 09β2 095 decay  005 gradient clipping of 10
Precomputing image codes and text embeddings We use pretrained FLANT5 XL Chung et al
2024 to precompute text embedding of the image captions For image code we only extract image
codes of the original image center crop in textconditional models training
Finetune image tokenizer Before twostage training for textconditional image generation models
we first finetune the image tokenizer on the joint of 50M LAIONCOCO and 10M internal high
aesthetic quality data
Visualizations In Figure 4 we select text prompts from COCOPrompts Lin et al 2014 to generate
images using models after stage I training and stage II training After stage I training the model
captures the textimage alignment while its ability to represent image details is not clear Stage II
training improves the visual aesthetic quality by a significant margin We explain this improvement
comes from two aspects high aesthetic quality images shift the domain and high image resolution
brings better visual details We notice that further increasing the image resolution to 1024 1024
could bring better visual quality and we leave it for future research
More visualizations on PartiPrompts Yu et al 2022 are in Appendix PartiPrompts have more longer
captions than COCOPrompts and our model demonstrates competitive performance in textimage
alignment for long caption image generation tasks
Limitation Due to the training data and model parameters our textconditional models have
several limitations such as text rendering errors counting errors and common misconceptions
These problems are promising to be mitigated when more training data and computation resources
are available in the future
9model parameters baseline sec vllm sec speedup ratio
B 111M 780 239 326
L 343M 1372 348 380
XL 775M 1976 484 408
XXL 14B 2638 636 414
Table 7 Optimized inference speed by vLLM serving framework The inference time is for
a batch 16 images generating 8 images with classifierfree guidance The image resolution is
384384 for all models
34 Inference Speed
We verify the effectiveness of vLLM Kwon et al 2023 serving framework on our methods Since our
models use the same architecture as Llama which is already supported by vLLM we can seamlessly
adopt its implementation As shown in Table 7 we achieve 326  414 speedup compared to the
baseline setting in the models from 111M to 14B parameters Please note that the baseline setting
has already integrated KVCache technique In the 3B model its head size 100 is not supported by
PagedAttention in vLLM
4 Related Work
Visual generation Generative adversarial network GAN Goodfellow et al 2014 Brock et al
2018 Karras et al 2019 Kang et al 2023 is the first representative visual generation method in deep
learning era To improve the distribution coverage several likelihoodbased methods are proposed
Diffusion models Ho et al 2020 Song  Ermon 2019 Song et al 2020 Dhariwal  Nichol 2021
view image generation as the reverse diffusion process from noises to images Maskedprediction
models Chang et al 2022 2023 Yu et al 2023ab apply language model BERTstyle Devlin
et al 2018 by learning to predict masked tokens Instead autoregressive models Esser et al 2021
Ramesh et al 2021 Yu et al 2022 leverage GPTstyle Radford et al 2018 to predict the next
token in a sequence To ease the modeling and improve the generation quality these methods always
introduce the image tokenization process Kingma  Welling 2013 Van Den Oord et al 2017 to
convert pixel space to semantic space
Multimodal foundation models Recently visionandlanguage models Liu et al 2024 Zhu
et al 2023 Dai et al 2024 Peng et al 2023 Zhang et al 2023 Ma et al 2024 have achieved
versatile visual understanding through visual instruction tuning Liu et al 2024 Zhu et al 2023
However unifying the understanding and generation in multimodal models is still in its early stages
Most existing methods Sun et al 2023ba Dong et al 2024 Ge et al 2023 try to collaborate a
pretrained diffusion model with existing models rather than utilizing a unified nexttoken prediction
paradigm These methods need sophisticated designs to connect the two parts with distinct training
paradigms which makes scaling up challenging Pioneering methods Lu et al 2022b 2023 Bai
et al 2023b Team et al 2023 Team 2024 attempt to incorporate image generation into LLM
using an autoregressive approach and achieve promising results They do not specifically focus on
demonstrating that a plain autoregressive approach can serve as a scalable image generator which is
our main argument in this work
5 Conclusion
In this work we delve into vanilla autoregressive models for scalable image generation By reexamin
ing their image tokenizers image generation models and training data our classconditional models
outperform the popular diffusion models and our textconditional models demonstrate competitive
performance of visual quality and text alignment
10References
Ibrahim M Alabdulmohsin Behnam Neyshabur and Xiaohua Zhai Revisiting neural scaling laws in
language and vision Advances in Neural Information Processing Systems  352230022312 2022
AlphaVLLM Large dit httpsgithubcomAlphaVLLMLLaMA2Accessorytree
mainLargeDiTImageNet  2024
Rohan Anil Andrew M Dai Orhan Firat Melvin Johnson Dmitry Lepikhin Alexandre Passos
Siamak Shakeri Emanuel Taropa Paige Bailey Zhifeng Chen et al Palm 2 technical report arXiv
preprint arXiv230510403  2023
Jason Ansel Edward Yang Horace He Natalia Gimelshein Animesh Jain Michael V oznesensky
Bin Bao Peter Bell David Berard Evgeni Burovski et al Pytorch 2 Faster machine learning
through dynamic python bytecode transformation and graph compilation In Proceedings of the
29th ACM International Conference on Architectural Support for Programming Languages and
Operating Systems Volume 2  pp 929947 2024
Anthropic Claude httpswwwanthropiccomindexintroducingclaude  2023
Jinze Bai Shuai Bai Yunfei Chu Zeyu Cui Kai Dang Xiaodong Deng Yang Fan Wenbin Ge
Yu Han Fei Huang et al Qwen technical report arXiv preprint arXiv230916609  2023a
Yutong Bai Xinyang Geng Karttikeya Mangalam Amir Bar Alan Yuille Trevor Darrell Jitendra
Malik and Alexei A Efros Sequential modeling enables scalable learning for large vision models
arXiv preprint arXiv231200785  2023b
Yoshua Bengio Nicholas Léonard and Aaron Courville Estimating or propagating gradients through
stochastic neurons for conditional computation arXiv preprint arXiv13083432  2013
James Betker Gabriel Goh Li Jing Tim Brooks Jianfeng Wang Linjie Li Long Ouyang Juntang
Zhuang Joyce Lee Yufei Guo et al Improving image generation with better captions Computer
Science httpscdn openai compapersdalle3 pdf  238 2023
Xiao Bi Deli Chen Guanting Chen Shanhuang Chen Damai Dai Chengqi Deng Honghui Ding
Kai Dong Qiushi Du Zhe Fu et al Deepseek llm Scaling opensource language models with
longtermism arXiv preprint arXiv240102954  2024
Andrew Brock Jeff Donahue and Karen Simonyan Large scale gan training for high fidelity natural
image synthesis arXiv preprint arXiv180911096  2018
Tim Brooks Bill Peebles Connor Holmes Will DePue Yufei Guo Li Jing David Schnurr Joe
Taylor Troy Luhman Eric Luhman Clarence Ng Ricky Wang and Aditya Ramesh Video
generation models as world simulators OpenAI  2024 URL httpsopenaicomresearch
videogenerationmodelsasworldsimulators 
Tom Brown Benjamin Mann Nick Ryder Melanie Subbiah Jared D Kaplan Prafulla Dhariwal
Arvind Neelakantan Pranav Shyam Girish Sastry Amanda Askell et al Language models are
fewshot learners Advances in neural information processing systems  3318771901 2020
Huiwen Chang Han Zhang Lu Jiang Ce Liu and William T Freeman Maskgit Masked generative
image transformer In Proceedings of the IEEECVF Conference on Computer Vision and Pattern
Recognition  pp 1131511325 2022
Huiwen Chang Han Zhang Jarred Barber AJ Maschinot Jose Lezama Lu Jiang MingHsuan Yang
Kevin Murphy William T Freeman Michael Rubinstein et al Muse Texttoimage generation
via masked generative transformers arXiv preprint arXiv230100704  2023
Charlie Chen Sebastian Borgeaud Geoffrey Irving JeanBaptiste Lespiau Laurent Sifre and John
Jumper Accelerating large language model decoding with speculative sampling arXiv preprint
arXiv230201318  2023a
Junsong Chen Jincheng Yu Chongjian Ge Lewei Yao Enze Xie Yue Wu Zhongdao Wang James
Kwok Ping Luo Huchuan Lu et al Pixart Fast training of diffusion transformer for photorealistic
texttoimage synthesis arXiv preprint arXiv231000426  2023b
11Shoufa Chen Mengmeng Xu Jiawei Ren Yuren Cong Sen He Yanping Xie Animesh Sinha Ping
Luo Tao Xiang and JuanManuel PerezRua Gentron Delving deep into diffusion transformers
for image and video generation arXiv preprint arXiv231204557  2023c
Aakanksha Chowdhery Sharan Narang Jacob Devlin Maarten Bosma Gaurav Mishra Adam
Roberts Paul Barham Hyung Won Chung Charles Sutton Sebastian Gehrmann et al Palm
Scaling language modeling with pathways Journal of Machine Learning Research  242401113
2023
Hyung Won Chung Le Hou Shayne Longpre Barret Zoph Yi Tay William Fedus Yunxuan Li
Xuezhi Wang Mostafa Dehghani Siddhartha Brahma et al Scaling instructionfinetuned language
models Journal of Machine Learning Research  2570153 2024
Wenliang Dai Junnan Li Dongxu Li Anthony Meng Huat Tiong Junqi Zhao Weisheng Wang
Boyang Li Pascale N Fung and Steven Hoi Instructblip Towards generalpurpose vision
language models with instruction tuning Advances in Neural Information Processing Systems  36
2024
Tri Dao Dan Fu Stefano Ermon Atri Rudra and Christopher Ré Flashattention Fast and memory
efficient exact attention with ioawareness Advances in Neural Information Processing Systems 
351634416359 2022
Jia Deng Wei Dong Richard Socher LiJia Li Kai Li and Li FeiFei Imagenet A largescale
hierarchical image database In 2009 IEEE conference on computer vision and pattern recognition 
pp 248255 Ieee 2009
Tim Dettmers bitsandbytes httpsgithubcomTimDettmersbitsandbytes  2022
Jacob Devlin MingWei Chang Kenton Lee and Kristina Toutanova Bert Pretraining of deep
bidirectional transformers for language understanding arXiv preprint arXiv181004805  2018
Prafulla Dhariwal and Alexander Nichol Diffusion models beat gans on image synthesis Advances
in neural information processing systems  3487808794 2021
Runpei Dong Chunrui Han Yuang Peng Zekun Qi Zheng Ge Jinrong Yang Liang Zhao Jianjian
Sun Hongyu Zhou Haoran Wei Xiangwen Kong Xiangyu Zhang Kaisheng Ma and Li Yi
DreamLLM Synergistic multimodal comprehension and creation In The Twelfth International
Conference on Learning Representations  2024
Patrick Esser Robin Rombach and Bjorn Ommer Taming transformers for highresolution image
synthesis In Proceedings of the IEEECVF conference on computer vision and pattern recognition 
pp 1287312883 2021
Patrick Esser Sumith Kulal Andreas Blattmann Rahim Entezari Jonas Müller Harry Saini Yam
Levi Dominik Lorenz Axel Sauer Frederic Boesel Dustin Podell Tim Dockhorn Zion En
glish Kyle Lacey Alex Goodwin Yannik Marek and Robin Rombach Scaling rectified flow
transformers for highresolution image synthesis 2024
Yuxin Fang Quan Sun Xinggang Wang Tiejun Huang Xinlong Wang and Yue Cao Eva02 A
visual representation for neon genesis arXiv preprint arXiv230311331  2023
Yuying Ge Sijie Zhao Ziyun Zeng Yixiao Ge Chen Li Xintao Wang and Ying Shan Making
llama see and draw with seed tokenizer arXiv preprint arXiv231001218  2023
Ian Goodfellow Jean PougetAbadie Mehdi Mirza Bing Xu David WardeFarley Sherjil Ozair
Aaron Courville and Yoshua Bengio Generative adversarial nets Advances in neural information
processing systems  27 2014
Google Bard httpsbardgooglecom  2023
Tom Henighan Jared Kaplan Mor Katz Mark Chen Christopher Hesse Jacob Jackson Heewoo
Jun Tom B Brown Prafulla Dhariwal Scott Gray et al Scaling laws for autoregressive generative
modeling arXiv preprint arXiv201014701  2020
12Martin Heusel Hubert Ramsauer Thomas Unterthiner Bernhard Nessler and Sepp Hochreiter Gans
trained by a two timescale update rule converge to a local nash equilibrium Advances in neural
information processing systems  30 2017
Jonathan Ho and Tim Salimans Classifierfree diffusion guidance arXiv preprint arXiv220712598 
2022
Jonathan Ho Ajay Jain and Pieter Abbeel Denoising diffusion probabilistic models Advances in
neural information processing systems  3368406851 2020
Jonathan Ho Chitwan Saharia William Chan David J Fleet Mohammad Norouzi and Tim Salimans
Cascaded diffusion models for high fidelity image generation The Journal of Machine Learning
Research  23122492281 2022a
Jonathan Ho Chitwan Saharia William Chan David J Fleet Mohammad Norouzi and Tim Salimans
Cascaded diffusion models for high fidelity image generation The Journal of Machine Learning
Research  23122492281 2022b
Jordan Hoffmann Sebastian Borgeaud Arthur Mensch Elena Buchatskaya Trevor Cai Eliza
Rutherford Diego de Las Casas Lisa Anne Hendricks Johannes Welbl Aidan Clark et al
Training computeoptimal large language models arXiv preprint arXiv220315556  2022
Phillip Isola JunYan Zhu Tinghui Zhou and Alexei A Efros Imagetoimage translation with
conditional adversarial networks In Proceedings of the IEEE conference on computer vision and
pattern recognition  pp 11251134 2017
Minguk Kang JunYan Zhu Richard Zhang Jaesik Park Eli Shechtman Sylvain Paris and Taesung
Park Scaling up gans for texttoimage synthesis In Proceedings of the IEEECVF Conference on
Computer Vision and Pattern Recognition  pp 1012410134 2023
Jared Kaplan Sam McCandlish Tom Henighan Tom B Brown Benjamin Chess Rewon Child Scott
Gray Alec Radford Jeffrey Wu and Dario Amodei Scaling laws for neural language models
arXiv preprint arXiv200108361  2020
Tero Karras Samuli Laine and Timo Aila A stylebased generator architecture for generative
adversarial networks In Proceedings of the IEEECVF conference on computer vision and pattern
recognition  pp 44014410 2019
Diederik P Kingma and Max Welling Autoencoding variational bayes arXiv preprint
arXiv13126114  2013
Woosuk Kwon Zhuohan Li Siyuan Zhuang Ying Sheng Lianmin Zheng Cody Hao Yu Joseph E
Gonzalez Hao Zhang and Ion Stoica Efficient memory management for large language model
serving with pagedattention In Proceedings of the ACM SIGOPS 29th Symposium on Operating
Systems Principles  2023
Tuomas Kynkäänniemi Tero Karras Samuli Laine Jaakko Lehtinen and Timo Aila Improved
precision and recall metric for assessing generative models Advances in neural information
processing systems  32 2019
LAION Laioncoco 600m httpslaionaibloglaioncoco  2022
Doyup Lee Chiheon Kim Saehoon Kim Minsu Cho and WookShin Han Autoregressive image
generation using residual quantization In Proceedings of the IEEECVF Conference on Computer
Vision and Pattern Recognition  pp 1152311532 2022
Daiqing Li Aleks Kamko Ehsan Akhgari Ali Sabet Linmiao Xu and Suhail Doshi Playground v2
5 Three insights towards enhancing aesthetic quality in texttoimage generation arXiv preprint
arXiv240217245  2024
Junnan Li Dongxu Li Caiming Xiong and Steven Hoi Blip Bootstrapping languageimage pre
training for unified visionlanguage understanding and generation In International conference on
machine learning  pp 1288812900 PMLR 2022
13TsungYi Lin Michael Maire Serge Belongie James Hays Pietro Perona Deva Ramanan Piotr
Dollár and C Lawrence Zitnick Microsoft coco Common objects in context In Computer Vision
ECCV 2014 13th European Conference Zurich Switzerland September 612 2014 Proceedings
Part V 13  pp 740755 Springer 2014
Haotian Liu Chunyuan Li Qingyang Wu and Yong Jae Lee Visual instruction tuning Advances in
neural information processing systems  36 2024
Cheng Lu Yuhao Zhou Fan Bao Jianfei Chen Chongxuan Li and Jun Zhu Dpmsolver A fast
ode solver for diffusion probabilistic model sampling in around 10 steps Advances in Neural
Information Processing Systems  3557755787 2022a
Jiasen Lu Christopher Clark Rowan Zellers Roozbeh Mottaghi and Aniruddha Kembhavi Unified
io A unified model for vision language and multimodal tasks arXiv preprint arXiv220608916 
2022b
Jiasen Lu Christopher Clark Sangho Lee Zichen Zhang Savya Khosla Ryan Marten Derek Hoiem
and Aniruddha Kembhavi Unifiedio 2 Scaling autoregressive multimodal models with vision
language audio and action arXiv preprint arXiv231217172  2023
Chuofan Ma Yi Jiang Jiannan Wu Zehuan Yuan and Xiaojuan Qi Groma Localized visual
tokenization for grounding multimodal large language models arXiv preprint arXiv240413013 
2024
Charlie Nash Jacob Menick Sander Dieleman and Peter W Battaglia Generating images with
sparse representations arXiv preprint arXiv210303841  2021
Alex Nichol Prafulla Dhariwal Aditya Ramesh Pranav Shyam Pamela Mishkin Bob McGrew
Ilya Sutskever and Mark Chen Glide Towards photorealistic image generation and editing with
textguided diffusion models arXiv preprint arXiv211210741  2021
OpenAI Chatgpt httpsopenaicomblogchatgpt  2022
OpenAI Consistency decoder httpsgithubcomopenaiconsistencydecoder  2023a
OpenAI Gpt4 technical report arXiv preprint arXiv230308774  2023b
OpenLMResearch Openllama 3b httpshuggingfacecoopenlmresearchopen_
llama_3b  2023
Long Ouyang Jeffrey Wu Xu Jiang Diogo Almeida Carroll Wainwright Pamela Mishkin Chong
Zhang Sandhini Agarwal Katarina Slama Alex Ray et al Training language models to follow
instructions with human feedback Advances in Neural Information Processing Systems  35
2773027744 2022
William Peebles and Saining Xie Scalable diffusion models with transformers In Proceedings of
the IEEECVF International Conference on Computer Vision  pp 41954205 2023
Zhiliang Peng Wenhui Wang Li Dong Yaru Hao Shaohan Huang Shuming Ma and Furu
Wei Kosmos2 Grounding multimodal large language models to the world arXiv preprint
arXiv230614824  2023
Dustin Podell Zion English Kyle Lacey Andreas Blattmann Tim Dockhorn Jonas Müller Joe
Penna and Robin Rombach Sdxl Improving latent diffusion models for highresolution image
synthesis arXiv preprint arXiv230701952  2023
Alec Radford Karthik Narasimhan Tim Salimans Ilya Sutskever et al Improving language
understanding by generative pretraining article  2018
Alec Radford Jeffrey Wu Rewon Child David Luan Dario Amodei Ilya Sutskever et al Language
models are unsupervised multitask learners OpenAI blog  189 2019
Colin Raffel Noam Shazeer Adam Roberts Katherine Lee Sharan Narang Michael Matena Yanqi
Zhou Wei Li and Peter J Liu Exploring the limits of transfer learning with a unified texttotext
transformer The Journal of Machine Learning Research  21154855551 2020
14Aditya Ramesh Mikhail Pavlov Gabriel Goh Scott Gray Chelsea V oss Alec Radford Mark Chen
and Ilya Sutskever Zeroshot texttoimage generation In International Conference on Machine
Learning  pp 88218831 PMLR 2021
Aditya Ramesh Prafulla Dhariwal Alex Nichol Casey Chu and Mark Chen Hierarchical text
conditional image generation with clip latents arXiv preprint arXiv220406125  123 2022
Jeff Rasley Samyam Rajbhandari Olatunji Ruwase and Yuxiong He Deepspeed System optimiza
tions enable training deep learning models with over 100 billion parameters In Proceedings of
the 26th ACM SIGKDD International Conference on Knowledge Discovery  Data Mining  pp
35053506 2020
Ali Razavi Aaron Van den Oord and Oriol Vinyals Generating diverse highfidelity images with
vqvae2 Advances in neural information processing systems  32 2019
Robin Rombach Andreas Blattmann Dominik Lorenz Patrick Esser and Björn Ommer High
resolution image synthesis with latent diffusion models In Proceedings of the IEEECVF confer
ence on computer vision and pattern recognition  pp 1068410695 2022
Chitwan Saharia William Chan Saurabh Saxena Lala Li Jay Whang Emily L Denton Kamyar
Ghasemipour Raphael Gontijo Lopes Burcu Karagol Ayan Tim Salimans et al Photorealistic
texttoimage diffusion models with deep language understanding Advances in Neural Information
Processing Systems  353647936494 2022
Tim Salimans Ian Goodfellow Wojciech Zaremba Vicki Cheung Alec Radford and Xi Chen
Improved techniques for training gans Advances in neural information processing systems  29
2016
Axel Sauer Katja Schwarz and Andreas Geiger Styleganxl Scaling stylegan to large diverse
datasets In ACM SIGGRAPH 2022 conference proceedings  pp 110 2022
Noam Shazeer Glu variants improve transformer arXiv preprint arXiv200205202  2020
Mohammad Shoeybi Mostofa Patwary Raul Puri Patrick LeGresley Jared Casper and Bryan Catan
zaro Megatronlm Training multibillion parameter language models using model parallelism
arXiv preprint arXiv190908053  2019
Jiaming Song Chenlin Meng and Stefano Ermon Denoising diffusion implicit models arXiv
preprint arXiv201002502  2020
Yang Song and Stefano Ermon Generative modeling by estimating gradients of the data distribution
Advances in neural information processing systems  32 2019
Jianlin Su Murtadha Ahmed Yu Lu Shengfeng Pan Wen Bo and Yunfeng Liu Roformer Enhanced
transformer with rotary position embedding Neurocomputing  568127063 2024
Quan Sun Qiying Yu Yufeng Cui Fan Zhang Xiaosong Zhang Yueze Wang Hongcheng Gao
Jingjing Liu Tiejun Huang and Xinlong Wang Generative pretraining in multimodality arXiv
preprint arXiv230705222  2023a
Quan Sun Qiying Yu Yufeng Cui Fan Zhang Xiaosong Zhang Yueze Wang Hongcheng Gao
Jingjing Liu Tiejun Huang and Xinlong Wang Generative pretraining in multimodality arXiv
preprint arXiv230705222  2023b
Chameleon Team Chameleon Mixedmodal earlyfusion foundation models arXiv preprint
arXiv240509818  2024
Gemini Team Rohan Anil Sebastian Borgeaud Yonghui Wu JeanBaptiste Alayrac Jiahui Yu Radu
Soricut Johan Schalkwyk Andrew M Dai Anja Hauth et al Gemini a family of highly capable
multimodal models arXiv preprint arXiv231211805  2023
InternLM Team Internlm A multilingual language model with progressively enhanced capabilities
2023
15Keyu Tian Yi Jiang Zehuan Yuan Bingyue Peng and Liwei Wang Visual autoregressive modeling
Scalable image generation via nextscale prediction arXiv preprint arXiv240402905  2024
Hugo Touvron Thibaut Lavril Gautier Izacard Xavier Martinet MarieAnne Lachaux Timothée
Lacroix Baptiste Rozière Naman Goyal Eric Hambro Faisal Azhar et al Llama Open and
efficient foundation language models arXiv preprint arXiv230213971  2023a
Hugo Touvron Louis Martin Kevin Stone Peter Albert Amjad Almahairi Yasmine Babaei Nikolay
Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale et al Llama 2 Open foundation
and finetuned chat models arXiv preprint arXiv230709288  2023b
Aaron Van Den Oord Oriol Vinyals et al Neural discrete representation learning Advances in
neural information processing systems  30 2017
Ashish Vaswani Noam Shazeer Niki Parmar Jakob Uszkoreit Llion Jones Aidan N Gomez Łukasz
Kaiser and Illia Polosukhin Attention is all you need Advances in neural information processing
systems  30 2017
Jason Wei Yi Tay Rishi Bommasani Colin Raffel Barret Zoph Sebastian Borgeaud Dani Yogatama
Maarten Bosma Denny Zhou Donald Metzler et al Emergent abilities of large language models
arXiv preprint arXiv220607682  2022
BigScience Workshop Teven Le Scao Angela Fan Christopher Akiki Ellie Pavlick Suzana Ili c
Daniel Hesslow Roman Castagné Alexandra Sasha Luccioni François Yvon et al Bloom A
176bparameter openaccess multilingual language model arXiv preprint arXiv221105100  2022
Zeyue Xue Guanglu Song Qiushan Guo Boxiao Liu Zhuofan Zong Yu Liu and Ping Luo Raphael
Texttoimage generation via large mixture of diffusion paths arXiv preprint arXiv230518295 
2023
Aiyuan Yang Bin Xiao Bingning Wang Borong Zhang Ce Bian Chao Yin Chenxu Lv Da Pan
Dian Wang Dong Yan et al Baichuan 2 Open largescale language models arXiv preprint
arXiv230910305  2023
Jiahui Yu Xin Li Jing Yu Koh Han Zhang Ruoming Pang James Qin Alexander Ku Yuanzhong
Xu Jason Baldridge and Yonghui Wu Vectorquantized image modeling with improved vqgan
arXiv preprint arXiv211004627  2021
Jiahui Yu Yuanzhong Xu Jing Yu Koh Thang Luong Gunjan Baid Zirui Wang Vijay Vasudevan
Alexander Ku Yinfei Yang Burcu Karagol Ayan et al Scaling autoregressive models for content
rich texttoimage generation arXiv preprint arXiv220610789  235 2022
Lijun Yu Yong Cheng Kihyuk Sohn José Lezama Han Zhang Huiwen Chang Alexander G
Hauptmann MingHsuan Yang Yuan Hao Irfan Essa et al Magvit Masked generative video
transformer In Proceedings of the IEEECVF Conference on Computer Vision and Pattern
Recognition  pp 1045910469 2023a
Lijun Yu José Lezama Nitesh B Gundavarapu Luca Versari Kihyuk Sohn David Minnen Yong
Cheng Agrim Gupta Xiuye Gu Alexander G Hauptmann et al Language model beats diffusion
tokenizer is key to visual generation arXiv preprint arXiv231005737  2023b
Biao Zhang and Rico Sennrich Root mean square layer normalization Advances in Neural
Information Processing Systems  32 2019
Richard Zhang Phillip Isola Alexei A Efros Eli Shechtman and Oliver Wang The unreasonable
effectiveness of deep features as a perceptual metric In Proceedings of the IEEE conference on
computer vision and pattern recognition  pp 586595 2018
Shilong Zhang Peize Sun Shoufa Chen Min Xiao Wenqi Shao Wenwei Zhang Kai Chen and
Ping Luo Gpt4roi Instruction tuning large language model on regionofinterest arXiv preprint
arXiv230703601  2023
Susan Zhang Stephen Roller Naman Goyal Mikel Artetxe Moya Chen Shuohui Chen Christopher
Dewan Mona Diab Xian Li Xi Victoria Lin et al Opt Open pretrained transformer language
models arXiv preprint arXiv220501068  2022
16Yanli Zhao Andrew Gu Rohan Varma Liang Luo ChienChin Huang Min Xu Less Wright Hamid
Shojanazeri Myle Ott Sam Shleifer et al Pytorch fsdp experiences on scaling fully sharded data
parallel arXiv preprint arXiv230411277  2023
Deyao Zhu Jun Chen Xiaoqian Shen Xiang Li and Mohamed Elhoseiny Minigpt4 Enhancing
visionlanguage understanding with advanced large language models 2023
17An abstract painting on a pillow with pink yellow and red flowers The dining room is decorated with elegant decorThe new tab in Powerpointis highlightedThe card is attached to an external PCI
A large building with columns and a clock towerTwo lions are laying under a tree in the wildAn assortment of party decorations with owls and otheritemsan image of the coordinate of two circlesFigure 5 Examples of stage I training data 50M subset of LAIONCOCO The short caption is
its original caption generated from BLIP Li et al 2022
A large cartoonlike painting of a smiling Mickey Mouse Mickey is wearing a red shirt and is holding a pair of white gloves The painting is displayed on a wall and the Mickey Mouse character appears to be the main focus of the artwork There are no geometric patterns or overlays in the imageAcozy bedroom with a large bed situated in the center of the room The bed is covered with a white comforter and a fur blanket adding warmth and comfort to the space A fireplace can be seen in the room providing additional warmth and ambiance The bedroom also features a large window allowing natural light to fill thee room and offering a beautiful view of the snowy landscape outside There are several candles placed around the room adding a touch of elegance and creating a serene atmosphereA beautiful blue flower with a white center surrounded by a few other blue flowers The blue flowers are adorned with water droplets giving them a fresh and vibrant appearance The scene appears to be set in a forest or a natural environment with the flowers standing out against the backdrop The combination of the blue flowers and the water droplets creates a visually appealing and serene atmosphere A cartoon drawing of a smiling lion standing on a rock in a grassy field The lion has a playful expression and its mouth is open possibly indicating that it is laughing or making a sound The lions mane is prominent adding to its majestic appearance The scene captures the lions joyful and carefree demeanor in a natural environment
Figure 6 Examples of stage II training data 10M internal high aesthetic quality images The
long caption is generated from LLaV A
A Examples of ImageText Pair Data
Training stage I 50M subset of LAIONCOCO LAION 2022 The original dataset has 600M
imagetext pair We filter these images by valid image URL aesthetic score watermark score CLIP
imagetext similarity score and image size The remaining images are about 50M Some examples
are shown in Figure 5
Training stage II 10M internal high aesthetic quality images Each image is provided a long
caption by LLaV A Liu et al 2024 using the prompt of Describe this image in as much detail as
possible Some examples are shown in Figure 6 We notice that the first sentence of the long caption
is always a summary description of its image so we use it as the short caption to augment the training
of textconditional image generation models
B More Results on ImageNet Benchmark
We provide more detailed performance on ImageNet 256 256 benchmark in Table 8 9 10 The
generated image is always resized to 256 256 when evaluating
18Model Para epochs cfg FID IS sFIDPreRec
B 111M 50 no 31352 39576 8749 0568 0614
B 111M 50 150 11984 95400 7335 0738 0517
B 111M 50 175 8690 124435 7165 0789 0469
B 111M 50 200 7390 153974 7250 0832 0417
B 111M 50 225 7220 178281 7489 0861 0384
B 111M 50 250 7824 197511 7857 0882 0349
B 111M 300 no 26262 48072 9216 0593 0616
B 111M 300 150 8738 120602 7668 0751 0535
B 111M 300 175 6116 159123 7364 0799 0492
B 111M 300 200 5464 193613 7503 0839 0457
B 111M 300 225 5641 220720 7668 0863 0411
B 111M 300 250 6390 246565 8041 0883 0382
L 343M 50 no 21812 59179 8772 0616 0640
L 343M 50 150 5781 153792 7096 0774 0555
L 343M 50 175 4218 200001 7015 0824 0509
L 343M 50 200 4317 242112 7077 0859 0468
L 343M 300 no 13452 82289 8324 0656 0638
L 343M 300 150 4079 198504 8157 0800 0552
L 343M 300 175 3805 248280 8487 0833 0515
L 343M 300 200 4407 288170 8871 0858 0481
XL 775M 50 no 19417 66196 8911 0610 0665
XL 775M 50 150 4808 172170 7298 0767 0585
XL 775M 50 175 3391 227081 7022 0812 0542
XL 775M 50 200 3642 268779 7244 0846 0502
XXL 14B 50 no 16822 74888 9285 0628 0660
XXL 14B 50 150 3844 195527 7496 0781 0577
XXL 14B 50 175 3094 253609 7305 0825 0529
XXL 14B 50 200 3644 296521 7410 0857 0511
3B 31B 50 no 13581 87902 7781 0648 0666
3B 31B 50 150 3050 222330 6489 0801 0575
3B 31B 50 175 3063 279716 6686 0843 0538
3B 31B 50 200 4212 325150 7027 0869 0492
validation data 1684 231811 3692 0752 0671
Table 8 Detailed performance on classconditional ImageNet 256 256 benchmark  The gener
ated image is 256 256 All experiments use the sampling configuration of topk  0 all topp 
10 temperature  10



19Model Para epochs cfg FID sFID IS PreRec
B 111M 50 no 41025 30788 9825 0523 0605
B 111M 50 150 18276 69337 7557 0677 0534
B 111M 50 175 12899 92447 6900 0738 0487
B 111M 50 200 10029 116372 6562 0787 0443
B 111M 50 225 8674 136621 6428 0818 0413
B 111M 50 250 8309 154719 6599 0843 0376
B 111M 50 275 8391 168629 6708 0860 0345
B 111M 100 no 33442 37528 9872 0536 0609
B 111M 100 150 15629 77247 7632 0698 0529
B 111M 100 175 10676 104581 6960 0754 0490
B 111M 100 200 8298 128941 6671 0795 0452
B 111M 100 225 7256 152502 6510 0827 0416
B 111M 100 250 7151 172677 6517 0850 0390
B 111M 200 no 32105 37993 10144 0559 0618
B 111M 200 150 12206 90783 7531 0716 0534
B 111M 200 175 8535 118399 7024 0766 0503
B 111M 200 200 6951 146077 6784 0808 0459
B 111M 200 225 6542 167825 6695 0833 0428
B 111M 200 250 6632 188157 6811 0853 0393
B 111M 300 no 32196 39877 11838 0570 0611
B 111M 300 150 12012 95553 8897 0725 0528
B 111M 300 175 8012 127957 8088 0778 0498
B 111M 300 200 6437 157173 7487 0814 0456
B 111M 300 225 6092 182538 7244 0845 0416
B 111M 300 250 6249 203886 6981 0861 0389
B 111M 300 275 6803 220708 6928 0876 0357
L 343M 50 no 25889 48053 9612 0570 0655
L 343M 50 150 7905 123830 7381 0732 0569
L 343M 50 175 5018 167310 6786 0784 0524
L 343M 50 200 4240 206739 6483 0825 0491
L 343M 50 225 4589 238890 6325 0850 0451
L 343M 100 no 24654 53166 10497 0594 0645
L 343M 100 150 6934 138852 7910 0748 0569
L 343M 100 175 4321 188536 7068 0802 0528
L 343M 100 200 3705 228305 6701 0839 0490
L 343M 100 225 4054 263864 6407 0858 0460
L 343M 200 no 19742 61715 7286 0601 0667
L 343M 200 150 4929 158546 6066 0759 0588
L 343M 200 175 3249 209372 5927 0805 0544
L 343M 200 200 3220 250697 5879 0841 0512
L 343M 200 225 3939 288217 6076 0865 0479
L 343M 300 no 19070 64349 8668 0607 0670
L 343M 300 150 4743 165381 6740 0758 0596
L 343M 300 175 3151 214152 6310 0803 0552
L 343M 300 200 3075 256067 6088 0832 0522
L 343M 300 225 3620 291695 6122 0854 0493
validation data 1684 231811 3692 0752 0671
Table 9 Detailed performance on classconditional ImageNet 256 256 benchmark  The gener
ated image is 384 384 and is resized to 256 256 when evaluating on ImageNet All experiments
use the sampling configuration of topk  0 all topp  10 temperature  10
20Model Para epochs cfg FID sFID IS PreRec
XL 775M 50 no 19820 61363 8067 0601 0669
XL 775M 50 150 5231 154249 6284 0746 0592
XL 775M 50 175 3420 202939 6090 0796 0560
XL 775M 50 200 3238 245680 6023 0826 0529
XL 775M 100 no 18037 69879 8388 0616 0665
XL 775M 100 150 4563 173749 6591 0759 0588
XL 775M 100 175 3089 225856 6157 0804 0551
XL 775M 100 200 3105 267608 6001 0833 0531
XL 775M 200 no 14772 80826 6840 0620 0681
XL 775M 200 150 3388 193477 5753 0771 0603
XL 775M 200 175 2617 245465 5652 0811 0566
XL 775M 200 200 2859 285900 5758 0840 0527
XL 775M 300 no 15549 79157 7049 0616 0689
XL 775M 300 150 3479 194448 5816 0763 0606
XL 775M 300 175 2629 244085 5594 0807 0579
XL 775M 300 200 2785 286875 5567 0836 0542
XXL 14B 50 no 17195 74123 8689 0605 0681
XXL 14B 50 150 4363 178228 6818 0758 0600
XXL 14B 50 175 2893 236210 6263 0805 0564
XXL 14B 50 200 3049 285390 6053 0842 0522
XXL 14B 200 no 13997 86776 8178 0637 0684
XXL 14B 200 150 3137 207870 6060 0774 0605
XXL 14B 200 175 2331 262995 5714 0816 0579
XXL 14B 200 200 2678 304631 5587 0840 0545
XXL 14B 300 no 14648 86328 8687 0628 0681
XXL 14B 300 150 3295 202586 6476 0770 0626
XXL 14B 300 175 2340 253906 5977 0809 0596
XXL 14B 300 200 2523 295374 5736 0836 0559
3B 31B 50 no 16431 72622 7217 0611 0677
3B 31B 50 150 3472 191979 5955 0768 0600
3B 31B 50 175 2611 251903 6167 0807 0568
3B 31B 50 200 3222 300887 5764 0847 0523
3B 31B 200 no 9949 108083 7088 0667 0672
3B 31B 200 150 2400 237683 5548 0794 0600
3B 31B 200 165 2264 268180 5426 0817 0581
3B 31B 200 175 2381 286091 5390 0828 0569
3B 31B 200 200 3011 321563 5514 0851 0538
3B 31B 300 no 9380 112877 8242 0685 0668
3B 31B 300 150 2388 233246 6145 0798 0601
3B 31B 300 160 2216 251338 6002 0811 0584
3B 31B 300 165 2189 263334 5965 0819 0581
3B 31B 300 175 2329 280104 5818 0828 0566
3B 31B 300 180 2370 287452 5825 0834 0570
3B 31B 300 200 2816 311597 5845 0848 0544
validation data 1684 231811 3692 0752 0671
Table 10 Detailed performance on classconditional ImageNet 256 256 benchmark  The gener
ated image is 384 384 and is resized to 256 256 when evaluating on ImageNet All experiments
use the sampling configuration of topk  0 all topp  10 temperature  10
21Figure 7 384 384 LlamaGen3B samples
Classifierfree guidance scale  40
Class label  golden retriever 207
Figure 8 384 384 LlamaGen3B samples
Classifierfree guidance scale  40
Class label  husky  250
22Figure 9 384 384 LlamaGen3B samples
Classifierfree guidance scale  40
Class label  cliff dropoff 972
Figure 10 384 384 LlamaGen3B samples
Classifierfree guidance scale  40
Class label  coral reef 973
23Figure 11 384 384 LlamaGen3B samples
Classifierfree guidance scale  40
Class label  space shuttle 812
Figure 12 384 384 LlamaGen3B samples
Classifierfree guidance scale  40
Class label  sport car  817
24A plate on a wooden table full of bread A cat resting on an open laptop computer
Two birds that are sitting in a marsh areaA large green truck on a city street
A brown cow laying on top of a lush green fieldA box of donuts of different colors and varieties
A large body of water sitting below a mountain rangeA few bags laying around in a living room
A pickup truck driving through a desert environment
A bathroom with a blue shower curtain and blue wallsFigure 13 Stage I Textconditional 256 256
image generation on COCOPrompts
a tigeran illustration of a teapot
a view of the Kremlin with snow fallinga corgi wearing a red bowtie and a purple party hat
the Eiffel Tower in winter
A bare kitchen has wood cabinets and white appliances
A photo of an Athenian vase with a painting of pandas playing soccer in the style of Egyptian hieroglyphicsGolden Gate bridge on the surface of Mars
A photograph of a portrait of a statue of a pharaoh wearing steampunk glasses white tshirt and leather jacketA closeup highcontrast photo of Sydney Opera House sitting next to Eiffel tower under a blue night sky of roiling energy exploding yellow stars and radiating swirls of blue
Figure 14 Stage I Textconditional 256 256
image generation on PartiPrompts
25an ostricha corgis head depicted as an explosion of a nebula
a sunken ship at the bottom of the ocean
A blue Porsche 356 parked in front of a yellow brick walla gorilla climbing up the side of the Great Pyramida racoon detective using a microscope while riding in a train
a photograph of a squirrel holding an arrow above its head and holding a longbow in its left hand
a photograph of the mona lisadrinking coffee as she has her breakfast her plate has an omeletteand croissantpanda mad scientist mixing sparkling chemicals highcontrast painting
a baby penguina chimpanzee
Dogs sitting around a poker tableA high resolution photo of a chicken working out in a gym
A photo of a fourleaf clover made of water
Dogs sitting around a poker table with beer bottles and chips Their hands are holding cards
Siberian husky playing the pianoa plant at the bottom of a shallow stream
A photo of an astronaut riding a horse in the forest There is a river in front of them with water lilies 
red apples on a tree with green leaves
A portrait of a metal statue of a pharaoh wearing steampunk glasses and a leather jacket over a white tshirt that has a drawing of a space shuttle on itFigure 15 Stage II Textconditional 512 512 image generation on PartiPrompts
26
  HighResolution Image Synthesis with Latent Diffusion Models
Robin Rombach1 Andreas Blattmann1Dominik Lorenz1Patrick Esser
 Bjorn Ommer1
1Ludwig Maximilian University of Munich  IWR Heidelberg University Germany
 Runway ML
httpsgithubcomCompVislatentdiffusion
Abstract
By decomposing the image formation process into a se
quential application of denoising autoencoders diffusion
models DMs achieve stateoftheart synthesis results on
image data and beyond Additionally their formulation al
lows for a guiding mechanism to control the image gen
eration process without retraining However since these
models typically operate directly in pixel space optimiza
tion of powerful DMs often consumes hundreds of GPU
days and inference is expensive due to sequential evalu
ations To enable DM training on limited computational
resources while retaining their quality and ﬂexibility we
apply them in the latent space of powerful pretrained au
toencoders In contrast to previous work training diffusion
models on such a representation allows for the ﬁrst time
to reach a nearoptimal point between complexity reduc
tion and detail preservation greatly boosting visual ﬁdelity
By introducing crossattention layers into the model archi
tecture we turn diffusion models into powerful and ﬂexi
ble generators for general conditioning inputs such as text
or bounding boxes and highresolution synthesis becomes
possible in a convolutional manner Our latent diffusion
models LDMs achieve new stateoftheart scores for im
age inpainting and classconditional image synthesis and
highly competitive performance on various tasks includ
ing texttoimage synthesis unconditional image generation
and superresolution while signiﬁcantly reducing computa
tional requirements compared to pixelbased DMs
1 Introduction
Image synthesis is one of the computer vision ﬁelds with
the most spectacular recent development but also among
those with the greatest computational demands Espe
cially highresolution synthesis of complex natural scenes
is presently dominated by scaling up likelihoodbased mod
els potentially containing billions of parameters in autore
gressive AR transformers 6667 In contrast the promis
ing results of GANs 3 27 40 have been revealed to be
mostly conﬁned to data with comparably limited variability
as their adversarial learning procedure does not easily scale
to modeling complex multimodal distributions Recently
diffusion models 82 which are built from a hierarchy of
denoising autoencoders have shown to achieve impressive
The ﬁrst two authors contributed equally to this workInputours  f 4
PSNR 274RFID 058DALLE  f 8
PSNR 228RFID 3201VQGAN  f 16 
PSNR 199RFID 498
Figure 1 Boosting the upper bound on achievable quality with
less agressive downsampling Since diffusion models offer excel
lent inductive biases for spatial data we do not need the heavy spa
tial downsampling of related generative models in latent space but
can still greatly reduce the dimensionality of the data via suitable
autoencoding models see Sec 3 Images are from the DIV2K 1
validation set evaluated at 5122px We denote the spatial down
sampling factor by f Reconstruction FIDs 29 and PSNR are
calculated on ImageNetval 12 see also Tab 8
results in image synthesis 3085 and beyond 7454857
and deﬁne the stateoftheart in classconditional image
synthesis 1531 and superresolution 72 Moreover even
unconditional DMs can readily be applied to tasks such
as inpainting and colorization 85 or strokebased syn
thesis 53 in contrast to other types of generative mod
els 194669 Being likelihoodbased models they do not
exhibit modecollapse and training instabilities as GANs
and by heavily exploiting parameter sharing they can
model highly complex distributions of natural images with
out involving billions of parameters as in AR models 67
Democratizing HighResolution Image Synthesis DMs
belong to the class of likelihoodbased models whose
modecovering behavior makes them prone to spend ex
cessive amounts of capacity and thus compute resources
on modeling imperceptible details of the data 16 73 Al
though the reweighted variational objective 30 aims to ad
dress this by undersampling the initial denoising steps DMs
are still computationally demanding since training and
evaluating such a model requires repeated function evalu
ations and gradient computations in the highdimensional
space of RGB images As an example training the most
powerful DMs often takes hundreds of GPU days  eg 150 
1000 V100 days in 15 and repeated evaluations on a noisy
version of the input space render also inference expensive
1arXiv211210752v2  csCV  13 Apr 2022so that producing 50k samples takes approximately 5 days
15 on a single A100 GPU This has two consequences for
the research community and users in general Firstly train
ing such a model requires massive computational resources
only available to a small fraction of the ﬁeld and leaves a
huge carbon footprint 65 86 Secondly evaluating an al
ready trained model is also expensive in time and memory
since the same model architecture must run sequentially for
a large number of steps  eg 25  1000 steps in 15
To increase the accessibility of this powerful model class
and at the same time reduce its signiﬁcant resource con
sumption a method is needed that reduces the computa
tional complexity for both training and sampling Reducing
the computational demands of DMs without impairing their
performance is therefore key to enhance their accessibility
Departure to Latent Space Our approach starts with
the analysis of already trained diffusion models in pixel
space Fig 2 shows the ratedistortion tradeoff of a trained
model As with any likelihoodbased model learning can
be roughly divided into two stages First is a perceptual
compression stage which removes highfrequency details
but still learns little semantic variation In the second stage
the actual generative model learns the semantic and concep
tual composition of the data  semantic compression  We
thus aim to ﬁrst ﬁnd a perceptually equivalent but compu
tationally more suitable space  in which we will train diffu
sion models for highresolution image synthesis
Following common practice 11 23 66 67 96 we sep
arate training into two distinct phases First we train
an autoencoder which provides a lowerdimensional and
thereby efﬁcient representational space which is perceptu
ally equivalent to the data space Importantly and in con
trast to previous work 2366 we do not need to rely on ex
cessive spatial compression as we train DMs in the learned
latent space which exhibits better scaling properties with
respect to the spatial dimensionality The reduced complex
ity also provides efﬁcient image generation from the latent
space with a single network pass We dub the resulting
model class Latent Diffusion Models LDMs
A notable advantage of this approach is that we need to
train the universal autoencoding stage only once and can
therefore reuse it for multiple DM trainings or to explore
possibly completely different tasks 81 This enables efﬁ
cient exploration of a large number of diffusion models for
various imagetoimage and texttoimage tasks For the lat
ter we design an architecture that connects transformers to
the DMs UNet backbone 71 and enables arbitrary types
of tokenbased conditioning mechanisms see Sec 33
In sum our work makes the following contributions 
i In contrast to purely transformerbased approaches
23 66 our method scales more graceful to higher dimen
sional data and can thus a work on a compression level
which provides more faithful and detailed reconstructions
than previous work see Fig 1 and b can be efﬁciently
Figure 2 Illustrating perceptual and semantic compression Most
bits of a digital image correspond to imperceptible details While
DMs allow to suppress this semantically meaningless information
by minimizing the responsible loss term gradients during train
ing and the neural network backbone training and inference still
need to be evaluated on all pixels leading to superﬂuous compu
tations and unnecessarily expensive optimization and inference
We propose latent diffusion models LDMs as an effective gener
ative model and a separate mild compression stage that only elim
inates imperceptible details Data and images from 30
applied to highresolution synthesis of megapixel images
ii We achieve competitive performance on multiple
tasks unconditional image synthesis inpainting stochastic
superresolution and datasets while signiﬁcantly lowering
computational costs Compared to pixelbased diffusion ap
proaches we also signiﬁcantly decrease inference costs
iii We show that in contrast to previous work 93
which learns both an encoderdecoder architecture and a
scorebased prior simultaneously our approach does not re
quire a delicate weighting of reconstruction and generative
abilities This ensures extremely faithful reconstructions
and requires very little regularization of the latent space
iv We ﬁnd that for densely conditioned tasks such
as superresolution inpainting and semantic synthesis our
model can be applied in a convolutional fashion and render
large consistent images of 10242px
v Moreover we design a generalpurpose conditioning
mechanism based on crossattention enabling multimodal
training We use it to train classconditional texttoimage
and layouttoimage models
vi Finally we release pretrained latent diffusion
and autoencoding models at https    github 
comCompVislatentdiffusion which might be
reusable for a various tasks besides training of DMs 81
2 Related Work
Generative Models for Image Synthesis The high di
mensional nature of images presents distinct challenges
to generative modeling Generative Adversarial Networks
GAN 27 allow for efﬁcient sampling of high resolution
images with good perceptual quality 3 42 but are difﬁ
2cult to optimize 2 28 54 and struggle to capture the full
data distribution 55 In contrast likelihoodbased meth
ods emphasize good density estimation which renders op
timization more wellbehaved Variational autoencoders
V AE 46 and ﬂowbased models 18 19 enable efﬁcient
synthesis of high resolution images 9 44 92 but sam
ple quality is not on par with GANs While autoregressive
models ARM 6 10 94 95 achieve strong performance
in density estimation computationally demanding architec
tures 97 and a sequential sampling process limit them to
low resolution images Because pixel based representations
of images contain barely perceptible highfrequency de
tails 1673 maximumlikelihood training spends a dispro
portionate amount of capacity on modeling them resulting
in long training times To scale to higher resolutions several
twostage approaches 2367101103 use ARMs to model
a compressed latent image space instead of raw pixels
Recently Diffusion Probabilistic Models DM 82
have achieved stateoftheart results in density estimation
45 as well as in sample quality 15 The generative power
of these models stems from a natural ﬁt to the inductive bi
ases of imagelike data when their underlying neural back
bone is implemented as a UNet 15 30 71 85 The best
synthesis quality is usually achieved when a reweighted ob
jective 30 is used for training In this case the DM corre
sponds to a lossy compressor and allow to trade image qual
ity for compression capabilities Evaluating and optimizing
these models in pixel space however has the downside of
low inference speed and very high training costs While
the former can be partially adressed by advanced sampling
strategies 47 75 84 and hierarchical approaches 31 93
training on highresolution image data always requires to
calculate expensive gradients We adress both drawbacks
with our proposed LDMs  which work on a compressed la
tent space of lower dimensionality This renders training
computationally cheaper and speeds up inference with al
most no reduction in synthesis quality see Fig 1
TwoStage Image Synthesis To mitigate the shortcom
ings of individual generative approaches a lot of research
11 23 67 70 101 103 has gone into combining the
strengths of different methods into more efﬁcient and per
formant models via a two stage approach VQV AEs 67
101 use autoregressive models to learn an expressive prior
over a discretized latent space 66 extend this approach to
texttoimage generation by learning a joint distributation
over discretized image and text representations More gen
erally 70 uses conditionally invertible networks to pro
vide a generic transfer between latent spaces of diverse do
mains Different from VQV AEs VQGANs 23 103 em
ploy a ﬁrst stage with an adversarial and perceptual objec
tive to scale autoregressive transformers to larger images
However the high compression rates required for feasible
ARM training which introduces billions of trainable pa
rameters 23 66 limit the overall performance of such approaches and less compression comes at the price of high
computational cost 23 66 Our work prevents such trade
offs as our proposed LDMs scale more gently to higher
dimensional latent spaces due to their convolutional back
bone Thus we are free to choose the level of compression
which optimally mediates between learning a powerful ﬁrst
stage without leaving too much perceptual compression up
to the generative diffusion model while guaranteeing high
ﬁdelity reconstructions see Fig 1
While approaches to jointly 93 or separately 80 learn
an encodingdecoding model together with a scorebased
prior exist the former still require a difﬁcult weighting be
tween reconstruction and generative capabilities 11 and
are outperformed by our approach Sec 4 and the latter
focus on highly structured images such as human faces
3 Method
To lower the computational demands of training diffu
sion models towards highresolution image synthesis we
observe that although diffusion models allow to ignore
perceptually irrelevant details by undersampling the corre
sponding loss terms 30 they still require costly function
evaluations in pixel space which causes huge demands in
computation time and energy resources
We propose to circumvent this drawback by introducing
an explicit separation of the compressive from the genera
tive learning phase see Fig 2 To achieve this we utilize
an autoencoding model which learns a space that is percep
tually equivalent to the image space but offers signiﬁcantly
reduced computational complexity
Such an approach offers several advantages i By leav
ing the highdimensional image space we obtain DMs
which are computationally much more efﬁcient because
sampling is performed on a lowdimensional space ii We
exploit the inductive bias of DMs inherited from their UNet
architecture 71 which makes them particularly effective
for data with spatial structure and therefore alleviates the
need for aggressive qualityreducing compression levels as
required by previous approaches 23 66 iii Finally we
obtain generalpurpose compression models whose latent
space can be used to train multiple generative models and
which can also be utilized for other downstream applica
tions such as singleimage CLIPguided synthesis 25
31 Perceptual Image Compression
Our perceptual compression model is based on previous
work 23 and consists of an autoencoder trained by com
bination of a perceptual loss 106 and a patchbased 33
adversarial objective 20 23 103 This ensures that the re
constructions are conﬁned to the image manifold by enforc
ing local realism and avoids bluriness introduced by relying
solely on pixelspace losses such as L2orL1objectives
More precisely given an image x2RHW3in RGB
space the encoder Eencodesxinto a latent representa
3tionzEx and the decoder Dreconstructs the im
age from the latent giving xDz DEx where
z2Rhwc Importantly the encoder downsamples the
image by a factor fHh Ww  and we investigate
different downsampling factors f 2m withm2N
In order to avoid arbitrarily highvariance latent spaces
we experiment with two different kinds of regularizations
The ﬁrst variant KLreg  imposes a slight KLpenalty to
wards a standard normal on the learned latent similar to a
V AE 46 69 whereas VQreg uses a vector quantization
layer 96 within the decoder This model can be interpreted
as a VQGAN 23 but with the quantization layer absorbed
by the decoder Because our subsequent DM is designed
to work with the twodimensional structure of our learned
latent space zEx we can use relatively mild compres
sion rates and achieve very good reconstructions This is
in contrast to previous works 23 66 which relied on an
arbitrary 1D ordering of the learned space zto model its
distribution autoregressively and thereby ignored much of
the inherent structure of z Hence our compression model
preserves details of xbetter see Tab 8 The full objective
and training details can be found in the supplement
32 Latent Diffusion Models
Diffusion Models 82 are probabilistic models designed to
learn a data distribution pxby gradually denoising a nor
mally distributed variable which corresponds to learning
the reverse process of a ﬁxed Markov Chain of length T
For image synthesis the most successful models 153072
rely on a reweighted variant of the variational lower bound
onpx which mirrors denoising scorematching 85
These models can be interpreted as an equally weighted
sequence of denoising autoencoders xttt 1T 
which are trained to predict a denoised variant of their input
xt wherextis a noisy version of the input x The corre
sponding objective can be simpliﬁed to Sec B
LDMExN01th
kxttk2
2i
 1
withtuniformly sampled from f1Tg
Generative Modeling of Latent Representations With
our trained perceptual compression models consisting of E
andD we now have access to an efﬁcient lowdimensional
latent space in which highfrequency imperceptible details
are abstracted away Compared to the highdimensional
pixel space this space is more suitable for likelihoodbased
generative models as they can now i focus on the impor
tant semantic bits of the data and ii train in a lower di
mensional computationally much more efﬁcient space
Unlike previous work that relied on autoregressive
attentionbased transformer models in a highly compressed
discrete latent space 2366103 we can take advantage of
imagespeciﬁc inductive biases that our model offers This
Semantic  
 Map
crossattentionLatent SpaceConditioning  
TextDiffusion Process
denoising step switch skip connectionRepres  
entations
Pixel SpaceImagesDenoising UNet
concatFigure 3 We condition LDMs either via concatenation or by a
more general crossattention mechanism See Sec 33
includes the ability to build the underlying UNet primar
ily from 2D convolutional layers and further focusing the
objective on the perceptually most relevant bits using the
reweighted bound which now reads
LLDM EExN01th
kzttk2
2i
2
The neural backbone tof our model is realized as a
timeconditional UNet 71 Since the forward process is
ﬁxedztcan be efﬁciently obtained from Eduring training
and samples from pz can be decoded to image space with
a single pass through D
33 Conditioning Mechanisms
Similar to other types of generative models 56 83
diffusion models are in principle capable of modeling
conditional distributions of the form pzjy This can
be implemented with a conditional denoising autoencoder
zttyand paves the way to controlling the synthesis
process through inputs ysuch as text 68 semantic maps
33 61 or other imagetoimage translation tasks 34
In the context of image synthesis however combining
the generative power of DMs with other types of condition
ings beyond classlabels 15 or blurred variants of the input
image 72 is so far an underexplored area of research
We turn DMs into more ﬂexible conditional image gener
ators by augmenting their underlying UNet backbone with
the crossattention mechanism 97 which is effective for
learning attentionbased models of various input modali
ties 3536 To preprocess yfrom various modalities such
as language prompts we introduce a domain speciﬁc en
coderthat projects yto an intermediate representation
y2RMd which is then mapped to the intermediate
layers of the UNet via a crossattention layer implementing
Attention QKV  softmax
QKT
p
d
V with
QWi
Qizt KWi
Ky VWi
Vy
Hereizt2RNdi
denotes a ﬂattened intermediate
representation of the UNet implementing andWi
V2
4CelebAHQ FFHQ LSUNChurches LSUNBeds ImageNet
Figure 4 Samples from LDMs trained on CelebAHQ 39 FFHQ 41 LSUNChurches 102 LSUNBedrooms 102 and class
conditional ImageNet 12 each with a resolution of 256256 Best viewed when zoomed in For more samples cf the supplement
Rddi
Wi
Q2RddWi
K2Rddare learnable pro
jection matrices 36 97 See Fig 3 for a visual depiction
Based on imageconditioning pairs we then learn the
conditional LDM via
LLDM EExyN01th
kzttyk2
2i
3
where bothandare jointly optimized via Eq 3 This
conditioning mechanism is ﬂexible as can be parameter
ized with domainspeciﬁc experts eg unmasked trans
formers 97 when yare text prompts see Sec 431
4 Experiments
LDMs provide means to ﬂexible and computationally
tractable diffusion based image synthesis of various image
modalities which we empirically show in the following
Firstly however we analyze the gains of our models com
pared to pixelbased diffusion models in both training and
inference Interestingly we ﬁnd that LDMs trained in VQ
regularized latent spaces sometimes achieve better sample
quality even though the reconstruction capabilities of VQ
regularized ﬁrst stage models slightly fall behind those of
their continuous counterparts cf Tab 8 A visual compari
son between the effects of ﬁrst stage regularization schemes
onLDM training and their generalization abilities to resolu
tions2562can be found in Appendix D1 In E2 we list
details on architecture implementation training and evalu
ation for all results presented in this section
41 On Perceptual Compression Tradeoffs
This section analyzes the behavior of our LDMs with dif
ferent downsampling factors f2f12481632gabbre
viated as LDMf where LDM1 corresponds to pixelbased
DMs To obtain a comparable testﬁeld we ﬁx the com
putational resources to a single NVIDIA A100 for all ex
periments in this section and train all models for the same
number of steps and with the same number of parameters
Tab 8 shows hyperparameters and reconstruction perfor
mance of the ﬁrst stage models used for the LDMs compared in this section Fig 6 shows sample quality as a func
tion of training progress for 2M steps of classconditional
models on the ImageNet 12 dataset We see that i small
downsampling factors for LDMf12gresult in slow train
ing progress whereas ii overly large values of fcause stag
nating ﬁdelity after comparably few training steps Revis
iting the analysis above Fig 1 and 2 we attribute this to
i leaving most of perceptual compression to the diffusion
model and ii too strong ﬁrst stage compression resulting
in information loss and thus limiting the achievable qual
ityLDMf416gstrike a good balance between efﬁciency
and perceptually faithful results which manifests in a sig
niﬁcant FID 29 gap of 38 between pixelbased diffusion
LDM1  and LDM8 after 2M training steps
In Fig 7 we compare models trained on CelebA
HQ 39 and ImageNet in terms sampling speed for differ
ent numbers of denoising steps with the DDIM sampler 84
and plot it against FIDscores 29 LDMf48goutper
form models with unsuitable ratios of perceptual and con
ceptual compression Especially compared to pixelbased
LDM1  they achieve much lower FID scores while simulta
neously signiﬁcantly increasing sample throughput Com
plex datasets such as ImageNet require reduced compres
sion rates to avoid reducing quality In summary LDM4
and8offer the best conditions for achieving highquality
synthesis results
42 Image Generation with Latent Diffusion
We train unconditional models of 2562images on
CelebAHQ 39 FFHQ 41 LSUNChurches and
Bedrooms 102 and evaluate the i sample quality and ii
their coverage of the data manifold using ii FID 29 and
ii PrecisionandRecall 50 Tab 1 summarizes our re
sults On CelebAHQ we report a new stateoftheart FID
of511 outperforming previous likelihoodbased models as
well as GANs We also outperform LSGM 93 where a la
tent diffusion model is trained jointly together with the ﬁrst
stage In contrast we train diffusion models in a ﬁxed space
5TexttoImage Synthesis on LAION 145B Model
A street sign that reads
Latent Diffusion A zombie in the
style of PicassoAn image of an animal
half mouse half octopusAn illustration of a slightly
conscious neural networkA painting of a
squirrel eating a burgerA watercolor painting of a
chair that looks like an octopusA shirt with the inscription
I love generative models 
Figure 5 Samples for userdeﬁned text prompts from our model for texttoimage synthesis LDM8 KL  which was trained on the
LAION 78 database Samples generated with 200 DDIM steps and  10 We use unconditional guidance 32 with s 100
Figure 6 Analyzing the training of classconditional LDMs with
different downsampling factors fover 2M train steps on the Im
ageNet dataset Pixelbased LDM1 requires substantially larger
train times compared to models with larger downsampling factors
LDMf416g Too much perceptual compression as in LDM32
limits the overall sample quality All models are trained on a sin
gle NVIDIA A100 with the same computational budget Results
obtained with 100 DDIM steps 84 and  0
Figure 7 Comparing LDMs with varying compression on the
CelebAHQ left and ImageNet right datasets Different mark
ers indicatef102050100200gsampling steps using DDIM
from right to left along each line The dashed line shows the FID
scores for 200 steps indicating the strong performance of LDM
f48g FID scores assessed on 5000 samples All models were
trained for 500k CelebA  2M ImageNet steps on an A100
and avoid the difﬁculty of weighing reconstruction quality
against learning the prior over the latent space see Fig 12
We outperform prior diffusion based approaches on all
but the LSUNBedrooms dataset where our score is close
to ADM 15 despite utilizing half its parameters and re
quiring 4times less train resources see Appendix E35CelebAHQ 256256 FFHQ 256256
Method FID Prec Recall Method FID Prec Recall
DCV AE 63 158   ImageBART 21 957  
VQGANT 23 k400 102   UNet GAN aug 77 109 76  
PGGAN 39 80   UDM 43 554  
LSGM 93 722   StyleGAN 41 416 071 046
UDM 43 716   ProjectedGAN 76 308 065 046
LDM4 ours 500sy 511 072 049 LDM4 ours 200s 498 073 050
LSUNChurches 256256 LSUNBedrooms 256256
Method FID Prec Recall Method FID Prec Recall
DDPM 30 789   ImageBART 21 551  
ImageBART 21 732   DDPM 30 49  
PGGAN 39 642   UDM 43 457  
StyleGAN 41 421   StyleGAN 41 235 059 048
StyleGAN2 42 386   ADM 15 190 066 051
ProjectedGAN 76 159 061 044 ProjectedGAN 76 152 061 034
LDM8ours 200s 402 064 052 LDM4 ours 200s 295 066 048
Table 1 Evaluation metrics for unconditional image synthesis
CelebAHQ results reproduced from 43 63 100 FFHQ from
42 43yNs refers toNsampling steps with the DDIM 84
sampler trained in KLregularized latent space Additional re
sults can be found in the supplementary
TextConditional Image Synthesis
Method FID IS Nparams
CogViewy17 2710 1820 4B selfranking rejection rate 0017
LAFITEy109 2694 2602 75M
GLIDE59 1224  6B 277 DDIM steps cfg 32 s 3
MakeAScene26 1184  4B cfg for AR models 98 s 5
LDMKL8 2331 2003 033 145B 250 DDIM steps
LDMKL8G1263 3029 042 145B 250 DDIM steps cfg 32 s 15
Table 2 Evaluation of textconditional image synthesis on the
256256sized MSCOCO 51 dataset with 250 DDIM 84
steps our model is on par with the most recent diffusion 59 and
autoregressive 26 methods despite using signiﬁcantly less pa
rametersyNumbers from 109 26
Moreover LDMs consistently improve upon GANbased
methods in Precision and Recall thus conﬁrming the ad
vantages of their modecovering likelihoodbased training
objective over adversarial approaches In Fig 4 we also
show qualitative results on each dataset
6Figure 8 Layouttoimage synthesis with an LDM on COCO 4
see Sec 431 Quantitative evaluation in the supplement D3
43 Conditional Latent Diffusion
431 Transformer Encoders for LDMs
By introducing crossattention based conditioning into
LDMs we open them up for various conditioning modali
ties previously unexplored for diffusion models For text
toimage image modeling we train a 145B parameter
KLregularized LDM conditioned on language prompts on
LAION400M 78 We employ the BERTtokenizer 14
and implement as a transformer 97 to infer a latent
code which is mapped into the UNet via multihead cross
attention Sec 33 This combination of domain speciﬁc
experts for learning a language representation and visual
synthesis results in a powerful model which generalizes
well to complex userdeﬁned text prompts cf Fig 8 and 5
For quantitative analysis we follow prior work and evaluate
texttoimage generation on the MSCOCO 51 validation
set where our model improves upon powerful AR 17 66
and GANbased 109 methods cf Tab 2 We note that ap
plying classiﬁerfree diffusion guidance 32 greatly boosts
sample quality such that the guided LDMKL8G is on par
with the recent stateoftheart AR 26 and diffusion mod
els 59 for texttoimage synthesis while substantially re
ducing parameter count To further analyze the ﬂexibility of
the crossattention based conditioning mechanism we also
train models to synthesize images based on semantic lay
outs on OpenImages 49 and ﬁnetune on COCO 4 see
Fig 8 See Sec D3 for the quantitative evaluation and im
plementation details
Lastly following prior work 3 15 21 23 we evalu
ate our bestperforming classconditional ImageNet mod
els withf2 f48gfrom Sec 41 in Tab 3 Fig 4 and
Sec D4 Here we outperform the state of the art diffu
sion model ADM 15 while signiﬁcantly reducing compu
tational requirements and parameter count cf Tab 18
432 Convolutional Sampling Beyond 2562
By concatenating spatially aligned conditioning informa
tion to the input of LDMs can serve as efﬁcient generalMethod FID IS Precision RecallNparams
BigGandeep 3 695 2036 26 087 028 340M 
ADM 15 1094 10098 069 063 554M 250 DDIM steps
ADMG 15 459 1867 082 052 608M 250 DDIM steps
LDM4 ours 1056 10349 124 071 062 400M 250 DDIM steps
LDM4 G ours 360 24767 559 087 048 400M 250 steps cfg 32 s 15
Table 3 Comparison of a classconditional ImageNet LDM with
recent stateoftheart methods for classconditional image gener
ation on ImageNet 12 A more detailed comparison with addi
tional baselines can be found in D4 Tab 10 and F cfg denotes
classiﬁerfree guidance with a scale sas proposed in 32
purpose imagetoimage translation models We use this
to train models for semantic synthesis superresolution
Sec 44 and inpainting Sec 45 For semantic synthe
sis we use images of landscapes paired with semantic maps
23 61 and concatenate downsampled versions of the se
mantic maps with the latent image representation of a f 4
model VQreg see Tab 8 We train on an input resolution
of2562crops from 3842 but ﬁnd that our model general
izes to larger resolutions and can generate images up to the
megapixel regime when evaluated in a convolutional man
ner see Fig 9 We exploit this behavior to also apply the
superresolution models in Sec 44 and the inpainting mod
els in Sec 45 to generate large images between 5122and
10242 For this application the signaltonoise ratio in
duced by the scale of the latent space signiﬁcantly affects
the results In Sec D1 we illustrate this when learning an
LDM on i the latent space as provided by a f 4 model
KLreg see Tab 8 and ii a rescaled version scaled by
the componentwise standard deviation
The latter in combination with classiﬁerfree guid
ance 32 also enables the direct synthesis of 2562im
ages for the textconditional LDMKL8G as in Fig 13
Figure 9 A LDM trained on 2562resolution can generalize to
larger resolution here 5121024  for spatially conditioned tasks
such as semantic synthesis of landscape images See Sec 432
44 SuperResolution with Latent Diffusion
LDMs can be efﬁciently trained for superresolution by
diretly conditioning on lowresolution images via concate
nation  cf Sec 33 In a ﬁrst experiment we follow SR3
7bicubic LDM SR SR3
Figure 10 ImageNet 64 256 superresolution on ImageNetVal
LDMSR has advantages at rendering realistic textures but SR3
can synthesize more coherent ﬁne structures See appendix for
additional samples and cropouts SR3 results from 72
72 and ﬁx the image degradation to a bicubic interpola
tion with 4downsampling and train on ImageNet follow
ing SR3s data processing pipeline We use the f 4 au
toencoding model pretrained on OpenImages VQreg cf
Tab 8 and concatenate the lowresolution conditioning y
and the inputs to the UNet ieis the identity Our quali
tative and quantitative results see Fig 10 and Tab 5 show
competitive performance and LDMSR outperforms SR3
in FID while SR3 has a better IS A simple image regres
sion model achieves the highest PSNR and SSIM scores
however these metrics do not align well with human per
ception 106 and favor blurriness over imperfectly aligned
high frequency details 72 Further we conduct a user
study comparing the pixelbaseline with LDMSR We fol
low SR3 72 where human subjects were shown a lowres
image in between two highres images and asked for pref
erence The results in Tab 4 afﬁrm the good performance
of LDMSR PSNR and SSIM can be pushed by using a
posthoc guiding mechanism 15 and we implement this
imagebased guider via a perceptual loss see Sec D6
SR on ImageNet Inpainting on Places
User Study PixelDM  f1LDM4 LAMA 88 LDM4
Task 1 Preference vs GT 160 304 136 210
Task 2 Preference Score 294 706 319 681
Table 4 Task 1 Subjects were shown ground truth and generated
image and asked for preference Task 2 Subjects had to decide
between two generated images More details in E36
Since the bicubic degradation process does not generalize
well to images which do not follow this preprocessing we
also train a generic model LDMBSR  by using more di
verse degradation The results are shown in Sec D61Method FID IS PSNR SSIMNparams samples
s
Image Regression 72 152 1211 279 0801 625M NA
SR3 72 52 1801 264 0762 625M NA
LDM4 ours 100 steps 28y48z1663 244 38 069014 169M 462
emphLDM4 ours big 100 steps 24y43z1749 24741 071015 552M 45
LDM4 ours 50 steps guiding 44y64z1537 258 37 074012 184M 038
Table 54upscaling results on ImageNetVal  2562y FID
features computed on validation splitz FID features computed
on train split Assessed on a NVIDIA A100
train throughput sampling throughputytrainval FID2k
Model regtype samplessec 256 512 hoursepoch epoch 6
LDM1 no ﬁrst stage 011 026 007 2066 2474
LDM4 KL w attn 032 097 034 766 1521
LDM4 VQ w attn 033 097 034 704 1499
LDM4 VQ wo attn 035 099 036 666 1595
Table 6 Assessing inpainting efﬁciencyy Deviations from Fig 7
due to varying GPU settingsbatch sizes cf the supplement
45 Inpainting with Latent Diffusion
Inpainting is the task of ﬁlling masked regions of an im
age with new content either because parts of the image are
are corrupted or to replace existing but undesired content
within the image We evaluate how our general approach
for conditional image generation compares to more special
ized stateoftheart approaches for this task Our evalua
tion follows the protocol of LaMa 88 a recent inpainting
model that introduces a specialized architecture relying on
Fast Fourier Convolutions 8 The exact training  evalua
tion protocol on Places 108 is described in Sec E22
We ﬁrst analyze the effect of different design choices for
the ﬁrst stage In particular we compare the inpainting ef
ﬁciency of LDM1 ie a pixelbased conditional DM with
LDM4  for both KLandVQregularizations as well as VQ
LDM4 without any attention in the ﬁrst stage see Tab 8
where the latter reduces GPU memory for decoding at high
resolutions For comparability we ﬁx the number of param
eters for all models Tab 6 reports the training and sampling
throughput at resolution 2562and5122 the total training
time in hours per epoch and the FID score on the validation
split after six epochs Overall we observe a speedup of at
least27between pixel and latentbased diffusion models
while improving FID scores by a factor of at least 16
The comparison with other inpainting approaches in
Tab 7 shows that our model with attention improves the
overall image quality as measured by FID over that of 88
LPIPS between the unmasked images and our samples is
slightly higher than that of 88 We attribute this to 88
only producing a single result which tends to recover more
of an average image compared to the diverse results pro
duced by our LDM cf Fig 21 Additionally in a user study
Tab 4 human subjects favor our results over those of 88
Based on these initial results we also trained a larger dif
fusion model  bigin Tab 7 in the latent space of the VQ
regularized ﬁrst stage without attention Following 15
the UNet of this diffusion model uses attention layers on
three levels of its feature hierarchy the BigGAN 3 residual
block for up and downsampling and has 387M parameters
8input result
Figure 11 Qualitative results on object removal with our big w
ftinpainting model For more results see Fig 22
instead of 215M After training we noticed a discrepancy
in the quality of samples produced at resolutions 2562and
5122 which we hypothesize to be caused by the additional
attention modules However ﬁnetuning the model for half
an epoch at resolution 5122allows the model to adjust to
the new feature statistics and sets a new state of the art FID
on image inpainting  big wo attn w ft in Tab 7 Fig 11
5 Limitations  Societal Impact
Limitations While LDMs signiﬁcantly reduce computa
tional requirements compared to pixelbased approaches
their sequential sampling process is still slower than that
of GANs Moreover the use of LDMs can be question
able when high precision is required although the loss of
image quality is very small in our f 4autoencoding mod
els see Fig 1 their reconstruction capability can become
a bottleneck for tasks that require ﬁnegrained accuracy in
pixel space We assume that our superresolution models
Sec 44 are already somewhat limited in this respect
Societal Impact Generative models for media like im
agery are a doubleedged sword On the one hand they4050 masked All samples
Method FID LPIPS FID LPIPS
LDM4 ours big w ft 939 0246 0042 150 0137 0080
LDM4 ours big wo ft 1289 0257 0047 240 0142 0085
LDM4 ours w attn 1187 0257 0042 215 0144 0084
LDM4 ours wo attn 1260 0259 0041 237 0145 0084
LaMa 88y1231 0243 0038 223 0134 0080
LaMa 88 120 024 221 014
CoModGAN 107 104 026 182 015
RegionWise 52 213 027 475 015
DeepFill v2 104 221 028 520 016
EdgeConnect 58 305 028 837 016
Table 7 Comparison of inpainting performance on 30k crops of
size512512from test images of Places 108 The column 40
50 reports metrics computed over hard examples where 4050
of the image region have to be inpaintedyrecomputed on our test
set since the original test set used in 88 was not available
enable various creative applications and in particular ap
proaches like ours that reduce the cost of training and in
ference have the potential to facilitate access to this tech
nology and democratize its exploration On the other hand
it also means that it becomes easier to create and dissemi
nate manipulated data or spread misinformation and spam
In particular the deliberate manipulation of images deep
fakes is a common problem in this context and women in
particular are disproportionately affected by it 13 24
Generative models can also reveal their training data
5 90 which is of great concern when the data contain
sensitive or personal information and were collected with
out explicit consent However the extent to which this also
applies to DMs of images is not yet fully understood
Finally deep learning modules tend to reproduce or ex
acerbate biases that are already present in the data 22 38
91 While diffusion models achieve better coverage of the
data distribution than eg GANbased approaches the ex
tent to which our twostage approach that combines adver
sarial training and a likelihoodbased objective misrepre
sents the data remains an important research question
For a more general detailed discussion of the ethical
considerations of deep generative models see eg 13
6 Conclusion
We have presented latent diffusion models a simple and
efﬁcient way to signiﬁcantly improve both the training and
sampling efﬁciency of denoising diffusion models with
out degrading their quality Based on this and our cross
attention conditioning mechanism our experiments could
demonstrate favorable results compared to stateoftheart
methods across a wide range of conditional image synthesis
tasks without taskspeciﬁc architectures
This work has been supported by the German Federal Ministry for
Economic Affairs and Energy within the project KIAbsicherung  Safe
AI for automated driving and by the German Research Foundation DFG
project 421703927
9References
1 Eirikur Agustsson and Radu Timofte NTIRE 2017 chal
lenge on single image superresolution Dataset and study
In2017 IEEE Conference on Computer Vision and Pattern
Recognition Workshops CVPR Workshops 2017 Honolulu
HI USA July 2126 2017  pages 11221131 IEEE Com
puter Society 2017 1
2 Martin Arjovsky Soumith Chintala and L eon Bottou
Wasserstein gan 2017 3
3 Andrew Brock Jeff Donahue and Karen Simonyan Large
scale GAN training for high ﬁdelity natural image synthe
sis In Int Conf Learn Represent  2019 1 2 7 8 22
28
4 Holger Caesar Jasper R R Uijlings and Vittorio Ferrari
Cocostuff Thing and stuff classes in context In 2018
IEEE Conference on Computer Vision and Pattern Recog
nition CVPR 2018 Salt Lake City UT USA June 18
22 2018  pages 12091218 Computer Vision Foundation 
IEEE Computer Society 2018 7 20 22
5 Nicholas Carlini Florian Tramer Eric Wallace Matthew
Jagielski Ariel HerbertV oss Katherine Lee Adam
Roberts Tom Brown Dawn Song Ulfar Erlingsson et al
Extracting training data from large language models In
30th USENIX Security Symposium USENIX Security 21 
pages 26332650 2021 9
6 Mark Chen Alec Radford Rewon Child Jeffrey Wu Hee
woo Jun David Luan and Ilya Sutskever Generative pre
training from pixels In ICML  volume 119 of Proceedings
of Machine Learning Research  pages 16911703 PMLR
2020 3
7 Nanxin Chen Yu Zhang Heiga Zen Ron J Weiss Mo
hammad Norouzi and William Chan Wavegrad Estimat
ing gradients for waveform generation In ICLR  OpenRe
viewnet 2021 1
8 Lu Chi Borui Jiang and Yadong Mu Fast fourier convolu
tion In NeurIPS  2020 8
9 Rewon Child Very deep vaes generalize autoregressive
models and can outperform them on images CoRR 
abs201110650 2020 3
10 Rewon Child Scott Gray Alec Radford and Ilya Sutskever
Generating long sequences with sparse transformers
CoRR  abs190410509 2019 3
11 Bin Dai and David P Wipf Diagnosing and enhancing V AE
models In ICLR Poster  OpenReviewnet 2019 2 3
12 Jia Deng Wei Dong Richard Socher LiJia Li Kai Li
and FeiFei Li Imagenet A largescale hierarchical im
age database In CVPR  pages 248255 IEEE Computer
Society 2009 1 5 7 22
13 Emily Denton Ethical considerations of generative ai AI
for Content Creation Workshop CVPR 2021 9
14 Jacob Devlin MingWei Chang Kenton Lee and
Kristina Toutanova BERT pretraining of deep bidirec
tional transformers for language understanding CoRR 
abs181004805 2018 7
15 Prafulla Dhariwal and Alex Nichol Diffusion models beat
gans on image synthesis CoRR  abs210505233 2021 1
2 3 4 6 7 8 18 22 25 26 2816 Sander Dieleman Musings on typicality 2020 1 3
17 Ming Ding Zhuoyi Yang Wenyi Hong Wendi Zheng
Chang Zhou Da Yin Junyang Lin Xu Zou Zhou Shao
Hongxia Yang and Jie Tang Cogview Mastering textto
image generation via transformers CoRR  abs210513290
2021 6 7
18 Laurent Dinh David Krueger and Yoshua Bengio Nice
Nonlinear independent components estimation 2015 3
19 Laurent Dinh Jascha SohlDickstein and Samy Ben
gio Density estimation using real NVP In 5th Inter
national Conference on Learning Representations ICLR
2017 Toulon France April 2426 2017 Conference Track
Proceedings  OpenReviewnet 2017 1 3
20 Alexey Dosovitskiy and Thomas Brox Generating images
with perceptual similarity metrics based on deep networks
In Daniel D Lee Masashi Sugiyama Ulrike von Luxburg
Isabelle Guyon and Roman Garnett editors Adv Neural
Inform Process Syst  pages 658666 2016 3
21 Patrick Esser Robin Rombach Andreas Blattmann and
Bjorn Ommer Imagebart Bidirectional context with multi
nomial diffusion for autoregressive image synthesis CoRR 
abs210808827 2021 6 7 22
22 Patrick Esser Robin Rombach and Bj orn Ommer A
note on data biases in generative models arXiv preprint
arXiv201202516  2020 9
23 Patrick Esser Robin Rombach and Bj orn Ommer Taming
transformers for highresolution image synthesis CoRR 
abs201209841 2020 2 3 4 6 7 21 22 29 34 36
24 Mary Anne Franks and Ari Ezra Waldman Sex lies and
videotape Deep fakes and free speech delusions Md L
Rev 78892 2018 9
25 Kevin Frans Lisa B Soros and Olaf Witkowski Clipdraw
Exploring texttodrawing synthesis through language
image encoders ArXiv  abs210614843 2021 3
26 Oran Gafni Adam Polyak Oron Ashual Shelly Sheynin
Devi Parikh and Yaniv Taigman Makeascene Scene
based texttoimage generation with human priors CoRR 
abs220313131 2022 6 7 16
27 Ian J Goodfellow Jean PougetAbadie Mehdi Mirza Bing
Xu David WardeFarley Sherjil Ozair Aaron C Courville
and Yoshua Bengio Generative adversarial networks
CoRR  2014 1 2
28 Ishaan Gulrajani Faruk Ahmed Martin Arjovsky Vincent
Dumoulin and Aaron Courville Improved training of
wasserstein gans 2017 3
29 Martin Heusel Hubert Ramsauer Thomas Unterthiner
Bernhard Nessler and Sepp Hochreiter Gans trained by
a two timescale update rule converge to a local nash equi
librium In Adv Neural Inform Process Syst  pages 6626
6637 2017 1 5 26
30 Jonathan Ho Ajay Jain and Pieter Abbeel Denoising dif
fusion probabilistic models In NeurIPS  2020 1 2 3 4
6 17
31 Jonathan Ho Chitwan Saharia William Chan David J
Fleet Mohammad Norouzi and Tim Salimans Cascaded
diffusion models for high ﬁdelity image generation CoRR 
abs210615282 2021 1 3 22
1032 Jonathan Ho and Tim Salimans Classiﬁerfree diffusion
guidance In NeurIPS 2021 Workshop on Deep Generative
Models and Downstream Applications  2021 6 7 16 22
28 37 38
33 Phillip Isola JunYan Zhu Tinghui Zhou and Alexei A
Efros Imagetoimage translation with conditional adver
sarial networks In CVPR  pages 59675976 IEEE Com
puter Society 2017 3 4
34 Phillip Isola JunYan Zhu Tinghui Zhou and Alexei A
Efros Imagetoimage translation with conditional adver
sarial networks 2017 IEEE Conference on Computer Vi
sion and Pattern Recognition CVPR  pages 59675976
2017 4
35 Andrew Jaegle Sebastian Borgeaud JeanBaptiste
Alayrac Carl Doersch Catalin Ionescu David Ding
Skanda Koppula Daniel Zoran Andrew Brock Evan
Shelhamer Olivier J H enaff Matthew M Botvinick
Andrew Zisserman Oriol Vinyals and Jo ao Carreira
Perceiver IO A general architecture for structured inputs
outputs CoRR  abs210714795 2021 4
36 Andrew Jaegle Felix Gimeno Andy Brock Oriol Vinyals
Andrew Zisserman and Jo ao Carreira Perceiver General
perception with iterative attention In Marina Meila and
Tong Zhang editors Proceedings of the 38th International
Conference on Machine Learning ICML 2021 1824 July
2021 Virtual Event  volume 139 of Proceedings of Machine
Learning Research  pages 46514664 PMLR 2021 4 5
37 Manuel Jahn Robin Rombach and Bj orn Ommer High
resolution complex scene synthesis with transformers
CoRR  abs210506458 2021 20 22 27
38 Niharika Jain Alberto Olmo Sailik Sengupta Lydia
Manikonda and Subbarao Kambhampati Imperfect ima
ganation Implications of gans exacerbating biases on fa
cial data augmentation and snapchat selﬁe lenses arXiv
preprint arXiv200109528  2020 9
39 Tero Karras Timo Aila Samuli Laine and Jaakko Lehti
nen Progressive growing of gans for improved quality sta
bility and variation CoRR  abs171010196 2017 5 6
40 Tero Karras Samuli Laine and Timo Aila A stylebased
generator architecture for generative adversarial networks
InIEEE Conf Comput Vis Pattern Recog  pages 4401
4410 2019 1
41 T Karras S Laine and T Aila A stylebased gener
ator architecture for generative adversarial networks In
2019 IEEECVF Conference on Computer Vision and Pat
tern Recognition CVPR  2019 5 6
42 Tero Karras Samuli Laine Miika Aittala Janne Hellsten
Jaakko Lehtinen and Timo Aila Analyzing and improv
ing the image quality of stylegan CoRR  abs191204958
2019 2 6 28
43 Dongjun Kim Seungjae Shin Kyungwoo Song Wanmo
Kang and IlChul Moon Score matching model for un
bounded data score CoRR  abs210605527 2021 6
44 Durk P Kingma and Prafulla Dhariwal Glow Generative
ﬂow with invertible 1x1 convolutions In S Bengio H Wal
lach H Larochelle K Grauman N CesaBianchi and R
Garnett editors Advances in Neural Information Process
ing Systems  2018 345 Diederik P Kingma Tim Salimans Ben Poole and
Jonathan Ho Variational diffusion models CoRR 
abs210700630 2021 1 3 16
46 Diederik P Kingma and Max Welling AutoEncoding Vari
ational Bayes In 2nd International Conference on Learn
ing Representations ICLR  2014 1 3 4 29
47 Zhifeng Kong and Wei Ping On fast sampling of diffusion
probabilistic models CoRR  abs210600132 2021 3
48 Zhifeng Kong Wei Ping Jiaji Huang Kexin Zhao and
Bryan Catanzaro Diffwave A versatile diffusion model
for audio synthesis In ICLR  OpenReviewnet 2021 1
49 Alina Kuznetsova Hassan Rom Neil Alldrin Jasper R R
Uijlings Ivan Krasin Jordi PontTuset Shahab Kamali
Stefan Popov Matteo Malloci Tom Duerig and Vittorio
Ferrari The open images dataset V4 uniﬁed image classi
ﬁcation object detection and visual relationship detection
at scale CoRR  abs181100982 2018 7 20 22
50 Tuomas Kynk aanniemi Tero Karras Samuli Laine Jaakko
Lehtinen and Timo Aila Improved precision and re
call metric for assessing generative models CoRR 
abs190406991 2019 5 26
51 TsungYi Lin Michael Maire Serge J Belongie
Lubomir D Bourdev Ross B Girshick James Hays Pietro
Perona Deva Ramanan Piotr Doll ar and C Lawrence Zit
nick Microsoft COCO common objects in context CoRR 
abs14050312 2014 6 7 27
52 Yuqing Ma Xianglong Liu Shihao Bai LeYi Wang Ais
han Liu Dacheng Tao and Edwin Hancock Regionwise
generative adversarial imageinpainting for large missing ar
eas ArXiv  abs190912507 2019 9
53 Chenlin Meng Yang Song Jiaming Song Jiajun Wu Jun
Yan Zhu and Stefano Ermon Sdedit Image synthesis
and editing with stochastic differential equations CoRR 
abs210801073 2021 1
54 Lars M Mescheder On the convergence properties of GAN
training CoRR  abs180104406 2018 3
55 Luke Metz Ben Poole David Pfau and Jascha Sohl
Dickstein Unrolled generative adversarial networks In
5th International Conference on Learning Representations
ICLR 2017 Toulon France April 2426 2017 Conference
Track Proceedings  OpenReviewnet 2017 3
56 Mehdi Mirza and Simon Osindero Conditional generative
adversarial nets CoRR  abs14111784 2014 4
57 Gautam Mittal Jesse H Engel Curtis Hawthorne and Ian
Simon Symbolic music generation with diffusion models
CoRR  abs210316091 2021 1
58 Kamyar Nazeri Eric Ng Tony Joseph Faisal Z Qureshi
and Mehran Ebrahimi Edgeconnect Generative im
age inpainting with adversarial edge learning ArXiv 
abs190100212 2019 9
59 Alex Nichol Prafulla Dhariwal Aditya Ramesh Pranav
Shyam Pamela Mishkin Bob McGrew Ilya Sutskever and
Mark Chen GLIDE towards photorealistic image genera
tion and editing with textguided diffusion models CoRR 
abs211210741 2021 6 7 16
60 Anton Obukhov Maximilian Seitzer PoWei Wu Se
men Zhydenko Jonathan Kyl and Elvis YuJing Lin
11Highﬁdelity performance metrics for generative models
in pytorch 2020 Version 030 DOI 105281zen
odo4957738 26 27
61 Taesung Park MingYu Liu TingChun Wang and Jun
Yan Zhu Semantic image synthesis with spatiallyadaptive
normalization In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition  2019 4 7
62 Taesung Park MingYu Liu TingChun Wang and Jun
Yan Zhu Semantic image synthesis with spatiallyadaptive
normalization In Proceedings of the IEEECVF Confer
ence on Computer Vision and Pattern Recognition CVPR 
June 2019 22
63 Gaurav Parmar Dacheng Li Kwonjoon Lee and Zhuowen
Tu Dual contradistinctive generative autoencoder In IEEE
Conference on Computer Vision and Pattern Recognition
CVPR 2021 virtual June 1925 2021  pages 823832
Computer Vision Foundation  IEEE 2021 6
64 Gaurav Parmar Richard Zhang and JunYan Zhu On
buggy resizing libraries and surprising subtleties in ﬁd cal
culation arXiv preprint arXiv210411222  2021 26
65 David A Patterson Joseph Gonzalez Quoc V  Le Chen
Liang LluisMiquel Munguia Daniel Rothchild David R
So Maud Texier and Jeff Dean Carbon emissions and
large neural network training CoRR  abs210410350
2021 2
66 Aditya Ramesh Mikhail Pavlov Gabriel Goh Scott
Gray Chelsea V oss Alec Radford Mark Chen and Ilya
Sutskever Zeroshot texttoimage generation CoRR 
abs210212092 2021 1 2 3 4 7 21 27
67 Ali Razavi A aron van den Oord and Oriol Vinyals Gen
erating diverse highﬁdelity images with VQV AE2 In
NeurIPS  pages 1483714847 2019 1 2 3 22
68 Scott E Reed Zeynep Akata Xinchen Yan Lajanugen Lo
geswaran Bernt Schiele and Honglak Lee Generative ad
versarial text to image synthesis In ICML  2016 4
69 Danilo Jimenez Rezende Shakir Mohamed and Daan
Wierstra Stochastic backpropagation and approximate in
ference in deep generative models In Proceedings of the
31st International Conference on International Conference
on Machine Learning ICML  2014 1 4 29
70 Robin Rombach Patrick Esser and Bj orn Ommer
Networktonetwork translation with conditional invertible
neural networks In NeurIPS  2020 3
71 Olaf Ronneberger Philipp Fischer and Thomas Brox U
net Convolutional networks for biomedical image segmen
tation In MICCAI 3  volume 9351 of Lecture Notes in
Computer Science  pages 234241 Springer 2015 2 3 4
72 Chitwan Saharia Jonathan Ho William Chan Tim Sal
imans David J Fleet and Mohammad Norouzi Im
age superresolution via iterative reﬁnement CoRR 
abs210407636 2021 1 4 8 16 22 23 27
73 Tim Salimans Andrej Karpathy Xi Chen and Diederik P
Kingma Pixelcnn Improving the pixelcnn with dis
cretized logistic mixture likelihood and other modiﬁcations
CoRR  abs170105517 2017 1 3
74 Dave Salvator NVIDIA Developer Blog https
  developer  nvidia  com  blog  getting immediatespeedupswitha100tf32  2020
28
75 Robin SanRoman Eliya Nachmani and Lior Wolf
Noise estimation for generative diffusion models CoRR 
abs210402600 2021 3
76 Axel Sauer Kashyap Chitta Jens M uller and An
dreas Geiger Projected gans converge faster CoRR 
abs211101007 2021 6
77 Edgar Sch onfeld Bernt Schiele and Anna Khoreva A u
net based discriminator for generative adversarial networks
In2020 IEEECVF Conference on Computer Vision and
Pattern Recognition CVPR 2020 Seattle WA USA June
1319 2020  pages 82048213 Computer Vision Founda
tion  IEEE 2020 6
78 Christoph Schuhmann Richard Vencu Romain Beaumont
Robert Kaczmarczyk Clayton Mullis Aarush Katta Theo
Coombes Jenia Jitsev and Aran Komatsuzaki Laion
400m Open dataset of clipﬁltered 400 million imagetext
pairs 2021 6 7
79 Karen Simonyan and Andrew Zisserman Very deep con
volutional networks for largescale image recognition In
Yoshua Bengio and Yann LeCun editors Int Conf Learn
Represent  2015 29 43 44 45
80 Abhishek Sinha Jiaming Song Chenlin Meng and Stefano
Ermon D2C diffusiondenoising models for fewshot con
ditional generation CoRR  abs210606819 2021 3
81 Charlie Snell Alien Dreams An Emerging Art Scene
https    ml  berkeley  edu  blog  posts 
clipart  2021 Online accessed November2021
2
82 Jascha SohlDickstein Eric A Weiss Niru Mah
eswaranathan and Surya Ganguli Deep unsupervised
learning using nonequilibrium thermodynamics CoRR 
abs150303585 2015 1 3 4 18
83 Kihyuk Sohn Honglak Lee and Xinchen Yan Learn
ing structured output representation using deep conditional
generative models In C Cortes N Lawrence D Lee
M Sugiyama and R Garnett editors Advances in Neural
Information Processing Systems  volume 28 Curran Asso
ciates Inc 2015 4
84 Jiaming Song Chenlin Meng and Stefano Ermon Denois
ing diffusion implicit models In ICLR  OpenReviewnet
2021 3 5 6 22
85 Yang Song Jascha SohlDickstein Diederik P Kingma
Abhishek Kumar Stefano Ermon and Ben Poole Score
based generative modeling through stochastic differential
equations CoRR  abs201113456 2020 1 3 4 18
86 Emma Strubell Ananya Ganesh and Andrew McCallum
Energy and policy considerations for modern deep learn
ing research In The ThirtyFourth AAAI Conference on
Artiﬁcial Intelligence AAAI 2020 The ThirtySecond In
novative Applications of Artiﬁcial Intelligence Conference
IAAI 2020 The Tenth AAAI Symposium on Educational
Advances in Artiﬁcial Intelligence EAAI 2020 New York
NY USA February 712 2020  pages 1369313696 AAAI
Press 2020 2
1287 Wei Sun and Tianfu Wu Learning layout and style re
conﬁgurable gans for controllable image synthesis CoRR 
abs200311571 2020 22 27
88 Roman Suvorov Elizaveta Logacheva Anton Mashikhin
Anastasia Remizova Arsenii Ashukha Aleksei Silvestrov
Naejin Kong Harshith Goka Kiwoong Park and Victor S
Lempitsky Resolutionrobust large mask inpainting with
fourier convolutions ArXiv  abs210907161 2021 8 9
26 32
89 Tristan Sylvain Pengchuan Zhang Yoshua Bengio R De
von Hjelm and Shikhar Sharma Objectcentric image gen
eration from layouts In ThirtyFifth AAAI Conference on
Artiﬁcial Intelligence AAAI 2021 ThirtyThird Conference
on Innovative Applications of Artiﬁcial Intelligence IAAI
2021 The Eleventh Symposium on Educational Advances
in Artiﬁcial Intelligence EAAI 2021 Virtual Event Febru
ary 29 2021  pages 26472655 AAAI Press 2021 20
22 27
90 Patrick Tinsley Adam Czajka and Patrick Flynn This face
does not exist but it might be yours identity leakage in
generative models In Proceedings of the IEEECVF Win
ter Conference on Applications of Computer Vision  pages
13201328 2021 9
91 Antonio Torralba and Alexei A Efros Unbiased look at
dataset bias In CVPR 2011  pages 15211528 IEEE 2011
9
92 Arash Vahdat and Jan Kautz NV AE A deep hierarchical
variational autoencoder In NeurIPS  2020 3
93 Arash Vahdat Karsten Kreis and Jan Kautz Score
based generative modeling in latent space CoRR 
abs210605931 2021 2 3 5 6
94 Aaron van den Oord Nal Kalchbrenner Lasse Espeholt
koray kavukcuoglu Oriol Vinyals and Alex Graves Con
ditional image generation with pixelcnn decoders In Ad
vances in Neural Information Processing Systems  2016 3
95 A aron van den Oord Nal Kalchbrenner and Koray
Kavukcuoglu Pixel recurrent neural networks CoRR 
abs160106759 2016 3
96 A aron van den Oord Oriol Vinyals and Koray
Kavukcuoglu Neural discrete representation learning In
NIPS  pages 63066315 2017 2 4 29
97 Ashish Vaswani Noam Shazeer Niki Parmar Jakob
Uszkoreit Llion Jones Aidan N Gomez Lukasz Kaiser
and Illia Polosukhin Attention is all you need In NIPS 
pages 59986008 2017 3 4 5 7
98 Rivers Have Wings Tweet on Classiﬁerfree
guidance for autoregressive models https 
  twitter  com  RiversHaveWings  status 
1478093658716966912  2022 6
99 Thomas Wolf Lysandre Debut Victor Sanh Julien Chau
mond Clement Delangue Anthony Moi Pierric Cistac
Tim Rault R emi Louf Morgan Funtowicz and Jamie
Brew Huggingfaces transformers Stateoftheart natural
language processing CoRR  abs191003771 2019 26
100 Zhisheng Xiao Karsten Kreis Jan Kautz and Arash Vah
dat V AEBM A symbiosis between variational autoen
coders and energybased models In 9th International Conference on Learning Representations ICLR 2021 Virtual
Event Austria May 37 2021  OpenReviewnet 2021 6
101 Wilson Yan Yunzhi Zhang Pieter Abbeel and Aravind
Srinivas Videogpt Video generation using VQV AE and
transformers CoRR  abs210410157 2021 3
102 Fisher Yu Yinda Zhang Shuran Song Ari Seff and Jianx
iong Xiao LSUN construction of a largescale image
dataset using deep learning with humans in the loop CoRR 
abs150603365 2015 5
103 Jiahui Yu Xin Li Jing Yu Koh Han Zhang Ruoming Pang
James Qin Alexander Ku Yuanzhong Xu Jason Baldridge
and Yonghui Wu Vectorquantized image modeling with
improved vqgan 2021 3 4
104 Jiahui Yu Zhe L Lin Jimei Yang Xiaohui Shen Xin Lu
and Thomas S Huang Freeform image inpainting with
gated convolution 2019 IEEECVF International Confer
ence on Computer Vision ICCV  pages 44704479 2019
9
105 K Zhang Jingyun Liang Luc Van Gool and Radu Timo
fte Designing a practical degradation model for deep blind
image superresolution ArXiv  abs210314006 2021 23
106 Richard Zhang Phillip Isola Alexei A Efros Eli Shecht
man and Oliver Wang The unreasonable effectiveness of
deep features as a perceptual metric In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog
nition CVPR  June 2018 3 8 19
107 Shengyu Zhao Jianwei Cui Yilun Sheng Yue Dong Xiao
Liang Eric IChao Chang and Yan Xu Large scale image
completion via comodulated generative adversarial net
works ArXiv  abs210310428 2021 9
108 Bolei Zhou Agata Lapedriza Aditya Khosla Aude Oliva
and Antonio Torralba Places A 10 million image database
for scene recognition IEEE Transactions on Pattern Anal
ysis and Machine Intelligence  4014521464 2018 8 9
26
109 Yufan Zhou Ruiyi Zhang Changyou Chen Chunyuan Li
Chris Tensmeyer Tong Yu Jiuxiang Gu Jinhui Xu and
Tong Sun LAFITE towards languagefree training for
texttoimage generation CoRR  abs211113792 2021 6
7 16
13Appendix
Figure 12 Convolutional samples from the semantic landscapes model as in Sec 432 ﬁnetuned on 5122images
14A painting of the last supper by Picasso 
An oil painting of a latent space An epic painting of Gandalf the Black
summoning thunder and lightning in the mountains 
A sunset over a mountain range vector image 
Figure 13 Combining classiﬁer free diffusion guidance with the convolutional sampling strategy from Sec 432 our 145B parameter
texttoimage model can be used for rendering images larger than the native 2562resolution the model was trained on
15A Changelog
Here we list changes between this version  httpsarxivorgabs211210752v2  of the paper and the
previous version iehttpsarxivorgabs211210752v1 
 We updated the results on texttoimage synthesis in Sec 43 which were obtained by training a new larger model 145B
parameters This also includes a new comparison to very recent competing methods on this task that were published on
arXiv at the same time as  59 109 or after  26 the publication of our work
 We updated results on classconditional synthesis on ImageNet in Sec 41 Tab 3 see also Sec D4 obtained by
retraining the model with a larger batch size The corresponding qualitative results in Fig 26 and Fig 27 were also
updated Both the updated texttoimage and the classconditional model now use classiﬁerfree guidance 32 as a
measure to increase visual ﬁdelity
 We conducted a user study following the scheme suggested by Saharia et al 72 which provides additional evaluation
for our inpainting Sec 45 and superresolution models Sec 44
 Added Fig 5 to the main paper moved Fig 18 to the appendix added Fig 13 to the appendix
B Detailed Information on Denoising Diffusion Models
Diffusion models can be speciﬁed in terms of a signaltonoise ratio SNR t 2
t
2
tconsisting of sequences tT
t1and
tT
t1which starting from a data sample x0 deﬁne a forward diffusion process qas
qxtjx0 Nxtjtx02
tI 4
with the Markov structure for st 
qxtjxs Nxtjtjsxs2
tjsI 5
tjst
s6
2
tjs2
t2
tjs2
s 7
Denoising diffusion models are generative models px0which revert this process with a similar Markov structure running
backward in time ie they are speciﬁed as
px0 Z
zpxTTY
t1pxt1jxt 8
The evidence lower bound ELBO associated with this model then decomposes over the discrete time steps as
logpx0KLqxTjx0jpxT TX
t1Eqxtjx0KLqxt1jxtx0jpxt1jxt 9
The priorpxTis typically choosen as a standard normal distribution and the ﬁrst term of the ELBO then depends only on
the ﬁnal signaltonoise ratio SNR T To minimize the remaining terms a common choice to parameterize pxt1jxtis to
specify it in terms of the true posterior qxt1jxtx0but with the unknown x0replaced by an estimate xxttbased on
the current step xt This gives 45
pxt1jxtqxt1jxtxxtt 10
Nxt1jxtt2
tjt12
t1
2
tI 11
where the mean can be expressed as
xtt tjt12
t1
2
txtt12
tjt1
2
txxtt 12
16In this case the sum of the ELBO simplify to
TX
t1Eqxtjx0KLqxt1jxtx0jpxt1 TX
t1ENj0I1
2SNRt1SNRtkx0xtx0ttk213
Following 30 we use the reparameterization
xtt  xttxxttt 14
to express the reconstruction term as a denoising objective
kx0xtx0ttk22
t
2
tktx0ttk215
and the reweighting which assigns each of the terms the same weight and results in Eq 1
17C Image Guiding Mechanisms
Samples 2562Guided Convolutional Samples 5122Convolutional Samples 5122
Figure 14 On landscapes convolutional sampling with unconditional models can lead to homogeneous and incoherent global structures
see column 2 L2guiding with a low resolution image can help to reestablish coherent global structures
An intriguing feature of diffusion models is that unconditional models can be conditioned at testtime 15 82 85 In
particular 15 presented an algorithm to guide both unconditional and conditional models trained on the ImageNet dataset
with a classiﬁer logpyjxt trained on each xtof the diffusion process We directly build on this formulation and introduce
posthoc imageguiding 
For an epsilonparameterized model with ﬁxed variance the guiding algorithm as introduced in 15 reads
 ztt q
12
trztlogpyjzt 16
This can be interpreted as an update correcting the score with a conditional distribution logpyjzt
So far this scenario has only been applied to singleclass classiﬁcation models We reinterpret the guiding distribution
pyjTDz0zt as a general purpose imagetoimage translation task given a target image y whereTcan be any
differentiable transformation adopted to the imagetoimage translation task at hand such as the identity a downsampling
operation or similar
18As an example we can assume a Gaussian guider with ﬁxed variance 2 1 such that
logpyjzt 1
2kyTDz0ztk2
2 17
becomes aL2regression objective
Fig 14 demonstrates how this formulation can serve as an upsampling mechanism of an unconditional model trained on
2562images where unconditional samples of size 2562guide the convolutional synthesis of 5122images and Tis a2
bicubic downsampling Following this motivation we also experiment with a perceptual similarity guiding and replace the
L2objective with the LPIPS 106 metric see Sec 44
19D Additional Results
D1 Choosing the SignaltoNoise Ratio for HighResolution Synthesis
KLreg wo rescaling KLreg w rescaling VQreg wo rescaling
Figure 15 Illustrating the effect of latent space rescaling on convolutional sampling here for semantic image synthesis on landscapes See
Sec 432 and Sec D1
As discussed in Sec 432 the signaltonoise ratio induced by the variance of the latent space  ie Varz2
t signiﬁcantly
affects the results for convolutional sampling For example when training a LDM directly in the latent space of a KL
regularized model see Tab 8 this ratio is very high such that the model allocates a lot of semantic detail early on in the
reverse denoising process In contrast when rescaling the latent space by the componentwise standard deviation of the
latents as described in Sec G the SNR is descreased We illustrate the effect on convolutional sampling for semantic image
synthesis in Fig 15 Note that the VQregularized space has a variance close to 1 such that it does not have to be rescaled
D2 Full List of all First Stage Models
We provide a complete list of various autoenconding models trained on the OpenImages dataset in Tab 8
D3 LayouttoImage Synthesis
Here we provide the quantitative evaluation and additional samples for our layouttoimage models from Sec 431 We
train a model on the COCO 4 and one on the OpenImages 49 dataset which we subsequently additionally ﬁnetune on
COCO Tab 9 shows the result Our COCO model reaches the performance of recent stateofthe art models in layoutto
image synthesis when following their training and evaluation protocol 89 When ﬁnetuning from the OpenImages model
we surpass these works Our OpenImages model surpasses the results of Jahn et al 37 by a margin of nearly 11 in terms of
FID In Fig 16 we show additional samples of the model ﬁnetuned on COCO
D4 ClassConditional Image Synthesis on ImageNet
Tab 10 contains the results for our classconditional LDM measured in FID and Inception score IS LDM8 requires
signiﬁcantly fewer parameters and compute requirements see Tab 18 to achieve very competitive performance Similar
to previous work we can further boost the performance by training a classiﬁer on each noise scale and guiding with it
20fjZj c RFID RIS PSNR PSIM SSIM
16VQGAN 23 16384 256 498  199 34 183042 051018
16VQGAN 23 1024 256 794  194 33 198043 050018
8DALLE 66 8192  3201  228 21 195051 073013
32 16384 16 3183 4040 107 1745 290 258048 041018
16 16384 8 515 14455 374 2083 361 173043 054018
8 16384 4 114 20192 397 2307 399 117036 065016
8 256 4 149 19420 387 2235 381 126037 062016
4 8192 3 058 22478 535 2743 426 053021 082010
4y8192 3 106 22194 458 2521 417 072026 076012
4 256 3 047 22381 458 2643 422 062024 080011
2 2048 2 016 23275 509 3085 412 027012 091005
2 64 2 040 22662 483 2913 346 038013 090005
32 KL 64 204 18953 368 2227 393 141040 061017
32 KL 16 73 13275 271 2038 356 188045 053018
16 KL 16 087 21031 397 2408 422 107036 068015
16 KL 8 263 17868 408 2194 392 149042 059017
8 KL 4 090 20990 492 2419 419 102035 069015
4 KL 3 027 22757 489 2753 454 055024 082011
2 KL 2 0086 23266 516 3247 419 020009 093004
Table 8 Complete autoencoder zoo trained on OpenImages evaluated on ImageNetVal ydenotes an attentionfree autoencoder
layouttoimage synthesis on the COCO dataset
Figure 16 More samples from our best model for layouttoimage synthesis LDM4  which was trained on the OpenImages dataset and
ﬁnetuned on the COCO dataset Samples generated with 100 DDIM steps and  0 Layouts are from the COCO validation set
see Sec C Unlike the pixelbased methods this classiﬁer is trained very cheaply in latent space For additional qualitative
results see Fig 26 and Fig 27
21COCO 256256 OpenImages 256256 OpenImages 512512
Method FID FID FID
LostGANV2 87 4255  
OCGAN 89 4165  
SPADE 62 4111  
VQGANT 37 5658 4533 4811
LDM8 100 steps ours 4206y 
LDM4 200 steps ours 40913202 3580
Table 9 Quantitative comparison of our layouttoimage models on the COCO 4 and OpenImages 49 datasetsy Training from scratch
on COCO Finetuning from OpenImages
Method FID IS Precision RecallNparams
SR3 72 1130    625M 
ImageBART 21 2119    35B 
ImageBART 21 744    35B 005 acc rate
VQGANT 23 1704 706 18   13B 
VQGANT 23 588 3048 36   13B 005 acc rate
BigGandeep 3 695 2036 26 087 028 340M 
ADM 15 1094 10098 069 063 554M 250 DDIM steps
ADMG 15 459 1867 082 052 608M 250 DDIM steps
ADMGADMU 15 385 22172 084 053 na 2 250 DDIM steps
CDM 31 488 15871 226   na 2 100 DDIM steps
LDM8 ours 1741 7292 26 065 062 395M 200 DDIM steps 29M train steps batch size 64
LDM8G ours 811 19043 260 083 036 506M 200 DDIM steps classiﬁer scale 10 29M train steps batch size 64
LDM8 ours 1551 7903 103 065 063 395M 200 DDIM steps 48M train steps batch size 64
LDM8G ours 776 20952 424 084 035 506M 200 DDIM steps classiﬁer scale 10 48M train steps batch size 64
LDM4 ours 1056 10349 124 071 062 400M 250 DDIM steps 178K train steps batch size 1200
LDM4G ours 395 17822 243 081 055 400M 250 DDIM steps unconditional guidance 32 scale 125 178K train steps batch size 1200
LDM4G ours 360 24767 559 087 048 400M 250 DDIM steps unconditional guidance 32 scale 15 178K train steps batch size 1200
Table 10 Comparison of a classconditional ImageNet LDM with recent stateoftheart methods for classconditional image generation
on the ImageNet 12 dataset Classiﬁer rejection sampling with the given rejection rate as proposed in 67
D5 Sample Quality vs V100 Days Continued from Sec 41
Figure 17 For completeness we also report the training progress of classconditional LDMs on the ImageNet dataset for a ﬁxed number
of 35 V100 days Results obtained with 100 DDIM steps 84 and  0 FIDs computed on 5000 samples for efﬁciency reasons
For the assessment of sample quality over the training progress in Sec 41 we reported FID and IS scores as a function
of train steps Another possibility is to report these metrics over the used resources in V100 days Such an analysis is
additionally provided in Fig 17 showing qualitatively similar results
22Method FID IS PSNR SSIM
Image Regression 72 152 1211 279 0801
SR3 72 52 1801 264 0762
LDM4 ours 100 steps 28y48z1663 244 38 069014
LDM4 ours 50 steps guiding 44y64z1537 258 37 074012
LDM4 ours 100 steps guiding 44y64z1541 257 37 073012
LDM4 ours 100 steps 15 ep 26y46z16976 503 24438 069014
PixelDM 100 steps 15 ep 51y 71z16306 467 24133 059012
Table 114upscaling results on ImageNetVal  2562y FID features computed on validation splitz FID features computed on train
split We also include a pixelspace baseline that receives the same amount of compute as LDM4  The last two rows received 15 epochs
of additional training compared to the former results
D6 SuperResolution
For better comparability between LDMs and diffusion models in pixel space we extend our analysis from Tab 5 by
comparing a diffusion model trained for the same number of steps and with a comparable number1of parameters to our
LDM The results of this comparison are shown in the last two rows of Tab 11 and demonstrate that LDM achieves better
performance while allowing for signiﬁcantly faster sampling A qualitative comparison is given in Fig 20 which shows
random samples from both LDM and the diffusion model in pixel space
D61 LDMBSR General Purpose SR Model via Diverse Image Degradation
bicubic LDMSR LDMBSR
Figure 18 LDMBSR generalizes to arbitrary inputs and can be used as a generalpurpose upsampler upscaling samples from a class
conditional LDM image cf Fig 4 to 10242resolution In contrast using a ﬁxed degradation process see Sec 44 hinders generalization
To evaluate generalization of our LDMSR we apply it both on synthetic LDM samples from a classconditional ImageNet
model Sec 41 and images crawled from the internet Interestingly we observe that LDMSR trained only with a bicubicly
downsampled conditioning as in 72 does not generalize well to images which do not follow this preprocessing Hence to
obtain a superresolution model for a wide range of real world images which can contain complex superpositions of camera
noise compression artifacts blurr and interpolations we replace the bicubic downsampling operation in LDMSR with the
degration pipeline from 105 The BSRdegradation process is a degradation pipline which applies JPEG compressions
noise camera sensor noise different image interpolations for downsampling Gaussian blur kernels and Gaussian noise in a
random order to an image We found that using the bsrdegredation process with the original parameters as in 105 leads to
a very strong degradation process Since a more moderate degradation process seemed apppropiate for our application we
adapted the parameters of the bsrdegradation our adapted degradation process can be found in our code base at https
githubcomCompVislatentdiffusion  Fig 18 illustrates the effectiveness of this approach by directly
comparing LDMSR with LDMBSR  The latter produces images much sharper than the models conﬁned to a ﬁxed pre
processing making it suitable for realworld applications Further results of LDMBSR are shown on LSUNcows in Fig 19
1It is not possible to exactly match both architectures since the diffusion model operates in the pixel space
23E Implementation Details and Hyperparameters
E1 Hyperparameters
We provide an overview of the hyperparameters of all trained LDM models in Tab 12 Tab 13 Tab 14 and Tab 15
CelebAHQ 256256 FFHQ 256256 LSUNChurches 256256 LSUNBedrooms 256256
f 4 4 8 4
zshape 64643 64643  64643
jZj 8192 8192  8192
Diffusion steps 1000 1000 1000 1000
Noise Schedule linear linear linear linear
Nparams 274M 274M 294M 274M
Channels 224 224 192 224
Depth 2 2 2 2
Channel Multiplier 1234 1234 12244 1234
Attention resolutions 32 16 8 32 16 8 32 16 8 4 32 16 8
Head Channels 32 32 24 32
Batch Size 48 42 96 48
Iterations410k 635k 500k 19M
Learning Rate 96e5 84e5 5e5 96e5
Table 12 Hyperparameters for the unconditional LDMs producing the numbers shown in Tab 1 All models trained on a single NVIDIA
A100
LDM1 LDM2 LDM4 LDM8 LDM16 LDM32
zshape 2562563 1281282 64643 32324 16168 88832
jZj  2048 8192 16384 16384 16384
Diffusion steps 1000 1000 1000 1000 1000 1000
Noise Schedule linear linear linear linear linear linear
Model Size 396M 391M 391M 395M 395M 395M
Channels 192 192 192 256 256 256
Depth 2 2 2 2 2 2
Channel Multiplier 112244 12244 1235 124 124 124
Number of Heads 1 1 1 1 1 1
Batch Size 7 9 40 64 112 112
Iterations 2M 2M 2M 2M 2M 2M
Learning Rate 49e5 63e5 8e5 64e5 45e5 45e5
Conditioning CA CA CA CA CA CA
CAresolutions 32 16 8 32 16 8 32 16 8 32 16 8 16 8 4 8 4 2
Embedding Dimension 512 512 512 512 512 512
Transformers Depth 1 1 1 1 1 1
Table 13 Hyperparameters for the conditional LDMs trained on the ImageNet dataset for the analysis in Sec 41 All models trained on a
single NVIDIA A100
E2 Implementation Details
E21 Implementations of for conditional LDMs
For the experiments on texttoimage and layouttoimage Sec 431 synthesis we implement the conditioner as an
unmasked transformer which processes a tokenized version of the input yand produces an output y where2
RMd More speciﬁcally the transformer is implemented from Ntransformer blocks consisting of global selfattention
layers layernormalization and positionwise MLPs as follows2
2adapted from httpsgithubcomlucidrainsxtransformers
24LDM1 LDM2 LDM4 LDM8 LDM16 LDM32
zshape 2562563 1281282 64643 32324 16168 88832
jZj  2048 8192 16384 16384 16384
Diffusion steps 1000 1000 1000 1000 1000 1000
Noise Schedule linear linear linear linear linear linear
Model Size 270M 265M 274M 258M 260M 258M
Channels 192 192 224 256 256 256
Depth 2 2 2 2 2 2
Channel Multiplier 112244 12244 1234 124 124 124
Attention resolutions 32 16 8 32 16 8 32 16 8 32 16 8 16 8 4 8 4 2
Head Channels 32 32 32 32 32 32
Batch Size 9 11 48 96 128 128
Iterations500k 500k 500k 500k 500k 500k
Learning Rate 9e5 11e4 96e5 96e5 13e4 13e4
Table 14 Hyperparameters for the unconditional LDMs trained on the CelebA dataset for the analysis in Fig 7 All models trained on a
single NVIDIA A100 All models are trained for 500k iterations If converging earlier we used the best checkpoint for assessing the
provided FID scores
Task TexttoImage LayouttoImage ClassLabeltoImage Super Resolution Inpainting SemanticMaptoImage
Dataset LAION OpenImages COCO ImageNet ImageNet Places Landscapes
f 8 4 8 4 4 4 8
zshape 32324 64643 32324 64643 64643 64643 32324
jZj  8192 16384 8192 8192 8192 16384
Diffusion steps 1000 1000 1000 1000 1000 1000 1000
Noise Schedule linear linear linear linear linear linear linear
Model Size 145B 306M 345M 395M 169M 215M 215M
Channels 320 128 192 192 160 128 128
Depth 2 2 2 2 2 2 2
Channel Multiplier 1244 1234 124 1235 1224 148 148
Number of Heads 8 1 1 1 1 1 1
Dropout   01    
Batch Size 680 24 48 1200 64 128 48
Iterations 390K 44M 170K 178K 860K 360K 360K
Learning Rate 10e4 48e5 48e5 10e4 64e5 10e6 48e5
Conditioning CA CA CA CA concat concat concat
CAresolutions 32 16 8 32 16 8 32 16 8 32 16 8   
Embedding Dimension 1280 512 512 512   
Transformer Depth 1 3 2 1   
Table 15 Hyperparameters for the conditional LDMs from Sec 4 All models trained on a single NVIDIA A100 except for the inpainting
model which was trained on eight V100
 TokEmb y PosEmby 18
fori 1N 
1 LayerNorm  19
2 MultiHeadSelfAttention 1  20
3 LayerNorm 2 21
 MLP3 2 22
 LayerNorm  23
24
Withavailable the conditioning is mapped into the UNet via the crossattention mechanism as depicted in Fig 3 We
modify the ablated UNet 15 architecture and replace the selfattention layer with a shallow unmasked transformer
consisting of Tblocks with alternating layers of i selfattention ii a positionwise MLP and iii a crossattention layer
25see Tab 16 Note that without ii and iii this architecture is equivalent to the ablated UNet
While it would be possible to increase the representational power of by additionally conditioning on the time step t we
do not pursue this choice as it reduces the speed of inference We leave a more detailed analysis of this modiﬁcation to future
work
For the texttoimage model we rely on a publicly available3tokenizer 99 The layouttoimage model discretizes the
spatial locations of the bounding boxes and encodes each box as a lbc tuple where ldenotes the discrete topleft and b
the bottomright position Class information is contained in c
See Tab 17 for the hyperparameters of and Tab 13 for those of the UNet for both of the above tasks
Note that the classconditional model as described in Sec 41 is also implemented via crossattention where is a single
learnable embedding layer with a dimensionality of 512 mapping classes yto2R1512
input Rhwc
LayerNorm Rhwc
Conv1x1 Rhwdnh
Reshape Rhwdnh
T8

SelfAttention
MLP
CrossAttentionRhwdnh
Rhwdnh
Rhwdnh
Reshape Rhwdnh
Conv1x1 Rhwc
Table 16 Architecture of a transformer block as described in Sec E21 replacing the selfattention layer of the standard ablated UNet
architecture 15 Here nhdenotes the number of attention heads and dthe dimensionality per head
TexttoImage LayouttoImage
seqlength 77 92
depthN 32 16
dim 1280 512
Table 17 Hyperparameters for the experiments with transformer encoders in Sec 43
E22 Inpainting
For our experiments on imageinpainting in Sec 45 we used the code of 88 to generate synthetic masks We use a ﬁxed
set of 2k validation and 30k testing samples from Places 108 During training we use random crops of size 256256
and evaluate on crops of size 512512 This follows the training and testing protocol in 88 and reproduces their reported
metrics seeyin Tab 7 We include additional qualitative results of LDM4 w attn in Fig 21 and of LDM4 wo attn big
w ft in Fig 22
E3 Evaluation Details
This section provides additional details on evaluation for the experiments shown in Sec 4
E31 Quantitative Results in Unconditional and ClassConditional Image Synthesis
We follow common practice and estimate the statistics for calculating the FID Precision and Recallscores 2950 shown in
Tab 1 and 10 based on 50k samples from our models and the entire training set of each of the shown datasets For calculating
FID scores we use the torchfidelity package 60 However since different data processing pipelines might lead to
different results 64 we also evaluate our models with the script provided by Dhariwal and Nichol 15 We ﬁnd that results
3httpshuggingfacecotransformersmodel_docberthtmlberttokenizerfast
26mainly coincide except for the ImageNet and LSUNBedrooms datasets where we notice slightly varying scores of 776
torchfidelity  vs 777 Nichol and Dhariwal and 295 vs 30 For the future we emphasize the importance of a
uniﬁed procedure for sample quality assessment Precision and Recall are also computed by using the script provided by
Nichol and Dhariwal
E32 TexttoImage Synthesis
Following the evaluation protocol of 66 we compute FID and Inception Score for the TexttoImage models from Tab 2 by
comparing generated samples with 30000 samples from the validation set of the MSCOCO dataset 51 FID and Inception
Scores are computed with torchfidelity 
E33 LayouttoImage Synthesis
For assessing the sample quality of our LayouttoImage models from Tab 9 on the COCO dataset we follow common
practice 37 87 89 and compute FID scores the 2048 unaugmented examples of the COCO Segmentation Challenge split
To obtain better comparability we use the exact same samples as in 37 For the OpenImages dataset we similarly follow
their protocol and use 2048 centercropped test images from the validation set
E34 Super Resolution
We evaluate the superresolution models on ImageNet following the pipeline suggested in 72 ie images with a shorter
size less than 256px are removed both for training and evaluation On ImageNet the lowresolution images are produced
using bicubic interpolation with antialiasing FIDs are evaluated using torchfidelity 60 and we produce samples
on the validation split For FID scores we additionally compare to reference features computed on the train split see Tab 5
and Tab 11
E35 Efﬁciency Analysis
For efﬁciency reasons we compute the sample quality metrics plotted in Fig 6 17 and 7 based on 5k samples Therefore
the results might vary from those shown in Tab 1 and 10 All models have a comparable number of parameters as provided
in Tab 13 and 14 We maximize the learning rates of the individual models such that they still train stably Therefore the
learning rates slightly vary between different runs cf Tab 13 and 14
E36 User Study
For the results of the user study presented in Tab 4 we followed the protocoll of 72 and and use the 2alternative forcechoice
paradigm to assess human preference scores for two distinct tasks In Task1 subjects were shown a low resolutionmasked
image between the corresponding ground truth high resolutionunmasked version and a synthesized image which was gen
erated by using the middle image as conditioning For SuperResolution subjects were asked Which of the two images is a
better high quality version of the low resolution image in the middle  For Inpainting we asked Which of the two images
contains more realistic inpainted regions of the image in the middle  In Task2 humans were similarly shown the low
resmasked version and asked for preference between two corresponding images generated by the two competing methods
As in 72 humans viewed the images for 3 seconds before responding
27F Computational Requirements
Method Generator Classiﬁer Overall Inference Nparams FID IS PrecisionRecall
Compute Compute Compute Throughput
LSUN Churches 2562
StyleGAN2 42y64  64  59M 386   
LDM8 ours 100 steps 410K 18  18 680 256M 402  064 052
LSUN Bedrooms 2562
ADM 15y1000 steps 232  232 003 552M 19  066 051
LDM4 ours 200 steps 19M 60  55 107 274M 295  066 048
CelebAHQ 2562
LDM4 ours 500 steps 410K 144  144 043 274M 511  072 049
FFHQ 2562
StyleGAN2 42 3213z 3213y 59M 38   
LDM4 ours 200 steps 635K 26  26 107 274M 498  073 050
ImageNet 2562
VQGANf4 ours ﬁrst stage 29  29  55M 058yy  
VQGANf8 ours ﬁrst stage 66  66  68M 114yy  
BigGANdeep 3y128256 128256  340M 695 2036 26 087 028
ADM 15 250 stepsy916  916 012 554M 1094 10098 069 063
ADMG 15 25 stepsy916 46 962 07 608M 558  081 049
ADMG 15 250 stepsy916 46 962 007 608M 459 1867 082 052
ADMGADMU 15 250 stepsy329 30 349 na na 385 22172 084 053
LDM8G ours 100 29M 79 12 91 193 506M 811 1904 26 083 036
LDM8 ours 200 ddim steps 29M batch size 64 79  79 19 395M 1741 7292 065 062
LDM4 ours 250 ddim steps 178K batch size 1200 271  271 07 400M 1056 10349 124 071 062
LDM4G ours 250 ddim steps 178K batch size 1200 classiﬁerfree guidance 32 scale 125 271  271 04 400M 395 17822 243 081 055
LDM4G ours 250 ddim steps 178K batch size 1200 classiﬁerfree guidance 32 scale 15 271  271 04 400M 360 24767 559 087 048
Table 18 Comparing compute requirements during training and inference throughput with stateoftheart generative models Compute
during training in V100days numbers of competing methods taken from 15 unless stated differently Throughput measured in sam
plessec on a single NVIDIA A100y Numbers taken from 15 z Assumed to be trained on 25M train examplesyy RFID vs ImageNet
validation set
In Tab 18 we provide a more detailed analysis on our used compute ressources and compare our best performing models
on the CelebAHQ FFHQ LSUN and ImageNet datasets with the recent state of the art models by using their provided
numbers cf 15 As they report their used compute in V100 days and we train all our models on a single NVIDIA A100
GPU we convert the A100 days to V100 days by assuming a 22speedup of A100 vs V100 744 To assess sample quality
we additionally report FID scores on the reported datasets We closely reach the performance of state of the art methods as
StyleGAN2 42 and ADM 15 while signiﬁcantly reducing the required compute resources
4This factor corresponds to the speedup of the A100 over the V100 for a UNet as deﬁned in Fig 1 in 74
28G Details on Autoencoder Models
We train all our autoencoder models in an adversarial manner following 23 such that a patchbased discriminator D 
is optimized to differentiate original images from reconstructions DEx To avoid arbitrarily scaled latent spaces we
regularize the latent zto be zero centered and obtain small variance by introducing an regularizing loss term Lreg
We investigate two different regularization methods i a lowweighted KullbackLeiblerterm between qEzjx 
NzEE2and a standard normal distribution Nz 01as in a standard variational autoencoder 46 69 and ii regu
larizing the latent space with a vector quantization layer by learning a codebook of jZjdifferent exemplars 96
To obtain highﬁdelity reconstructions we only use a very small regularization for both scenarios ie we either weight the
KLterm by a factor106or choose a high codebook dimensionality jZj
The full objective to train the autoencoding model EDreads
LAutoencoder  min
EDmax
 
LrecxDExLadvDEx  logD x LregxED
25
DM Training in Latent Space Note that for training diffusion models on the learned latent space we again distinguish two
cases when learning pzorpzjySec 43 i For a KLregularized latent space we sample zExExEx
whereN01 When rescaling the latent we estimate the componentwise variance
21
bchwX
bchwzbchw2
from the ﬁrst batch in the data where 1
bchwP
bchwzbchw The output ofEis scaled such that the rescaled latent has
unit standard deviation iez z
Ex
 ii For a VQregularized latent space we extract zbefore the quantization layer
and absorb the quantization operation into the decoder ie it can be interpreted as the ﬁrst layer of D
H Additional Qualitative Results
Finally we provide additional qualitative results for our landscapes model Fig 12 23 24 and 25 our classconditional
ImageNet model Fig 26  27 and our unconditional models for the CelebAHQ FFHQ and LSUN datasets Fig 28  31
Similar as for the inpainting model in Sec 45 we also ﬁnetuned the semantic landscapes model from Sec 432 directly on
5122images and depict qualitative results in Fig 12 and Fig 23 For our those models trained on comparably small datasets
we additionally show nearest neighbors in VGG 79 feature space for samples from our models in Fig 32  34
29bicubic LDMBSR
Figure 19 LDMBSR generalizes to arbitrary inputs and can be used as a generalpurpose upsampler upscaling samples from the LSUN
Cows dataset to 10242resolution
30input GT Pixel Baseline 1 Pixel Baseline 2 LDM 1 LDM 2
Figure 20 Qualitative superresolution comparison of two random samples between LDMSR and baselinediffusionmodel in Pixelspace
Evaluated on imagenet validationset after same amount of training steps
31input GT LaMa 88 LDM 1 LDM 2 LDM 3
Figure 21 Qualitative results on image inpainting In contrast to 88 our generative approach enables generation of multiple diverse
samples for a given input
32input result input result
Figure 22 More qualitative results on object removal as in Fig 11
33Semantic Synthesis on FlickrLandscapes 23  5122ﬁnetuning
Figure 23 Convolutional samples from the semantic landscapes model as in Sec 432 ﬁnetuned on 5122images
34Figure 24 A LDM trained on 2562resolution can generalize to larger resolution for spatially conditioned tasks such as semantic synthesis
of landscape images See Sec 432
35Semantic Synthesis on FlickrLandscapes 23
Figure 25 When provided a semantic map as conditioning our LDMs generalize to substantially larger resolutions than those seen during
training Although this model was trained on inputs of size 2562it can be used to create highresolution samples as the ones shown here
which are of resolution 1024384 36Random class conditional samples on the ImageNet dataset
Figure 26 Random samples from LDM4 trained on the ImageNet dataset Sampled with classiﬁerfree guidance 32 scale s 50and
200 DDIM steps with  10
37Random class conditional samples on the ImageNet dataset
Figure 27 Random samples from LDM4 trained on the ImageNet dataset Sampled with classiﬁerfree guidance 32 scale s 30and
200 DDIM steps with  10
38Random samples on the CelebAHQ dataset
Figure 28 Random samples of our best performing model LDM4 on the CelebAHQ dataset Sampled with 500 DDIM steps and  0
FID  515
39Random samples on the FFHQ dataset
Figure 29 Random samples of our best performing model LDM4 on the FFHQ dataset Sampled with 200 DDIM steps and  1 FID
 498
40Random samples on the LSUNChurches dataset
Figure 30 Random samples of our best performing model LDM8 on the LSUNChurches dataset Sampled with 200 DDIM steps and
 0FID  448
41Random samples on the LSUNBedrooms dataset
Figure 31 Random samples of our best performing model LDM4 on the LSUNBedrooms dataset Sampled with 200 DDIM steps and
 1FID  295
42Nearest Neighbors on the CelebAHQ dataset
Figure 32 Nearest neighbors of our best CelebAHQ model computed in the feature space of a VGG16 79 The leftmost sample is
from our model The remaining samples in each row are its 10 nearest neighbors
43Nearest Neighbors on the FFHQ dataset
Figure 33 Nearest neighbors of our best FFHQ model computed in the feature space of a VGG16 79 The leftmost sample is from our
model The remaining samples in each row are its 10 nearest neighbors
44Nearest Neighbors on the LSUNChurches dataset
Figure 34 Nearest neighbors of our best LSUNChurches model computed in the feature space of a VGG16 79 The leftmost sample
is from our model The remaining samples in each row are its 10 nearest neighbors
45
  Segment Anything
Alexander Kirillov124Eric Mintun2Nikhila Ravi12Hanzi Mao2Chloe Rolland3Laura Gustafson3
Tete Xiao3Spencer Whitehead Alexander C Berg WanYen Lo Piotr Doll ar4Ross Girshick4
1project lead2joint ﬁrst author3equal contribution4directional lead
Meta AI Research FAIR
b Model Segment Anything Model SAMpromptimagevalid maskimage encoderprompt encoderlightweight mask decoder
a Task promptable segmentationsegmentation promptimagemodelcat withblack earsvalid mask
c Data data engine top  dataset bottom1 billion masks11 million images privacy respectinglicensed imagesannotatetraindatamodelSegment Anything 1B SA1B
Figure 1 We aim to build a foundation model for segmentation by introducing three interconnected components a prompt
able segmentation task a segmentation model SAM that powers data annotation and enables zeroshot transfer to a range
of tasks via prompt engineering and a data engine for collecting SA1B our dataset of over 1 billion masks
Abstract
We introduce the Segment Anything SA project a new
task model and dataset for image segmentation Using our
efﬁcient model in a data collection loop we built the largest
segmentation dataset to date by far with over 1 billion
masks on 11M licensed and privacy respecting images The
model is designed and trained to be promptable so it can
transfer zeroshot to new image distributions and tasks We
evaluate its capabilities on numerous tasks and ﬁnd that
its zeroshot performance is impressive  often competitive
with or even superior to prior fully supervised results We
are releasing the Segment Anything Model SAM and cor
responding dataset SA1B of 1B masks and 11M images at
httpssegmentanythingcom to foster research into foun
dation models for computer vision
1 Introduction
Large language models pretrained on webscale datasets
are revolutionizing NLP with strong zeroshot and fewshot
generalization 10 These foundation models 8 can
generalize to tasks and data distributions beyond those seen
during training This capability is often implemented with
prompt engineering in which handcrafted text is used to
prompt the language model to generate a valid textual re
sponse for the task at hand When scaled and trained with
abundant text corpora from the web these models zero and
fewshot performance compares surprisingly well to evenmatching in some cases ﬁnetuned models 10 21 Empir
ical trends show this behavior improving with model scale
dataset size and total training compute 56 10 21 51
Foundation models have also been explored in computer
vision albeit to a lesser extent Perhaps the most promi
nent illustration aligns paired text and images from the web
For example CLIP 82 and ALIGN 55 use contrastive
learning to train text and image encoders that align the two
modalities Once trained engineered text prompts enable
zeroshot generalization to novel visual concepts and data
distributions Such encoders also compose effectively with
other modules to enable downstream tasks such as image
generation  eg DALLE 83 While much progress has
been made on vision and language encoders computer vi
sion includes a wide range of problems beyond this scope
and for many of these abundant training data does not exist
In this work our goal is to build a foundation model for
image segmentation  That is we seek to develop a prompt
able model and pretrain it on a broad dataset using a task
that enables powerful generalization With this model we
aim to solve a range of downstream segmentation problems
on new data distributions using prompt engineering
The success of this plan hinges on three components
taskmodel  and data  To develop them we address the
following questions about image segmentation
1 What task will enable zeroshot generalization
2 What is the corresponding model architecture
3 What data can power this task and model
1arXiv230402643v1  csCV  5 Apr 2023These questions are entangled and require a comprehen
sive solution We start by deﬁning a promptable segmenta
tiontask that is general enough to provide a powerful pre
training objective and to enable a wide range of downstream
applications This task requires a model that supports ﬂex
ible prompting and can output segmentation masks in real
time when prompted to allow for interactive use To train
our model we need a diverse largescale source of data 
Unfortunately there is no webscale data source for seg
mentation to address this we build a data engine ie
we iterate between using our efﬁcient model to assist in data
collection and using the newly collected data to improve the
model We introduce each interconnected component next
followed by the dataset we created and the experiments that
demonstrate the effectiveness of our approach
Task 2 In NLP and more recently computer vision
foundation models are a promising development that can
perform zeroshot and fewshot learning for new datasets
and tasks often by using prompting techniques Inspired
by this line of work we propose the promptable segmen
tation task  where the goal is to return a valid segmenta
tion mask given any segmentation prompt see Fig 1a A
prompt simply speciﬁes what to segment in an image eg
a prompt can include spatial or text information identifying
an object The requirement of a valid output mask means
that even when a prompt is ambiguous and could refer to
multiple objects for example a point on a shirt may in
dicate either the shirt or the person wearing it the output
should be a reasonable mask for at least one of those ob
jects We use the promptable segmentation task as both a
pretraining objective and to solve general downstream seg
mentation tasks via prompt engineering
Model 3 The promptable segmentation task and the goal
of realworld use impose constraints on the model architec
ture In particular the model must support ﬂexible prompts 
needs to compute masks in amortized realtime to allow in
teractive use and must be ambiguityaware  Surprisingly
we ﬁnd that a simple design satisﬁes all three constraints
a powerful image encoder computes an image embedding
a prompt encoder embeds prompts and then the two infor
mation sources are combined in a lightweight mask decoder
that predicts segmentation masks We refer to this model as
the Segment Anything Model or SAM see Fig 1b By
separating SAM into an image encoder and a fast prompt
encoder  mask decoder the same image embedding can
be reused and its cost amortized with different prompts
Given an image embedding the prompt encoder and mask
decoder predict a mask from a prompt in 50ms in a web
browser We focus on point box and mask prompts and
also present initial results with freeform text prompts To
make SAM ambiguityaware we design it to predict mul
tiple masks for a single prompt allowing SAM to naturally
handle ambiguity such as the shirt vs person exampleData engine 4 To achieve strong generalization to new
data distributions we found it necessary to train SAM on
a large and diverse set of masks beyond any segmenta
tion dataset that already exists While a typical approach
for foundation models is to obtain data online 82 masks
are not naturally abundant and thus we need an alternative
strategy Our solution is to build a data engine ie we
codevelop our model with modelintheloop dataset an
notation see Fig 1c Our data engine has three stages
assistedmanual semiautomatic  and fully automatic  In
the ﬁrst stage SAM assists annotators in annotating masks
similar to a classic interactive segmentation setup In the
second stage SAM can automatically generate masks for
a subset of objects by prompting it with likely object lo
cations and annotators focus on annotating the remaining
objects helping increase mask diversity In the ﬁnal stage
we prompt SAM with a regular grid of foreground points
yielding on average 100 highquality masks per image
Dataset 5 Our ﬁnal dataset SA1B includes more than
1Bmasks from 11M licensed and privacypreserving im
ages see Fig 2 SA1B collected fully automatically us
ing the ﬁnal stage of our data engine has 400 more masks
than any existing segmentation dataset 66 44 117 60
and as we verify extensively the masks are of high quality
and diversity Beyond its use in training SAM to be robust
and general we hope SA1B becomes a valuable resource
for research aiming to build new foundation models
Responsible AI 6 We study and report on potential fair
ness concerns and biases when using SA1B and SAM Im
ages in SA1B span a geographically and economically di
verse set of countries and we found that SAM performs sim
ilarly across different groups of people Together we hope
this will make our work more equitable for realworld use
cases We provide model and dataset cards in the appendix
Experiments 7 We extensively evaluate SAM First us
ing a diverse new suite of 23 segmentation datasets we ﬁnd
that SAM produces highquality masks from a single fore
ground point often only slightly below that of the manu
ally annotated ground truth Second we ﬁnd consistently
strong quantitative and qualitative results on a variety of
downstream tasks under a zeroshot transfer protocol using
prompt engineering including edge detection object pro
posal generation instance segmentation and a preliminary
exploration of texttomask prediction These results sug
gest that SAM can be used outofthebox with prompt en
gineering to solve a variety of tasks involving object and
image distributions beyond SAMs training data Neverthe
less room for improvement remains as we discuss in 8
Release We are releasing the SA1B dataset for research
purposes and making SAM available under a permissive
open license Apache 20 at httpssegmentanythingcom
We also showcase SAMs capabilities with an online demo
250 masks
 50100 masks
 100200 masks
 200300 masks
 300400 masks
 400500 masks
 500 masks
Figure 2 Example images with overlaid masks from our newly introduced dataset SA1B  SA1B contains 11M diverse
highresolution licensed and privacy protecting images and 11B highquality segmentation masks These masks were
annotated fully automatically by SAM and as we verify by human ratings and numerous experiments are of high quality and
diversity We group images by number of masks per image for visualization there are 100 masks per image on average
32 Segment Anything Task
We take inspiration from NLP where the next token pre
diction task is used for foundation model pretraining and
to solve diverse downstream tasks via prompt engineer
ing 10 To build a foundation model for segmentation
we aim to deﬁne a task with analogous capabilities
Task We start by translating the idea of a prompt from NLP
to segmentation where a prompt can be a set of foreground
 background points a rough box or mask freeform text
or in general any information indicating what to segment
in an image The promptable segmentation task  then is to
return a valid segmentation mask given any prompt  The re
quirement of a valid mask simply means that even when
a prompt is ambiguous and could refer to multiple objects
eg recall the shirt vs person example and see Fig 3
the output should be a reasonable mask for at least oneof
those objects This requirement is similar to expecting a lan
guage model to output a coherent response to an ambiguous
prompt We choose this task because it leads to a natural
pretraining algorithm anda general method for zeroshot
transfer to downstream segmentation tasks via prompting
Pretraining The promptable segmentation task suggests a
natural pretraining algorithm that simulates a sequence of
prompts  eg points boxes masks for each training sam
ple and compares the models mask predictions against the
ground truth We adapt this method from interactive seg
mentation 109 70 although unlike interactive segmenta
tion whose aim is to eventually predict a valid mask after
enough user input our aim is to always predict a valid mask
forany prompt even when the prompt is ambiguous  This
ensures that a pretrained model is effective in use cases that
involve ambiguity including automatic annotation as re
quired by our data engine 4 We note that performing well
at this task is challenging and requires specialized modeling
and training loss choices which we discuss in 3
Zeroshot transfer Intuitively our pretraining task en
dows the model with the ability to respond appropriately to
any prompt at inference time and thus downstream tasks
can be solved by engineering appropriate prompts For ex
ample if one has a bounding box detector for cats cat in
stance segmentation can be solved by providing the detec
tors box output as a prompt to our model In general a wide
array of practical segmentation tasks can be cast as prompt
ing In addition to automatic dataset labeling we explore
ﬁve diverse example tasks in our experiments in 7
Related tasks Segmentation is a broad ﬁeld theres in
teractive segmentation 57 109 edge detection 3 su
per pixelization 85 object proposal generation 2 fore
ground segmentation 94 semantic segmentation 90 in
stance segmentation 66 panoptic segmentation 59 etc
The goal of our promptable segmentation task is to produce
Figure 3 Each column shows 3 valid masks generated by
SAM from a single ambiguous point prompt green circle
a broadly capable model that can adapt to many though
not all existing and new segmentation tasks via prompt
engineering This capability is a form of task generaliza
tion 26 Note that this is different than previous work on
multitask segmentation systems In a multitask system a
single model performs a ﬁxed set of tasks eg joint seman
tic instance and panoptic segmentation 114 19 54 but
the training and test tasks are the same An important dis
tinction in our work is that a model trained for promptable
segmentation can perform a new different task at inference
time by acting as a component in a larger system eg to
perform instance segmentation a promptable segmentation
model is combined with an existing object detector
Discussion Prompting and composition are powerful tools
that enable a single model to be used in extensible ways po
tentially to accomplish tasks unknown at the time of model
design This approach is analogous to how other founda
tion models are used eg how CLIP 82 is the textimage
alignment component of the DALL E 83 image generation
system We anticipate that composable system design pow
ered by techniques such as prompt engineering will enable
a wider variety of applications than systems trained specif
ically for a ﬁxed set of tasks Its also interesting to com
pare promptable and interactive segmentation through the
lens of composition while interactive segmentation mod
els are designed with human users in mind a model trained
for promptable segmentation can also be composed into a
larger algorithmic system as we will demonstrate
4score
score
score
valid masksimage
image
encoder
image
embeddingmask points box textprompt encodermask decoder
convFigure 4 Segment Anything Model SAM overview A heavyweight image encoder outputs an image embedding that can
then be efﬁciently queried by a variety of input prompts to produce object masks at amortized realtime speed For ambiguous
prompts corresponding to more than one object SAM can output multiple valid masks and associated conﬁdence scores
3 Segment Anything Model
We next describe the Segment Anything Model SAM
for promptable segmentation SAM has three components
illustrated in Fig 4 an image encoder a ﬂexible prompt
encoder and a fast mask decoder We build on Transformer
vision models 14 33 20 62 with speciﬁc tradeoffs for
amortized realtime performance We describe these com
ponents at a highlevel here with details in A
Image encoder Motivated by scalability and powerful pre
training methods we use an MAE 47 pretrained Vision
Transformer ViT 33 minimally adapted to process high
resolution inputs 62 The image encoder runs once per
image and can be applied prior to prompting the model
Prompt encoder We consider two sets of prompts sparse
points boxes text and dense masks We represent
points and boxes by positional encodings 95 summed with
learned embeddings for each prompt type and freeform text
with an offtheshelf text encoder from CLIP 82 Dense
prompts  ie masks are embedded using convolutions and
summed elementwise with the image embedding
Mask decoder The mask decoder efﬁciently maps the im
age embedding prompt embeddings and an output token
to a mask This design inspired by 14 20 employs a
modiﬁcation of a Transformer decoder block 103 followed
by a dynamic mask prediction head Our modiﬁed decoder
block uses prompt selfattention and crossattention in two
directions prompttoimage embedding and viceversa to
update allembeddings After running two blocks we up
sample the image embedding and an MLP maps the output
token to a dynamic linear classiﬁer which then computes
the mask foreground probability at each image location
Resolving ambiguity With one output the model will av
erage multiple valid masks if given an ambiguous prompt
To address this we modify the model to predict multiple
output masks for a single prompt see Fig 3 We found
3 mask outputs is sufﬁcient to address most common cases
nested masks are often at most three deep whole part and
subpart During training we backprop only the minimumloss 15 45 64 over masks To rank masks the model pre
dicts a conﬁdence score  ie estimated IoU for each mask
Efﬁciency The overall model design is largely motivated
by efﬁciency Given a precomputed image embedding the
prompt encoder and mask decoder run in a web browser on
CPU in 50ms This runtime performance enables seam
less realtime interactive prompting of our model
Losses and training We supervise mask prediction with
the linear combination of focal loss 65 and dice loss 73
used in 14 We train for the promptable segmentation task
using a mixture of geometric prompts for text prompts see
75 Following 92 37 we simulate an interactive setup
by randomly sampling prompts in 11 rounds per mask al
lowing SAM to integrate seamlessly into our data engine
4 Segment Anything Data Engine
As segmentation masks are not abundant on the inter
net we built a data engine to enable the collection of our
11B mask dataset SA1B The data engine has three
stages 1 a modelassisted manual annotation stage 2 a
semiautomatic stage with a mix of automatically predicted
masks and modelassisted annotation and 3 a fully auto
matic stage in which our model generates masks without
annotator input We go into details of each next
Assistedmanual stage In the ﬁrst stage resembling clas
sic interactive segmentation a team of professional annota
tors labeled masks by clicking foreground  background ob
ject points using a browserbased interactive segmentation
tool powered by SAM Masks could be reﬁned using pixel
precise brush and eraser tools Our modelassisted an
notation runs in realtime directly inside a browser using
precomputed image embeddings enabling a truly interac
tive experience We did not impose semantic constraints for
labeling objects and annotators freely labeled both stuff
and things 1 We suggested annotators label objects
they could name or describe but did not collect these names
or descriptions Annotators were asked to label objects in
order of prominence and were encouraged to proceed to the
next image once a mask took over 30 seconds to annotate
5At the start of this stage SAM was trained using com
mon public segmentation datasets After sufﬁcient data an
notation SAM was retrained using only newly annotated
masks As more masks were collected the image encoder
was scaled from ViTB to ViTH and other architectural de
tails evolved in total we retrained our model 6 times Av
erage annotation time per mask decreased from 34 to 14
seconds as the model improved We note that 14 seconds
is 65faster than mask annotation for COCO 66 and
only 2slower than boundingbox labeling with extreme
points 76 71 As SAM improved the average number of
masks per image increased from 20 to 44 masks Overall
we collected 43M masks from 120k images in this stage
Semiautomatic stage In this stage we aimed to increase
thediversity of masks in order to improve our models
ability to segment anything To focus annotators on less
prominent objects we ﬁrst automatically detected conﬁdent
masks Then we presented annotators with images preﬁlled
with these masks and asked them to annotate any additional
unannotated objects To detect conﬁdent masks we trained
a bounding box detector 84 on all ﬁrst stage masks using a
generic object category During this stage we collected an
additional 59M masks in 180k images for a total of 102M
masks As in the ﬁrst stage we periodically retrained our
model on newly collected data 5 times Average annota
tion time per mask went back up to 34 seconds excluding
the automatic masks as these objects were more challeng
ing to label The average number of masks per image went
from 44 to 72 masks including the automatic masks
Fully automatic stage In the ﬁnal stage annotation was
fully automatic  This was feasible due to two major en
hancements to our model First at the start of this stage we
had collected enough masks to greatly improve the model
including the diverse masks from the previous stage Sec
ond by this stage we had developed the ambiguityaware
model which allowed us to predict valid masks even in am
biguous cases Speciﬁcally we prompted the model with a
3232 regular grid of points and for each point predicted
a set of masks that may correspond to valid objects With
the ambiguityaware model if a point lies on a part or sub
part our model will return the subpart part and whole ob
ject The IoU prediction module of our model is used to se
lectconﬁdent masks moreover we identiﬁed and selected
only stable masks we consider a mask stable if threshold
ing the probability map at 05and05 results in
similar masks Finally after selecting the conﬁdent and
stable masks we applied nonmaximal suppression NMS
to ﬁlter duplicates To further improve the quality of smaller
masks we also processed multiple overlapping zoomedin
image crops For further details of this stage see B We
applied fully automatic mask generation to all 11M images
in our dataset producing a total of 11B highquality masks
We describe and analyze the resulting dataset SA1B next
Figure 5 Imagesize normalized mask center distributions
5 Segment Anything Dataset
Our dataset SA1B consists of 11M diverse high
resolution licensed and privacy protecting images and
11B highquality segmentation masks collected with our
data engine We compare SA1B with existing datasets
and analyze mask quality and properties We are releasing
SA1B to aid future development of foundation models for
computer vision We note that SA1B will be released un
der a favorable license agreement for certain research uses
and with protections for researchers
Images  We licensed a new set of 11M images from a
provider that works directly with photographers These im
ages are high resolution 3300 4950 pixels on average
and the resulting data size can present accessibility and stor
age challenges Therefore we are releasing downsampled
images with their shortest side set to 1500 pixels Even af
ter downsampling our images are signiﬁcantly higher reso
lution than many existing vision datasets  eg COCO 66
images are 480640 pixels Note that most models today
operate on much lower resolution inputs Faces and vehicle
license plates have been blurred in the released images
Masks  Our data engine produced 11B masks 991 of
which were generated fully automatically Therefore the
quality of the automatic masks is centrally important We
compare them directly to professional annotations and look
at how various mask properties compare to prominent seg
mentation datasets Our main conclusion as borne out in
the analysis below and the experiments in 7 is that our
automatic masks are high quality and effective for training
models Motivated by these ﬁndings SA1B only includes
automatically generated masks
Mask quality To estimate mask quality we randomly sam
pled 500 images  50k masks and asked our professional
annotators to improve the quality of all masks in these im
ages Annotators did so using our model and pixelprecise
brush and eraser editing tools This procedure resulted
in pairs of automatically predicted and professionally cor
rected masks We computed IoU between each pair and
found that 94 of pairs have greater than 90 IoU and
97 of pairs have greater than 75 IoU For comparison
prior work estimates interannotator consistency at 8591
IoU 44 60 Our experiments in 7 conﬁrm by human rat
ings that mask quality is high relative to a variety of datasets
and that training our model on automatic masks is nearly as
good as using all masks produced by the data engine
6SA1B
11M images
1129M 11B masksLVIS v1
0120M images
15M masksCOCO
0123M images
09M masksADE20K
0028M images
07M masksOpen Images
1M images
27M masks
10 1150 51100 101200 200
Number of masks per image04080Percent of images
000 025 050 075
Relative segmentation mask size100
102Percent of masks
00 02 04 06 08
Concavity051015Percent of masksFigure 6 Dataset mask properties The legend references the number of images and masks in each dataset Note that SA1B
has 11more images and 400 more masks than the largest existing segmentation dataset Open Images 60
Per country
image count
 100k
 100k
 10k
 1k
RUS
THA
USA
ITA
GBR
DEU
ESP
IDN
UKR
FRA
JPN
MYS
TUR
IND
CHN
POL
NLD
VNM
BRA
CAN
GRC
AUS
PRT
CZE
BLR
ROU
KOR
ARE
AUT
SWE
TWN
HKG
CHE
ISR
SGP
HUN
BEL
HRV
BGR
PHL
KAZ
MEX
NOR
MMR
ZAF
SRB
DNK
MAR
FIN
LVA
50 most common countries ISO codes0200k400k600k800kNumber of images per countryAsia  Oceania
Africa
Europe
North America
Latin America  Caribbean
Figure 7 Estimated geographic distribution of SA1B images Most of the worlds countries have more than 1000 images in
SA1B and the three countries with the most images are from different parts of the world
Mask properties In Fig 5 we plot the spatial distribution
of object centers in SA1B compared to the largest existing
segmentation datasets Common photographer biases are
present in all datasets We observe that SA1B has greater
coverage of image corners compared to LVIS v1 44 and
ADE20K 117 the two most similarly distributed datasets
while COCO 66 and Open Images V5 60 have a more
prominent center bias In Fig 6 legend we compare these
datasets by size SA1B has 11 more images and 400 
more masks than the second largest Open Images On av
erage it has 36more masks per image than Open Images
The closest dataset in this respect ADE20K still has 35 
fewer masks per image Fig 6 left plots the masksper
image distribution Next we look at imagerelative mask
size square root of the mask area divided by image area
in Fig 6 middle As expected since our dataset has more
masks per image it also tends to include a greater percent
age of small and medium relativesize masks Finally to
analyze shape complexity we look at mask concavity 1
minus mask area divided by area of masks convex hull in
Fig 6 right Since shape complexity is correlated with
mask size we control for the datasets mask size distribu
tions by ﬁrst performing stratiﬁed sampling from binned
mask sizes We observe that the concavity distribution of
our masks is broadly similar to that of other datasets
6 Segment Anything RAI Analysis
We next perform a Responsible AI RAI analysis of our
work by investigating potential fairness concerns and bi
ases when using SA1B and SAM We focus on the geo
graphic and income distribution of SA1B and fairness of
SAM across protected attributes of people We also provide
dataset data annotation and model cards in FSA1B  images
 countries imgs masks SA1B COCO OI
Africa 54 300k 28M 28 30 17
Asia  Oceania 70 39M 423M 362 114 143
Europe 47 54M 540M 498 342 362
Latin America  Carib 42 380k 36M 35 31 50
North America 4830k 80M 77 483 428
high income countries 81 58M 598M 540 891 875
middle income countries 108 49M 499M 450 105 120
low income countries 28 100k 94M 09 04 05
Table 1 Comparison of geographic and income representa
tion SA1B has higher representation in Europe and Asia 
Oceania as well as middle income countries Images from
Africa Latin America  Caribbean as well as low income
countries are underrepresented in all datasets
Geographic and income representation We infer the
country images were photographed in using standard meth
ods see C In Fig 7 we visualize the percountry image
counts in SA1B left and the 50 countries with the most
images right We note that the topthree countries are
from different parts of the world Next in Table 1 we com
pare the geographic and income representation of SA1B
COCO 66 and Open Images 60 SA1B has a substan
tially higher percentage of images in Europe and Asia 
Oceania as well as in middle income countries All datasets
underrepresent Africa as well as low income countries We
note that in SA1B all regions including Africa have at
least 28 million masks 10 more than the total number of
masks of any previous dataset Finally we observe that the
average number of masks per image not shown is fairly
consistent across region and income 94108 per image
7mIoU at
1 point 3 points
perceived gender presentation
feminine 544 17 90406
masculine 557 17 90106
perceived age group
older 629 67 92613
middle 545 13 90205
young 542 22 91207mIoU at
1 point 3 points
perceived skin tone
1 529 22 91009
2 515 14 91105
3 522 19 91407
4 515 27 91710
5 524 42 92514
6 567 63 91224
Table 2 SAMs performance segmenting people across per
ceived gender presentation age group and skin tone 95
conﬁdence intervals are shown Within each grouping all
conﬁdence intervals overlap except older vs middle
Fairness in segmenting people We investigate potential
fairness concerns across perceived gender presentation per
ceived age group and perceived skin tone by measuring
the performance discrepancy of SAM between groups We
use the More Inclusive Annotations for People MIAP 87
dataset for gender presentation and age and a proprietary
dataset for skin tone see C Our evaluation uses simu
lated interactive segmentation with random sampling of 1
and 3 points see D Table 2 top left shows results for
perceived gender presentation We note that females have
been shown to be underrepresented in detection and seg
mentation datasets 115 but observe that SAM performs
similarly across groups We repeat the analysis for per
ceived age in Table 2 bottom left noting that those who
are perceived to be younger and older have been shown to
be underrepresented in largescale datasets 110 SAM per
forms best on those who are perceived older although the
conﬁdence interval is large Finally we repeat the anal
ysis for perceived skin tone in Table 2 right noting that
those with lighter apparent skin tones have been shown to
be overrepresented and those with darker skin tones under
represented in largescale datasets 110 As MIAP does
not contain perceived skin tone annotations we use a pro
prietary dataset that contains annotations for the perceived
Fitzpatrick skin type 36 which ranges from 1 lightest
skin tone to 6 darkest skin tone While the means vary
somewhat we do not ﬁnd a signiﬁcant difference across
groups We believe our ﬁndings stem from the nature of
the task and acknowledge biases may arise when SAM is
used as a component in larger systems Finally in C we
extend the analysis to segmenting clothing where we ﬁnd
an indication of bias across perceived gender presentation
7 ZeroShot Transfer Experiments
In this section we present zeroshot transfer experiments
with SAM the Segment Anything Model We consider ﬁve
tasks four of which differ signiﬁcantly from the promptable
segmentation task used to train SAM These experiments
evaluate SAM on datasets and tasks that were not seen during training our usage of zeroshot transfer follows its
usage in CLIP 82 The datasets may include novel image
distributions such as underwater or egocentric images  eg
Fig 8 that to our knowledge do not appear in SA1B
Our experiments begin by testing the core goal of
promptable segmentation producing a valid mask from any
prompt We emphasize the challenging scenario of a single
foreground point prompt since it is more likely to be am
biguous than other more speciﬁc prompts Next we present
a sequence of experiments that traverse low mid and high
level image understanding and roughly parallel the histori
cal development of the ﬁeld Speciﬁcally we prompt SAM
to 1 perform edge detection 2 segment everything ie
object proposal generation 3 segment detected objects
ie instance segmentation and 4 as a proofofconcept to
segment objects from freeform text These four tasks dif
fer signiﬁcantly from the promptable segmentation task that
SAM was trained on and are implemented via prompt engi
neering Our experiments conclude with an ablation study
Implementation Unless otherwise speciﬁed 1 SAM
uses an MAE 47 pretrained ViTH 33 image encoder
and 2 SAM was trained on SA1B noting that this dataset
includes only automatically generated masks from the ﬁnal
stage of our data engine For all other model and training
details such as hyperparameters refer to A
71 ZeroShot Single Point Valid Mask Evaluation
Task We evaluate segmenting an object from a single fore
ground point This task is illposed as one point can refer
to multiple objects Ground truth masks in most datasets
do not enumerate allpossible masks which can make au
tomatic metrics unreliable Therefore we supplement the
standard mIoU metric  ie the mean of all IoUs between
predicted and ground truth masks with a human study in
which annotators rate mask quality from 1 nonsense to 10
pixelperfect See D1 E and G for additional details
By default we sample points from the center of ground
truth masks at a maximal value of the masks interior dis
tance transform following the standard evaluation proto
col in interactive segmentation 92 Since SAM is capable
of predicting multiple masks we evaluate only the models
most conﬁdent mask by default The baselines are all
singlemask methods We compare mainly to RITM 92
a strong interactive segmenter that performs best on our
benchmark compared to other strong baselines 67 18
Datasets We use a newly compiled suite of 23 datasets
with diverse image distributions Fig 8 lists the datasets
and shows a sample from each one see appendix Table 7 for
more details We use all 23 datasets for mIoU evaluation
For the human study we use the subset listed in Fig 9b
due to the resource requirements of such studies This
subset includes both datasets for which SAM outperforms
and underperforms RITM according to automatic metrics
8ADE20K 117 BBBC038v1 12 Cityscapes 25 DOORS 80 DRAM 24 EgoHOS 113 GTEA 34 63 Hypersim 86
IBD 17 iShape 111 LVIS 44 NDD20 100 NDISPark 22 23 OVIS 81 PPDLS 74 Plittersdorf 46
STREETS 91 TimberSeg 38 TrashCan 52 VISOR 28 27 WoodScape 112 PIDRay 104 ZeroWastef 6
Figure 8 Samples from the 23 diverse segmentation datasets used to evaluate SAMs zeroshot transfer capabilities
20 0 20 40
IoU delta at 1 center pointGTEA 34 63TrashCan 52DRAM 24PIDRay 104Cityscapes 25WoodScape 112IBD 17EgoHOS 113Plittersdorf 46VISOR 28 27NDISPark 22 23Hypersim 86OVIS 81ADE20K 117iShape 111ZeroWastef 6STREETS 91LVIS 44NDD20 100TimberSeg 38DOORS 80BBBC038v1 12PPDLS 74
2141506558200603081518276170788891173185211289411447469
a SAM vs RITM 92 on 23 datasets
LVIS VISOR DRAM IBD NDD20 OVIS iShape
Datasets579Avg mask ratingGround Truth
SAM
SAM  single output
RITM
b Mask quality ratings by human annotators
123 5 9
Number of points5075mIoU 23 datasets
SAM oracle
SAM
RITM
SimpleClick
FocalClick
123 5 9
Number of points5075mIoU 23 datasets
SAM oracle
c Center points default d Random points
Figure 9 Point to mask evaluation on 23 datasets a Mean IoU of SAM and the strongest single point segmenter RITM 92
Due to ambiguity a single mask may not match ground truth circles show oracle results of the most relevant of SAMs 3
predictions b Perdataset comparison of mask quality ratings by annotators from 1 worst to 10 best All methods use
the ground truth mask center as the prompt c d mIoU with varying number of points SAM signiﬁcantly outperforms prior
interactive segmenters with 1 point and is on par with more points Low absolute mIoU at 1 point is the result of ambiguity
Results First we look at automatic evaluation on the full
suite of 23 datasets using mIoU We compare perdataset
results in Fig 9a against RITM SAM yields higher re
sults on 16 of the 23 datasets by as much as 47 IoU We
also present an oracle result in which the most relevant
of SAMs 3 masks is selected by comparing them to the
ground truth rather than selecting the most conﬁdent mask
This reveals the impact of ambiguity on automatic evalu
ation In particular with the oracle to perform ambiguity
resolution SAM outperforms RITM on alldatasets
Results of the human study are presented in Fig 9b Er
ror bars are 95 conﬁdence intervals for mean mask rat
ings all differences are signiﬁcant see E for details We
observe that the annotators consistently rate the quality of
SAMs masks substantially higher than the strongest base
line RITM An ablated ambiguityunaware version of
SAM with a single output mask has consistently lower rat
ings though still higher than RITM SAMs mean ratingsfall between 7 and 9 which corresponds to the qualitative
rating guideline  A high score 79 The object is identi
ﬁable and errors are small and rare  eg missing a small
heavily obscured disconnected component   These re
sults indicate that SAM has learned to segment valid masks
from a single point Note that for datasets like DRAM and
IBD where SAM is worse on automatic metrics it receives
consistently higher ratings in the human study 
Fig 9c shows additional baselines SimpleClick 67 and
FocalClick 18 which obtain lower single point perfor
mance than RITM and SAM As the number of points in
creases from 1 to 9 we observe that the gap between meth
ods decreases This is expected as the task becomes easier
also SAM is not optimized for the very high IoU regime
Finally in Fig 9d we replace the default center point sam
pling with random point sampling We observe that the gap
between SAM and the baselines grows and SAM is able to
achieve comparable results under either sampling method
9image ground truth SAM
Figure 10 Zeroshot edge prediction on BSDS500 SAM
was not trained to predict edge maps nor did it have access
to BSDS images or annotations during training
method year ODS OIS AP R50
HED 108 2015 788 808 840 923
EDETR 79 2022 840 858 896 930
zeroshot transfer methods
Sobel ﬁlter 1968 539   
Canny 13 1986 600 640 580 
FelzHutt 35 2004 610 640 560 
SAM 2023 768 786 794 928
Table 3 Zeroshot transfer to edge detection on BSDS500
72 ZeroShot Edge Detection
Approach We evaluate SAM on the classic lowlevel task
of edge detection using BSDS500 72 3 We use a sim
pliﬁed version of our automatic mask generation pipeline
Speciﬁcally we prompt SAM with a 16 16 regular grid of
foreground points resulting in 768 predicted masks 3 per
point Redundant masks are removed by NMS Then edge
maps are computed using Sobel ﬁltering of unthresholded
mask probability maps and standard lightweight postpro
cessing including edge NMS see D2 for details
Results We visualize representative edge maps in Fig 10
see Fig 15 for more Qualitatively we observe that even
though SAM was not trained for edge detection it produces
reasonable edge maps Compared to the ground truth SAM
predicts more edges including sensible ones that are not an
notated in BSDS500 This bias is reﬂected quantitatively in
Table 3 recall at 50 precision R50 is high at the cost of
precision SAM naturally lags behind stateoftheart meth
ods that learn the biases of BSDS500 ie which edges to
suppress Nevertheless SAM performs well compared to
pioneering deep learning methods such as HED 108 also
trained on BSDS500 and signiﬁcantly better than prior
though admittedly outdated zeroshot transfer methods
73 ZeroShot Object Proposals
Approach Next we evaluate SAM on the midlevel task
of object proposal generation 2 102 This task has played
an important role in object detection research serving as anmask AR1000
method all small med large freq com rare
ViTDetH 62 630 517 808 870 631 633 583
zeroshot transfer methods
SAM  single out 549 428 767 744 547 598 620
SAM 593 455 816 869 591 639 658
Table 4 Object proposal generation on LVIS v1 SAM is
applied zeroshot ie it was not trained for object proposal
generation nor did it access LVIS images or annotations
intermediate step in pioneering systems  eg 102 41 84
To generate object proposals we run a slightly modiﬁed
version of our automatic mask generation pipeline and out
put the masks as proposals see D3 for details
We compute the standard average recall AR metric on
LVIS v1 44 We focus on LVIS because its large number
of categories presents a challenging test We compare to
astrong baseline implemented as a ViTDet 62 detector
with cascade Mask RCNN 48 11 ViTH We note that
this baseline corresponds to the Detector Masquerading
as Proposal generator DMP method 16 that was shown
to game AR making it a truly demanding comparison
Results In Table 4 we see unsurprisingly that using the
detections from ViTDetH as object proposals  ie the
DMP method 16 that games AR performs the best over
all However SAM does remarkably well on several met
rics Notably it outperforms ViTDetH on medium and
large objects as well as rare and common objects In fact
SAM only underperforms ViTDetH on small objects and
frequent objects where ViTDetH can easily learn LVIS
speciﬁc annotation biases since it was trained on LVIS un
like SAM We also compare against an ablated ambiguity
unaware version of SAM single out which performs
signiﬁcantly worse than SAM on all AR metrics
74 ZeroShot Instance Segmentation
Approach Moving to higherlevel vision we use SAM
as the segmentation module of an instance segmenter The
implementation is simple we run a object detector the
ViTDet used before and prompt SAM with its output
boxes This illustrates composing SAM in a larger system
Results We compare the masks predicted by SAM and
ViTDet on COCO and LVIS in Table 5 Looking at the
mask AP metric we observe gaps on both datasets where
SAM is reasonably close though certainly behind ViTDet
By visualizing outputs we observed that SAM masks are
often qualitatively better than those of ViTDet with crisper
boundaries see D4 and Fig 16 To investigate this ob
servation we conducted an additional human study asking
annotators to rate the ViTDet masks and SAM masks on the
1 to 10 quality scale used before In Fig 11 we observe that
SAM consistently outperforms ViTDet in the human study
10COCO 66 LVIS v1 44
method AP APSAPMAPLAP APSAPMAPL
ViTDetH 62 510 320 543 689 466 350 580 663
zeroshot transfer methods segmentation module only
SAM 465 308 510 617 447 325 576 655
Table 5 Instance segmentation results SAM is prompted
with ViTDet boxes to do zeroshot segmentation The fully
supervised ViTDet outperforms SAM but the gap shrinks
on the higherquality LVIS masks Interestingly SAM out
performs ViTDet according to human ratings see Fig 11
1 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings86  006 LVIS GT
81  007 SAM
79  008 ViTDetH
76  012 COCO GT
Figure 11 Mask quality rating distribution from our human
study for ViTDet and SAM both applied to LVIS ground
truth boxes We also report LVIS and COCO ground truth
quality The legend shows rating means and 95 conﬁ
dence intervals Despite its lower AP Table 5 SAM has
higher ratings than ViTDet suggesting that ViTDet exploits
biases in the COCO and LVIS training data
We hypothesize that on COCO where the mask AP gap
is larger and the ground truth quality is relatively low as
borne out by the human study ViTDet learns the speciﬁc
biases of COCO masks SAM being a zeroshot method
is unable to exploit these generally undesirable biases
The LVIS dataset has higher quality ground truth but there
are still speciﬁc idiosyncrasies  eg masks do not contain
holes they are simple polygons by construction and biases
for modal vs amodal masks Again SAM is not trained to
learn these biases while ViTDet can exploit them
75 ZeroShot TexttoMask
Approach Finally we consider an even higherlevel task
segmenting objects from freeform text This experiment
is a proofofconcept of SAMs ability to process text
prompts While we used the exact same SAM in all prior
experiments for this one SAMs training procedure is mod
iﬁed to make it textaware but in a way that does not require
new text annotations Speciﬁcally for each manually col
lected mask with area larger than 1002we extract the CLIP
image embedding Then during training we prompt SAM
with the extracted CLIP image embeddings as its ﬁrst in
teraction The key observation here is that because CLIPs
image embeddings are trained to align with its textembed
dings we can train with image embeddings but use text
embeddings for inference That is at inference time we run
text through CLIPs text encoder and then give the resulting
text embedding as a prompt to SAM see D5 for details
     a wheel
3
     beaver tooth grille 3
     a wiper
7
     a wiper  point
 3
     wipers
7
     wipers  point
 3
Figure 12 Zeroshot texttomask SAM can work with
simple and nuanced text prompts When SAM fails to make
a correct prediction an additional point prompt can help
Results We show qualitative results in Fig 12 SAM
can segment objects based on simple text prompts like a
wheel as well as phrases like beaver tooth grille When
SAM fails to pick the right object from a text prompt only
an additional point often ﬁxes the prediction similar to 31
76 Ablations
We perform several ablations on our 23 dataset suite with
the single center point prompt protocol Recall that a sin
gle point may be ambiguous and that ambiguity may not
be represented in the ground truth which contains only a
single mask per point Since SAM is operating in a zero
shot transfer setting there can be systematic biases between
SAMs topranked mask vs the masks resulting from data
annotation guidelines We therefore additionally report the
best mask with respect to the ground truth oracle
Fig 13 left plots SAMs performance when trained on
cumulative data from the data engine stages We observe
that each stage increases mIoU When training with all three
stages the automatic masks vastly outnumber the manual
and semiautomatic masks To address this we found that
oversampling the manual and semiautomatic masks during
training by 10gave best results This setup complicates
training We therefore tested a fourth setup that uses only
the automatically generated masks With this data SAM
performs only marginally lower than using all data  05
mIoU Therefore by default we use only the automatically
generated masks to simplify the training setup
In Fig 13 middle we look at the impact of data volume
The full SA1B contains 11M images which we uniformly
subsample to 1M and 01M for this ablation At 01M im
ages we observe a large mIoU decline under all settings
However with 1M images about 10 of the full dataset
we observe results comparable to using the full dataset
This data regime which still includes approximately 100M
masks may be a practical setting for many use cases
11manual  semi
automatic automatic automatic
only
Training data stages506070mIoU 23 datasets1 point oracle
1 point
01M 1M 11M
Training images707580mIoU 23 datasets
1 point oracle
2 points3 points5 points
91M
ViTB308M
ViTL636M
ViTH
Number of parameters606570mIoU 23 datasets
1 point oracle
1 point
Figure 13 Ablation studies of our data engine stages image encoder scaling and training data scaling Left Each data
engine stage leads to improvements on our 23 dataset suite and training with only the automatic data our default yields
similar results to using data from all three stages Middle SAM trained with 10 of SA1B and full SA1B is comparable
We train with all 11M images by default but using 1M images is a reasonable practical setting Right Scaling SAMs image
encoder shows meaningful yet saturating gains Nevertheless smaller image encoders may be preferred in certain settings
Finally Fig 13 right shows results with ViTB ViTL
and ViTH image encoders ViTH improves substantially
over ViTB but has only marginal gains over ViTL Further
image encoder scaling does not appear fruitful at this time
8 Discussion
Foundation models Pretrained models have been adapted
to downstream tasks since the early days of machine learn
ing 99 This paradigm has become increasingly impor
tant in recent years with a growing emphasis on scale and
such models have recently been rebranded as founda
tion models ie models that are trained on broad data
at scale and are adaptable to a wide range of downstream
tasks 8 Our work correlates well with this deﬁnition
though we note that a foundation model for image segmen
tation is an inherently limited scope since it represents an
important yet fractional subset of computer vision We
also contrast one aspect of our approach with 8 which
emphasizes the role of selfsupervised learning in founda
tion models While our model is initialized with a self
supervised technique MAE 47 the vast majority of its
capabilities come from largescale supervised training In
cases where data engines can scale available annotations
like ours supervised training provides an effective solution
Compositionality Pretrained models can power new ca
pabilities even beyond ones imagined at the moment of
training One prominent example is how CLIP 82 is used
as a component in larger systems such as DALL E 83
Our goal is to make this kind of composition straightfor
ward with SAM We aim to achieve this by requiring SAM
to predict a valid mask for a wide range of segmentation
prompts The effect is to create a reliable interface between
SAM and other components For example MCC 106 can
easily use SAM to segment an object of interest and achieve
strong generalization to unseen objects for 3D reconstruc
tion from a single RGBD image In another example SAM
can be prompted with gaze points detected by a wearable
device enabling new applications Thanks to SAMs abil
ity to generalize to new domains like egocentric images
such systems work without need for additional trainingLimitations While SAM performs well in general it is
not perfect It can miss ﬁne structures hallucinates small
disconnected components at times and does not produce
boundaries as crisply as more computationally intensive
methods that zoomin eg 18 In general we expect
dedicated interactive segmentation methods to outperform
SAM when many points are provided eg 67 Unlike
these methods SAM is designed for generality and breadth
of use rather than high IoU interactive segmentation More
over SAM can process prompts in realtime but neverthe
less SAMs overall performance is not realtime when using
a heavy image encoder Our foray into the texttomask task
is exploratory and not entirely robust although we believe
it can be improved with more effort While SAM can per
form many tasks it is unclear how to design simple prompts
that implement semantic and panoptic segmentation Fi
nally there are domainspeciﬁc tools such as 7 that we
expect to outperform SAM in their respective domains
Conclusion The Segment Anything project is an attempt to
lift image segmentation into the era of foundation models
Our principal contributions are a new task promptable seg
mentation model SAM and dataset SA1B that make
this leap possible Whether SAM achieves the status of a
foundation model remains to be seen by how it is used in
the community but regardless we expect the perspective of
this work the release of over 1B masks and our promptable
segmentation model will help pave the path ahead
Acknowledgments We would like to thank Aaron Ad
cock and Jitendra Malik for helpful discussion We thank
Vaibhav Aggarwal and Yanghao Li for help with scal
ing the model We thank ChengYang Fu Jiabo Hu and
Robert Kuo for help with data annotation platform We
thank Allen Goodman and Bram Wasti for help in optimiz
ing webversion of our model Finally we thank Morteza
Behrooz Ashley Gabriel Ahuva Goldstand Sumanth Gur
ram Somya Jain Devansh Kukreja Joshua Lane Lilian
Luong Mallika Malhotra William Ngan Omkar Parkhi
Nikhil Raina Dirk Rowe Neil Sejoor Vanessa Stark Bala
Varadarajan and Zachary Winstrom for their help in mak
ing the demo dataset viewer and other assets and tooling
12References
1 Edward H Adelson On seeing stuff the perception of materials by
humans and machines Human vision and electronic imaging VI 
2001 5
2 Bogdan Alexe Thomas Deselaers and Vittorio Ferrari What is an
object CVPR  2010 4 10
3 Pablo Arbel aez Michael Maire Charless Fowlkes and Jitendra
Malik Contour detection and hierarchical image segmentation
TPAMI  2010 4 10 21 28
4 Jimmy Lei Ba Jamie Ryan Kiros and Geoffrey E Hinton Layer
normalization arXiv160706450  2016 16
5 Hangbo Bao Li Dong and Furu Wei BEiT BERT pretraining of
image transformers arXiv210608254  2021 17
6 Dina Bashkirova Mohamed Abdelfattah Ziliang Zhu James Akl
Fadi Alladkani Ping Hu Vitaly Ablavsky Berk Calli Sarah Adel
Bargal and Kate Saenko ZeroWaste dataset Towards deformable
object segmentation in cluttered scenes CVPR  2022 9 20
7 Stuart Berg Dominik Kutra Thorben Kroeger Christoph N
Straehle Bernhard X Kausler Carsten Haubold Martin Schiegg
Janez Ales Thorsten Beier Markus Rudy Kemal Eren Jaime I
Cervantes Buote Xu Fynn Beuttenmueller Adrian Wolny Chong
Zhang Ullrich Koethe Fred A Hamprecht and Anna Kreshuk
ilastik interactive machine learning for bioimage analysis Na
ture Methods  2019 12
8 Rishi Bommasani Drew A Hudson Ehsan Adeli Russ Altman
Simran Arora Sydney von Arx Michael S Bernstein Jeannette
Bohg Antoine Bosselut Emma Brunskill et al On the opportu
nities and risks of foundation models arXiv210807258  2021 1
12
9 Gustav Bredell Christine Tanner and Ender Konukoglu Iterative
interaction training for segmentation editing networks MICCAI 
2018 17
10 Tom Brown Benjamin Mann Nick Ryder Melanie Subbiah
Jared D Kaplan Prafulla Dhariwal Arvind Neelakantan Pranav
Shyam Girish Sastry Amanda Askell Sandhini Agarwal Ariel
HerbertV oss Gretchen Krueger Tom Henighan Rewon Child
Aditya Ramesh Daniel Ziegler Jeffrey Wu Clemens Winter Chris
Hesse Mark Chen Eric Sigler Mateusz Litwin Scott Gray Ben
jamin Chess Jack Clark Christopher Berner Sam McCandlish
Alec Radford Ilya Sutskever and Dario Amodei Language models
are fewshot learners NeurIPS  2020 1 4
11 Zhaowei Cai and Nuno Vasconcelos Cascade RCNN Delving into
high quality object detection CVPR  2018 10
12 Juan C Caicedo Allen Goodman Kyle W Karhohs Beth A Ci
mini Jeanelle Ackerman Marzieh Haghighi CherKeng Heng Tim
Becker Minh Doan Claire McQuin Mohammad Rohban Shan
tanu Singh and Anne E Carpenter Nucleus segmentation across
imaging experiments the 2018 data science bowl Nature Methods 
2019 9 19 20
13 John Canny A computational approach to edge detection TPAMI 
1986 10 21
14 Nicolas Carion Francisco Massa Gabriel Synnaeve Nicolas
Usunier Alexander Kirillov and Sergey Zagoruyko Endtoend
object detection with Transformers ECCV  2020 5 16 17
15 Guillaume Charpiat Matthias Hofmann and Bernhard Sch olkopf
Automatic image colorization via multimodal predictions ECCV 
2008 5 17
16 Neelima Chavali Harsh Agrawal Aroma Mahendru and Dhruv
Batra Objectproposal evaluation protocol is gameable CVPR 
2016 10 21
17 Jiazhou Chen Yanghui Xu Shufang Lu Ronghua Liang and Lian
gliang Nan 3D instance segmentation of MVS buildings IEEE
Transactions on Geoscience and Remote Sensing  2022 9 19 20
23 24
18 Xi Chen Zhiyan Zhao Yilei Zhang Manni Duan Donglian Qi and
Hengshuang Zhao FocalClick towards practical interactive image
segmentation CVPR  2022 8 9 12 1919 Bowen Cheng Ishan Misra Alexander G Schwing Alexander Kir
illov and Rohit Girdhar Maskedattention mask transformer for
universal image segmentation CVPR  2022 4
20 Bowen Cheng Alex Schwing and Alexander Kirillov Per
pixel classiﬁcation is not all you need for semantic segmentation
NeurIPS  2021 5 16 17
21 Aakanksha Chowdhery Sharan Narang Jacob Devlin Maarten
Bosma Gaurav Mishra Adam Roberts Paul Barham Hyung Won
Chung Charles Sutton Sebastian Gehrmann et al PaLM Scaling
language modeling with pathways arXiv220402311  2022 1
22 Luca Ciampi Carlos Santiago Joao Costeira Claudio Gennaro and
Giuseppe Amato Domain adaptation for trafﬁc density estimation
International Joint Conference on Computer Vision Imaging and
Computer Graphics Theory and Applications  2021 9 20
23 Luca Ciampi Carlos Santiago Joao Costeira Claudio Gennaro and
Giuseppe Amato Night and day instance segmented park NDIS
Park dataset a collection of images taken by day and by night for
vehicle detection segmentation and counting in parking areas Zen
odo 2022 9 20
24 Nadav Cohen Yael Newman and Ariel Shamir Semantic segmen
tation in art paintings Computer Graphics Forum  2022 9 19 20
23 24
25 Marius Cordts Mohamed Omran Sebastian Ramos Timo Rehfeld
Markus Enzweiler Rodrigo Benenson Uwe Franke Stefan Roth
and Bernt Schiele The Cityscapes dataset for semantic urban scene
understanding CVPR  2016 9 19 20
26 Bruno da Silva George Konidaris and Andrew Barto Learning
parameterized skills ICML  2012 4
27 Dima Damen Hazel Doughty Giovanni Maria Farinella Antonino
Furnari Jian Ma Evangelos Kazakos Davide Moltisanti Jonathan
Munro Toby Perrett Will Price and Michael Wray Rescaling
egocentric vision Collection pipeline and challenges for EPIC
KITCHENS100 IJCV  2022 9 20 23 24
28 Ahmad Darkhalil Dandan Shan Bin Zhu Jian Ma Amlan Kar
Richard Higgins Sanja Fidler David Fouhey and Dima Damen
EPICKITCHENS VISOR benchmark Video segmentations and
object relations NeurIPS  2022 9 19 20 23 24
29 Terrance De Vries Ishan Misra Changhan Wang and Laurens
Van der Maaten Does object recognition work for everyone CVPR
workshops  2019 18
30 Mark D ıaz Ian Kivlichan Rachel Rosen Dylan Baker Razvan
Amironesei Vinodkumar Prabhakaran and Emily Denton Crowd
WorkSheets Accounting for individual and collective identities un
derlying crowdsourced dataset annotation ACM Conference on
Fairness Accountability and Transparency  2022 25
31 Henghui Ding Scott Cohen Brian Price and Xudong Jiang
PhraseClick toward achieving ﬂexible interactive segmentation by
phrase and click ECCV  2020 11
32 Piotr Doll ar and C Lawrence Zitnick Fast edge detection using
structured forests TPAMI  2014 21
33 Alexey Dosovitskiy Lucas Beyer Alexander Kolesnikov Dirk
Weissenborn Xiaohua Zhai Thomas Unterthiner Mostafa De
hghani Matthias Minderer Georg Heigold Sylvain Gelly Jakob
Uszkoreit and Neil Houlsby An image is worth 16x16 words
Transformers for image recognition at scale ICLR  2021 5 8
16
34 Alireza Fathi Xiaofeng Ren and James M Rehg Learning to rec
ognize objects in egocentric activities CVPR  2011 9 19 20
35 Pedro F Felzenszwalb and Daniel P Huttenlocher Efﬁcient graph
based image segmentation IJCV  2004 10
36 Thomas B Fitzpatrick The validity and practicality of sunreactive
skin types i through vi Archives of Dermatology  1988 8
37 Marco Forte Brian Price Scott Cohen Ning Xu and Franc ois
Pitie Getting to 99 accuracy in interactive segmentation
arXiv200307932  2020 5 17
38 JeanMichel Fortin Olivier Gamache Vincent Grondin Franc ois
Pomerleau and Philippe Gigu ere Instance segmentation for au
tonomous log grasping in forestry operations IROS  2022 9 20
1339 Timnit Gebru Jamie Morgenstern Briana Vecchione Jen
nifer Wortman Vaughan Hanna Wallach Hal Daum e Iii and Kate
Crawford Datasheets for datasets Communications of the ACM 
2021 25
40 Golnaz Ghiasi Yin Cui Aravind Srinivas Rui Qian TsungYi Lin
Ekin D Cubuk Quoc V Le and Barret Zoph Simple copypaste is a
strong data augmentation method for instance segmentation CVPR 
2021 16 18 22
41 Ross Girshick Jeff Donahue Trevor Darrell and Jitendra Malik
Rich feature hierarchies for accurate object detection and semantic
segmentation CVPR  2014 10
42 Priya Goyal Piotr Doll ar Ross Girshick Pieter Noordhuis Lukasz
Wesolowski Aapo Kyrola Andrew Tulloch Yangqing Jia and
Kaiming He Accurate large minibatch SGD Training ImageNet
in 1 hour arXiv170602677  2017 17
43 Kristen Grauman Andrew Westbury Eugene Byrne Zachary
Chavis Antonino Furnari Rohit Girdhar Jackson Hamburger
Hao Jiang Miao Liu Xingyu Liu Miguel Martin Tushar Na
garajan Ilija Radosavovic Santhosh Kumar Ramakrishnan Fiona
Ryan Jayant Sharma Michael Wray Mengmeng Xu Eric Zhong
cong Xu Chen Zhao Siddhant Bansal Dhruv Batra Vincent Car
tillier Sean Crane Tien Do Morrie Doulaty Akshay Erapalli
Christoph Feichtenhofer Adriano Fragomeni Qichen Fu Chris
tian Fuegen Abrham Gebreselasie Cristina Gonzalez James Hillis
Xuhua Huang Yifei Huang Wenqi Jia Weslie Khoo Jachym Ko
lar Satwik Kottur Anurag Kumar Federico Landini Chao Li
Yanghao Li Zhenqiang Li Karttikeya Mangalam Raghava Mod
hugu Jonathan Munro Tullie Murrell Takumi Nishiyasu Will
Price Paola Ruiz Puentes Merey Ramazanova Leda Sari Kiran
Somasundaram Audrey Southerland Yusuke Sugano Ruijie Tao
Minh V o Yuchen Wang Xindi Wu Takuma Yagi Yunyi Zhu
Pablo Arbelaez David Crandall Dima Damen Giovanni Maria
Farinella Bernard Ghanem Vamsi Krishna Ithapu C V  Jawahar
Hanbyul Joo Kris Kitani Haizhou Li Richard Newcombe Aude
Oliva Hyun Soo Park James M Rehg Yoichi Sato Jianbo Shi
Mike Zheng Shou Antonio Torralba Lorenzo Torresani Mingfei
Yan and Jitendra Malik Ego4D Around the World in 3000 Hours
of Egocentric Video CVPR  2022 20
44 Agrim Gupta Piotr Dollar and Ross Girshick LVIS A dataset for
large vocabulary instance segmentation CVPR  2019 2 6 7 9 10
11 19 20 21 24
45 Abner GuzmanRivera Dhruv Batra and Pushmeet Kohli Multiple
choice learning Learning to produce multiple structured outputs
NeurIPS  2012 5 17
46 Timm Haucke Hjalmar S K uhl and V olker Steinhage
SOCRATES Introducing depth in visual wildlife monitoring using
stereo vision Sensors  2022 9 20
47 Kaiming He Xinlei Chen Saining Xie Yanghao Li Piotr Doll ar
and Ross Girshick Masked autoencoders are scalable vision learn
ersCVPR  2022 5 8 12 16 17
48 Kaiming He Georgia Gkioxari Piotr Doll ar and Ross Girshick
Mask RCNN ICCV  2017 10
49 Kaiming He Xiangyu Zhang Shaoqing Ren and Jian Sun Deep
residual learning for image recognition CVPR  2016 16
50 Dan Hendrycks and Kevin Gimpel Gaussian error linear units
gelus arXiv160608415  2016 16
51 Jordan Hoffmann Sebastian Borgeaud Arthur Mensch Elena
Buchatskaya Trevor Cai Eliza Rutherford Diego de Las Casas
Lisa Anne Hendricks Johannes Welbl Aidan Clark et al Training
computeoptimal large language models arXiv220315556  2022
1
52 Jungseok Hong Michael Fulton and Junaed Sattar TrashCan A
semanticallysegmented dataset towards visual detection of marine
debris arXiv200708097  2020 9 19 20
53 Gao Huang Yu Sun Zhuang Liu Daniel Sedra and Kilian Q Wein
berger Deep networks with stochastic depth ECCV  2016 17
54 Jitesh Jain Jiachen Li MangTik Chiu Ali Hassani Nikita Orlov
and Humphrey Shi Oneformer One transformer to rule universal
image segmentation arXiv221106220  2022 455 Chao Jia Yinfei Yang Ye Xia YiTing Chen Zarana Parekh
Hieu Pham Quoc Le YunHsuan Sung Zhen Li and Tom Duerig
Scaling up visual and visionlanguage representation learning with
noisy text supervision ICML  2021 1
56 Jared Kaplan Sam McCandlish Tom Henighan Tom B Brown
Benjamin Chess Rewon Child Scott Gray Alec Radford Jeffrey
Wu and Dario Amodei Scaling laws for neural language models
arXiv200108361  2020 1
57 Michael Kass Andrew Witkin and Demetri Terzopoulos Snakes
Active contour models IJCV  1988 4
58 Dahun Kim TsungYi Lin Anelia Angelova In So Kweon and
Weicheng Kuo Learning openworld object proposals without
learning to classify IEEE Robotics and Automation Letters  2022
21
59 Alexander Kirillov Kaiming He Ross Girshick Carsten Rother
and Piotr Doll ar Panoptic segmentation CVPR  2019 4
60 Alina Kuznetsova Hassan Rom Neil Alldrin Jasper Uijlings Ivan
Krasin Jordi PontTuset Shahab Kamali Stefan Popov Matteo
Malloci Alexander Kolesnikov Tom Duerig and Vittorio Ferrari
The open images dataset v4 Uniﬁed image classiﬁcation object
detection and visual relationship detection at scale IJCV  2020 2
6 7 18 19
61 Alexandre Lacoste Alexandra Luccioni Victor Schmidt and
Thomas Dandres Quantifying the carbon emissions of machine
learning arXiv191009700  2019 28
62 Yanghao Li Hanzi Mao Ross Girshick and Kaiming He Explor
ing plain vision transformer backbones for object detection ECCV 
2022 5 10 11 16 21 23 24
63 Yin Li Zhefan Ye and James M Rehg Delving into egocentric
actions CVPR  2015 9 20
64 Zhuwen Li Qifeng Chen and Vladlen Koltun Interactive image
segmentation with latent diversity CVPR  2018 5 17 19
65 TsungYi Lin Priya Goyal Ross Girshick Kaiming He and Piotr
Dollar Focal loss for dense object detection ICCV  2017 5 17
66 TsungYi Lin Michael Maire Serge Belongie James Hays Pietro
Perona Deva Ramanan Piotr Doll ar and C Lawrence Zitnick Mi
crosoft COCO Common objects in context ECCV  2014 2 4 6
7 11 18 19 20
67 Qin Liu Zhenlin Xu Gedas Bertasius and Marc Niethammer Sim
pleClick Interactive image segmentation with simple vision trans
formers arXiv221011006  2022 8 9 12 19
68 Ilya Loshchilov and Frank Hutter Decoupled weight decay regu
larization ICLR  2019 17
69 Cathy H Lucas Daniel OB Jones Catherine J Hollyhead Robert H
Condon Carlos M Duarte William M Graham Kelly L Robinson
Kylie A Pitt Mark Schildhauer and Jim Regetz Gelatinous zoo
plankton biomass in the global oceans geographic variation and
environmental drivers Global Ecology and Biogeography  2014
20
70 Sabarinath Mahadevan Paul V oigtlaender and Bastian Leibe Iter
atively trained interactive segmentation BMVC  2018 4 17
71 KevisKokitsi Maninis Sergi Caelles Jordi PontTuset and Luc
Van Gool Deep extreme cut From extreme points to object seg
mentation CVPR  2018 6
72 David Martin Charless Fowlkes Doron Tal and Jitendra Malik
A database of human segmented natural images and its applica
tion to evaluating segmentation algorithms and measuring ecologi
cal statistics ICCV  2001 10 21 28
73 Fausto Milletari Nassir Navab and SeyedAhmad Ahmadi VNet
Fully convolutional neural networks for volumetric medical image
segmentation 3DV 2016 5 17
74 Massimo Minervini Andreas Fischbach Hanno Scharr and
Sotirios A Tsaftaris Finelygrained annotated datasets for image
based plant phenotyping Pattern Recognition Letters  2016 9 20
75 Margaret Mitchell Simone Wu Andrew Zaldivar Parker Barnes
Lucy Vasserman Ben Hutchinson Elena Spitzer Inioluwa Debo
rah Raji and Timnit Gebru Model cards for model reporting Pro
ceedings of the conference on fairness accountability and trans
parency  2019 25 28
1476 Dim P Papadopoulos Jasper RR Uijlings Frank Keller and Vittorio
Ferrari Extreme clicking for efﬁcient object annotation ICCV 
2017 6
77 David Patterson Joseph Gonzalez Quoc Le Chen Liang Lluis
Miquel Munguia Daniel Rothchild David So Maud Texier and
Jeff Dean Carbon emissions and large neural network training
arXiv210410350  2021 28
78 Matthew E Peters Waleed Ammar Chandra Bhagavatula and Rus
sell Power Semisupervised sequence tagging with bidirectional
language models Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics  2017 18
79 Mengyang Pu Yaping Huang Yuming Liu Qingji Guan and
Haibin Ling EDTER Edge detection with transformer CVPR 
2022 10
80 Mattia Pugliatti and Francesco Topputo DOORS Dataset fOr
bOuldeRs Segmentation Zenodo  2022 9 20
81 Jiyang Qi Yan Gao Yao Hu Xinggang Wang Xiaoyu Liu Xiang
Bai Serge Belongie Alan Yuille Philip Torr and Song Bai Oc
cluded video instance segmentation A benchmark ICCV  2022 9
20 23 24
82 Alec Radford Jong Wook Kim Chris Hallacy Aditya Ramesh
Gabriel Goh Sandhini Agarwal Girish Sastry Amanda Askell
Pamela Mishkin Jack Clark et al Learning transferable visual
models from natural language supervision ICML  2021 1 2 4 5
8 12 16 22
83 Aditya Ramesh Mikhail Pavlov Gabriel Goh Scott Gray Chelsea
V oss Alec Radford Mark Chen and Ilya Sutskever Zeroshot text
toimage generation ICML  2021 1 4 12
84 Shaoqing Ren Kaiming He Ross Girshick and Jian Sun Faster
RCNN Towards realtime object detection with region proposal
networks NeurIPS  2015 6 10
85 Xiaofeng Ren and Jitendra Malik Learning a classiﬁcation model
for segmentation ICCV  2003 4
86 Mike Roberts Jason Ramapuram Anurag Ranjan Atulit Kumar
Miguel Angel Bautista Nathan Paczan Russ Webb and Joshua M
Susskind Hypersim A photorealistic synthetic dataset for holistic
indoor scene understanding ICCV  2021 9 19 20
87 Candice Schumann Susanna Ricco Utsav Prabhu Vittorio Ferrari
and Caroline Pantofaru A step toward more inclusive people anno
tations for fairness Proceedings of the 2021 AAAIACM Conference
on AI Ethics and Society  2021 8 19
88 Seﬁk Ilkin Serengil and Alper Ozpinar LightFace A hybrid deep
face recognition framework ASYU  2020 26
89 Seﬁk Ilkin Serengil and Alper Ozpinar HyperExtended LightFace
A facial attribute analysis framework ICEET  2021 26
90 Jamie Shotton John Winn Carsten Rother and Antonio Crimin
isi TextonBoost Joint appearance shape and context modeling for
mulitclass object recognition and segmentation ECCV  2006 4
91 Corey Snyder and Minh Do STREETS A novel camera network
dataset for trafﬁc ﬂow NeurIPS  2019 9 20
92 Konstantin Soﬁiuk Ilya A Petrov and Anton Konushin Reviving
iterative training with mask guidance for interactive segmentation
ICIP  2022 5 8 9 17 19 23 24 28
93 Nitish Srivastava Geoffrey Hinton Alex Krizhevsky Ilya
Sutskever and Ruslan Salakhutdinov Dropout A simple way to
prevent neural networks from overﬁtting The Journal of Machine
Learning Research  2014 16
94 Chris Stauffer and W Eric L Grimson Adaptive background mix
ture models for realtime tracking CVPR  1999 4
95 Matthew Tancik Pratul Srinivasan Ben Mildenhall Sara
FridovichKeil Nithin Raghavan Utkarsh Singhal Ravi Ra
mamoorthi Jonathan Barron and Ren Ng Fourier features let net
works learn high frequency functions in low dimensional domains
NeurIPS  2020 5 16
96 Yansong Tang Yi Tian Jiwen Lu Jianjiang Feng and Jie Zhou
Action recognition in RGBD egocentric videos ICIP  2017 2097 Yansong Tang Zian Wang Jiwen Lu Jianjiang Feng and Jie Zhou
Multistream deep neural networks for RGBD egocentric action
recognition IEEE Transactions on Circuits and Systems for Video
Technology  2019 20
98 The World Bank The world by income and regions
2022 httpsdatatopicsworldbankorgworlddevelopment
indicatorstheworldbyincomeandregionhtml 18
99 Sebastian Thrun Is learning the nth thing any easier than learning
the ﬁrst NeurIPS  1995 12
100 Cameron Trotter Georgia Atkinson Matt Sharpe Kirsten Richard
son A Stephen McGough Nick Wright Ben Burville and Per
Berggren NDD20 A largescale fewshot dolphin dataset for
coarse and ﬁnegrained categorisation arXiv200513359  2020
9 19 20 23 24
101 United States Environmental Protection Agency Greenhouse Gas
Equivalencies Calculator httpswwwepagovenergygreenhouse
gasequivalenciescalculator 2022 28
102 Koen EA van de Sande Jasper RR Uijlings Theo Gevers and
Arnold WM Smeulders Segmentation as selective search for ob
ject recognition ICCV  2011 10
103 Ashish Vaswani Noam Shazeer Niki Parmar Jakob Uszkoreit
Llion Jones Aidan N Gomez Lukasz Kaiser and Illia Polosukhin
Attention is all you need NeurIPS  2017 5 16
104 Boying Wang Libo Zhang Longyin Wen Xianglong Liu and Yan
jun Wu Towards realworld prohibited item detection A large
scale xray benchmark CVPR  2021 9 19 20
105 Weiyao Wang Matt Feiszli Heng Wang Jitendra Malik and
Du Tran Openworld instance segmentation Exploiting pseudo
ground truth from learned pairwise afﬁnity CVPR  2022 21
106 ChaoYuan Wu Justin Johnson Jitendra Malik Christoph Feicht
enhofer and Georgia Gkioxari Multiview compressive coding for
3D reconstruction CVPR  2023 12
107 Jianxiong Xiao James Hays Krista Ehinger Aude Oliva and An
tonio Torralba SUN database Largescale scene recognition from
abbey to zoo CVPR  2010 20
108 Saining Xie and Zhuowen Tu Holisticallynested edge detection
ICCV  2015 10
109 Ning Xu Brian Price Scott Cohen Jimei Yang and Thomas S
Huang Deep interactive object selection CVPR  2016 4 19
110 Kaiyu Yang Klint Qinami Li FeiFei Jia Deng and Olga Rus
sakovsky Towards fairer datasets Filtering and balancing the dis
tribution of the people subtree in the imagenet hierarchy Proceed
ings of the 2020 conference on fairness accountability and trans
parency  2020 8
111 Lei Yang Yan Zi Wei Yisheng HE Wei Sun Zhenhang Huang
Haibin Huang and Haoqiang Fan iShape A ﬁrst step towards
irregular shape instance segmentation arXiv210915068  2021 9
20 23 24
112 Senthil Yogamani Ciar an Hughes Jonathan Horgan Ganesh Sistu
Padraig Varley Derek ODea Michal Uric ar Stefan Milz Mar
tin Simon Karl Amende et al WoodScape A multitask multi
camera ﬁsheye dataset for autonomous driving ICCV  2019 9
20
113 Lingzhi Zhang Shenghao Zhou Simon Stent and Jianbo Shi Fine
grained egocentric handobject segmentation Dataset model and
applications ECCV  2022 9 19 20
114 Wenwei Zhang Jiangmiao Pang Kai Chen and Chen Change Loy
KNet Towards uniﬁed image segmentation NeurIPS  2021 4
115 Jieyu Zhao Tianlu Wang Mark Yatskar Vicente Ordonez and Kai
Wei Chang Men also like shopping Reducing gender bias ampli
ﬁcation using corpuslevel constraints arXiv170709457  2017 8
116 Bolei Zhou Agata Lapedriza Aditya Khosla Aude Oliva and An
tonio Torralba Places A 10 million image database for scene
recognition TPAMI  2017 20
117 Bolei Zhou Hang Zhao Xavier Puig Tete Xiao Sanja Fidler
Adela Barriuso and Antonio Torralba Semantic understanding of
scenes through the ADE20K dataset IJCV  2019 2 7 9 20
15Appendix
Table of contents
 A Segment Anything Model and Task Details
 B Automatic Mask Generation Details
 C RAI Additional Details
 D Experiment Implementation Details
 E Human Study Experimental Design
 F Dataset Annotation and Model Cards
 G Annotation Guidelines
A Segment Anything Model and Task Details
Image encoder In general the image encoder can be any
network that outputs a CHWimage embedding Mo
tivated by scalability and access to strong pretraining we
use an MAE 47 pretrained Vision Transformer ViT 33
with minimal adaptations to process high resolution inputs
speciﬁcally a ViTH16 with 14 14 windowed attention
and four equallyspaced global attention blocks follow
ing 62 The image encoders output is a 16 downscaled
embedding of the input image Since our runtime goal is to
process each prompt in realtime we can afford a high num
ber of image encoder FLOPs because they are computed
only once per image notper prompt
Following standard practices  eg 40 we use an in
put resolution of 1024 1024 obtained by rescaling the im
age and padding the shorter side The image embedding
is therefore 6464 To reduce the channel dimension fol
lowing 62 we use a 1 1 convolution to get to 256 chan
nels followed by a 3 3 convolution also with 256 channels
Each convolution is followed by a layer normalization 4
Prompt encoder Sparse prompts are mapped to 256
dimensional vectorial embeddings as follows A point is
represented as the sum of a positional encoding 95 of the
points location and one of two learned embeddings that in
dicate if the point is either in the foreground or background
A box is represented by an embedding pair 1 the posi
tional encoding of its topleft corner summed with a learned
embedding representing topleft corner and 2 the same
structure but using a learned embedding indicating bottom
right corner Finally to represent freeform text we use the
text encoder from CLIP 82 any text encoder is possible in
general We focus on geometric prompts for the remainder
of this section and discuss text prompts in depth in D5
Dense prompts  ie masks have a spatial correspon
dence with the image We input masks at a 4 lower res
olution than the input image then downscale an additional
4using two 22 stride2 convolutions with output chan
nels 4 and 16 respectively A ﬁnal 1 1 convolution maps
the channel dimension to 256 Each layer is separated by
GELU activations 50 and layer normalization The mask
image
embedding
256x64x64x2
token
to image
attn2x
conv
trans
IoU
scoresmlpmasksdot product
per mask
prompt tokens
Ntokensx256output tokens
output
token
per mask
IoU
output
tokenmlp
mask decoderself attntoken to image attnmlpimage to token attnFigure 14 Details of the lightweight mask decoder A
twolayer decoder updates both the image embedding and
prompt tokens via crossattention Then the image embed
ding is upscaled from which the updated output tokens are
used to dynamically predict masks Not illustrated for ﬁg
ure clarity At every attention layer positional encodings
are added to the image embedding and the entire original
prompt token including position encoding is readded to
the token queries and keys
and image embedding are then added elementwise If there
is no mask prompt a learned embedding representing no
mask is added to each image embedding location
Lightweight mask decoder This module efﬁciently maps
the image embedding and a set of prompt embeddings to an
output mask To combine these inputs we take inspiration
from Transformer segmentation models 14 20 and modify
a standard Transformer decoder 103 Before applying our
decoder we ﬁrst insert into the set of prompt embeddings
a learned output token embedding that will be used at the
decoders output analogous to the class token in 33
For simplicity we refer to these embeddings  notincluding
the image embedding collectively as tokens
Our decoder design is shown in Fig 14 Each decoder
layer performs 4 steps 1 selfattention on the tokens 2
crossattention from tokens as queries to the image em
bedding 3 a pointwise MLP updates each token and 4
crossattention from the image embedding as queries to
tokens This last step updates the image embedding with
prompt information During crossattention the image em
bedding is treated as a set of 642256dimensional vectors
Each selfcrossattention and MLP has a residual connec
tion 49 layer normalization and a dropout 93 of 01 at
training The next decoder layer takes the updated tokens
and the updated image embedding from the previous layer
We use a twolayer decoder
To ensure the decoder has access to critical geometric in
formation the positional encodings are added to the image
embedding whenever they participate in an attention layer
Additionally the entire original prompt tokens including
their positional encodings are readded to the updated to
kens whenever they participate in an attention layer This
allows for a strong dependence on both the prompt tokens
geometric location and type
After running the decoder we upsample the updated im
age embedding by 4 with two transposed convolutional
16layers now its downscaled 4 relative to the input image
Then the tokens attend once more to the image embedding
and we pass the updated output token embedding to a small
3layer MLP that outputs a vector matching the channel di
mension of the upscaled image embedding Finally we pre
dict a mask with a spatially pointwise product between the
upscaled image embedding and the MLPs output
The transformer uses an embedding dimension of 256
The transformer MLP blocks have a large internal dimen
sion of 2048 but the MLP is applied only to the prompt to
kens for which there are relatively few rarely greater than
20 However in crossattention layers where we have a
6464 image embedding we reduce the channel dimension
of the queries keys and values by 2 to 128 for computa
tional efﬁciency All attention layers use 8 heads
The transposed convolutions used to upscale the output
image embedding are 2 2 stride 2 with output channel di
mensions of 64 and 32 and have GELU activations They
are separated by layer normalization
Making the model ambiguityaware As described a sin
gle input prompt may be ambiguous in the sense that it cor
responds to multiple valid masks and the model will learn
to average over these masks We eliminate this problem
with a simple modiﬁcation instead of predicting a single
mask we use a small number of output tokens and predict
multiple masks simultaneously By default we predict three
masks since we observe that three layers whole part and
subpart are often enough to describe nested masks During
training we compute the loss described shortly between
the ground truth and each of the predicted masks but only
backpropagate from the lowest loss This is a common tech
nique used for models with multiple outputs 15 45 64
For use in applications wed like to rank predicted masks
so we add a small head operating on an additional output
token that estimates the IoU between each predicted mask
and the object it covers
Ambiguity is much rarer with multiple prompts and the
three output masks will usually become similar To mini
mize computation of degenerate losses at training and en
sure the single unambiguous mask receives a regular gradi
ent signal we only predict a single mask when more than
one prompt is given This is accomplished by adding a
fourth output token for an additional mask prediction This
fourth mask is never returned for a single prompt and is the
only mask returned for multiple prompts
Losses We supervise mask prediction with a linear combi
nation of focal loss 65 and dice loss 73 in a 201 ratio of
focal loss to dice loss following 20 14 Unlike 20 14
we observe that auxiliary deep supervision after each de
coder layer is unhelpful The IoU prediction head is trained
with meansquareerror loss between the IoU prediction and
the predicted masks IoU with the ground truth mask It is
added to the mask loss with a constant scaling factor of 10Training algorithm Following recent approaches 92 37
we simulate an interactive segmentation setup during train
ing First with equal probability either a foreground point
or bounding box is selected randomly for the target mask
Points are sampled uniformly from the ground truth mask
Boxes are taken as the ground truth masks bounding box
with random noise added in each coordinate with standard
deviation equal to 10 of the box sidelength to a maxi
mum of 20 pixels This noise proﬁle is a reasonable com
promise between applications like instance segmentation
which produce a tight box around the target object and in
teractive segmentation where a user may draw a loose box
After making a prediction from this ﬁrst prompt subse
quent points are selected uniformly from the error region
between the previous mask prediction and the ground truth
mask Each new point is foreground or background if the er
ror region is a false negative or false positive respectively
We also supply the mask prediction from the previous it
eration as an additional prompt to our model To provide
the next iteration with maximal information we supply the
unthresholded mask logits instead of the binarized mask
When multiple masks are returned the mask passed to the
next iteration and used to sample the next point is the one
with the highest predicted IoU
We ﬁnd diminishing returns after 8 iteratively sampled
points we have tested up to 16 Additionally to encour
age the model to beneﬁt from the supplied mask we also
use two more iterations where no additional points are sam
pled One of these iterations is randomly inserted among the
8 iteratively sampled points and the other is always at the
end This gives 11 total iterations one sampled initial in
put prompt 8 iteratively sampled points and two iterations
where no new external information is supplied to the model
so it can learn to reﬁne its own mask predictions We note
that using a relatively large number of iterations is possible
because our lightweight mask decoder requires less than 1
of the image encoders compute and therefore each itera
tion adds only a small overhead This is unlike previous
interactive methods that perform only one or a few interac
tive steps per optimizer update 70 9 37 92
Training recipe We use the AdamW 68 optimizer  1
092 0999 and a linear learning rate warmup 42 for
250 iterations and a stepwise learning rate decay schedule
The initial learning rate  lr after warmup is 8e4 We
train for 90k iterations  2 SA1B epochs and decrease the
lrby a factor of 10 at 60k iterations and again at 86666 it
erations The batch size is 256 images To regularize SAM
we set weight decay  wd to 01 and apply drop path 53
dp with a rate of 04 We use a layerwise learning rate
decay 5  ld of 08 No data augmentation is applied We
initialize SAM from an MAE 47 pretrained ViTH We
distribute training across 256 GPUs due to the large image
encoder and 10241024 input size To limit GPU mem
17ory usage we train with up to 64 randomly sampled masks
per GPU Additionally we ﬁnd that lightly ﬁltering SA1B
masks to discard any that cover more than 90 of the image
qualitatively improves results
For ablations and others variations on training  eg text
tomask D5 we deviate from the default recipe above as
follows When training with data from the ﬁrst and sec
ond data engine stages only we augment the input with
largescale jitter 40 with a scale range of 01 20 In
tuitively data augmentation may be helpful when training
data is more limited To train ViTB and ViTL we use
180k iterations with batch size 128 distributed across 128
GPUs We set lr8e44e4ld 0608 wd 01 and
dp 0604 for ViTBL respectively
B Automatic Mask Generation Details
Here we discuss details of the data engines fully auto
matic stage that was used to generate the released SA1B
Cropping Masks were generated from a regular grid of
3232 points on the full image and 20 additional zoomed
in image crops arising from 2 2 and 44 partially over
lapping windows using 16 16 and 88 regular point grids
respectively The original highresolution images were used
for cropping this was the only time we used them We re
moved masks that touch the inner boundaries of the crops
We applied standard greedy boxbased NMS boxes were
used for efﬁciency in two phases ﬁrst within each crop and
second across crops When applying NMS within a crop
we used the models predicted IoU to rank masks When
applying NMS across crops we ranked masks from most
zoomedin  ie from a 44 crop to least zoomedin  ie
the original image based on their source crop In both
cases we used an NMS threshold of 07
Filtering We used three ﬁlters to increase mask qual
ity First to keep only conﬁdent masks we ﬁltered by the
models predicted IoU score at a threshold of 880 Second
to keep only stable masks we compared two binary masks
resulting from the same underlying soft mask by threshold
ing it at different values We kept the prediction  ie the
binary mask resulting from thresholding logits at 0 only if
the IoU between its pair of 1 and 1 thresholded masks was
equal to or greater than 950 Third we noticed that occa
sionally an automatic mask would cover the entire image
These masks were generally uninteresting and we ﬁltered
them by removing masks that covered 95 or more of an
image All ﬁltering thresholds were selected to achieve both
a large number of masks and high mask quality as judged by
professional annotators using the method described in 5
Postprocessing We observed two error types that are eas
ily mitigated with postprocessing First an estimated 4
of masks include small spurious components To address
these we removed connected components with area lessthan 100 pixels including removing entire masks if the
largest component is below this threshold Second another
estimated 4 of masks include small spurious holes To
address these we ﬁlled holes with area less than 100 pixels
Holes were identiﬁed as components of inverted masks
Automatic mask generation model We trained a special
version of SAM for fully automatic mask generation that
sacriﬁces some inference speed for improved mask gener
ation properties We note the differences between our de
fault SAM and the one used for data generation here it
was trained on manual and semiautomatic data only it was
trained for longer 177656 iterations instead of 90k with
largescale jitter data augmentation 40 simulated interac
tive training used only point and mask prompts no boxes
and sampled only 4 points per mask during training reduc
ing from our default of 9 to 4 sped up training iterations
and had no impact on 1point performance though it would
harm mIoU if evaluating with more points and ﬁnally the
mask decoder used 3 layers instead of 2
SA1B examples We show SA1B samples in Fig 2 For
more examples please see our dataset explorer
C RAI Additional Details
Inferring geographic information for SA1B While the
images in SA1B are not geotagged each image has a cap
tion describing its contents and where it was taken We infer
approximate image geolocations from these captions using
an Elmobased named entity recognition model 78 Each
extracted location entity is mapped to every matching coun
try province and city Captions are mapped to a single
country by ﬁrst considering the matching countries then
provinces and ﬁnally cities We note that there are ambigu
ities and potential for biases with this method  eg Geor
gia may refer to the country or the US state As such we
use the extracted locations to analyze the dataset as a whole
but do not release the inferred locations The captions will
not be released publicly as required by the image provider
Inferring geographic information for COCO and Open
Images The COCO 66 and Open Images 60 datasets
do not provide geolocations Following 29 we retrieve
geographic metadata using the Flickr API We retrieved
locations for 24 of the COCO training set 19562 im
ages and for Open Images we retrieved 18 of the train
ing set 493517 images after only considering images with
masks We note that the geographic information is approx
imate and the sample of images with this information may
not fully match the full dataset distribution
Inferring income information We use each images in
ferred country to look up its income level using the levels
deﬁned by The World Bank 98 We collapse the upper
middle and lowermiddle levels into a single middle level
18mIoU at
1 point 3 points
perceived gender presentation
feminine 763 11 90705
masculine 810 12 92304mIoU at
1 point 3 points
perceived age group
older 819 38 92816
middle 782 08 91303
young 773 27 91509
Table 6 SAMs performance segmenting clothing across
perceived gender presentation and age group The intervals
for perceived gender are disjoint with mIoU for masculine
being higher Conﬁdence intervals for age group overlap
Fairness in segmenting people To investigate SAMs fair
ness at segmenting people we use the More Inclusive Anno
tations for People MIAP 87 test set annotations for Open
Images 60 which allows us to compare SAMs perfor
mance across perceived gender presentation and perceived
age group MIAP provides box annotations while we need
ground truth masks for this analysis To get ground truth
masks we select each personcategory mask from Open
Images if its corresponding bounding box is within a 1
margin based on relative box side lengths of an annotated
bounding box in MIAP resulting in 39k masks
Fairness in segmenting clothing We extend our analysis
from 6 to clothing segmentation We look at SAMs per
formance on clothing relative to the attributes of those wear
ing the clothes We use all 65k ground truth masks from
Open Images that have a category under the clothing super
class and reside within a person box from MIAP In Table 6
we compare performance across perceived gender presenta
tion and age group We ﬁnd that SAM is better at segment
ing clothing on those who present predominantly mascu
line with disjoint 95 conﬁdence intervals The gap closes
when moving from 1 to 3 point evaluation Differences for
perceived age group are not signiﬁcant Our results indicate
there is a bias when segmenting clothing across perceived
gender presentation with a one point prompt and we en
courage users of SAM to be mindful of this limitation
D Experiment Implementation Details
D1 ZeroShot Single Point Valid Mask Evaluation
Datasets We built a new segmentation benchmark to eval
uate the zeroshot transfer capabilities of our model using a
suite of 23 diverse segmentation datasets from prior work
A description of each dataset is given in Table 7 For exam
ples see main text Fig 8 This suite covers a range of do
mains including egocentric 34 28 113 microscopy 12
Xray 104 underwater 52 100 aerial 17 simula
tion 86 driving 25 and painting 24 images For ef
ﬁcient evaluation we subsampled datasets with more than
15k masks Speciﬁcally we randomly picked images so
that the total number of masks in the sampled images was
10k We blurred faces of people in all the datasetsPoint sampling Our default point sampling follows stan
dard practice in interactive segmentation 109 64 92 The
ﬁrst point is chosen deterministically as the point farthest
from the object boundary Each subsequent point is the
farthest from the boundary of the error region between
ground truth and the previous prediction Some experiments
where speciﬁed use a more challenging sampling strategy
in which the ﬁrst point is a random point rather than a deter
ministically selected center point Each subsequent point
is selected as described above This setting better reﬂects
use cases in which the ﬁrst point is not reliably near the
center of the mask such as prompting from eye gaze
Evaluation We measure IoU between a prediction after
Npoint prompts and a ground truth mask where N
f12359gand points are sampled iteratively with either
of the strategies described above The perdataset mIoU is
the permask IoU averaged across all objects in the dataset
Finally we report the topline metric by averaging the per
dataset mIoUs across all 23 datasets Our evaluation differs
from the standard interactive segmentation evaluation pro
tocol which measures the average number of points needed
to achieveX IoU with up to 20 points We focus on pre
dictions after just one or possibly a few points since many
of our use cases involve a single or very few prompts Given
our application focus which requires realtime prompt pro
cessing we expect the best interactive segmentation models
to outperform SAM when using a large number of points
Baselines We use three recent strong interactive base
lines RITM 92 FocalClick 18 and SimpleClick 67
For each we use the largest models trained on the broad
est datasets publicly released by the authors For RITM
we use HRNet32 ITM trained on the combination of
COCO 66 and LVIS 44 introduced by the authors
For FocalClick we use SegFormerB3S2 trained on a
combined dataset that includes 8 different segmentation
datasets 18 For SimpleClick we use ViTH448 trained
on a combination of COCO and LVIS We follow the sug
gested default strategies for data preprocessing  ie data
augmentations or image resizing and do not change or
adapt any parameters for our evaluation In our experi
ments we observe that RITM outperforms other baselines
on our 23 dataset suite with 1 point evaluation Therefore
we use RITM as the default baseline When evaluating with
more points we report results for all baselines
Single point ambiguity and oracle evaluation In addition
to IoU after Npoints prompts we report SAMs oracle
performance at 1 point by evaluating the predicted mask that
best matches ground truth from amongst SAMs three pre
dictions rather than using the one that SAM itself ranks
ﬁrst as we do by default This protocol addresses possible
single point prompt ambiguity by relaxing the requirement
to guess the one right mask among several valid objects
19datasetabbreviation
 linkimage
typedescriptionmask
typesource split images
sampled masks
sampled
Plant Phenotyping Datasets
Leaf Segmentation 74PPDLS Plants Leaf segmentation for images of tobacco and ara plants Instance NA 182 2347
BBBC038v1 from Broad
Bioimage Benchmark
Collection 12BBBC038v1 MicroscopyBiological images of cells in a variety of settings testing
robustness in nuclei segmentationInstance Train 227 10506
Dataset fOr bOuldeRs
Segmentation 80DOORS BouldersSegmentation masks of single boulders positioned on the
surface of a spherical meshInstance DS1 10000 10000
TimberSeg 10 38 TimberSeg LogsSegmentation masks of individual logs in piles of timber in
various environments and conditions Images are taken from
an operators pointofviewInstance NA 220 2487
Northumberland Dolphin
Dataset 2020 100NDD20 UnderwaterSegmentation masks of two different dolphin species in
images taken above and under waterInstance NA 4402 6100
Large V ocabulary Instance
Segmentation 44LVIS ScenesAdditional annotations for the COCO 66 dataset to enable
the study of longtailed object detection and segmentationInstance Validation v05 945 9642
STREETS 91 STREETSTrafﬁc
cameraSegmentation masks of cars in trafﬁc camera footage Instance NA 819 9854
ZeroWastef 6 ZeroWastef RecyclingSegmentation masks in cluttered scenes of deformed
recycling wasteInstance Train 2947 6155
iShape 111 iShapeIrregular
shapesSegmentation masks of irregular shapes like antennas logs
fences and hangersInstance Validation 754 9742
ADE20K 117 ADE20K ScenesObject and part segmentation masks for images from
SUN 107 and Places 116 datasetsInstance Validation 302 10128
Occluded Video Instance
Segmentation 81OVIS OcclusionsInstance segmentation masks in videos focusing on objects
that are occludedInstance Train 2044 10011
Hypersim 86 Hypersim SimulationPhotorealistic synthetic dataset of indoor scenes with instance
masksInstanceEvermotion archinteriors
volumes 155 excluding
20254049338 9445
Night and Day Instance
Segmented Park 22 23NDISPark Parking lotsImages of parking lots from video footage taken at day and
night during different weather conditions and camera angles
for vehicle segmentationInstance Train 111 2577
EPICKITCHENS
VISOR 28 27VISOR EgocentricSegmentation masks for hands and active objects in
egocentric video from the cooking dataset
EPICKITCHENS 27Instance Validation 1864 10141
Plittersdorf dataset 46 PlittersdorfStereo
imagesSegmentation masks of wildlife in images taken with the
SOCRATES stereo camera trapInstance Train validation test 187 546
Egocentric HandObject
Segmentation 113EgoHOS EgocentricFinegrained egocentric handobject segmentation dataset
Dataset contains mask annotations for existing datasetsInstanceTrain including only
Ego4D 43 and
THUREAD 97 962940 9961
InstanceBuilding 2D 17 IBD DronesHighresolution drone UA V images annotated with roof
instance segmentation masksInstance Train 2D annotations 467 11953
WoodScape 112 WoodScapeFisheye
drivingFisheye driving dataset with segmentation masks Images are
taken from four surroundview camerasInstance Set 1 107 10266
Cityscapes 25 Cityscapes Driving Stereo video of street scenes with segmentation masks Panoptic Validation 293 9973
PIDray 104 PIDRay XraySegmentation masks of prohibited items in Xray images of
baggageInstance Test hard 3733 8892
Diverse Realism in Art
Movements 24DRAM PaintingsDomain adaptation dataset for semantic segmentation of art
paintingsSemantic Test 718 1179
TrashCan 52 TrashCan UnderwaterSegmentation masks of trash in images taken by underwater
ROVs Images are sourced from the JEDI 69 datasetInstance Train instance task 5936 9540
Georgia Tech Egocentric
Activity Datasets 34 63GTEA EgocentricVideos are composed of four different subjects performing
seven types of daily activities with segmentation masks of
handsInstanceTrain segmenting hands
task652 1208
Table 7 Segmentation datasets used to evaluate zeroshot segmentation with point prompts The 23 datasets cover a broad
range of domains see column image type To make our evaluation efﬁcient we subsample datasets that have more than
15k masks Speciﬁcally we randomly sampled images so that the total number of masks in the images is 10k
20image ground truth SAM image ground truth SAM
Figure 15 Additional visualizations of zeroshot edge predictions on BSDS500 Recall that SAM was not trained to predict
edge maps and did not have access to BSDS images and annotations during training
D2 ZeroShot Edge Detection
Dataset and metrics We perform zeroshot edge detection
experiments on BSDS500 72 3 The ground truth for each
image comes from the manual annotations of ﬁve different
subjects We report results on the 200 image test subset
using the four standard metrics for edge detection 3 32
optimal dataset scale ODS optimal image scale OIS av
erage precision AP and recall at 50 precision R50
Method For zeroshot transfer we use a simpliﬁed ver
sion of our automatic mask generation pipeline We prompt
SAM with a 1616 regular grid of foreground points
which yields 768 predicted masks three per point We do
not ﬁlter by predicted IoU or stability Redundant masks
are removed by NMS Then we apply a Sobel ﬁlter to the
remaining masks unthresholded probability maps and set
values to zero if they do not intersect with the outer bound
ary pixels of a mask Finally we take a pixelwise max over
all the predictions linearly normalize the result to 01 and
apply edge NMS 13 to thin the edges
Visualizations In Fig 15 we show additional examples
of zeroshot edge predictions from SAM These qualitative
examples further illustrate how SAM tends to output sensi
ble edge maps despite not being trained for edge detection
We see that the edges can align well with the human anno
tations Although as previously mentioned since SAM is
not trained for edge detection it does not learn the biases of
the BSDS500 dataset and often outputs more edges than are
present in the ground truth annotations
D3 ZeroShot Object Proposals
Dataset and metrics We report the standard average recall
AR metric for masks at 1000 proposals on the LVIS v1
validation set 44 Since LVIS has highquality masks for
1203 object classes it provides a challenging test for ob
ject proposal generation We focus on AR1000 due to the
openworld nature of our model which will likely produce
many valid masks outside even the 1203 classes in LVIS To
measure performance on frequent common and rare categories we use AR1000 but measured against a ground
truth set containing just the corresponding LVIS categories
Baseline We use cascade ViTDetH as a baseline the
strongest model from 62 by AP on LVIS As noted in the
main text an object detector trained indomain can game
AR 16 and is expected to be a stronger baseline than other
models that focus on openworld proposals or segmenta
tion 58 105 To produce 1000 proposals we disable score
thresholding in the three cascade stages and as raise the
maximum number of predictions per stage to 1000
Method We use a modiﬁed version of SAMs automatic
mask generation pipeline for zeroshot transfer First to
make inference time comparable to that of ViTDet we do
not process image crops Second we remove ﬁltering by
predicted IoU and stability This leaves two tunable param
eters to get 1000 masks per image the input point grid and
the NMS threshold duplicate mask suppression We choose
a 6464 point grid and an NMS threshold of 09 which
produces 900 masks per image on average At evaluation
if greater than 1000 masks have been proposed in an im
age they are ranked by the average of their conﬁdence and
stability scores then truncated to the top 1000 proposals
We hypothesize that SAMs ability to output multiple
masks is especially valuable for this task since recall should
beneﬁt from proposals generated at multiple scales from
a single input point To test this we compare to an ab
lated version SAM that only outputs a single mask instead
of three SAM  singleoutput Since this model produces
fewer masks we further increase the number of points sam
pled and NMS threshold to 128 128 and 095 respectively
obtaining 950 masks per image on average Additionally
singleoutput SAM does not produce the IoU score used
to rank masks for NMS in the automatic mask generation
pipeline so instead masks are ranked randomly Testing
suggests this has similar performance to more sophisticated
methods of ranking masks such as using the max logit value
of the mask as a proxy for model conﬁdence
21ground truth ViTDet SAM ground truth ViTDet SAM
Figure 16 Zeroshot instance segmentation on LVIS v1 SAM produces higher quality masks than ViTDet As a zeroshot
model SAM does not have the opportunity to learn speciﬁc training data biases see topright as an example where SAM
makes a modal prediction whereas the ground truth in LVIS is amodal given that mask annotations in LVIS have no holes
D4 ZeroShot Instance Segmentation
Method For zeroshot instance segmentation we prompt
SAM with the boxes output by a fullysupervised ViTDetH
on COCO and LVIS v1 validation splits We apply an ad
ditional mask reﬁnement iteration by feeding the most con
ﬁdent predicted mask together with the box prompt back
to the mask decoder to produce the ﬁnal prediction We
show zeroshot instance segmentations predicted on LVIS
in Fig 16 Compared to ViTDet SAM tends to produce
higher quality masks with cleaner boundaries We conﬁrm
this observation with human studies in 74 Note that as a
zeroshot model SAM is not able to learn annotation biases
in a dataset For instance we see that SAM makes a valid
modal prediction for the plate whereas LVIS masks cannot
contain holes by design so the plate is annotated amodally
D5 ZeroShot TexttoMask
Model and training We use the largest publicly available
CLIP model 82  ViTL14336px  to compute text
and image embeddings which we 2normalize prior to use
To train SAM we use masks from the ﬁrst two stages of our
data engine Moreover we discard all masks with an area
smaller than 1002pixels We train this model with large
scale jitter 40 for 120k iterations with batch size 128 All
other training parameters follow our default settings
Generating training prompts To extract an input prompt
we ﬁrst expand the bounding box around each mask by a
random factor from 1 to 2 squarecrop the expanded
box to maintain its aspect ratio and resize it to 336 336
pixels Before feeding the crop to the CLIP image encoder
with 50 probability we zeroout pixels outside the mask
To ensure the embedding focuses on the object we use
masked attention in the last layer to restrict attention from
the output token to the image positions inside the mask Fi
nally our prompt is the output token embedding For train
ing we supply the CLIPbased prompt ﬁrst followed by ad
ditional iterative point prompts to reﬁne the prediction
Figure 17 Visualization of thresholding the similarities of
mask embeddings from SAMs latent space A query is in
dicated by the magenta box top row shows matches at a low
threshold bottom row at a high threshold The most similar
mask embeddings in the same image can often be seman
tically similar to the query mask embedding even though
SAM is not trained with explicit semantic supervision
Inference During inference we use the CLIP text encoder
without any modiﬁcations to create a prompt for SAM We
rely on the fact that text and image embeddings are aligned
by CLIP which allows us to train without any explicit text
supervision while using textbased prompts for inference
D6 Probing the Latent Space of SAM
Finally we perform an initial investigation to qualita
tively probe the latent space learned by SAM In particu
lar we are interested in whether SAM is able to capture any
semantics in its representation even though is not trained
with explicit semantic supervision To do so we compute
mask embeddings by extracting an image embedding from
SAM from an image crop around a mask and its horizon
tally ﬂipped version multiplying the image embedding by
the binary mask and averaging over spatial locations In
Fig 17 we show 3 examples of a query mask and similar
masks in the latent space in the same image We observe
22that the nearest neighbors for each query show some albeit
imperfect shape and semantic similarity Although these
results are preliminary they indicate that the representations
from SAM may be useful for a variety of purposes such as
further data labeling understanding the contents of datasets
or as features for downstream tasks
E Human Study Experimental Design
Here we describe details of the human study used to eval
uate mask quality in 71 and 74 The purpose of the
human study is to address two limitations of using IoU to
ground truth as a measure of predicted mask quality The
ﬁrst limitation is that for ambiguous inputs such as a single
point the model may be strongly penalized for returning a
valid mask of a different object than the ground truth The
second limitation is that ground truth masks may include
various biases such as systematic errors in the edge qual
ity or decisions to modally or amodally segment occluding
objects A model trained indomain can learn these biases
and obtain a higher IoU without necessarily producing bet
ter masks Human review can obtain a measure of mask
quality independent of an underlying ground truth mask in
order to alleviate these issues
Models For singlepoint evaluation we use RITM 92
singleoutput SAM and SAM to test two hypotheses First
we hypothesize that SAM produces visually higher quality
masks than baseline interactive segmentation models when
given a single point even when metrics such as IoU with
ground truth do not reveal this Second we hypothesize
that SAMs ability to disambiguate masks improves mask
quality for single point inputs since single output SAM may
return masks that average over ambiguous masks
For instance segmentation experiments we evaluate cas
cade ViTDetH 62 and SAM in order to test the hypothesis
that SAM produces visually higher quality masks even if it
obtains a lower AP due to the inability to learn speciﬁc an
notation biases of the validation dataset
Datasets For singlepoint experiments we select 7 datasets
from our set of 23 datasets since the full suite is too large
for human review We choose LVIS v05 17 VISOR 28
27 DRAM 24 IBD 17 NDD20 100 OVIS 81 and
iShape 111 which provide a diverse collection of images
including scenelevel egocentric drawn overhead under
water and synthetic imagery Additionally this set includes
datasets both where SAM outperforms RITM with IoU met
rics and viceversa For instance segmentation experiments
we use the LVIS v1 validation set allowing for direct com
parison to ViTDet which was trained on LVIS
Methodology We presented masks generated by the mod
els to professional annotators and asked them to rate each
mask using provided guidelines see G for the complete
guidelines Annotators were sourced from the same company that collected manually annotated masks for the data
engine An annotator was provided access to an image the
predicted mask of a single model and the input to the model
either a single point or single box and asked to judge the
mask on three criterion Does the mask correspond to a
valid object Does the mask have a clean boundary and
Does the mask correspond to the input They then submit
ted a rating from 110 indicating the overall mask quality
A score of 1 indicates a mask that corresponds to no ob
ject at all a low score 24 indicates that the mask has huge
errors such including huge regions of other objects or hav
ing large areas of nonsensical boundaries a middle score
56 indicates masks that are mostly sensible but still have
signiﬁcant semantic or boundary errors a high score 7
9 indicates masks with only minor boundary errors and a
score of 10 is for masks with no visible errors Annotators
were provided with ﬁve different views each designed to
help identify different error types
For single point experiments 1000 masks per dataset
were selected randomly from the same subsets used for
benchmarking zeroshot interactive segmentation see D1
for details on these subsets The model input was the cen
termost point calculated as the largest value of the distance
transform from the edge of the mask For instance seg
mentation experiments 1000 masks were selected from the
LVIS v1 validation set and the model input was the LVIS
ground truth box In all experiments masks with a size
smaller than 242pixels were excluded from sampling to
prevent showing raters a mask that was too small to judge
accurately For both memory and display reasons large im
ages were rescaled to have a max sidelength of 2000 before
predicting a mask In all experiments the same inputs were
fed to each model to produce a predicted mask
For comparison the ground truth masks from each
dataset were also submitted for rating For singlepoint
experiments this gave 4000 total rating jobs per dataset
1000 masks each for RITM SAM singleoutput SAM
and ground truth for instance segmentation experiments
it gave 3000 total jobs ViTDet SAM and ground truth
For each dataset these jobs were inserted with random
ordering into a queue from which 30 annotators drew jobs
In initial testing of the review study we provided each job to
ﬁve different annotators and found reasonable consistency
in scores the average standard deviation in score over the
ﬁve annotators was 083 Additionally the annotation com
pany deployed quality assurance testers who spot checked
a fraction of results for extreme departures from the guide
lines Thus for our experiments each job  ie rating one
mask in one image was completed by only a single anno
tator Average time spent per annotator per job was 90 sec
onds longer than our initial target of 30 seconds but still
sufﬁciently fast to collect a large number of ratings on each
of the 7 selected datasets
231 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings65  015 RITM
77  012 SAM  single output81  010 SAM
85  009 GTa LVIS v05 17
1 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings63  016 RITM
75  013 SAM  single output83  009 SAM
85  013 GT b VISOR 28 27
1 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings59  014 RITM
68  015 SAM  single output77  013 SAM
80  015 GT
c DRAM 24
1 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings71  012 RITM
79  011 SAM  single output83  008 SAM
84  009 GT d IBD 17
1 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings64  017 RITM
82  011 SAM  single output86  010 SAM
89  006 GT
e NDD20 100
1 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings61  015 RITM
77  012 SAM  single output72  013 SAM
88  009 GT f OVIS 81
1 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings49  016 RITM
62  017 SAM  single output71  015 SAM
93  006 GT
g iShape 111
Figure 18 Mask quality rating distributions by dataset from our human evaluation study
SAMbaseline SAMSAM single out
dataset pvalue CI99 pvalue CI99
point input RITM 92 baseline
LVIS v05 44 4e69 140 184 2e11 029 064
VISOR 28 27 7e98 181 224 7e26 058 094
DRAM 24 1e76 154 200 2e24 062 103
IBD 17 2e57 103 139 1e15 032 062
NDD20 100 2e86 188 237 5e08 019 055
OVIS 81 2e64 138 184 3e10 027 063
iShape 111 2e88 197 247 7e23 065 110
box input ViTDetH 62 baseline
LVIS v1 44 2e05 011 042 NA NA
Table 8 Statistical tests showing signiﬁcance that SAM has
higher mask quality ratings than baseline and singleoutput
SAM Pvalues are calculated by paired ttest while conﬁ
dence intervals for the difference in mean scores are calcu
lated by paired bootstrap on 10k samples All pvalues are
signiﬁcant and all conﬁdence intervals exclude zero
Results Fig 18 shows histograms over ratings for each
dataset in the singlepoint experiments We run statisticaltests for two hypotheses 1 that SAM gets higher scores
than the baseline model RITM or ViTDet and 2 that
SAM gets higher scores than singleoutput SAM Pvalues
are calculated via a paired ttest on the means of the model
scores which we supplement with a paired bootstrap test on
10k samples to ﬁnd the 99 conﬁdence interval for the dif
ference of means Table 8 shows pvalues and conﬁdence
intervals for these tests All statistical tests are strongly sig
niﬁcant and all conﬁdence intervals exclude zero
For instance segmentation Fig 11 of the main text
shows the histogram for ratings To compare to COCO
ground truth we additionally include 794 ratings of COCO
ground truth masks that were collected during our testing of
the human review process These masks were presented to
raters using an identical setup as the LVIS results For fair
comparison results for LVIS in Fig 11 were subsampled
to the same 794 inputs for each model and ground truth
For Table 8 the full 1000 ratings are used to run statistical
tests which show that SAMs mask quality improvement
over ViTDet is statistically signiﬁcant
24F Dataset Annotation and Model Cards
In F1 we provide a Dataset Card for SA1B follow
ing 39 in a list of questions and answers Next we pro
vide a Data Annotation Card in F2 for the ﬁrst two stages
of our data engine described in 4 following CrowdWork
Sheets 30 again as a list of questions and answers We
provide a Model Card following 75 in Table 9
F1 Dataset Card for SA1B
Motivation
1For what purpose was the dataset created Was there a speciﬁc task in
mind Was there a speciﬁc gap that needed to be ﬁlled Please provide a
description The contributions of our dataset to the vision community are
fourfold 1 We release a dataset of 11M images and 11B masks by far the
largest segmentation dataset to date 2 The dataset we release is privacy
protecting we have blurred faces and license plates in all images 3 The
dataset is licensed under a broad set of terms of use which can be found
at httpsaifacebookcomdatasetssegmentanything 4 The data is more
geographically diverse than its predecessors and we hope it will bring the
community one step closer to creating fairer and more equitable models
2Who created the dataset  eg which team research group and on behalf
of which entity  eg company institution organization The dataset was
created by the FAIR team of Meta AI The underlying images were collected
and licensed from a third party photo company
3Who funded the creation of the dataset If there is an associated grant
please provide the name of the grantor and the grant name and number
Meta AI funded the creation of the dataset
4Any other comments No
Composition
1What do the instances that comprise the dataset represent  eg documents
photos people countries Are there multiple types of instances  eg
movies users and ratings people and interactions between them nodes
and edges Please provide a description All of the instances in the dataset
are photos The photos vary in subject matter common themes of the photo
include locations objects scenes All of the photos are distinct however
there are some sets of photos that were taken of the same subject matter
2How many instances are there in total of each type if appropriate There
are 11 million images
3Does the dataset contain all possible instances or is it a sample not nec
essarily random of instances from a larger set If the dataset is a sample
then what is the larger set Is the sample representative of the larger set
eg geographic coverage If so please describe how this representa
tiveness was validatedveriﬁed If it is not representative of the larger set
please describe why not  eg to cover a more diverse range of instances
because instances were withheld or unavailable The dataset is composed
of images licensed from a photo provider The dataset contains all instances
licensed The images are photos ie not artwork although there are a few
exceptions The dataset includes all generated masks for each image in the
dataset We withheld 2k randomly selected images for testing purposes
4What data does each instance consist of Raw data  eg unprocessed
text or images or features In either case please provide a description
Each instance in the dataset is an image The images were processed to blur
faces and license plates to protect the identities of those in the image
5Is there a label or target associated with each instance If so please provide
a description Each image is annotated with masks There are no categories
or text associated with the masks The average image has 100 masks and
there are 11B masks in total
6Is any information missing from individual instances If so please provide
a description explaining why this information is missing  eg because it
was unavailable This does not include intentionally removed information
but might include eg redacted text Yes Each image is accompanied by
a short caption that describes the content and place of the photo in a free
form text Per our agreement with the photo provider we are not allowed to
release these captions However we use them in our paper to analyze the
geographical distribution of the dataset7Are relationships between individual instances made explicit  eg users
movie ratings social network links If so please describe how these rela
tionships are made explicit No there are no known relationships between
instances in the dataset
8Are there any errors sources of noise or redundancies in the dataset If
so please provide a description Errors The masks are generated by a
segmentation model so there may be errors or inconsistencies in the masks
Redundancies While no two images are the same there are instances of
images of the same subject taken close together in time
9Is the dataset selfcontained or does it link to or otherwise rely on external
resources  eg websites tweets other datasets If it links to or relies on
external resources a are there guarantees that they will exist and remain
constant over time b are there ofﬁcial archival versions of the complete
dataset  ie including the external resources as they existed at the time
the dataset was created c are there any restrictions  eg licenses fees
associated with any of the external resources that might apply to a dataset
consumer Please provide descriptions of all external resources and any
restrictions associated with them as well as links or other access points as
appropriate The dataset is selfcontained
10 Does the dataset contain data that might be considered conﬁdential  eg
data that is protected by legal privilege or by doctorpatient conﬁdentiality
data that includes the content of individuals nonpublic communications
If so please provide a description No
11 Does the dataset contain data that if viewed directly might be offensive
insulting threatening or might otherwise cause anxiety If so please de
scribe why We have two safety measures to prevent objectionable content
1 Photos are licensed from a photo provider and had to meet the terms of
service of the photo provider We requested that all objectionable content
be ﬁltered from the images we licensed 2 If a user observes objectionable
images in the dataset we invite them to report the images at segment
anythingmetacom for removal Despite the measures taken we observe
that a small portion of images contains scenes of protests or other gatherings
that focus on a diverse spectrum of religious beliefs or political opinions that
may be offensive We were not able to produce a ﬁltering strategy that re
moves all such images and rely on users to report this type of content
12 Does the dataset identify any subpopulations  eg by age gender If so
please describe how these subpopulations are identiﬁed and provide a de
scription of their respective distributions within the dataset The dataset
does not identify any subpopulations of the people in the photos
13 Is it possible to identify individuals  ie one or more natural persons ei
ther directly or indirectly  ie in combination with other data from the
dataset If so please describe how No Images were subjected to a face
blurring model to remove any personally identiﬁable information If a user
observes any anonymization issue we invite them to report the issue and
the image ids at segmentanythingmetacom
14 Does the dataset contain data that might be considered sensitive in any way
eg data that reveals race or ethnic origins sexual orientations religious
beliefs political opinions or union memberships or locations ﬁnancial or
health data biometric or genetic data forms of government identiﬁcation
such as social security numbers criminal history If so please provide
a description The dataset contains scenes of protests or other gatherings
that may suggest religious beliefs political opinions or union memberships
However the faces of all people in the dataset have been anonymized via
facial blurring so it is not possible to identify any person in the dataset
15 Any other comments No
Collection Process
1How was the data associated with each instance acquired Was the data
directly observable  eg raw text movie ratings reported by subjects  eg
survey responses or indirectly inferredderived from other data  eg part
ofspeech tags modelbased guesses for age or language If the data was
reported by subjects or indirectly inferredderived from other data was the
data validatedveriﬁed If so please describe how The released masks
associated with each image were automatically inferred by our segmentation
model SAM The masks that were collected using modelassisted manual
annotation will not be released Quality was validated as described in 5
2What mechanisms or procedures were used to collect the data  eg hard
ware apparatuses or sensors manual human curation software programs
software APIs How were these mechanisms or procedures validated The
images in the dataset are licensed from an image provider They are all pho
tos taken by photographers with different cameras
253If the dataset is a sample from a larger set what was the sampling strategy
eg deterministic probabilistic with speciﬁc sampling probabilities We
withheld 2k randomly selected images for testing purposes The rest of
the licensed images are included in the dataset
4Who was involved in the data collection process  eg students crowdwork
ers contractors and how were they compensated  eg how much were
crowdworkers paid The released masks were automatically inferred by
SAM For details on our modelassisted manual annotation process see our
Data Annotation Card in F2 Note these masks will not be released
5Over what timeframe was the data collected Does this timeframe match
the creation timeframe of the data associated with the instances  eg recent
crawl of old news articles If not please describe the timeframe in which
the data associated with the instances was created The licensed photos
vary in their date taken over a wide range of years up to 2022
6Were any ethical review processes conducted  eg by an institutional re
view board If so please provide a description of these review processes
including the outcomes as well as a link or other access point to any sup
porting documentation If the dataset does not relate to people you may skip
the remaining questions in this section We underwent an internal privacy
review to evaluate and determine how to mitigate any potential risks with
respect to the privacy of people in the photos Blurring faces and license
plates protects the privacy of the people in the photos
7Did you collect the data from the individuals in question directly or obtain
it via third parties or other sources  eg websites We licensed the data
from a third party photo provider
8Were the individuals in question notiﬁed about the data collection If so
please describe or show with screenshots or other information how no
tice was provided and provide a link or other access point to or other
wise reproduce the exact language of the notiﬁcation itself The images
are licensed from a third party who provided appropriate representations
regarding the collection of any notices and consents as required from indi
viduals In addition all identiﬁable information  eg faces license plates
was blurred Under the terms of the dataset license it is prohibited to attempt
to identify or associate an image with a particular individual
9Did the individuals in question consent to the collection and use of their
data If so please describe or show with screenshots or other informa
tion how consent was requested and provided and provide a link or other
access point to or otherwise reproduce the exact language to which the
individuals consented The images are licensed from a third party who pro
vided appropriate representations regarding the collection of any notices and
consents as required from individuals In addition all identiﬁable informa
tion  eg faces license plates was blurred from all images For avoidance
of doubt under the terms of the dataset license it is prohibited to attempt to
identify or associate an image with a particular individual
10 If consent was obtained were the consenting individuals provided with a
mechanism to revoke their consent in the future or for certain uses If
so please provide a description as well as a link or other access point
to the mechanism if appropriate We invite users to report at segment
anythingmetacom for images removal
11 Has an analysis of the potential impact of the dataset and its use on data
subjects  eg a data protection impact analysis been conducted If so
please provide a description of this analysis including the outcomes as
well as a link or other access point to any supporting documentation To
eliminate any potential impact on people whose photos are included in the
dataset identiﬁable information faces license plates has been blurred
12 Any other comments No
Preprocessing  Cleaning  Labeling
1Was any preprocessing  cleaning  labeling of the data done  eg dis
cretization or bucketing tokenization partofspeech tagging SIFT fea
ture extraction removal of instances processing of missing values If so
please provide a description If not you may skip the remaining questions
in this section We resized the highresolution licensed images such that
the shorter side is 1500 pixels and only processed the images to remove any
identiﬁable and personal information from the photos faces license plates
2Was the raw data saved in addition to the preprocessedcleanedlabeled
data  eg to support unanticipated future uses If so please provide a link
or other access point to the raw data No as we removed the data for
safety reasons and to respect privacy we do not release the unaltered photos
3Is the software that was used to preprocesscleanlabel the data avail
able If so please provide a link or other access point We used theRetinaFace 88 89 model httpsgithubcomserengilretinaface to detect
faces The model used to blur license plates has not been made public
Uses
1Has the dataset been used for any tasks already If so please provide a
description The dataset was used to train our segmentation model SAM
2Is there a repository that links to any or all papers or systems that use the
dataset If so please provide a link or other access point No However all
users of the dataset must cite it so its use is trackable via citation explorers
3What other tasks could the dataset be used for We intend the dataset
to be a largescale segmentation dataset However we invite the research
community to gather additional annotations for the dataset
4Is there anything about the composition of the dataset or the way it was
collected and preprocessedcleanedlabeled that might impact future uses
For example is there anything that a dataset consumer might need to know
to avoid uses that could result in unfair treatment of individuals or groups
eg stereotyping quality of service issues or other risks or harms  eg
legal risks ﬁnancial harms If so please provide a description Is there
anything a dataset consumer could do to mitigate these risks or harms We
have an analysis of the approximate geographic and income level coverage
of our dataset in 6 While we believe our dataset to be more representative
than most of the publicly existing datasets at this time we acknowledge
that we do not have parity across all groups and we encourage users to be
mindful of potential biases their models have learned using this dataset
5Are there tasks for which the dataset should not be used If so please pro
vide a description Full terms of use for the dataset including prohibited use
cases can be found at httpsaifacebookcomdatasetssegmentanything
6Any other comments No
Distribution
1Will the dataset be distributed to third parties outside of the entity  eg
company institution organization on behalf of which the dataset was cre
ated If so please provide a description The dataset will be available for
the research community
2How will the dataset will be distributed  eg tarball on website API
GitHub Does the dataset have a digital object identiﬁer DOI The
dataset is available at httpsaifacebookcomdatasetssegmentanything
3When will the dataset be distributed The dataset will be released in 2023
4Will the dataset be distributed under a copyright or other intellectual
property IP license andor under applicable terms of use ToU If
so please describe this license andor ToU and provide a link or other
access point to or otherwise reproduce any relevant licensing terms
or ToU as well as any fees associated with these restrictions Yes
The license agreement and terms of use for the dataset can be found at
httpsaifacebookcomdatasetssegmentanything Users must agree to the
terms of use before downloading or using the dataset
5Have any third parties imposed IPbased or other restrictions on the data
associated with the instances If so please describe these restrictions and
provide a link or other access point to or otherwise reproduce any relevant
licensing terms as well as any fees associated with these restrictions Full
terms of use and restrictions on use of the SA1B dataset can be found at
httpsaifacebookcomdatasetssegmentanything
6Do any export controls or other regulatory restrictions apply to the dataset
or to individual instances If so please describe these restrictions and pro
vide a link or other access point to or otherwise reproduce any supporting
documentation The license and restrictions on use of the SA1B dataset
can be found at httpsaifacebookcomdatasetssegmentanything
7Any other comments No
Maintenance
1Who will be supportinghostingmaintaining the dataset The dataset will
be hosted at httpsaifacebookcomdatasetssegmentanything and main
tained by Meta AI
2How can the ownercuratormanager of the dataset be contacted  eg email
address Please email segmentanythingmetacom
3Is there an erratum If so please provide a link or other access point No
4Will the dataset be updated  eg to correct labeling errors add new in
stances delete instances If so please describe how often by whom and
how updates will be communicated to dataset consumers  eg mailing list
26GitHub To aid reproducibility of research using SA1B the only updates
will be to remove reported images
5If the dataset relates to people are there applicable limits on the retention of
the data associated with the instances  eg were the individuals in question
told that their data would be retained for a ﬁxed period of time and then
deleted If so please describe these limits and explain how they will be
enforced There are no limits on data retention We took measures to remove
personally identiﬁable information from any images of people Users may
report content for potential removal here segmentanythingmetacom
6Will older versions of the dataset continue to be sup
portedhostedmaintained If so please describe how If not please
describe how its obsolescence will be communicated to dataset consumers
No as the only updates will be to remove potentially harmful content we
will not keep older versions with the content
7If others want to extendaugmentbuild oncontribute to the dataset is there
a mechanism for them to do so If so please provide a description Will
these contributions be validatedveriﬁed If so please describe how If not
why not Is there a process for communicatingdistributing these contribu
tions to dataset consumers If so please provide a description We encour
age users to gather further annotations for SA1B Any users who generate
annotations will be liable for hosting and distributing their annotations
8Any other comments No
F2 Data Annotation Card
Task Formulation
1At a high level what are the subjective aspects of your task Segmenting
objects present in an image is inherently a subjective task For instance
one annotator may segment two boots as one mask whereas another may
segment each boot separately Depending on annotatorss skills the quality
of the mask and the number of masks per image are different between an
notators Despite these subjective aspects of the task we believed efﬁcient
annotation was possible as the data was annotated in a permask fashion
with the main focus on the diversity of the data rather than completeness
2What assumptions do you make about annotators Our annotators worked
full time on our annotation task with very small attrition rate This made
it possible to train the annotators providing feedback and answering their
questions on a regular basis Speciﬁcally 1 By giving a clear understand
ing of the goals of this work and providing clear guidelines including vi
suals and video recordings of the tasks annotators had enough context to
understand and perform the tasks reasonably 2 Sharing objectives and
key results and meeting weekly with annotators increased the likelihood
that annotators improved annotation quality and quantity over time
3How did you choose the speciﬁc wording of your task instructions What
steps if any were taken to verify the clarity of task instructions and wording
for annotators As our task was annotating images the annotation guide
lines included visual examples Our research team completed 30 annotation
tasks to identify any obvious challenges using the annotation tool collec
tively decide how to handle complex cases and reﬁne the guidelines The
research team met with the annotators weekly for feedback sessions Videos
of the research team performing the task were shared live with the annota
tors followed by QA sessions Annotators were able to give feedback on
unclear aspects both during the feedback session and asynchronously
4What if any risks did your task pose for annotators and were they informed
of the risks prior to engagement with the task No identiﬁed risks Images
were ﬁltered for objectionable content prior to the annotation phase
5What are the precise instructions that were provided to annotators We
provide only highlevel instructions Given an image we aim at segment
ing every possible object Annotators generate a mask for every potential
object they can identify An object can be segmented using our interactive
segmentation tool either by using corrective foregroundbackground clicks
to addremove parts of the mask or by drawing a bounding box around the
object Masks can be reﬁned using pixelprecise tools
Selecting Annotations
1Are there certain perspectives that should be privileged If so how did you
seek these perspectives out We chose to work with annotators that have
worked on other vision annotation tasks before
2Are there certain perspectives that would be harmful to include If so how
did you screen these perspectives out No3Were sociodemographic characteristics used to select annotators for your
task If so please detail the process No
4If you have any aggregated sociodemographic statistics about your anno
tator pool please describe Do you have reason to believe that sociode
mographic characteristics of annotators may have impacted how they an
notated the data Why or why not We worked with 130 annotators The
annotators were all based in Kenya We do not believe sociodemographic
characteristics of annotators meaningfully impacted the annotated data
5Consider the intended context of use of the dataset and the individuals
and communities that may be impacted by a model trained on this dataset
Are these communities represented in your annotator pool The Segment
Anything 1B SA1B dataset is to be used for research purposes only
The SA1B dataset is one of the most geographically diverse segmentation
dataset as discussed in 6 In addition we analyze the responsible AI axes
of a model trained on the dataset in 6
Platform and Infrastructure Choices
1What annotation platform did you utilize At a high level what considera
tions informed your decision to choose this platform Did the chosen plat
form sufﬁciently meet the requirements you outlined for annotator pools
Are any aspects not covered We used a proprietary annotation platform
2What if any communication channels did your chosen platform offer to
facilitate communication with annotators How did this channel of com
munication inﬂuence the annotation process andor resulting annotations
We manually reviewed annotations and shared feedback with the annotators
on a weekly basis We communicated common mistakes or inconsisten
cies and the corresponding corrections In addition the annotators were
given feedback for improvements daily by the annotation QA team Out
side the weekly feedback sessions annotators had access to a spreadsheet
and chat group to facilitate communication with the research team This
process greatly improved the average speed and quality of the annotations
3How much were annotators compensated Did you consider any partic
ular pay standards when determining their compensation If so please
describe Annotators were compensated with an hourly wage set by the
vendor The vendor is a Certiﬁed B Corporation
Dataset Analysis and Evaluation
1How do you deﬁne the quality of annotations in your context and how did
you assess the quality in the dataset you constructed Annotators were ﬁrst
placed into training They followed a 1day training session led by the ven
dor and then were asked to annotate a large number of examples from a
training queue Annotators graduated from training to production after the
vendor QA team in collaboration with the research team manually spot
checked the annotators masks to ensure quality On average annotators
spent one week in training before graduating Production quality assess
ment followed a similar process the vendor QA team and the research team
manually reviewed the annotations weekly sharing feedback weekly
2Have you conducted any analysis on disagreement patterns If so what
analyses did you use and what were the major ﬁndings Did you analyze
potential sources of disagreement We pointed out common mistakes dur
ing weekly meetings with the annotators
3How do the individual annotator responses relate to the ﬁnal labels released
in the dataset The annotations were only used to train early versions of the
SAM model and we do not currently plan to release them
Dataset Release and Maintenance
1Do you have reason to believe the annotations in this dataset may change
over time Do you plan to update your dataset No except to remove
objectionable images
2Are there any conditions or deﬁnitions that if changed could impact the
utility of your dataset We do not believe so
3Will you attempt to track impose limitations on or otherwise inﬂuence how
your dataset is used If so how The SA1B dataset will be released under
a license agreement allowing use for certain research purposes and protec
tions for researchers Researchers must agree to the terms of the license
agreement to access the dataset
4Were annotators informed about how the data is externalized If changes to
the dataset are made will they be informed No we do not plan to release
the manual annotations at the moment
5Is there a process by which annotators can later choose to withdraw their
data from the dataset If so please detail No
27Model Overview
Name SAM or Segment Anything Model
Version 10
Date 2023
Organization The FAIR team of Meta AI
Mode type Promptable segmentation model
Architecture See 3
Repository httpsgithubcomfacebookresearchsegmentanything
Citation httpsresearchfacebookcompublicationssegmentanything
License Apache 20
Intended Use
Primary intended uses SAM is intended to be used for any promptbased segmentation task We explored its use in segmenting objects
from a point 71 edge detection 72 segmenting all objects 73 and segmenting detected objects 74
We explored how SAM can integrate with other vision models to segment objects from text 75
Primary intended users SAM was primarily developed for research The license for SAM can be found at
httpsgithubcomfacebookresearchsegmentanything
Outofscope use cases See terms of use for SAM found at httpsgithubcomfacebookresearchsegmentanything See Use Cases under
Ethical Considerations 
Caveats and recommendations SAM has impressive zeroshot performance across a wide range of tasks We note however that in the zeroshot
setting there may be multiple valid ground truth masks for a given input We recommend users take this into
consideration when using SAM for zeroshot segmentation SAM can miss ﬁne structures and can hallucinate
small disconnected components See 8 for a discussion of limitations
Relevant Factors
Groups SAM was designed to segment any object This includes stuff andthings 
Instrumentation and environment We benchmarked SAM on a diverse set of datasets and found that SAM can handle a variety of visual data including
simulations paintings underwater images microscopy images driving data stereo images ﬁsheye images  See
D1 and Table 7 for information on the benchmarks used
Metrics
Model performance measures We evaluated SAM on a variety of metrics based on the downstream task in our experiments
mIoU  We used the mean intersectionoverunion after a given number of prompts to evaluate the segmen
tation quality of a mask when prompted with points
Human evaluation  We performed a human study detailed in E to evaluate the real world performance
of SAM We compared the masks generated by SAM to a baseline stateoftheart interactive segmentation
model RITM 92 using a perceptual quality scale from 1 to 10
AP We used average precision to evaluate instance segmentation for a given box and edge detection
AR1000  We used average recall to evaluate object proposal generation
ODS OIS AP  R50  We used the standard edge detection evaluation metrics from BSDS500 72 3
Evaluation Data
Data sources See D1
Training Data
Data source See Data Card in F1
Ethical Considerations
Data We trained SAM on licensed images The images were ﬁltered for objectionable content by the provider but we
acknowledge the possibility of false negatives We performed a geographic analysis of the SA1B dataset in 6
While SA1B is more geographically diverse than many of its predecessors we acknowledge that some geographic
regions and economic groups are underrepresented
Cost and impact of compute SAM was trained on 256 A100 GPUS for 68 hours We acknowledge the environmental impact and cost of training
large scale models The environmental impact of training the released SAM model is approximately 6963 kWh
resulting in an estimated 28 metric tons of carbon dioxide given the speciﬁc data center used using the calculation
described in 77 and the ML CO 2Impact calculator 61 This is equivalent to 7k miles driven by the average
gasolinepowered passenger vehicle in the US 101 We released the SAM models to both reduce the need for
retraining and lower the barrier to entry for large scale vision research
Risks and harms We evaluated SAM for fairness in 6 Downstream use cases of SAM will create their own potential for biases
and fairness concerns As such we recommend users run their own fairness evaluation when using SAM for their
speciﬁc use case
Use cases We implore users to use their best judgement for downstream use of the model
Table 9 Model Card for SAM following the procedure detailed in 75
28We have several models that when provided with a click or a box as input output a mask We would
like to compare the quality of these models by rating the quality of their masks on many examples
The interface will be different than for regular mask annotation
 Each job reviews one mask in one image
 On the right there will be ﬁve image thumbnails in two rows Each thumbnail can be moused
over to show the image at a larger size Clicking on the thumbnail will make it full screen and
clicking again will return to the original screen
 The images show the same mask in ﬁve different views On the top row left the image
without the mask middle the mask overlaid on the image and right the mask alone On
the bottom row left a zoomed in view of the object without a mask and right a zoomed
in view of the mask overlaid on the image These views are provided to make it easy to see
different types of mask errors
 The mask will be in red when overlaid on the image
 When shown by itself the mask is yellow and the background is purple
 Each image will include either a blue dot or a blue and white box This is the input to the
model as if you had clicked at this location or drawn this box
 On the left there are buttons labeled 110 This is used to rate the quality of the shown mask
Objective and Setup
 Example interface page There will be ﬁve images on the
right and a question box on the left
Mouse over an image to show the full image
 Click on an image to make it full screen The arrows will cy
cle between images Click again to return to previous view
The ﬁrst image on the top row shows the image without a
mask A blue point will be on the object of interest or a
blue and white box will surround it
The second image on the top row shows the mask for the
object in red
The third image on the top row shows the mask only The
mask is in yellow and the background is purple
The ﬁrst image on the bottom row shows a zoomed in view
of the object without a mask
The second image on the bottom row shows a zoomed in
view of the object with a mask The mask is in red
On the left are buttons to rate the mask quality with selec
tions 110What we would like you to do for each job
 Please aim to spend up to 30 seconds per job
 Mouseover or click each of the three images of the mask on the right to get a sense of the
quality of the mask The thumbnail is too small to judge a mask do not judge a mask by the
thumbnail alone Each image can provide a different signal on possible mask errors
 The unzoomed image can give context for the mask does this mask correspond to an actual
object
 The maskonly image can show if the mask has small holes or separated incorrect pixels
 The zoomed image can show if the mask boundaries make sense
 Judge the quality of the mask on three criterion Examples will follow
 Does the mask correspond to an actual object
 Does the mask have a good boundary
 Does the mask correspond to the provided point or box
 Rate the quality of the mask on a scale of 110 using the dropdown box on the left
 Next are details and examples for judging mask quality according to the three criterion These
are just examples and other cases may come up please use your best judgment when deter
mining if something is a good mask
TaskDoes the mask correspond to an actual object
 Valid objects can include
 Entire single objects such as a person shirt or tree
 Logical parts of objects a chair leg a car door a tabletop
 Collections of objects a stack of books a crowd of people
 Stuff the ground the sky
 Example errors a mask may have The severity of these errors may be minor or major
 Include a piece of another object the mask of a person including the arm of a nearby
person
 Miss part of an object the mask covers only one part of a building obscured by a tree in
the foreground
 Combine two unrelated things a single mask covers both a mug and a pen on a desk
 Include an arbitrary part of a collection for a point input a point is on one apple but
the mask covers three apples in a pile of many apples If a box surrounds an arbitrary
collection it is not an error to provide a mask for these objects
 If you are unsure a good ruleofthumb is can you name the object in question However
some things that are hard to name may still be good objects an unusual component of a
machine something at the edge of the image for which it is hard to determine what it is
Judging Mask Quality 1 of 3
Does the mask have a good boundary
 Errors in the boundary can include
 Incorrect holes in the mask
 Incorrect pixels included separated from the main part of the mask
 Poor edge quality where the mask does not exactly match the edge of the object
 Failure to consistently handle obscuring foreground objects a mask that covers obscuring
objects is ﬁne and a mask that doesnt cover obscuring objects is ﬁne but one that does
some of both has an error
 Pixelation of a small mask is not an error as long as the mask still matches the edges of
the object
Judging Mask Quality 2 of 3Does the mask correspond to the provided point or box
 For points
 The point needs to be on the mask
 The size or position of the object with respect to the point does not matter a point on
someones gloved hand can correspond to the glove or to the entire person both are valid
masks
 For boxes
 The object needs to be the best object that is the size of the box if a box is around some
ones entire head but the mask is of their hair this is an error their hair is in the box but is
not the correct object
 If the box clearly corresponds to a given object but is slightly smaller than it it is okay if
the mask goes slightly outside a box if a box around a person misses their extended hand
the mask can still include their hand even if the mask goes outside the box
Judging Mask Quality 3 of 3
 Example error of Include a piece of another object The
elephant mask contains a piece of another nearby elephant
Example error of Missing a part of an object the mask is
missing a disconnected part of the object the back half of
the zebra and the right portion of the plate
Example error of Include an arbitrary part of a collection
In top top image the point is on one orange rind but the
mask covers two orange rinds This is a mask error the
mask covers an arbitrary number of objects in the collection
and should either cover one orange rind or all of them In
the bottom image the box is around both vegetables Since
this is the best match to the box this is not a mask error
Example error for Incorrect holes in the mask This mask
has holes in the upper left and on the left sides black ar
rows These holes are much easier to see on the mask
only image
Example error for Incorrect pixels included separated from
the main part of the mask The mask only view reveals a
few stray incorrect pixels on the clock face
Example error for Poor edge quality The mask has poor
edge quality both along the edge of the umbrella as well as
along the thin pole
Figure 19 Here we provide the complete guidelines given to annotations for the human review of mask quality Some images
been edited slightly and faces have been blurred to enable release Best viewed with zoom part 1 of 2
G Annotation Guidelines
We provide the complete guidelines given to annotations
for the human review of mask quality in Fig 19 and Fig 20
29Example for Combine two unrelated things The point in
dicates the lizard but the mask covers both the lizard and a
bird This is a mask error
Example error for Failure to consistently handle obscuring
foreground objects The pole on the right blue arrow is
excluded from the mask while the pole on the left is in
cluded in the object black arrow The mask should either
include or exclude both of these
Example of Pixelation of a small mask this mask has an
imperfect boundary since it extends beyond the object at
the black arrow However the blocky pattern of the mask
is not an error since when zoomed in this much the image
is also blocky the same way
Example error for consistency with the provided point The
mask does not agree with the blue point so this is a mask
error
Example for consistency with the provided point For this
input point but the logo left and the container right are
valid objects since the blue point lies on both of them Nei
ther mask has a mask error
Example for consistency with a box The box surrounds the
bowl of oranges but the mask is only of a single orange
This is a mask error
Example for consistency with a box The boxs shape ﬁts
the zebra Even though the mask extends slightly outside
the box to include the zebras left leg this is not an errorOverall mask quality is subjective each of the above errors may hurt mask quality only a little or a
lot depending on how large the error is Please use your best judgment when choosing mask scores
and try to stay consistent from masktomask Here are some general guidelines for what different
scores should correspond to
 A score of 1 It is not possible to tell what object this mask corresponds to This includes the
case that there is no mask visible at all
 A low score 24 The object is mostly identiﬁable but the mask quality is extremely poor
eg large regions of the mask cover other objects large regions of the object missing ex
tremely splotchy mask boundaries that cut through the middle of the object
 A mid score 56 The object is identiﬁable and the boundary is mostly correct but there
are major errors missing a signiﬁcant disconnected part of the object containing a signiﬁcant
part of another object very poor boundary quality in one area of the object but not the entire
object
 A high score 79 The object is identiﬁable and errors are small and rare missing a small
heavily obscured disconnected component having small regions where the mask boundary
does not quite match the object boundary
 A score of 10 The mask is pixelperfect it has no identiﬁable errors at all
Mask Scoring
Example of a mask with a score of 1 It is not clear what
object this mask corresponds to
Example of a mask with a low score 24 The main ob
ject is identiﬁable but the mask includes a large incorrect
portion of another object
Example of a mask with a low score 24 The main ob
ject is identiﬁable but a large random part of the object is
missing
Example of a mask with a lowtomedium score 45 The
object is identiﬁable and the edges are all correct but the
mask incorrectly includes the hand of the person on the left
Example of a mask with a medium score 56 The mask
clearly corresponds to the plate but the boundary with the
wafﬂe is quite poor
Example of a mask with a medium score 56 the object
is easy to identify and most of the edges make sense How
ever there is a signiﬁcant disconnected part their arm inside
the frame that is mostly missing as well as splotchy pixels
in this region
Example of a mask with a mediumtohigh score 68 the
mask has two smallish regions of poor boundary at the top
of the mask and on the bottom right
Example of a mask with a mediumtohigh score 68 The
wreath is a valid object that is the size of the box the entire
wreath  clock would also be a valid object However there
are incorrect stray mask pixels on the clock
Example of a mask with a high score 79 The boundary of
the horse is almost entirely correct except for the right side
of its back leg The mask consistently includes all of the
equipment that horse is wearing and has logical boundaries
Example of a mask with a very high score  9 There are
only minor errors around the edge of the mask The blocky
pixelation is not an error since the image is also blocky at
this scale
Example of a mask with a very high score 910 the mask
has only very minor errors in the edge on the bottom right
Example of a mask with a very high score 910 There are
only minor errors around the edge of the mask
Figure 20 Here we provide the complete guidelines given to annotations for the human review of mask quality Some images
been edited slightly and faces have been blurred to enable release Best viewed with zoom part 2 of 2
30
  Denoising Diffusion Probabilistic Models
Jonathan Ho
UC Berkeley
jonathanhoberkeleyeduAjay Jain
UC Berkeley
ajayjberkeleyeduPieter Abbeel
UC Berkeley
pabbeelcsberkeleyedu
Abstract
We present high quality image synthesis results using diffusion probabilistic models
a class of latent variable models inspired by considerations from nonequilibrium
thermodynamics Our best results are obtained by training on a weighted variational
bound designed according to a novel connection between diffusion probabilistic
models and denoising score matching with Langevin dynamics and our models nat
urally admit a progressive lossy decompression scheme that can be interpreted as a
generalization of autoregressive decoding On the unconditional CIFAR10 dataset
we obtain an Inception score of 946 and a stateoftheart FID score of 317 On
256x256 LSUN we obtain sample quality similar to ProgressiveGAN Our imple
mentation is available at httpsgithubcomhojonathanhodiffusion 
1 Introduction
Deep generative models of all kinds have recently exhibited high quality samples in a wide variety
of data modalities Generative adversarial networks GANs autoregressive models ﬂows and
variational autoencoders V AEs have synthesized striking image and audio samples  14273
58382510324457263345 and there have been remarkable advances in energybased
modeling and score matching that have produced images comparable to those of GANs 11 55
Figure 1 Generated samples on CelebAHQ 256256left and unconditional CIFAR10 right
34th Conference on Neural Information Processing Systems NeurIPS 2020 Vancouver CanadaarXiv200611239v2  csLG  16 Dec 2020latexit sha1_base647yFrn0YPyuP5dVIvc7Tl2zcbSgAAABHicbVBNSwMxEJ2tX7VdNWjl2ARPJXdKuix6MVjBfsB7VKyaXYbmk2WJKvU0lixYMiXv0p3vw3pu0etPXBwOO9GWbmhSln2njet1NYW9Y3Cpul3Z29bL7sFhS8tMEdokkkvVCbGmnAnaNMxw2kkVxUnIaTsc3cz89gNVmklxb8YpDRIcCxYxgo2Vm65x6WIFYuHBislHtuxat6c6BV4uekAjkafferN5AkS6gwhGOtu76XmmCClWGE02mpl2maYjLCMe1aKnBCdTCZHz5Fp1YZoEgqW8Kgufp7YoITrcdJaDsTbIZ62ZuJ3ndzERXwYSJNDNUkMWiKOPISDRLAQ2YosTwsSWYKGZvRWSIFSbGZlWyIfjLL6SVq3qn1drdxeVnUeRxGO4QTOwIdLqMMtNKAJBDJ4hld4c56cFfdVi0Fpx85gjwPn8AXOGk5olatexit
xTxtxt1x0
latexit sha1_base64l4LvSgM7PR7Ikkuy5soikK4gpUAAAEoXictVLditNAFE7XqGv92a5eejOYLexKLU0VFKRQ9EYvhCrb3YUklOlk2g6dnzBzYrcb8zKlUgazhJK6atuiB4YODMTn8YJZwY6nW1vRvuzVu39U7967CgcfjwzKhUEzokiit9McaGcibpEBhwepFoisWY0PxG3hP9MtWFKnsIyoZHAU8kmjGCwplHjeygwzAjThNM4KzjSXaZj05zFHIlp5pNZ4C1VgsUkliB2TXoQLYCpe4rJwZhJM6NPMJyLPt9IM0SwBA0tOUaVGBs8J8mWVRH6eSjhtdpd0pBu4qVjxnLYPR4d7XMFYkFVQC4diYwO8kEGVYA7P183qYGmr3meMpDawqsaAmykpEctS0lhhNlLZPAiqt1YwMC2OWYmwjiynNtq8ws4XpDB5FWVMJilQSVaNJilHoFABL4qZpgT40irYntTOisgMa0zAkqC0QbYMquIfCcYssbsBH1UNIFUUJgGVePGfhR1qyj1YETXAaHSqAnp836lGftUfdNcFiqbBT8L2jouQdvE9iVAoVUyDWONFa5XVYlJSjezEPTBlmCSiVQgw65or2vBaE0Y5z1e4DVeBmhstwJyo5C0YeZ53vdoz19lhVjly71K6xRbZbOrbLCS8HMwmVZ7W9zeFc567b953uxxde82a3vOYeJcz4zkun77xzBs7QIbUPNVP7Ustdz33vDtxPq9C92jrnkbMhbvAD81mObwlatexitpxt1xt
latexit sha1_base64XVzP503G8Ma8Lkwk3KKGZcZJbZ0AAACEnicbVC7SgNBFJ2Nrxhfq5Y2g0FICsNuFEwZsLGMYB6QLMvsZDYZMvtg5q4Y1nyDjb9iY6GIrZWdfMk2SImHrhwOOde7r3HiwVXYFkRm5tfWNzK79d2Nnd2z8wD49aKkokZU0aiUh2PKKY4CFrAgfBOrFkJPAEa3uj66nfvmdS8Si8g3HMnIAMQu5zSkBLrlmO3R4MGZBSLyAw9Pz0YeKmcG5P8CNekKDsmkWrYs2AV4mdkSLK0HDN714oknAQqCCKNW1rRiclEjgVLBJoZcoFhM6IgPW1TQkAVNOOntpgs00sdJHWFgGfq4kRKAqXGgac7p0eqZW8qud1EBrTsrDOAEW0vkiPxEYIjzNBe5ZBTEWBNCJde3YjokklDQKRZ0CPbyy6ukVa3YF5Xq7WWxXsviyKMTdIpKyEZXqI5uUAM1EUVP6AW9oXfj2Xg1PozPeWvOyGaO0R8YX7bCp4Flatexitqxtxt1
latexit sha1_base64eAZ87UuTmAQoJ4u19RGH5tAbCIAAACC3icbVC7TgJBFJ31ifhatbSZQEywkOyiiZQkNpaYyCMBspkdZmHC7MOZu0ay0tv4KzYWGmPrD9j5N87CFgieZJIz59ybe9xI8EVWNaPsbK6tr6xmdvKbs7u2bB4dNFcaSsgYNRSjbLlFM8IA1gINg7Ugy4ruCtdzRVeq37plUPAxuYRyxnk8GAfc4JaAlxyzclbogaHrJQ8TBAjnvsmcGZPTh2zaJWtKfAysTNSRBnqjvnd7Yc09lkAVBClOrYVQS8hEjgVbJLvxopFhI7IgHU0DYjPVCZ3jLBJ1rpYyUgWApp8R0J8pcaqyvTRdWil4reZ0YvGov4UEUAwvobJAXCwwhToPBfS4ZBTHWhFDJ9a6YDokkFHR8eR2CvXjyMmlWyvZ5uXJzUaxVszhy6BgVUAnZ6BLV0DWqowai6Am9oDf0bjwbr8aH8TkrXTGyniP0B8bXL1hmu8latexitFigure 2 The directed graphical model considered in this work
This paper presents progress in diffusion probabilistic models  53 A diffusion probabilistic model
which we will call a diffusion model for brevity is a parameterized Markov chain trained using
variational inference to produce samples matching the data after ﬁnite time Transitions of this chain
are learned to reverse a diffusion process which is a Markov chain that gradually adds noise to the
data in the opposite direction of sampling until signal is destroyed When the diffusion consists of
small amounts of Gaussian noise it is sufﬁcient to set the sampling chain transitions to conditional
Gaussians too allowing for a particularly simple neural network parameterization
Diffusion models are straightforward to deﬁne and efﬁcient to train but to the best of our knowledge
there has been no demonstration that they are capable of generating high quality samples We
show that diffusion models actually are capable of generating high quality samples sometimes
better than the published results on other types of generative models Section 4 In addition we
show that a certain parameterization of diffusion models reveals an equivalence with denoising
score matching over multiple noise levels during training and with annealed Langevin dynamics
during sampling Section 32  5561 We obtained our best sample quality results using this
parameterization Section 42 so we consider this equivalence to be one of our primary contributions
Despite their sample quality our models do not have competitive log likelihoods compared to other
likelihoodbased models our models do however have log likelihoods better than the large estimates
annealed importance sampling has been reported to produce for energy based models and score
matching  1155 We ﬁnd that the majority of our models lossless codelengths are consumed
to describe imperceptible image details Section 43 We present a more reﬁned analysis of this
phenomenon in the language of lossy compression and we show that the sampling procedure of
diffusion models is a type of progressive decoding that resembles autoregressive decoding along a bit
ordering that vastly generalizes what is normally possible with autoregressive models
2 Background
Diffusion models  53 are latent variable models of the form px0R
px0Tdx1T where
x1xTare latents of the same dimensionality as the data x0qx0 The joint distribution
px0Tis called the reverse process  and it is deﬁned as a Markov chain with learned Gaussian
transitions starting at pxT NxT0I
px0TpxTTY
t1pxt1jxt pxt1jxtNxt1xttxtt 1
What distinguishes diffusion models from other types of latent variable models is that the approximate
posteriorqx1Tjx0 called the forward process ordiffusion process  is ﬁxed to a Markov chain that
gradually adds Gaussian noise to the data according to a variance schedule 1T
qx1Tjx0TY
t1qxtjxt1 q xtjxt1Nxtp
1txt1tI 2
Training is performed by optimizing the usual variational bound on negative log likelihood
Elogpx0Eq
logpx0T
qx1Tjx0
Eq
logpxTX
t1logpxt1jxt
qxtjxt1
L3
The forward process variances tcan be learned by reparameterization  33 or held constant as
hyperparameters and expressiveness of the reverse process is ensured in part by the choice of
Gaussian conditionals in pxt1jxt because both processes have the same functional form when
tare small  53 A notable property of the forward process is that it admits sampling xtat an
arbitrary timestep tin closed form using the notation t 1tandtQt
s1s we have
qxtjx0 Nxtptx01tI 4
2Efﬁcient training is therefore possible by optimizing random terms of Lwith stochastic gradient
descent Further improvements come from variance reduction by rewriting L3 as
Eq
DKLqxTjx0kpxTz
LTX
t1DKLqxt1jxtx0kpxt1jxt z 
Lt1logpx0jx1z
L0
5
See Appendix A for details The labels on the terms are used in Section 3 Equation 5 uses KL
divergence to directly compare pxt1jxtagainst forward process posteriors which are tractable
when conditioned on x0
qxt1jxtx0 Nxt1txtx0tI 6
where txtx0pt1t
1tx0pt1t1
1txtand t1t1
1tt 7
Consequently all KL divergences in Eq 5 are comparisons between Gaussians so they can be
calculated in a RaoBlackwellized fashion with closed form expressions instead of high variance
Monte Carlo estimates
3 Diffusion models and denoising autoencoders
Diffusion models might appear to be a restricted class of latent variable models but they allow a
large number of degrees of freedom in implementation One must choose the variances tof the
forward process and the model architecture and Gaussian distribution parameterization of the reverse
process To guide our choices we establish a new explicit connection between diffusion models
and denoising score matching Section 32 that leads to a simpliﬁed weighted variational bound
objective for diffusion models Section 34 Ultimately our model design is justiﬁed by simplicity
and empirical results Section 4 Our discussion is categorized by the terms of Eq 5
31 Forward process and LT
We ignore the fact that the forward process variances tare learnable by reparameterization and
instead ﬁx them to constants see Section 4 for details Thus in our implementation the approximate
posteriorqhas no learnable parameters so LTis a constant during training and can be ignored
32 Reverse process and L1T1
Now we discuss our choices in pxt1jxt Nxt1xttxttfor1tT First
we set xtt 2
tIto untrained time dependent constants Experimentally both 2
ttand
2
tt1t1
1tthad similar results The ﬁrst choice is optimal for x0N 0I and the
second is optimal for x0deterministically set to one point These are the two extreme choices
corresponding to upper and lower bounds on reverse process entropy for data with coordinatewise
unit variance 53
Second to represent the mean xtt we propose a speciﬁc parameterization motivated by the
following analysis of Lt Withpxt1jxt Nxt1xtt2
tI we can write
Lt1Eq1
22
tktxtx0xttk2
C 8
whereCis a constant that does not depend on  So we see that the most straightforward parameteri
zation of is a model that predicts t the forward process posterior mean However we can expand
Eq 8 further by reparameterizing Eq 4 as xtx0 ptx0p1tforN0Iand
applying the forward process posterior formula 7
Lt1CEx0
1
22
tt
xtx01ptxtx0p
1t
xtx0t2
9
Ex0
1
22
t1pt
xtx0tp1t
xtx0t2
10
3Algorithm 1 Training
1repeat
2x0qx0
3tUniformf1Tg
4N0I
5 Take gradient descent step on
rptx0p1tt2
6until convergedAlgorithm 2 Sampling
1xTN0I
2fortT 1do
3zN0Iift1 elsez0
4xt11pt
xt1tp1txtt
tz
5end for
6return x0
Equation 10 reveals that must predict1pt
xttp1t
given xt Since xtis available as
input to the model we may choose the parameterization
xtt t
xt1ptxtp
1txt
1pt
xttp1txtt
11
where is a function approximator intended to predict fromxt To sample xt1pxt1jxtis
to compute xt11pt
xttp1txtt
tz where zN0I The complete sampling
procedure Algorithm 2 resembles Langevin dynamics with as a learned gradient of the data
density Furthermore with the parameterization 11 Eq 10 simpliﬁes to
Ex02
t
22
tt1tptx0p
1tt2
12
which resembles denoising score matching over multiple noise scales indexed by t55 As Eq 12
is equal to one term of the variational bound for the Langevinlike reverse process 11 we see
that optimizing an objective resembling denoising score matching is equivalent to using variational
inference to ﬁt the ﬁnitetime marginal of a sampling chain resembling Langevin dynamics
To summarize we can train the reverse process mean function approximator to predict t or by
modifying its parameterization we can train it to predict  There is also the possibility of predicting
x0 but we found this to lead to worse sample quality early in our experiments We have shown that
theprediction parameterization both resembles Langevin dynamics and simpliﬁes the diffusion
models variational bound to an objective that resembles denoising score matching Nonetheless
it is just another parameterization of pxt1jxt so we verify its effectiveness in Section 4 in an
ablation where we compare predicting against predicting t
33 Data scaling reverse process decoder and L0
We assume that image data consists of integers in f01 255gscaled linearly to 11 This
ensures that the neural network reverse process operates on consistently scaled inputs starting from
the standard normal prior pxT To obtain discrete log likelihoods we set the last term of the reverse
process to an independent discrete decoder derived from the Gaussian Nx0x112
1I
px0jx1 DY
i1Zxi
0
xi
0Nxi
x112
1dx
x 1 ifx 1
x1
255ifx1x 1 ifx1
x1
255ifx113
whereDis the data dimensionality and the isuperscript indicates extraction of one coordinate
It would be straightforward to instead incorporate a more powerful decoder like a conditional
autoregressive model but we leave that to future work Similar to the discretized continuous
distributions used in V AE decoders and autoregressive models  3452 our choice here ensures that
the variational bound is a lossless codelength of discrete data without need of adding noise to the
data or incorporating the Jacobian of the scaling operation into the log likelihood At the end of
sampling we display x11noiselessly
34 Simpliﬁed training objective
With the reverse process and decoder deﬁned above the variational bound consisting of terms derived
from Eqs 12 and 13 is clearly differentiable with respect to and is ready to be employed for
4Table 1 CIFAR10 results NLL measured in bitsdim
Model IS FID NLL Test Train
Conditional
EBM 11 830 37 9
JEM 17 876 38 4
BigGAN 3 922 14 73
StyleGAN2  ADA v1 29 1006 267
Unconditional
Diffusion original 53 540
Gated PixelCNN 59 460 65 93 303 290
Sparse Transformer 7 280
PixelIQN 43 529 49 46
EBM 11 678 38 2
NCSNv2 56 3175
NCSN 55 887012 2532
SNGAN 39 822005 217
SNGANDDLS 4 909010 1542
StyleGAN2  ADA v1 29 974005 326
Ours L ﬁxed isotropic  767013 1351370 369
Ours Lsimple  946011 317375 372Table 2 Unconditional CIFAR10 reverse
process parameterization and training objec
tive ablation Blank entries were unstable to
train and generated poor samples with outof
range scores
Objective IS FID
prediction baseline
L learned diagonal  728010 2369
L ﬁxed isotropic  806009 1322
kk2 
prediction ours
L learned diagonal   
L ﬁxed isotropic  767013 1351
kk2Lsimple 946011 317
training However we found it beneﬁcial to sample quality and simpler to implement to train on the
following variant of the variational bound
Lsimple Etx0hptx0p
1tt2i
14
wheretis uniform between 1andT Thet 1 case corresponds to L0with the integral in the
discrete decoder deﬁnition 13 approximated by the Gaussian probability density function times the
bin width ignoring 2
1and edge effects The t1cases correspond to an unweighted version of
Eq 12 analogous to the loss weighting used by the NCSN denoising score matching model  55
LTdoes not appear because the forward process variances tare ﬁxed Algorithm 1 displays the
complete training procedure with this simpliﬁed objective
Since our simpliﬁed objective 14 discards the weighting in Eq 12 it is a weighted variational
bound that emphasizes different aspects of reconstruction compared to the standard variational
bound  1822 In particular our diffusion process setup in Section 4 causes the simpliﬁed objective
to downweight loss terms corresponding to small t These terms train the network to denoise data
with very small amounts of noise so it is beneﬁcial to downweight them so that the network can
focus on more difﬁcult denoising tasks at larger tterms We will see in our experiments that this
reweighting leads to better sample quality
4 Experiments
We setT 1000 for all experiments so that the number of neural network evaluations needed
during sampling matches previous work  5355 We set the forward process variances to constants
increasing linearly from 1 104toT 002 These constants were chosen to be small
relative to data scaled to 11 ensuring that reverse and forward processes have approximately
the same functional form while keeping the signaltonoise ratio at xTas small as possible  LT
DKLqxTjx0kN0I105bits per dimension in our experiments
To represent the reverse process we use a UNet backbone similar to an unmasked PixelCNN 52
48 with group normalization throughout  66 Parameters are shared across time which is speciﬁed
to the network using the Transformer sinusoidal position embedding  60 We use selfattention at
the1616feature map resolution 63 60 Details are in Appendix B
41 Sample quality
Table 1 shows Inception scores FID scores and negative log likelihoods lossless codelengths on
CIFAR10 With our FID score of 317 our unconditional model achieves better sample quality than
most models in the literature including class conditional models Our FID score is computed with
respect to the training set as is standard practice when we compute it with respect to the test set the
score is 524 which is still better than many of the training set FID scores in the literature
5Figure 3 LSUN Church samples FID 789
 Figure 4 LSUN Bedroom samples FID 490
Algorithm 3 Sending x0
1 Send xTqxTjx0usingpxT
2fortT1 21do
3 Send xtqxtjxt1x0usingpxtjxt1
4end for
5 Send x0usingpx0jx1Algorithm 4 Receiving
1 Receive xTusingpxT
2fortT1 10do
3 Receive xtusingpxtjxt1
4end for
5return x0
We ﬁnd that training our models on the true variational bound yields better codelengths than training
on the simpliﬁed objective as expected but the latter yields the best sample quality See Fig 1 for
CIFAR10 and CelebAHQ 256256samples Fig 3 and Fig 4 for LSUN 256256samples  71
and Appendix D for more
42 Reverse process parameterization and training objective ablation
In Table 2 we show the sample quality effects of reverse process parameterizations and training
objectives Section 32 We ﬁnd that the baseline option of predicting works well only when
trained on the true variational bound instead of unweighted mean squared error a simpliﬁed objective
akin to Eq 14 We also see that learning reverse process variances by incorporating a parameterized
diagonal xtinto the variational bound leads to unstable training and poorer sample quality
compared to ﬁxed variances Predicting  as we proposed performs approximately as well as
predicting when trained on the variational bound with ﬁxed variances but much better when trained
with our simpliﬁed objective
43 Progressive coding
Table 1 also shows the codelengths of our CIFAR10 models The gap between train and test is at
most 003 bits per dimension which is comparable to the gaps reported with other likelihoodbased
models and indicates that our diffusion model is not overﬁtting see Appendix D for nearest neighbor
visualizations Still while our lossless codelengths are better than the large estimates reported for
energy based models and score matching using annealed importance sampling  11 they are not
competitive with other types of likelihoodbased generative models 7
Since our samples are nonetheless of high quality we conclude that diffusion models have an inductive
bias that makes them excellent lossy compressors Treating the variational bound terms L1LT
as rate andL0as distortion our CIFAR10 model with the highest quality samples has a rate of 178
bitsdim and a distortion of 197 bitsdim which amounts to a root mean squared error of 095 on a
scale from 0 to 255 More than half of the lossless codelength describes imperceptible distortions
Progressive lossy compression We can probe further into the ratedistortion behavior of our model
by introducing a progressive lossy code that mirrors the form of Eq 5 see Algorithms 3 and 4
which assume access to a procedure such as minimal random coding  1920 that can transmit a
sample xqxusing approximately DKLqxkpxbits on average for any distributions pand
q for which only pis available to the receiver beforehand When applied to x0qx0 Algorithms 3
and 4 transmit xTx0in sequence using a total expected codelength equal to Eq 5 The receiver
6at any timet has the partial information xtfully available and can progressively estimate
x0x0
xtp
1txt
pt 15
due to Eq 4 A stochastic reconstruction x0px0jxtis also valid but we do not consider
it here because it makes distortion more difﬁcult to evaluate Figure 5 shows the resulting rate
distortion plot on the CIFAR10 test set At each time t the distortion is calculated as the root mean
squared errorp
kx0x0k2D and the rate is calculated as the cumulative number of bits received
so far at time t The distortion decreases steeply in the lowrate region of the ratedistortion plot
indicating that the majority of the bits are indeed allocated to imperceptible distortions
0200 400 600 8001000020406080
Reverse process steps  TtDistortion RMSE
0200 400 600 8001000005115
Reverse process steps  TtRate bitsdim
0 05 1 15020406080
Rate bitsdimDistortion RMSE
Figure 5 Unconditional CIFAR10 test set ratedistortion vs time Distortion is measured in root mean squared
error on a 0255 scale See Table 4 for details
Progressive generation We also run a progressive unconditional generation process given by
progressive decompression from random bits In other words we predict the result of the reverse
process x0 while sampling from the reverse process using Algorithm 2 Figures 6 and 10 show the
resulting sample quality of x0over the course of the reverse process Large scale image features
appear ﬁrst and details appear last Figure 7 shows stochastic predictions x0px0jxtwithxt
frozen for various t Whentis small all but ﬁne details are preserved and when tis large only large
scale features are preserved Perhaps these are hints of conceptual compression 18
Figure 6 Unconditional CIFAR10 progressive generation  x0over time from left to right Extended samples
and sample quality metrics over time in the appendix Figs 10 and 14
Figure 7 When conditioned on the same latent CelebAHQ 256256samples share highlevel attributes
Bottomright quadrants are xt and other quadrants are samples from px0jxt
Connection to autoregressive decoding Note that the variational bound 5 can be rewritten as
LDKLqxTkpxT EqX
t1DKLqxt1jxtkpxt1jxt
Hx0 16
See Appendix A for a derivation Now consider setting the diffusion process length Tto the
dimensionality of the data deﬁning the forward process so that qxtjx0places all probability mass
onx0with the ﬁrst tcoordinates masked out ie qxtjxt1masks out the tthcoordinate setting
pxTto place all mass on a blank image and for the sake of argument taking pxt1jxtto
7Figure 8 Interpolations of CelebAHQ 256x256 images with 500 timesteps of diffusion
be a fully expressive conditional distribution With these choices DKLqxTkpxT  0  and
minimizingDKLqxt1jxtkpxt1jxttrainspto copy coordinates t 1T unchanged
and to predict the tthcoordinate given t 1T  Thus training pwith this particular diffusion is
training an autoregressive model
We can therefore interpret the Gaussian diffusion model 2 as a kind of autoregressive model with
a generalized bit ordering that cannot be expressed by reordering data coordinates Prior work has
shown that such reorderings introduce inductive biases that have an impact on sample quality  38
so we speculate that the Gaussian diffusion serves a similar purpose perhaps to greater effect since
Gaussian noise might be more natural to add to images compared to masking noise Moreover the
Gaussian diffusion length is not restricted to equal the data dimension for instance we use T 1000 
which is less than the dimension of the 32323or2562563images in our experiments
Gaussian diffusions can be made shorter for fast sampling or longer for model expressiveness
44 Interpolation
We can interpolate source images x0x0
0qx0in latent space using qas a stochastic encoder
xtx0
tqxtjx0 then decoding the linearly interpolated latent xt 1x0x0
0into image
space by the reverse process x0px0jxt In effect we use the reverse process to remove
artifacts from linearly interpolating corrupted versions of the source images as depicted in Fig 8
left We ﬁxed the noise for different values of soxtandx0
tremain the same Fig 8 right
shows interpolations and reconstructions of original CelebAHQ 256256images t 500  The
reverse process produces highquality reconstructions and plausible interpolations that smoothly
vary attributes such as pose skin tone hairstyle expression and background but not eyewear Larger
tresults in coarser and more varied interpolations with novel samples at t 1000 Appendix Fig 9
5 Related Work
While diffusion models might resemble ﬂows  946103251623 and V AEs  334737
diffusion models are designed so that qhas no parameters and the toplevel latent xThas nearly zero
mutual information with the data x0 Our prediction reverse process parameterization establishes a
connection between diffusion models and denoising score matching over multiple noise levels with
annealed Langevin dynamics for sampling  5556 Diffusion models however admit straightforward
log likelihood evaluation and the training procedure explicitly trains the Langevin dynamics sampler
using variational inference see Appendix C for details The connection also has the reverse
implication that a certain weighted form of denoising score matching is the same as variational
inference to train a Langevinlike sampler Other methods for learning transition operators of Markov
chains include infusion training  2 variational walkback  15 generative stochastic networks  1
and others 50 54 36 42 35 65
By the known connection between score matching and energybased modeling our work could have
implications for other recent work on energybased models  67691270131141178 Our
ratedistortion curves are computed over time in one evaluation of the variational bound reminiscent
of how ratedistortion curves can be computed over distortion penalties in one run of annealed
importance sampling  24 Our progressive decoding argument can be seen in convolutional DRAW
and related models  1840 and may also lead to more general designs for subscale orderings or
sampling strategies for autoregressive models 38 64
86 Conclusion
We have presented high quality image samples using diffusion models and we have found connections
among diffusion models and variational inference for training Markov chains denoising score
matching and annealed Langevin dynamics and energybased models by extension autoregressive
models and progressive lossy compression Since diffusion models seem to have excellent inductive
biases for image data we look forward to investigating their utility in other data modalities and as
components in other types of generative models and machine learning systems
Broader Impact
Our work on diffusion models takes on a similar scope as existing work on other types of deep
generative models such as efforts to improve the sample quality of GANs ﬂows autoregressive
models and so forth Our paper represents progress in making diffusion models a generally useful
tool in this family of techniques so it may serve to amplify any impacts that generative models have
had and will have on the broader world
Unfortunately there are numerous wellknown malicious uses of generative models Sample gen
eration techniques can be employed to produce fake images and videos of high proﬁle ﬁgures for
political purposes While fake images were manually created long before software tools were avail
able generative models such as ours make the process easier Fortunately CNNgenerated images
currently have subtle ﬂaws that allow detection  62 but improvements in generative models may
make this more difﬁcult Generative models also reﬂect the biases in the datasets on which they
are trained As many large datasets are collected from the internet by automated systems it can be
difﬁcult to remove these biases especially when the images are unlabeled If samples from generative
models trained on these datasets proliferate throughout the internet then these biases will only be
reinforced further
On the other hand diffusion models may be useful for data compression which as data becomes
higher resolution and as global internet trafﬁc increases might be crucial to ensure accessibility of
the internet to wide audiences Our work might contribute to representation learning on unlabeled
raw data for a large range of downstream tasks from image classiﬁcation to reinforcement learning
and diffusion models might also become viable for creative uses in art photography and music
Acknowledgments and Disclosure of Funding
This work was supported by ONR PECASE and the NSF Graduate Research Fellowship under grant
number DGE1752814 Googles TensorFlow Research Cloud TFRC provided Cloud TPUs
References
1Guillaume Alain Yoshua Bengio Li Yao Jason Yosinski Eric ThibodeauLaufer Saizheng Zhang and
Pascal Vincent GSNs generative stochastic networks Information and Inference A Journal of the IMA 
52210249 2016
2Florian Bordes Sina Honari and Pascal Vincent Learning to generate samples from noise through infusion
training In International Conference on Learning Representations  2017
3Andrew Brock Jeff Donahue and Karen Simonyan Large scale GAN training for high ﬁdelity natural
image synthesis In International Conference on Learning Representations  2019
4 Tong Che Ruixiang Zhang Jascha SohlDickstein Hugo Larochelle Liam Paull Yuan Cao and Yoshua
Bengio Your GAN is secretly an energybased model and you should use discriminator driven latent
sampling arXiv preprint arXiv200306060  2020
5Tian Qi Chen Yulia Rubanova Jesse Bettencourt and David K Duvenaud Neural ordinary differential
equations In Advances in Neural Information Processing Systems  pages 65716583 2018
6Xi Chen Nikhil Mishra Mostafa Rohaninejad and Pieter Abbeel PixelSNAIL An improved autoregres
sive generative model In International Conference on Machine Learning  pages 863871 2018
7Rewon Child Scott Gray Alec Radford and Ilya Sutskever Generating long sequences with sparse
transformers arXiv preprint arXiv190410509  2019
98Yuntian Deng Anton Bakhtin Myle Ott Arthur Szlam and MarcAurelio Ranzato Residual energybased
models for text generation arXiv preprint arXiv200411714  2020
9Laurent Dinh David Krueger and Yoshua Bengio NICE Nonlinear independent components estimation
arXiv preprint arXiv14108516  2014
10 Laurent Dinh Jascha SohlDickstein and Samy Bengio Density estimation using Real NVP arXiv
preprint arXiv160508803  2016
11 Yilun Du and Igor Mordatch Implicit generation and modeling with energy based models In Advances in
Neural Information Processing Systems  pages 36033613 2019
12 Ruiqi Gao Yang Lu Junpei Zhou SongChun Zhu and Ying Nian Wu Learning generative ConvNets
via multigrid modeling and sampling In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition  pages 91559164 2018
13 Ruiqi Gao Erik Nijkamp Diederik P Kingma Zhen Xu Andrew M Dai and Ying Nian Wu Flow
contrastive estimation of energybased models In Proceedings of the IEEECVF Conference on Computer
Vision and Pattern Recognition  pages 75187528 2020
14 Ian Goodfellow Jean PougetAbadie Mehdi Mirza Bing Xu David WardeFarley Sherjil Ozair Aaron
Courville and Yoshua Bengio Generative adversarial nets In Advances in Neural Information Processing
Systems  pages 26722680 2014
15 Anirudh Goyal Nan Rosemary Ke Surya Ganguli and Yoshua Bengio Variational walkback Learning a
transition operator as a stochastic recurrent net In Advances in Neural Information Processing Systems 
pages 43924402 2017
16 Will Grathwohl Ricky T Q Chen Jesse Bettencourt and David Duvenaud FFJORD Freeform
continuous dynamics for scalable reversible generative models In International Conference on Learning
Representations  2019
17 Will Grathwohl KuanChieh Wang JoernHenrik Jacobsen David Duvenaud Mohammad Norouzi and
Kevin Swersky Your classiﬁer is secretly an energy based model and you should treat it like one In
International Conference on Learning Representations  2020
18 Karol Gregor Frederic Besse Danilo Jimenez Rezende Ivo Danihelka and Daan Wierstra Towards
conceptual compression In Advances In Neural Information Processing Systems  pages 35493557 2016
19 Prahladh Harsha Rahul Jain David McAllester and Jaikumar Radhakrishnan The communication
complexity of correlation In TwentySecond Annual IEEE Conference on Computational Complexity
CCC07  pages 1023 IEEE 2007
20 Marton Havasi Robert Peharz and José Miguel HernándezLobato Minimal random code learning
Getting bits back from compressed model parameters In International Conference on Learning Represen
tations  2019
21 Martin Heusel Hubert Ramsauer Thomas Unterthiner Bernhard Nessler and Sepp Hochreiter GANs
trained by a two timescale update rule converge to a local Nash equilibrium In Advances in Neural
Information Processing Systems  pages 66266637 2017
22 Irina Higgins Loic Matthey Arka Pal Christopher Burgess Xavier Glorot Matthew Botvinick Shakir Mo
hamed and Alexander Lerchner betaV AE Learning basic visual concepts with a constrained variational
framework In International Conference on Learning Representations  2017
23 Jonathan Ho Xi Chen Aravind Srinivas Yan Duan and Pieter Abbeel Flow Improving ﬂowbased
generative models with variational dequantization and architecture design In International Conference on
Machine Learning  2019
24 Sicong Huang Alireza Makhzani Yanshuai Cao and Roger Grosse Evaluating lossy compression rates of
deep generative models In International Conference on Machine Learning  2020
25 Nal Kalchbrenner Aaron van den Oord Karen Simonyan Ivo Danihelka Oriol Vinyals Alex Graves and
Koray Kavukcuoglu Video pixel networks In International Conference on Machine Learning  pages
17711779 2017
26 Nal Kalchbrenner Erich Elsen Karen Simonyan Seb Noury Norman Casagrande Edward Lockhart
Florian Stimberg Aaron van den Oord Sander Dieleman and Koray Kavukcuoglu Efﬁcient neural audio
synthesis In International Conference on Machine Learning  pages 24102419 2018
27 Tero Karras Timo Aila Samuli Laine and Jaakko Lehtinen Progressive growing of GANs for improved
quality stability and variation In International Conference on Learning Representations  2018
28 Tero Karras Samuli Laine and Timo Aila A stylebased generator architecture for generative adversarial
networks In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages
1044014410 2019
29 Tero Karras Miika Aittala Janne Hellsten Samuli Laine Jaakko Lehtinen and Timo Aila Training
generative adversarial networks with limited data arXiv preprint arXiv200606676v1  2020
30 Tero Karras Samuli Laine Miika Aittala Janne Hellsten Jaakko Lehtinen and Timo Aila Analyzing and
improving the image quality of StyleGAN In Proceedings of the IEEECVF Conference on Computer
Vision and Pattern Recognition  pages 81108119 2020
31 Diederik P Kingma and Jimmy Ba Adam A method for stochastic optimization In International
Conference on Learning Representations  2015
32 Diederik P Kingma and Prafulla Dhariwal Glow Generative ﬂow with invertible 1x1 convolutions In
Advances in Neural Information Processing Systems  pages 1021510224 2018
33 Diederik P Kingma and Max Welling Autoencoding variational Bayes arXiv preprint arXiv13126114 
2013
34 Diederik P Kingma Tim Salimans Rafal Jozefowicz Xi Chen Ilya Sutskever and Max Welling Improved
variational inference with inverse autoregressive ﬂow In Advances in Neural Information Processing
Systems  pages 47434751 2016
35 John Lawson George Tucker Bo Dai and Rajesh Ranganath Energyinspired models Learning with
samplerinduced distributions In Advances in Neural Information Processing Systems  pages 85018513
2019
36 Daniel Levy Matt D Hoffman and Jascha SohlDickstein Generalizing Hamiltonian Monte Carlo with
neural networks In International Conference on Learning Representations  2018
37 Lars Maaløe Marco Fraccaro Valentin Liévin and Ole Winther BIV A A very deep hierarchy of
latent variables for generative modeling In Advances in Neural Information Processing Systems  pages
65486558 2019
38 Jacob Menick and Nal Kalchbrenner Generating high ﬁdelity images with subscale pixel networks and
multidimensional upscaling In International Conference on Learning Representations  2019
39 Takeru Miyato Toshiki Kataoka Masanori Koyama and Yuichi Yoshida Spectral normalization for
generative adversarial networks In International Conference on Learning Representations  2018
40 Alex Nichol VQDRAW A sequential discrete V AE arXiv preprint arXiv200301599  2020
41 Erik Nijkamp Mitch Hill Tian Han SongChun Zhu and Ying Nian Wu On the anatomy of MCMCbased
maximum likelihood learning of energybased models arXiv preprint arXiv190312370  2019
42 Erik Nijkamp Mitch Hill SongChun Zhu and Ying Nian Wu Learning nonconvergent nonpersistent
shortrun MCMC toward energybased model In Advances in Neural Information Processing Systems 
pages 52335243 2019
43 Georg Ostrovski Will Dabney and Remi Munos Autoregressive quantile networks for generative modeling
InInternational Conference on Machine Learning  pages 39363945 2018
44 Ryan Prenger Rafael Valle and Bryan Catanzaro WaveGlow A ﬂowbased generative network for
speech synthesis In ICASSP 20192019 IEEE International Conference on Acoustics Speech and Signal
Processing ICASSP  pages 36173621 IEEE 2019
45 Ali Razavi Aaron van den Oord and Oriol Vinyals Generating diverse highﬁdelity images with VQ
V AE2 In Advances in Neural Information Processing Systems  pages 1483714847 2019
46 Danilo Rezende and Shakir Mohamed Variational inference with normalizing ﬂows In International
Conference on Machine Learning  pages 15301538 2015
47 Danilo Jimenez Rezende Shakir Mohamed and Daan Wierstra Stochastic backpropagation and approx
imate inference in deep generative models In International Conference on Machine Learning  pages
12781286 2014
48 Olaf Ronneberger Philipp Fischer and Thomas Brox UNet Convolutional networks for biomedical
image segmentation In International Conference on Medical Image Computing and ComputerAssisted
Intervention  pages 234241 Springer 2015
49 Tim Salimans and Durk P Kingma Weight normalization A simple reparameterization to accelerate
training of deep neural networks In Advances in Neural Information Processing Systems  pages 901909
2016
50 Tim Salimans Diederik Kingma and Max Welling Markov Chain Monte Carlo and variational inference
Bridging the gap In International Conference on Machine Learning  pages 12181226 2015
1151 Tim Salimans Ian Goodfellow Wojciech Zaremba Vicki Cheung Alec Radford and Xi Chen Improved
techniques for training gans In Advances in Neural Information Processing Systems  pages 22342242
2016
52 Tim Salimans Andrej Karpathy Xi Chen and Diederik P Kingma PixelCNN Improving the PixelCNN
with discretized logistic mixture likelihood and other modiﬁcations In International Conference on
Learning Representations  2017
53 Jascha SohlDickstein Eric Weiss Niru Maheswaranathan and Surya Ganguli Deep unsupervised
learning using nonequilibrium thermodynamics In International Conference on Machine Learning  pages
22562265 2015
54 Jiaming Song Shengjia Zhao and Stefano Ermon ANICEMC Adversarial training for MCMC In
Advances in Neural Information Processing Systems  pages 51405150 2017
55 Yang Song and Stefano Ermon Generative modeling by estimating gradients of the data distribution In
Advances in Neural Information Processing Systems  pages 1189511907 2019
56 Yang Song and Stefano Ermon Improved techniques for training scorebased generative models arXiv
preprint arXiv200609011  2020
57 Aaron van den Oord Sander Dieleman Heiga Zen Karen Simonyan Oriol Vinyals Alex Graves Nal
Kalchbrenner Andrew Senior and Koray Kavukcuoglu WaveNet A generative model for raw audio
arXiv preprint arXiv160903499  2016
58 Aaron van den Oord Nal Kalchbrenner and Koray Kavukcuoglu Pixel recurrent neural networks
International Conference on Machine Learning  2016
59 Aaron van den Oord Nal Kalchbrenner Oriol Vinyals Lasse Espeholt Alex Graves and Koray
Kavukcuoglu Conditional image generation with PixelCNN decoders In Advances in Neural Information
Processing Systems  pages 47904798 2016
60 Ashish Vaswani Noam Shazeer Niki Parmar Jakob Uszkoreit Llion Jones Aidan N Gomez Łukasz
Kaiser and Illia Polosukhin Attention is all you need In Advances in Neural Information Processing
Systems  pages 59986008 2017
61 Pascal Vincent A connection between score matching and denoising autoencoders Neural Computation 
23716611674 2011
62 ShengYu Wang Oliver Wang Richard Zhang Andrew Owens and Alexei A Efros Cnngenerated images
are surprisingly easy to spotfor now In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition  2020
63 Xiaolong Wang Ross Girshick Abhinav Gupta and Kaiming He Nonlocal neural networks In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages 77947803
2018
64 Auke J Wiggers and Emiel Hoogeboom Predictive sampling with forecasting autoregressive models
arXiv preprint arXiv200209928  2020
65 Hao Wu Jonas Köhler and Frank Noé Stochastic normalizing ﬂows arXiv preprint arXiv200206707 
2020
66 Yuxin Wu and Kaiming He Group normalization In Proceedings of the European Conference on Computer
Vision ECCV  pages 319 2018
67 Jianwen Xie Yang Lu SongChun Zhu and Yingnian Wu A theory of generative convnet In International
Conference on Machine Learning  pages 26352644 2016
68 Jianwen Xie SongChun Zhu and Ying Nian Wu Synthesizing dynamic patterns by spatialtemporal
generative convnet In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pages 70937101 2017
69 Jianwen Xie Zilong Zheng Ruiqi Gao Wenguan Wang SongChun Zhu and Ying Nian Wu Learning
descriptor networks for 3d shape synthesis and analysis In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition  pages 86298638 2018
70 Jianwen Xie SongChun Zhu and Ying Nian Wu Learning energybased spatialtemporal generative
convnets for dynamic patterns IEEE Transactions on Pattern Analysis and Machine Intelligence  2019
71 Fisher Yu Yinda Zhang Shuran Song Ari Seff and Jianxiong Xiao LSUN Construction of a largescale
image dataset using deep learning with humans in the loop arXiv preprint arXiv150603365  2015
72 Sergey Zagoruyko and Nikos Komodakis Wide residual networks arXiv preprint arXiv160507146 
2016
12Extra information
LSUN FID scores for LSUN datasets are included in Table 3 Scores marked withare reported
by StyleGAN2 as baselines and other scores are reported by their respective authors
Table 3 FID scores for LSUN 256256datasets
Model LSUN Bedroom LSUN Church LSUN Cat
ProgressiveGAN 27 834 642 3752
StyleGAN 28 265 421853
StyleGAN2 30  386 693
Ours Lsimple  636 789 1975
Ours Lsimple  large 490  
Progressive compression Our lossy compression argument in Section 43 is only a proof of concept
because Algorithms 3 and 4 depend on a procedure such as minimal random coding  20 which is
not tractable for high dimensional data These algorithms serve as a compression interpretation of the
variational bound 5 of SohlDickstein et al 53 not yet as a practical compression system
Table 4 Unconditional CIFAR10 test set ratedistortion values accompanies Fig 5
Reverse process time  Tt 1 Rate bitsdim Distortion RMSE 0255
1000 177581 095136
900 011994 1202277
800 005415 1847482
700 002866 2443656
600 001507 3080948
500 000716 3803236
400 000282 4612765
300 000081 5418826
200 000013 6097170
100 000000 6760125
A Extended derivations
Below is a derivation of Eq 5 the reduced variance variational bound for diffusion models This
material is from SohlDickstein et al 53 we include it here only for completeness
LEq
logpx0T
qx1Tjx0
17
Eq2
4logpxTX
t1logpxt1jxt
qxtjxt13
5 18
Eq
logpxTX
t1logpxt1jxt
qxtjxt1logpx0jx1
qx1jx0
19
Eq
logpxTX
t1logpxt1jxt
qxt1jxtx0qxt1jx0
qxtjx0logpx0jx1
qx1jx0
20
Eq
logpxT
qxTjx0X
t1logpxt1jxt
qxt1jxtx0logpx0jx1
21
13Eq
DKLqxTjx0kpxT X
t1DKLqxt1jxtx0kpxt1jxtlogpx0jx1
22
The following is an alternate version of L It is not tractable to estimate but it is useful for our
discussion in Section 43
LEq2
4logpxTX
t1logpxt1jxt
qxtjxt13
5 23
Eq2
4logpxTX
t1logpxt1jxt
qxt1jxtqxt1
qxt3
5 24
Eq2
4logpxT
qxTX
t1logpxt1jxt
qxt1jxtlogqx03
5 25
DKLqxTkpxT Eq2
4X
t1DKLqxt1jxtkpxt1jxt3
5Hx0 26
B Experimental details
Our neural network architecture follows the backbone of PixelCNN  52 which is a UNet  48
based on a Wide ResNet  72 We replaced weight normalization  49 with group normalization  66
to make the implementation simpler Our 3232models use four feature map resolutions  3232
to44 and our 256256models use six All models have two convolutional residual blocks
per resolution level and selfattention blocks at the 1616resolution between the convolutional
blocks  6 Diffusion time tis speciﬁed by adding the Transformer sinusoidal position embedding  60
into each residual block Our CIFAR10 model has 357 million parameters and our LSUN and
CelebAHQ models have 114 million parameters We also trained a larger variant of the LSUN
Bedroom model with approximately 256 million parameters by increasing ﬁlter count
We used TPU v38 similar to 8 V100 GPUs for all experiments Our CIFAR model trains at 21
steps per second at batch size 128 106 hours to train to completion at 800k steps and sampling
a batch of 256 images takes 17 seconds Our CelebAHQLSUN 2562 models train at 22 steps
per second at batch size 64 and sampling a batch of 128 images takes 300 seconds We trained on
CelebAHQ for 05M steps LSUN Bedroom for 24M steps LSUN Cat for 18M steps and LSUN
Church for 12M steps The larger LSUN Bedroom model was trained for 115M steps
Apart from an initial choice of hyperparameters early on to make network size ﬁt within memory
constraints we performed the majority of our hyperparameter search to optimize for CIFAR10 sample
quality then transferred the resulting settings over to the other datasets
We chose the tschedule from a set of constant linear and quadratic schedules all
constrained so that LT0 We setT 1000 without a sweep and we chose a linear
schedule from 1 104toT 002
We set the dropout rate on CIFAR10 to 01by sweeping over the values f01020304g
Without dropout on CIFAR10 we obtained poorer samples reminiscent of the overﬁtting
artifacts in an unregularized PixelCNN  52 We set dropout rate on the other datasets to
zero without sweeping
We used random horizontal ﬂips during training for CIFAR10 we tried training both with
and without ﬂips and found ﬂips to improve sample quality slightly We also used random
horizontal ﬂips for all other datasets except LSUN Bedroom
We tried Adam  31 and RMSProp early on in our experimentation process and chose the
former We left the hyperparameters to their standard values We set the learning rate to
2104without any sweeping and we lowered it to 2105for the 256256images
which seemed unstable to train with the larger learning rate
14We set the batch size to 128 for CIFAR10 and 64 for larger images We did not sweep over
these values
We used EMA on model parameters with a decay factor of 09999 We did not sweep over
this value
Final experiments were trained once and evaluated throughout training for sample quality Sample
quality scores and log likelihood are reported on the minimum FID value over the course of training
On CIFAR10 we calculated Inception and FID scores on 50000 samples using the original code
from the OpenAI  51 and TTUR  21 repositories respectively On LSUN we calculated FID
scores on 50000 samples using code from the StyleGAN2  30 repository CIFAR10 and CelebAHQ
were loaded as provided by TensorFlow Datasets  httpswwwtensorfloworgdatasets 
and LSUN was prepared using code from StyleGAN Dataset splits or lack thereof are standard
from the papers that introduced their usage in a generative modeling context All details can be found
in the source code release
C Discussion on related work
Our model architecture forward process deﬁnition and prior differ from NCSN  5556 in subtle but
important ways that improve sample quality and notably we directly train our sampler as a latent
variable model rather than adding it after training posthoc In greater detail
1We use a UNet with selfattention NCSN uses a ReﬁneNet with dilated convolutions We
condition all layers on tby adding in the Transformer sinusoidal position embedding rather
than only in normalization layers NCSNv1 or only at the output v2
2Diffusion models scale down the data with each forward process step by ap1tfactor
so that variance does not grow when adding noise thus providing consistently scaled inputs
to the neural net reverse process NCSN omits this scaling factor
3Unlike NCSN our forward process destroys signal  DKLqxTjx0kN0I0 ensur
ing a close match between the prior and aggregate posterior of xT Also unlike NCSN our
tare very small which ensures that the forward process is reversible by a Markov chain
with conditional Gaussians Both of these factors prevent distribution shift when sampling
4Our Langevinlike sampler has coefﬁcients learning rate noise scale etc derived rig
orously from tin the forward process Thus our training procedure directly trains our
sampler to match the data distribution after Tsteps it trains the sampler as a latent variable
model using variational inference In contrast NCSNs sampler coefﬁcients are set by hand
posthoc and their training procedure is not guaranteed to directly optimize a quality metric
of their sampler
D Samples
Additional samples Figure 11 13 16 17 18 and 19 show uncurated samples from the diffusion
models trained on CelebAHQ CIFAR10 and LSUN datasets
Latent structure and reverse process stochasticity During sampling both the prior xT
N0Iand Langevin dynamics are stochastic To understand the signiﬁcance of the second source
of noise we sampled multiple images conditioned on the same intermediate latent for the CelebA
256256dataset Figure 7 shows multiple draws from the reverse process x0px0jxtthat
share the latent xtfort2f1000750500250g To accomplish this we run a single reverse chain
from an initial draw from the prior At the intermediate timesteps the chain is split to sample multiple
images When the chain is split after the prior draw at xT1000  the samples differ signiﬁcantly
However when the chain is split after more steps samples share highlevel attributes like gender
hair color eyewear saturation pose and facial expression This indicates that intermediate latents
likex750encode these attributes despite their imperceptibility
Coarsetoﬁne interpolation Figure 9 shows interpolations between a pair of source CelebA
256256images as we vary the number of diffusion steps prior to latent space interpolation
Increasing the number of diffusion steps destroys more structure in the source images which the
15model completes during the reverse process This allows us to interpolate at both ﬁne granularities
and coarse granularities In the limiting case of 0diffusion steps the interpolation mixes source
images in pixel space On the other hand after 1000 diffusion steps source information is lost and
interpolations are novel samples
SourceRecλ01λ02λ03λ04λ05λ06λ07λ08λ09RecSource1000 steps875 steps750 steps625 steps500 steps375 steps250 steps125 steps0 steps
Figure 9 Coarsetoﬁne interpolations that vary the number of diffusion steps prior to latent mixing
0 200 400 600 800 1000246810
Reverse process steps  TtInception Score
0 200 400 600 800 10000100200300
Reverse process steps  TtFID
Figure 10 Unconditional CIFAR10 progressive sampling quality over time
16Figure 11 CelebAHQ 256256generated samples
17a Pixel space nearest neighbors
b Inception feature space nearest neighbors
Figure 12 CelebAHQ 256256nearest neighbors computed on a 100100crop surrounding the
faces Generated samples are in the leftmost column and training set nearest neighbors are in the
remaining columns
18Figure 13 Unconditional CIFAR10 generated samples
19Figure 14 Unconditional CIFAR10 progressive generation
20a Pixel space nearest neighbors
b Inception feature space nearest neighbors
Figure 15 Unconditional CIFAR10 nearest neighbors Generated samples are in the leftmost column
and training set nearest neighbors are in the remaining columns
21Figure 16 LSUN Church generated samples FID 789
22Figure 17 LSUN Bedroom generated samples large model FID 490
23Figure 18 LSUN Bedroom generated samples small model FID 636
24Figure 19 LSUN Cat generated samples FID 1975
25
  Neural Discrete Representation Learning
Aaron van den Oord
DeepMind
avdnoordgooglecomOriol Vinyals
DeepMind
vinyalsgooglecomKoray Kavukcuoglu
DeepMind
koraykgooglecom
Abstract
Learning useful representations without supervision remains a key challenge in
machine learning In this paper we propose a simple yet powerful generative
model that learns such discrete representations Our model the Vector Quantised
Variational AutoEncoder VQV AE differs from V AEs in two key ways the
encoder network outputs discrete rather than continuous codes and the prior
is learnt rather than static In order to learn a discrete latent representation we
incorporate ideas from vector quantisation VQ Using the VQ method allows the
model to circumvent issues of posterior collapse  where the latents are ignored
when they are paired with a powerful autoregressive decoder  typically observed
in the V AE framework Pairing these representations with an autoregressive prior
the model can generate high quality images videos and speech as well as doing
high quality speaker conversion and unsupervised learning of phonemes providing
further evidence of the utility of the learnt representations
1 Introduction
Recent advances in generative modelling of images  3812132210 audio  3726 and videos
2011 have yielded impressive samples and applications  2418 At the same time challenging
tasks such as fewshot learning  34 domain adaptation  17 or reinforcement learning  35 heavily
rely on learnt representations from raw data but the usefulness of generic representations trained in
an unsupervised fashion is still far from being the dominant approach
Maximum likelihood and reconstruction error are two common objectives used to train unsupervised
models in the pixel domain however their usefulness depends on the particular application the
features are used in Our goal is to achieve a model that conserves the important features of the
data in its latent space while optimising for maximum likelihood As the work in  7 suggests the
best generative models as measured by loglikelihood will be those without latents but a powerful
decoder such as PixelCNN However in this paper we argue for learning discrete and useful latent
variables which we demonstrate on a variety of domains
Learning representations with continuous features have been the focus of many previous work
163969 however we concentrate on discrete representations  2733828 which are potentially
a more natural ﬁt for many of the modalities we are interested in Language is inherently discrete
similarly speech is typically represented as a sequence of symbols Images can often be described
concisely by language  40 Furthermore discrete representations are a natural ﬁt for complex
reasoning planning and predictive learning eg if it rains I will use an umbrella While using
discrete latent variables in deep learning has proven challenging powerful autoregressive models
have been developed for modelling distributions over discrete variables 37
In our work we introduce a new family of generative models succesfully combining the variational
autoencoder V AE framework with discrete latent representations through a novel parameterisation
of the posterior distribution of discrete latents given an observation Our model which relies on
vector quantization VQ is simple to train does not suffer from large variance and avoids the
31st Conference on Neural Information Processing Systems NIPS 2017 Long Beach CA USAarXiv171100937v2  csLG  30 May 2018posterior collapse issue which has been problematic with many V AE models that have a powerful
decoder often caused by latents being ignored Additionally it is the ﬁrst discrete latent V AE model
that get similar performance as its continuous counterparts while offering the ﬂexibility of discrete
distributions We term our model the VQV AE
Since VQV AE can make effective use of the latent space it can successfully model important
features that usually span many dimensions in data space for example objects span many pixels in
images phonemes in speech the message in a text fragment etc as opposed to focusing or spending
capacity on noise and imperceptible details which are often local
Lastly once a good discrete latent structure of a modality is discovered by the VQV AE we train
a powerful prior over these discrete random variables yielding interesting samples and useful
applications For instance when trained on speech we discover the latent structure of language
without any supervision or prior knowledge about phonemes or words Furthermore we can equip
our decoder with the speaker identity which allows for speaker conversion ie transferring the
voice from one speaker to another without changing the contents We also show promising results on
learning long term structure of environments for RL
Our contributions can thus be summarised as
Introducing the VQV AE model which is simple uses discrete latents does not suffer from
posterior collapse and has no variance issues
We show that a discrete latent model VQV AE perform as well as its continuous model
counterparts in loglikelihood
When paired with a powerful prior our samples are coherent and high quality on a wide
variety of applications such as speech and video generation
We show evidence of learning language through raw speech without any supervision and
show applications of unsupervised speaker conversion
2 Related Work
In this work we present a new way of training variational autoencoders  2332 with discrete latent
variables  27 Using discrete variables in deep learning has proven challenging as suggested by
the dominance of continuous latent variables in most of current work  even when the underlying
modality is inherently discrete
There exist many alternatives for training discrete V AEs The NVIL  27 estimator use a singlesample
objective to optimise the variational lower bound and uses various variancereduction techniques to
speed up training VIMCO  28 optimises a multisample objective  5 which speeds up convergence
further by using multiple samples from the inference network
Recently a few authors have suggested the use of a new continuous reparemetrisation based on the
socalled Concrete  25 or Gumbelsoftmax  19 distribution which is a continuous distribution and
has a temperature constant that can be annealed during training to converge to a discrete distribution
in the limit In the beginning of training the variance of the gradients is low but biased and towards
the end of training the variance becomes high but unbiased
None of the above methods however close the performance gap of V AEs with continuous latent
variables where one can use the Gaussian reparameterisation trick which beneﬁts from much lower
variance in the gradients Furthermore most of these techniques are typically evaluated on relatively
small datasets such as MNIST and the dimensionality of the latent distributions is small eg below
8 In our work we use three complex image datasets CIFAR10 ImageNet and DeepMind Lab and
a raw speech dataset VCTK
Our work also extends the line of research where autoregressive distributions are used in the decoder
of V AEs andor in the prior  14 This has been done for language modelling with LSTM decoders  4
and more recently with dilated convolutional decoders  42 PixelCNNs  2938 are convolutional
autoregressive models which have also been used as distribution in the decoder of V AEs 15 7
Finally our approach also relates to work in image compression with neural networks Theis et al
36 use scalar quantisation to compress activations for lossy image compression before arithmetic
encoding Other authors  1 propose a method for similar compression model with vector quantisation
2The authors propose a continuous relaxation of vector quantisation which is annealed over time
to obtain a hard clustering In their experiments they ﬁrst train an autoencoder afterwards vector
quantisation is applied to the activations of the encoder and ﬁnally the whole network is ﬁne tuned
using the softtohard relaxation with a small learning rate In our experiments we were unable to
train using the softtohard relaxation approach from scratch as the decoder was always able to invert
the continuous relaxation during training so that no actual quantisation took place
3 VQVAE
Perhaps the work most related to our approach are V AEs V AEs consist of the following parts
an encoder network which parameterises a posterior distribution qzjxof discrete latent random
variableszgiven the input data x a prior distribution pz and a decoder with a distribution pxjz
over input data
Typically the posteriors and priors in V AEs are assumed normally distributed with diagonal covari
ance which allows for the Gaussian reparametrisation trick to be used  3223 Extensions include
autoregressive prior and posterior models  14 normalising ﬂows  3110 and inverse autoregressive
posteriors 22
In this work we introduce the VQV AE where we use discrete latent variables with a new way of
training inspired by vector quantisation VQ The posterior and prior distributions are categorical
and the samples drawn from these distributions index an embedding table These embeddings are
then used as input into the decoder network
31 Discrete Latent variables
We deﬁne a latent embedding space e2RKDwhereKis the size of the discrete latent space ie
aKway categorical and Dis the dimensionality of each latent embedding vector ei Note that
there areKembedding vectors ei2RDi212K  As shown in Figure 1 the model takes an
inputx that is passed through an encoder producing output zex The discrete latent variables z
are then calculated by a nearest neighbour lookup using the shared embedding space eas shown in
equation 1 The input to the decoder is the corresponding embedding vector ekas given in equation 2
One can see this forward computation pipeline as a regular autoencoder with a particular nonlinearity
that maps the latents to 1ofK embedding vectors The complete set of parameters for the model are
union of parameters of the encoder decoder and the embedding space e For sake of simplicity we
use a single random variable zto represent the discrete latent variables in this Section however for
speech image and videos we actually extract a 1D 2D and 3D latent feature spaces respectively
The posterior categorical distribution qzjxprobabilities are deﬁned as onehot as follows
qzkjx 1for k  argmin jkzexejk2
0otherwise 1
wherezexis the output of the encoder network We view this model as a V AE in which we
can bound logpxwith the ELBO Our proposal distribution qzkjxis deterministic and by
deﬁning a simple uniform prior over zwe obtain a KL divergence constant and equal to logK
The representation zexis passed through the discretisation bottleneck followed by mapping onto
the nearest element of embedding eas given in equations 1 and 2
zqx ekwherekargminjkzexejk2 2
32 Learning
Note that there is no real gradient deﬁned for equation 2 however we approximate the gradient
similar to the straightthrough estimator  3 and just copy gradients from decoder input zqxto
encoder output zex One could also use the subgradient through the quantisation operation but this
simple estimator worked well for the initial experiments in this paper
3Figure 1 Left A ﬁgure describing the VQV AE Right Visualisation of the embedding space The
output of the encoder zxis mapped to the nearest point e2 The gradientrzLin red will push the
encoder to change its output which could alter the conﬁguration in the next forward pass
During forward computation the nearest embedding zqxequation 2 is passed to the decoder and
during the backwards pass the gradient rzLis passed unaltered to the encoder Since the output
representation of the encoder and the input to the decoder share the same Ddimensional space
the gradients contain useful information for how the encoder has to change its output to lower the
reconstruction loss
As seen on Figure 1 right the gradient can push the encoders output to be discretised differently in
the next forward pass because the assignment in equation 1 will be different
Equation 3 speciﬁes the overall loss function It is has three components that are used to train
different parts of VQV AE The ﬁrst term is the reconstruction loss or the data term which optimizes
the decoder and the encoder through the estimator explained above Due to the straightthrough
gradient estimation of mapping from zextozqx the embeddings eireceive no gradients from
the reconstruction loss logpzjzqx Therefore in order to learn the embedding space we use one
of the simplest dictionary learning algorithms Vector Quantisation VQ The VQ objective uses
thel2error to move the embedding vectors eitowards the encoder outputs zexas shown in the
second term of equation 3 Because this loss term is only used for updating the dictionary one can
alternatively also update the dictionary items as function of moving averages of zexnot used for
the experiments in this work For more details see Appendix A1
Finally since the volume of the embedding space is dimensionless it can grow arbitrarily if the
embeddings eido not train as fast as the encoder parameters To make sure the encoder commits to
an embedding and its output does not grow we add a commitment loss the third term in equation 3
Thus the total training objective becomes
L logpxjzqx ksgzexek2
2kzexsgek2
2 3
where sg stands for the stopgradient operator that is deﬁned as identity at forward computation time
and has zero partial derivatives thus effectively constraining its operand to be a nonupdated constant
The decoder optimises the ﬁrst loss term only the encoder optimises the ﬁrst and the last loss terms
and the embeddings are optimised by the middle loss term We found the resulting algorithm to be
quite robust to  as the results did not vary for values of ranging from 01to20 We use 025
in all our experiments although in general this would depend on the scale of reconstruction loss
Since we assume a uniform prior for z the KL term that usually appears in the ELBO is constant
wrt the encoder parameters and can thus be ignored for training
In our experiments we deﬁne Ndiscrete latents eg we use a ﬁeld of 32 x 32 latents for ImageNet
or 8 x 8 x 10 for CIFAR10 The resulting loss Lis identical except that we get an average over N
terms forkmeans and commitment loss  one for each latent
The loglikelihood of the complete model logpxcan be evaluated as follows
logpx  logX
kpxjzkpzk
Because the decoder pxjzis trained with zzqxfrom MAPinference the decoder should not
allocate any probability mass to pxjzforz6zqxonce it has fully converged Thus we can write
4logpxlogpxjzqxpzqx We empirically evaluate this approximation in section 4 From
Jensens inequality we also can write logpxlogpxjzqxpzqx
33 Prior
The prior distribution over the discrete latents pzis a categorical distribution and can be made
autoregressive by depending on other zin the feature map Whilst training the VQV AE the prior is
kept constant and uniform After training we ﬁt an autoregressive distribution over zpz so that
we can generate xvia ancestral sampling We use a PixelCNN over the discrete latents for images
and a WaveNet for raw audio Training the prior and the VQV AE jointly which could strengthen our
results is left as future research
4 Experiments
41 Comparison with continuous variables
As a ﬁrst experiment we compare VQV AE with normal V AEs with continuous variables as well as
VIMCO  28 with independent Gaussian or categorical priors We train these models using the same
standard V AE architecture on CIFAR10 while varying the latent capacity number of continuous or
discrete latent variables as well as the dimensionality of the discrete space K The encoder consists
of 2 strided convolutional layers with stride 2 and window size 44 followed by two residual
33blocks implemented as ReLU 3x3 conv ReLU 1x1 conv all having 256 hidden units The
decoder similarly has two residual 33blocks followed by two transposed convolutions with stride
2 and window size 44 We use the ADAM optimiser  21 with learning rate 2e4 and evaluate
the performance after 250000 steps with batchsize 128 For VIMCO we use 50 samples in the
multisample training objective
The V AE VQV AE and VIMCO models obtain 451 bitsdim 467 bitsdim and 514 respectively
All reported likelihoods are lower bounds Our numbers for the continuous V AE are comparable to
those reported for a Deep convolutional V AE 454 bitsdim 13 on this dataset
Our model is the ﬁrst among those using discrete latent variables which challenges the performance
of continuous V AEs Thus we get very good reconstructions like regular V AEs provide with the
compressed representation that symbolic representations provide A few interesting characteristics
implications and applications of the VQV AEs that we train is shown in the next subsections
42 Images
Images contain a lot of redundant information as most of the pixels are correlated and noisy therefore
learning models at the pixel level could be wasteful
In this experiment we show that we can model x 1281283images by compressing them to a
z 32321discrete space with K512 via a purely deconvolutional pxjz So a reduction of
12812838
32329426in bits We model images by learning a powerful prior PixelCNN over z This
allows to not only greatly speed up training and sampling but also to use the PixelCNNs capacity to
capture the global structure instead of the lowlevel statistics of images
Figure 2 Left ImageNet 128x128x3 images right reconstructions from a VQV AE with a 32x32x1
latent space with K512
5Reconstructions from the 32x32x1 space with discrete latents are shown in Figure 2 Even considering
that we greatly reduce the dimensionality with discrete encoding the reconstructions look only slightly
blurrier than the originals It would be possible to use a more perceptual loss function than MSE over
pixels here eg a GAN 12 but we leave that as future work
Next we train a PixelCNN prior on the discretised 32x32x1 latent space As we only have 1 channel
not 3 as with colours we only have to use spatial masking in the PixelCNN The capacity of the
PixelCNN we used was similar to those used by the authors of the PixelCNN paper 38
Figure 3 Samples 128x128 from a VQV AE with a PixelCNN prior trained on ImageNet images
From left to right kit fox gray whale brown bear admiral butterﬂy coral reef alp microwave
pickup
Samples drawn from the PixelCNN were mapped to pixelspace with the decoder of the VQV AE
and can be seen in Figure 3
Figure 4 Samples 128x128 from a VQV AE with a PixelCNN prior trained on frames captured
from DeepMind Lab
We also repeat the same experiment for 84x84x3 frames drawn from the DeepMind Lab environment
2 The reconstructions looked nearly identical to their originals Samples drawn from the PixelCNN
prior trained on the 21x21x1 latent space and decoded to the pixel space using a deconvolutional
model decoder can be seen in Figure 4
Finally we train a second VQV AE with a PixelCNN decoder on top of the 21x21x1 latent space
from the ﬁrst VQV AE on DMLAB frames This setup typically breaks V AEs as they suffer from
posterior collapse ie the latents are ignored as the decoder is powerful enough to model x
perfectly Our model however does not suffer from this and the latents are meaningfully used We use
only three latent variables each with K512 and their own embedding space e at the second stage
for modelling the whole image and as such the model cannot reconstruct the image perfectly  which
is consequence of compressing the image onto 3 x 9 bits ie less than a ﬂoat32 Reconstructions
sampled from the discretised global code can be seen in Figure 5
6Figure 5 Top original images Bottom reconstructions from a 2 stage VQV AE with 3 latents to
model the whole image 27 bits and as such the model cannot reconstruct the images perfectly The
reconstructions are generated by sampled from the second PixelCNN prior in the 21x21 latent domain
of ﬁrst VQV AE and then decoded with standard VQV AE decoder to 84x84 A lot of the original
scene including textures room layout and nearby walls remain but the model does not try to store
the pixel values themselves which means the textures are generated procedurally by the PixelCNN
Figure 6 Left original waveform middle reconstructed with same speakerid right reconstructed
with different speakerid The contents of the three waveforms are the same
43 Audio
In this set of experiments we evaluate the behaviour of discrete latent variables on models of raw
audio In all our audio experiments we train a VQV AE that has a dilated convolutional architecture
similar to WaveNet decoder All samples for this section can be played from the following url
httpsavdnoordgithubiohomepagevqvae 
We ﬁrst consider the VCTK dataset which has speech recordings of 109 different speakers  41
We train a VQV AE where the encoder has 6 strided convolutions with stride 2 and windowsize 4
This yields a latent space 64x smaller than the original waveform The latents consist of one feature
map and the discrete space is 512dimensional The decoder is conditioned on both the latents and a
onehot embedding for the speaker
First we ran an experiment to show that VQV AE can extract a latent space that only conserves
longterm relevant information After training the model given an audio example we can encode
it to the discrete latent representation and reconstruct by sampling from the decoder Because the
dimensionality of the discrete representation is 64 times smaller the original sample cannot be
perfectly reconstructed sample by sample As it can be heard from the provided samples and as
shown in Figure 7 the reconstruction has the same content same text contents but the waveform
is quite different and prosody in the voice is altered This means that the VQV AE has without
any form of linguistic supervision learned a highlevel abstract space that is invariant to lowlevel
features and only encodes the content of the speech This experiment conﬁrms our observations from
before that important features are often those that span many dimensions in the input data space in
this case phoneme and other highlevel content in waveform
We have then analysed the unconditional samples from the model to understand its capabilities Given
the compact and abstract latent representation extracted from the audio we trained the prior on top of
this representation to model the longterm dependencies in the data For this task we have used a
larger dataset of 460 speakers  30 and trained a VQV AE model where the resolution of discrete
space is 128 times smaller Next we trained the prior as usual on top of this representation on chunks
of 40960 timesteps 256 seconds which yields 320 latent timesteps While samples drawn from even
the best speech models like the original WaveNet  37 sound like babbling  samples from VQV AE
contain clear words and partsentences see samples linked above We conclude that VQV AE was
able to model a rudimentary phonemelevel language model in a completely unsupervised fashion
from raw audio waveforms
7Next we attempted the speaker conversion where the latents are extracted from one speaker and then
reconstructed through the decoder using a separate speaker id As can be heard from the samples
the synthesised speech has the same content as the original sample but with the voice from the
second speaker This experiment again demonstrates that the encoded representation has factored out
speakerspeciﬁc information the embeddings not only have the same meaning regardless of details
in the waveform but also across different voicecharacteristics
Finally in an attempt to better understand the content of the discrete codes we have compared the
latents onetoone with the groundtruth phonemesequence which was not used any way to train the
VQV AE With a 128dimensional discrete space that runs at 25Hz encoder downsampling factor
of640 we mapped every of the 128 possible latent values to one of the 41 possible phoneme values1
by taking the conditionally most likely phoneme The accuracy of this 41way classiﬁcation was
493 while a random latent space would result in an accuracy of 72prior most likely phoneme
It is clear that these discrete latent codes obtained in a fully unsupervised way are highlevel speech
descriptors that are closely related to phonemes
44 Video
For our ﬁnal experiment we have used the DeepMind Lab  2 environment to train a generative model
conditioned on a given action sequence In Figure 7 we show the initial 6frames that are input to the
model followed by 10frames that are sampled from VQV AE with all actions set to forward top row
andright bottom row Generation of the video sequence with the VQV AE model is done purely in
the latent space ztwithout the need to generate the actual images themselves Each image in the
sequencextis then created by mapping the latents with a deterministic decoder to the pixel space
after all the latents are generated using only the prior model pz1z T Therefore VQV AE can
be used to imagine long sequences purely in latent space without resorting to pixel space It can be
seen that the model has learnt to successfully generate a sequence of frames conditioned on given
action without any degradation in the visual quality whilst keeping the local geometry correct For
completeness we trained a model without actions and obtained similar results not shown due to
space constraints
Figure 7 First 6 frames are provided to the model following frames are generated conditioned on an
action Top repeated action move forward bottom repeated action move right
5 Conclusion
In this work we have introduced VQV AE a new family of models that combine V AEs with vector
quantisation to obtain a discrete latent representation We have shown that VQV AEs are capable of
modelling very long term dependencies through their compressed discrete latent space which we have
demonstrated by generating 128128colour images sampling action conditional video sequences
and ﬁnally using audio where even an unconditional model can generate surprisingly meaningful
chunks of speech and doing speaker conversion All these experiments demonstrated that the discrete
latent space learnt by VQV AEs capture important features of the data in a completely unsupervised
manner Moreover VQV AEs achieve likelihoods that are almost as good as their continuous latent
variable counterparts on CIFAR10 data We believe that this is the ﬁrst discrete latent variable model
that can successfully model long range sequences and fully unsupervisedly learn highlevel speech
descriptors that are closely related to phonemes
1Note that the encoderdecoder pairs could make the meaning of every discrete latent depend on previous
latents in the sequence eg bitrigrams and thus achieve a higher compression which means a more advanced
mapping to phonemes would results in higher accuracy
8References
1Eirikur Agustsson Fabian Mentzer Michael Tschannen Lukas Cavigelli Radu Timofte Luca Benini and
Luc Van Gool Softtohard vector quantization for endtoend learned compression of images and neural
networks arXiv preprint arXiv170400648  2017
2Charles Beattie Joel Z Leibo Denis Teplyashin Tom Ward Marcus Wainwright Heinrich Küttler Andrew
Lefrancq Simon Green Víctor Valdés Amir Sadik et al Deepmind lab arXiv preprint arXiv161203801 
2016
3Yoshua Bengio Nicholas Léonard and Aaron Courville Estimating or propagating gradients through
stochastic neurons for conditional computation arXiv preprint arXiv13083432  2013
4Samuel R Bowman Luke Vilnis Oriol Vinyals Andrew M Dai Rafal Jozefowicz and Samy Bengio
Generating sentences from a continuous space arXiv preprint arXiv151106349  2015
5Yuri Burda Roger Grosse and Ruslan Salakhutdinov Importance weighted autoencoders arXiv preprint
arXiv150900519  2015
6Xi Chen Yan Duan Rein Houthooft John Schulman Ilya Sutskever and Pieter Abbeel Infogan
Interpretable representation learning by information maximizing generative adversarial nets CoRR 
abs160603657 2016
7Xi Chen Diederik P Kingma Tim Salimans Yan Duan Prafulla Dhariwal John Schulman Ilya Sutskever
and Pieter Abbeel Variational lossy autoencoder arXiv preprint arXiv161102731  2016
8Aaron Courville James Bergstra and Yoshua Bengio A spike and slab restricted boltzmann machine In
Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics  pages
233241 2011
9Emily Denton Sam Gross and Rob Fergus Semisupervised learning with contextconditional generative
adversarial networks arXiv preprint arXiv161106430  2016
10 Laurent Dinh Jascha SohlDickstein and Samy Bengio Density estimation using real nvp arXiv preprint
arXiv160508803  2016
11 Chelsea Finn Ian Goodfellow and Sergey Levine Unsupervised learning for physical interaction through
video prediction In Advances in Neural Information Processing Systems  pages 6472 2016
12 Ian Goodfellow Jean PougetAbadie Mehdi Mirza Bing Xu David WardeFarley Sherjil Ozair Aaron
Courville and Yoshua Bengio Generative adversarial nets In Advances in neural information processing
systems  pages 26722680 2014
13 Karol Gregor Frederic Besse Danilo Jimenez Rezende Ivo Danihelka and Daan Wierstra Towards
conceptual compression In Advances In Neural Information Processing Systems  pages 35493557 2016
14 Karol Gregor Ivo Danihelka Andriy Mnih Charles Blundell and Daan Wierstra Deep autoregressive
networks arXiv preprint arXiv13108499  2013
15 Ishaan Gulrajani Kundan Kumar Faruk Ahmed Adrien Ali Taiga Francesco Visin David Vázquez and
Aaron C Courville Pixelvae A latent variable model for natural images CoRR  abs161105013 2016
16 Geoffrey E Hinton and Ruslan R Salakhutdinov Reducing the dimensionality of data with neural networks
science  3135786504507 2006
17 Judy Hoffman Erik Rodner Jeff Donahue Trevor Darrell and Kate Saenko Efﬁcient learning of
domaininvariant image representations arXiv preprint arXiv13013224  2013
18 Phillip Isola JunYan Zhu Tinghui Zhou and Alexei A Efros Imagetoimage translation with conditional
adversarial networks arXiv preprint arXiv161107004  2016
19 Eric Jang Shixiang Gu and Ben Poole Categorical reparameterization with gumbelsoftmax arXiv
preprint arXiv161101144  2016
20 Nal Kalchbrenner Aaron van den Oord Karen Simonyan Ivo Danihelka Oriol Vinyals Alex Graves and
Koray Kavukcuoglu Video pixel networks arXiv preprint arXiv161000527  2016
21 Diederik Kingma and Jimmy Ba Adam A method for stochastic optimization arXiv preprint
arXiv14126980  2014
22 Diederik P Kingma Tim Salimans Rafal Jozefowicz Xi Chen Ilya Sutskever and Max Welling Improved
variational inference with inverse autoregressive ﬂow NIPS 2016  2016
23 Diederik P Kingma and Max Welling Autoencoding variational bayes arXiv preprint arXiv13126114 
2013
24 Christian Ledig Lucas Theis Ferenc Huszár Jose Caballero Andrew Cunningham Alejandro Acosta
Andrew Aitken Alykhan Tejani Johannes Totz Zehan Wang et al Photorealistic single image super
resolution using a generative adversarial network arXiv preprint arXiv160904802  2016
925 Chris J Maddison Andriy Mnih and Yee Whye Teh The concrete distribution A continuous relaxation of
discrete random variables arXiv preprint arXiv161100712  2016
26 Soroush Mehri Kundan Kumar Ishaan Gulrajani Rithesh Kumar Shubham Jain Jose Sotelo Aaron
Courville and Yoshua Bengio Samplernn An unconditional endtoend neural audio generation model
arXiv preprint arXiv161207837  2016
27 Andriy Mnih and Karol Gregor Neural variational inference and learning in belief networks arXiv
preprint arXiv14020030  2014
28 Andriy Mnih and Danilo Jimenez Rezende Variational inference for monte carlo objectives CoRR 
abs160206725 2016
29 Aaron van den Oord Nal Kalchbrenner and Koray Kavukcuoglu Pixel recurrent neural networks arXiv
preprint arXiv160106759  2016
30 Vassil Panayotov Guoguo Chen Daniel Povey and Sanjeev Khudanpur Librispeech an asr corpus
based on public domain audio books In Acoustics Speech and Signal Processing ICASSP 2015 IEEE
International Conference on  pages 52065210 IEEE 2015
31 Danilo Jimenez Rezende and Shakir Mohamed Variational inference with normalizing ﬂows arXiv
preprint arXiv150505770  2015
32 Danilo Jimenez Rezende Shakir Mohamed and Daan Wierstra Stochastic backpropagation and approxi
mate inference in deep generative models arXiv preprint arXiv14014082  2014
33 Ruslan Salakhutdinov and Geoffrey Hinton Deep boltzmann machines In Artiﬁcial Intelligence and
Statistics  pages 448455 2009
34 Adam Santoro Sergey Bartunov Matthew Botvinick Daan Wierstra and Timothy Lillicrap Oneshot
learning with memoryaugmented neural networks arXiv preprint arXiv160506065  2016
35 Richard S Sutton and Andrew G Barto Reinforcement learning An introduction  volume 1 MIT press
Cambridge 1998
36 Lucas Theis Wenzhe Shi Andrew Cunningham and Ferenc Huszár Lossy image compression with
compressive autoencoders arXiv preprint arXiv170300395  2017
37 Aäron van den Oord Sander Dieleman Heiga Zen Karen Simonyan Oriol Vinyals Alex Graves Nal
Kalchbrenner Andrew Senior and Koray Kavukcuoglu Wavenet A generative model for raw audio
CoRR abs160903499  2016
38 Aaron van den Oord Nal Kalchbrenner Lasse Espeholt Oriol Vinyals Alex Graves et al Conditional
image generation with pixelcnn decoders In Advances in Neural Information Processing Systems  pages
47904798 2016
39 Pascal Vincent Hugo Larochelle Isabelle Lajoie Yoshua Bengio and PierreAntoine Manzagol Stacked
denoising autoencoders Learning useful representations in a deep network with a local denoising criterion
Journal of Machine Learning Research  11Dec33713408 2010
40 Oriol Vinyals Alexander Toshev Samy Bengio and Dumitru Erhan Show and tell A neural image
caption generator In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pages 31563164 2015
41 Junichi Yamagishi English multispeaker corpus for cstr voice cloning toolkit URL httphomepages inf
ed ac ukjyamagispage3page58page58 html  2012
42 Zichao Yang Zhiting Hu Ruslan Salakhutdinov and Taylor BergKirkpatrick Improved variational
autoencoders for text modeling using dilated convolutions CoRR  abs170208139 2017
10A Appendix
A1 VQVAE dictionary updates with Exponential Moving Averages
As mentioned in Section 32 one can also use exponential moving averages EMA to update the dictionary
items instead of the loss term from Equation 3
ksgzexek2
2 4
Letfzi1zi2z inigbe the set of nioutputs from the encoder that are closest to dictionary item ei so that
we can write the loss asniX
jkzijeik2
2 5
The optimal value for eihas a closed form solution which is simply the average of elements in the set
ei1
niniX
jzij
This update is typically used in algorithms such as KMeans
However we cannot use this update directly when working with minibatches Instead we can use exponential
moving averages as an online version of this update
Nt
iNt1
int
i1 6
mt
imt1
iX
jzt
ij1 7
et
imt
i
Nt
i 8
witha value between 0 and 1 We found  099to work well in practice
11
  RetrievalAugmented Generation for
KnowledgeIntensive NLP Tasks
Patrick Lewisyz Ethan Perez
Aleksandra Piktusy Fabio Petroniy Vladimir Karpukhiny Naman Goyaly Heinrich Küttlery
Mike Lewisy Wentau Yihy Tim Rocktäschelyz Sebastian Riedelyz Douwe Kielay
yFacebook AI ResearchzUniversity College LondonNew York University
plewisfbcom
Abstract
Large pretrained language models have been shown to store factual knowledge
in their parameters and achieve stateoftheart results when ﬁnetuned on down
stream NLP tasks However their ability to access and precisely manipulate knowl
edge is still limited and hence on knowledgeintensive tasks their performance
lags behind taskspeciﬁc architectures Additionally providing provenance for their
decisions and updating their world knowledge remain open research problems Pre
trained models with a differentiable access mechanism to explicit nonparametric
memory have so far been only investigated for extractive downstream tasks We
explore a generalpurpose ﬁnetuning recipe for retrievalaugmented generation
RAG  models which combine pretrained parametric and nonparametric mem
ory for language generation We introduce RAG models where the parametric
memory is a pretrained seq2seq model and the nonparametric memory is a dense
vector index of Wikipedia accessed with a pretrained neural retriever We com
pare two RAG formulations one which conditions on the same retrieved passages
across the whole generated sequence and another which can use different passages
per token We ﬁnetune and evaluate our models on a wide range of knowledge
intensive NLP tasks and set the state of the art on three open domain QA tasks
outperforming parametric seq2seq models and taskspeciﬁc retrieveandextract
architectures For language generation tasks we ﬁnd that RAG models generate
more speciﬁc diverse and factual language than a stateoftheart parametriconly
seq2seq baseline
1 Introduction
Pretrained neural language models have been shown to learn a substantial amount of indepth knowl
edge from data  47 They can do so without any access to an external memory as a parameterized
implicit knowledge base  5152 While this development is exciting such models do have down
sides They cannot easily expand or revise their memory cant straightforwardly provide insight into
their predictions and may produce hallucinations  38 Hybrid models that combine parametric
memory with nonparametric ie retrievalbased memories  202648 can address some of these
issues because knowledge can be directly revised and expanded and accessed knowledge can be
inspected and interpreted REALM  20 and ORQA  31 two recently introduced models that
combine masked language models  8 with a differentiable retriever have shown promising resultsarXiv200511401v4  csCL  12 Apr 2021The	Divine
Comedy	x qQuery
Encoder
qx
MIPS p θGenerator  pθ
Parametric
Margin
alize
This	14th	century	work
is	divided	into	3
sections	Inferno
Purgatorio	
Paradiso									 yEndtoEnd Backprop through q and  p θ
Barack	Obama	was
born	in	Hawaii x
Fact V eriﬁcation Fact Querysupports 	y
Question GenerationFact V eriﬁcation
Label GenerationDocument
IndexDefine	middle	ear x
Question Answering
Question QueryThe	middle	ear	includes
the	tympanic	cavity	and
the	three	ossicles		 y
Question Answering
Answer GenerationRetriever pη
NonParametric
z 4
z3
z2
z 1dz
Jeopardy Question
Generation
Answer QueryFigure 1 Overview of our approach We combine a pretrained retriever  Query Encoder Document
Index  with a pretrained seq2seq model  Generator  and ﬁnetune endtoend For query x we use
Maximum Inner Product Search MIPS to ﬁnd the topK documents zi For ﬁnal prediction y we
treatzas a latent variable and marginalize over seq2seq predictions given different documents
but have only explored opendomain extractive question answering Here we bring hybrid parametric
and nonparametric memory to the workhorse of NLP ie sequencetosequence seq2seq models
We endow pretrained parametricmemory generation models with a nonparametric memory through
a generalpurpose ﬁnetuning approach which we refer to as retrievalaugmented generation RAG
We build RAG models where the parametric memory is a pretrained seq2seq transformer and the
nonparametric memory is a dense vector index of Wikipedia accessed with a pretrained neural
retriever We combine these components in a probabilistic model trained endtoend Fig 1 The
retriever Dense Passage Retriever  26 henceforth DPR provides latent documents conditioned on
the input and the seq2seq model BART  32 then conditions on these latent documents together with
the input to generate the output We marginalize the latent documents with a topK approximation
either on a peroutput basis assuming the same document is responsible for all tokens or a pertoken
basis where different documents are responsible for different tokens Like T5  51 or BART RAG
can be ﬁnetuned on any seq2seq task whereby both the generator and retriever are jointly learned
There has been extensive previous work proposing architectures to enrich systems with nonparametric
memory which are trained from scratch for speciﬁc tasks eg memory networks  6455 stack
augmented networks  25 and memory layers  30 In contrast we explore a setting where both
parametric and nonparametric memory components are pretrained and preloaded with extensive
knowledge Crucially by using pretrained access mechanisms the ability to access knowledge is
present without additional training
Our results highlight the beneﬁts of combining parametric and nonparametric memory with genera
tion for knowledgeintensive tasks tasks that humans could not reasonably be expected to perform
without access to an external knowledge source Our RAG models achieve stateoftheart results
on open Natural Questions  29 WebQuestions  3 and CuratedTrec  2 and strongly outperform
recent approaches that use specialised pretraining objectives on TriviaQA  24 Despite these being
extractive tasks we ﬁnd that unconstrained generation outperforms previous extractive approaches
For knowledgeintensive generation we experiment with MSMARCO  1 and Jeopardy question
generation and we ﬁnd that our models generate responses that are more factual speciﬁc and
diverse than a BART baseline For FEVER  56 fact veriﬁcation we achieve results within 43 of
stateoftheart pipeline models which use strong retrieval supervision Finally we demonstrate that
the nonparametric memory can be replaced to update the models knowledge as the world changes1
2 Methods
We explore RAG models which use the input sequence xto retrieve text documents zand use them
as additional context when generating the target sequence y As shown in Figure 1 our models
leverage two components i a retriever pzjxwith parameters that returns topK truncated
distributions over text passages given a query xand ii a generator pyijxzy 1i1parametrized
1Code to run experiments with RAG has been opensourced as part of the HuggingFace Transform
ers Library  66 and can be found at httpsgithubcomhuggingfacetransformersblobmaster
examplesrag  An interactive demo of RAG models can be found at httpshuggingfacecorag
2bythat generates a current token based on a context of the previous i1tokensy1i1 the original
inputxand a retrieved passage z
To train the retriever and generator endtoend we treat the retrieved document as a latent variable
We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text In one approach RAGSequence  the model uses the same document
to predict each target token The second approach RAGToken  can predict each target token based
on a different document In the following we formally introduce both models and then describe the
pandpcomponents as well as the training and decoding procedure
21 Models
RAGSequence Model The RAGSequence model uses the same retrieved document to generate
the complete sequence  Technically it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability pyjxvia a topK approximation Concretely the
top K documents are retrieved using the retriever and the generator produces the output sequence
probability for each document which are then marginalized
pRAGSequence yjxX
z2topkpjxpzjxpyjxz X
z2topkpjxpzjxNY
ipyijxzy 1i1
RAGToken Model In the RAGToken model we can draw a different latent document for each
target token and marginalize accordingly This allows the generator to choose content from several
documents when producing an answer Concretely the top K documents are retrieved using the
retriever and then the generator produces a distribution for the next output token for each document
before marginalizing and repeating the process with the following output token Formally we deﬁne
pRAGToken yjxNY
iX
z2topkpjxpzjxpyijxzy 1i1
Finally we note that RAG can be used for sequence classiﬁcation tasks by considering the target class
as a target sequence of length one in which case RAGSequence and RAGToken are equivalent
22 Retriever DPR
The retrieval component pzjxis based on DPR 26 DPR follows a biencoder architecture
pzjxexp
dzqx
dz BERTdzqx BERTqx
where dzis a dense representation of a document produced by a BERT BASE document encoder 8
andqxa query representation produced by a query encoder  also based on BERT BASE Calculating
topk pjx the list ofkdocumentszwith highest prior probability pzjx is a Maximum Inner
Product Search MIPS problem which can be approximately solved in sublinear time  23 We use
a pretrained biencoder from DPR to initialize our retriever and to build the document index This
retriever was trained to retrieve documents which contain answers to TriviaQA  24 questions and
Natural Questions 29 We refer to the document index as the nonparametric memory 
23 Generator BART
The generator component pyijxzy 1i1could be modelled using any encoderdecoder We use
BARTlarge  32 a pretrained seq2seq transformer  58 with 400M parameters To combine the input
xwith the retrieved content zwhen generating from BART we simply concatenate them BART was
pretrained using a denoising objective and a variety of different noising functions It has obtained
stateoftheart results on a diverse set of generation tasks and outperforms comparablysized T5
models 32 We refer to the BART generator parameters as the parametric memory henceforth
24 Training
We jointly train the retriever and generator components without any direct supervision on what
document should be retrieved Given a ﬁnetuning training corpus of inputoutput pairs xjyj we
3minimize the negative marginal loglikelihood of each targetP
jlogpyjjxjusing stochastic
gradient descent with Adam  28 Updating the document encoder BERTdduring training is costly as
it requires the document index to be periodically updated as REALM does during pretraining  20
We do not ﬁnd this step necessary for strong performance and keep the document encoder and
index ﬁxed only ﬁnetuning the query encoder BERT qand the BART generator
25 Decoding
At test time RAGSequence and RAGToken require different ways to approximate arg maxypyjx
RAGToken The RAGToken model can be seen as a standard autoregressive seq2seq genera
tor with transition probability p0
yijxy 1i1 P
z2topkpjxpzijxpyijxziy1i1To
decode we can plug p0
yijxy 1i1into a standard beam decoder
RAGSequence For RAGSequence the likelihood pyjxdoes not break into a conventional per
token likelihood hence we cannot solve it with a single beam search Instead we run beam search for
each document z scoring each hypothesis using pyijxzy 1i1 This yields a set of hypotheses
Y some of which may not have appeared in the beams of all documents To estimate the probability
of an hypothesis ywe run an additional forward pass for each document zfor whichydoes not
appear in the beam multiply generator probability with pzjxand then sum the probabilities across
beams for the marginals We refer to this decoding procedure as Thorough Decoding For longer
output sequencesjYjcan become large requiring many forward passes For more efﬁcient decoding
we can make a further approximation that pyjxzi0whereywas not generated during beam
search from xzi This avoids the need to run additional forward passes once the candidate set Yhas
been generated We refer to this decoding procedure as Fast Decoding
3 Experiments
We experiment with RAG in a wide range of knowledgeintensive tasks For all experiments we use
a single Wikipedia dump for our nonparametric knowledge source Following Lee et al 31 and
Karpukhin et al 26 we use the December 2018 dump Each Wikipedia article is split into disjoint
100word chunks to make a total of 21M documents We use the document encoder to compute an
embedding for each document and build a single MIPS index using FAISS  23 with a Hierarchical
Navigable Small World approximation for fast retrieval  37 During training we retrieve the top
kdocuments for each query We consider k2f510gfor training and set kfor test time using dev
data We now discuss experimental details for each task
31 Opendomain Question Answering
Opendomain question answering QA is an important realworld application and common testbed
for knowledgeintensive tasks  20 We treat questions and answers as inputoutput text pairs xy
and train RAG by directly minimizing the negative loglikelihood of answers We compare RAG to
the popular extractive QA paradigm  573126 where answers are extracted spans from retrieved
documents relying primarily on nonparametric knowledge We also compare to ClosedBook
QA approaches  52 which like RAG generate answers but which do not exploit retrieval instead
relying purely on parametric knowledge We consider four popular opendomain QA datasets Natural
Questions NQ  29 TriviaQA TQA  24 WebQuestions WQ  3 and CuratedTrec CT  2 As
CT and WQ are small we follow DPR  26 by initializing CT and WQ models with our NQ RAG
model We use the same traindevtest splits as prior work  3126 and report Exact Match EM
scores For TQA to compare with T5 52 we also evaluate on the TQA Wiki test set
32 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with freeform abstractive
text generation To test RAGs natural language generation NLG in a knowledgeintensive setting
we use the MSMARCO NLG task v21  43 The task consists of questions ten gold passages
retrieved from a search engine for each question and a full sentence answer annotated from the
retrieved passages We do not use the supplied passages only the questions and answers to treat
4MSMARCO as an opendomain abstractive QA task MSMARCO has some questions that cannot be
answered in a way that matches the reference answer without access to the gold passages such as
What is the weather in V olcano CA so performance will be lower without using gold passages
We also note that some MSMARCO questions cannot be answered using Wikipedia alone Here
RAG can rely on parametric knowledge to generate reasonable responses
33 Jeopardy Question Generation
To evaluate RAGs generation abilities in a nonQA setting we study opendomain question gen
eration Rather than use questions from standard opendomain QA tasks which typically consist
of short simple questions we propose the more demanding task of generating Jeopardy questions
Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity
For example The World Cup is the answer to the question In 1986 Mexico scored as the ﬁrst
country to host this international sports competition twice As Jeopardy questions are precise
factual statements generating Jeopardy questions conditioned on their answer entities constitutes a
challenging knowledgeintensive generation task
We use the splits from SearchQA  10 with 100K train 14K dev and 27K test examples As
this is a new task we train a BART model for comparison Following  67 we evaluate using the
SQuADtuned QBLEU1 metric  42 QBLEU is a variant of BLEU with a higher weight for
matching entities and has higher correlation with human judgment for question generation than
standard metrics We also perform two human evaluations one to assess generation factuality and
one for speciﬁcity We deﬁne factuality as whether a statement can be corroborated by trusted external
sources and speciﬁcity as high mutual dependence between the input and output  33 We follow
best practice and use pairwise comparative evaluation  34 Evaluators are shown an answer and two
generated questions one from BART and one from RAG They are then asked to pick one of four
optionsquuestion A is better question B is better both are good or neither is good
34 Fact Veriﬁcation
FEVER  56 requires classifying whether a natural language claim is supported or refuted by
Wikipedia or whether there is not enough information to decide The task requires retrieving
evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify
whether the claim is true false or unveriﬁable from Wikipedia alone FEVER is a retrieval problem
coupled with an challenging entailment reasoning task It also provides an appropriate testbed for
exploring the RAG models ability to handle classiﬁcation rather than generation We map FEVER
class labels supports refutes or not enough info to single output tokens and directly train with
claimclass pairs Crucially unlike most other approaches to FEVER we do not use supervision on
retrieved evidence In many realworld applications retrieval supervision signals arent available and
models that do not require such supervision will be applicable to a wider range of tasks We explore
two variants the standard 3way classiﬁcation task supportsrefutesnot enough info and the 2way
supportsrefutes task studied in Thorne and Vlachos 57 In both cases we report label accuracy
4 Results
41 Opendomain Question Answering
Table 1 shows results for RAG along with stateoftheart models On all four opendomain QA
tasks RAG sets a new state of the art only on the T5comparable split for TQA RAG combines
the generation ﬂexibility of the closedbook parametric only approaches and the performance of
openbook retrievalbased approaches Unlike REALM and T5SSM RAG enjoys strong results
without expensive specialized salient span masking pretraining  20 It is worth noting that RAGs
retriever is initialized using DPRs retriever which uses retrieval supervision on Natural Questions
and TriviaQA RAG compares favourably to the DPR QA system which uses a BERTbased cross
encoder to rerank documents along with an extractive reader RAG demonstrates that neither a
reranker nor extractive reader is necessary for stateoftheart performance
There are several advantages to generating answers even when it is possible to extract them Docu
ments with clues about the answer but do not contain the answer verbatim can still contribute towards
a correct answer being generated which is not possible with standard extractive approaches leading
5Table 1 OpenDomain QA Test Scores For TQA
left column uses the standard test set for Open
Domain QA right column uses the TQAWiki
test set See Appendix D for further details
Model NQ TQA WQ CT
Closed
BookT511B 52 345  501 374 
T511BSSM52 366  605 447 
Open
BookREALM 20 404    407 468
DPR 26 415 579  411 506
RAGToken 441 552661 455 500
RAGSeq 445 568 680 452 522Table 2 Generation and classiﬁcation Test Scores
MSMARCO SotA is  4 FEVER3 is  68 and
FEVER2 is  57 Uses gold contextevidence
Best model without gold access underlined
Model Jeopardy MSMARCO FVR3 FVR2
B1 QB1 RL B1 Label Acc
SotA   498499768 922 
BART 151 197 382 416 640 811
RAGTok 173 222 401 415725 895RAGSeq 147 214 408 442
to more effective marginalization over documents Furthermore RAG can generate correct answers
even when the correct answer is not in any retrieved document achieving 118 accuracy in such
cases for NQ where an extractive model would score 0
42 Abstractive Question Answering
As shown in Table 2 RAGSequence outperforms BART on Open MSMARCO NLG by 26 Bleu
points and 26 RougeL points RAG approaches stateoftheart model performance which is
impressive given that i those models access gold passages with speciﬁc information required to
generate the reference answer  ii many questions are unanswerable without the gold passages and
iii not all questions are answerable from Wikipedia alone Table 3 shows some generated answers
from our models Qualitatively we ﬁnd that RAG models hallucinate less and generate factually
correct text more often than BART Later we also show that RAG generations are more diverse than
BART generations see 45
43 Jeopardy Question Generation
Table 2 shows that RAGToken performs better than RAGSequence on Jeopardy question generation
with both models outperforming BART on QBLEU1 4 shows human evaluation results over 452
pairs of generations from BART and RAGToken Evaluators indicated that BART was more factual
than RAG in only 71 of cases while RAG was more factual in 427 of cases and both RAG and
BART were factual in a further 17 of cases clearly demonstrating the effectiveness of RAG on
the task over a stateoftheart generation model Evaluators also ﬁnd RAG generations to be more
speciﬁc by a large margin Table 3 shows typical generations from each model
Jeopardy questions often contain two separate pieces of information and RAGToken may perform
best because it can generate responses that combine content from several documents Figure 2 shows
an example When generating Sun the posterior is high for document 2 which mentions The
Sun Also Rises Similarly document 1 dominates the posterior when A Farewell to Arms is
generated Intriguingly after the ﬁrst token of each book is generated the document posterior ﬂattens
This observation suggests that the generator can complete the titles without depending on speciﬁc
documents In other words the models parametric knowledge is sufﬁcient to complete the titles We
ﬁnd evidence for this hypothesis by feeding the BARTonly baseline with the partial decoding The
Sun BART completes the generation The SunAlso Rises isanovel bythis author ofThe Sun
Also Rises indicating the title The Sun Also Rises is stored in BARTs parameters Similarly
BART will complete the partial decoding The SunAlso Rises isanovel bythis author ofA
with The SunAlso Rises isanovel bythis author ofAFarewell toArms  This example shows
how parametric and nonparametric memories work together the nonparametric component helps
to guide the generation drawing out speciﬁc knowledge stored in the parametric memory
44 Fact Veriﬁcation
Table 2 shows our results on FEVER For 3way classiﬁcation RAG scores are within 43 of
stateoftheart models which are complex pipeline systems with domainspeciﬁc architectures and
substantial engineering trained using intermediate retrieval supervision which RAG does not require
6Document 1  his works are considered classics of American
literature  His wartime experiences formed the basis for his novel
A Farewell to Arms 1929 
Document 2   artists of the 1920s Lost Generation expatriate
community His debut novel The Sun Also Rises  was published
in 1926
BOS
TheSunAlsoRisesisa
novelbythis
authorofA
Farewellto
ArmsDoc 1
Doc 2
Doc 3
Doc 4
Doc 5Figure 2 RAGToken document posterior pzijxyiyifor each generated token for input Hem
ingway for Jeopardy generation with 5 retrieved documents The posterior for document 1 is high
when generating A Farewell to Arms and for document 2 when generating The Sun Also Rises
Table 3 Examples from generation tasks RAG models generate more speciﬁc and factually accurate
responses  indicates factually incorrect responses  indicates partially correct responses
Task Input Model Generation
MS
MARCOdeﬁne middle
earBARTThe middle ear is the part of the ear between the middle ear and the nose
RAGT The middle ear is the portion of the ear internal to the eardrum
RAGS The middle ear includes the tympanic cavity and the three ossicles
what currency
needed in
scotlandBART The currency needed in Scotland is Pound sterling
RAGT Pound is the currency needed in Scotland
RAGS The currency needed in Scotland is the pound sterling
Jeopardy
Question
Gener
ationWashingtonBARTThis state has the largest number of counties in the US
RAGT Its the only US state named for a US president
RAGS Its the state where youll ﬁnd Mount Rainier National Park
The Divine
ComedyBARTThis epic poem by Dante is divided into 3 parts the Inferno the Purgatorio  the Purgatorio
RAGT Dantes Inferno is the ﬁrst part of this epic poem
RAGS This 14th century work is divided into 3 sections Inferno Purgatorio  Paradiso
For 2way classiﬁcation we compare against Thorne and Vlachos 57 who train RoBERTa  35
to classify the claim as true or false given the gold evidence sentence RAG achieves an accuracy
within 27 of this model despite being supplied with only the claim and retrieving its own evidence
We also analyze whether documents retrieved by RAG correspond to documents annotated as gold
evidence in FEVER We calculate the overlap in article titles between the top kdocuments retrieved
by RAG and gold evidence annotations We ﬁnd that the top retrieved document is from a gold article
in 71 of cases and a gold article is present in the top 10 retrieved articles in 90 of cases
45 Additional Results
Generation Diversity Section 43 shows that RAG models are more factual and speciﬁc than
BART for Jeopardy question generation Following recent work on diversitypromoting decoding
335939 we also investigate generation diversity by calculating the ratio of distinct ngrams to
total ngrams generated by different models Table 5 shows that RAGSequences generations are
more diverse than RAGTokens and both are signiﬁcantly more diverse than BART without needing
any diversitypromoting decoding
Retrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task
To assess the effectiveness of the retrieval mechanism we run ablations where we freeze the retriever
during training As shown in Table 6 learned retrieval improves results for all tasks
We compare RAGs dense retriever to a word overlapbased BM25 retriever  53 Here we replace
RAGs retriever with a ﬁxed BM25 system and use BM25 retrieval scores as logits when calculating
pzjx Table 6 shows the results For FEVER BM25 performs best perhaps since FEVER claims are
heavily entitycentric and thus wellsuited for word overlapbased retrieval Differentiable retrieval
improves results on all other tasks especially for OpenDomain QA where it is crucial
Index hotswapping An advantage of nonparametric memory models like RAG is that knowledge
can be easily updated at test time Parametriconly models like T5 or BART need further training to
update their behavior as the world changes To demonstrate we build an index using the DrQA  5
Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer
index from our main results December 2018 We prepare a list of 82 world leaders who had changed
7Table 4 Human assessments for the Jeopardy
Question Generation Task
Factuality Speciﬁcity
BART better 71 168
RAG better 427 374
Both good 117 118
Both poor 177 69
No majority 208 201Table 5 Ratio of distinct to total trigrams for
generation tasks
MSMARCO Jeopardy QGen
Gold 896 900
BART 707 324
RAGToken 778 468
RAGSeq 835 538
Table 6 Ablations on the dev set As FEVER is a classiﬁcation task both RAG models are equivalent
Model NQ TQA WQ CT JeopardyQGen MSMarco FVR3 FVR2
Exact Match B1 QB1 RL B1 Label Accuracy
RAGTokenBM25 297 415 321 331 175 223 555 484751 916RAGSequenceBM25 318 441 366 338 111 195 565 469
RAGTokenFrozen 378 501 371 511 167 217 559 494729 894RAGSequenceFrozen 412 521 418 526 118 196 567 473
RAGToken 435 548 465 519 179 226 562 494745 906RAGSequence 440 558 449 534 153 215 572 475
between these dates and use a template Who is position eg Who is the President of Peru
to query our NQ RAG model with each index RAG answers 70 correctly using the 2016 index for
2016 world leaders and 68 using the 2018 index for 2018 world leaders Accuracy with mismatched
indices is low 12 with the 2018 index and 2016 leaders 4 with the 2016 index and 2018 leaders
This shows we can update RAGs world knowledge by simply replacing its nonparametric memory
Effect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent
documents and we do not observe signiﬁcant differences in performance between them We have the
ﬂexibility to adjust the number of retrieved documents at test time which can affect performance and
runtime Figure 3 left shows that retrieving more documents at test time monotonically improves
Opendomain QA results for RAGSequence but performance peaks for RAGToken at 10 retrieved
documents Figure 3 right shows that retrieving more documents leads to higher RougeL for
RAGToken at the expense of Bleu1 but the effect is less pronounced for RAGSequence
10 20 30 40 50
KR e t r i e v e dD o c s394041424344NQ Exact MatchRAGTok
RAGSeq
10 20 30 40 50
KR e t r i e v e dD o c s4050607080NQ Answer Recall  KRAGTok
RAGSeq
Fixed DPR
BM25
10 20 30 40 50
KR e t r i e v e dD o c s4850525456Bleu1  RougeL scoreRAGTok RL
RAGTok B1
RAGSeq RL
RAGSeq B1
Figure 3 Left NQ performance as more documents are retrieved Center Retrieval recall perfor
mance in NQ Right MSMARCO Bleu1 and RougeL as more documents are retrieved
5 Related Work
SingleTask Retrieval Prior work has shown that retrieval improves performance across a variety of
NLP tasks when considered in isolation Such tasks include opendomain question answering  529
fact checking  56 fact completion  48 longform question answering  12 Wikipedia article
generation  36 dialogue  4165913 translation  17 and language modeling  1927 Our
work uniﬁes previous successes in incorporating retrieval into individual tasks showing that a single
retrievalbased architecture is capable of achieving strong performance across several tasks
8GeneralPurpose Architectures for NLP Prior work on generalpurpose architectures for NLP
tasks has shown great success without the use of retrieval A single pretrained language model
has been shown to achieve strong performance on various classiﬁcation tasks in the GLUE bench
marks  6061 after ﬁnetuning  498 GPT2  50 later showed that a single lefttoright pretrained
language model could achieve strong performance across both discriminative and generative tasks
For further improvement BART  32 and T5  5152 propose a single pretrained encoderdecoder
model that leverages bidirectional attention to achieve stronger performance on discriminative
and generative tasks Our work aims to expand the space of possible tasks with a single uniﬁed
architecture by learning a retrieval module to augment pretrained generative language models
Learned Retrieval There is signiﬁcant work on learning to retrieve documents in information
retrieval more recently with pretrained neural language models  4426 similar to ours Some
work optimizes the retrieval module to aid in a speciﬁc downstream task such as question answering
using search  46 reinforcement learning  66362 or a latent variable approach  3120 as in our
work These successes leverage different retrievalbased architectures and optimization techniques to
achieve strong performance on a single task while we show that a single retrievalbased architecture
can be ﬁnetuned for strong performance on a variety of tasks
Memorybased Architectures Our document index can be seen as a large external memory for
neural networks to attend to analogous to memory networks  6455 Concurrent work  14 learns
to retrieve a trained embedding for each entity in the input rather than to retrieve raw text as in our
work Other work improves the ability of dialog models to generate factual text by attending over
fact embeddings  1513 A key feature of our memory is that it is comprised of raw text rather
distributed representations which makes the memory both i humanreadable lending a form of
interpretability to our model and ii humanwritable enabling us to dynamically update the models
memory by editing the document index This approach has also been used in knowledgeintensive
dialog where generators have been conditioned on retrieved text directly albeit obtained via TFIDF
rather than endtoend learnt retrieval 9
RetrieveandEdit approaches Our method shares some similarities with retrieveandedit style
approaches where a similar training inputoutput pair is retrieved for a given input and then edited
to provide a ﬁnal output These approaches have proved successful in a number of domains including
Machine Translation  1822 and Semantic Parsing  21 Our approach does have several differences
including less of emphasis on lightly editing a retrieved item but on aggregating content from several
pieces of retrieved content as well as learning latent retrieval and retrieving evidence documents
rather than related training pairs This said RAG techniques may work well in these settings and
could represent promising future work
6 Discussion
In this work we presented hybrid generation models with access to parametric and nonparametric
memory We showed that our RAG models obtain state of the art results on opendomain QA We
found that people prefer RAGs generation over purely parametric BART ﬁnding RAG more factual
and speciﬁc We conducted an thorough investigation of the learned retrieval component validating
its effectiveness and we illustrated how the retrieval index can be hotswapped to update the model
without requiring any retraining In future work it may be fruitful to investigate if the two components
can be jointly pretrained from scratch either with a denoising objective similar to BART or some
another objective Our work opens up new research directions on how parametric and nonparametric
memories interact and how to most effectively combine them showing promise in being applied to a
wide variety of NLP tasks
9Broader Impact
This work offers several positive societal beneﬁts over previous work the fact that it is more
strongly grounded in real factual knowledge in this case Wikipedia makes it hallucinate less
with generations that are more factual and offers more control and interpretability RAG could be
employed in a wide variety of scenarios with direct beneﬁt to society for example by endowing it
with a medical index and asking it opendomain questions on that topic or by helping people be more
effective at their jobs
With these advantages also come potential downsides Wikipedia or any potential external knowledge
source will probably never be entirely factual and completely devoid of bias Since RAG can be
employed as a language model similar concerns as for GPT2  50 are valid here although arguably
to a lesser extent including that it might be used to generate abuse faked or misleading content in
the news or on social media to impersonate others or to automate the production of spamphishing
content  54 Advanced language models may also lead to the automation of various jobs in the
coming decades  16 In order to mitigate these risks AI systems could be employed to ﬁght against
misleading content and automated spamphishing
Acknowledgments
The authors would like to thank the reviewers for their thoughtful and constructive feedback on this
paper as well as HuggingFace for their help in opensourcing code to run RAG models The authors
would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice EP
thanks supports from the NSF Graduate Research Fellowship PL is supported by the FAIR PhD
program
References
1Payal Bajaj Daniel Campos Nick Craswell Li Deng Jianfeng Gao Xiaodong Liu Rangan
Majumder Andrew McNamara Bhaskar Mitra Tri Nguyen Mir Rosenberg Xia Song Alina
Stoica Saurabh Tiwary and Tong Wang MS MARCO A Human Generated MAchine
Reading COmprehension Dataset arXiv161109268 cs  November 2016 URL http
arxivorgabs161109268  arXiv 161109268
2Petr Baudiš and Jan Šediv y Modeling of the question answering task in the yodaqa system In
International Conference of the CrossLanguage Evaluation Forum for European Languages 
pages 222228 Springer 2015 URL httpslinkspringercomchapter101007
2F9783319240275_20 
3Jonathan Berant Andrew Chou Roy Frostig and Percy Liang Semantic Parsing on Freebase
from QuestionAnswer Pairs In Proceedings of the 2013 Conference on Empirical Methods
in Natural Language Processing  pages 15331544 Seattle Washington USA October 2013
Association for Computational Linguistics URL httpwwwaclweborganthology
D131160 
4Bin Bi Chenliang Li Chen Wu Ming Yan and Wei Wang Palm Pretraining an autoencod
ingautoregressive language model for contextconditioned generation ArXiv  abs200407159
2020 URL httpsarxivorgabs200407159 
5Danqi Chen Adam Fisch Jason Weston and Antoine Bordes Reading Wikipedia to Answer
OpenDomain Questions In Proceedings of the 55th Annual Meeting of the Association for
Computational Linguistics Volume 1 Long Papers  pages 18701879 Vancouver Canada
July 2017 Association for Computational Linguistics doi 1018653v1P171171 URL
httpswwwaclweborganthologyP171171 
6Eunsol Choi Daniel Hewlett Jakob Uszkoreit Illia Polosukhin Alexandre Lacoste and
Jonathan Berant Coarsetoﬁne question answering for long documents In Proceedings of the
55th Annual Meeting of the Association for Computational Linguistics Volume 1 Long Papers 
pages 209220 Vancouver Canada July 2017 Association for Computational Linguistics doi
1018653v1P171020 URL httpswwwaclweborganthologyP171020 
107Christopher Clark and Matt Gardner Simple and Effective MultiParagraph Reading Compre
hension arXiv171010723 cs  October 2017 URL httparxivorgabs171010723 
arXiv 171010723
8Jacob Devlin MingWei Chang Kenton Lee and Kristina Toutanova BERT Pretraining of
Deep Bidirectional Transformers for Language Understanding In Proceedings of the 2019 Con
ference of the North American Chapter of the Association for Computational Linguistics Human
Language Technologies Volume 1 Long and Short Papers  pages 41714186 Minneapolis
Minnesota June 2019 Association for Computational Linguistics doi 1018653v1N191423
URL httpswwwaclweborganthologyN191423 
9Emily Dinan Stephen Roller Kurt Shuster Angela Fan Michael Auli and Jason Weston Wiz
ard of wikipedia Knowledgepowered conversational agents In International Conference on
Learning Representations  2019 URL httpsopenreviewnetforumidr1l73iRqKm 
10 Matthew Dunn Levent Sagun Mike Higgins V  Ugur Guney V olkan Cirik and Kyunghyun
Cho SearchQA A New QA Dataset Augmented with Context from a Search Engine
arXiv170405179 cs  April 2017 URL httparxivorgabs170405179  arXiv
170405179
11 Angela Fan Mike Lewis and Yann Dauphin Hierarchical neural story generation In Proceed
ings of the 56th Annual Meeting of the Association for Computational Linguistics Volume 1
Long Papers  pages 889898 Melbourne Australia July 2018 Association for Computational
Linguistics doi 1018653v1P181082 URL httpswwwaclweborganthology
P181082 
12 Angela Fan Yacine Jernite Ethan Perez David Grangier Jason Weston and Michael Auli ELI5
Long form question answering In Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics  pages 35583567 Florence Italy July 2019 Association for
Computational Linguistics doi 1018653v1P191346 URL httpswwwaclweborg
anthologyP191346 
13 Angela Fan Claire Gardent Chloe Braud and Antoine Bordes Augmenting transformers
with KNNbased composite memory 2020 URL httpsopenreviewnetforumid
H1gx1CNKPH 
14 Thibault Févry Livio Baldini Soares Nicholas FitzGerald Eunsol Choi and Tom Kwiatkowski
Entities as experts Sparse memory access with entity supervision ArXiv  abs200407202
2020 URL httpsarxivorgabs200407202 
15 Marjan Ghazvininejad Chris Brockett MingWei Chang Bill Dolan Jianfeng Gao Wen
tau Yih and Michel Galley A knowledgegrounded neural conversation model In AAAI
Conference on Artiﬁcial Intelligence  2018 URL httpswwwaaaiorgocsindexphp
AAAIAAAI18paperview16710 
16 Katja Grace John Salvatier Allan Dafoe Baobao Zhang and Owain Evans When will AI
exceed human performance evidence from AI experts CoRR  abs170508807 2017 URL
httparxivorgabs170508807 
17 Jiatao Gu Yong Wang Kyunghyun Cho and Victor OK Li Search engine guided neural
machine translation In AAAI Conference on Artiﬁcial Intelligence  2018 URL https
wwwaaaiorgocsindexphpAAAIAAAI18paperview17282 
18 Jiatao Gu Yong Wang Kyunghyun Cho and Victor OK Li Search engine guided neural
machine translation In 32nd AAAI Conference on Artiﬁcial Intelligence AAAI 2018  32nd
AAAI Conference on Artiﬁcial Intelligence AAAI 2018 pages 51335140 AAAI press 2018
32nd AAAI Conference on Artiﬁcial Intelligence AAAI 2018  Conference date 02022018
Through 07022018
19 Kelvin Guu Tatsunori B Hashimoto Yonatan Oren and Percy Liang Generating sentences by
editing prototypes Transactions of the Association for Computational Linguistics  6437450
2018 doi 101162tacl_a_00030 URL httpswwwaclweborganthologyQ181031 
1120 Kelvin Guu Kenton Lee Zora Tung Panupong Pasupat and MingWei Chang REALM
Retrievalaugmented language model pretraining ArXiv  abs200208909 2020 URL https
arxivorgabs200208909 
21 Tatsunori B Hashimoto Kelvin Guu Yonatan Oren and Percy S Liang A
retrieveandedit framework for predicting structured outputs In S Bengio
H Wallach H Larochelle K Grauman N CesaBianchi and R Garnett ed
itors Advances in Neural Information Processing Systems 31  pages 10052
10062 Curran Associates Inc 2018 URL httppapersnipsccpaper
8209aretrieveandeditframeworkforpredictingstructuredoutputs
pdf
22 Nabil Hossain Marjan Ghazvininejad and Luke Zettlemoyer Simple and effective retrieve
editrerank text generation In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics  pages 25322538 Online July 2020 Association for Computa
tional Linguistics doi 1018653v12020aclmain228 URL httpswwwaclweborg
anthology2020aclmain228 
23 Jeff Johnson Matthijs Douze and Hervé Jégou Billionscale similarity search with gpus arXiv
preprint arXiv170208734  2017 URL httpsarxivorgabs170208734 
24 Mandar Joshi Eunsol Choi Daniel Weld and Luke Zettlemoyer TriviaQA A Large Scale
Distantly Supervised Challenge Dataset for Reading Comprehension In Proceedings of the
55th Annual Meeting of the Association for Computational Linguistics Volume 1 Long Papers 
pages 16011611 Vancouver Canada July 2017 Association for Computational Linguistics
doi 1018653v1P171147 URL httpswwwaclweborganthologyP171147 
25 Armand Joulin and Tomas Mikolov Inferring algorithmic patterns with stack
augmented recurrent nets In Proceedings of the 28th International Conference on
Neural Information Processing Systems  Volume 1  NIPS15 page 190198 Cam
bridge MA USA 2015 MIT Press URL httpspapersnipsccpaper
5857inferringalgorithmicpatternswithstackaugmentedrecurrentnets 
26 Vladimir Karpukhin Barlas Oguz Sewon Min Ledell Wu Sergey Edunov Danqi Chen and
Wentau Yih Dense passage retrieval for opendomain question answering arXiv preprint
arXiv200404906  2020 URL httpsarxivorgabs200404906 
27 Urvashi Khandelwal Omer Levy Dan Jurafsky Luke Zettlemoyer and Mike Lewis Generaliza
tion through memorization Nearest neighbor language models In International Conference on
Learning Representations  2020 URL httpsopenreviewnetforumidHklBjCEKvH 
28 Diederik P Kingma and Jimmy Ba Adam A method for stochastic optimization In Yoshua
Bengio and Yann LeCun editors 3rd International Conference on Learning Representations
ICLR 2015 San Diego CA USA May 79 2015 Conference Track Proceedings  2015 URL
httparxivorgabs14126980 
29 Tom Kwiatkowski Jennimaria Palomaki Olivia Redﬁeld Michael Collins Ankur Parikh
Chris Alberti Danielle Epstein Illia Polosukhin Matthew Kelcey Jacob Devlin Ken
ton Lee Kristina N Toutanova Llion Jones MingWei Chang Andrew Dai Jakob
Uszkoreit Quoc Le and Slav Petrov Natural Questions a Benchmark for Ques
tion Answering Research Transactions of the Association of Computational Lin
guistics  2019 URL httpstomkwiatusersx20webcorpgooglecompapers
naturalquestionsmain1455kwiatkowskipdf 
30 Guillaume Lample Alexandre Sablayrolles Marc Aurelio Ranzato Ludovic Denoyer and
Herve Jegou Large memory layers with product keys In H Wallach H Larochelle
A Beygelzimer F d AlchéBuc E Fox and R Garnett editors Advances in Neural In
formation Processing Systems 32  pages 85488559 Curran Associates Inc 2019 URL http
papersnipsccpaper9061largememorylayerswithproductkeyspdf 
31 Kenton Lee MingWei Chang and Kristina Toutanova Latent retrieval for weakly supervised
open domain question answering In Proceedings of the 57th Annual Meeting of the Association
12for Computational Linguistics  pages 60866096 Florence Italy July 2019 Association for
Computational Linguistics doi 1018653v1P191612 URL httpswwwaclweborg
anthologyP191612 
32 Mike Lewis Yinhan Liu Naman Goyal Marjan Ghazvininejad Abdelrahman Mohamed
Omer Levy Veselin Stoyanov and Luke Zettlemoyer BART Denoising sequencetosequence
pretraining for natural language generation translation and comprehension arXiv preprint
arXiv191013461  2019 URL httpsarxivorgabs191013461 
33 Jiwei Li Michel Galley Chris Brockett Jianfeng Gao and Bill Dolan A diversitypromoting
objective function for neural conversation models In Proceedings of the 2016 Conference of the
North American Chapter of the Association for Computational Linguistics Human Language
Technologies  pages 110119 San Diego California June 2016 Association for Computational
Linguistics doi 1018653v1N161014 URL httpswwwaclweborganthology
N161014 
34 Margaret Li Jason Weston and Stephen Roller Acuteeval Improved dialogue evaluation
with optimized questions and multiturn comparisons ArXiv  abs190903087 2019 URL
httpsarxivorgabs190903087 
35 Hairong Liu Mingbo Ma Liang Huang Hao Xiong and Zhongjun He Robust neural machine
translation with joint textual and phonetic embedding In Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics  pages 30443049 Florence Italy
July 2019 Association for Computational Linguistics doi 1018653v1P191291 URL
httpswwwaclweborganthologyP191291 
36 Peter J Liu Mohammad Saleh Etienne Pot Ben Goodrich Ryan Sepassi Lukasz Kaiser
and Noam Shazeer Generating wikipedia by summarizing long sequences In International
Conference on Learning Representations  2018 URL httpsopenreviewnetforum
idHyg0vbWC 
37 Yury A Malkov and D A Yashunin Efﬁcient and robust approximate nearest neighbor search
using hierarchical navigable small world graphs IEEE Transactions on Pattern Analysis and
Machine Intelligence  42824836 2016 URL httpsarxivorgabs160309320 
38 Gary Marcus The next decade in ai four steps towards robust artiﬁcial intelligence arXiv
preprint arXiv200206177  2020 URL httpsarxivorgabs200206177 
39 Luca Massarelli Fabio Petroni Aleksandra Piktus Myle Ott Tim Rocktäschel Vassilis
Plachouras Fabrizio Silvestri and Sebastian Riedel How decoding strategies affect the
veriﬁability of generated text arXiv preprint arXiv191103587  2019 URL https
arxivorgabs191103587 
40 Paulius Micikevicius Sharan Narang Jonah Alben Gregory Diamos Erich Elsen David Garcia
Boris Ginsburg Michael Houston Oleksii Kuchaiev Ganesh Venkatesh and Hao Wu Mixed
precision training In ICLR  2018 URL httpsopenreviewnetforumidr1gs9JgRZ 
41 Nikita Moghe Siddhartha Arora Suman Banerjee and Mitesh M Khapra Towards exploit
ing background knowledge for building conversation systems In Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing  pages 23222332 Brus
sels Belgium OctoberNovember 2018 Association for Computational Linguistics doi
1018653v1D181255 URL httpswwwaclweborganthologyD181255 
42 Preksha Nema and Mitesh M Khapra Towards a better metric for evaluating question generation
systems In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing  pages 39503959 Brussels Belgium OctoberNovember 2018 Association for
Computational Linguistics doi 1018653v1D181429 URL httpswwwaclweborg
anthologyD181429 
43 Tri Nguyen Mir Rosenberg Xia Song Jianfeng Gao Saurabh Tiwary Rangan Majumder
and Li Deng MS MARCO A human generated machine reading comprehension dataset In
Tarek Richard Besold Antoine Bordes Artur S dAvila Garcez and Greg Wayne editors
Proceedings of the Workshop on Cognitive Computation Integrating neural and symbolic
13approaches 2016 colocated with the 30th Annual Conference on Neural Information Processing
Systems NIPS 2016 Barcelona Spain December 9 2016  volume 1773 of CEUR Workshop
Proceedings  CEURWSorg 2016 URL httpceurwsorgVol1773CoCoNIPS_
2016_paper9pdf 
44 Rodrigo Nogueira and Kyunghyun Cho Passage reranking with BERT arXiv preprint
arXiv190104085  2019 URL httpsarxivorgabs190104085 
45 Myle Ott Sergey Edunov Alexei Baevski Angela Fan Sam Gross Nathan Ng David Grangier
and Michael Auli fairseq A fast extensible toolkit for sequence modeling In Proceedings
of the 2019 Conference of the North American Chapter of the Association for Computational
Linguistics Demonstrations  pages 4853 Minneapolis Minnesota June 2019 Association
for Computational Linguistics doi 1018653v1N194009 URL httpswwwaclweb
organthologyN194009 
46 Ethan Perez Siddharth Karamcheti Rob Fergus Jason Weston Douwe Kiela and Kyunghyun
Cho Finding generalizable evidence by learning to convince qa models In Proceedings
of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing EMNLPIJCNLP  pages
24022411 Hong Kong China November 2019 Association for Computational Linguistics
doi 1018653v1D191244 URL httpswwwaclweborganthologyD191244 
47 Fabio Petroni Tim Rocktäschel Sebastian Riedel Patrick Lewis Anton Bakhtin Yuxiang Wu
and Alexander Miller Language models as knowledge bases In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing EMNLPIJCNLP  pages 24632473 Hong
Kong China November 2019 Association for Computational Linguistics doi 1018653v1
D191250 URL httpswwwaclweborganthologyD191250 
48 Fabio Petroni Patrick Lewis Aleksandra Piktus Tim Rocktäschel Yuxiang Wu Alexander H
Miller and Sebastian Riedel How context affects language models factual predictions In
Automated Knowledge Base Construction  2020 URL httpsopenreviewnetforum
id025X0zPfn 
49 Alec Radford Karthik Narasimhan Tim Salimans and Ilya Sutskever Im
proving Language Understanding by Generative PreTraining 2018 URL
httpss3uswest2amazonawscomopenaiassetsresearchcovers
languageunsupervisedlanguage_understanding_paperpdf 
50 Alec Radford Jeff Wu Rewon Child David Luan Dario Amodei and Ilya
Sutskever Language models are unsupervised multitask learners 2019 URL
httpsd4mucfpksywvcloudfrontnetbetterlanguagemodelslanguage_
models_are_unsupervised_multitask_learnerspdf 
51 Colin Raffel Noam Shazeer Adam Roberts Katherine Lee Sharan Narang Michael Matena
Yanqi Zhou Wei Li and Peter J Liu Exploring the limits of transfer learning with a uniﬁed
texttotext transformer arXiv eprints  2019 URL httpsarxivorgabs191010683 
52 Adam Roberts Colin Raffel and Noam Shazeer How much knowledge can you pack into
the parameters of a language model arXiv eprints  2020 URL httpsarxivorgabs
200208910 
53 Stephen Robertson and Hugo Zaragoza The probabilistic relevance framework Bm25 and
beyond Found Trends Inf Retr  34333389 April 2009 ISSN 15540669 doi 101561
1500000019 URL httpsdoiorg1015611500000019 
54 Irene Solaiman Miles Brundage Jack Clark Amanda Askell Ariel HerbertV oss Jeff Wu Alec
Radford and JianBing Wang Release strategies and the social impacts of language models
ArXiv  abs190809203 2019
55 Sainbayar Sukhbaatar Arthur Szlam Jason Weston and Rob Fergus Endtoend memory net
works In C Cortes N D Lawrence D D Lee M Sugiyama and R Garnett editors Advances
in Neural Information Processing Systems 28  pages 24402448 Curran Associates Inc 2015
URL httppapersnipsccpaper5846endtoendmemorynetworkspdf 
1456 James Thorne Andreas Vlachos Christos Christodoulopoulos and Arpit Mittal FEVER a
largescale dataset for fact extraction and VERiﬁcation In Proceedings of the 2018 Conference
of the North American Chapter of the Association for Computational Linguistics Human
Language Technologies Volume 1 Long Papers  pages 809819 New Orleans Louisiana
June 2018 Association for Computational Linguistics doi 1018653v1N181074 URL
httpswwwaclweborganthologyN181074 
57 James H Thorne and Andreas Vlachos Avoiding catastrophic forgetting in mitigating model
biases in sentencepair classiﬁcation with elastic weight consolidation ArXiv  abs200414366
2020 URL httpsarxivorgabs200414366 
58 Ashish Vaswani Noam Shazeer Niki Parmar Jakob Uszkoreit Llion Jones Aidan N Gomez
Ł ukasz Kaiser and Illia Polosukhin Attention is all you need In I Guyon U V  Luxburg
S Bengio H Wallach R Fergus S Vishwanathan and R Garnett editors Advances in Neural
Information Processing Systems 30  pages 59986008 Curran Associates Inc 2017 URL
httppapersnipsccpaper7181attentionisallyouneedpdf 
59 Ashwin Vijayakumar Michael Cogswell Ramprasaath Selvaraju Qing Sun Stefan Lee David
Crandall and Dhruv Batra Diverse beam search for improved description of complex scenes
AAAI Conference on Artiﬁcial Intelligence  2018 URL httpswwwaaaiorgocsindex
phpAAAIAAAI18paperview17329 
60 Alex Wang Amanpreet Singh Julian Michael Felix Hill Omer Levy and Samuel Bowman
GLUE A multitask benchmark and analysis platform for natural language understanding
InProceedings of the 2018 EMNLP Workshop BlackboxNLP Analyzing and Interpreting
Neural Networks for NLP  pages 353355 Brussels Belgium November 2018 Association for
Computational Linguistics doi 1018653v1W185446 URL httpswwwaclweborg
anthologyW185446 
61 Alex Wang Yada Pruksachatkun Nikita Nangia Amanpreet Singh Julian Michael Felix
Hill Omer Levy and Samuel Bowman SuperGLUE A Stickier Benchmark for General
Purpose Language Understanding Systems In H Wallach H Larochelle A Beygelzimer
F dtextquotesingle AlchéBuc E Fox and R Garnett editors Advances in Neural Information
Processing Systems 32  pages 32613275 Curran Associates Inc 2019 URL https
arxivorgabs190500537 
62 Shuohang Wang Mo Yu Xiaoxiao Guo Zhiguo Wang Tim Klinger Wei Zhang Shiyu Chang
Gerry Tesauro Bowen Zhou and Jing Jiang R3 Reinforced rankerreader for opendomain
question answering In Sheila A McIlraith and Kilian Q Weinberger editors Proceedings of
the ThirtySecond AAAI Conference on Artiﬁcial Intelligence AAAI18 the 30th innovative
Applications of Artiﬁcial Intelligence IAAI18 and the 8th AAAI Symposium on Educational
Advances in Artiﬁcial Intelligence EAAI18 New Orleans Louisiana USA February 27
2018  pages 59815988 AAAI Press 2018 URL httpswwwaaaiorgocsindex
phpAAAIAAAI18paperview16712 
63 Shuohang Wang Mo Yu Jing Jiang Wei Zhang Xiaoxiao Guo Shiyu Chang Zhiguo Wang
Tim Klinger Gerald Tesauro and Murray Campbell Evidence aggregation for answer re
ranking in opendomain question answering In ICLR  2018 URL httpsopenreview
netforumidrJl3yMAb 
64 Jason Weston Sumit Chopra and Antoine Bordes Memory networks In Yoshua Bengio
and Yann LeCun editors 3rd International Conference on Learning Representations ICLR
2015 San Diego CA USA May 79 2015 Conference Track Proceedings  2015 URL
httparxivorgabs14103916 
65 Jason Weston Emily Dinan and Alexander Miller Retrieve and reﬁne Improved sequence
generation models for dialogue In Proceedings of the 2018 EMNLP Workshop SCAI The 2nd
International Workshop on SearchOriented Conversational AI  pages 8792 Brussels Belgium
October 2018 Association for Computational Linguistics doi 1018653v1W185713 URL
httpswwwaclweborganthologyW185713 
1566 Thomas Wolf Lysandre Debut Victor Sanh Julien Chaumond Clement Delangue Anthony
Moi Pierric Cistac Tim Rault Rémi Louf Morgan Funtowicz Joe Davison Sam Shleifer
Patrick von Platen Clara Ma Yacine Jernite Julien Plu Canwen Xu Teven Le Scao Sylvain
Gugger Mariama Drame Quentin Lhoest and Alexander M Rush Huggingfaces transformers
Stateoftheart natural language processing ArXiv  abs191003771 2019
67 Shiyue Zhang and Mohit Bansal Addressing semantic drift in question generation for semi
supervised question answering In Proceedings of the 2019 Conference on Empirical Meth
ods in Natural Language Processing and the 9th International Joint Conference on Natural
Language Processing EMNLPIJCNLP  pages 24952509 Hong Kong China Novem
ber 2019 Association for Computational Linguistics doi 1018653v1D191253 URL
httpswwwaclweborganthologyD191253 
68 Wanjun Zhong Jingjing Xu Duyu Tang Zenan Xu Nan Duan Ming Zhou Jiahai Wang and
Jian Yin Reasoning over semanticlevel graph for fact checking ArXiv  abs190903745 2019
URL httpsarxivorgabs190903745 
16Appendices for RetrievalAugmented Generation for
KnowledgeIntensive NLP Tasks
A Implementation Details
For Opendomain QA we report test numbers using 15 retrieved documents for RAGToken models
For RAGSequence models we report test results using 50 retrieved documents and we use the
Thorough Decoding approach since answers are generally short We use greedy decoding for QA as
we did not ﬁnd beam search improved results For OpenMSMarco and Jeopardy question generation
we report test numbers using ten retrieved documents for both RAGToken and RAGSequence
and we also train a BARTlarge model as a baseline We use a beam size of four and use the Fast
Decoding approach for RAGSequence models as Thorough Decoding did not improve performance
B Human Evaluation
Figure 4 Annotation interface for human evaluation of factuality A popout for detailed instructions
and a worked example appear when clicking view tool guide
Figure 4 shows the user interface for human evaluation To avoid any biases for screen position
which model corresponded to sentence A and sentence B was randomly selected for each example
Annotators were encouraged to research the topic using the internet and were given detailed instruc
tions and worked examples in a full instructions tab We included some gold sentences in order to
assess the accuracy of the annotators Two annotators did not perform well on these examples and
their annotations were removed from the results
C Training setup Details
We train all RAG models and BART baselines using Fairseq  452We train with mixed precision
ﬂoating point arithmetic  40 distributing training across 8 32GB NVIDIA V100 GPUs though
training and inference can be run on one GPU We ﬁnd that doing Maximum Inner Product Search
with FAISS is sufﬁciently fast on CPU so we store document index vectors on CPU requiring 100
GB of CPU memory for all of Wikipedia After submission We have ported our code to HuggingFace
Transformers  663 which achieves equivalent performance to the previous version but is a cleaner
and easier to use implementation This version is also opensourced We also compress the document
index using FAISSs compression tools reducing the CPU memory requirement to 36GB Scripts to
run experiments with RAG can be found at httpsgithubcomhuggingfacetransformers
blobmasterexamplesragREADMEmd and an interactive demo of a RAG model can be found
athttpshuggingfacecorag
2httpsgithubcompytorchfairseq
3httpsgithubcomhuggingfacetransformers
17D Further Details on OpenDomain QA
For opendomain QA multiple answer annotations are often available for a given question These
answer annotations are exploited by extractive models during training as typically all the answer
annotations are used to ﬁnd matches within documents when preparing training data For RAG we
also make use of multiple annotation examples for Natural Questions and WebQuestions by training
the model with each qapair separately leading to a small increase in accuracy For TriviaQA
there are often many valid answers to a given question some of which are not suitable training targets
such as emoji or spelling variants For TriviaQA we ﬁlter out answer candidates if they do not occur
in top 1000 documents for the query
CuratedTrec preprocessing The answers for CuratedTrec are given in the form of regular expres
sions which has been suggested as a reason why it is unsuitable for answergeneration models 20
To overcome this we use a preprocessing step where we ﬁrst retrieve the top 1000 documents for
each query and use the answer that most frequently matches the regex pattern as the supervision
target If no matches are found we resort to a simple heuristic generate all possible permutations for
each regex replacing nondeterministic symbols in the regex nested tree structure with a whitespace
TriviaQA Evaluation setups The opendomain QA community customarily uses public develop
ment datasets as test datasets as test data for QA datasets is often restricted and dedicated to reading
compehension purposes We report our results using the datasets splits used in DPR  26 which are
consistent with common practice in Opendomain QA For TriviaQA this test dataset is the public
TriviaQA Web Development split Roberts et al 52 used the TriviaQA ofﬁcial Wikipedia test set
instead Févry et al 14 follow this convention in order to compare with Roberts et al 52 See
appendix of  14 We report results on both test sets to enable fair comparison to both approaches
We ﬁnd that our performance is much higher using the ofﬁcial Wiki test set rather than the more
conventional opendomain test set which we attribute to the ofﬁcial Wiki test set questions being
simpler to answer from Wikipedia
E Further Details on FEVER
For FEVER classiﬁcation we follow the practice from  32 and ﬁrst regenerate the claim and
then classify using the representation of the ﬁnal hidden state before ﬁnally marginalizing across
documents to obtain the class probabilities The FEVER task traditionally has two subtasks The
ﬁrst is to classify the claim as either Supported Refuted or Not Enough Info which is the task
we explore in the main paper FEVERs other subtask involves extracting sentences from Wikipedia
as evidence supporting the classiﬁcation prediction As FEVER uses a different Wikipedia dump to
us directly tackling this task is not straightforward We hope to address this in future work
F Null Document Probabilities
We experimented with adding Null document mechanism to RAG similar to REALM  20 in order
to model cases where no useful information could be retrieved for a given input Here if kdocuments
were retrieved we would additionally retrieve an empty document and predict a logit for the null
document before marginalizing over k 1predictions We explored modelling this null document
logit by learning i a document embedding for the null document ii a static learnt bias term or
iii a neural network to predict the logit We did not ﬁnd that these improved performance so in
the interests of simplicity we omit them For Open MSMARCO where useful retrieved documents
cannot always be retrieved we observe that the model learns to always retrieve a particular set of
documents for questions that are less likely to beneﬁt from retrieval suggesting that null document
mechanisms may not be necessary for RAG
G Parameters
Our RAG models contain the trainable parameters for the BERTbase query and document encoder of
DPR with 110M parameters each although we do not train the document encoder ourselves and
406M trainable parameters from BARTlarge 406M parameters making a total of 626M trainable
18Table 7 Number of instances in the datasets used A hidden subset of this data is used for evaluation
Task Train Development Test
Natural Questions 79169 8758 3611
TriviaQA 78786 8838 11314
WebQuestions 3418 362 2033
CuratedTrec 635 134 635
Jeopardy Question Generation 97392 13714 26849
MSMARCO 153726 12468 101093
FEVER3way 145450 10000 10000
FEVER2way 96966 6666 6666
parameters The best performing closedbook parametric only opendomain QA model is T511B
with 11 Billion trainable parameters The T5 model with the closest number of parameters to our
models is T5large 770M parameters which achieves a score of 289 EM on Natural Questions  52
substantially below the 445 that RAGSequence achieves indicating that hybrid parametricnon
parametric models require far fewer trainable parameters for strong opendomain QA performance
The nonparametric memory index does not consist of trainable parameters but does consists of 21M
728 dimensional vectors consisting of 153B values These can be easily be stored at 8bit ﬂoating
point precision to manage memory and disk footprints
H Retrieval Collapse
In preliminary experiments we observed that for some tasks such as story generation  11 the
retrieval component would collapse and learn to retrieve the same documents regardless of the
input In these cases once retrieval had collapsed the generator would learn to ignore the documents
and the RAG model would perform equivalently to BART The collapse could be due to a lessexplicit
requirement for factual knowledge in some tasks or the longer target sequences which could result
in less informative gradients for the retriever Perez et al 46 also found spurious retrieval results
when optimizing a retrieval component in order to improve performance on downstream tasks
I Number of instances per dataset
The number of training development and test datapoints in each of our datasets is shown in Table 7
19
  Provided proper attribution is provided Google hereby grants permission to
reproduce the tables and figures in this paper solely for use in journalistic or
scholarly works
Attention Is All You Need
Ashish Vaswani
Google Brain
avaswanigooglecomNoam Shazeer
Google Brain
noamgooglecomNiki Parmar
Google Research
nikipgooglecomJakob Uszkoreit
Google Research
uszgooglecom
Llion Jones
Google Research
lliongooglecomAidan N Gomez 
University of Toronto
aidancstorontoeduŁukasz Kaiser
Google Brain
lukaszkaisergooglecom
Illia Polosukhin 
illiapolosukhingmailcom
Abstract
The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder The best
performing models also connect the encoder and decoder through an attention
mechanism We propose a new simple network architecture the Transformer
based solely on attention mechanisms dispensing with recurrence and convolutions
entirely Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train Our model achieves 284 BLEU on the WMT 2014 English
toGerman translation task improving over the existing best results including
ensembles by over 2 BLEU On the WMT 2014 EnglishtoFrench translation task
our model establishes a new singlemodel stateoftheart BLEU score of 418 after
training for 35 days on eight GPUs a small fraction of the training costs of the
best models from the literature We show that the Transformer generalizes well to
other tasks by applying it successfully to English constituency parsing both with
large and limited training data
Equal contribution Listing order is random Jakob proposed replacing RNNs with selfattention and started
the effort to evaluate this idea Ashish with Illia designed and implemented the first Transformer models and
has been crucially involved in every aspect of this work Noam proposed scaled dotproduct attention multihead
attention and the parameterfree position representation and became the other person involved in nearly every
detail Niki designed implemented tuned and evaluated countless model variants in our original codebase and
tensor2tensor Llion also experimented with novel model variants was responsible for our initial codebase and
efficient inference and visualizations Lukasz and Aidan spent countless long days designing various parts of and
implementing tensor2tensor replacing our earlier codebase greatly improving results and massively accelerating
our research
Work performed while at Google Brain
Work performed while at Google Research
31st Conference on Neural Information Processing Systems NIPS 2017 Long Beach CA USAarXiv170603762v7  csCL  2 Aug 20231 Introduction
Recurrent neural networks long shortterm memory  13 and gated recurrent  7 neural networks
in particular have been firmly established as state of the art approaches in sequence modeling and
transduction problems such as language modeling and machine translation  3525 Numerous
efforts have since continued to push the boundaries of recurrent language models and encoderdecoder
architectures 38 24 15
Recurrent models typically factor computation along the symbol positions of the input and output
sequences Aligning the positions to steps in computation time they generate a sequence of hidden
states ht as a function of the previous hidden state ht1and the input for position t This inherently
sequential nature precludes parallelization within training examples which becomes critical at longer
sequence lengths as memory constraints limit batching across examples Recent work has achieved
significant improvements in computational efficiency through factorization tricks  21 and conditional
computation  32 while also improving model performance in case of the latter The fundamental
constraint of sequential computation however remains
Attention mechanisms have become an integral part of compelling sequence modeling and transduc
tion models in various tasks allowing modeling of dependencies without regard to their distance in
the input or output sequences  219 In all but a few cases  27 however such attention mechanisms
are used in conjunction with a recurrent network
In this work we propose the Transformer a model architecture eschewing recurrence and instead
relying entirely on an attention mechanism to draw global dependencies between input and output
The Transformer allows for significantly more parallelization and can reach a new state of the art in
translation quality after being trained for as little as twelve hours on eight P100 GPUs
2 Background
The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU
16 ByteNet  18 and ConvS2S  9 all of which use convolutional neural networks as basic building
block computing hidden representations in parallel for all input and output positions In these models
the number of operations required to relate signals from two arbitrary input or output positions grows
in the distance between positions linearly for ConvS2S and logarithmically for ByteNet This makes
it more difficult to learn dependencies between distant positions  12 In the Transformer this is
reduced to a constant number of operations albeit at the cost of reduced effective resolution due
to averaging attentionweighted positions an effect we counteract with MultiHead Attention as
described in section 32
Selfattention sometimes called intraattention is an attention mechanism relating different positions
of a single sequence in order to compute a representation of the sequence Selfattention has been
used successfully in a variety of tasks including reading comprehension abstractive summarization
textual entailment and learning taskindependent sentence representations 4 27 28 22
Endtoend memory networks are based on a recurrent attention mechanism instead of sequence
aligned recurrence and have been shown to perform well on simplelanguage question answering and
language modeling tasks 34
To the best of our knowledge however the Transformer is the first transduction model relying
entirely on selfattention to compute representations of its input and output without using sequence
aligned RNNs or convolution In the following sections we will describe the Transformer motivate
selfattention and discuss its advantages over models such as 17 18 and 9
3 Model Architecture
Most competitive neural sequence transduction models have an encoderdecoder structure  5235
Here the encoder maps an input sequence of symbol representations x1  x nto a sequence
of continuous representations z z1  z n Given z the decoder then generates an output
sequence y1  y mof symbols one element at a time At each step the model is autoregressive
10 consuming the previously generated symbols as additional input when generating the next
2Figure 1 The Transformer  model architecture
The Transformer follows this overall architecture using stacked selfattention and pointwise fully
connected layers for both the encoder and decoder shown in the left and right halves of Figure 1
respectively
31 Encoder and Decoder Stacks
Encoder The encoder is composed of a stack of N 6 identical layers Each layer has two
sublayers The first is a multihead selfattention mechanism and the second is a simple position
wise fully connected feedforward network We employ a residual connection  11 around each of
the two sublayers followed by layer normalization  1 That is the output of each sublayer is
LayerNorm x Sublayer x where Sublayer xis the function implemented by the sublayer
itself To facilitate these residual connections all sublayers in the model as well as the embedding
layers produce outputs of dimension dmodel  512 
Decoder The decoder is also composed of a stack of N 6identical layers In addition to the two
sublayers in each encoder layer the decoder inserts a third sublayer which performs multihead
attention over the output of the encoder stack Similar to the encoder we employ residual connections
around each of the sublayers followed by layer normalization We also modify the selfattention
sublayer in the decoder stack to prevent positions from attending to subsequent positions This
masking combined with fact that the output embeddings are offset by one position ensures that the
predictions for position ican depend only on the known outputs at positions less than i
32 Attention
An attention function can be described as mapping a query and a set of keyvalue pairs to an output
where the query keys values and output are all vectors The output is computed as a weighted sum
3Scaled DotProduct Attention
 MultiHead Attention
Figure 2 left Scaled DotProduct Attention right MultiHead Attention consists of several
attention layers running in parallel
of the values where the weight assigned to each value is computed by a compatibility function of the
query with the corresponding key
321 Scaled DotProduct Attention
We call our particular attention Scaled DotProduct Attention Figure 2 The input consists of
queries and keys of dimension dk and values of dimension dv We compute the dot products of the
query with all keys divide each bydk and apply a softmax function to obtain the weights on the
values
In practice we compute the attention function on a set of queries simultaneously packed together
into a matrix Q The keys and values are also packed together into matrices KandV We compute
the matrix of outputs as
Attention Q K V   softmaxQKT
dkV 1
The two most commonly used attention functions are additive attention  2 and dotproduct multi
plicative attention Dotproduct attention is identical to our algorithm except for the scaling factor
of1dk Additive attention computes the compatibility function using a feedforward network with
a single hidden layer While the two are similar in theoretical complexity dotproduct attention is
much faster and more spaceefficient in practice since it can be implemented using highly optimized
matrix multiplication code
While for small values of dkthe two mechanisms perform similarly additive attention outperforms
dot product attention without scaling for larger values of dk3 We suspect that for large values of
dk the dot products grow large in magnitude pushing the softmax function into regions where it has
extremely small gradients4 To counteract this effect we scale the dot products by1dk
322 MultiHead Attention
Instead of performing a single attention function with dmodeldimensional keys values and queries
we found it beneficial to linearly project the queries keys and values htimes with different learned
linear projections to dkdkanddvdimensions respectively On each of these projected versions of
queries keys and values we then perform the attention function in parallel yielding dvdimensional
4To illustrate why the dot products get large assume that the components of qandkare independent random
variables with mean 0and variance 1 Then their dot product qkPdk
i1qiki has mean 0and variance dk
4output values These are concatenated and once again projected resulting in the final values as
depicted in Figure 2
Multihead attention allows the model to jointly attend to information from different representation
subspaces at different positions With a single attention head averaging inhibits this
MultiHead Q K V   Concathead 1 head hWO
where head i Attention QWQ
i KWK
i V WV
i
Where the projections are parameter matrices WQ
iRdmodeldkWK
iRdmodeldkWV
iRdmodeldv
andWORhdvdmodel
In this work we employ h 8 parallel attention layers or heads For each of these we use
dkdvdmodelh 64  Due to the reduced dimension of each head the total computational cost
is similar to that of singlehead attention with full dimensionality
323 Applications of Attention in our Model
The Transformer uses multihead attention in three different ways
In encoderdecoder attention layers the queries come from the previous decoder layer
and the memory keys and values come from the output of the encoder This allows every
position in the decoder to attend over all positions in the input sequence This mimics the
typical encoderdecoder attention mechanisms in sequencetosequence models such as
38 2 9
The encoder contains selfattention layers In a selfattention layer all of the keys values
and queries come from the same place in this case the output of the previous layer in the
encoder Each position in the encoder can attend to all positions in the previous layer of the
encoder
Similarly selfattention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position We need to prevent leftward
information flow in the decoder to preserve the autoregressive property We implement this
inside of scaled dotproduct attention by masking out setting to  all values in the input
of the softmax which correspond to illegal connections See Figure 2
33 Positionwise FeedForward Networks
In addition to attention sublayers each of the layers in our encoder and decoder contains a fully
connected feedforward network which is applied to each position separately and identically This
consists of two linear transformations with a ReLU activation in between
FFN x  max0  xW 1b1W2b2 2
While the linear transformations are the same across different positions they use different parameters
from layer to layer Another way of describing this is as two convolutions with kernel size 1
The dimensionality of input and output is dmodel  512  and the innerlayer has dimensionality
dff 2048 
34 Embeddings and Softmax
Similarly to other sequence transduction models we use learned embeddings to convert the input
tokens and output tokens to vectors of dimension dmodel We also use the usual learned linear transfor
mation and softmax function to convert the decoder output to predicted nexttoken probabilities In
our model we share the same weight matrix between the two embedding layers and the presoftmax
linear transformation similar to  30 In the embedding layers we multiply those weights bydmodel
5Table 1 Maximum path lengths perlayer complexity and minimum number of sequential operations
for different layer types nis the sequence length dis the representation dimension kis the kernel
size of convolutions and rthe size of the neighborhood in restricted selfattention
Layer Type Complexity per Layer Sequential Maximum Path Length
Operations
SelfAttention On2d O1 O1
Recurrent Ond2 On On
Convolutional Oknd2 O1 Ologkn
SelfAttention restricted Ornd O1 Onr
35 Positional Encoding
Since our model contains no recurrence and no convolution in order for the model to make use of the
order of the sequence we must inject some information about the relative or absolute position of the
tokens in the sequence To this end we add positional encodings to the input embeddings at the
bottoms of the encoder and decoder stacks The positional encodings have the same dimension dmodel
as the embeddings so that the two can be summed There are many choices of positional encodings
learned and fixed 9
In this work we use sine and cosine functions of different frequencies
PEpos2isinpos100002id model
PEpos2i1cospos100002id model
where posis the position and iis the dimension That is each dimension of the positional encoding
corresponds to a sinusoid The wavelengths form a geometric progression from 2πto10000 2π We
chose this function because we hypothesized it would allow the model to easily learn to attend by
relative positions since for any fixed offset kPEposkcan be represented as a linear function of
PEpos
We also experimented with using learned positional embeddings  9 instead and found that the two
versions produced nearly identical results see Table 3 row E We chose the sinusoidal version
because it may allow the model to extrapolate to sequence lengths longer than the ones encountered
during training
4 Why SelfAttention
In this section we compare various aspects of selfattention layers to the recurrent and convolu
tional layers commonly used for mapping one variablelength sequence of symbol representations
x1  x nto another sequence of equal length z1  z n with xi ziRd such as a hidden
layer in a typical sequence transduction encoder or decoder Motivating our use of selfattention we
consider three desiderata
One is the total computational complexity per layer Another is the amount of computation that can
be parallelized as measured by the minimum number of sequential operations required
The third is the path length between longrange dependencies in the network Learning longrange
dependencies is a key challenge in many sequence transduction tasks One key factor affecting the
ability to learn such dependencies is the length of the paths forward and backward signals have to
traverse in the network The shorter these paths between any combination of positions in the input
and output sequences the easier it is to learn longrange dependencies  12 Hence we also compare
the maximum path length between any two input and output positions in networks composed of the
different layer types
As noted in Table 1 a selfattention layer connects all positions with a constant number of sequentially
executed operations whereas a recurrent layer requires Onsequential operations In terms of
computational complexity selfattention layers are faster than recurrent layers when the sequence
6length nis smaller than the representation dimensionality d which is most often the case with
sentence representations used by stateoftheart models in machine translations such as wordpiece
38 and bytepair  31 representations To improve computational performance for tasks involving
very long sequences selfattention could be restricted to considering only a neighborhood of size rin
the input sequence centered around the respective output position This would increase the maximum
path length to Onr We plan to investigate this approach further in future work
A single convolutional layer with kernel width k  n does not connect all pairs of input and output
positions Doing so requires a stack of Onkconvolutional layers in the case of contiguous kernels
orOlogknin the case of dilated convolutions  18 increasing the length of the longest paths
between any two positions in the network Convolutional layers are generally more expensive than
recurrent layers by a factor of k Separable convolutions  6 however decrease the complexity
considerably to Okndnd2 Even with kn however the complexity of a separable
convolution is equal to the combination of a selfattention layer and a pointwise feedforward layer
the approach we take in our model
As side benefit selfattention could yield more interpretable models We inspect attention distributions
from our models and present and discuss examples in the appendix Not only do individual attention
heads clearly learn to perform different tasks many appear to exhibit behavior related to the syntactic
and semantic structure of the sentences
5 Training
This section describes the training regime for our models
51 Training Data and Batching
We trained on the standard WMT 2014 EnglishGerman dataset consisting of about 45 million
sentence pairs Sentences were encoded using bytepair encoding  3 which has a shared source
target vocabulary of about 37000 tokens For EnglishFrench we used the significantly larger WMT
2014 EnglishFrench dataset consisting of 36M sentences and split tokens into a 32000 wordpiece
vocabulary  38 Sentence pairs were batched together by approximate sequence length Each training
batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000
target tokens
52 Hardware and Schedule
We trained our models on one machine with 8 NVIDIA P100 GPUs For our base models using
the hyperparameters described throughout the paper each training step took about 04 seconds We
trained the base models for a total of 100000 steps or 12 hours For our big modelsdescribed on the
bottom line of table 3 step time was 10 seconds The big models were trained for 300000 steps
35 days
53 Optimizer
We used the Adam optimizer  20 with β1 09β2 098andϵ 109 We varied the learning
rate over the course of training according to the formula
lrate d05
modelminstep_num05 step _numwarmup _steps15 3
This corresponds to increasing the learning rate linearly for the first warmup _steps training steps
and decreasing it thereafter proportionally to the inverse square root of the step number We used
warmup _steps  4000 
54 Regularization
We employ three types of regularization during training
7Table 2 The Transformer achieves better BLEU scores than previous stateoftheart models on the
EnglishtoGerman and EnglishtoFrench newstest2014 tests at a fraction of the training cost
ModelBLEU Training Cost FLOPs
ENDE ENFR ENDE ENFR
ByteNet 18 2375
DeepAtt  PosUnk 39 392 101020
GNMT  RL 38 246 3992 231019141020
ConvS2S 9 2516 4046 961018151020
MoE 32 2603 4056 201019121020
DeepAtt  PosUnk Ensemble 39 404 801020
GNMT  RL Ensemble 38 2630 4116 181020111021
ConvS2S Ensemble 9 2636 4129 771019121021
Transformer base model 273 381 331018
Transformer big 284 418 231019
Residual Dropout We apply dropout  33 to the output of each sublayer before it is added to the
sublayer input and normalized In addition we apply dropout to the sums of the embeddings and the
positional encodings in both the encoder and decoder stacks For the base model we use a rate of
Pdrop 01
Label Smoothing During training we employed label smoothing of value ϵls 0136 This
hurts perplexity as the model learns to be more unsure but improves accuracy and BLEU score
6 Results
61 Machine Translation
On the WMT 2014 EnglishtoGerman translation task the big transformer model Transformer big
in Table 2 outperforms the best previously reported models including ensembles by more than 20
BLEU establishing a new stateoftheart BLEU score of 284 The configuration of this model is
listed in the bottom line of Table 3 Training took 35days on 8P100 GPUs Even our base model
surpasses all previously published models and ensembles at a fraction of the training cost of any of
the competitive models
On the WMT 2014 EnglishtoFrench translation task our big model achieves a BLEU score of 410
outperforming all of the previously published single models at less than 14the training cost of the
previous stateoftheart model The Transformer big model trained for EnglishtoFrench used
dropout rate Pdrop 01 instead of 03
For the base models we used a single model obtained by averaging the last 5 checkpoints which
were written at 10minute intervals For the big models we averaged the last 20 checkpoints We
used beam search with a beam size of 4and length penalty α 0638 These hyperparameters
were chosen after experimentation on the development set We set the maximum output length during
inference to input length  50 but terminate early when possible 38
Table 2 summarizes our results and compares our translation quality and training costs to other model
architectures from the literature We estimate the number of floating point operations used to train a
model by multiplying the training time the number of GPUs used and an estimate of the sustained
singleprecision floatingpoint capacity of each GPU5
62 Model Variations
To evaluate the importance of different components of the Transformer we varied our base model
in different ways measuring the change in performance on EnglishtoGerman translation on the
5We used values of 28 37 60 and 95 TFLOPS for K80 K40 M40 and P100 respectively
8Table 3 Variations on the Transformer architecture Unlisted values are identical to those of the base
model All metrics are on the EnglishtoGerman translation development set newstest2013 Listed
perplexities are perwordpiece according to our bytepair encoding and should not be compared to
perword perplexities
N d model dff h d k dvPdrop ϵlstrain PPL BLEU params
steps dev dev 106
base 6 512 2048 8 64 64 01 01 100K 492 258 65
A1 512 512 529 249
4 128 128 500 255
16 32 32 491 258
32 16 16 501 254
B16 516 251 58
32 501 254 60
C2 611 237 36
4 519 253 50
8 488 255 80
256 32 32 575 245 28
1024 128 128 466 260 168
1024 512 254 53
4096 475 262 90
D00 577 246
02 495 255
00 467 253
02 547 257
E positional embedding instead of sinusoids 492 257
big 6 1024 4096 16 03 300K 433 264 213
development set newstest2013 We used beam search as described in the previous section but no
checkpoint averaging We present these results in Table 3
In Table 3 rows A we vary the number of attention heads and the attention key and value dimensions
keeping the amount of computation constant as described in Section 322 While singlehead
attention is 09 BLEU worse than the best setting quality also drops off with too many heads
In Table 3 rows B we observe that reducing the attention key size dkhurts model quality This
suggests that determining compatibility is not easy and that a more sophisticated compatibility
function than dot product may be beneficial We further observe in rows C and D that as expected
bigger models are better and dropout is very helpful in avoiding overfitting In row E we replace our
sinusoidal positional encoding with learned positional embeddings  9 and observe nearly identical
results to the base model
63 English Constituency Parsing
To evaluate if the Transformer can generalize to other tasks we performed experiments on English
constituency parsing This task presents specific challenges the output is subject to strong structural
constraints and is significantly longer than the input Furthermore RNN sequencetosequence
models have not been able to attain stateoftheart results in smalldata regimes 37
We trained a 4layer transformer with dmodel  1024 on the Wall Street Journal WSJ portion of the
Penn Treebank  25 about 40K training sentences We also trained it in a semisupervised setting
using the larger highconfidence and BerkleyParser corpora from with approximately 17M sentences
37 We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens
for the semisupervised setting
We performed only a small number of experiments to select the dropout both attention and residual
section 54 learning rates and beam size on the Section 22 development set all other parameters
remained unchanged from the EnglishtoGerman base translation model During inference we
9Table 4 The Transformer generalizes well to English constituency parsing Results are on Section 23
of WSJ
Parser Training WSJ 23 F1
Vinyals  Kaiser el al 2014 37 WSJ only discriminative 883
Petrov et al 2006 29 WSJ only discriminative 904
Zhu et al 2013 40 WSJ only discriminative 904
Dyer et al 2016 8 WSJ only discriminative 917
Transformer 4 layers WSJ only discriminative 913
Zhu et al 2013 40 semisupervised 913
Huang  Harper 2009 14 semisupervised 913
McClosky et al 2006 26 semisupervised 921
Vinyals  Kaiser el al 2014 37 semisupervised 921
Transformer 4 layers semisupervised 927
Luong et al 2015 23 multitask 930
Dyer et al 2016 8 generative 933
increased the maximum output length to input length  300 We used a beam size of 21andα 03
for both WSJ only and the semisupervised setting
Our results in Table 4 show that despite the lack of taskspecific tuning our model performs sur
prisingly well yielding better results than all previously reported models with the exception of the
Recurrent Neural Network Grammar 8
In contrast to RNN sequencetosequence models  37 the Transformer outperforms the Berkeley
Parser 29 even when training only on the WSJ training set of 40K sentences
7 Conclusion
In this work we presented the Transformer the first sequence transduction model based entirely on
attention replacing the recurrent layers most commonly used in encoderdecoder architectures with
multiheaded selfattention
For translation tasks the Transformer can be trained significantly faster than architectures based
on recurrent or convolutional layers On both WMT 2014 EnglishtoGerman and WMT 2014
EnglishtoFrench translation tasks we achieve a new state of the art In the former task our best
model outperforms even all previously reported ensembles
We are excited about the future of attentionbased models and plan to apply them to other tasks We
plan to extend the Transformer to problems involving input and output modalities other than text and
to investigate local restricted attention mechanisms to efficiently handle large inputs and outputs
such as images audio and video Making generation less sequential is another research goals of ours
The code we used to train and evaluate our models is available at httpsgithubcom
tensorflowtensor2tensor 
Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful
comments corrections and inspiration
References
1Jimmy Lei Ba Jamie Ryan Kiros and Geoffrey E Hinton Layer normalization arXiv preprint
arXiv160706450  2016
2Dzmitry Bahdanau Kyunghyun Cho and Yoshua Bengio Neural machine translation by jointly
learning to align and translate CoRR  abs14090473 2014
3Denny Britz Anna Goldie MinhThang Luong and Quoc V  Le Massive exploration of neural
machine translation architectures CoRR  abs170303906 2017
4Jianpeng Cheng Li Dong and Mirella Lapata Long shortterm memorynetworks for machine
reading arXiv preprint arXiv160106733  2016
105Kyunghyun Cho Bart van Merrienboer Caglar Gulcehre Fethi Bougares Holger Schwenk
and Yoshua Bengio Learning phrase representations using rnn encoderdecoder for statistical
machine translation CoRR  abs14061078 2014
6Francois Chollet Xception Deep learning with depthwise separable convolutions arXiv
preprint arXiv161002357  2016
7Junyoung Chung Çaglar Gülçehre Kyunghyun Cho and Yoshua Bengio Empirical evaluation
of gated recurrent neural networks on sequence modeling CoRR  abs14123555 2014
8Chris Dyer Adhiguna Kuncoro Miguel Ballesteros and Noah A Smith Recurrent neural
network grammars In Proc of NAACL  2016
9Jonas Gehring Michael Auli David Grangier Denis Yarats and Yann N Dauphin Convolu
tional sequence to sequence learning arXiv preprint arXiv170503122v2  2017
10 Alex Graves Generating sequences with recurrent neural networks arXiv preprint
arXiv13080850  2013
11 Kaiming He Xiangyu Zhang Shaoqing Ren and Jian Sun Deep residual learning for im
age recognition In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 770778 2016
12 Sepp Hochreiter Yoshua Bengio Paolo Frasconi and Jürgen Schmidhuber Gradient flow in
recurrent nets the difficulty of learning longterm dependencies 2001
13 Sepp Hochreiter and Jürgen Schmidhuber Long shortterm memory Neural computation 
9817351780 1997
14 Zhongqiang Huang and Mary Harper Selftraining PCFG grammars with latent annotations
across languages In Proceedings of the 2009 Conference on Empirical Methods in Natural
Language Processing  pages 832841 ACL August 2009
15 Rafal Jozefowicz Oriol Vinyals Mike Schuster Noam Shazeer and Yonghui Wu Exploring
the limits of language modeling arXiv preprint arXiv160202410  2016
16 Łukasz Kaiser and Samy Bengio Can active memory replace attention In Advances in Neural
Information Processing Systems NIPS  2016
17 Łukasz Kaiser and Ilya Sutskever Neural GPUs learn algorithms In International Conference
on Learning Representations ICLR  2016
18 Nal Kalchbrenner Lasse Espeholt Karen Simonyan Aaron van den Oord Alex Graves and Ko
ray Kavukcuoglu Neural machine translation in linear time arXiv preprint arXiv161010099v2 
2017
19 Yoon Kim Carl Denton Luong Hoang and Alexander M Rush Structured attention networks
InInternational Conference on Learning Representations  2017
20 Diederik Kingma and Jimmy Ba Adam A method for stochastic optimization In ICLR  2015
21 Oleksii Kuchaiev and Boris Ginsburg Factorization tricks for LSTM networks arXiv preprint
arXiv170310722  2017
22 Zhouhan Lin Minwei Feng Cicero Nogueira dos Santos Mo Yu Bing Xiang Bowen
Zhou and Yoshua Bengio A structured selfattentive sentence embedding arXiv preprint
arXiv170303130  2017
23 MinhThang Luong Quoc V  Le Ilya Sutskever Oriol Vinyals and Lukasz Kaiser Multitask
sequence to sequence learning arXiv preprint arXiv151106114  2015
24 MinhThang Luong Hieu Pham and Christopher D Manning Effective approaches to attention
based neural machine translation arXiv preprint arXiv150804025  2015
1125 Mitchell P Marcus Mary Ann Marcinkiewicz and Beatrice Santorini Building a large annotated
corpus of english The penn treebank Computational linguistics  192313330 1993
26 David McClosky Eugene Charniak and Mark Johnson Effective selftraining for parsing In
Proceedings of the Human Language Technology Conference of the NAACL Main Conference 
pages 152159 ACL June 2006
27 Ankur Parikh Oscar Täckström Dipanjan Das and Jakob Uszkoreit A decomposable attention
model In Empirical Methods in Natural Language Processing  2016
28 Romain Paulus Caiming Xiong and Richard Socher A deep reinforced model for abstractive
summarization arXiv preprint arXiv170504304  2017
29 Slav Petrov Leon Barrett Romain Thibaux and Dan Klein Learning accurate compact
and interpretable tree annotation In Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting of the ACL  pages 433440 ACL July
2006
30 Ofir Press and Lior Wolf Using the output embedding to improve language models arXiv
preprint arXiv160805859  2016
31 Rico Sennrich Barry Haddow and Alexandra Birch Neural machine translation of rare words
with subword units arXiv preprint arXiv150807909  2015
32 Noam Shazeer Azalia Mirhoseini Krzysztof Maziarz Andy Davis Quoc Le Geoffrey Hinton
and Jeff Dean Outrageously large neural networks The sparselygated mixtureofexperts
layer arXiv preprint arXiv170106538  2017
33 Nitish Srivastava Geoffrey E Hinton Alex Krizhevsky Ilya Sutskever and Ruslan Salakhutdi
nov Dropout a simple way to prevent neural networks from overfitting Journal of Machine
Learning Research  15119291958 2014
34 Sainbayar Sukhbaatar Arthur Szlam Jason Weston and Rob Fergus Endtoend memory
networks In C Cortes N D Lawrence D D Lee M Sugiyama and R Garnett editors
Advances in Neural Information Processing Systems 28  pages 24402448 Curran Associates
Inc 2015
35 Ilya Sutskever Oriol Vinyals and Quoc VV Le Sequence to sequence learning with neural
networks In Advances in Neural Information Processing Systems  pages 31043112 2014
36 Christian Szegedy Vincent Vanhoucke Sergey Ioffe Jonathon Shlens and Zbigniew Wojna
Rethinking the inception architecture for computer vision CoRR  abs151200567 2015
37 Vinyals  Kaiser Koo Petrov Sutskever and Hinton Grammar as a foreign language In
Advances in Neural Information Processing Systems  2015
38 Yonghui Wu Mike Schuster Zhifeng Chen Quoc V Le Mohammad Norouzi Wolfgang
Macherey Maxim Krikun Yuan Cao Qin Gao Klaus Macherey et al Googles neural machine
translation system Bridging the gap between human and machine translation arXiv preprint
arXiv160908144  2016
39 Jie Zhou Ying Cao Xuguang Wang Peng Li and Wei Xu Deep recurrent models with
fastforward connections for neural machine translation CoRR  abs160604199 2016
40 Muhua Zhu Yue Zhang Wenliang Chen Min Zhang and Jingbo Zhu Fast and accurate
shiftreduce constituent parsing In Proceedings of the 51st Annual Meeting of the ACL Volume
1 Long Papers  pages 434443 ACL August 2013
12Attention Visualizations
InputInput Layer5
It
is
in
this
spirit
that
a
majority
of
American
governments
have
passed
new
laws
since
2009
making
the
registration
or
voting
process
more
difficult

EOS
pad
pad
pad
pad
pad
pad
It
is
in
this
spirit
that
a
majority
of
American
governments
have
passed
new
laws
since
2009
making
the
registration
or
voting
process
more
difficult

EOS
pad
pad
pad
pad
pad
pad
Figure 3 An example of the attention mechanism following longdistance dependencies in the
encoder selfattention in layer 5 of 6 Many of the attention heads attend to a distant dependency of
the verb making completing the phrase makingmore difficult Attentions here shown only for
the word making Different colors represent different heads Best viewed in color
13InputInput Layer5
The
Law
will
never
be
perfect

but
its
application
should
be
just

this
is
what
we
are
missing

in
my
opinion

EOS
pad
The
Law
will
never
be
perfect

but
its
application
should
be
just

this
is
what
we
are
missing

in
my
opinion

EOS
pad
InputInput Layer5
The
Law
will
never
be
perfect

but
its
application
should
be
just

this
is
what
we
are
missing

in
my
opinion

EOS
pad
The
Law
will
never
be
perfect

but
its
application
should
be
just

this
is
what
we
are
missing

in
my
opinion

EOS
padFigure 4 Two attention heads also in layer 5 of 6 apparently involved in anaphora resolution Top
Full attentions for head 5 Bottom Isolated attentions from just the word its for attention heads 5
and 6 Note that the attentions are very sharp for this word
14InputInput Layer5
The
Law
will
never
be
perfect

but
its
application
should
be
just

this
is
what
we
are
missing

in
my
opinion

EOS
pad
The
Law
will
never
be
perfect

but
its
application
should
be
just

this
is
what
we
are
missing

in
my
opinion

EOS
pad
InputInput Layer5
The
Law
will
never
be
perfect

but
its
application
should
be
just

this
is
what
we
are
missing

in
my
opinion

EOS
pad
The
Law
will
never
be
perfect

but
its
application
should
be
just

this
is
what
we
are
missing

in
my
opinion

EOS
padFigure 5 Many of the attention heads exhibit behaviour that seems related to the structure of the
sentence We give two such examples above from two different heads from the encoder selfattention
at layer 5 of 6 The heads clearly learned to perform different tasks
15
  Sigmoid Loss for Language Image PreTraining
Xiaohua ZhaiBasil Mustafa Alexander Kolesnikov Lucas Beyer
Google DeepMind Z urich Switzerland
fxzhai basilm akolesnikov lbeyer ggooglecom
Abstract
We propose a simple pairwise Sigmoid loss for
LanguageImage Pretraining SigLIP Unlike standard
contrastive learning with softmax normalization the sig
moid loss operates solely on imagetext pairs and does not
require a global view of the pairwise similarities for nor
malization The sigmoid loss simultaneously allows fur
ther scaling up the batch size while also performing bet
ter at smaller batch sizes Combined with Lockedimage
Tuning with only four TPUv4 chips we train a SigLiT
model that achieves 845 ImageNet zeroshot accuracy
in two days The disentanglement of the batch size from
the loss further allows us to study the impact of exam
ples vs pairs and negative to positive ratio Finally we
push the batch size to the extreme up to one million and
ﬁnd that the beneﬁts of growing batch size quickly dimin
ish with a more reasonable batch size of 32 k being suf
ﬁcient We release our models at httpsgithub
comgoogleresearchbig_vision and hope our
research motivates further explorations in improving the
quality and efﬁciency of languageimage pretraining
1 Introduction
Contrastive pretraining using weak supervision from
imagetext pairs found on the web is becoming the goto
method for obtaining generic computer vision backbones
slowly replacing pretraining on large labelled multiclass
datasets The highlevel idea is to simultaneously learn
an aligned representation space for images and texts using
paired data Seminal works CLIP 36 and ALIGN 23 es
tablished the viability of this approach at a large scale and
following their success many large imagetext datasets be
came available privately 59 13 21 49 and publicly 40
6 15 7 41
The standard recipe to pretrain such models leverages
the imagetext contrastive objective It aligns the image and
equal contributionTable 1 SigLiT and SigLIP results  Sigmoid loss is mem
ory efﬁcient allows larger batch sizes BS that unlocks
language image pretraining with a small number of chips
SigLiT model with a frozen public
 B8 checkpoint 42
trained on the LiT imagetext dataset 59 using four TPU
v4 chips for one day achieves 797 0shot accuracy on
ImageNet The same setup with a g14 checkpoint 58
leads to 845 accuracy trained for two days With a pub
lic unlocked
 B16 image checkpoint 42 trained on the
WebLI dataset 13 SigLIP achieves 710 0shot accu
racy using 16 TPUv4 chips for three days The last two
rows show results with randomly initialized models
Image Text BS TPUv4 Days INet0
SigLiT
 B8 L32 k 4 1 798
SigLiT
 g14 L 20 k 4 2 845
SigLIP
 B16 B 16 k 16 3 710
SigLIP B16 B 32 k 32 2 721
SigLIP B16 B 32 k 32 5 734
We use a variant of the L model with 12 layers
text embeddings for matching positive imagetext pairs
while making sure that unrelated negative imagetext pairs
are dissimilar in the embedding space This is achieved via a
batchlevel softmaxbased contrastive loss applied twice to
normalize the pairwise similarity scores across all images
then all texts A naive implementation of the softmax is
numerically unstable it is usually stabilized by subtracting
the maximum input value before applying the softmax 18
which requires another pass over the full batch
In this paper we propose a simpler alternative the sig
moid loss It does not require any operation across the full
batch and hence greatly simpliﬁes the distributed loss im
plementation and boosts efﬁciency Additionally it con
ceptually decouples the batch size from the deﬁnition of
the task We compare the proposed sigmoid loss with the
standard softmax loss across multiple setups In partic
ular we investigate sigmoidbased loss with two promi
1nent approaches for imagetext learning CLIP 36 and
LiT 59 which we call sigmoid language image pre
training  SigLIP  and sigmoid LiT  SigLiT  respectively
We ﬁnd that the sigmoid loss performs signiﬁcantly better
than the softmax loss when the batch size is smaller than
16 k As the train batch size grows the gap closes Impor
tantly the sigmoid loss is symmetric requires just a single
pass and a typical implementation requires less memory
than the softmax loss This enables successful training of a
SigLiT model at a batch size of one million  However we
ﬁnd that the performance saturates with growing batch size
both for softmax and sigmoid The good news is that a rea
sonable batch size ie 32 k is sufﬁcient for imagetext pre
training This conclusion also holds for multilingual SigLIP
training on over 100 languages
In Table 1 we present setups for imagetext pretraining
that require a moderate amount of TPUv4 chips for training
SigLiT is surprisingly efﬁcient reaching 797 zeroshot
accuracy on ImageNet in just a single day on four chips
SigLIPs more demanding fromscratch training reaches
734 zeroshot accuracy in 5 days with 32 TPUv4 chips
This compares favorably to prior works such as FLIP 30
and CLIP 36 which require approximately 5 and 10 days
respectively on 256 TPUv3 cores When ﬁnetuning a pre
trained vision backbone in SigLIP denoted as
 in Table 1
we found that disabling the weight decay on the pretrained
backbone leads to better results see Figure 4 for details
We hope our work paves the way for making the nascent
languageimage pretraining ﬁeld more accessible
2 Related Work
Contrastive learning with the sigmoid loss One prior
work proposes a similar sigmoid loss for the task of unsu
pervised dimensionality reduction 19 in the scope of con
trastive imagetext learning the vast majority of works rely
on the softmaxbased InfoNCE loss as popularized by 46
In supervised classiﬁcation the sigmoid loss has already
been shown to be slightly more effective and robust than
the softmax loss 3 51
Contrastive languageimage pretraining has become
popular since CLIP 36 and ALIGN 23 applied softmax
contrastive learning 60 46 10 24 to largescale image
text datasets Both models perform very well on zeroshot
transfer tasks including classiﬁcation and retrieval Follow
up works show that contrastively pretrained models pro
duce good representations for ﬁnetuning 53 16 linear
regression 23 object detection 31 semantic segmenta
tion 33 and video tasks 57
Generative languageimage pretraining Besides soft
max contrastive pretraining various alternatives have been
proposed GIT 49 SimVLM 50 and LEMON 21 suc
cessfully pretrain models using a generative text decoderAlgorithm 1 Sigmoid loss pseudoimplementation
1 img_emb  image model embedding n dim
2 txt_emb  text model embedding n dim
3 t_prime b  learnable temperature and bias
4 n  minibatch size
5
6t  expt_prime
7zimg  l2_normalizeimg_emb
8ztxt  l2_normalizetxt_emb
9logits  dotzimg ztxtT t  b
10labels  2 eyen  onesn  1 with diagonal 1
11l  sumlog_sigmoidlabels logits  n
instead while CoCa 56 adds such a decoder to the dis
criminative CLIPALIGN setup thus combining the pros
and cons of both approaches into a single very capable
model BLIP 28 further proposes CapFilt which uses the
generative decoder to create better captions and the discrim
inative part of the model to ﬁlter pairs LanguageImage
pretraining is a very active ﬁeld and surveys 8 rapidly be
come outdated
Efﬁcient languageimage pretraining On the other hand
few works have tried making language image pretraining
more efﬁcient LiT 59 and FLIP 30 are notable attempts
the former requires a pretrained and locked backbone and
the latter sacriﬁces quality by randomly dropping visual to
kens BASIC 35 and LAION 52 look at scaling batch
size but only go up to 16 k and 160 k respectively by using
many hundreds of chips and for the former also mixing in a
large private classiﬁcation dataset 35 55 The recent Lion
optimizer 12 claims to be able to reduce the training cost
to reach similar quality
3 Method
In this section we ﬁrst review the widelyused softmax
based contrastive loss We then introduce the pairwise sig
moid loss and discuss its efﬁcient implementation
Given a minibatch BfI1T1I2T2gof
imagetext pairs the contrastive learning objective encour
ages embeddings of matching pairs IiTito align with
each other while pushing embeddings of unmatched pairs
IiTj6iapart For practical purposes it is assumed that
for all images i the text associated with a different image j
is not related to i and viceversa This assumption is usu
ally noisy and imperfect
31 Softmax loss for language image pretraining
When using the softmax loss to formalize this objective
an image model fand a text model gare trained to
2Device 1 Device 2 Device 3
I₁I₂I₃I₄I₅I₆I₇I₈I₉I₁₀I₁₁I₁₂Device 1T₁
T₂
T₃
T₄Device 2T₅
T₆
T₇
T₈Device 3T₉
T₁₀
T₁₁
T₁₂a Initially each device holds 4
image and 4 text representations
Each device needs to see the rep
resentations from other devices
to calculate the full loss
Device 1 Device 2 Device 3
I₁I₂I₃I₄I₅I₆I₇I₈I₉I₁₀I₁₁I₁₂Device 1T₁  
T₂  
T₃  
T₄ Device 2T₅  
T₆  
T₇  
T₈ Device 3T₉  
T₁₀  
T₁₁  
T₁₂ 

loss33 33 33 33 33 33 33 33 33 33 33 33
Device 1 Device 2 Device 3b They each compute the com
ponent of the loss highlighted
for their representations which
includes the positives
Device 1 Device 2 Device 3
I₁I₂I₃I₄I₅I₆I₇I₈I₉I₁₀I₁₁I₁₂Device 3T₁     
T₂     
T₃     
T₄     Device 1T₅     
T₆     
T₇     
T₈     Device 2T₉     
T₁₀     
T₁₁     
T₁₂     

loss66 66 66 66 66 66 66 66 66 66 66 66
Device 1 Device 2 Device 3c Texts are swapped across the
devices so device 1 now has I14
andT58etc The new loss is
computed and accumulated with
the previous
Device 1 Device 2 Device 3
I₁I₂I₃I₄I₅I₆I₇I₈I₉I₁₀I₁₁I₁₂Device 2T₁         
T₂         
T₃         
T₄         Device 3T₅         
T₆         
T₇         
T₈         Device 1T₉         
T₁₀         
T₁₁         
T₁₂         

loss           
Device 1 Device 2 Device 3
  
Cross Device Σd This repeats till every image
 text pair have interacted eg
device 1 has the loss of I14and
T112 A ﬁnal crossdevice sum
brings everything together
Figure 1 Efﬁcient loss implementation demonstrated via a mock setup with 3 devices and a global batch size of 12 There
are no allgathers and at any point in time only the bright yellow square size 44 is materialized in memory
minimize the following objective
1
2jBjjBjX
i10
BBBimagetext softmaxz 
logetxiyi
PjBj
j1etxiyjtextimage softmaxz 
logetxiyi
PjBj
j1etxjyi1
CCCA
where xifIi
kfIik2andyigTi
kgTik2 In this paper we
adopt the vision transformer architecture 17 for images
and the transformer architecture 47 for texts Note that
due to the asymmetry of the softmax loss the normalization
is independently performed two times across images and
across texts 36 The scalar tis parametrized as expt0
wheret0is a global freely learnable parameter
32 Sigmoid loss for language image pretraining
Instead of the softmaxbased contrastive loss we pro
pose a simpler alternative that does not require computing
global normalization factors The sigmoidbased loss pro
cesses every imagetext pair independently effectively turn
ing the learning problem into the standard binary classiﬁca
tion on the dataset of all pair combinations with a positive
labels for the matching pairs IiTiand negative labels for
all other pairs IiTj6i It is deﬁned as follows
1
jBjjBjX
i1jBjX
j1log1
1 ezijtxiyjb
 z 
Lij
wherezijis the label for a given image and text input which
equals 1 if they are paired and 1otherwise At initialization the heavy imbalance coming from the many nega
tives dominates the loss leading to large initial optimization
steps attempting to correct this bias To alleviate this we
introduce an additional learnable bias term bsimilar to the
temperature t We initialize t0andbtolog 10 and10re
spectively This makes sure the training starts roughly close
to the prior and does not require massive overcorrection
Algorithm 1 presents a pseudocode implementation of the
proposed sigmoid loss for language image pretraining
33 Efﬁcient chunked implementation
Contrastive training typically utilizes data parallelism
Computing the loss when data is split across Ddevices
necessitates gathering all embeddings 59 with expensive
allgathers and more importantly the materialization of a
memoryintensivejBjjBj matrix of pairwise similarities
The sigmoid loss however is particularly amenable to
a memory efﬁcient fast and numerically stable implemen
tation that ameliorates both these issues Denoting the per
device batch size as bjBj
D the loss is reformulated as
1
jBjDX
di1z
A8device diBswap negs
across devicesz
DX
dj1Cper device
lossz  
bdi1X
ibdiz
all local
positivesbdj1X
jbdjz
negs from
next deviceLij
This is particularly simple for the sigmoid loss as each pair
is an independent term in the loss Figure 1 illustrates this
32 8 32 262 1024
Batch Size k8182838485ImageNet 0shot
SigLiT
4 8 16 32 98 307
Batch Size k6668707274
SigLIP
Sigmoid
Softmax
16 32 65 131 245
Batch Size k30313233343536XM TI 36 lang avg
mSigLIPFigure 2 The effect of pretraining batch size Left SigLiT results  trained for 18B seen examples Sigmoid loss outper
forms the softmax loss signiﬁcantly with small batch sizes and performs similarly at larger batch sizes We successfully
trained an SigLiT model with up to one million batch size However performance for both sigmoid and softmax saturate at
around 32 k batch size Middle SigLIP results  trained for 9B seen examples Both sigmoid loss and softmax loss saturate
at a reasonable batch size while the peak of the sigmoid loss comes earlier and slightly outperforms the peak of the softmax
loss A very large batch size hurts both losses Right mSigLIP results  trained for 30B seen examples With a multilingual
setup using over 100 languages 32 k batch size is surprisingly sufﬁcient and scaling beyond that hurts performance on a
36language crossmodal retrieval task
method In words we ﬁrst compute the component of the
loss corresponding to the positive pairs and b1nega
tive pairs We then permute representations across devices
so each device takes negatives from its neighbouring de
vice next iteration of sum B The loss is then calculated
with respect to this chunk sum C This is done indepen
dently in each device such that each device computes the
loss with respect to its local batch b Losses can then simply
be summed across all devices sum A Individual collec
tive permutes for sum B are fast and indeed Dcollective
permutes is typically faster than two allgathers between D
devices and the memory cost at any given moment is re
duced fromjBj2tob2for sum C Usuallybis constant as
scalingjBjis achieved by increasing the number of accel
erators Due to being quadratic with respect to the batch
size the vanilla loss computation rapidly bottlenecks scal
ing up This chunked approach enabled training with batch
sizes over 1 million on relatively few devices
4 Results
In this section we evaluate the proposed SigLiT and
SigLIP models across a wide range of batch sizes We dis
cuss what can be achieved with a small number of accel
erator chips using both SigLiT and SigLIP recipes We
also brieﬂy discuss the impact of batch size on multilin
gual language image pretraining We ablate the importance
of our largebatch stabilization modiﬁcation and the intro
duced learned bias term and present a study on the effect of
positive and negative pairs ratio in the sigmoid loss Lastlywe explore SigLIPs data noise robustness
To validate our models we report zeroshot transfer re
sults on the ImageNet dataset 14 and zeroshot retrieval
results across 36 languages on the XM3600 dataset 44
We use the ScalingViTAdafactor optimizer 58 by default
for all our experiments
41 SigLiT Scaling batch size to the limit
Following 59 we use the same precomputed embed
dings for the images using a ViTg vision model and train
a base size text tower from scratch with the same hyperpa
rameters using the LiT imagetext dataset 59
We perform a study over a wide range of batch sizes
from 512 to 1M demonstrating the impact of batch size
for contrastive learning Results are presented in Figure 2
left When the batch size is smaller than 16k sigmoid loss
outperforms softmax loss by a large margin With growing
batch sizes we observe that softmax loss quickly catches
up and potentially slightly underperforms sigmoid loss with
a large enough batch size Overall we recommend using
the SigLIP recipe for large batch sizes as well due to the
simplicity compute savings and straightforward memory
efﬁcient implementation
There is a consensus that contrastive learning beneﬁts
from large batch sizes while most of the existing studies
stop at 64 k batch size 59 35 10 We successfully trained
an SigLiT model at one million batch size to explore the
limit of contrastive learning To our surprise the perfor
mance saturates at 32 k batch size further scaling up the
batch size only gives a minor boost and the model peaks at
4450 900 3000 18000
Examples Seen M7879808182838485ImageNet 0shot
    8k
262kSigmoid
SoftmaxFigure 3 SigLiT ImageNet 0shot transfer results with
different training durations Large batch size results in a
big performance boost but needs a sufﬁciently long sched
ule to ramp up as for short schedules very large batch size
results in a small number of gradient update steps
256 k batch size Our best SigLiT with a Bsized text mode
achieves 847 zeroshot transfer accuracy on ImageNet
while the original LiT paper reports a slightly better 852
score with a 10 times larger gsized text model Figure 3
presents the impact of training duration for different batch
sizes It demonstrates that large 262kbatch size signiﬁ
cantly outperforms smaller 8kbatch size when trained for
a sufﬁciently long time Note that for short training dura
tions large batch size leads to the fewer absolute number of
update steps and thus needs more time to ramp up
42 SigLIP Sigmoid loss is beneﬁcial for language
image pretraining
We pretrain SigLIP models on the WebLI dataset 13
using only English image and text pairs We use CLIP We
bLI to denote the CLIP baseline pretrained on WebLI with
the standard softmax loss We use moderatelysized mod
els B16 ViT for image embeddings and Bsized trans
former for text embeddings The input images are resized to
224224 resolution The text is tokenized by a 32 k vocab
ulary sentencepiece tokenizer 27 trained on the English
C4 dataset 37 and a maximum of 16 text tokens are kept
Figure 2 middle plot shows SigLIP results With less than
32 k batch size SigLIP outperforms CLIP WebLI base
lines On the other end of the scale the memory efﬁciency
of the sigmoid loss enabled much larger batch sizes For ex
ample with four TPUv4 chips we could ﬁt a batch size of
4096 with a Base SigLIP but only 2048 with a correspond
ing CLIP model The two advantages together demonstrate
signiﬁcant beneﬁts of the sigmoid loss for language image
pretraining with ﬁxed resources which will be discussed
in Section 4516 k 32 k 64 k 128 k 240 k
INet0 716 732 732 732 731
XM avg 348 349 344 336 327
XM de 547 548 554 543 547
XM en 465 462 465 466 466
XM hi 91 85 79 81 73
XM ru 501 499 497 486 493
XM zh 307 325 320 306 237
Table 2 Multilingual SigLIP results with various batch
sizes pretrained for 30 billion seen examples We report
zeroshot transfer results on ImageNet INet0 and aver
aged text to image retrieval results across 36 languages on
the crossmodal 3600 dataset XM The full table on 36 lan
guages can be found in Appendix
As batch size increases the gap between the sigmoid and
the softmax losses diminish SigLIP performs best at batch
size 32 k whereas the softmax loss required 98 k for optimal
performance and still didnt outperform the sigmoid based
variant Scaling further a larger batch size like 307 k hurts
both losses
43 mSigLIP Multilingual pretraining
We further scale up the training data by keeping all the
100 languages from the WebLI dataset 13 With multi
lingual data one usually needs to use a larger international
vocabulary We ﬁrst verify the impact of two tokenizers a
small multilingual vocabulary with 32 k tokens 37 and a
large multilingual vocabulary with 250 k tokens 54 We
train Bsized ViT and text models for 900Mtotal exam
ples seen and observe slightly more than 1 improvement
when using a larger vocabulary
However the token embeddings become huge for very
large vocabulary sizes Following the standard setup we
would need to store a NWtoken embedding lookup table
to train the multilingual model where Nis the vocabulary
size mentioned above and Wis the embedding dimension
of the text model To save memory we propose to use a
bottlenecked token embedding We use NKembed
ding matrix and additional KWprojection where the
bottleneckKis much smaller than W
In our experiments we observed that using a large mul
tilingual vocabulary with a bottleneck can be scaled up as
efﬁciently as using a small multilingual vocabulary Specif
ically by enabling the bottleneck of size K 96 for Base
architecture with W 768  we only see about a half per
cent quality drop on ImageNet zeroshot transfer compared
to using the full 250kvocabulary
512 4 8 16 24010203040506070INet 0shot
12 4 8 16 24
Examples Seen 100M010203040506070INet 10shotfromscratch
finetune
finetune wo encwdFigure 4 Top SigLIP with pretrained encoders ramps up
quickly However only disabling weight decay on the pre
trained encoder weights leads to stable behavior and good
ImageNet 0shot transfer results Bottom  ImageNet 10
shot transfer results where decaying the pretrained weights
leads to deterioration of the pretrained model visual repre
sentation quality Disabling weight decay ﬂattens the curve
With the memory improvements we train mSigLIP
models for various batch sizes for a total of 30 billion ex
amples seen Table 2 and Figure 2 right plot show the
results We were expecting a large batch size to improve
multilingual pretraining where the model sees more ex
amples from the same language as hard negatives in a sin
gle minibatch However we didnt observe clear improve
ments with a batch size larger than 32 k A batch size of
32 k is sufﬁcient for a multilingual setup as well On the
XM3600 crossmodal retrieval tasks we found that going
beyond 32 k batch size leads to worse results on average
while on ImageNet zeroshot transfer it stays ﬂat mSigLIP
sets the new stateoftheart on XM3600 text to image re
trieval task with only a Base size model Our best result is
349 which is more than 6 higher than the previously
reported result 285 13 with a standard LiT model 59
using a much larger four billion ViTe model We further
scale up mSigLIP training in Section 46
44 SigLiT with four TPUv4 chips
For many practitioners the important question usually is
what can be trained with a limited amount of resources
We explore the usage of SigLiT models in this section with
only four TPUv4 chips as the memory efﬁcient sigmoid
loss is suitable for this application scenario
3456Loss Lβ20999
β2095
110wL
1B 2B 3B 4B 5B
Examples seen24ΔwFigure 5 The effect of Adam and AdaFactors 2As
we increase batchsize we observe more frequent training
instability This instability seen in the loss curves top is
caused by spikes in gradient norm middle leading to large
parameter updates bottom Decreasing the 2momentum
stabilizes training Occasional gradient spikes still happen
see step at 2B but do not destabilize the training process
We follow the same setup as in section 41 We use
the publicly available ViTAugRegB8 42 model as the
frozen 
  vision tower and precompute embeddings to ac
celerate the training 59 The text model is a Large Trans
former but with a depth of only 12 layers instead of 24
It is trained using the LION 12 optimizer with decoupled
weight decay 1107 linearly warmup of learning rate
over 65k steps up to a peak of 1104 followed by a co
sine decay to 0 We train for a total of 65 000 steps with a
batch size of 32k  this leads to just under one day of train
ing Table 1 shows the results when training a model on four
chips for one day achieving 797 0shot ImageNet classi
ﬁcation accuracy very competitive in this limited resource
regime With a ViTg14 58 model as the vision tower and
a Large text tower we can train at 20 k batch size on four
chips for 107 k steps in under two days This further pushes
the 0shot ImageNet classiﬁcation accuracy up to 845
45 SigLIP with a small amount of TPUv4 chips
Its resource demanding to train a CLIP model from
scratch in general with SigLIP its possible to ﬁt a larger
train batch size with fewer amount of chips In this section
we explore ways to train SigLIP models efﬁciently with pre
trained weights We use pretrained weights to initialize the
image model to accelerate the pretraining which was orig
61  16k 1  16k 1  164 1  16 1  16607080
ImageNet 0shot
Random
Hard
Hard matched pairs
Easy
1  16k 1  16k 1  164 1  16 1  16151050
Learned bias
1  16k 1  16k 1  164 1  16 1  16201510505
Average logit of pos and negFigure 6 The effect of batch composition We simulate various batch compositions by masking out negatives either
randomly keeping only the hardest or the easiest With no masking we have 16 k negatives for each positive in the batch
116 k and the strongest masking we apply 116 results in almost balanced minibatches In one setting we match total
pairs seen by training for signiﬁcantly longer We observe ImageNet 0shot score the ﬁnal value of the learned bias and the
average logits of positive and negative pairs Overall the imbalance does not seem to be detrimental but ﬁnding an efﬁcient
way of mining negatives might be beneﬁcial
inally discussed in 59 We use the public and unlocked
ViTAugRegB16 42 model to initialize our vision tower
and ﬁnetune on the same WebLI English data as used for
SigLIP In all the experiments we apply a 01 learning rate
multiplier to the pretrained image tower to make it suitable
for ﬁnetuning
Figure 4 presents unlocked
 ﬁnetuning results along
side fromscratch randomly initialized baselines We used
16 TPUv4 chips and train at 16 k batch size for 24 B ex
amples seen We found that the ﬁnetuning setup doesnt
perform well outofthebox this is consistent with prior
works 59 where ﬁnetuning image models degraded visual
representation quality This is evidenced by ImageNet 10
shot linear classiﬁcation where in Figure 4 the ﬁnetuned
setup is barely better than the fromscratch baseline
We hypothesize that the default weight decay applied to
the pretrained weights reduces their effectiveness Moti
vated by the ﬁnetuning recipe from 17 58 25 that uses
no weight decay we also propose disabling weight decay on
the pretrained weights for SigLIP training Weight decay
is therefore only applied to the randomly initialized weights
in the text model This simple modiﬁcation signiﬁcantly
improved SigLIP results Figure 4 shows that with our im
proved recipe SigLIP reaches 71 0shot accuracy on Im
ageNet using 16kbatch size trained on 16 chips for three
days We also present fromscratch results in the bottom
rows of Table 1 with 32 TPUv4 chips for only two days
SigLIP achieves 721 0shot accuracy This presents a
signiﬁcant training cost reduction eg compared to CLIP
approx 2500 TPUv3days for 726 reported in 3046 Scaling up SigLIP and mSigLIP
In this section we scale up SigLIP by overtraining the
model 45 1 We present results in Table 3 using ViTB
ViTL or So400m 1 as the vision encoder with a text en
coder of the same size B L and So400m respectively
Following the recipe described in Section 42 we train both
models for 40 billion examples seen at batch size 32 k but
use256162 256 image patches and 64 text tokens in
stead of 16 To get SigLIP models for different resolutions
we train for 5 billion more examples at the target resolution
with a 100x smaller learning rate and no weight decay In
Table 3 we report zeroshot classiﬁcation results on Im
ageNet 14 ObjectNet 2 ImageNetv2 39 ImageNet
ReaL 3 and zeroshot imagetotext I T retrieval text
toimage IT retrieval results on MSCOCO 11
We also scale up the multilingual mSigLIP ViTB model
in the same way We report imagetext retrieval results
across 36 languages on the XM3600 benchmark 44 The
scaledup mSigLIP ViTB model achieves the stateofthe
art426 image retrieval recall1 and 541 text retrieval
recall1 for a Base model This is slightly outperformed
by the Large model in 48 getting 4296 image retrieval
recall1 Detailed results are provided in Appendix Table 9
and Figure 8 denoted as 32 k
47 Stabilizing largebatch training
As we move to large batch sizes the language image pre
training using transformers becomes increasingly more un
stable even when using a modestlysized model eg Base
size The reason for these instabilities is large spikes in the
7MethodImage Encoder ImageNet1k COCO R1
ViT size  Patches Validation v2 ReaL ObjectNet I T TI
CLIP B 196 683 619  553 524 331
OpenCLIP B 196 702 623  560 594 423
EV ACLIP B 196 747 670  623 587 422
SigLIP B 196 762 696 828 707 644 472
SigLIP B 256 767 700 831 713 651 474
SigLIP B 576 786 721 845 738 675 497
SigLIP B 1024 792 730 849 747 676 504
CLIP L 256 755 690  699 563 365
OpenCLIP L 256 740 611  664 621 461
CLIPAv2 L 256 797 728  711 641 463
EV ACLIP L 256 798 729  753 637 475
SigLIP L 256 805 742 859 779 695 511
CLIP L 576 766 720  709 579 371
CLIPAv2 L 576 803 735  731 655 472
EV ACLIP L 576 804 738  784 641 479
SigLIP L 576 821 759 870 810 706 527
OpenCLIP G 2B 256 801 736  730 673 514
CLIPAv2 H 630M 576 818 756  774 672 492
EV ACLIP E 5B 256 820 757  796 688 511
SigLIP SO 400M 729 832 772 875 829 702 520
Table 3 Comparison with other publicly released models Our SigLIP models outperform all prior models eg Open
CLIP 22 and CLIP 36 by a signiﬁcant margin on both zeroshot classiﬁcation and retrieval tasks Compared to the concur
rent EV ACLIP 43 and CLIPAv2 29 our SigLIPL performs better across the board in both the low and high resolution
cases Especially noteworthy is the ShapeOptimized 400M parameter ViT 1 architecture which outperforms all signiﬁ
cantly larger models We publicly release our models httpsgithubcomgoogleresearchbig_vision 
gradient norms which translate to largemagnitude changes
in the weights that may destabilize the training process
see Figure 5 We observe that reducing 2in Adam and
AdaFactor from its default 0999 to 095 which was sug
gested in 20 9 is enough to stabilize the training Intu
itively this allows recovering from gradient spikes quicker
We opt for setting 2 095for all our experiments
48 Negative ratio in sigmoid loss
One question which arises when shifting the perspective
from the softmaxs pick the right class view to the sig
moids rate this pair view is the imbalance in positive
versus negative pairs For a batch size jBj the batch con
tainsjBjpositive pairs but jBj2jBj negative examples
In the modest batchsize of 16 k there are actually 268 M
negative examples for only 16 k positive ones At the same
time because the sigmoid loss decomposes into a sum of
perexample losses we can perform controlled experiments
to study the effect of the minibatch composition and distribution of examples visited We run experiments in the
SigLiT setup at batchsize 16 k for 900 M steps and vary
the composition of the batch by masking out  ie ignoring
enough negative examples to reach a target positive  neg
ative ratio masking in the following ways
Random Randomly choose negative pairs to mask
Hard Keep hardest negative pairs highest loss
Easy Keep easiest negatives pairs lowest loss
Hard  matching total pairs seen Masking exam
ples while training for a ﬁxed number of steps does
decrease the total number of pairs seen during train
ing Hence in the matched pairs setting we increase
the number of training steps by the masking ratio in
order to keep the number of pairs seen constant
Figure 6 shows the effect of the various masking strate
gies Randomly removing negatives to rebalance does dete
riorate performance Keeping the easiest examples does not
work at all while keeping the hardest negatives does almost
800 02 04050052054056ImageNet 0shot
Image
00 02 04
Text
Sigmoid
Softmax
00 02 04
pcorruption
Batch
00 01 02
Image  Text
00 01 02
Image Text  BatchFigure 7 Sigmoidtraining increases robustness to data noise Titles show the type of corruption applied and xaxes show
the probability with which they are applied With increasing corruption severity Mscale models trained with sigmoid loss
for 36 billion examples retain superiority over corresponding softmax baseline
maintain the quality indicating that as could be expected
a lot of the learning on the negative side comes from the
harder examples This is further conﬁrmed by the slightly
increased performance of training longer on the hardest ex
amples in order to match the total pairs seen
We also look at the value of the learned bias at the end of
training as well as the average logit value for positive and
negative examples across these settings and ﬁnd the result
mostly follows what one would expect as fewer negatives
are present the bias and logits become more positive over
all Interestingly when training with more hard negative
pairs the average logits of positive pairs stays mostly ﬂat
This study conﬁrms that 1 the imbalance does not seem
to be a major reason for concern while at the same time 2
coming up with an efﬁcient way of including more negative
examples can be promising but is not trivial
49 Bias term in sigmoid loss
We ablate the bias term in the loss function using the
Base architecture with an 8 k batch size trained for 900M
examples with the SigLIP setup Zeroshot transfer results
are reported on ImageNet 14 Oxfordiiit pet 34 and Ci
far100 26 Table 4 presents results with and without a bias
term in the sigmoid loss
Table 4 Bias b and temperature t0 initialization Re
sults are reported using Base architecture 8 k batch size
trained for 900M examples Enabling the bias term b with
10initialization improves results consistently
b t0INet0 Pet0 C1000
na log 10 620 818 599
10 log 10 630 824 610
10 log 1 610 800 604
0 log 10 617 799 590
0 log 1 537 732 538Enabling the bias term with a 10initialization consis
tently improves performance across all tasks This is be
cause the bias term ensures that the training starts close to
the prior preventing dramatic overcorrection in early op
timization In contrast a randomly chosen bias term ini
tialization such as the 0 initialization in Table 4 fails to
address the overcorrection issue leading to signiﬁcantly
worse results This effect is particularly noticeable when
using a small temperature t0initialization We set the bias
and temperature initialization to b10andt0 log 10
hencet 10  as the default for all experiments
410 Label noise robustness
Prior works demonstrated improved robustness against
label noise when using the sigmoid loss for classiﬁcation
models 3 This property would be particularly useful here
in the face of the famously noisy nature of popular large
scale imagetext datasets In order to study this for SigLIP
we train M16 image models alongside an M text model at
batch size 16384 for 36 billion seen examples We corrupt
the training data using one of the following methods
Image  With probability p replace the image with uni
form random noise
Text With probability p replace tokenized text with a
new sequence of randomly sampled tokens up to some
sampled sequence length
Batch alignment  Randomly shufﬂe the ordering of
p of the batch
Image  text  Apply both with probability peach
Image text  batch  Alongside 4 also shufﬂe frac
tionpof alignments
Results from varying the likelihood of the corruption are
shown in Figure 7 Models trained with sigmoid loss are
increasingly robust to all kinds of added noise
95 Conclusion
We conducted a study on two languageimage pre
training instances that used the sigmoid loss SigLiT and
SigLIP Our results demonstrate that the sigmoid loss per
forms better than the softmax baseline particularly for
small train batch sizes This loss function is also more mem
ory efﬁcient which allows larger train batch sizes without
requiring additional resources We performed a thorough
investigation of the batch size in contrastive learning Sur
prisingly we found that a relatively modest batch size of
32 k yielded nearly optimal performance Further studies
have been performed to understand better the introduced
bias term in the sigmoid loss robustness to data noises and
the impact of positive and negative pairs ratio in the sigmoid
loss We hope this work will facilitate languageimage pre
training research with limited resources
Acknowledgements We thank Daniel Keysers Ilya Tol
stikhin Olivier Bousquet and Michael Tschannen for their
valuable feedback and discussions on this paper We thank
Joan Puigcerver Josip Djolonga and Black Hechtman for
discussions on efﬁcient implementations of the chunked
contrastive loss We thank Kaiming He and Xinlei Chen for
the discussion of 2to stabilize the training We also thank
Ross Wightman for spotting a mistake in the pseudocode in
the ﬁrst version of this paper Boris Dayma and Krzysztof
Maziarz for spotting typos in the second and third versions
which made tvst0confusing We thank the Google Deep
mind team for providing a supportive research environment
We use the bigvision codebase 5 4 for all experi
ments in this project
10References
1 Ibrahim Alabdulmohsin Xiaohua Zhai Alexander
Kolesnikov and Lucas Beyer Getting vit in shape
Scaling laws for computeoptimal model design In
NeurIPS  2023 7 8 17
2 Andrei Barbu David Mayo Julian Alverio William Luo
Christopher Wang Dan Gutfreund Josh Tenenbaum and
Boris Katz ObjectNet A largescale biascontrolled dataset
for pushing the limits of object recognition models In
NeurIPS  2019 7 17
3 Lucas Beyer Olivier J H enaff Alexander Kolesnikov Xi
aohua Zhai and A aron van den Oord Are we done with
imagenet CoRR  abs200607159 2020 2 7 9 17
4 Lucas Beyer Xiaohua Zhai and Alexander Kolesnikov Bet
ter plain vit baselines for imagenet1k 2022 10 17
5 Lucas Beyer Xiaohua Zhai and Alexander Kolesnikov Big
vision httpsgithubcomgoogleresearch
big_vision  2022 10 17
6 Minwoo Byeon Beomhee Park Haecheon Kim Sungjun
Lee Woonhyuk Baek and Saehoon Kim Coyo
700m Imagetext pair dataset httpsgithubcom
kakaobraincoyodataset  2022 1
7 Soravit Changpinyo Piyush Sharma Nan Ding and Radu
Soricut Conceptual 12M Pushing webscale imagetext
pretraining to recognize longtail visual concepts In CVPR 
2021 1
8 Feilong Chen Duzhen Zhang Minglun Han XiuYi Chen
Jing Shi Shuang Xu and Bo Xu VLP A survey on vision
language pretraining Int J Autom Comput  2013856
2023 2
9 Mark Chen Alec Radford Rewon Child Jeffrey Wu Hee
woo Jun David Luan and Ilya Sutskever Generative pre
training from pixels In Proceedings of the 37th Interna
tional Conference on Machine Learning ICML 2020 1318
July 2020 Virtual Event  volume 119 of Proceedings of Ma
chine Learning Research  pages 16911703 PMLR 2020
8
10 Ting Chen Simon Kornblith Mohammad Norouzi and Ge
offrey E Hinton A simple framework for contrastive learn
ing of visual representations In ICML  2020 2 4
11 Xinlei Chen Hao Fang TsungYi Lin Ramakrishna Vedan
tam Saurabh Gupta Piotr Doll ar and C Lawrence Zitnick
Microsoft COCO captions Data collection and evaluation
server CoRR  abs150400325 2015 7 17
12 Xiangning Chen Chen Liang Da Huang Esteban Real
Kaiyuan Wang Yao Liu Hieu Pham Xuanyi Dong Thang
Luong ChoJui Hsieh Yifeng Lu and Quoc V  Le Symbolic
discovery of optimization algorithms 2023 2 6
13 Xi Chen Xiao Wang Soravit Changpinyo A J Piergio
vanni Piotr Padlewski Daniel Salz Sebastian Goodman
Adam Grycner Basil Mustafa Lucas Beyer Alexander
Kolesnikov Joan Puigcerver Nan Ding Keran Rong Has
san Akbari Gaurav Mishra Linting Xue Ashish Thapliyal
James Bradbury Weicheng Kuo Mojtaba Seyedhosseini
Chao Jia Burcu Karagol Ayan Carlos Riquelme Andreas
Steiner Anelia Angelova Xiaohua Zhai Neil Houlsby andRadu Soricut Pali A jointlyscaled multilingual language
image model CoRR  abs220906794 2022 1 5 6 17
14 Jia Deng Wei Dong Richard Socher LiJia Li Kai Li
and Li FeiFei Imagenet A largescale hierarchical image
database In CVPR  2009 4 7 9 17
15 Karan Desai Gaurav Kaul Zubin Aysola and Justin John
son Redcaps Webcurated imagetext data created by the
people for the people In Joaquin Vanschoren and Sai
Kit Yeung editors Proceedings of the Neural Information
Processing Systems Track on Datasets and Benchmarks 1
NeurIPS Datasets and Benchmarks 2021 December 2021
virtual  2021 1
16 Xiaoyi Dong Jianmin Bao Ting Zhang Dongdong Chen
Shuyang Gu Weiming Zhang Lu Yuan Dong Chen Fang
Wen and Nenghai Yu Clip itself is a strong ﬁnetuner
Achieving 857 and 880 top1 accuracy with vitb and
vitl on imagenet CoRR  abs221206138 2022 2
17 Alexey Dosovitskiy Lucas Beyer Alexander Kolesnikov
Dirk Weissenborn Xiaohua Zhai Thomas Unterthiner
Mostafa Dehghani Matthias Minderer Georg Heigold Syl
vain Gelly Jakob Uszkoreit and Neil Houlsby An image is
worth 1616 words Transformers for image recognition at
scale In ICLR  2021 3 7 17
18 Ian Goodfellow Yoshua Bengio and Aaron Courville
Deep Learning  MIT Press 2016 httpwww
deeplearningbookorg  1
19 Raia Hadsell Sumit Chopra and Yann LeCun Dimension
ality reduction by learning an invariant mapping In CVPR 
volume 2 2006 2
20 Kaiming He Xinlei Chen Saining Xie Yanghao Li Pi
otr Doll ar and Ross B Girshick Masked autoencoders
are scalable vision learners In IEEECVF Conference on
Computer Vision and Pattern Recognition CVPR 2022 New
Orleans LA USA June 1824 2022  pages 1597915988
IEEE 2022 8
21 Xiaowei Hu Zhe Gan Jianfeng Wang Zhengyuan Yang
Zicheng Liu Yumao Lu and Lijuan Wang Scaling up
visionlanguage pretraining for image captioning CoRR 
abs211112233 2021 1 2
22 Gabriel Ilharco Mitchell Wortsman Nicholas Carlini
Rohan Taori Achal Dave Vaishaal Shankar Hongseok
Namkoong John Miller Hannaneh Hajishirzi Ali Farhadi
and Ludwig Schmidt OpenCLIP Zenodo 2021 8
23 Chao Jia Yinfei Yang Ye Xia YiTing Chen Zarana Parekh
Hieu Pham Quoc V  Le YunHsuan Sung Zhen Li and Tom
Duerig Scaling up visual and visionlanguage representation
learning with noisy text supervision In ICML  2021 1 2
24 Prannay Khosla Piotr Teterwak Chen Wang Aaron Sarna
Yonglong Tian Phillip Isola Aaron Maschinot Ce Liu and
Dilip Krishnan Supervised contrastive learning In Hugo
Larochelle MarcAurelio Ranzato Raia Hadsell Maria
Florina Balcan and HsuanTien Lin editors Advances in
Neural Information Processing Systems 33 Annual Con
ference on Neural Information Processing Systems 2020
NeurIPS 2020 December 612 2020 virtual  2020 2
25 Alexander Kolesnikov Lucas Beyer Xiaohua Zhai Joan
Puigcerver Jessica Yung Sylvain Gelly and Neil Houlsby
11Big transfer BiT General visual representation learning In
ECCV  2020 7
26 Alex Krizhevsky Learning multiple layers of features from
tiny images Technical report Univ of Toronto 2009 9
27 Taku Kudo and John Richardson SentencePiece A sim
ple and language independent subword tokenizer and detok
enizer for neural text processing In EMNLP  2018 5 14
28 Junnan Li Dongxu Li Caiming Xiong and Steven C H
Hoi BLIP bootstrapping languageimage pretraining for
uniﬁed visionlanguage understanding and generation In
Kamalika Chaudhuri Stefanie Jegelka Le Song Csaba
Szepesv ari Gang Niu and Sivan Sabato editors Interna
tional Conference on Machine Learning ICML 2022 17
23 July 2022 Baltimore Maryland USA  volume 162 of
Proceedings of Machine Learning Research  pages 12888
12900 PMLR 2022 2
29 Xianhang Li Zeyu Wang and Cihang Xie Clipav2 Scal
ing CLIP training with 811 zeroshot imagenet accuracy
within a 10 000 budget an extra 4 000 unlocks 818
accuracy CoRR  abs230615658 2023 8
30 Yanghao Li Haoqi Fan Ronghang Hu Christoph Feichten
hofer and Kaiming He Scaling languageimage pretraining
via masking CoRR  abs221200794 2022 2 7
31 Matthias Minderer Alexey A Gritsenko Austin Stone
Maxim Neumann Dirk Weissenborn Alexey Dosovitskiy
Aravindh Mahendran Anurag Arnab Mostafa Dehghani
Zhuoran Shen Xiao Wang Xiaohua Zhai Thomas Kipf
and Neil Houlsby Simple openvocabulary object detection
In Shai Avidan Gabriel J Brostow Moustapha Ciss e Gio
vanni Maria Farinella and Tal Hassner editors Computer
Vision  ECCV 2022  17th European Conference Tel Aviv
Israel October 2327 2022 Proceedings Part X  volume
13670 of Lecture Notes in Computer Science  pages 728
755 Springer 2022 2
32 Margaret Mitchell Simone Wu Andrew Zaldivar Parker
Barnes Lucy Vasserman Ben Hutchinson Elena Spitzer
Inioluwa Deborah Raji and Timnit Gebru Model cards
for model reporting In danah boyd and Jamie H Morgen
stern editors Proceedings of the Conference on Fairness
Accountability and Transparency FAT 2019 Atlanta GA
USA January 2931 2019  pages 220229 ACM 2019 17
33 Jishnu Mukhoti TsungYu Lin Omid Poursaeed Rui Wang
Ashish Shah Philip H S Torr and SerNam Lim Open
vocabulary semantic segmentation with patch aligned con
trastive learning 2022 2
34 Omkar M Parkhi Andrea Vedaldi Andrew Zisserman and
C V  Jawahar Cats and dogs In IEEE Conference on Com
puter Vision and Pattern Recognition  2012 9
35 Hieu Pham Zihang Dai Golnaz Ghiasi Hanxiao Liu
Adams Wei Yu MinhThang Luong Mingxing Tan and
Quoc V  Le Combined scaling for zeroshot transfer learn
ing CoRR  abs211110050 2021 2 4
36 Alec Radford Jong Wook Kim Chris Hallacy Aditya
Ramesh Gabriel Goh Sandhini Agarwal Girish Sastry
Amanda Askell Pamela Mishkin Jack Clark Gretchen
Krueger and Ilya Sutskever Learning transferable visual
models from natural language supervision In ICML  2021
1 2 3 837 Colin Raffel Noam Shazeer Adam Roberts Katherine Lee
Sharan Narang Michael Matena Yanqi Zhou Wei Li and
Peter J Liu Exploring the limits of transfer learning with a
uniﬁed texttotext transformer arXiv eprints  2019 5 14
38 Colin Raffel Noam Shazeer Adam Roberts Katherine Lee
Sharan Narang Michael Matena Yanqi Zhou Wei Li and
Peter J Liu Exploring the limits of transfer learning with
a uniﬁed texttotext transformer J Mach Learn Res 
21140114067 2020 17
39 Benjamin Recht Rebecca Roelofs Ludwig Schmidt and
Vaishaal Shankar Do ImageNet classiﬁers generalize to Im
ageNet In ICML  2019 7 17
40 Christoph Schuhmann Romain Beaumont Richard Vencu
Cade Gordon Ross Wightman Mehdi Cherti Theo
Coombes Aarush Katta Clayton Mullis Mitchell Worts
man Patrick Schramowski Srivatsa Kundurthy Katherine
Crowson Ludwig Schmidt Robert Kaczmarczyk and Jenia
Jitsev LAION5B an open largescale dataset for training
next generation imagetext models CoRR  abs221008402
2022 1
41 Krishna Srinivasan Karthik Raman Jiecao Chen Michael
Bendersky and Marc Najork WIT wikipediabased image
text dataset for multimodal multilingual machine learning
CoRR  abs210301913 2021 1
42 Andreas Steiner Alexander Kolesnikov Xiaohua Zhai Ross
Wightman Jakob Uszkoreit and Lucas Beyer How to train
your ViT Data augmentation and regularization in vision
transformers CoRR  abs210610270 2021 1 6 7
43 Quan Sun Yuxin Fang Ledell Wu Xinlong Wang and Yue
Cao EV ACLIP improved training techniques for CLIP at
scale CoRR  abs230315389 2023 8
44 Ashish V  Thapliyal Jordi PontTuset Xi Chen and Radu
Soricut Crossmodal3600 A massively multilingual mul
timodal evaluation dataset In Yoav Goldberg Zornitsa
Kozareva and Yue Zhang editors Proceedings of the 2022
Conference on Empirical Methods in Natural Language Pro
cessing EMNLP 2022 Abu Dhabi United Arab Emirates
December 711 2022  pages 715729 Association for Com
putational Linguistics 2022 4 7 17
45 Hugo Touvron Thibaut Lavril Gautier Izacard Xavier
Martinet MarieAnne Lachaux Timoth ee Lacroix Baptiste
Rozi ere Naman Goyal Eric Hambro Faisal Azhar Aur elien
Rodriguez Armand Joulin Edouard Grave and Guillaume
Lample Llama Open and efﬁcient foundation language
models CoRR  abs230213971 2023 7
46 A aron van den Oord Yazhe Li and Oriol Vinyals Repre
sentation learning with contrastive predictive coding CoRR 
abs180703748 2018 2
47 Ashish Vaswani Noam Shazeer Niki Parmar Jakob Uszko
reit Llion Jones Aidan N Gomez Lukasz Kaiser and Illia
Polosukhin Attention is all you need In NeurIPS  2017 3
17
48 Alexander Visheratin Nllbclip  train performant multilin
gual image retrieval model on a budget 2023 7
49 Jianfeng Wang Zhengyuan Yang Xiaowei Hu Linjie Li
Kevin Lin Zhe Gan Zicheng Liu Ce Liu and Lijuan Wang
GIT A generative imagetotext transformer for vision and
language CoRR  abs220514100 2022 1 2
1250 Zirui Wang Jiahui Yu Adams Wei Yu Zihang Dai Yulia
Tsvetkov and Yuan Cao Simvlm Simple visual language
model pretraining with weak supervision In The Tenth In
ternational Conference on Learning Representations ICLR
2022 Virtual Event April 2529 2022  OpenReviewnet
2022 2
51 Ross Wightman Hugo Touvron and Herv e Jegou Resnet
strikes back An improved training procedure in timm
CoRR  abs211000476 2021 2
52 Mitchell Wortsman Reaching 80 zeroshot accuracy with
OpenCLIP VITG14 trained on LAION2B https
webarchiveorgweb20230127012732
httpslaionaibloggiantopenclip  2
53 Mitchell Wortsman Gabriel Ilharco Jong Wook Kim
Mike Li Simon Kornblith Rebecca Roelofs Raphael Gon
tijo Lopes Hannaneh Hajishirzi Ali Farhadi Hongseok
Namkoong and Ludwig Schmidt Robust ﬁnetuning of
zeroshot models In IEEECVF Conference on Computer
Vision and Pattern Recognition CVPR 2022 New Orleans
LA USA June 1824 2022  pages 79497961 IEEE 2022
2
54 Linting Xue Noah Constant Adam Roberts Mihir Kale
Rami AlRfou Aditya Siddhant Aditya Barua and Colin
Raffel mT5 A massively multilingual pretrained textto
text transformer In NAACLHLT  2021 5 17
55 Jianwei Yang Chunyuan Li Pengchuan Zhang Bin Xiao Ce
Liu Lu Yuan and Jianfeng Gao Uniﬁed contrastive learn
ing in imagetextlabel space In IEEECVF Conference on
Computer Vision and Pattern Recognition CVPR 2022 New
Orleans LA USA June 1824 2022  pages 1914119151
IEEE 2022 2
56 Jiahui Yu Zirui Wang Vijay Vasudevan Legg Yeung
Mojtaba Seyedhosseini and Yonghui Wu Coca Con
trastive captioners are imagetext foundation models CoRR 
abs220501917 2022 2
57 Lu Yuan Dongdong Chen YiLing Chen Noel Codella
Xiyang Dai Jianfeng Gao Houdong Hu Xuedong Huang
Boxin Li Chunyuan Li Ce Liu Mengchen Liu Zicheng Liu
Yumao Lu Yu Shi Lijuan Wang Jianfeng Wang Bin Xiao
Zhen Xiao Jianwei Yang Michael Zeng Luowei Zhou and
Pengchuan Zhang Florence A new foundation model for
computer vision CoRR  abs211111432 2021 2
58 Xiaohua Zhai Alexander Kolesnikov Neil Houlsby and Lu
cas Beyer Scaling vision transformers CVPR  2022 1 4
6 7 14
59 Xiaohua Zhai Xiao Wang Basil Mustafa Andreas Steiner
Daniel Keysers Alexander Kolesnikov and Lucas Beyer
Lit Zeroshot transfer with lockedimage text tuning In
IEEECVF Conference on Computer Vision and Pattern
Recognition CVPR 2022 New Orleans LA USA June 18
24 2022  pages 1810218112 IEEE 2022 1 2 3 4 6 7
14
60 Yuhao Zhang Hang Jiang Yasuhide Miura Christopher D
Manning and Curtis P Langlotz Contrastive learning of
medical visual representations from paired images and text
In Zachary C Lipton Rajesh Ranganath Mark P Sendak
Michael W Sjoding and Serena Yeung editors Proceed
ings of the Machine Learning for Healthcare ConferenceMLHC 2022 56 August 2022 Durham NC USA  volume
182 of Proceedings of Machine Learning Research  pages
225 PMLR 2022 2
13A More results for SigLiT
In section 41 we use the same precomputed embed
dings for the images using a ViTg vision model from 59
Only resize augmentation is applied to a ﬁxed 288288
resolution We train a standard base size text tower using
the ScalingViTAdafactor optimizer 58 with 1 09and
2 095 We use 0001 learning rate with a linear warmup
schedule for the ﬁrst 200 M examples seen and then the
learning rate is decayed to zero with a cosine learning rate
schedule Weight decay is set to 00001 for all the experi
ments The text is tokenized by a 32 k vocabulary sentence
piece tokenizer 27 trained on the English C4 dataset 37
and a maximum of 16 text tokens are kept Table 8 shows
results with multiple train examples seen and batch sizes
for both the sigmoid loss and the softmax loss baseline
For training SigLiT in under a day with 4 chips Sec
tion 44 we used the LION optimizer with peak learning
rate1104and weight decay 1107 The learning rate
was warmed linearly to the peak in 65 k steps then cosine
decayed to zero for the remaining 585 k steps
B More results for SigLIP
In Table 5 we present more results for SigLIP Base with
multiple train examples seen 3 billion examples and 9 bil
lion examples respectively
Batch Size3 B 9 B
sigmoid softmax sigmoid softmax
512 515 477  
1 k 573 532  
2 k 621 593  
4 k 653 638 684 666
8 k 686 666 706 694
16 k   723 717
32 k 699 699 734 729
98 k 695 697 730 732
307 k   716 726
Table 5 SigLIP zeorshot accuracy  on the ImageNet
benchmark Both the sigmoid loss and the softmax loss
baseline are presented Experiments are performed on mul
tiple train examples seen 3 B 9 B and train batch sizes
from 512 to 307 k When trained for 9 B examples the
peak of the sigmoid loss comes earlier at 32 k than the peak
of the softmax loss at 98 k Together with the memory ef
ﬁcient advantage for the sigmoid loss it allows one to train
the best languageimage model with much fewer amount of
acceleratorsBS Default Best Best LR Best WD
8 k 701 701 0001 00001
16 k 700 700 0001 00001
32 k 682 690 00003 000003
Table 6 Default hyperparameters across different batch
sizes perform either the best or close to the best hyperpa
rameter from a sweep Zeroshot accuracy on ImageNet
is reported BSbatch size LRlearning rate WDweight
decay
C Robustness of SigLIP results
Hyperparameters for different batch sizes Sigmoid
loss doesnt require tuning hyperparameters for different
batch sizes For example in both the SigLiP and SigLiT
setup we only used default 0001 learning rate and 00001
weight decay across a wide range of batch sizes from 512
to 1024k We further performed a sweep of 9 hyperparam
eters across 3 batch sizes on the fromscratch SigLIP setup
for 3B seen examples learning rate f00003 0001 0003 g
weight decayf000003 00001 00003 g batch size
f8 k 16 k 32 kg We observed in Table 6 that the default
LRWD is either the best or close to the best
Standard deviation We repeat SigLIP training ﬁve
times using the recommended 32k batch size and 3B seen
examples We report the average and std in Table 7 The std
of the ﬁve runs is very small for both sigmoid and softmax
Alternative optimizers We repeat the same experiment
with AdamW optimizer ﬁve times and got very similar re
sults and std as reported in Table 7 We tested a linear learn
ing rate scheduler instead of the default cosine learning rate
scheduler it achieves 699 accuracy
D More results for mSigLIP
We present the mSigLIP Base crossmodal retrieval re
sults on the Crossmodal3600 dataset across all the 36 lan
gauges in Figure 8 and Table 9
Loss Optimizer Results 
Softmax ViTAdafactor 699 01
Sigmoid ViTAdafactor 701 02
Sigmoid AdamW 703 01
Table 7 Mean and standard deviation of ﬁve repeated ex
periments Zeroshot accuracy on ImageNet is reported
14Batch Size450 M 900 M 3 B 18 B
sigmoid softmax sigmoid softmax sigmoid softmax sigmoid softmax
512 725 695 750 728 772 746  
1 k 755 736 772 760 796 779  
2 k 771 763 793 781 813 801 822 812
4 k 792 783 808 798 824 812 830 820
8 k 808 797 820 810 831 826 836 831
16 k 812 812 827 821 838 835 842 841
32 k 819 814 831 827 842 840 846 844
64 k 816 816 830 828 843 841 847 844
128 k 805 800 831 832 842 844 847 846
256 k 728 722 821 817 843 842 847 846
1024 k       847 
Table 8 SigLiT zeroshot accuracy  on the ImageNet benchmark Both the sigmoid loss and the softmax loss
baseline are presented Extensive experiments are performed on multiple train examples seen 450 M 900 M 3 B 18 B and
train batch sizes from 512 to 1 M
arbncsdadeelenesfafifilfrhihrhuiditiwjakominlnoplptquz rorusvswtethtrukvizhavg000102030405060708 16 k 32 k 64 k 128 k 240 k 32 k
arbncsdadeelenesfafifilfrhihrhuiditiwjakominlnoplptquz rorusvswtethtrukvizhavg0001020304050616 k 32 k 64 k 128 k 240 k 32 k
Figure 8 Imagetotext and texttoimage zeroshot retrieval recall1 results on all 36 languages of Crossmodal3600 
Top Image to text Bottom text to image Colors are batch sizes 32 k represents the scaled up results as described in
Section 46
E Label noise experiments
All models had an M16 image tower and a M text tower
They were trained from random initialisation for 36B ex
amples seen with a batch size of 16384 A cosine learning
rate schedule was used with an initial linear warmup for
10 of steps up to a peak learning rate of 0001
15LangImagetotext Texttoimage
16 k 32 k 64 k 128 k 240 k 32 k 16 k 32 k 64 k 128 k 240 k 32 k
ar 524 513 515 515 511 597 376 374 371 363 360 449
bn 114 108 104 103 99 301 55 62 49 51 44 200
cs 541 537 537 528 518 589 418 416 415 399 394 470
da 627 624 620 604 593 684 470 470 456 430 435 529
de 703 714 712 711 702 797 547 548 554 543 547 653
el 369 358 351 345 338 474 224 228 220 213 208 322
en 501 505 502 499 507 525 465 462 465 466 466 476
es 647 649 672 653 656 663 548 550 555 545 552 570
fa 570 578 561 553 546 662 396 402 384 384 383 500
ﬁ 549 541 538 517 517 591 377 371 364 340 345 440
ﬁl 232 228 229 214 212 292 128 129 124 122 113 204
fr 657 669 670 661 665 712 559 571 555 544 543 618
hi 199 188 199 195 174 322 91 85 79 81 73 173
hr 527 530 530 499 496 626 382 371 364 352 343 472
hu 570 571 563 548 530 629 414 402 402 386 382 512
id 648 671 666 654 647 737 485 494 495 478 473 605
it 659 664 671 652 661 723 555 564 558 548 541 623
iw 484 479 477 461 452 622 318 318 319 301 301 480
ja 464 459 429 437 302 551 310 313 292 289 185 423
ko 508 495 494 502 468 614 344 347 332 331 315 459
mi 04 04 06 06 04 03 02 02 02 02 02 03
nl 596 604 589 583 579 636 489 495 489 484 479 536
no 614 624 620 609 599 653 453 462 450 435 437 500
pl 622 620 620 611 605 671 488 474 487 468 467 567
pt 631 636 649 643 632 654 524 523 523 519 524 573
quz 68 64 64 66 67 68 27 26 27 27 28 29
ro 521 514 510 506 493 610 372 356 343 345 325 493
ru 622 636 631 627 631 684 501 499 497 486 493 599
sv 623 635 635 631 612 677 479 482 476 462 462 520
sw 148 144 143 142 138 174 78 72 71 69 63 107
te 12 12 12 17 11 84 04 03 03 05 03 43
th 361 358 356 356 283 390 216 231 222 216 168 246
tr 531 545 537 529 512 620 373 374 378 370 361 481
uk 514 515 512 499 492 612 345 332 338 325 324 483
vi 596 598 595 585 588 684 414 419 419 406 403 523
zh 441 457 441 419 361 539 307 325 320 306 237 468
avg 472 474 471 463 450 541 348 349 344 336 327 426
Table 9 Imagetotext text retrieval and texttoimage image retrieval zeroshot recall1 results on all 36 languages
of Crossmodal3600  with mSigLIP models trained at different batch sizes for 30 B total examples seen 32 k represents
the scaled up results as described in Section 46
16F Model Card
We provide a description of our models following 32
Model Architecture The model is trained using the
contrastive pretraining technique with sigmoid loss
as described in this paper This contrastive model
contains two encoders ie vision transformer en
coder 17 and language transformer encoder 47 The
vision and language encoders always have the same
size one of ViTB ViTL and SoViT400M 1
Inputs The vision encoder takes an image  224
2243256256338438435125123 as
input The text encoder takes a tokenized text 38 54
cropped to the ﬁrst 64 tokens as input
Outputs The vision and text encoders both output a d
dimensional feature vector where dis 768 1024 and
1152 for ViTB ViTL and SoViT400M respectively
Intended Use The models are designed for multi
modal research purposes The models can be used
for zeroshot image classiﬁcation and zeroshot image
text retrieval by comparing both feature vectors We
provide both enonly and i18ntrained models to en
courage research on the impact of this choice
Training Data The contrastive model is pretrained
fromscratch using the WebLI 13 dataset SigLIP
models are pretrained on a WebLI subset ﬁltered to
contain mostly English mSigLIP models are pre
trained on the WebLI dataset without language ﬁlters
Evaluation Data Zeroshot classiﬁcation is per
formed on ImageNet 14 ImageNet v2 39 Ima
geNet Real 3 and ObjectNet 2 Zeroshot re
trieval is performed on COCO 11 and the multilin
gual XM3600 dataset 44
Hardware  Software  The models are developed
in the bigvision codebase 5 4 and trained on
Google Cloud TPUs
17
  σGPTs A New Approach to Autoregressive
Models
Arnaud Pannatier12 Evann Courdier12 and François Fleuret3
1Idiap Research Institute Martigny Switzerland
2Ecole Polytechnique Fédérale de Lausanne Lausanne Switzerland
3Université de Genève Geneva Switzerland
Abstract Autoregressive models such as the GPT family use a fixed
order usually lefttoright to generate sequences However this is not
a necessity In this paper we challenge this assumption and show that
by simply adding a positional encoding for the output this order can be
modulated onthefly persample which offers key advantageous proper
ties It allows for the sampling of and conditioning on arbitrary subsets
of tokens and it also allows sampling in one shot multiple tokens dynam
ically according to a rejection strategy leading to a sublinear number
of model evaluations We evaluate our method across various domains
including language modeling pathsolving and aircraft vertical rate pre
diction decreasing the number of steps required for generation by an
order of magnitude
Keywords Autoregressive models Permutations Transformers Re
jection Sampling
1 Introduction
Transformers demonstrate exceptional autoregressive capabilities across modal
ities The traditional take for autoregression is to follow the natural order of the
data for example lefttoright for text In the case of vision the usual scheme
is to unfold the images following a rasterscan order and to use transformers
to model the obtained sequence In this work we make a distinction between
the order of the input data and the order of autoregression highlighting that
while they are typically aligned in most applications they need not be Our
investigation involves training and generating sequences in a randomly shuffled
order using transformers While changing the sequence order is more challenging
during training it also reveals fascinating properties of the models
By breaking away from the standard autoregression order one can use the
model to predict the tokens in any particular order With this scheme the model
is capable of predicting at any moment of the generation the conditional distri
bution of the remaining tokens Having these estimates allows quantifying the
possible outcomes of the generation at any given point More interestingly they
can be leveraged to do rejection sampling allowing to generate sequences by
burst with a dynamical number of stepsarXiv240409562v1  csLG  15 Apr 20242 A Pannatier et al
x1 x2 x3    xT1 xT0
0
ϕσ1xσ1
ϕσ1
ϕσ2xσ2
ϕσ2
ϕσ3  xσT2
ϕσT2
ϕσT1xσT1
ϕσT1
ϕσTˆxσ1 ˆxσ2 ˆxσ3    ˆxσT1 ˆxσT
GPTˆx1 ˆx2 ˆx3   ˆxT1 ˆxT
InputToken content
Token pos enc
Target pos encOutput
Fig1 In our σGPT an arbitrary shuffling order σcan be chosen onthefly
for every sample It induces an input order 0 σ1 σ2   and an output or
derσ1 σ2 σ3    where the input is first padded with a 0to ensure a
consistent number of tokens Tokens are shuffled accordingly and these orders
are both encoded separately with two positional encodings concatenated to the
input allowing the model to sample consistently in the autoregressive process
The output is finally shuffled back to the true order
σGPT GPTDiffusion
Models
Sample Anywhere   
Conditional Density Estimation   
Arbitrary conditioning   tildelow
Infilling   tildelow
Burst Sampling   
Loglikelihood Training   
Table 1 Comparison between our approach a standard causal transformer en
coder called GPT here and diffusion models Our model allows the sampling
of a token at any position in the sequence to model the remaining density ac
cording to a partially sampled sequence naturally supports infilling and can be
used to sample the sequence by burst allowing faster generation Compared to
diffusion models it can be trained easily using crossentropyσGPTs A New Approach to Autoregressive Models 3
This work is structured as follows we first introduce σGPTs and shuffled
autoregression and show that a model trained with this method combined with
a curriculum method can even increase the performance of the underlying model
We then present the additional properties of σGPTs summarized in Table 1
in particular for estimating conditional probabilities and we present our token
based rejection sampling scheme which allows for generating the sequence per
burst and its theoretical properties We evaluate our model and our scheme
on three main tasks which are open text generation pathsolving and aircraft
vertical rate prediction
Contributions
Introduce σGPTanovelarchitecturewithtwopositionalencodingsrelated
respectively to the input and output order that allows a causal transformer
to generate sequences in any order which can be modulated on the fly for
any pass through the model
Demonstrate that our method can reach similar performance as lefttoright
trained autoregressive models when trained with a curriculum scheme
Demonstrate that our method can be used to generate samples in any or
der allowing for the generation of samples conditioned on any part of the
sequence
Introduce a novel tokenbased rejection sampling scheme that leads to the
generation of samples per burst
2 Methodology
21 σGPTs Shuffled Autoregression
We propose a novel approach for training autoregressive models which involves
doing nexttoken prediction on a shuffled input sequence We present σGPT
where σdenotes the permutation used to shuffle the sequence and by GPT we
mean any causal transformer encoder or causal transformer decoder without
crossattention such as 13 To train such a model each sequence is shuffled
randomly during training The model is then tasked to predict the next token
in the shuffled sequence conditioned on all the tokens it has seen before This
training is done as usual with a standard crossentropy loss Besides the ran
domization of the order of the sequence and the addition of a double positional
encoding no other changes are needed to the model or training pipelines For
the rest of the paper we use lefttoright order to mention the usual order in
which models are trained even in the case of 2D data which are usually mapped
to a sequence using a rasterscan order And we use random order to mean that
the input has been shuffled
22 Double Positional Encodings
To be able to model sequences in any order each token needs to have informa
tion about its position and the one of the next token in the shuffled sequence4 A Pannatier et al
Specifically when handling a sequence of tokens alongside a given permutation
σ every token contains three distinct pieces of information its value xσt its
current position σt and the position σt 1of the subsequent token in the
shuffled sequence that are all concatenated The necessity for double positional
encoding arises from the intrinsic characteristics of transformers Given that
each token attends to every previous token in a positioninvariant manner each
token needs to contain information about its position in the original sequence
so other tokens can know where they are located And each token needs to know
the position of the next token in the shuffled sequence as it is the target of
the prediction The double positional encoding is the only architectural change
needed to train autoregressive models in random order In this work we used
the standard sinusoidal positional encoding 15 for both the input and output
positional encodings
23 Conditional Probabilities and Infilling
0 20 40 60 80 100 120 140050100150200250
Infilling
Samples
True
CFL
0 20 40 60 80 100 120050100150200250
Conditional Density Estimation
Fig2 Left We can infill the sequence by conditioning on the known part black
points Right We can also have estimates of the density at any point of the
sequence
Our method allows making conditional density estimation of the rest of the
sequence It is capable of making predictions all over the task space conditioned
on any known subpart of the task This can be done by prompting the model
with the known part of the sequence and then decoding in parallel and in one
pass the remaining tokens Such evaluations are not possible with autoregressive
models trained in a lefttoright order as they need to follow the specific order
theyve been trained in Examples showing that the model usually has good
estimates of the unconditioned distribution can be seen in Figures 2 and 3aσGPTs A New Approach to Autoregressive Models 5
a Left The theoretical density of the
optimal path in the maze Right The es
timated probability of the class path at
every position before starting autoregres
sion We see that the model has good esti
mates of the true density
b Two different conditional samplings for
each maze The known part of the path
purple is prompted first and the rest of
the sequence can be completed coherently
Fig3 Conditional density estimation and infilling on the maze pathsolving
task
Directly related to conditional density estimation is that our method nat
urally supports infilling as it is straightforward to prompt the model with the
known part of a signal and to decode autoregressively or by burst the rest of
the signal Figures 2 and 3a shows example of such samplings
24 Tokenbased Rejection Sampling
Autoregressive generation is a slow process as each token has to be gener
ated sequentially Even with caching strategies this still scales linearly with the
sequence length and it becomes prohibitively expensive for long sequences 16
As our model allows for the generation of tokens in any order we can leverage
that fact and sample tokens in parallel at every position of the sequence We can
then evaluate the candidate sequence under different orders and accept multiple
tokens in one pass This algorithm runs efficiently on GPU as both the sam
pling at every position and the evaluation under different orders can be made in
parallel in a forward pass and using an adapted KVcaching mechanism We
describe this caching mechanism more in detail in Appendix C of the supple
mentarymaterialWhenconditionedonpartiallycompletedsequencesthemodel
outputs distributions that are compatible with different possible outcomes and
when evaluating under different orders for generation the distribution of tokens
is constrained to tokens that are compatible with the previous tokens seen in
one given order As both the sampling and evaluation can be done in parallel
we can compute the acceptance decision efficiently for every token
This strategy outputs a decision for each remaining token but the decisions
made by models become sometimes nonsensical when two mutually exclusive6 A Pannatier et al
Algorithm 1 Tokenbased rejection sampling following notation of 4
Given minimum target length T y trained σGPT and number of orders No
Given a prompt xiXof length t0of initial tokens  Xcan be the empty set
Settt0
while t  Tdo
In parallel compute distribution conditioned on prompt pxiXit     T
In parallel sample at every position xipxiXit     T
Draw Norandom order σand in parallel compute all logits qxiXxσii
t     T
In parallel sample Tt rtU01from a uniform distribution
In parallel compute the acceptance decision airimin1 pxiX
qxiXxσifor
every order
Select the order that accepts the most tokens before seeing a first rejection
Keep that order and add the aaccepted tokens before the first rejection to the
prompt
Settta
end while
tokens are part of the prompt Once a rejection is seen all subsequent accepted
tokens in the order of evaluation should be discarded Indeed the scheme rejects
tokens that are incoherent with the ones already seen and asking a model to
make predictions based on incoherent tokens might lead to incoherent decisions
Using multiple orders allows keeping the one that accepts the most tokens in its
evaluation Even if it is dynamic this algorithm can still easily generate multiple
samplesatoncebyacceptingthesameamountoftokensforeachsequenceinthe
batch Our rejection sampling algorithm is given in pseudocode in Algorithm 1
Other models such as Mask Git 3 or diffusion models 71 are doing gen
eration by burst However these models usually require fixing the number of
steps or a masking schedule beforehand Our method on the other hand adapts
dynamically to the underlying statistics of the data and thus does not require
this extra hyperparameter We evaluate it on three synthetic cases to showcase
this dynamic capability we present the results in Section 38
25 Other orders
Ourdoublepositionalencoding scheme allows fortraining andevaluatingmodels
inanyorderUsingarandomizedorderduringtrainingallowsconditionaldensity
estimation infilling and burst sampling at inference time However the double
positional encoding scheme allows any order to be used and it can be used to
train models in a deterministic order that is not lefttoright As an example we
use a deterministic fractal order to see how it compares to a random or leftto
right order This order starts in the middle of the sequence then recursively goes
to the first quarter and threequarters of the sequence and goes on recursively
until all the positions have been visited Such an order is fully deterministic
yet we make the hypothesis that this order leads to more difficult training forσGPTs A New Approach to Autoregressive Models 7
the model as it cannot rely on the locality of the information We present the
results in Section 35 Note that under perfect models the order of modeling and
decoding should not matter because of the chain rules of probability We give
more details about it in Appendix A1 of the supplementary material
26 Denoising diffusion models
Denoising diffusion models 7 is a family of generative models that can also
be used to generate sequences in a few steps They are trained to reverse a
diffusion process that is applied to the data Diffusion processes can be both
continuous and discrete In this work we use as a baseline only the discrete
diffusion case in particular using a uniform diffusion process 1 To be able to
compare the methods fairly we use the same transformer architecture for both
σGPT and the diffusion model changing only the training objective Compared
toσGPT diffusion models are not dynamic and require a fixed number of steps
to generate a sequence independently of the underlying statistics of the data
They also dont natively support conditional density estimation and infilling
3 Results
31 General performance
We tested our model across three main distinct tasks language modeling maze
path solving and aircraft verticalrate prediction
LanguageModelingWeusedboththeGPT2123MmodelontheWikitext
103 dataset 11 and GPT2 345M on OpenWeb Text 5
Maze Path Solving This task involves determining a valid path between a
starting and ending point in 13 x 21 mazes featuring 15 barriers Presented
with an image of an empty maze with start and end points the model is
tasked with producing an image with a legitimate path
AircraftVerticalRatePredictionThistaskusesrealaircrafttrajectorydata
with its aircraft type The data represents trajectories conditioned by air
traffic control directives The models objective is to predict the vertical
trajectory from a planes current altitude to a specified control level
Additionally to these tasks we created a synthetic benchmark for evaluating our
burst sampling algorithm
Product Dataset This toy example represents a pure product law case and is
made of a sequence of length 100 with two classes 01 given by a Bernoulli
law with p  10
Step Dataset This toy example comprises sequences of two classes 01 of
length 100 which are 0 everywhere except on a step of length 10 placed
randomly in the sequence8 A Pannatier et al
Joint Law dataset This toy example represents a pure joint law and consists
of a sequence of length 100 with 100 different classes the model should
predict a random generation of these different classes
The general results of our models are presented in Table 2 These results
indicate that training in a random order while requiring more computetime as
we describe in Section 32 reaches similar performances to lefttoright trained
models For the text modeling to have a fair comparison during training we
monitor the validation perplexity of the sequence evaluated in a lefttoright
order Training in random order for text modeling was plateauing at a higher
lefttoright validation perplexity but using a curriculum scheme allows reaching
the same performances as presented in Section 33 For the path solving and
the vertical rate prediction the models were able to reach the same leftto
right validation loss during training In inference we noticed a one percent drop
in accuracy compared to diffusion models and lefttoright trained GPT For
the vertical rate prediction task the dataset that we used is limited to around
23000 different sequences we noticed that the standard lefttoright GPT was
sometimes stuck repeating the same altitude we think this is a modeling issue
due to the small data regime σGPT does not seem to suffer as much from this
problem and offers a decrease in MSE We hypothesize that this behavior comes
from using a random order in inference which forces the model to fix some tokens
over the whole sequence early in the generation By doing so the model gains
the advantage of having a sketch of the whole sample and then concentrates on
completing a coherent sample
Table 2 General results We report the validation perplexity for text generation
the test accuracy for the maze solver and the mean squared error MSE for the
vertical rate prediction σGPT reaches a similar performance as GPT in text
generation and maze solving and it outperforms GPT in the case of the vertical
rate prediction We report the validation perplexity for the text generation For
the path solver we report the test accuracy on 1000 novel mazes For the vertical
prediction task we report the mean squared error on the test set We report the
meanandstandarddeviationforthepathsolvingandtheverticalrateprediction
taskWedonotreportthevalidationerrorforthetextgenerationforthediscrete
diffusion Dis Diff as the training objective is different
Textgeneration Path Solving Vertical Rate
OWT Val Perp   Wiki103 Val Perp   Accuracy   MSE  
GPT 1814 2030 9960 070 2748 707
σGPT 1864 1669 9830 067 1414 41
Dis Diff   9920 067 10594 13σGPTs A New Approach to Autoregressive Models 9
Table 3 Training efficiency Number of stepsepochs required to reach the same
performance and comparison with as GPT trained causally As learning to pre
dict in any order is a more challenging task it is expected to need more com
puting time to reach the same accuracy We dont report the standard deviation
for text generation as we limited the training to one run
Order Textgeneration Maze Solver Climbing Rate
σGPT 32500 780 65 1107 45
GPT 16500 193 49 250 36
32 Training Efficiency
Modeling sequences in a random order is a more challenging task than modeling
in lefttoright order We think this is due to two main factors at the beginning
of the sequences models cannot rely on adjacent tokens to make educated guesses
for the next token Second some tasks are harder to learn in one direction than
another and by modeling the data in any direction we are always in the harder
scenario We give an example of one task that is harder to learn in one direction
in Appendix A of the supplementary material
This implies that we expect and see an increase in the number of steps or
epochs required to learn a task As previously mentioned we dont see experi
mentally a drop in the validation performance of our model in the case of the
pathfinding algorithm or the vertical rate forecasting but the time to reach the
same performance increased In the case of text modeling the models plateaued
before reaching the same accuracy when trained in random order We treat that
case in the following section We report in table Table 3 the increase in training
steps or epoch to reach the same accuracy We see that most of the time the
number of epochs or steps needed to reach the same performance drastically in
creases We think again that this is due to the increased complexity of modeling
the sequence without having to rely on local information
33 Curriculum Learning
For text modeling we found a gap in validation perplexity in the lefttoright
order between models trained purely in a random order and models trained in a
lefttoright order We see in Table 4 that σGPT is stuck at larger perplexity in
both Open Web Text and WikiText103 3043 vs 1814 and 3985 vs 2030 We
foundthattrainingforlongerandusinglargermodeldidnthelpinreducingthat
gap To solve that problem we introduced a curriculum learning scheme where
the model is shown first more sequences in lefttoright order and progressively
learns to model the sequence randomly Surprisingly using this scheme helped
drastically the model which managed to get even better performance than left
toright trained transformers in the Wikitext103 case and reduce drastically the
gap for models trained on OpenWebText10 A Pannatier et al
Table 4 Curriculum learning We monitor the Validation Perplexity using a left
toright order during training to have a comparable evaluation We see that there
is a gap between the model trained purely in a lefttoright fashion GPT and
others trained in a random order  σGPT Training for longer and larger models
didnt help in removing that gap We introduce a curriculum learning scheme
thatstartspresentingthemodelwithsomepercentageofthedatawritteninthe
corresponding label in a lefttoright order at the beginning of the training and
goes linearly to 100 of sequence in a random order at the end of the training
We see that training with this scheme removes the gap between σGPT and
regular GPT and it reaches even better than lefttoright performance in the
WikiText103 case
Textgeneration Val Perp  
Min LefttoRight
Openweb Text  GPT 345 M
GPT 1814
σGPT curr 50 1864
σGPT no curr 3043
WikiText 103  GPT 128M
σGPT curr 50 1669
σGPT curr 100 1938
σGPT curr 10 1945
GPT 2030
σGPT no curr 3985
34 Open Text Generation tSNE of generated sequence
To get a qualitative sense of the generated text by the different methods We
generate 3000 sequences of 1024 tokens with each method embed each sequence
using an embedding model and then project the embeddings to 2D using tSNE
WepresenttheresultsinFigure4WeusedOpenAI textembedding3small 12
to embed the generated sequences into a single 1536 vector embedding We rep
resent as green embeddings of sequences of the validation set used as reference
We compute the tSNE using the whole 15000 embeddings and then plot each
method blue and the other considered method small gray dots We first see
that embeddings of GPT σGPT σGPT with burst sampling and diffusion
are spread over the whole space showing that the model can generate sequences
that are coherent with the validation set
35 Fractal order
We describe here the results that we get when training a GPT using a determin
istic but not lefttoright order We described the order in Section 25 We train
a GPT using this specific order for the different tasks and present the resultsσGPTs A New Approach to Autoregressive Models 11
100
 50
 0 50 100100
50
050100GPT rs
a Rej Sampling
100
 50
 0 50 100100
50
050100GPT ar random
 bσGPT
100
 50
 0 50 100100
50
050100  GPT ar causal c GPT
100
 50
 0 50 100100
50
050100Diffusion d Diffusion
Fig4 2D tSNE of textsmall3embeddings of 3000 sequences generated
by each method We compute the tSNE of all the embeddings together and
then we display in each graph the embeddings of the validation set green
the embeddings of the corresponding method blue and the embeddings of the
other methods gray We see that the embeddings of the generated sequences
have the same overall distribution compared to validation sets which seems to
indicate that GPTσGPT σGPT with burst sampling and diffusion models
can generate sequences of similar quality
in Table 5 We found that training in that order was as difficult for the model
as training in a random order and we noticed a small drop in performance com
pared to σGPT We suspect that this is due to the high discontinuity of the
order of the sequence which is such that two consecutive tokens are seen far
away in the sequences When predicting the first tokens the model therefore
cannot rely on information contained in neighboring tokens to make its predic
tion As the training behavior seen in models trained in random and fractal order
is similar we think that the drop in training efficiency comes more from the fact
that the model cannot exploit this neighboring information than changing the
order at every batch
Additionally models trained in a fractal cannot be used as such for infilling
and conditional density estimation and therefore cannot be used with our rejec
tion sampling scheme As the order is fixed for every batch it might not even
need to have a double positional encoding
36 Memorizing
As learning sequences in any direction is harder than modeling them under a
predefined order we also expect that the critical dataset size when the model
switches from memorization to generalization will increase We follow the same
hypotheses than 14 namely that the model has two mechanisms one general
izing and one memorizing the data As the mechanism of generalizing is more
efficient as the dataset grows it will be selected by gradient descent once the size
of the dataset gets beyond a critical size As learning in a random order is a
more difficult task we expect that generalization is more difficult in that setup
as well hence the memorization regime should hold for bigger dataset sizes
We reduce drastically the training dataset size in the case of the pathfinding12 A Pannatier et al
Textgeneration Path Solver Vertical Rate
Val Perp   Rand ord Test Acc   MSE  
σGPT 2446 9830 067 1414 41
Fractal GPT 2779 9800 194 1457 26
Table 5 Results for GPT trained in a fractal order compared to a standard
lefttoright GPT and random  σGPT order We found that training models
in this highly noncontinuous order is as hard as training them in a random
order and additionally models trained in that order cannot be used for condi
tional density estimation infilling or rejection sampling For text modeling we
report the model perplexity on the validation set in a random order for σGPT
and in fractal order for the fractal GPT as lefttoright validation perplexity is
meaningless for fractal GPT which did not see sequences in that order during
training
1000 10000 100000
N Training samplesCausal Fractal Randomordermem 
 99238gen 
 1000915gen 
 1000991
mem 
 98703mem 
 1000538gen 
 996940
mem 
 97301mem 
 1000386gen 
 972958Maze Memoïzation vs Generalization
20406080
Fig5 Number of examples needed to switch from memorization to generaliza
tion The model is trained on a restricted dataset size in the pathfinding task
We see that the model trained in a random order needs more examples to switch
from memorization to generalization At 1k samples both models are fully in a
memorization regime at 100k both generalize but in between at 10k the model
trained in a random order is still in a memorization regimeσGPTs A New Approach to Autoregressive Models 13
task and we present the results in Figure 5 Once it gets to 1000 examples both
models trained in lefttoright fractal and random order are in a memorizing
regime getting perfect accuracy on the training data but very low on the valida
tion data Conversely once the dataset gets bigger than 100k examples models
trained in all the different orders are in a generalization regime The transition
happens in between and we find that it happens faster in the lefttoright order
at around 10k samples the models trained lefttoright can generalize while
models trained in a random order are still in a memorization regime We see
also that models trained in a fractal order start generalizing faster than models
trained in a random order suggesting that the model can rely on seeing always
the same order to generalize more rapidly
37 Infilling and Conditional density estimation
We show in Figure 2 that our model can be used to infill the sequence by con
ditioning on the known part of the sequence In this figure the larger points are
part of the prompt and one can see the generated sequence complete sequence
that matches the prompt This figure also shows that our model has good esti
mates of the density at any point of the sequence We represent at each point
in the sequence the probability of the next sample given the known part of the
sequence as shades of gray the darker the more probable We see that during
generation the model sees multiple possible outcomes that are coherent with
the known part of the sequence They are then constrained to a single sequence
during the sampling For lefttoright trained models we can only have estimates
of the density for the next tokens and we cant know what the model estimates
for the rest of the sequence In the case of the pathfinding task we show in Fig
ure 3a that the model conditioned only on an empty maze has good estimates of
the true density of the optimal paths highlighting that the model has already
partially solved the problem before starting generation During sampling the
order of the generation and the sampling procedure influence which path is se
lected from this joint law We show as well in Section 23 that we can constrain
the generation of mazes and that the model can generate coherent samples based
on some prompted tokens that can be chosen on the fly
We also give some interactive examples of text generation in the supplemen
tary material
38 Tokenbased Rejection Sampling Scheme
We applied our tokenbased rejection sampling scheme to the problem of text
generation pathfinding and vertical rate forecasting Figures 6a to 6c We found
that in the three cases our method was able to generate samples of comparable
quality with an order of magnitude fewer steps than the autoregressive method
Wecomparedtodiscretediffusionmodelsaswellandwecanseethatourmethod
always outputs coherent samples while the samples generated by the diffusion
model are sometimes incoherent if the number of steps is not high enough In the14 A Pannatier et al
case of pathsolving and in the three synthetic tasks we see that at a comparable
number of steps for both methods our model shows better generation quality
We tested our tokenbased rejection sampling scheme on three synthetic
cases We estimate the number of steps required to generate the sequence using
a perfect model in Appendix B of the supplementary material We found that
our scheme was close to the optimal heuristics in all three cases In the case of
the product dataset requiring only one step to accept the sequence In the case
of the step dataset our scheme required four steps to accept the sequence In
the case of the joint law dataset our scheme required a few more steps which
is expected as the task is more complicated We see that it manages to generate
valid samples with a number of steps close to the optimal heuristics and with a
large increase in performance compared to diffusion models at the same step
1 10 100 500
Steps020406080100Error Rate
25500 250167125100502010522
11
a PathSolving
100101102
steps050100150200250300mse1251020
500
250100 50 25 10 51
2
4 b VerticalRate Pred
100101102103
Steps103Perplexity
 5
5042
5002
20
2501661
10
1001234 c Text Modeling
1 10 100 500
Steps020406080100Error rate
1251020501251020501122 5 10 20 50 100125167250 500Optimal
autoregressive
burst_sample
Diffusion
d Product Task
1 10 100 500
Steps020406080100Error Rate
125102050
500 25016712510050201052211 e Step Task
1 10 100 500
Steps020406080100Error rate
1251020505002501671251005020 10 5 2211 f Permutation Task
Fig6 We plot the performance vs steps of our σGPT used for autoregression
in random order blue to σGPT with rejection sampling per burst orange
against diffusion models gray We denote in the text the predefined number of
steps chosen for generation in the diffusion models For rejection sampling we
note the number of orders used for the evaluation We see that increasing the
numberofordersleadstoadecreasednumberofstepsForthesynthetictaskswe
also represent heuristics for the optimal number of steps needed to generate the
sequence gray line as described in Appendix B of the supplementary material
and we see that our scheme is close to this heuristicsσGPTs A New Approach to Autoregressive Models 15
4 Related works
41 Shuffling in language models
In exploring the landscape of shuffling used by autoregressive models one finds
noteworthy precedence in the work of 18 XLNet which uses randomization of
the sequence order as a pretraining task for sentence encoding in the context
of natural language understanding Our approach is similar with regards to the
shuffling of the sequence and the shuffled language modeling but differs in its
implementation we use double positional encoding and a regular causal mask
instead of relying on two streams and the modification of the attention matrix
The objective of the two models differs as well XLNET is used to encode in
formation similarly to BERT while our approach is generative A very recent
approach by 6 trains a transformer both on a lefttoright and righttoleft
order which solves a classic problem of the transformer model to understand
the context of the token in both directions Our approach by default trains the
model in any order and we can consider that it sees an exponential number of
different sequences during training However our section on memoization shows
that this does not serve as a good data augmentation technique and that the
model seems to memorize the data more in that case
42 Burst sampling scheme
Other works are trying to solve the problem of the linear time required by
autoregression using burst sampling Maskgit 3 for example uses a BERT
like masked language model MLM and uses a custom decoding scheme which
samples multiple tokens at the same time to generate an output The number of
tokens generated at each pass is fixed by the masking schedule and the model
utilizes a confidencedriven decoding scheme to choose which tokens to predict
In a distinct approach 10 introduced a method that leverages an auxiliary
model to guide the generation process Alternatively the approach outlined by
9 primarily focuses on generating preliminary drafts of an image which are
then progressively enhanced in subsequent iterations Following work in video
generation 216 are leveraging a MaskGit 3 like approach for generation as
autoregressive generation of video would be too costly Our rejection sampling
schemeallowsaswelltogeneratethesequencebyburstbutincontrasttoanother
schemethenumberoftokensacceptedisdynamicanddependsonthedatabeing
modelled which allows faster generation when the underlying data distribution
is simple
5 Conclusion
Training GPTlike models in different orders offer different desirable properties
It allows for the conditional prediction based on any subset of the tokens of the
sequence it naturally can be used for infilling and as the model can do partial16 A Pannatier et al
prediction we can leverage them to do rejection sampling and accept multiple
tokensatthesametimeduringgenerationOurfindingsindicatethatconditional
prediction learned by the models matches the theoretical partial distribution
showing that the model is indeed able to understand and reconstruct the signal
in any order As the training objective of modeling sequences in any order is
harder than training in a fixed order it has an impact on the training efficiency
and in a small dataset size we show that it leads to more memoïzation Finally
we showed that our model was able to generate sequences by burst using a
novel pertoken rejection sampling scheme reaching optimal heuristics in some
cases and decreasing the number of steps needed for generation by an order of
magnitude
Acknowledgement We thank Youssef Saied for his help and good remarks on
the overall project We thank Romain Fournier for his precious help on some
theoretical aspects of the analysis Arnaud Pannatier was supported by SkySoft
ATM for the project MALAT Machine Learning for Air Traffic and the Swiss
Innovation Agency Innosuisse under grant number 324321 IPICT  Evann
Courdier was supported by the Swiss Center for Drones and Robotics  SCDR
of the Swiss Department of Defence Civil Protection and Sport via armasuisse
ST under project No 05038
References
1 Austin J Johnson DD Ho J Tarlow D Van Den Berg R Structured de
noising diffusion models in discrete statespaces Advances in Neural Information
Processing Systems 34 1798117993 2021
2 Chang H Zhang H Barber J Maschinot A Lezama J Jiang L Yang
MH Murphy K Freeman WT Rubinstein M Li Y Krishnan D Muse
Texttoimage generation via masked generative transformers 2023
3 Chang H Zhang H Jiang L Liu C Freeman WT Maskgit Masked genera
tiveimagetransformerInProceedingsoftheIEEECVFConferenceonComputer
Vision and Pattern Recognition CVPR pp 1131511325 June 2022
4 Chen C Borgeaud S Irving G Lespiau JB Sifre L Jumper J Acceler
ating large language model decoding with speculative sampling arXiv preprint
arXiv230201318 2023
5 Gokaslan A Cohen V Openwebtext corpus httpSkylion007githubio
OpenWebTextCorpus 2019
6 Golovneva O AllenZhu Z Weston J Sukhbaatar S Reverse training to nurse
the reversal curse 2024
7 Ho J Jain A Abbeel P Denoising diffusion probabilistic models Advances in
neural information processing systems 33 68406851 2020
8 Kojima T Gu SS Reid M Matsuo Y Iwasawa Y Large language models
are zeroshot reasoners In Oh AH Agarwal A Belgrave D Cho K eds
Advances in Neural Information Processing Systems 2022
9 Lee D Kim C Kim S Cho M HAN WS Draftandrevise Effective image
generation with contextual rqtransformer In Koyejo S Mohamed S Agarwal
ABelgraveDChoKOhAedsAdvancesinNeuralInformationProcessing
Systems vol 35 pp 3012730138 Curran Associates Inc 2022σGPTs A New Approach to Autoregressive Models 17
10 Lezama J Chang H Jiang L Essa I Improved masked image generation with
tokencritic In Avidan S Brostow G Cissé M Farinella GM Hassner T
eds Computer Vision  ECCV 2022 pp 7086 Springer Nature Switzerland
Cham 2022
11 Merity S Xiong C Bradbury J Socher R Pointer sentinel mixture models
In International Conference on Learning Representations 2017
12 OpenAI New embedding models and api updates httpsopenaicomblog
newembeddingmodelsandapiupdates 2024 accessed 01032024
13 RadfordAWuJChildRLuanDAmodeiDSutskeverIetalLanguage
models are unsupervised multitask learners OpenAI blog 18 9 2019
14 Varma V Shah R Kenton Z Kramár J Kumar R Explaining grokking
through circuit efficiency arXiv preprint arXiv230902390 2023
15 Vaswani A Shazeer N Parmar N Uszkoreit J Jones L Gomez AN
Kaiser Lu Polosukhin I Attention is all you need In Guyon I Luxburg
UV Bengio S Wallach H Fergus R Vishwanathan S Garnett R eds
Advances in Neural Information Processing Systems vol 30 Curran Associates
Inc 2017 httpsproceedingsneuripsccpaper_filespaper2017file
3f5ee243547dee91fbd053c1c4a845aaPaperpdf
16 Villegas R Babaeizadeh M Kindermans PJ Moraldo H Zhang H Saffar
MT Castro S Kunze J Erhan D Phenaki Variable length video generation
from open domain textual descriptions In International Conference on Learning
Representations 2023
17 Wei J Wang X Schuurmans D Bosma M brian ichter Xia F Chi EH
Le QV Zhou D Chain of thought prompting elicits reasoning in large language
models In Oh AH Agarwal A Belgrave D Cho K eds Advances in Neural
Information Processing Systems 2022
18 Yang Z Dai Z Yang Y Carbonell J Salakhutdinov RR Le QV Xlnet
Generalized autoregressive pretraining for language understanding In Wallach
H Larochelle H Beygelzimer A d AlchéBuc F Fox E Garnett R eds
Advances in Neural Information Processing Systems vol 32 Curran Associates
Inc 2019
A Shuffled sequences are harder to learn
Using a random order in the training of a transformer often leads to a harder
task to learn To showcase this property we designed the following task The
model is asked to model a lazy random walk that starts from a multinomial
distribution made of four Dirac distributions at 100 120 130 140 with equal
probabilities The random walk evolves with the following law
pXtXt1 

2
5ifXtXt1 1
1
5ifXtXt1
0otherwise1
With pX0 100  pX0 120  pX0 130  pX0 140  1 4
In this setup predicting future tokens conditioned on past tokens is easy and
is given by
Xt3 NXt2t3t
08 218 A Pannatier et al
with t1 t 2 t 3
noteEX2  2041  020  0 8
The exact probability mass function pmf can also be computed
pXt3xXt2 nX
o0n
ono
u
pu
upd
dpo
o 3
with nt3t2unoxXt2dnou
Predicting tokens conditioned on future information is much harder in that
setup as the model has to take into account the initial multinomial distribution
and keep track of all the different possibilities
In such cases the exact pmf is given by 
pXt1xXt2 P
iSPi x t 1Px X t2 t2t1P
iSPi Xt2 t24
Where Pa b n   paths abinnsteps Pn
o0n
ono
u
 with un
oba
0 50 100 150 200 250000002004006008010X10X100115
data dist
theory
model
0 100 200 300 400050100150200250ModelX100100x
Fig7 Random walks at different altitudes results  Left Comparison with the
theoretical density at position 10 is hard to model as the model needs to take
into account the number of possible paths from each starting point  Middle
Empirical density  Right Learned density The model is capable of modeling
the density in two directions when conditioned on one position even if one order
is more complicated than the other
When trained with a lefttoright order the model quickly learns the lazy
random walk distribution but is never asked to learn more than that However
when trained with a random order the model has to learn to compute the more
complex multinomial distribution which is a much harder task as the model has
to learn the global statistics of the sequenceσGPTs A New Approach to Autoregressive Models 19
A1 Learning a harder problem Chain rule of probability
Shuffling the order of the tokens in a sequence has the effect of creating a harder
problem for the model But if the model has access to enough data and to learn
the distribution perfectly then shuffling should not change the results If the
model learned the final distribution perfectly the decoding order should not
matter in theory as stated by the chain rule indeed
pX1x1 X2x2     X TxT 
pX1x1pX2x2X1x1  
pXTxtX1x1    X T1xt1
pXσ1xσ1pXσ2xσ2Xσ1xσ1  
pXσTxσtXσ1xσ1    X σT1xσt15
For any permutation σ1 2T
σ1σ2σT

This is not what we see in practice as models are not perfect and datasets
are finite so the impact of the order has its importance
Another demonstration of this effect is the chain of thought stepbystep
prompting strategy 178 of large language models Where adding a few to
kens between the question and the answer increases the working memory of the
transformer which seems to help the models make fewer mistakes
B Estimation of number of steps for burst sampling
B1 Deterministic case and lucky samplings
In this section we describe the fully deterministic case and the lucky samplings
In the deterministic case and assuming a perfect model the model will accept
the sequence in one step Indeed if the model already knows the sequence the
probability distribution wont change when it is seen under a different order
therefore the rejection sampling ratio will always be one and the model will
accept the sequence in one step
In is the same for lucky samplings again assuming a perfect model and a
valid sample the probability of a token given a specific order can only be higher
than the probability of the token conditioned only on the prompt as the tokens
seen can only remove valid possibilities Hence qx pxand the model will
accept the sequence in one step
B2 Product Dataset Number of passes needed to generate a valid
sequence
In this section we give the number of steps required to generate a valid sequence
in the case of the product dataset using our rejection sampling scheme under20 A Pannatier et al
a perfect model The product dataset is made of a sequence of length 100 with
two classes 01 given by a Bernoulli law with p  10 At the beginning of the
generation a perfect model will predict pxt 1  0 1t 1     100
As all tokens are independent the conditioning of the model will not change the
probability of the next token Therefore the model will accept all tokens in the
first step of the rejection sampling scheme Our scheme should generate a valid
sample for this distribution in at most 1 step
B3 Step Dataset Number of passes needed to generate a valid
sequence
In this section we give the number of steps required to generate a valid sequence
in the case of the step dataset using our rejection sampling scheme under a
perfect model The step dataset is made of a sequence of length 100 of classes
01The sequence is 0 everywhere except on a step of ones of length 10 placed
randomly in the sequence At the beginning of the generation a perfect model
cannot do better than predicting pxt 1  01t 1     100
Sampling with this law will lead to a sequence with 10 ones on average The
number of accepted tokens in the first round of rejection sampling will then
depend on the random order sampled The first token given by the order will
always be accepted as qxt pxt Now the next tokens are either com
patible with the tokens already accepted or not If the next token is compatible
with the already accepted tokens then it will be accepted with probability 1
aspxt qxtxaccepted  If it is not compatible then it will be rejected as
qxtxaccepted   0and the rejection sampling stops rejecting all the remaining
tokens
Now the reason for noncompatibility in the step dataset is either that
a the model has already seen a 1 and sees a second 1 that is too far away
b that the model sees a one and then a zero nearby which would mean the
end of the sequence which is not long enough or
c that it has seen only 0s and no places are remaining for 1s Therefore it
has to reject the zero that would lead to an empty sequence 1 In case ab
the model accepts a one in the first step and in case c the start of the step is
confined between the two last zeros where there are still enough places 2 Once
a token from the step is accepted or there is only one place for a step the signal
is deterministic and a perfect model will 3 sample a complete step Our scheme
should generate a valid sample for this distribution in at most 3 steps
B4 Permutation
When wanting to generate a valid permutation per burst assuming a perfect
model when given a partially completed permutation a perfect model cannot
do better than predicting a uniform law on the remaining tokensσGPTs A New Approach to Autoregressive Models 21
There is a wellknown formula for the number of different classes that one
gets when sampling with a uniform law it is given by
E of incoherent tokens  6
TE classes being sampled  7
 T1TTT18
More generally the probability of sampling exactly k classes is given by
psampling exactly kclasses  T
k
kT
k
TT9
10
WhereT
k
is the Stirling number of the second kind
Having a closed formula for the expected number of steps is hard but we
can estimate it numerically using Equation 9 For a sequence of length 100
we find that the expected number of steps is 52 with a standard deviation of
06 Which gives us a lower bound for the number of steps required to generate
a valid permutation
Note that in this specific case for our rejection sampling scheme to be able to
generate a valid permutation in this number of steps the model needs to validate
the partial sequence under an order which is such that each unique classes are
seen first because once a class is repeated the model will have to reject the rest
of the sequence
C Caching Scheme for Burst rejection sampling
Autoregressive transformers usually rely on a KVcaching mechanism to go from
aON3toaON2costofgenerationthisKVcachecanbeadaptedtogenerate
a sample by burst If we already have accepted a set of tokens T1and we have
a KV cache K1 V1that stores the keys and values for this set of tokens and we
want to generate a prediction for all the remaining set of T2tokens in parallel
The output of the transformer corresponding to the remaining set of tokens
is
V
2 softmax
QT2KT
1DV1
V2
11
With D diag qiviThis formula parallelizes easily We give the corre
sponding code in pytorch
1def burst self  kv_cache  q k v
2 k1  v1  kv_cache get 
3
4 att1  q  k1 transpose 2 122 A Pannatier et al
5  no masking on KV cache  tokens can see them all
6
7 att2  q  ksum 1 keepdim  True 
8 att  torch cat  att1  att2  dim  1  10  math
 sqrt k size  1
9 att  F softmax att  dim  1
10 att  self  attn_drop att
11
12 att1  att    1
13 att2  att    1
14
15  Works even if the cache is empty
16 y  att1  v1  att2  v
17 y  rearrange y b h t e  b t h e
18
19 y  self  resid_drop  self  proj y
20
21 return y
Listing 11 KV Caching scheme of burst sampling
D Additional experiments for the vertical rate forecasting
We present here additional results concerning the vertical rate modeling experi
ment Table 6 We report the Mean Square Error MSE compared to the ground
truth when prompted by partially completed trajectories The idea is the see
the effect of partial lefttoright conditioning on the quality of the generated
sequences 0 10 and 50 denote the percentage of the actual flight that is
given as a prompt to the model In this setup we see that σGPT outperforms
models trained causally for a generation As the dataset size is small we noticed
that models trained in a lefttoright manner were suffering from a repetition
problem that arises when the modeling capabilities of the models are insuffi
cient As the prompting is given to the model in lefttoright order we see that
causal models can outperform random models when prompted with half of the
actual sequence
Weseeaswellthatdiffusionmodelsusuallyoutperformautoregressivemodels
in this task However they need to be retrained to be able to generate sequences
conditioned on a partial sequence We see that the autoregressive models can be
usedinamoreflexiblewayastheycanbeusedtogeneratesequencesconditioned
on a partial sequence without retrainingσGPTs A New Approach to Autoregressive Models 23
Table 6 This table presents the Mean Square Error MSE results for the climb
ingrateforecastingtaskTheMSEiscalculatedontheentiregeneratedsequence
conditioned on different points during the climb the start 0 early stage 10
of the climb midway 50 of the climb and in the middle of the first climb
For each validation sequence we generated 20 sequences autoregressively using
the model following three different schemes causal scheme random scheme and
binary search tree order We also report the performance of diffusion models for
comparison but we only report their performance for the entire sequence as they
need to be retrained to be able to generate sequences conditioned on a partial
sequence
Size Method 0 10 50 Mid Climb
SmallFractal 145726 12789 1619 14666 1722 6236 844
Causal 2748707 1799 480529127461100
Random 141441 1238 60 638 12 401 27
Diffusion 10594 13   
BaseFractal 158159 25235 139 16235 2835 8844 157
Causal 4247109 2877 05 700 57 615 68
Random 146364 1193 78 653 51 416 07
Diffusion 10796 24   
TinyFractal 152387 25536 1683 15088 2203 9728 449
Causal 2909130 2240 18 589 16 539 57
Random 12952371081 61 7264736708
Diffusion 12328 69   
  