Auffusion: Leveraging the Power of Diffusion and Large Language Models for
Text-to-Audio Generation
Beijing University of Posts and Telecommunications
{jinlong xue, yayue.deng, yingming.gao, yli01 }@bupt.edu.cn
Abstract
Recent advancements in diffusion models and large lan-
guage models (LLMs) have significantly propelled the field
of AIGC. Text-to-Audio (TTA), a burgeoning AIGC appli-
cation designed to generate audio from natural language
prompts, is attracting increasing attention. However, ex-
isting TTA studies often struggle with generation quality
and text-audio alignment, especially for complex textual
inputs. Drawing inspiration from state-of-the-art Text-to-
Image (T2I) diffusion models, we introduce Auffusion, a
TTA system adapting T2I model frameworks to TTA task,
by effectively leveraging their inherent generative strengths
and precise cross-modal alignment. Our objective and sub-
jective evaluations demonstrate that Auffusion surpasses
previous TTA approaches using limited data and computa-
tional resource. Furthermore, previous studies in T2I rec-
ognizes the significant impact of encoder choice on cross-
modal alignment, like fine-grained details and object bind-
ings, while similar evaluation is lacking in prior TTA works.
Through comprehensive ablation studies and innovative
cross-attention map visualizations, we provide insightful as-
sessments of text-audio alignment in TTA. Our findings re-
veal Auffusions superior capability in generating audios
that accurately match textual descriptions, which further
demonstrated in several related tasks, such as audio style
transfer, inpainting and other manipulations. Project page
is available at https://auffusion.github.io.
1. Introduction
Text-to-audio (TTA) generation is an emerging application
that focuses on synthesizing diverse audio outputs based
on text prompts. With the integration of artificial intelli-
gence into the realm of AIGC, the scope of TTA appli-
cations has expanded significantly, covering areas such as
movie dubbing and musical composition. Early TTA mod-
els, as referenced in [22, 29], primarily relied on one-hot
labels, leading to the generation of monotonous audio con-
*Corresponding author.strained by limited label space and generative capacity. In
contrast, natural descriptive text delivers more comprehen-
sive and fine-grained information. Thereby, the following
works [9, 16, 19, 27, 28, 55] develop their models based on
textual content.
Recent advancements in diffusion generative models
[15, 48] and large language models [6, 30, 39, 40, 47]
have showcased remarkable capabilities in content gener-
ation and understanding. Leveraging these advancements,
the first diffusion based TTA Diffsound [55] outperforms
previous TTA systems by generating discrete tokens quan-
tized from mel-spectrograms using a diffusion model. Later
Diffsound is surpassed by AudioGen [24] using an autore-
gressive model in a discrete space of waveform. Inspired
by [43], AudioLDM [27] is the first to utilize a contin-
uous latent diffusion model (LDM) [43], achieving better
quality and computational efficiency compared to other dis-
crete token based TTA systems [24, 55]. Similarly, many
well-performed TTA systems, including AudioLDM2 [28],
Tango [9], Make-an-Audio 1/2 [16, 19], integrate LDM into
their TTA frameworks to facilitate denoising processes in
the latent space. However, these models still require ex-
tensive computational resources and large-scale datasets for
training from scratch. Moreover, these models only em-
phasis coarse-grained performance and neglect fine-grained
text-audio alignment. Our work concentrates on addressing
these two critical challenges, providing effective solutions
and valuable insights.
In developing a powerful TTA model, two primary ob-
jectives are paramount: 1) mastering the distribution of
natural audio, and 2) achieving precise text-audio align-
ment. These competencies are also paralleled in T2I [2, 45]
tasks, where robust generative abilities and accurate text-
image alignment are similarly crucial. To this end, we in-
troduce our TTA system Auffusion that adapts Latent Dif-
fusion Model (LDM) originally pretrained for T2I tasks.
This adaptation enables Auffusion to leverage the LDMs
inherent generative strength and transfer it alignment under-
standing effectively for text-audio alignment in TTA appli-
cation. The comprehensive subjective and objective evalua-
1arXiv:2401.01044v1  [cs.SD]  2 Jan 2024tion metrics show our proposed system Auffusion achieves
better quality and alignment. Moreover, Auffusion demon-
strates performance comparable to other baseline models
trained on datasets 60 times larger. Riffusion [7], an app
for 5-second music generation, also uses a pretrained LDM
[43]. However, it is specifically designed for music and does
not extend to broader TTA tasks. Additionally, Riffusion
simply quantizes audio signals into images, a non-reversible
process that leads to a great precision loss. In contrast, our
Auffusion model features a carefully designed feature space
transformation pipeline, enabling lossless audio conversion.
On the other hand, the text encoder serves as a critical
bridge between text and audio, representing a key compo-
nent in TTA systems. Different from the extensive stud-
ies [1, 17, 45] conducted in the T2I domain, where the im-
pact of text encoders on aspects such as fine-grained de-
tails and object bindings has been widely explored, their
influence in TTA has not been thoroughly explored. Gen-
erally, text encoders in existing TTA systems fall into two
categories: 1) multi-modal contrastive learning-based mod-
els, such as CLIP [39] and CLAP [6], and 2) text-only
large language models (LLMs) like BERT [4] and T5 [40].
However findings from previous studies can sometimes be
contradictory. Diffsound [55] employs CLIP, pretrained
on text-image pairs, claiming superior performance over
text-only model BERT. Conversely, AudioLDM [27] uses
CLAP model, pretrained on text-audio pairs, suggesting ad-
vantages using audio only features over using combined
audio-text features or text-only features. Building upon
the same LDM used in AudioLDM, Tango [9] owes dif-
ferent opinions. They advocate for instruction-tuned LLMs
(Flan-T5) [47] to better grasp textual descriptions and cross-
modal correlations, challenging the notion of embedding
audio and text in a shared space. However, comprehensive
ablation studies using these text encoders are lacking.
To address the debates outlined above, our study con-
ducts a thorough investigation into the performance of var-
ious text encoders and baseline models. Moving beyond
traditional evaluation metrics, we innovatively assess text-
audio alignment by visualizing the cross-attention map to
provide intuitive observation for the first time in TTA task.
We find that our model achieves better fine-grained text-
audio alignment. Additionally, we demonstrate versatile
audio manipulations enabled by our models generative ca-
pacity and clear text-audio alignment.
In summary, the contributions of our work are: 1) We
propose Auffusion, a TTA model that integrates a pow-
erful pretrained LDM from T2I in order to inherit gen-
erative strengths and enhance cross-modal alignment, and
our method demonstrates superior performance compared
to existing TTA systems; 2) We conduct an extensive in-
vestigation into the performance between different text en-
coders, and we provide a novel and insightful demonstrationin TTA task to assess text-audio alignment utilizing visual-
izations of cross-attention maps across different models.
2. Related Work
2.1. Text-to-Image Synthesis
Text-to-Image (T2I) synthesis, particularly through diffu-
sion models, has seen significant advancements in recent
years. Pioneering models like DALL-E [41] treats T2I as
a sequence-to-sequence translation task, encoding images
into discrete latent tokens using pretrained VQ-V AE [52].
DALL-E 2 [42] employs the CLIP text encoder and two
diffusion models, first predicting CLIP [39] visual features
from text and then synthesizing the image. Another famous
model Imagen [45] uses the T5 encoder [40] for text feature
extraction and a cascade of diffusion models for initial im-
age synthesis and subsequent super-resolution. Stable Dif-
fusion [43] optimizes computational efficiency by mapping
images from pixel to compressed latent space using a con-
tinuous V AE trained with discriminators, followed by dif-
fusion in this latent space. These models [36, 42, 43, 45]
demonstrate remarkable diversity and quality in image gen-
eration, guided by text prompts and operating either directly
in image space or within a latent space.
2.2. Text-to-Audio Synthesis
Text-to-Audio (TTA) synthesis has witnessed significant
advancements. Diffsound [55] leverages VQ-V AE [52]
model trained on mel-spectrograms and convert them into
discrete codes, where a non-autoregressive token based dif-
fusion model is then used to generate audio signals. Sim-
ilarly, AudioGen [24] uses a VQ-V AE based approach [3]
but focuses on encoding raw waveform data and employs
an autoregressive model for generation. Other studies
include the use of Latent Diffusion Models (LDMs), as
seen in AudioLDM 1/2 [27, 28], Make-An-Audio 1/2 [16,
19], and Tango [9]. AudioLDM [27], utilizes audio fea-
tures extracted by a pretrained contrastive text-audio model
CLAP [6] as a condition during training, while leveraging
text features during inference. This approach benefits from
CLAPs ability to map audio and captions to a shared latent
space. AudioLDM2 [28] first employs an auto-regressive
model (AR) to generate AudioMAE [18] features from text,
then uses them to condition the LDM. These two methods
both alleviate the reliance of audio-text pair data. Other
methods [9, 16, 19], on the other hand, employ text features
in both training and inference stages. Make-An-Audio [19]
LDM is similar to AudioLDM. Make-An-Audio2 [16] em-
phasize the temporal information by changing 2D spatial
structure to 1D temporal structure, and they additionally
replace U-Net design to transformer. However, neither of
these two models is open source.
2Figure 1. An overview of Auffusion architecture. The whole training and inference process include back-and-forth transformation between
four feature spaces: audio, spectrogram, pixel and latent space. Note that U-Net is initialized with pretrained text-to-image LDM.
3. Auffusion
3.1. Overview
Our proposed method Auffusion, as depicted in Fig. 1 has
four main components: 1) text encoder; 2) latent diffusion
model (LDM); 3) pixel V AE; 4) HiFi-GAN vocoder. In or-
der to achieve the TTA task and utilize the powerful pre-
trained model from T2I task, the whole process involves
conversion between four feature spaces: audio, spectro-
gram, pixel and latent space. The spectrogram feature is a
key proxy that bridges the audio space and pixel space. Dur-
ing training, audio is first converted into mel-spectrogram
and normalize for image space, then LDM is conditioned on
textual embeddings extracted by textual condition encoder
and trained in the pixel space learned by V AE. In inference,
this process is reversed: starting from standard Gaussian
noise, the latent representation is generated through reverse
diffusion process conditioned on text embeddings. There-
after the pixel V AE decoder reconstructs the pixel space and
generated image is denormalized into mel-spectrogram. Fi-
nally, the pretrained HiFi-GAN vocoder synthesizes the au-
dio from this mel-spectrogram.
3.2. Feature Space Transformation
Given an audio-text pair, we first convert the audio signal
xaudioRTinto mel-spectrogram xmelRdl, where d
andlrepresent the mel-channels and the number of frames
respectively. In order to transform mel-spectrogram into
image-like data without precision loss, we conduct nor-
malization by utilizing the mean µand variance δcalcu-
lated from the whole dataset rather than on individual mel-
spectrogram [7]. The normalized spectrogram xnorm canbe viewed as gray-scale image and then converted into RGB
image data xRcdl, where c,dandlare referred to as
the image channel, height and width respectively.
3.3. Latent Diffusion Model
To guide the construction of the audio signals pixel dis-
tribution z0using a text prompt τ, we fine-tune the U-Net
diffusion module by minimizing mean squared error (MSE)
in the noise space. The objective function is defined as:
ℓθ=||ϵθ(zt, t, τ)ϵ||2
2 (1)
Here, ϵ N (0, I)represents Gaussian noise, tis a ran-
dom time step, and ϵθis the text-guided denoising network,
comprising a U-Net with a cross-attention component for
text guidance τ.
In this process, the V AE encoder processes the image-
like input xinto compressed latent vector z0. The diffusion
process then operates in this latent space, gradually trans-
forming z0into Gaussian noise zT. The model is trained
to reverse this transformation, recovering the original data
distribution from the noise. This process involves two key
steps: the forward process that converts z0intozTand the
reverse process that recovers z0from zT.
Forward process is defined by a fixed Markov chain
from data z0to the latent variable zT.
q(z1, . . . , z T|z0) :=TY
t=1q(zt|zt1) (2)
The entire procedure transforms the initial latent data z0
into noise latent variables zTin accordance with a prede-
termined noise schedule β1, . . . , β T.
q(zt|zt1) :=N(zt;p
1βtzt1, βtI) (3)
3where βtis a small positive constant. q(zt|zt1)represents
a function where a small Gaussian noise is added to the dis-
tribution of zt1.
Reverse process converts the latent variables from zTto
z0with learnable parameters θ, aimed at recovering samples
from Gaussian noise zT N(0, I).
pθ(z0, . . . , z T1|zT) :=TY
t=1pθ(zt1|zt) (4)
pθ(zt1|zt) :=N(zt1;µθ(zt, t),Σθ(zt, t)) (5)
Note that µθtakes the diffusion step tNand variable
ztas inputs and outputs zt1for each iteration.
3.4. Conditioning Processes
The previous work AudioLDM [27] adopts concatenation
operation between pooled text embedding extracted from
CLAP and time embedding to guide the generation pro-
cess in LDM. By contrast, we turn diffusion model gener-
ation into more flexible and understandable by conducting
a cross-attention mechanism [53] between conditional em-
bedding sequence and latent vectors in the U-Net backbone.
More formally, we donates ϑi(zt)RNdi
ϵa interme-
diate representation of the i-thlayer of U-Net estimation
ϵθ. Then, a linear projection is applied to the deep spatial
features of the noisy data ϑi(zt).
Q=W(i)
qϑi(zt) (6)
The conditional embedding τis also projected via learned
linear projections.
K=W(i)
kτ, V =W(i)
vτ, (7)
where W(i)
qRddτ,W(i)
vRddi
ϵandW(i)
kRddτ
are learnable matrices. The attention value and attention
score are calculated as follows:
Attention (Q, K, V ) =score (Q, K)V
score (Q, K) =softmax (QKT

d)(8)
The condition approach allows us to visualize the 2D at-
tention map [12, 35, 50] by reshaping attention score back
to latent image shape. Furthermore, it provides an intuitive
measurement to access the understanding ability of various
text encoders. We discuss the compared results in Sec. 4.3.
Meanwhile, based on the visualized attention map, we find
that the pretrained LDM is capable of adequately transfer-
ring cross-modal understanding ability from T2I to TTA
task, resulting in better alignment. Overall, we highlight
the importance of the conditioning process in enhancing the
audio-text models ability to extract key information from
text descriptions and accurately match the desired audio, as
demonstrated in Fig. 2.3.5. Text Encoder
Inspired by eDiff-I [1] who uses an ensemble of encoders
to provide multi-source information to LDM, we combine
CLAP and FlanT5 text encoders as conditions. We use ran-
dom dropout on each of these embeddings independently
during training. When all two embeddings are dropped, it
corresponds to unconditional training, which is useful for
performing classifier-free guidance [14]. We conduct com-
prehensive comparison for various text encoders and results
are shown in Sec 4.3.
3.6. Classifier-Free Guidance
To guide the reverse diffusion process, we utilize classifier-
free guidance [14] based on the text input τusing:
ˆϵθ(zt, t, τ) = (1 + w)ϵ(zt, t, τ)wϵ(zt, t)(9)
At the inference stage, the guidance scale wdetermines how
much the text input influences the noise estimation ˆϵθcom-
pared to the unconditional estimation. We randomly discard
the text condition at a rate of 10% during training.
4. Experiments
4.1. Experimental Setup
Dataset. We follow previous works [9, 16, 27] and use a va-
riety of different audio datasets with audio caption or audio
labels to train our model, including AudioCaps (AC) [20],
WavCaps [33], MACS [32], Clotho [5], ESC50 [38], Urban-
Sound [46], Music Instruments dataset1and GTZAN [51].
The WavCaps dataset consists of ChatGPT-assisted weakly-
labeled audio captions for the FreeSound2, BBC Sound Ef-
fects (SFX)3, SoundBible4and the AudioSet strongly la-
beled subset [11], containing 403,050 audio clips with an
average duration of 68 seconds. AudioCaps is a subset of
AudioSet (AS) [8] with handcrafted captions and it contains
about 46K ten-second audio clips. This results in a dataset
composed of 0.47 million audio text pairs, with a total du-
ration of approximately 7.7K hours.
It is noted that the duration of the audio samples in
AudioSet and AudioCaps is 10 seconds, while it is much
longer in FreeSound and BBC SFX datasets (86s and 115s
in average). To avoid the imbalance caused by longer au-
dio, which often contains repeated sounds like background
sounds, we only use the first thirty seconds of audios for
all datasets and randomly select ten-second segments dur-
ing training. Finally, we have in total 0.4M audio samples
with a total duration of around 2K hours for model training.
1https://www.kaggle.com/datasets/soumendraprasad/musical-
instruments-sound-dataset
2https://freesound.org/
3https://sound-effects.bbcrewind.co.uk/
4https://soundbible.com/
4Table 1. The comparison between our model Auffusion and baseline TTA models. Although our model Auffusion is only trained on a
much smaller dataset AC, our model outperforms other baselines on AC test set and has comparable zero-shot result in Clotho test set.
Model Pretrain Duration(h) ParamsAudioCaps Clotho
FD FADKL IS CLAP FD FADKL IS CLAP
Riffusion  1990 1.1B 26.28 4.68 1.57 7.21 47.6% 31.63 6.11 2.66 6.50 47.5%
AudioGen-v2-medium % 6824 1.5B 17.86 1.73 1.59 9.31 48.5% 23.26 2.55 2.56 7.19 46.7%
AudioLDM-S-full-v2 % 9031 421M 30.58 4.40 1.79 6.96 42.1% 26.51 3.54 2.62 6.58 49.9%
AudioLDM-L-full % 9031 975M 29.77 4.04 1.78 7.50 42.9% 24.13 3.02 2.56 7.49 51.0%
AudioLDM2 % 29510 1.1B 26.05 1.94 1.76 7.31 46.0% 23.53 3.06 2.47 9.05 48.1%
AudioLDM2-large % 29510 1.5B 25.59 2.19 1.70 7.83 47.9% 23.31 3.00 2.41 8.88 49.0%
Tango % 145 1.3B 24.82 1.77 1.43 7.20 55.0% 31.67 3.22 2.57 7.18 46.6%
Tango-Full % 3400 1.3B 30.68 3.67 1.63 4.79 51.9% 25.83 3.17 2.35 6.51 50.3%
Auffusion % 145 1.1B 24.45 2.25 1.39 10.14 54.7% 29.01 2.67 2.66 9.46 47.6%
Auffusion  145 1.1B 21.99 1.63 1.36 10.57 55.3% 25.64 2.35 2.59 9.01 48.2%
Auffusion-Full % 1990 1.1B 24.11 1.67 1.46 8.39 51.6% 19.14 1.99 2.42 10.33 52.8%
Auffusion-Full  1990 1.1B 23.08 1.76 1.36 10.28 55.6% 17.97 1.96 2.38 11.29 55.0%
Training Setup. We utilize the pretrained Stable Dif-
fusion v1.55, including its V AE and U-Net, and later fine-
tune the U-Net on audio datasets. All datasets are re-
sampled to 16kHz sampling rate and mono format, with
samples padded to 10.24 seconds. We then extract mel-
spectrograms from audios using parameters of 256 mel fil-
ter bands, 1024 window length, 2048 FFT, and 160 hop
size, resulting in (1,256,1024) mel-spectrograms, akin to
grayscale images with 256 height and 1024 width in 1 chan-
nels. These are normalized and channel-repeated to cre-
ate RGB-like images suitable for V AE encoder input. For
high-fidelity audio conversion, previous methods adopt neu-
ral vocoder [21, 25]. In order to match our need using our
specific mel-spectrogram parameters, we train a new HiFi-
GAN vocoder [21] using the same datasets described above.
This training employs the AdamW optimizer [31] with a 2e-
4 learning rate and 16 batch size on one A6000 GPU. Fi-
nally, we freeze the text encoder and finetune the pretrained
Stable Diffusions U-Net using AdamW optimizer with a
3e-5 learning rate, at a 20 batch size for 100K steps. Our
model can be trained only taking a total of 48 hours on one
A6000 GPU.
Objective Evaluation. In our experimental evalua-
tion, we follow previous evaluation methods [9, 16, 27]
and employ a suite of objective metrics to assess the
quality and fidelity of generated audio samples, includ-
ing Frechet Distance (FD), Frechet Audio Distance (FAD),
KullbackLeibler (KL) divergence, Inception Score (IS)
and CLAP score. Analogous to the Frechet Inception Dis-
tance (FID) [13] used in image synthesis, the FD score in
audio domain quantifies the global similarity between cre-
ated audio samples and the target samples without the need
of using paired reference audio samples. The IS score is ef-
5https://huggingface.co/runwayml/stable-diffusion-v1-5fective at assessing both the quality and variety of samples.
The KL score is calculated using paired samples and it mea-
sures the divergence between two probability distributions.
These three metrics are all grounded in the advanced audio
classifier PANNs [23]. FAD score has a similar idea to FD
but it uses VGGish [10] as feature extractor. The evaluate
suite that we uses for FD, FAD, KL and IS is in project6. Be-
sides, we also use pretrained CLAP7model to compute the
similarity of the text caption and generated audio to evaluate
the text-audio alignment, similar to CLIP score.
Subjective Evaluation. Following previous evaluation
method [9, 24] in TTA field, we ask five human evaluators
to assess two aspects of the generated audio, including over-
all audio quality (OVL) and relevance to the text caption
(REL). We randomly select 30 audio samples from each of
the AC and Clotho test sets and ask participants to rate them
on a scale from 1 to 100 with 10-point intervals. Results are
shown in Table 3.
Baseline Models. To comprehensively compare our
models with others, our study employs five baseline mod-
els, including three diffusion based models Riffusion [7],
AudioLDM [27], AudioLDM2 [28], Tango [9], and one
auto-regressive generative model based on discrete audio
token AudioGen [24]. We re-implement Riffusion8, orig-
inally trained on music datasets for only 5s audio, to gen-
erate 10s audio using a 160 hop length and trained it on
our datasets. We use the other baseline models released
by authors on huggingface respectively. AudioLDM-S-
full-v29and AudioLDM-L-full10have 412M and 975M pa-
rameters and trained them on AudioCaps, AudioSet and
6https://github.com/haoheliu/audioldm eval
7https://huggingface.co/laion/clap-htsat-unfused
8https://huggingface.co/riffusion/riffusion-model-v1
9https://huggingface.co/cvssp/audioldm-s-full-v2
10https://huggingface.co/cvssp/audioldm-l-full
5other 2 datasets including 9031h audio data for more than
1.5M train steps. AudioLDM211and AudioLDM2-large12
have 1.1B and 1.5B parameters respectively and trained on
29510h diverse audio data. Tango is trained on Audio-
Caps dataset and Tango-Full is trained on datasets similar
with our datasets settings but using different preprocessing.
AudioGen-v2-medium13has 1.5B parameters and is trained
on AudioCaps, AudioSet, and eight other datasets around
4K hours data.
4.2. Results
Evaluation Setup. We compare our model Auffusion
trained on single dataset AudioCaps (AC) and Auffusion-
Full trained on whole datasets with other baselines in both
AudioCaps test set and Clotho test set. We also conduct ab-
lation studies on the impact of pretrained SD models. Both
Auffusion and Auffusion-Full use CLIP as default text en-
coder. We report our result in Table 1.
Objective Evaluation. When trained solely on AC
dataset, our model Auffusion-with-pretrained outperforms
the previous state-of-the-art Tango in AC test set, with 21.99
FD, 1.63 FAD, 1.36 KL, 10.57 IS and 55.3% in CLAP
score, and achieve comparable zero-shot Clotho test set re-
sults to other baseline models trained on datasets over 60
times larger. Notably, Tango and Auffusion-no-pretrained
both trained solely on AC dataset exhibit a huge drop on
Clotho test set, indicating a problem of overfitting. In con-
trast, our Auffusion-with-pretrained still maintain its perfor-
mance, demonstrating generalization ability. This suggests
that the generative capacity and cross-modal alignment of
the pretrained SD model can be effectively transferred to
mel-spectrogram domain, even with a small dataset.
When trained in much larger datasets using same training
steps, Auffusion-Full-with-pretrained achieves the state-
of-the-art performance in Clotho test set. Besides, it
shows a negligible decrease in AC test set compared to
Auffusion-with-pretrained, and records a slight increase in
CLAP score. This indicates the robustness and strong
generalization ability of the pretrained SD, even when
dealing with different data distributions. Additionally,
both our Auffusion-with-pretrained and Auffusion-Full-
with-pretrained models significantly outperform other base-
lines in terms of IS and CLAP scores. A higher IS score
implies that our model can generate mel-spectrograms with
both high fidelity and diversity. A higher CLAP score indi-
cates our models enhanced capability to adhere to textual
descriptions and produce more relevant audio. We also find
that our re-implement Riffusion yields much inferior re-
sults, indicating that the precision loss caused by the quan-
tization transform has a great impact.
11https://huggingface.co/cvssp/audioldm2
12https://huggingface.co/cvssp/audioldm2-large
13https://huggingface.co/facebook/audiogen-mediumSubjective Evaluation. Our subjective human evalua-
tion results are presented in Table 3. In the All Event
column, our model Auffusion-w-clip demonstrates superior
performance over other baseline models, achieving an OVL
score of 69.36 and a REL score of 70.25. Additionally, the
REL has significant gains compared to other models, show-
ing strong text-audio alignment. We delve deeper into the
impact of numbers of events and provide intuitive visualiza-
tion for text-audio alignment in Sec. 4.3.
4.3. Analysis
Effect of Text Encoder. To assess the performance of dif-
ferent text encoders and explore the effectiveness of a dual
text encoder approach in TTA applications, we compared
several encoder options including CLIP, CLAP, FlanT5-
base, FlanT5-large, and a combined CLAP and FlanT5-
large encoder. The original SDv1.5 uses the CLIP L/14
model, trained on text-image pairs, while AudioLDM em-
ploys the CLAP model, trained on text-audio pairs. Tango
suggests that FlanT5, an instruction-tuned LLM, enhances
textual understanding, but lacks an encoder ablation study.
Inspired by eDiff-I [1], which uses an ensemble of encoders
to provide multi-source information, we experimented with
a combined CLAP and FlanT5-large encoder by concatena-
tion. The results are shown in Table 2.
Our findings reveal that FlanT5-large surpasses FlanT5-
base in all evaluation metrics, underscoring the importance
of the text encoders size for understanding textual cap-
tions. FlanT5-large shows results comparable to CLIP, with
CLIP excelling in IS score and FlanT5-large in FAD score.
This mirrors Imagen [45] findings, where T5-XXL matches
CLIP in objective scores but exceeds smaller T5 models.
Notably, the CLAP model outperforms both text encoders,
especially in FAD and CLAP scores, demonstrating its ad-
vanced audio domain expertise. The elevated CLAP score
may be attributed to the use of same model during train-
ing. Combining CLAP and FlanT5-large encoders lever-
ages both acoustic and rich semantic knowledge, yielding
the best overall objective performance.
Text-Audio Alignment. In our investigation into the im-
pact of varying text encoders on TTA alignmenta crucial
aspect of TTA taskswe are the first to examine the cross-
attention mechanisms between text and LDM outputs using
method [12]. This approach allows us to intuitively observe
the focal points of the LDM during the TTA process. How-
ever, due to the global conditioning approach employed by
AudioLDM and the use of GPT for generating AudioMAE
features in AudioLDM2, these models do not provide a
direct correlation between text and LDM. Therefore we
present a comparative visualization of cross-attention maps
using various encoders within the Auffusion framework,
and the Tango models who also adopt cross attention, as
depicted in Figure 2. For consistency, we standardize the
6Figure 2. The visualization of cross attention maps for Auffusion with different text encoders and Tango model. Auffusion-no-pretrain use
fixed CLIP encoder and LDM is trained from scratch. The LDMs in 2 to 4 rows are initialized with SDv1.5 with different encoders. The
last row shows the Tangos cross attention map, and Tango uses FlanT5-large as condition encoder.
Table 2. The results evaluated on AudioCaps test set and Clotho test set with different settings of conditional encoder.
AudioCaps Clotho
Auffusion-Full-with FD FADKL IS CLAP FD FADKL IS CLAP
CLIP 23.08 1.76 1.36 10.28 55.6% 17.97 1.96 2.38 11.29 55.0%
CLAP 21.92 1.57 1.35 10.01 58.3% 17.79 1.82 2.32 11.02 59.1%
FlanT5-base 23.00 1.55 1.50 10.11 53.0% 20.05 2.03 2.50 10.88 52.9%
FlanT5-large 22.31 1.41 1.42 9.37 54.6% 18.09 1.62 2.35 10.16 55.6%
Clap+FlanT5-large 22.55 1.50 1.32 10.34 57.4% 17.59 1.87 2.25 10.93 59.5%
diffusion steps to 50 and adjust all cross-attention maps to a
uniform square dimension for clear comparison.
Upon comparing the attention maps of the first and sec-
ond lines, it is evident that using pre-trained LDM exhibits
superior distinguishability, with clear attention across al-
most all tokens. Notably, the gunshot token within the
highlighted red area is prominent, and the dogs and gun-
shot sounds in the generated audio overlap in the latter
section. This suggests that the pre-trained LDM possesses
advanced prior knowledge, enabling it to effectively trans-
fer its text-image alignment capabilities to text-audio align-
ment tasks. Although the CLAP model achieves higher
objective scores, it surprisingly produces similar attention
maps for each token. Furthermore, the dogs and gun-
shot sounds are indistinguishable, occurring simultane-
ously in the generated audio, which indicates that the CLAP
model struggles to differentiate and isolate fine-grained
events or entities. We believe the reason is that CLAP model
can only gather global acoustic information and have is-sue capturing temporal and local information in the text.
Furthermore, all the objective evaluation also only focus
and compute global features, therefore such inconsistency
exists. Recent study [54] corroborates that current CLAP
models do not truly comprehend natural language, focusing
instead on global information.
In contrast, when comparing the attention maps of the
last two lines, where Tango also employs FlanT5-large as
a text encoder, the Tangos attention maps appear muddled,
particularly for the dog token highlighted in blue area,
which results in the omission of the dog sound in the gen-
erated audio. Additionally, we find that Tango often pro-
duces extraneous sounds, such as unintended bird noises,
that are not present in the text captions. These findings high-
light that Auffusion, by leveraging the robust text-image
alignment capabilities of pre-trained LDMs, can generate
audio that more accurately reflects the given captions.
Performance against Number of Events. To better as-
sess fine-grained text-audio alignment, we evaluate perfor-
7Table 3. Subjective evaluation for all baseline models and different encoders used in Auffusion categorized by the number of events in the
text. OVL measures the overall quality and REL shows the relevance. ACC represents the mean accuracy of audio events matching the text
in multi-event conditions, indicating the fine-grained alignment between text and audio.
ModelAll Event Single Event Two Events Multi Events
OVL REL RELCLAPRELCLAPRELCLAPACC
Groundtruth 71.56 74.01 73.70 50.9% 75.50 51.5% 72.85 48.7% 84.6%
AudioGen-v2-medium 63.86 59.80 59.75 47.2% 58.50 45.9% 61.15 48.3% 68.5%
AudioLDM-L-full 60.33 57.36 58.00 51.1% 60.40 50.8% 53.70 51.1% 53.2%
AudioLDM2-large 65.53 59.23 61.50 50.6% 60.20 48.3% 56.00 46.9% 61.4%
Tango-Full 67.78 65.05 63.75 48.2% 67.75 52.3% 62.65 53.6% 69.6%
Auffusion-w-clip 69.36 70.25 70.65 52.5% 73.45 53.2% 66.65 55.3% 73.9%
Auffusion-w-clap 69.76 67.76 68.95 55.6% 71.25 57.5% 63.10 58.9% 71.0%
Auffusion-w-flant5 70.13 70.65 69.55 51.1% 74.60 53.0% 67.80 54.3% 73.3%
Auffusion-w-clap-flant5 69.80 71.86 72.10 55.3% 73.90 56.8% 69.60 58.9% 74.1%
mance across varying event numbers in the AudioCaps and
Clotho test set. For instance, a sequence like A man talking
followed by plastic clacking then a power tool drilling com-
prises three distinct events. We categorize the test sets into
three groups: single event, two events, and multiple events
(three or more), randomly selecting 20 captions from each
category for generation. Human raters are asked to rate
the relevance between text and audio for each group, using
REL metric. Besides, we select an additional 80 samples
for multiple events. We ask raters to count the number of
events in the audios that accurately appear in the text, and
calculate the mean accuracy denoted as ACC to reflect the
fine-grained text-audio alignment. These results for base-
lines and Auffusion with various encoders trained on whole
datasets are presented in Table 3. Additionally, we used the
objective CLAP score for comparison.
We assume that human raters can directly and faithfully
represent true performance. We find that the CLAP score
can not accurately reflect detailed alignment ability, par-
ticularly in multi-event evaluation, compared with human
evaluation. Our analysis concludes that CLAP primarily
extracts global features, lacking in fine-grained evaluation
capacity. Additionally, we observe that AudioLDM, Au-
dioLDM2, and AudioGen exhibit inferior performance in
REL and we can tell from ACC score that they fail to gen-
erate matching audio in multi-event scenarios. This is at-
tributed to AudioLDM using globally pooled CLAP em-
beddings for conditioning, while AudioLDM2 first employs
an auto-regressive model (AR) to generate AudioMAE fea-
tures from text, then uses these to condition the LDM. Con-
sequently, fine-grained information is lost in AudioLDM,
and the AR model in AudioLDM2 introduces error accu-
mulation. In contrast, Tango and our Auffusion, which
adopt cross-attention between text embedding sequences
and LDM, demonstrate better alignment. Moreover, our
findings across various encoders align with the attention
map results illustrated in previous part. Despite Auffusionscombination with the CLAP encoder yielding a higher
CLAP score, evaluations using ACC and REL especially
in multi-event scenarios reveal that the CLAP encoder cap-
tures less fine-grained information compared to CLIP and
FlanT5 encoders.
Applications. Leveraging our systems exceptional text
comprehension capabilities and robust text-audio align-
ment, we demonstrate its versatile applications inspired by
T2I tasks [34, 44, 49, 56]. These include audio style trans-
fer, audio inpainting, and attention-based techniques such as
word swap and text attention re-weighting. We demonstrate
these capabilities in Appendix E. Our method offers a sig-
nificantly more controllable and fine-grained manipulation
compared to previous methods[9, 24, 27, 28].
5. Conclusion
In this study, we introduce Auffusion, a text-to-audio (TTA)
generation model that harnesses the robust generative capa-
bilities and precise cross-modal alignment abilities of pre-
trained text-to-image (T2I) models. Our extensive objec-
tive and subjective evaluations demonstrate that Auffusion
surpasses other state-of-the-art models, achieving superior
performance with limited data and computational resources.
Recognizing the significant impact of different encoders
on cross-modal alignment in T2I, we pioneer in the TTA
field by conducting comprehensive investigations and inno-
vatively adopting cross-attention map visualization. This
approach offers an intuitive evaluation of text-audio align-
ment. Our findings demonstrate that Auffusion exhibits an
exceptional ability to generate audio that accurately aligns
with text descriptions, surpassing existing methods, which
further evidenced in several audio manipulations, includ-
ing audio style transfer, inpainting, word swapping, and re-
weighting. In the future, we aim to delve into a broader
spectrum of innovative audio applications, based on the ro-
bust text-audio alignment capabilities of our system.
8References
[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,
Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,
Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu
Liu. ediff-i: Text-to-image diffusion models with an ensem-
ble of expert denoisers. CoRR , abs/2211.01324, 2022. 2, 4,
6
[2] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze
Xie, Yue Wu, Zhongdao Wang, James T. Kwok, Ping Luo,
Huchuan Lu, and Zhenguo Li. Pixart- α: Fast training of dif-
fusion transformer for photorealistic text-to-image synthesis.
CoRR , abs/2310.00426, 2023. 1
[3] Alexandre D efossez, Jade Copet, Gabriel Synnaeve, and
Yossi Adi. High fidelity neural audio compression. CoRR ,
abs/2210.13438, 2022. 2
[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: pre-training of deep bidirectional trans-
formers for language understanding. In NAACL-HLT (1) ,
pages 41714186. Association for Computational Linguis-
tics, 2019. 2
[5] Konstantinos Drossos, Samuel Lipping, and Tuomas Virta-
nen. Clotho: an audio captioning dataset. In 2020 IEEE
International Conference on Acoustics, Speech and Signal
Processing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020 ,
pages 736740. IEEE, 2020. 4, 1
[6] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail,
and Huaming Wang. CLAP: learning audio concepts from
natural language supervision. CoRR , abs/2206.04769, 2022.
1, 2
[7] Seth* Forsgren and Hayk* Martiros. Riffusion - Stable dif-
fusion for real-time music generation. 2022. 2, 3, 5
[8] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren
Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal,
and Marvin Ritter. Audio set: An ontology and human-
labeled dataset for audio events. In 2017 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Process-
ing, ICASSP 2017, New Orleans, LA, USA, March 5-9, 2017 ,
pages 776780. IEEE, 2017. 4
[9] Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish,
and Soujanya Poria. Text-to-audio generation using
instruction-tuned LLM and latent diffusion model. CoRR ,
abs/2304.13731, 2023. 1, 2, 4, 5, 8
[10] Shawn Hershey, Sourish Chaudhuri, Daniel P. W. Ellis,
Jort F. Gemmeke, Aren Jansen, R. Channing Moore, Manoj
Plakal, Devin Platt, Rif A. Saurous, Bryan Seybold, Mal-
colm Slaney, Ron J. Weiss, and Kevin W. Wilson. CNN ar-
chitectures for large-scale audio classification. In 2017 IEEE
International Conference on Acoustics, Speech and Signal
Processing, ICASSP 2017, New Orleans, LA, USA, March
5-9, 2017 , pages 131135. IEEE, 2017. 5
[11] Shawn Hershey, Daniel P. W. Ellis, Eduardo Fonseca, Aren
Jansen, Caroline Liu, R. Channing Moore, and Manoj Plakal.
The benefit of temporally-strong labels in audio event clas-
sification. In IEEE International Conference on Acoustics,
Speech and Signal Processing, ICASSP 2021, Toronto, ON,
Canada, June 6-11, 2021 , pages 366370. IEEE, 2021. 4, 1[12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-
age editing with cross-attention control. In ICLR . OpenRe-
view.net, 2023. 4, 6
[13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. In Advances in Neural Information Processing Systems
30: Annual Conference on Neural Information Processing
Systems 2017, December 4-9, 2017, Long Beach, CA, USA ,
pages 66266637, 2017. 5
[14] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. CoRR , abs/2207.12598, 2022. 4
[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In NeurIPS , 2020. 1
[16] Jiawei Huang, Yi Ren, Rongjie Huang, Dongchao Yang,
Zhenhui Ye, Chen Zhang, Jinglin Liu, Xiang Yin, Zejun Ma,
and Zhou Zhao. Make-an-audio 2: Temporal-enhanced text-
to-audio generation. CoRR , abs/2305.18474, 2023. 1, 2, 4,
5
[17] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xi-
hui Liu. T2i-compbench: A comprehensive benchmark for
open-world compositional text-to-image generation. CoRR ,
abs/2307.06350, 2023. 2
[18] Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski,
Michael Auli, Wojciech Galuba, Florian Metze, and
Christoph Feichtenhofer. Masked autoencoders that listen.
InNeurIPS , 2022. 2
[19] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren,
Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin,
and Zhou Zhao. Make-an-audio: Text-to-audio generation
with prompt-enhanced diffusion models. In ICML , pages
1391613932. PMLR, 2023. 1, 2
[20] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and
Gunhee Kim. Audiocaps: Generating captions for audios
in the wild. In Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, NAACL-
HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume
1 (Long and Short Papers) , pages 119132. Association for
Computational Linguistics, 2019. 4, 1
[21] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan:
Generative adversarial networks for efficient and high fi-
delity speech synthesis. In Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Infor-
mation Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual , 2020. 5, 1
[22] Qiuqiang Kong, Yong Xu, Turab Iqbal, Yin Cao, Wenwu
Wang, and Mark D. Plumbley. Acoustic scene generation
with conditional samplernn. In ICASSP , pages 925929.
IEEE, 2019. 1
[23] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,
Wenwu Wang, and Mark D. Plumbley. Panns: Large-scale
pretrained audio neural networks for audio pattern recogni-
tion. IEEE ACM Trans. Audio Speech Lang. Process. , 28:
28802894, 2020. 5
[24] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer,
Alexandre D efossez, Jade Copet, Devi Parikh, Yaniv Taig-
9man, and Yossi Adi. Audiogen: Textually guided audio gen-
eration. In ICLR . OpenReview.net, 2023. 1, 2, 5, 8
[25] Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro,
and Sungroh Yoon. Bigvgan: A universal neural vocoder
with large-scale training. In The Eleventh International Con-
ference on Learning Representations, ICLR 2023, Kigali,
Rwanda, May 1-5, 2023 . OpenReview.net, 2023. 5
[26] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich,
Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu,
Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut,
and Daniel Haziza. xformers: A modular and hackable
transformer modelling library. https://github.com/
facebookresearch/xformers , 2022. 1
[27] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu,
Danilo P. Mandic, Wenwu Wang, and Mark D. Plumbley.
Audioldm: Text-to-audio generation with latent diffusion
models. In ICML , pages 2145021474. PMLR, 2023. 1,
2, 4, 5, 8
[28] Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qi-
uqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang,
and Mark D. Plumbley. Audioldm 2: Learning holistic
audio generation with self-supervised pretraining. CoRR ,
abs/2308.05734, 2023. 1, 2, 5, 8
[29] Xubo Liu, Turab Iqbal, Jinzheng Zhao, Qiushi Huang,
Mark D. Plumbley, and Wenwu Wang. Conditional sound
generation using neural discrete time-frequency representa-
tion learning. In MLSP , pages 16. IEEE, 2021. 1
[30] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar
Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-
moyer, and Veselin Stoyanov. Roberta: A robustly optimized
BERT pretraining approach. CoRR , abs/1907.11692, 2019.
1
[31] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. In 7th International Conference on Learning
Representations, ICLR 2019, New Orleans, LA, USA, May
6-9, 2019 . OpenReview.net, 2019. 5
[32] Irene Mart ın-Morat o and Annamaria Mesaros. What is the
ground truth? reliability of multi-annotator data for audio
tagging. In 29th European Signal Processing Conference,
EUSIPCO 2021, Dublin, Ireland, August 23-27, 2021 , pages
7680. IEEE, 2021. 4, 1
[33] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong,
Tom Ko, Chengqi Zhao, Mark D. Plumbley, Yuexian Zou,
and Wenwu Wang. Wavcaps: A chatgpt-assisted weakly-
labelled audio captioning dataset for audio-language multi-
modal research. CoRR , abs/2303.17395, 2023. 4
[34] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun
Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided im-
age synthesis and editing with stochastic differential equa-
tions. In The Tenth International Conference on Learn-
ing Representations, ICLR 2022, Virtual Event, April 25-29,
2022 . OpenReview.net, 2022. 8, 4
[35] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and
Daniel Cohen-Or. Null-text inversion for editing real images
using guided diffusion models. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition, CVPR 2023, Van-
couver, BC, Canada, June 17-24, 2023 , pages 60386047.
IEEE, 2023. 4[36] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,
Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya
Sutskever, and Mark Chen. GLIDE: towards photorealis-
tic image generation and editing with text-guided diffusion
models. In International Conference on Machine Learning,
ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA ,
pages 1678416804. PMLR, 2022. 2
[37] Nathana el Perraudin, P eter Bal azs, and Peter L.
Søndergaard. A fast griffin-lim algorithm. In IEEE
Workshop on Applications of Signal Processing to Audio
and Acoustics, WASPAA 2013, New Paltz, NY, USA, October
20-23, 2013 , pages 14. IEEE, 2013. 2
[38] Karol J. Piczak. ESC: dataset for environmental sound clas-
sification. In Proceedings of the 23rd Annual ACM Con-
ference on Multimedia Conference, MM 15, Brisbane, Aus-
tralia, October 26 - 30, 2015 , pages 10151018. ACM, 2015.
4, 1
[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In ICML , pages
87488763. PMLR, 2021. 1, 2
[40] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J. Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. J. Mach. Learn. Res. , 21:
140:1140:67, 2020. 1, 2
[41] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. In ICML , pages 8821
8831. PMLR, 2021. 2
[42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with CLIP latents. CoRR , abs/2204.06125, 2022. 2
[43] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR , pages 10674
10685. IEEE, 2022. 1, 2
[44] Chitwan Saharia, William Chan, Huiwen Chang, Chris A.
Lee, Jonathan Ho, Tim Salimans, David J. Fleet, and Mo-
hammad Norouzi. Palette: Image-to-image diffusion mod-
els. In SIGGRAPH 22: Special Interest Group on Computer
Graphics and Interactive Techniques Conference, Vancouver,
BC, Canada, August 7 - 11, 2022 , pages 15:115:10. ACM,
2022. 8
[45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed
Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan,
Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad
Norouzi. Photorealistic text-to-image diffusion models with
deep language understanding. In NeurIPS , 2022. 1, 2, 6
[46] Justin Salamon, Christopher Jacoby, and Juan Pablo Bello. A
dataset and taxonomy for urban sound research. In Proceed-
ings of the ACM International Conference on Multimedia,
MM 14, Orlando, FL, USA, November 03 - 07, 2014 , pages
10411044. ACM, 2014. 4, 1
10[47] Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Long-
pre, Jason Wei, Hyung Won Chung, Barret Zoph, William
Fedus, Xinyun Chen, Tu Vu, Yuexin Wu, Wuyang Chen, Al-
bert Webson, Yunxuan Li, Vincent Zhao, Hongkun Yu, Kurt
Keutzer, Trevor Darrell, and Denny Zhou. Flan-moe: Scal-
ing instruction-finetuned language models with sparse mix-
ture of experts. CoRR , abs/2305.14705, 2023. 1, 2
[48] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Mah-
eswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In ICML ,
pages 22562265. JMLR.org, 2015. 1
[49] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,
Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,
Naejin Kong, Harshith Goka, Kiwoong Park, and Victor
Lempitsky. Resolution-robust large mask inpainting with
fourier convolutions. In IEEE/CVF Winter Conference on
Applications of Computer Vision, WACV 2022, Waikoloa, HI,
USA, January 3-8, 2022 , pages 31723182. IEEE, 2022. 8
[50] Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang,
Gefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin,
and Ferhan Ture. What the DAAM: interpreting stable dif-
fusion using cross attention. In Proceedings of the 61st An-
nual Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), ACL 2023, Toronto, Canada,
July 9-14, 2023 , pages 56445659. Association for Compu-
tational Linguistics, 2023. 4
[51] George Tzanetakis and Perry R. Cook. Musical genre clas-
sification of audio signals. IEEE Trans. Speech Audio Pro-
cess. , 10(5):293302, 2002. 4, 1
[52] A aron van den Oord, Oriol Vinyals, and Koray
Kavukcuoglu. Neural discrete representation learning.
InNIPS , pages 63066315, 2017. 2
[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NIPS , pages 5998
6008, 2017. 4
[54] Ho-Hsiang Wu, Oriol Nieto, Juan Pablo Bello, and Justin
Salamon. Audio-text models do not yet leverage natural
language. In IEEE International Conference on Acoustics,
Speech and Signal Processing ICASSP 2023, Rhodes Island,
Greece, June 4-10, 2023 , pages 15. IEEE, 2023. 7
[55] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao
Weng, Yuexian Zou, and Dong Yu. Diffsound: Discrete dif-
fusion model for text-to-sound generation. IEEE ACM Trans.
Audio Speech Lang. Process. , 31:17201733, 2023. 1, 2
[56] Lvmin Zhang and Maneesh Agrawala. Adding condi-
tional control to text-to-image diffusion models. CoRR ,
abs/2302.05543, 2023. 8
11Auffusion: Leveraging the Power of Diffusion and Large Language Models for
Text-to-Audio Generation
Supplementary Material
A. Dataset Details
Table 4. Statistics for the all datasets used in the this paper.
Dataset Samples Hours (Original) Hours (Processed) Source
AudioCaps 41K 110h 110h [20]
MACS 3930 11h 11h [32]
Clotho 5929 37h 37h [5]
ESC50 2000 3h 3h [38]
UrbanSound8K 8266 9h 9h [46]
Music Instruments 4587 11h 10h [9]
GTZAN 2997 8h 8h [51]
BBC Sound Effects 31K 997h 232h https://sound-effects.bbcrewind.co.uk/
FreeSound 206K 6246h 1283h https://freesound.org/
AudioSet SL 108K 296h 296h [11]
SoundBible 612 4h 3h https://soundbible.com/
Total 402K 7732h 1990h -
As indicated in Table 4, we collect a large-scale audio-text dataset comprising approximately 0.47M audio samples,
amounting to a total duration of about 7.7K hours. This dataset encompasses a diverse range of sounds, including musical
instruments, sound effects, human voices, and sounds from nature and everyday life. We only utilize the first 30 seconds
of each audio sample for long duration audio and exclude any samples shorter than 1 second. Consequently, our model is
trained on approximately 0.4M audio samples, collectively amounting to about 2K hours.
B. Experiment Details
B.1. Vocoder
In this work, we adopt HiFi-GAN vocoder [21] as a converter from V AE decoder output to finally generated audio. It is widely
used for speech waveform and audio sample reconstructed from mel-spectrogram. However, the default and pretrained HiFi-
GAN14use 80 mel filter bands, 256 hop size and trained on 22050 sample rate speech, resulting in (1,80,882) mel-spectogram
for 10-second audio, which does not suit V AE input requirements. Therefore we train our own HiFi-GAN vocoder using 256
mel filter bands, 1024 window length, 2048 FFT, and 160 hop size, resulting in (1,256,1024) mel-spectrograms. Then we
repeat by channel and convert grayscale image to RGB image to match V AE encoder input. The (3,256,1024) image is then
8x downsampled by V AE as (4,32,128) latent features sent to LDM. We train this vocoder using AdaW optimizer with 2e-4
learning rate and 16 batch size on one A6000 GPU. We release this pretrained vocoder in our open-source implementation.
B.2. Configuration
We utilize the pretrained Stable Diffusion v1.515, including its V AE and 860M-parameter U-Net. we freeze the text encoder
and finetune U-Net using AdamW optimizer with a 3e-5 learning rate and a constant scheduler, at a 20 batch size for 100K
steps in both Auffusion and Auffusion-Full setups. For GPU memory efficiency, we use xformer [26] and mixed precision,
and our model can be trained only taking a total of 48 hours on one A6000 GPU. In comparison, AudioGen [24] utilizes 64
A100 GPU with a batch size of 256. AudioLDM [27] use one A100 GPU for 1.5M train steps and AudioLDM2 [28] use 8
A100 GPU for the same steps. Tango use 4 A6000 GPU for 200K steps.
14https://github.com/jik876/hifi-gan
15https://huggingface.co/runwayml/stable-diffusion-v1-5
1B.3. Riffusion Re-implementation
Riffusion [7] is originally trained on music datasets for only 5-second sound clips using a 44100 sample rate, a 441 hop size,
and 512 mel filter bands. We changed the sample rate to 16000 and the hop size to 160 to match audio generation setup. After
converting audio to a mel-spectrogram, Riffusion simply applies min-max normalization to each individual mel-spectrogram
and quantizes it into an image. Therefore, it is a non-reversible process that leads to information loss. To convert back to
audio, Riffusion uses the Griffin-Lim [37] algorithm, which is not sensitive to initial data range and iteratively estimates the
missing phase information. However, the quality is not comparable to that of deep-learning-based vocoders. We use the same
training setup to re-implement Riffusion with same whole datasets.
C. Effect of Guidance Scale and Inference Steps
The number of inference steps and the classifier-free guidance scale are of crucial importance for sampling from latent dif-
fusion models. We report the effect of varying these parameters on audio generation using AudioCaps test set in Table 5. On
the left, with a fixed guidance scale of 7.5, we explore inference steps ranging from 10 to 200. Unlike previous studies [9, 27]
using 200 steps, we find that our Auffusion model have competent performance even with fewer inference steps, suggesting
strong generative capabilities. Therefore we choose using 100 inference steps for Auffusion model. On the right, we fix the
steps at 100 and adjust the guidance scale. We find that guidance 5 has the best FD and FAD score, and guidance 10 excels
in IS and CLAP score, therefore we choose balanced guidance scale 7.5 for Auffusion model to generate audio.
Table 5. Effect on the objective evaluation metrics with a varying number of inference steps and classifier-free guidance scale.
Varying Steps Varying Guidance
Guidance Steps FD  FADKL IS CLAPSteps Guidance FD  FADKL IS CLAP
7.510 21.15 2.48 1.54 7.89 48.9%
1001 32.32 5.17 2.47 4.56 32.2%
25 22.52 2.03 1.36 9.88 55.3% 2 23.44 2.86 1.63 6.4 47.1%
50 23.35 2.01 1.35 10.09 55.5% 5 21.24 1.72 1.37 9.29 54.9%
100 23.44 1.96 1.36 10.21 55.6% 7.5 23.44 1.96 1.36 10.21 55.6%
200 23.34 1.92 1.36 10.20 55.6% 10 24.69 2.31 1.36 10.50 55.9%
D. Evaluation
Despite many objective evaluation method exits, they can only assess global performance. Subjective method is a much more
direct approach and can be tailored to specific needs. We design three scoring tasks to evaluate performance. We first ask
human raters to evaluate the overall qaulity (OVL) from 0 to 100 with 10-point intervals. Then they need to rate the relevance
(REL) to the text caption, evaluate the global text-audio alignment. Finally, we ask testers to count the number of events that
appear in the audio to assess the fine-grained text-audio alignment. Our designed subjective evaluation is shown in Figure 3.
Figure 3. Screenshot of subjective evaluation.
2E. Demos
E.1. Text-to-Audio Generation
Figure 4. Demo of audio generation with the Auffusion-Full model.
3E.2. Audio Style Transfer Examples
We show the audio style transfer ability of our Auffusion. We adopt the similar image-to-image manipulation method first
introduced in T2I task by using shallow reverse process [34] to audio domain. In the figures below, we show the original
audio samples on the left, and six transferred audios guided by textual descriptions using different starting points of the
reverse process n. Asnincreases, more noise will be added to the original audio. Diffusion model will pay more attention
on text guidance and the generated audio will become less similar to the original one. When n= 1, the added noise is at its
maximum, and information from the original audio will not be retained. We gradually increase nand set n= 0.7for the last
generated sample. For instance, the original sound of a baby crying gradually transitions into the sound of a cat meowing
in Figure 5.
Figure 5. Audio style transfer gradually from baby crying tocat meowing .
Figure 6. Audio style transfer gradually from cat screaming tocar racing .
Figure 7. Audio style transfer gradually from bird chirping toambulance siren .
4E.3. Audio Inpainting Examples
In the figure below, we demonstrate the audio inpainting capability of our Auffusion model. Each audio sample is extended to
a duration of 10 seconds. In the first row of each example, we present the groundtruth samples. For the unprocessed samples,
we mask a segment from 2.5 to 7.5 seconds in the original audio and use these masked samples as input for inpainting. The
inpainting results are produced using the same text prompt as the original groundtruth sample. We observe that our model
can comprehend the textual description and audio context, thereby generating appropriate content for the masked segment.
Figure 8. The examples of audio inpainting ability of Auffusion.
5E.4. Word Swap Examples
Showing the replacement ability of attention map in TTA task. In below cases, we swap tokens in the original prompt with
others. By changing huge tosmall , we observe that the sound effect in the corresponding part changes to a less echoic and
clearer sound. By replacing gunshots tospeech , the corresponding sound is replaced with a human voice.
Figure 9. Demo of word swapping manipulation.
E.5. Attention Re-weighting Examples
Showing the re-weighting ability of attention map in TTA task. By increasing the cross attention of specific words (marked
with an arrow), we control the effect only on specific words without significant change the image. We find that increasing
the weight on the verb chopping enhances the frequency of the action sound, while amplifying the adjective huge affects the
sound effect.
Figure 10. Demo of attention re-weighting manipulation.
6
  VeCLIP: Improving CLIP Training via Visual-enriched Captions
Zhengfeng Lai1, Haotian Zhang2, Bowen Zhang2, Wentao Wu2, Haoping Bai2, Aleksei Timofeev2,
Xianzhi Du2, Zhe Gan2, Jiulong Shan2, Chen-Nee Chuah1, Yinfei Yang2, Meng Cao2
1University of California, Davis2Apple AI/ML
{lzhengfeng, chuah }@ucdavis.edu
{haotian zhang2, yinfeiy, mengcao }@apple.com
Abstract
Large-scale web-crawled datasets are fundamental for
the success of pre-training vision-language models, such as
CLIP . However, the inherent noise and potential irrelevance
of web-crawled AltTexts pose challenges in achieving pre-
cise image-text alignment. Existing methods utilizing large
language models (LLMs) for caption rewriting have shown
promise on small, curated datasets like CC3M and CC12M.
This study introduces a scalable pipeline for noisy caption
rewriting. Unlike recent LLM rewriting techniques, we em-
phasize the incorporation of visual concepts into captions,
termed as Visual- enriched Captions (VeCap). To ensure data
diversity, we propose a novel mixed training scheme that op-
timizes the utilization of AltTexts alongside newly generated
VeCap. We showcase the adaptation of this method for train-
ing CLIP on large-scale web-crawled datasets, termed Ve-
CLIP . Employing this cost-effective pipeline, we effortlessly
scale our dataset up to 300 million samples named VeCap
dataset. Our results show significant advantages in image-
text alignment and overall model performance. For example,
VeCLIP achieves up to +25.2% gain in COCO and Flickr30k
retrieval tasks under the 12M setting. For data efficiency,
VeCLIP achieves +3% gain while only using 14% of the data
employed in the vanilla CLIP and 11% in ALIGN. We also
note the VeCap data is complementary with other well cu-
rated datasets good for zero-shot classification tasks. When
combining VeCap and DFN, our model can achieve strong
performance on both of image-text retrieval and zero-shot
classification tasks, e.g.83.1% accuracy@1 on ImageNet
zero-shot for a H/14 model. We release the pre-trained mod-
els at https://github.com/apple/ml-veclip .
Work done during an internship at Apple.Equal contribution.
Corresponding author.1. Introduction
Large-scale vision-language representation learning, exem-
plified by CLIP [ 32], has gained wide attention due to the
transferability of knowledge learned from image-text pairs
to diverse downstream tasks such as zero-shot image classi-
fication and image-text retrieval [ 17,19,20]. CLIP training
is straightforward via the image-text contrastive loss, but
involves a large-scale dataset of 400 million image-text pairs
crawled from the Web. Consequently, CLIP embeddings
lead to consistent improvement across various downstream
tasks compared to other vision pre-training methods such as
SimCLR [ 6] and MAE [ 15]. CLIP achieves success via two
scalable paradigms: data and computational resources. First,
the massive web-crawled data [ 35,36] enable the training
to be scalable and meet the requirements of data-hungry
backbones ( e.g., ViT [ 11]). Second, the simple image-text
contrastive loss grants favorable scaling properties to the
computational resources.
Despite the availability of large-scale web-crawled data,
their quality can be low or noisy. For example, AltTexts
suffer from two major issues: 1) they can be noisy, uninfor-
mative, or irrelevant to the images; 2) they may not describe
all visual contents in the image. For example, as shown
in Figure 1, in the first image, we observe a house with a
white roof and a porch. However, the corresponding caption
only describes the address, which proves overly abstract for
effective vision-language alignment in training. Our obser-
vations demonstrate that caption quality plays a pivotal role
in CLIPs performance, as detailed in Table 6b and the Ap-
pendix ( e.g., CC3M vs. our web-crawled 3M). It is worth
noting that the captions in CC3M are derived from human an-
notations, which may require heavy resources when further
scaling up. This motivates the main open research question
addressed in this work: Can we devise a scalable and cost-
effective pipeline to improve captions within these noisy
datasets at scale (e.g., up to million or billion level)?
One natural direction is to deploy Large Language Mod-
els (LLMs) to rephrase captions [ 12]. However, the major
1arXiv:2310.07699v3  [cs.CV]  13 Mar 2024Figure 1. Noisy web-crawled data and the limitation of LLM rewrite. AltTexts can be noisy and uninformative; it may not describe all visual objects
present in the image. Simple LLM rewrite [ 12] on such raw and noisy captions cannot introduce new image-relevant information. After applying our proposed
VeCap, new captions are enriched with more image-specific concepts. We keep all image-text pairs for pre-training rather than filtering out those with noisy
AltTexts, as images of rich visual objects still contribute effectively to the training process.
limitation of such methods lies in the inability of LLMs to
generate and introduce new image-specific details. LLMs
can only modify sentence syntax in this scenario. For exam-
ple, we follow a recent work [ 12] and use LLM to rewrite the
raw captions from the Web: as shown in Fig. 1, LLM rewrite
cannot introduce any new information and thus the new cap-
tion remains similar to AltText. In other words, if the origi-
nal AltTexts are noisy, the benefits brought by LLM rewrite
might yield only trivial improvements. In essence, the re-
liance on high-quality captions within pre-training datasets
limits the effectiveness of simple LLM rewrite. However,
sourcing such high-quality datasets like manually curated
CC3M and CC12M [ 5] remains challenging, and further
scaling up to larger datasets becomes both time-consuming
and labor-intensive to meet the prerequisites for CLIP pre-
training. Therefore, in this work, we focus on building a
scalable and cost-effective pipeline tailored to raw and noisy
web-crawled data to improve CLIP.
In addition to data quality, the diversity of data signifi-
cantly impacts VLM pre-training [ 2,27]. Methods relying
on LLM-based rewriting may diminish data variety, given
that LLMs tend to apply a uniform style in their sentence
rephrasing. Moreover, existing works mainly focus on image
augmentations, while texts are disregarded and unaltered dur-
ing training without augmentation [ 12]. This may also incur
overfitting issues as the text encoders repeatedly encounter
the same texts in each epoch. Since these techniques have
exclusively undergone assessment on meticulously curated
datasets like CC3M and CC12M [ 5], their suitability for
extensive, uncensored web-crawled data remains uncertain.Consequently, there is a pressing need to build a scalable
approach to enhance data quality, diversity, and training
methodologies to improve pre-training for VLMs on both
model performance and data efficiency.
Concurrently, alongside the evolution of CLIP, there has
been substantial progress in the development of instruction
fine-tuned LLMs. These models and their multimodal ex-
tensions have demonstrated outstanding performance, sur-
passing human capabilities in various natural language and
vision tasks. Inspired by these models, we investigate the
potential of utilizing them to improve the noisy captions
gathered from the Internet. Specifically, we initially employ
LLaV A [ 24], a Language-Vision Assistant, to leverage visual
concepts extracted from the images. Given that AltTexts may
lack informativeness, our objective is to integrate the newly
derived visual concepts into the caption. However, it is worth
noting that LLaV A [ 24] fine-tuned its language decoder on
its own generated dataset, potentially losing its ability to ac-
commodate comprehensive instructions. Consequently, we
further propose to utilize an LLM to refine the sentence by
fusing the generated caption from LLaV A and the original
AltText. This process aims to maximize image-specific infor-
mation for optimal vision-language alignment. We denote
the caption generated from LLM as LLM Visual- enriched
Captions (VeCapap), or VeCap for short. For data variety,
we propose VeCLIP and introduce a mixed training scheme,
alternating between VeCap and the original AltText. This
strategy ensures that the model captures all pertinent informa-
tion without oversight. We generalize this scalable pipeline
to curate five pre-training datasets ranging from small-scale
2to large-scale up to 300M. Overall, our contributions are
summarized below:
We present a visual-enriched re-captioning technique for
CLIP training. This marks the initial endeavor to leverage
visual concepts extracted from images and inject them into
the captioning process.
Our pipeline is cost-effective and capable of processing
data at a scale exceeding 300M, named VeCap. Then, we
propose VeCLIP with a mixed training scheme that uses
VeCap to improve CLIP training on model performance.
VeCLIP can achieve up to 25.2% improvement over CLIP
in retrival tasks. For training data efficiency, e.g., we use
only 5%data in training but achieve competitive results
in image-text retrieval tasks.
VeCap data is also complementary with other well curated
datasets. A CLIP-H/14 model trained on the combination
of VeCap and DFN achieves strong performance on both of
image-text retrieval and zero-shot classification tasks, with
an impressive 83.1% zero-shot accuracy@1 on ImageNet.
2. Related Work
Contrastive language-image pre-training. CLIP [ 32] has
shown its effectiveness in acquiring transferable image repre-
sentations via text supervision after large-scale pre-training.
Similar models such as ALIGN [ 17], Florence [ 44], BA-
SIC [ 30] and OpenCLIP [ 8] have shown impressive zero-
shot image classification and image-text retrieval capabili-
ties. SLIP [ 26] and DeCLIP [ 21] incorporate self-supervised
training techniques to improve performance. CoCa [ 43]
introduces an additional decoder alongside the contrastive
loss. LiT [ 47] proposes to keep a pre-trained image encoder
frozen and fine-tune text encoders to improve the zero-shot
transferability. Nevertheless, the majority of these subse-
quent studies incorporate supplementary training inputs and
losses, potentially exerting adverse effects on both training
efficiency and memory usage.
Improving image-text datasets. Given the importance
of the pre-training data, many works focus on improving
the datasets, such as filtering less informative image-text
pairs [ 1,4,12,25]. However, these methods may disre-
gard a large amount of data even though some images have
rich visual concepts. An alternative approach is to rewrite
the caption to enhance the alignment between texts and im-
ages. For example, LaCLIP [ 12] employs LLMs to perform
rewriting. Nevertheless, their evaluation was conducted on
small-scale and meticulously curated datasets like CC3M
and CC12M [ 5], where the initial captions were already of
high quality. As shown in Fig. 1, the advantage of employing
LLM rewrite on noisy web-crawled data is marginal if the
AltText is noisy.3. Methodology
3.1. Preliminary
CLIP. The Contrastive Language-Image Pre-training (CLIP)
method has shown its effectiveness in training vision models
via language supervision. Specifically, a batch of Nimage-
text pairs {xI, xT}is sampled from the massive training data
during each training iteration. We apply data augmentations
to the images before inputting them into the vision encoder.
We denote fIandfTas the normalized features extracted
by the vision and text encoders, respectively. We use the
contrastive loss to train the model, where the paired images
and texts are treated as positive pairs and the remaining as
negative samples. The training loss iterating over images
can be formulated as follows:
LI=NX
i=1logexp
sim(fI(aug(xi
I)), fT(xi
T))/τ
PN
k=1exp
sim(fI(aug(xi
I)), fT(xk
T))/τ,
(1)
where (xi
I, xi
T)is the ithimage-text pair in the batch, and
aug()refers to image augmentations. sim(,)is the simi-
larity measurement function. We set τas a learnable tem-
perature parameter that scales the logits in experiments. The
loss iterating over texts is symmetrical and denoted as LT.
Finally, the training loss is L= (LI+LT)/2.
3.2. Recaptioning with Visual Concept Exploitation
Web-crawled captions (AltTexts) can be noisy and uninfor-
mative to the images. LaCLIP [ 12] used LLM to rewrite
the caption. As shown in Fig. 1, this may not be applicable
if the captions are noisy as LLM can only reconstruct the
sentence but cannot introduce new information without any
information provided by the image. Given the inherent noise
in AltTexts, we advocate for the utilization of pre-trained
multimodal models to generate augmented captions with
richer visual concepts derived from the images. In this sub-
section, we use LLaV A [ 24] as one example and present a
scalable and cost-effective pipeline for scaling up.
LLaV A and image captioning for Visual-enriched Cap-
tions (VeCap). As a multimodal model, LLaV A connects
the open-set visual encoder of CLIP [ 32] with an LLM, such
as LLaMA [ 38], then fine-tune them on a visual instruction-
tuning dataset. LLaV A shows its effectiveness in leveraging
the capabilities of pre-trained LLM and vision foundation
models. Given an input image xI, we get fIfrom CLIPs
vision encoder. Then, LLaV A applies a trainable projection
matrixWto convert fIinto language embedding tokens
to achieve the image-language alignment. To mitigate the
influence of AltText, we have devised AltText-independent
prompts tailored for LLaV A, ensuring the full exploitation
of visual concepts. We refrain from incorporating AltText
information into LLaV A, while acknowledging the potential
loss of pre-trained knowledge during fine-tuning of the LLM
3Figure 2. An overview of the scalable VeCap recaptioning piepline. First, we focus on exploiting visual concepts in images via leveraging a multimodal
LLM (LLaV A) to describe the image with a designed prompt independent of AltText to generate Visual-enriched Captions (VeC). Second, we leverage an
LLM to do ethical check and fuse the concepts from both AltText and VeC to generate the final caption, denoted as VeCap.
component on the generated dataset. This trade-off, however,
may limit its capacity to comprehend more intricate instruc-
tions. Thus, we adopt a straightforward yet potent prompt,
Describe the image concisely, less than 20 words , allowing
LLaV A to generate visual concepts directly from the image
autonomously. We denote this captions generated by LLaV A
asxTv. Subsequently, the image-text pair is converted as
(xI, xTv).
3.3. Scalable LLM Rewrite for Concept Fusion
Given the limited language capacity of LLaV A, we only
use LLaV A to extract all possible visual clues. Then, we
employ LLMs to refine the caption by fusing both the knowl-
edge from AltText xTand the novel visual concepts from
xTv. This step has three main advantages: 1) It ensures
the retention of information delineated in AltText, thereby
amplifying the informativeness of the caption; 2) It can serve
as a form of strong augmentation in textual data, character-
ized by a profound restructuring of sentence syntax instead
of focusing on word-level modifications used in existing lan-
guage augmentation techniques [ 37,40]; 3) It can mitigate
the hallucination issue arising from large vision-language
models (e.g., LLaV A) to ensure that the entity described in
the ultimate caption is present in the image.
Generating rewrites for a vast corpus of texts using closed-
source models like ChatGPT or Bard is impractical, consider-
ing the substantial financial costs and time incurred through
API utilization. Therefore, to facilitate the rewriting tasks ona large-scale dataset, we turn to open-source state-of-the-art
LLMs. Due to the license issue, we select Vicuna-1.1 [ 48],
renowned for its robust performance in text completion tasks,
as one example of LLM rewriting in this study. We formulate
a context input as the following three components. First, we
include a sentence designed to apprise the LLM of the task,
specifically, rewriting and fusing two attached sentences.
This serves as an initial contextual cue to orient the LLM
towards comprehending the overarching objective. Second,
we impose several constraints on the ultimate output. For in-
stance, our goal is to position attributes prior to noun entities,
all while refraining from introducing any novel semantic
interpretations. Furthermore, it is essential that the sentence
refrains from commencing with the phrase The image and
instead directly expounds upon all-encompassed concepts.
Finally, the last part of the context includes two sentences ( xv
andxTv) that require fusing and rewriting, followed by the
separation symbol. This ensures that the LLM is furnished
with the specific texts to be fused and rewritten as part of
its context input. By integrating these three components, we
establish an all-encompassing context that steers the LLM
towards proficiently crafting diverse and knowledge-fused
text rewrites.
Scalable batch-inference process. Employing the
crafted context input as a prompt, Vicuna showcases its profi-
ciency in executing text completion and producing rephrased
renditions of the associated text samples. However, single-
item inference may be time-consuming and not scalable for
4massive data. Therefore, we conduct this process in a batch-
inference process instead of a single-item inference as shown
in Fig 2: we group our data into batches and implement a
batch-inference process to achieve up to 64 times faster on
Nvidia A100. Specifically, we use Vicuna-1.1-13B model
to generate the final output as xTl. The final prompt is as
follows: [ Rephrase the following two sentences into one
short sentence while adhering to the provided instructions:
Place attributes before noun entities without introducing new
meaning. Do not start with The image. + 1. AltText; 2.
model generated caption. ] We denote the caption from LLM
as LLM-VeCap, or VeCap for short.
Potential ethics of LLM and failure cases processing.
While upscaling the LLM rewriting process, we identify two
scenarios in which LLM encounters difficulties in executing
the designated task: 1) Ethical Concerns . If the AltText
contains content either illegal or violent, LLM may reply,
I am sorry that I cannot...; 2) Length Constraint . In cases
where the AltText exceeds an optimal length, the processing
time of the LLM may be significantly prolonged, thus im-
peding large-scale rewriting. To address the first scenario,
we use the model generation captions as the only input to
be rewritten via LLM to form VeCap, thereby preemptively
excluding potentially unlawful or aggressive content. In the
second scenario, we mitigate this issue by preserving the
generated catpion but truncating the AltText to conform to
the maximum allowable length, thus we have more visual
concepts aligned with the image.
3.4. VeCLIP: Mixed Training Scheme with Visual-
enriched Captions for CLIP
As LLM rewriting may introduce a consistent style, there
could be a decline in data diversity for large-scale pre-
training, even if data quality is enhanced. To enhance data
diversity, we propose a mixed training scheme to serve as
additional text augmentations applied in pre-training:
mix(xt)Uniform ([xT, xTl]). (2)
Then, the training loss iterating over the images becomes:
LI=PN
i=1logexp
sim(fI(aug(xi
I)),fT(mix(xi
t)))/τ
PN
k=1exp
sim(fI(aug(xi
I)),fT(mix(xk
t)))/τ.
The only difference with the original CLIP training is that
we alternate the AltTexts with our rephrased sentences, with
all other components remaining unaltered. This modifica-
tion does not incur additional computational complexity or
parameter overheads compared to the standard CLIP train-
ing process. Through the strategic alternation of AltTexts
and our captions, we improve both the quality and diversity
of the pre-training dataset without filtering any data points.
This approach empowers the model to glean insights from
both AltText and VeCap. This simple yet effective strategy
elevates the training regimen for CLIP, offering a scalableframework for optimizing other vision-language pre-training
efforts utilizing extensive web-crawled data.
4. Experiments
4.1. Pre-training Datasets and Downstream Tasks
Pre-training datasets and training setup. We conduct pre-
training experiments on four scales of our datasets (named
VeCap) to show the efficiency and scalability of our method.
Specifically, we set 3M as small scale, 12M as medium
scale, and 100M+ as large scale. We use ViT-B/16 [ 11]
as the vision encoder of CLIP training. Our batch size is
8,192 for small/medium scales (3M/12M), and 32,768 for
large scales (100M+). For efficiency purposes, we employ
JAX [3] and train models on 64 TPUs for the 3M/12M
settings, whereas we utilize 512 TPUs for the 100M/200M
pre-training configurations. All models are trained with the
AXLearn framework.1More details can be found in the
Appendix A. To show its generalizability and effectiveness,
we also evaluate it on well-curated CC3M/CC12M besides
our crawled noisy WIT data, as shown in our ablation studies
and Appendix C.2. We evaluate all pre-trained models on
the following three tasks.
Zero-shot image classification. We evaluate all the mod-
els on ImageNet [ 10], ImageNetV2 [ 33], and VTAB [ 46].
We select 9 tasks (6 from natural sets and 3 from special-
ized sets) that are suitable for zero-shot classification tasks
such as Flowers102 [ 28] and Caltech-101 [ 14] as zero-shot
classification tasks. We list the details in the Appendix.
Zero-shot image-text retrieval. We evaluate the pre-
trained models on COCO [ 23] and Flickr30k [ 31] cross-
modal retrieval tasks: Image-to-Text (denoted as I2T) and
Text-to-Image (T2I) retrieval. For Flickr30k, we evaluate
them on the standard 1K test set. We report the results in
terms of Recall@ kas R@1, R@5, and R@10.
Zero-shot image-to-image retrieval. We select
GPR1200 [ 34] for image-to-image retrieval. GPR1200 [ 34]
serves as a general-purpose benchmark for content-based
image retrieval, encompassing subsets drawn from six dif-
ferent domains. It includes 1200 categories (10 images per
category). Following [ 34], we do not split images as query
and index sets for evaluation. Instead, we perform retrieval
of the nearest neighbor for each image and utilize the remain-
ing images as the index set. We report the mean Average
Precision (mAP).
4.2. Results on Retrieval Tasks
I2T and T2I retrieval. We summarize the main results
in Table 1. We show consistent improvements across
Recall@ kmetrics in both COCO and Flickr30k datasets
for both I2T and T2I retrieval tasks. Specifically, for small
and medium scales (3M/12M), we attain an improvement
1https://github.com/apple/axlearn
5Table 1. Results (Recall@ k) on zero-shot image-to-text and text-to-image retrieval tasks on COCO and Flickr30k. 1.4B-CLIP denotes the
in-house CLIP pre-trained on 1.4B web-crawled image-text pairs. We use ViT-B/16 as the vision encoder of CLIP. (*) Denote FLIP uses
ViT-L/14.
Data ModelCOCO Flickr30k
Image-to-Text Text-to-Image Image-to-Text Text-to-Image
R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10
1.8B ALIGN [17] 58.60 83.00 89.70 45.60 69.80 78.60 88.60 98.70 99.70 75.70 93.80 96.80
400M FLIP* [22] 60.20 82.60 89.90 44.20 69.20 78.40 89.10 98.50 99.60 75.40 92.50 95.90
400M OpenAI CLIP 53.76 77.92 85.53 33.09 58.42 68.90 88.00 98.70 99.40 68.70 90.60 95.20
1.4B In-house CLIP 61.38 82.80 89.78 44.48 69.19 78.28 87.60 97.90 98.80 71.70 91.30 95.24
3MCLIP 5.46 15.34 22.42 3.28 10.44 15.96 12.20 27.80 37.50 6.36 19.16 27.58
VeCLIP 22.30 45.00 56.16 13.01 31.61 42.42 40.60 67.30 76.70 27.58 52.44 63.20
Performance Gain +16.84 +29.66 +33.74 +9.73 +21.17 +26.46 +28.40 +39.50 +39.20 +21.22 +33.28 +35.62
12MCLIP 24.52 48.28 59.82 14.28 34.52 46.29 44.70 71.80 80.40 29.06 58.62 70.00
VeCLIP 47.78 72.54 81.56 31.62 57.19 68.47 73.90 92.30 95.90 55.68 80.78 87.64
Performance Gain +23.26 +24.26 +21.74 +17.34 +22.67 +22.18 +29.20 +20.50 +15.50 +26.62 +22.16 +17.64
100MCLIP 47.24 72.34 81.56 30.61 56.49 67.91 74.40 93.20 96.70 57.16 88.12 88.98
VeCLIP 64.82 85.56 91.98 46.12 71.19 80.23 89.30 97.70 99.20 73.10 89.12 93.14
Performance Gain +17.58 +13.22 +10.42 +15.51 +14.70 +12.32 +14.90 +4.50 +2.50 +15.94 +1.00 +4.16
200MCLIP 52.20 76.22 85.04 34.97 60.42 71.08 80.90 94.90 97.60 63.26 86.58 92.26
VeCLIP 67.20 87.28 92.70 48.40 73.26 81.79 91.10 98.50 99.70 76.32 93.50 96.40
Performance Gain +15.00 +11.06 +7.66 +13.43 +12.84 +10.71 +10.20 +3.60 +2.10 +13.06 +6.92 +4.14
300MCLIP 54.24 78.14 86.48 36.98 62.32 72.70 81.30 95.80 97.80 65.80 88.28 93.16
VeCLIP 67.80 87.94 92.84 48.91 73.54 82.11 91.20 99.10 99.80 76.30 93.00 96.44
Performance Gain +13.56 +9.80 +6.36 +11.93 +11.22 +9.41 +9.90 +3.30 +2.00 +10.50 +4.72 +3.28
of +16.84%/+23.26% in Recall@1 for COCO image-to-
text retrieval, respectively. Notably, the strides made in
Flickr30k are particularly noteworthy, with a remarkable
+28.40%/+29.20% improvement in Recall@1. Subsequently,
we scale our approach to 100M and 200M, where we ob-
serve sustained and substantial improvements. Notably, we
achieve a noteworthy +17.58%/+15.00% enhancement in
COCO image-to-text retrieval performance using 100M and
200M, respectively. Furthermore, we observe a diminishing
improvement margin as we scale up the dataset. Initially, we
achieve a substantial 28.40% improvement in image-to-text
retrieval for Flickr30k with the 3M dataset, which subse-
quently decreases to 10.20% when employing the 200M
dataset. These findings show the advantages of our proposed
pipeline for enhancing CLIP pre-training. By demonstrating
its scalability from 3M to 300M, we provide compelling evi-
dence of its applicability in real-world scenarios, particularly
for training CLIP from scratch using WIT datasets.
Image-to-image retrieval. We use GPR1200 [ 34] with
6 domains for this setting: Google Landmarks V2 (natural
and architectural landmarks) denoted as Land, IMDB Faces
denoted as Faces, iNat (plants, animals, insects and fungi),
INSTRE (planar images and photographs of logos/toys) de-
noted as INST, ImageNet Sketch denoted as Sketch, and SOP
(products and objects, partly isolated). The results (mAP)
are summarized in Table 2. We attain a performance gain of
5.22%/3.92% under small/medium scales (3M/12M). Evenupon upscaling the dataset to 200M, we observe a notable
1.84% increase in average score across six domains. Notably,
our primary performance boost is derived from the Sketch do-
main, underlining the crucial role of visual concepts in zero-
shot transferability. Consequently, our visually-enriched
captions play a pivotal role in learning such transferability
towards downstream tasks.
Data efficiency for pre-training. To show the data ef-
ficiency of VeCLIP, we include ALIGN [ 17], pre-trained
on 1.8B data (denoted as 1.8B-ALIGN), and our in-house
CLIP [ 32] model trained on 1.4B data (denoted as 1.4B-
CLIP) as baselines trained at a significantly larger scale. We
use these models utilizing over tenfold more data compared
to our setting to show the data efficiency of VeCLIP training.
VeCLIP can outperform 1.4B-CLIP model when scaling up
to 100M, representing approximately 7% of its size, across
nearly all downstream tasks. Specifically, in COCO, we
achieve +3.44%/+1.64% gain in Recall@1 for both retrieval
tasks. Upon further scaling to 200M, the improvement be-
comes even more pronounced, reaching +5.82%/+3.92%.
Furthermore, we achieve a notable +8.60%/+2.80% gain in
COCO retrieval, as well as a +2.50%/+0.62% improvement
in Flickr30k, when compared to the 1.8B-ALIGN model.
Remarkably, these improvements are achieved with only
11.1% of the data utilized in the pre-training process. These
results show the data efficacy of VeCLIP. When we scale
it to 300M, the results are similar to 200M. The results on
6Table 2. Image-to-image retrieval results (mAP) on 6-domain GPR1200 [ 34].
Data ModelDomain Name
Land Faces iNat INST Sketch SOP All
3MCLIP 57.98 20.76 17.61 31.14 18.23 74.29 36.67
VeCLIP 66.55 23.51 20.43 38.63 24.59 77.65 41.89
12MCLIP 74.47 30.65 23.60 52.15 30.68 84.25 49.30
VeCLIP 79.30 31.72 25.53 56.65 41.42 84.69 53.22
100MCLIP 85.64 51.68 29.66 68.19 42.45 90.38 61.33
VeCLIP 85.59 42.83 30.72 71.96 52.59 90.54 62.37
200MCLIP 86.96 56.54 30.95 71.51 46.03 90.95 63.83
VeCLIP 86.40 48.48 31.72 73.74 56.52 91.16 65.67
Table 3. Zero-shot classification results (Top- kAccuracy) on ImageNet and
ImageNetV2 [33].
Data ModelImageNet ImageNetV2
Top-1 Top-5 Top-10 Top-1 Top-5 Top-10
3MCLIP 5.46 21.05 28.70 7.09 18.52 25.83
VeCLIP 15.98 34.11 43.23 13.51 30.03 38.93
12MCLIP 31.60 58.80 69.49 27.03 52.68 63.37
VeCLIP 38.11 66.74 76.36 32.53 60.16 70.50
100MCLIP 58.64 85.82 91.79 50.96 79.77 86.91
VeCLIP 60.77 87.77 93.16 54.17 82.51 89.24
200MCLIP 63.72 89.26 94.11 56.84 83.50 89.79
VeCLIP 64.62 90.27 94.90 57.67 85.24 91.62
Figure 3. Performance gain on downstream tasks across
different data scales.
Table 4. Zero-shot classification accuracy. Top-1 accuracies (% ) of VTAB [ 46] across 9 tasks (6 from natural and 3 from specialized sets)
are reported. Full table can be found in Appendix Table A7.
Data ModelNatural Sets Specialized SetsAverageCaltech101 CIFAR100 SVHN DTD OxPet Flowers102 EuroSAT RESISC45 Camelyon
3MCLIP 39.50 9.83 20.89 7.42 7.44 10.40 11.94 7.93 50.65 18.45
VeCLIP 54.30 17.74 18.74 11.23 10.09 22.75 7.35 16.54 52.52 23.48
12MCLIP 70.43 30.06 30.11 30.69 34.51 33.67 8.87 30.05 53.46 35.76
VeCLIP 70.58 45.10 23.61 30.90 36.22 43.94 27.46 38.09 55.54 41.27
100MCLIP 81.44 54.75 38.70 57.28 70.51 51.71 34.45 48.56 53.87 54.59
VeCLIP 81.64 64.62 46.49 57.51 64.81 66.41 46.23 51.75 58.51 59.78
200MCLIP 82.30 61.87 42.83 64.29 75.60 58.67 46.73 55.59 59.30 60.79
VeCLIP 83.14 68.14 44.93 61.95 72.61 68.51 47.36 55.10 62.59 62.70
300M can be found in Appendix. Therefore, we stop further
scaling up the dataset.
4.3. Results on Image Classification
ImageNet. We use the same prompt as CLIP (A photo
of a [classname].) for zero-shot evaluation on both Im-
ageNet [ 10] and ImageNetV2 [ 33]. The main results are
summarized in Table 3. We report Top-1, Top-5, and Top-
10 accuracies. In small and medium-scale settings, we ob-
serve a substantial improvement: +10.52%/+6.42% gains
in Top-1 accuracy on ImageNet/ImageNetV2 under the 3M
setting, and +6.51%/5.50% gains under the 12M setting.
While the improvement becomes marginal upon scaling to100M/200M, we still achieve noteworthy +2.07%/+3.21%
and +0.90%/+0.83% gains on 100M and 200M across Ima-
geNet and ImageNetV2, respectively. This shows the data
efficiency of our pre-training approach.
Visual Task Adaptation Benchmark (VTAB). Besides
ImageNet/ImageNetV2, we also select VTAB [ 46] for eval-
uation. Table 4 summarizes zero-shot image classification
results for both the original CLIP models and our models,
utilizing the identical prompt set from CLIP. Our approach
consistently achieves comparable or superior performance
to CLIP across the majority of datasets. For instance, we
observe an average accuracy gain of over 5% under settings
of 3M, 12M, and 100M. Even upon scaling up to 200M,
7we maintain a notable gain of +1.91%. These results show
great robustness on zero-shot classification tasks across dif-
ferent data distributions. We show the overall trend of the
performance gain over the data scale in Figure 3.
4.4. Performance trend across scales
Besides the performance gain, we also visualize the perfor-
mance trend across data scales in pre-training. As shown
in Figure 4, the performance of CLIP utilizing original Alt-
Texts exhibits a marked surge with the increased data size:
while its starting point is poor at 3M, it demonstrates swift
progression up to 12M and 100M. However, once scaled
beyond 100 million, the performance trend exhibits a grad-
ual and eventually saturated growth. On the other hand,
commencing with a higher baseline, VeCLIP employing Ve-
Cap demonstrates substantial improvement in comparison to
CLIP within small to medium scales (3M and 12M). As we
progress beyond 300M, the performance gains of VeCLIP
become relatively incremental but still noticeable in retrieval
tasks. Both CLIP and VeCLIP reach a saturation point when
scaled up to 100M: once over 100M, the performance gain
becomes gradual and marginal.
4.5. Complementary to other datasets to achieve
state-of-the-art performance
Our VeCap datasets with visual-enriched captions can also be
complementary to other well-curated dataset. For example,
DFN [ 13] has shown benefits on CLIP. To demonstrate that,
we train CLIP models with VeCap and DFN separately and
also a combination with them. All the models are trained
under same configuration for learning rate, maximum steps,
and so on.
We summarize the results in Table 5. The high-quality
descriptive captions from VeCap can achieve superior results
compared to DFN in retrieval tasks. However, the perfor-
mance on classification tasks are inferior. After we combine
DFN and VeCap for training, CLIP can achieve the most
improvements for all model sizes.
We also train a H/14 model with resolution 336x336,
and compare it with the state-of-the-art models like Meta-
CLIP [ 42] and DFN [ 13]. The results are summarized in
row 6 to 8 of table 5. Albeit trained on different resolutions
and recipes, the CLIP model with VeCap+DFN is compat-
ible with other models and provide yet another option for
downstream tasks2.
Our VeCLIP with DFN [ 13] can outperform FLIP [ 22]
and OpenAI CLIP with different backbones (as shown in
Table A10 in Appendix). Specifically, our ViT-H/14 model
achieves impressive 83.1% of accuracy on ImageNet. We
leave the further study of combing the synthetic data (VeCap)
with other data curation approaches as a future work.
2Note we took the DFN-H/14 model from its original paper, which is
trained 7 epochs, our model is only trained roughly around 2 epochs.4.6. Ablation Study
Importance of visual-enriched concepts. Different from
previous rewriting methods, our primary emphasis lies in fus-
ing visual-enriched concepts extracted from images. The ab-
lation findings are summarized in Table 6a. We use 3M/12M
as examples to show the performance gain in small/medium
scales. Original AltTexts shows its limitation in retrieval
tasks due to its noise and limited image-specific information.
VeC generated from LLaV A can boost the performance on
retrieval tasks but may hurt the performance on ImageNet
zero-shot task. Introducing VeCap can further improve Ve-
Cap in all settings. Intriguingly, the zero-shot ImageNet
results still lag behind the original AltText. In essence, our
VeCap exerts a profound influence on retrieval prowess yet
exerts a negative effect on classification tasks. We posit
that this phenomenon arises from the following two rea-
sons: 1) there can be a distributional shift in prompts from
pre-training to zero-shot inference in ImageNet, particularly
noteworthy given the extended length and augmented visual
content of VeCap; 2) the data diversity is hurt by LLM rewrit-
ing as LLM uses the same writing/paraphrasing style to fuse
VeCap and AltText.
Importance of mixed training strategies. To mitigate
the aforementioned issues, we propose a mixed training
scheme to alternate between AltTexts and VeCap to provide
more data variety during pre-training. We summarize the
ablation results of VeCLIP in Table 6b. First, we observe a
slight performance improvement by randomly selecting one
AltText in cases where multiple AltTexts are associated with
an image. This practice augments data diversity during pre-
training. Second, interchanging between AltText and VeCap
proves to be advantageous, not only in retaining substantial
performance gains in retrieval tasks but also in markedly
elevating zero-shot results on ImageNet. Lastly, leveraging
all AltTexts and VeCap within the mixed training approach in
VeCLIP achieves superior results across nearly all settings.
Larger backbone architecture. We also investigate a
larger backbone architecture, e.g., ViT-L/14 and ViT-H/14.
The detailed results can be found in both Table 5 and Ap-
pendix C.1. VeCLIP scaled up in backbone size can consis-
tently outperform the original CLIP in all downstream tasks.
Besides, a larger backbone (ViT-L/14) can also achieve up
to 5.87% improvement compared to ViT-B/16. These find-
ings support the effectiveness of VeCLIP in improving CLIP
pre-training, regardless of the specific underlying backbone
architecture.
Generalizability of VeCap on well-curated datasets.
Besides our WIT datasets, we evaluate VeCap on well-
curated CC3M/CC12M. Table 6b shows CLIP achieves bet-
ter performance when pre-trained on CC3M compared to
pre-trained on WIT-3M, indicating the importance of high-
quality captions for pre-training. With VeCap to further
improve the quality of CC3Ms captions, CLIP can achieve
8(a) CLIP (b) VeCLIP
(c) CLIP (d) VeCLIP
Figure 4. Performance trend with ViT-B/16 as the vision backbone. (a) and (c) show the trend of CLIP with original AltTexts while (b) and
(d) show the trend of VeCLIP with LLM-VeC. The performance is improved significantly when we scale pre-training data up to 100M. Once
over 100M, the performance gain becomes gradual and incremental.
Table 5. CLIP training with VeCap and DFN [13], and its comparison with the state-of-the-art models.
Model Resolution DataCOCO (R@1) Flickr30k (R@1)ImageNet
I2T T2I I2T T2I
B/16 224DFN [13] 63.0 43.2 87.1 70.4 76.2
VeCap+DFN 66.3 45.1 88.8 73.6 76.2
Comparison to other state-of-the-art models
DFN [13] 68.5 48.5 89.2 75.1 81.0
L/14 224 FLIP [22] 60.2 44.2 89.1 75.4 74.6
VeCap+DFN 70.8 49.5 92.4 78.4 81.1
224 MetaCLIP [42] 67.2 49.5 92.1 78.5 80.5
H/14 378 DFN [13] 71.8 55.6 94.0 82.1 84.4
336 VeCap+DFN 72.8 52.3 93.6 82.6 83.1
9Data CaptionPrompt
ConstraintCOCO (R@1) Flickr30k (R@1)ImageNet ImageNetV2I2T T2I I2T T2I
WIT-3MAltText - 5.18 3.40 10.50 6.88 8.02 6.88
VeC - 16.76 9.57 32.60 20.06 7.31 6.58
VeCap  17.34 9.52 37.30 21.62 8.12 6.83
VeCap  18.10 9.51 40.00 21.94 8.20 7.39
WIT-12MAltText - 22.58 14.23 44.40 30.90 31.14 25.91
VeC - 40.06 24.59 64.10 43.46 7.29 14.74
VeCap  44.52 27.46 70.90 50.46 21.05 18.11
VeCap  46.82 26.61 72.60 50.94 20.99 18.41
(a) Importance of visual-enriched concepts for data quality. We use the AltText with Highest CLIP Score (HCS) if multiple
AltTexts exist on the same image in all settings.
Data AltText VeCapTraining
SamplingCOCO (R@1) Flickr30k (R@1)ImageNet ImageNetV2I2T T2I I2T T2I
WIT-3M  HCS 5.18 3.40 10.50 6.88 8.02 6.88
  random 5.46 3.28 12.20 6.36 8.26 7.09
  HCS 18.10 9.51 40.00 21.94 8.20 7.39
  HCS&mixed 19.70 12.14 39.30 25.60 14.83 12.36
  random&mixed 22.30 13.01 40.60 27.58 15.98 13.51
WIT-12M  HCS 22.58 14.23 44.40 30.90 31.14 25.91
  random 23.32 14.28 44.70 29.06 31.60 27.03
  HCS 46.82 26.61 72.60 50.94 20.99 18.41
  HCS&mixed 46.00 31.10 72.50 56.82 37.45 32.41
  random&mixed 47.78 31.62 73.90 55.68 38.11 32.51
CC3M  - 13.88 9.64 26.30 18.04 14.59 12.52
  random&mixed 32.04 22.07 57.20 36.54 20.73 17.90
(b) Importance of the mixed training scheme for data variety. HCS refers to using the AltText with Highest CLIP Score while random refers to
randomly selecting one if multiple AltTexts exist.
Table 6. Ablation study of VeCLIP. The highest score is bold, and the second is underlined. mixed is our proposed mixed training scheme
to alternate among captions.
significant improvement, since the captions of CC3M are
of higher quality than our noisy WIT dataset. CC3M with
its original captions can outperform the performance of our
WIT-3M with AltTexts, indicating CC3M is of higher quality.
VeCap can significantly improve CLIP under CC3M settings,
e.g., +18.16% on the I2T task of COCO and +6.14% on Ima-
geNet, showing its generalizability on well-curated datasets.
More results are in Appendix C.2.
5. Discussion
Conclusion. We present a simple yet effective approach
to improve CLIP pre-training with leveraging LLaV A and
LLMs to rewrite the captions with more visual-enriched
concepts. VeCLIP is intentionally designed to be scalable
and adaptable for handling extensive image-text datasets
obtained from web crawling. We conduct a thorough evalua-
tion of VeCLIP on a diverse range of raw and noisy datasets,
spanning small, medium, and large scales. The results re-
veal a substantial performance boost, providing compelling
evidence for the effectiveness of our strategy in enhancing
large-scale VLM pre-training. VeCLIP can significantly re-
duce the computational cost and the size of training data forlarge models to reach competitive results as vanilla CLIP.
Future work. We employ CLIP as an illustrative in-
stance to highlight the importance of aligning text and im-
ages within the training dataset. For future work, we plan
to use the collected large-scale dataset to improve the pre-
training of other types of VLMs. Further, LLM can generate
outputs that encompass factual inaccuracies and hallucina-
tions. Thus, we also plan to delve into more sophisticated
filtering techniques to remove such descriptions.
Limitation. We only leverage LLaV A to exploit the
visual concepts. However, the quality measurement metric
for such generative AI is still under study.
References
[1]Amro Abbas, Kushal Tirumala, D aniel Simig, Surya Ganguli,
and Ari S Morcos. Semdedup: Data-efficient learning at
web-scale through semantic deduplication. arXiv preprint
arXiv:2303.09540 , 2023. 3
[2]James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng
Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee,
Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu,
Yunxin Jiao, and Aditya Ramesh. Improving image generation
with better captions. OpenAI , 2023. 2
10[3]James Bradbury, Roy Frostig, Peter Hawkins, Matthew James
Johnson, Chris Leary, Dougal Maclaurin, George Necula,
Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne,
and Qiao Zhang. JAX: composable transformations of
Python+NumPy programs. Github , 2018. 5, 1
[4]Liangliang Cao, Bowen Zhang, Chen Chen, Yinfei Yang,
Xianzhi Du, Wencong Zhang, Zhiyun Lu, and Yantao Zheng.
Less is more: Removing text-regions improves clip training
efficiency and robustness. arXiv preprint arXiv:2305.05095 ,
2023. 3
[5]Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
Soricut. Conceptual 12m: Pushing web-scale image-text pre-
training to recognize long-tail visual concepts. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 35583568, 2021. 2, 3, 1
[6]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In International conference on
machine learning , pages 15971607. PMLR, 2020. 1
[7]Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing
image scene classification: Benchmark and state of the art.
Proceedings of the IEEE , 105(10):18651883, 2017. 2
[8]Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell
Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-
mann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scal-
ing laws for contrastive language-image learning. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 28182829, 2023. 3
[9]M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A.
Vedaldi. Describing textures in the wild. In Proceedings of
the IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR) , 2014. 2
[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li
Fei-Fei. Imagenet: A large-scale hierarchical image database.
In2009 IEEE conference on computer vision and pattern
recognition , pages 248255. Ieee, 2009. 5, 7, 1
[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Transform-
ers for image recognition at scale. In International Conference
on Learning Representations , 2020. 1, 5
[12] Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yon-
glong Tian. Improving clip training with language rewrites.
arXiv preprint arXiv:2305.20088 , 2023. 1, 2, 3
[13] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig
Schmidt, Alexander T Toshev, and Vaishaal Shankar. Data fil-
tering networks. In NeurIPS 2023 Workshop on Distribution
Shifts: New Frontiers with Foundation Models , 2023. 8, 9, 2,
5
[14] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener-
ative visual models from few training examples: An incre-
mental bayesian approach tested on 101 object categories. In
2004 conference on computer vision and pattern recognition
workshop , pages 178178. IEEE, 2004. 5, 2
[15] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Dollar, and Ross Girshick. Masked autoencoders are scalablevision learners. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 16000
16009, 2022. 1
[16] Patrick Helber, Benjamin Bischke, Andreas Dengel, and
Damian Borth. Introducing eurosat: A novel dataset and
deep learning benchmark for land use and land cover classifi-
cation. In IGARSS 2018-2018 IEEE International Geoscience
and Remote Sensing Symposium , pages 204207. IEEE, 2018.
2
[17] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representation
learning with noisy text supervision. In International confer-
ence on machine learning , pages 49044916. PMLR, 2021.
1, 3, 6
[18] Alex Krizhevsky. Learning multiple layers of features from
tiny images. Canadian Institute for Advanced Research , 2009.
2
[19] Gukyeong Kwon, Zhaowei Cai, Avinash Ravichandran, Er-
han Bas, Rahul Bhotika, and Stefano Soatto. Masked vision
and language modeling for multi-modal representation learn-
ing. In The Eleventh International Conference on Learning
Representations , 2023. 1
[20] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip:
Bootstrapping language-image pre-training for unified vision-
language understanding and generation. In International Con-
ference on Machine Learning , pages 1288812900. PMLR,
2022. 1
[21] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichten-
hofer, and Kaiming He. Scaling language-image pre-training
via masking. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 23390
23400, 2023. 3
[22] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichten-
hofer, and Kaiming He. Scaling language-image pre-training
via masking. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 23390
23400, 2023. 6, 8, 9, 2, 5
[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer VisionECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings, Part
V 13, pages 740755. Springer, 2014. 5
[24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. arXiv preprint arXiv:2304.08485 ,
2023. 2, 3
[25] Pratyush Maini, Sachin Goyal, Zachary C Lipton, J Zico
Kolter, and Aditi Raghunathan. T-mars: Improving visual
representations by circumventing text feature learning. arXiv
preprint arXiv:2307.03132 , 2023. 3
[26] Norman Mu, Alexander Kirillov, David Wagner, and Sain-
ing Xie. Slip: Self-supervision meets language-image pre-
training. In European Conference on Computer Vision , pages
529544. Springer, 2022. 3
[27] Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Se-
woong Oh, and Ludwig Schmidt. Improving multi-
11modal datasets with image captioning. arXiv preprint
arXiv:2307.10350 , 2023. 2
[28] Maria-Elena Nilsback and Andrew Zisserman. Automated
flower classification over a large number of classes. In 2008
Sixth Indian conference on computer vision, graphics & image
processing , pages 722729. IEEE, 2008. 5, 2
[29] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and
CV Jawahar. Cats and dogs. In 2012 IEEE conference on
computer vision and pattern recognition , pages 34983505.
IEEE, 2012. 2
[30] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu,
Adams Wei Yu, Minh-Thang Luong, Mingxing Tan, and
Quoc V Le. Combined scaling for zero-shot transfer learning.
arXiv preprint arXiv:2111.10050 , 2021. 3
[31] Bryan A Plummer, Liwei Wang, Chris M Cervantes,
Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.
Flickr30k entities: Collecting region-to-phrase correspon-
dences for richer image-to-sentence models. In Proceedings
of the IEEE international conference on computer vision ,
pages 26412649, 2015. 5
[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervision.
InICML , pages 87488763, 2021. 1, 3, 6
[33] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and
Vaishaal Shankar. Do imagenet classifiers generalize to im-
agenet? In International conference on machine learning ,
pages 53895400. PMLR, 2019. 5, 7
[34] Konstantin Schall, Kai Uwe Barthel, Nico Hezel, and Klaus
Jung. Gpr1200: a benchmark for general-purpose content-
based image retrieval. In International Conference on Multi-
media Modeling , pages 205216. Springer, 2022. 5, 6, 7
[35] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:
Open dataset of clip-filtered 400 million image-text pairs.
arXiv preprint arXiv:2111.02114 , 2021. 1
[36] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes,
Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.
Laion-5b: An open large-scale dataset for training next gen-
eration image-text models. Advances in Neural Information
Processing Systems , 35:2527825294, 2022. 1
[37] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improv-
ing neural machine translation models with monolingual data.
arXiv preprint arXiv:1511.06709 , 2015. 4
[38] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-
tinet, Marie-Anne Lachaux, Timoth ee Lacroix, Baptiste
Rozi ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971 , 2023. 3
[39] Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Co-
hen, and Max Welling. Rotation equivariant cnns for digital
pathology. In Medical Image Computing and Computer As-
sisted InterventionMICCAI 2018: 21st International Confer-
ence, Granada, Spain, September 16-20, 2018, Proceedings,
Part II 11 , pages 210218. Springer, 2018. 2[40] Jason Wei and Kai Zou. Eda: Easy data augmentation tech-
niques for boosting performance on text classification tasks.
arXiv preprint arXiv:1901.11196 , 2019. 4
[41] Wentao Wu, Aleksei Timofeev, Chen Chen, Bowen Zhang,
Kun Duan, Shuangning Liu, Yantao Zheng, Jon Shlens, Xi-
anzhi Du, Zhe Gan, et al. Mofi: Learning image represen-
tations from noisy entity annotated images. arXiv preprint
arXiv:2306.07952 , 2023. 1
[42] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Rus-
sell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke
Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip
data. arXiv preprint arXiv:2309.16671 , 2023. 8, 9
[43] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-
jtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive
captioners are image-text foundation models. arXiv preprint
arXiv:2205.01917 , 2022. 3
[44] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,
Boxin Li, Chunyuan Li, et al. Florence: A new foundation
model for computer vision. arXiv preprint arXiv:2111.11432 ,
2021. 3
[45] Netzer Yuval. Reading digits in natural images with unsuper-
vised feature learning. In Proceedings of the NIPS Workshop
on Deep Learning and Unsupervised Feature Learning , 2011.
2
[46] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre
Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, An-
dre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al.
A large-scale study of representation learning with the visual
task adaptation benchmark. arXiv preprint arXiv:1910.04867 ,
2019. 5, 7, 1, 4
[47] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner,
Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit:
Zero-shot transfer with locked-image text tuning. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 1812318133, 2022. 3
[48] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li,
Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez,
and Ion Stoica. Judging llm-as-a-judge with mt-bench and
chatbot arena, 2023. 4
121. Appendix
We provide additional details for datasets, experimental set-
tings, results, and analysis in the supplementary material.
A. Dataset details
Pre-training datasets. Instead of using well-curated
datasets, we use image-AltText pairs sampled from a web-
crawled dataset [ 41]. We collect 300M image-text pairs
from the Web and denote it as WIT-300M. Based on WIT-
300M, we build four subsets to cover from small to large
scales. Specifically, WIT-200M is a subset of WIT-300M.
WIT-100M is a subset of WIT-200M. WIT-12M is a subset
of WIT-100M. WIT-3M is a subset of WIT-12M.
VTAB datasets. We choose 9 classification datasets
suitable for zero-shot evaluation from VTAB [ 46]. Table A1
summarizes zero-shot image classification datasets. For both
original CLIP models and our models, we use the identi-
cal prompt set from CLIP. Every class label is expanded
using a collection of prompt templates, as defined by CLIP,
including examples like A photo of a [classname]. The
class embedding is then computed by taking the average
of the embeddings of all such templates, followed by L2-
normalization.
B. Implementation details
Pre-training hyper-parameters. We summarize the pre-
training hyper-parameters for CLIP training in Table A2. We
pre-train models on up to 512 TPUs with JAX [3].
C. More experimental results
In this section, we present more detailed experimental results
and our ablation studies (e.g., generalization of VeCLIP with
a large backbone, public and well-curated datasets for pre-
training).
C.1. Larger backbone architectures
We also investigate the performance of VeCLIP using a larger
backbone architecture, ViT-L/14. The comparison results
are summarized in Table A3. First, VeCLIP shows a consis-
tent improvement over CLIP employing ViT-L/14 across all
downstream tasks. Second, VeCLIP utilizing ViT-L/14 sur-
passes its counterpart employing ViT-B/16, notably excelling
in image classification tasks, achieving a notable improve-
ment of over 5% on both ImageNet and ImageNetV2. This
shows that VeCLIP has the potential to be scalable with
larger backbone architectures and larger-scale datasets.
C.2. Generalization on well-curated datasets:
CC3M and CC12M
Besides our crawled noisy WIT datasets, we also use a well-
curated dataset, e.g., CC3M and CC12M [ 5], to show the ef-fectiveness and generalizability of our proposed approach on
well-curated datasets. CC3M and CC12M [ 5] were curated
via several rounds of comprehensive refining and filtering to
get high-quality image-caption pairs. We show high-quality
examples of CC3M and the comparison of CC3Ms captions
and WIT-3Ms AltTexts in Appendix D. We present an ex-
perimental comparison between our crawled WIT datasets
and well-curated CC3M/CC12M [5] in this subsection.
3M. As shown in Table A4, CC3M outperforms WIT-3M
when coupled with CLIP pre-training, yielding a notable
increase of +10.70% on the COCO I2T task. Additionally,
VeCLIP exhibits substantial improvement for both WIT-3M
and CC3M. Notably, we achieve a remarkable over 30%
improvement on the I2T task in Flickr30K, and an impressive
over 5% boost on ImageNet and ImageNetV2.
12M. Similar to 3M settings, CC12M exhibits superior
quality and attains better results in contrast to WIT-12M
when utilized with CLIP and original AltTexts. VeCLIP
demonstrates notable improvements for both WIT-12M and
CC12M. For instance, VeCLIP yields a remarkable +12.27%
increase in the I2T task of COCO, along with an impressive
over 5% improvement on both ImageNet and ImageNetV2.
These findings emphasize the effectiveness and generaliz-
ability of VeCLIP in both noisy web-crawled datasets and
meticulously curated datasets, where a richer set of visual
concepts is harnessed for pre-training.
C.3. Complete visual descriptions vs simplified en-
tity representations
In Table 6b of the main paper, we note that sole training on
VeCap might detriment zero-shot performance in compari-
son to the original AltText. Conversely, our mixed training
approach yields optimal outcomes. This intriguing finding
propels us toward a more profound investigation of zero-shot
classification tasks. Following established works [ 12,32],
we employ an identical set of prompting templates, such as
a photo of a [CLS] for ImageNet [ 10]. It is conceivable
that this direct and uncomplicated prompt may diverge sig-
nificantly from VeCaps pre-training, which encompasses a
more extensive and intricate set of visual concepts. To ad-
dress this, we reformulate VeCap into a format as Simplified
Entity Representation (SER). Specifically, we employ the
NLTK package to extract entities from VeCap and subse-
quently apply filtering to retain only noun entities, denoted
as(A, B, C... )U. This transformation results in VeCap
being presented as a photo of [ U], offering a concise repre-
sentation of all extracted entities. The results are summarized
in Table A5. Surprisingly, we find that even with SER-style
captions, the zero-shot performance remains inferior to that
achieved with the original AltText. We hypothesize that this
discrepancy may arise from a lack of data diversity. When
all sentences adhere to the same distribution, there exists
a risk of overfitting in the pre-trained model, resulting in
1Table A1. Details of 9 VTAB zero-shot classification datasets.
Dataset Metric Categories Train Size Test Size
CIFAR-100 [18] Accuracy 100 50,000 10,000
SVHN [45] Accuracy 10 73,257 26,032
DTD [9] Accuracy 47 3,760 1,880
Oxford Pets [29] Mean per class 37 3,680 3,669
Caltech101 [14] Mean per class 102 3,060 6,085
Flowers102 [28] Mean per class 102 2,040 6,149
EuroSAT [16] Accuracy 10 10,000 5,000
RESISC45 [7] Accuracy 45 25,200 6,300
Camelyon [39] Accuracy 2 262,144 32,768
Table A2. Details of the pre-training hyper-parameters for CLIP training on our web-crawled datasets.
(a)Pre-training hyper-parameters on 3M.
Config Value
Batch size 8,192
Optimizer AdamW
Learning rate 5104
Weight decay 0.5
Adam β β1, β2= (0.9,0.98)
Adam ϵ 1108
Total epochs 40
Warm up epochs 1
Learning rate schedule cosine decay(b)Pre-training hyper-parameters on 12M.
Config Value
Batch size 8,192
Optimizer AdamW
Learning rate 5104
Weight decay 0.5
Adam β β1, β2= (0.9,0.98)
Adam ϵ 1108
Total epochs 35
Warm up epochs 1
Learning rate schedule cosine decay
(c)Pre-training hyper-parameters on 100M.
Config Value
Batch size 32,768
Optimizer AdamW
Learning rate 5104
Weight decay 0.2
Adam β β1, β2= (0.9,0.98)
Adam ϵ 1106
Total epochs 32
Warm up iterations 2,000
Learning rate schedule cosine decay(d)Pre-training hyper-parameters on 200M.
Config Value
Batch size 32,768
Optimizer AdamW
Learning rate 5104
Weight decay 0.2
Adam β β1, β2= (0.9,0.98)
Adam ϵ 1106
Total epochs 32
Warm up iterations 2,000
Learning rate schedule cosine decay
suboptimal performance in downstream tasks.
C.4. Main results with WIT-300M
We show the detailed results with the Web-crawled Image-
Text 300M dataset (WIT-300M) here. We summarize the
results on various downstream tasks in Table A8. There are
two major observations. First, we observe that the results ob-
tained with a dataset size of 300M are close to those achieved
with 200M for both CLIP and VeCLIP models. This suggests
that a dataset scale of 200 million is sufficient for effectively
training a ViT-B/16-based CLIP model. Second, VeCLIP
achieves significant improvement on retrieval tasks even un-
der 300M settings. Nevertheless, the improvement observedin ImageNet/ImageNetV2 is marginal.
As shown in Table A9, our VeCLIP with DFN [ 13] can
outperform FLIP [ 22] and OpenAI CLIP with different back-
bones. Specifically, our ViT-H/14 model achieves impressive
83.1% of accuracy on ImageNet. We leave the further study
of combing the synthetic data (VeCap) with other data cura-
tion approaches as a future work.
D. Caption quality comparison between well-
curated Datasets and WIT datasets
In Appendix C.2, we find CLIP performs notably better
when pre-trained on CC3M compared to the case of being
pre-trained on noisy crawled WIT datasets due to several
2Table A3. Ablation studies on different backbones with VeCLIP. We use 200M as the pre-training dataset.
Model BackboneCOCO (R@1) Flickr30k (R@1)ImageNet ImageNetV2I2T T2I I2T T2I
CLIP ViT-B/16 52.20 34.97 80.90 63.23 63.72 56.84
VeCLIP ViT-B/16 67.20 48.40 91.10 76.32 64.62 57.67
Performance Gain +15.00 +13.43 +10.20 +13.06 +0.90 +0.81
CLIP ViT-L/14 53.92 37.86 84.60 66.78 68.51 61.13
VeCLIP ViT-L/14 69.92 51.32 92.60 79.04 69.85 63.54
Performance Gain +16.00 +13.46 +8.00 +12.26 +1.34 +2.41
VeCLIP ViT-L/14 vs B/16 +2.72 +2.92 +1.50 +2.72 +5.23 +5.87
Table A4. Ablation studies on well-curated datasets (CC3M and CC12M [ 5]) and the effect of data quality with ViT-B/16 as the vision
backbone.
Model ModelCOCO (R@1) Flickr30k (R@1)ImageNet ImageNetV2I2T T2I I2T T2I
WIT-3MCLIP 5.18 3.40 10.50 6.88 8.02 6.88
VeCLIP 22.30 13.01 40.60 27.58 15.98 13.51
Performance Gain +17.12 +9.61 +30.10 +20.70 +7.96 +6.63
CC3MCLIP 13.88 9.64 26.30 18.04 14.59 12.52
VeCLIP 32.04 22.07 57.20 36.54 20.73 17.90
Performance Gain +18.16 +12.43 +30.90 +18.50 +6.14 +5.38
WIT-12MCLIP 22.58 14.23 44.40 30.90 31.14 25.91
VeCLIP 47.78 31.62 73.90 55.68 38.11 32.51
Performance Gain +25.20 +17.39 +29.50 +24.78 +6.97 +6.60
CC12MCLIP 37.96 24.40 59.70 44.90 39.24 34.41
VeCLIP 53.23 36.90 75.20 62.10 45.32 40.21
Performance Gain +15.27 +12.50 +15.50 +17.20 +6.08 +5.80
rounds of filtering and refining involved in the curation
of CC3M and CC12M. In this section, we show detailed
captions from CC3M and compare them with AltTexts from
WIT datasets.
Here we provide more examples of AltText and LLM-
VeC from WIT-3M:
1.AltText: Ring Capri Pomellato  Pomellato Online
Boutique
VeCap: Pomellatos Ring Capri features a delicate and
elegant white stone or possibly three pearls, set against a
white background.
2.AltText: Fiamma F45 L 450 Royal Blue Awning.
VeCap: The Fiamma F45 L 450 Royal Blue Awning is
featured on a white car with a visible red logo for perfect
closing, parked in a driveway under a tree, with a house
in the background.
3.AltText: Union votes for strike on pensions
VeCap: The man with white hair, dressed in a suit and
tie, exhibits a surprised or expressive look on his face,with his mouth open and hand near his face, creating a
dynamic and energetic expression.
4.AltText: r/reallifedoodles - I can show you the world
VeCap: The large orange and black drone hovers in the
air, carrying two small teddy bears attached to it, above a
patio area, as seen in the image.
5.AltText: 20 Amazon Skincare Products That Keep Sell-
ing Out
VeCap: 20 Amazon skincare products that keep selling
out feature a happy woman with dark skin, wearing a
white shirt and covering her face with her hands, with a
white spot or patch on her skin.
6.AltText: Durable White Arcane Dining Console Table
With 6 Hidden Chairs
VeCap: A durable white arcane dining console table with
6 hidden chairs is visually appealing and ready for use,
as seen in the image featuring a dining set with a white
table and two benches, surrounded by black chairs.
7.AltText: Peaceful apartment with wi fi internet access,
near old Quebec.
3Table A5. Ablation studies on VeCap and Simplied Entities Representation (SER). We use ViT-B/16 as the backbone and use 200M as the
pre-trained dataset.
Model CaptionCOCO (R@1) Flickr30k (R@1)ImageNet ImageNetV2I2T T2I I2T T2I
CLIP AltText 52.20 34.97 80.90 63.23 63.72 56.84
VeCLIP SER 65.88 49.04 89.20 75.96 58.58 52.89
VeCLIP VeCap 67.20 48.40 91.10 76.32 64.62 57.67
Table A6. Zero-shot classification accuracy. Top-1 Accuracies (% ) of VTAB [ 46] across 9 tasks (6 from natural and 3 from specialized sets)
are reported.
Data ModelNatural Sets Specialized SetsAverageCaltech101 CIFAR100 SVHN DTD OxPet Flowers102 EuroSAT RESISC45 Camelyon
Model Architecture: ViT-B/16
3MCLIP 39.50 9.83 20.89 7.42 7.44 10.40 11.94 7.93 50.65 18.45
VeCLIP 54.30 17.74 18.74 11.23 10.09 22.75 7.35 16.54 52.52 23.48
Performance Gain +14.80 +7.91 -2.15 +3.81 +2.65 +12.35 -4.59 +8.61 +1.87 +5.03
12MCLIP 70.43 30.06 30.11 30.69 34.51 33.67 8.87 30.05 53.46 35.76
VeCLIP 70.58 45.10 23.61 30.90 36.22 43.94 27.46 38.09 55.54 41.27
Performance Gain +0.15 +15.04 -6.50 +0.21 +1.71 +10.27 +18.59 +8.04 +2.08 +5.51
100MCLIP 81.44 54.75 38.70 57.28 70.51 51.71 34.45 48.56 53.87 54.59
VeCLIP 81.64 64.62 46.49 57.51 64.81 66.41 46.23 51.75 58.51 59.78
Performance Gain +0.20 +9.87 +7.79 +0.23 -5.70 +14.70 +11.78 +3.19 +4.64 +5.19
200MCLIP 82.30 61.87 42.83 64.29 75.60 58.67 46.73 55.59 59.30 60.79
VeCLIP 83.14 68.14 44.93 61.95 72.61 68.51 47.36 55.10 62.59 62.70
Performance Gain +0.84 +6.27 +2.10 -2.34 -2.99 +9.84 +0.63 -0.49 +3.29 +1.91
300MCLIP 83.58 63.36 50.04 66.16 74.30 61.81 39.95 56.44 53.94 61.06
VeCLIP 83.07 68.37 50.07 65.98 75.36 69.71 48.28 58.09 51.94 63.43
Performance Gain -0.51 +5.01 +0.03 -0.18 1.06 +7.90 +8.33 +1.65 -2.00 +2.37
VeCap: Experience a peaceful stay in a cozy apartment
with Wi-Fi internet access, located near historic Old Que-
bec, featuring a charming dining room with a set table
and chairs on a hardwood floor, complete with a white
refrigerator in the background.
8.AltText: CABLE BUJIA CHEVROLET CORSA 1.0 1.4
EFI FERRAZZI CABLE BUJIA CHEVROLET CORSA
1.0 1.4 EFI FERRAZZI
VeCap An array of cords and wires, comprising a black
rubber cable, is displayed on a pristine surface, featuring
diverse configurations and orientations, with some lying
horizontally and others positioned at angles.
Here we provide more examples of original caption and
VeCap from CC3M:
1.CC3M Caption: person runs with the ball during their
training session on friday.
VeCap: A group of soccer players, clad in red and black
jerseys, are energetically engaging in a game on a vast
field, with some running and others immersed in the
action, dispersed across the terrain.
2.CC3M Caption: a house with red roof with some bushes
and a lamp post in front.VeCap: A prominent two-story beige building with a
distinctive tile roof stands out in the area, illuminated by
a nearby lamp post. The building appears to be a complex
with several houses or apartments, adding a touch of
complexity to the surroundings.
3.CC3M Caption: eating a big sweet cupcake with choco-
late at cafe.
VeCap: A person holds a half-eaten blueberry muffin on
a plate, standing next to a dining table with a cup, while
eating a big sweet cupcake with chocolate at a cafe.
4.CC3M Caption: paper heart with red ribbon and a bow.
VeCap: A pink background showcases a heart-shaped
box with a bow, adorned in white with the message
Happy Valentines Day, positioned centrally within the
image.
5.CC3M Caption: person andactor at the premiere
VeCap: Two individuals, a man and a woman, are de-
picted standing together, both attired in formal attire. The
man is donning a tuxedo with a black bow tie, while the
woman is wearing a long dress. They seem to be posi-
tioning themselves for a photograph, possibly at a formal
event.
4Table A7. Image-to-Image retrieval results (mAP) on 6-domain
GPR1200.
Data ModelDomain Name
Land Faces iNat INST Sketch SOP All
3MCLIP 57.98 20.76 17.61 31.14 18.23 74.29 36.67
VeCLIP 66.55 23.51 20.43 38.63 24.59 77.65 41.89
12MCLIP 74.47 30.65 23.60 52.15 30.68 84.25 49.30
VeCLIP 79.30 31.72 25.53 56.65 41.42 84.69 53.22
100MCLIP 85.64 51.68 29.66 68.19 42.45 90.38 61.33
VeCLIP 85.59 42.83 30.72 71.96 52.59 90.54 62.37
200MCLIP 86.96 56.54 30.95 71.51 46.03 90.95 63.83
VeCLIP 86.40 48.48 31.72 73.74 56.52 91.16 65.67
300MCLIP 87.17 57.09 31.83 72.80 47.03 91.30 64.54
VeCLIP 86.22 48.51 32.05 75.29 56.18 91.25 66.91Table A8. Zero-shot classification results (Top- kAccuracy) on
ImageNet and ImageNetV2.
Data ModelImageNet ImageNetV2
Top-1 Top-5 Top-10 Top-1 Top-5 Top-10
3MCLIP 5.46 21.05 28.70 7.09 18.52 25.83
VeCLIP 15.98 34.11 43.23 13.51 30.03 38.93
12MCLIP 31.60 58.80 69.49 27.03 52.68 63.37
VeCLIP 38.11 66.74 76.36 32.53 60.16 70.50
100MCLIP 58.64 85.82 91.79 50.96 79.77 86.91
VeCLIP 60.77 87.77 93.16 54.17 82.51 89.24
200MCLIP 63.72 89.26 94.11 56.84 83.50 89.79
VeCLIP 64.62 90.27 94.90 57.67 85.24 91.62
300MCLIP 65.70 90.55 94.87 58.58 85.32 91.35
VeCLIP 65.71 91.15 95.36 58.76 86.31 91.95
Table A9. Comparison bwetween VeCLIP and other models.
Backbone Model DataCOCO (R@1) Flickr30k (R@1)ImageNetI2T T2I I2T T2I
ViT-B/16OpenAI CLIP OpenAI-400M 53.8 33.1 88.0 68.7 68.6
FLIP [22] LAION-400M - - - - 68.0
VeCLIP DFN [13] + VeCap 66.3 45.1 88.8 73.6 76.2
ViT-L/14OpenAI CLIP OpenAI-400M 58.4 37.8 88.0 68.7 75.3
FLIP [22] LAION-400M 60.2 44.2 89.1 75.4 74.6
VeCLIP DFN [13] + VeCap 71.1 51.1 93.1 81.0 82.0
ViT-H/14 VeCLIP DFN [13] + VeCap 72.8 52.3 93.6 82.6 83.1
6.CC3M Caption: wedding ceremony on the beach
VeCap: A picturesque wedding ceremony unfolds on a
stunning white sandy beach, where perfectly arranged
chairs accommodate guests in formal attire. The groom
and bride exude joy and love, basking in the warm sun-
light.
7.CC3M Caption: revenge is a dish best served cold ...
with lots of lettuce .
VeCap: A large, possibly turtle, tortoise with an angry
expression sits on rocks, displaying a saying or text mes-
sage that reads Revenge is a dish best cold served with
lots of lettuce.
8.CC3M Caption: interior of an abandoned factory
VeCap: The sunlit interior of an industrial building stands
in contrast to its darker exterior, with numerous windows
allowing natural light to flood the space, giving it an
empty and open appearance devoid of people or personal
touches.
Examining the aforementioned instances, it becomes evi-
dent that CC3Ms captions exhibit a notable level of preci-
sion and high quality, displaying a closer alignment with the
corresponding images. Conversely, WIT-3Ms AltTexts tend
to be more cluttered, signaling a comparatively subpar per-
formance in contrast to CC3M. Upon implementing VeCap,
even though CC3Ms captions are of high quality, they are
enhanced with more visual concepts leveraged via VeCap.Such integration of enriched visual concepts accounts for the
significant improvement we achieve in retrieval tasks (the
results are shown in Table A4).
E. More examples of WIT with VeCap
We conduct our scalable pipeline over 200 million image-
text pairs. We randomly select more examples below to
show the advantages of VeCap against the original AltText
in terms of visual concepts. The examples are visualized in
Figure A1.
5Figure A1. More examples of VeCap captions and AltTexts.
6
  TACOTRON : T OWARDS END-TO-ENDSPEECH SYN-
THESIS
Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weissy, Navdeep Jaitly,
Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengioy, Quoc Le, Yannis Agiomyrgiannakis,
Rob Clark, Rif A. Saurous
Google, Inc.
fyxwang,rjryan,rif g@google.com
ABSTRACT
A text-to-speech synthesis system typically consists of multiple stages, such as a
text analysis frontend, an acoustic model and an audio synthesis module. Build-
ing these components often requires extensive domain expertise and may contain
brittle design choices. In this paper, we present Tacotron, an end-to-end genera-
tive text-to-speech model that synthesizes speech directly from characters. Given
<text, audio >pairs, the model can be trained completely from scratch with ran-
dom initialization. We present several key techniques to make the sequence-to-
sequence framework perform well for this challenging task. Tacotron achieves a
3.82 subjective 5-scale mean opinion score on US English, outperforming a pro-
duction parametric system in terms of naturalness. In addition, since Tacotron
generates speech at the frame level, its substantially faster than sample-level au-
toregressive methods.
1 I NTRODUCTION
Modern text-to-speech (TTS) pipelines are complex (Taylor, 2009). For example, it is common for
statistical parametric TTS to have a text frontend extracting various linguistic features, a duration
model, an acoustic feature prediction model and a complex signal-processing-based vocoder (Zen
et al., 2009; Agiomyrgiannakis, 2015). These components are based on extensive domain expertise
and are laborious to design. They are also trained independently, so errors from each component
may compound. The complexity of modern TTS designs thus leads to substantial engineering efforts
when building a new system.
There are thus many advantages of an integrated end-to-end TTS system that can be trained on <text,
audio>pairs with minimal human annotation. First, such a system alleviates the need for laborious
feature engineering, which may involve heuristics and brittle design choices. Second, it more easily
allows for rich conditioning on various attributes, such as speaker or language, or high-level features
like sentiment. This is because conditioning can occur at the very beginning of the model rather
than only on certain components. Similarly, adaptation to new data might also be easier. Finally,
a single model is likely to be more robust than a multi-stage model where each components errors
can compound. These advantages imply that an end-to-end model could allow us to train on huge
amounts of rich, expressive yet often noisy data found in the real world.
TTS is a large-scale inverse problem: a highly compressed source (text) is decompressed into
audio. Since the same text can correspond to different pronunciations or speaking styles, this is a
particularly difﬁcult learning task for an end-to-end model: it must cope with large variations at the
signal level for a given input. Moreover, unlike end-to-end speech recognition (Chan et al., 2016)
These authors really like tacos.
yThese authors would prefer sushi.
1arXiv:1703.10135v2  [cs.CL]  6 Apr 2017Attention 
Pre-net CBHG 
Character embeddings Attention 
RNNDecoder 
RNN 
Pre-net Attention 
RNNDecoder 
RNN
Pre-net Attention 
RNNDecoder 
RNN
Pre-net CBHG Linear-scale 
spectrogram 
Seq2seq target 
with r=3 Griffin-Lim reconstruction 
Attention is applied 
to all decoder steps 
<GO> frame Figure 1: Model architecture. The model takes characters as input and outputs the corresponding
raw spectrogram, which is then fed to the Grifﬁn-Lim reconstruction algorithm to synthesize speech.
or machine translation (Wu et al., 2016), TTS outputs are continuous, and output sequences are
usually much longer than those of the input. These attributes cause prediction errors to accumulate
quickly. In this paper, we propose Tacotron, an end-to-end generative TTS model based on the
sequence-to-sequence (seq2seq) (Sutskever et al., 2014) with attention paradigm (Bahdanau et al.,
2014). Our model takes characters as input and outputs raw spectrogram, using several techniques
to improve the capability of a vanilla seq2seq model. Given <text, audio >pairs, Tacotron can
be trained completely from scratch with random initialization. It does not require phoneme-level
alignment, so it can easily scale to using large amounts of acoustic data with transcripts. With a
simple waveform synthesis technique, Tacotron produces a 3.82 mean opinion score (MOS) on an
US English eval set, outperforming a production parametric system in terms of naturalness1.
2 R ELATED WORK
WaveNet (van den Oord et al., 2016) is a powerful generative model of audio. It works well for TTS,
but is slow due to its sample-level autoregressive nature. It also requires conditioning on linguistic
features from an existing TTS frontend, and thus is not end-to-end: it only replaces the vocoder and
acoustic model. Another recently-developed neural model is DeepV oice (Arik et al., 2017), which
replaces every component in a typical TTS pipeline by a corresponding neural network. However,
each component is independently trained, and its nontrivial to change the system to train in an
end-to-end fashion.
To our knowledge, Wang et al. (2016) is the earliest work touching end-to-end TTS using seq2seq
with attention. However, it requires a pre-trained hidden Markov model (HMM) aligner to help the
seq2seq model learn the alignment. Its hard to tell how much alignment is learned by the seq2seq
per se. Second, a few tricks are used to get the model trained, which the authors note hurts prosody.
Third, it predicts vocoder parameters hence needs a vocoder. Furthermore, the model is trained on
phoneme inputs and the experimental results seem to be somewhat limited.
Char2Wav (Sotelo et al., 2017) is an independently-developed end-to-end model that can be trained
on characters. However, Char2Wav still predicts vocoder parameters before using a SampleRNN
neural vocoder (Mehri et al., 2016), whereas Tacotron directly predicts raw spectrogram. Also, their
seq2seq and SampleRNN models need to be separately pre-trained, but our model can be trained
1Sound demos can be found at https://google.github.io/tacotron
2from scratch. Finally, we made several key modiﬁcations to the vanilla seq2seq paradigm. As
shown later, a vanilla seq2seq model does not work well for character-level inputs.
3 M ODEL ARCHITECTURE
The backbone of Tacotron is a seq2seq model with attention (Bahdanau et al., 2014; Vinyals et al.,
2015). Figure 1 depicts the model, which includes an encoder, an attention-based decoder, and a
post-processing net. At a high-level, our model takes characters as input and produces spectrogram
frames, which are then converted to waveforms. We describe these components below.
Conv1D layers Highway layers 
Conv1D bank + stacking Max-pool along time (stride=1) Bidirectional RNN 
Residual connection 
Conv1D projections 
Figure 2: The CBHG (1-D convolution bank + highway network + bidirectional GRU) module
adapted from Lee et al. (2016).
3.1 CBHG MODULE
We ﬁrst describe a building block dubbed CBHG, illustrated in Figure 2. CBHG consists of a
bank of 1-D convolutional ﬁlters, followed by highway networks (Srivastava et al., 2015) and a
bidirectional gated recurrent unit (GRU) (Chung et al., 2014) recurrent neural net (RNN). CBHG
is a powerful module for extracting representations from sequences. The input sequence is ﬁrst
convolved with Ksets of 1-D convolutional ﬁlters, where the k-th set contains Ckﬁlters of width
k(i.e.k= 1;2; : : : ; K ). These ﬁlters explicitly model local and contextual information (akin to
modeling unigrams, bigrams, up to K-grams). The convolution outputs are stacked together and
further max pooled along time to increase local invariances. Note that we use a stride of 1 to
preserve the original time resolution. We further pass the processed sequence to a few ﬁxed-width
1-D convolutions, whose outputs are added with the original input sequence via residual connections
(He et al., 2016). Batch normalization (Ioffe & Szegedy, 2015) is used for all convolutional layers.
The convolution outputs are fed into a multi-layer highway network to extract high-level features.
Finally, we stack a bidirectional GRU RNN on top to extract sequential features from both forward
and backward context. CBHG is inspired from work in machine translation (Lee et al., 2016),
where the main differences from Lee et al. (2016) include using non-causal convolutions, batch
normalization, residual connections, and stride=1 max pooling. We found that these modiﬁcations
improved generalization.
3.2 E NCODER
The goal of the encoder is to extract robust sequential representations of text. The input to the
encoder is a character sequence, where each character is represented as a one-hot vector and em-
3Table 1: Hyper-parameters and network architectures. conv- k-c-ReLU denotes 1-D convolution
with width kandcoutput channels with ReLU activation. FC stands for fully-connected.
Spectral analysis pre-emphasis : 0.97; frame length : 50 ms;
frame shift : 12.5 ms; window type : Hann
Character embedding 256-D
Encoder CBHG Conv1D bank :K=16, conv- k-128-ReLU
Max pooling : stride=1, width=2
Conv1D projections : conv-3-128-ReLU
!conv-3-128-Linear
Highway net : 4 layers of FC-128-ReLU
Bidirectional GRU : 128 cells
Encoder pre-net FC-256-ReLU!Dropout(0.5)!
FC-128-ReLU!Dropout(0.5)
Decoder pre-net FC-256-ReLU!Dropout(0.5)!
FC-128-ReLU!Dropout(0.5)
Decoder RNN 2-layer residual GRU (256 cells)
Attention RNN 1-layer GRU (256 cells)
Post-processing net Conv1D bank :K=8, conv-k-128-ReLU
CBHG Max pooling : stride=1, width=2
Conv1D projections : conv-3-256-ReLU
!conv-3-80-Linear
Highway net : 4 layers of FC-128-ReLU
Bidirectional GRU : 128 cells
Reduction factor ( r) 2
bedded into a continuous vector. We then apply a set of non-linear transformations, collectively
called a pre-net, to each embedding. We use a bottleneck layer with dropout as the pre-net in this
work, which helps convergence and improves generalization. A CBHG module transforms the pre-
net outputs into the ﬁnal encoder representation used by the attention module. We found that this
CBHG-based encoder not only reduces overﬁtting, but also makes fewer mispronunciations than a
standard multi-layer RNN encoder (see our linked page of audio samples).
3.3 D ECODER
We use a content-based tanh attention decoder (see e.g. Vinyals et al. (2015)), where a stateful recur-
rent layer produces the attention query at each decoder time step. We concatenate the context vector
and the attention RNN cell output to form the input to the decoder RNNs. We use a stack of GRUs
with vertical residual connections (Wu et al., 2016) for the decoder. We found the residual con-
nections speed up convergence. The decoder target is an important design choice. While we could
directly predict raw spectrogram, its a highly redundant representation for the purpose of learning
alignment between speech signal and text (which is really the motivation of using seq2seq for this
task). Because of this redundancy, we use a different target for seq2seq decoding and waveform syn-
thesis. The seq2seq target can be highly compressed as long as it provides sufﬁcient intelligibility
and prosody information for an inversion process, which could be ﬁxed or trained. We use 80-band
mel-scale spectrogram as the target, though fewer bands or more concise targets such as cepstrum
could be used. We use a post-processing network (discussed below) to convert from the seq2seq
target to waveform.
We use a simple fully-connected output layer to predict the decoder targets. An important trick we
discovered was predicting multiple, non-overlapping output frames at each decoder step. Predicting
rframes at once divides the total number of decoder steps by r, which reduces model size, training
time, and inference time. More importantly, we found this trick to substantially increase convergence
speed, as measured by a much faster (and more stable) alignment learned from attention. This is
likely because neighboring speech frames are correlated and each character usually corresponds to
multiple frames. Emitting one frame at a time forces the model to attend to the same input token for
multiple timesteps; emitting multiple frames allows the attention to move forward early in training.
A similar trick is also used in Zen et al. (2016) but mainly to speed up inference.
4The ﬁrst decoder step is conditioned on an all-zero frame, which represents a <GO>frame. In
inference, at decoder step t, the last frame of the rpredictions is fed as input to the decoder at step
t+ 1. Note that feeding the last prediction is an ad-hoc choice here  we could use all rpredictions.
During training, we always feed every r-th ground truth frame to the decoder. The input frame is
passed to a pre-net as is done in the encoder. Since we do not use techniques such as scheduled
sampling (Bengio et al., 2015) (we found it to hurt audio quality), the dropout in the pre-net is
critical for the model to generalize, as it provides a noise source to resolve the multiple modalities
in the output distribution.
3.4 P OST-PROCESSING NET AND WAVEFORM SYNTHESIS
As mentioned above, the post-processing nets task is to convert the seq2seq target to a target that
can be synthesized into waveforms. Since we use Grifﬁn-Lim as the synthesizer, the post-processing
net learns to predict spectral magnitude sampled on a linear-frequency scale. Another motivation of
the post-processing net is that it can see the full decoded sequence. In contrast to seq2seq, which
always runs from left to right, it has both forward and backward information to correct the prediction
error for each individual frame. In this work, we use a CBHG module for the post-processing net,
though a simpler architecture likely works as well. The concept of a post-processing network is
highly general. It could be used to predict alternative targets such as vocoder parameters, or as a
WaveNet-like neural vocoder (van den Oord et al., 2016; Mehri et al., 2016; Arik et al., 2017) that
synthesizes waveform samples directly.
We use the Grifﬁn-Lim algorithm (Grifﬁn & Lim, 1984) to synthesize waveform from the predicted
spectrogram. We found that raising the predicted magnitudes by a power of 1.2 before feeding
to Grifﬁn-Lim reduces artifacts, likely due to its harmonic enhancement effect. We observed that
Grifﬁn-Lim converges after 50 iterations (in fact, about 30 iterations seems to be enough), which
is reasonably fast. We implemented Grifﬁn-Lim in TensorFlow (Abadi et al., 2016) hence its also
part of the model. While Grifﬁn-Lim is differentiable (it does not have trainable weights), we do not
impose any loss on it in this work. We emphasize that our choice of Grifﬁn-Lim is for simplicity;
while it already yields strong results, developing a fast and high-quality trainable spectrogram to
waveform inverter is ongoing work.
4 M ODEL DETAILS
Table 1 lists the hyper-parameters and network architectures. We use log magnitude spectrogram
with Hann windowing, 50 ms frame length, 12.5 ms frame shift, and 2048-point Fourier transform.
We also found pre-emphasis (0.97) to be helpful. We use 24 kHz sampling rate for all experiments.
We use r= 2 (output layer reduction factor) for the MOS results in this paper, though larger rvalues
(e.g.r= 5) also work well. We use the Adam optimizer (Kingma & Ba, 2015) with learning rate
decay, which starts from 0.001 and is reduced to 0.0005, 0.0003, and 0.0001 after 500K, 1M and 2M
global steps, respectively. We use a simple 1 loss for both seq2seq decoder (mel-scale spectrogram)
and post-processing net (linear-scale spectrogram). The two losses have equal weights.
We train using a batch size of 32, where all sequences are padded to a max length. Its a com-
mon practice to train sequence models with a loss mask, which masks loss on zero-padded frames.
However, we found that models trained this way dont know when to stop emitting outputs, causing
repeated sounds towards the end. One simple trick to get around this problem is to also reconstruct
the zero-padded frames.
5 E XPERIMENTS
We train Tacotron on an internal North American English dataset, which contains about 24.6 hours
of speech data spoken by a professional female speaker. The phrases are text normalized, e.g. 16
is converted to sixteen.
50 50 100 150 200 250 300 350
Decoder timesteps020406080Encoder states
0.00.10.20.30.40.50.60.70.80.9
(a) Vanilla seq2seq + scheduled sampling
0 10 20 30 40 50 60 70
Decoder timesteps020406080Encoder states
0.00.10.20.30.40.50.60.70.80.9
(b) GRU encoder
0 10 20 30 40 50 60 70
Decoder timesteps020406080Encoder states
0.00.10.20.30.40.50.60.70.80.9
(c) Tacotron (proposed)
Figure 3: Attention alignments on a test phrase. The decoder length in Tacotron is shorter due to
the use of the output reduction factor r=5.
5.1 A BLATION ANALYSIS
We conduct a few ablation studies to understand the key components in our model. As is common
for generative models, its hard to compare models based on objective metrics, which often do not
correlate well with perception (Theis et al., 2015). We mainly rely on visual comparisons instead.
We strongly encourage readers to listen to the provided samples.
First, we compare with a vanilla seq2seq model. Both the encoder and decoder use 2 layers of
residual RNNs, where each layer has 256 GRU cells (we tried LSTM and got similar results). No
pre-net or post-processing net is used, and the decoder directly predicts linear-scale log magnitude
spectrogram. We found that scheduled sampling (sampling rate 0.5) is required for this model to
learn alignments and generalize. We show the learned attention alignment in Figure 3. Figure 3(a)
reveals that the vanilla seq2seq learns a poor alignment. One problem is that attention tends to
60 20 40 60 80 100 120 140
Frame02004006008001000DFT bin(a) Without post-processing net
0 20 40 60 80 100 120 140
Frame02004006008001000DFT bin
(b) With post-processing net
Figure 4: Predicted spectrograms with and without using the post-processing net.
get stuck for many frames before moving forward, which causes bad speech intelligibility in the
synthesized signal. The naturalness and overall duration are destroyed as a result. In contrast, our
model learns a clean and smooth alignment, as shown in Figure 3(c).
Second, we compare with a model with the CBHG encoder replaced by a 2-layer residual GRU
encoder. The rest of the model, including the encoder pre-net, remain exactly the same. Comparing
Figure 3(b) and 3(c), we can see that the alignment from the GRU encoder is noisier. Listening to
synthesized signals, we found that noisy alignment often leads to mispronunciations. The CBHG
encoder reduces overﬁtting and generalizes well to long and complex phrases.
Figures 4(a) and 4(b) demonstrate the beneﬁt of using the post-processing net. We trained a model
without the post-processing net while keeping all the other components untouched (except that the
decoder RNN predicts linear-scale spectrogram). With more contextual information, the prediction
from the post-processing net contains better resolved harmonics (e.g. higher harmonics between
bins 100 and 400) and high frequency formant structure, which reduces synthesis artifacts.
5.2 M EAN OPINION SCORE TESTS
We conduct mean opinion score tests, where the subjects were asked to rate the naturalness of the
stimuli in a 5-point Likert scale score. The MOS tests were crowdsourced from native speakers.
7100 unseen phrases were used for the tests and each phrase received 8 ratings. When computing
MOS, we only include ratings where headphones were used. We compare our model with a para-
metric (based on LSTM (Zen et al., 2016)) and a concatenative system (Gonzalvo et al., 2016),
both of which are in production. As shown in Table 2, Tacotron achieves an MOS of 3.82, which
outperforms the parametric system. Given the strong baselines and the artifacts introduced by the
Grifﬁn-Lim synthesis, this represents a very promising result.
Table 2: 5-scale mean opinion score evaluation.
mean opinion score
Tacotron 3.820.085
Parametric 3.690.109
Concatenative 4.090.119
6 D ISCUSSIONS
We have proposed Tacotron, an integrated end-to-end generative TTS model that takes a character
sequence as input and outputs the corresponding spectrogram. With a very simple waveform syn-
thesis module, it achieves a 3.82 MOS score on US English, outperforming a production parametric
system in terms of naturalness. Tacotron is frame-based, so the inference is substantially faster
than sample-level autoregressive methods. Unlike previous work, Tacotron does not need hand-
engineered linguistic features or complex components such as an HMM aligner. It can be trained
from scratch with random initialization. We perform simple text normalization, though recent ad-
vancements in learned text normalization (Sproat & Jaitly, 2016) may render this unnecessary in the
future.
We have yet to investigate many aspects of our model; many early design decisions have gone
unchanged. Our output layer, attention module, loss function, and Grifﬁn-Lim-based waveform
synthesizer are all ripe for improvement. For example, its well known that Grifﬁn-Lim outputs
may have audible artifacts. We are currently working on fast and high-quality neural-network-based
spectrogram inversion.
ACKNOWLEDGMENTS
The authors would like to thank Heiga Zen and Ziang Xie for constructive discussions and feedback.
REFERENCES
Mart ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. TensorFlow: Large-scale machine
learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 , 2016.
Yannis Agiomyrgiannakis. V ocaine the vocoder and applications in speech synthesis. In Acoustics,
Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on , pp. 4230
4234. IEEE, 2015.
Sercan Arik, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew Gibiansky, Yongguo
Kang, Xian Li, John Miller, Jonathan Raiman, Shubho Sengupta, and Mohammad Shoeybi. Deep
voice: Real-time neural text-to-speech. arXiv preprint arXiv:1702.07825 , 2017.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473 , 2014.
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence
prediction with recurrent neural networks. In Advances in Neural Information Processing Sys-
tems, pp. 11711179, 2015.
William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. Listen, attend and spell: A neural
network for large vocabulary conversational speech recognition. In Acoustics, Speech and Signal
Processing (ICASSP), 2016 IEEE International Conference on , pp. 49604964. IEEE, 2016.
8Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 , 2014.
Xavi Gonzalvo, Siamak Tazari, Chun-an Chan, Markus Becker, Alexander Gutkin, and Hanna Silen.
Recent advances in Google real-time HMM-driven unit selection synthesizer. In Proc. Inter-
speech , pp. 22382242, 2016.
Daniel Grifﬁn and Jae Lim. Signal estimation from modiﬁed short-time fourier transform. IEEE
Transactions on Acoustics, Speech, and Signal Processing , 32(2):236243, 1984.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp.
770778, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167 , 2015.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Proceedings of the
3rd International Conference on Learning Representations (ICLR) , 2015.
Jason Lee, Kyunghyun Cho, and Thomas Hofmann. Fully character-level neural machine translation
without explicit segmentation. arXiv preprint arXiv:1610.03017 , 2016.
Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo,
Aaron Courville, and Yoshua Bengio. SampleRNN: An unconditional end-to-end neural audio
generation model. arXiv preprint arXiv:1612.07837 , 2016.
Jose Sotelo, Soroush Mehri, Kundan Kumar, Jo ao Felipe Santos, Kyle Kastner, Aaron Courville, and
Yoshua Bengio. Char2Wav: End-to-end speech synthesis. In ICLR2017 workshop submission ,
2017.
Richard Sproat and Navdeep Jaitly. RNN approaches to text normalization: A challenge. arXiv
preprint arXiv:1611.00068 , 2016.
Rupesh Kumar Srivastava, Klaus Greff, and J urgen Schmidhuber. Highway networks. arXiv preprint
arXiv:1505.00387 , 2015.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
InAdvances in neural information processing systems , pp. 31043112, 2014.
Paul Taylor. Text-to-speech synthesis . Cambridge university press, 2009.
Lucas Theis, A aron van den Oord, and Matthias Bethge. A note on the evaluation of generative
models. arXiv preprint arXiv:1511.01844 , 2015.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A generative model for
raw audio. arXiv preprint arXiv:1609.03499 , 2016.
Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. Gram-
mar as a foreign language. In Advances in Neural Information Processing Systems , pp. 2773
2781, 2015.
Wenfu Wang, Shuang Xu, and Bo Xu. First step towards end-to-end parametric TTS synthesis:
Generating spectral parameters with neural attention. In Proceedings Interspeech , pp. 22432247,
2016.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Googles neural machine trans-
lation system: Bridging the gap between human and machine translation. arXiv preprint
arXiv:1609.08144 , 2016.
Heiga Zen, Keiichi Tokuda, and Alan W Black. Statistical parametric speech synthesis. Speech
Communication , 51(11):10391064, 2009.
9Heiga Zen, Yannis Agiomyrgiannakis, Niels Egberts, Fergus Henderson, and Przemysław Szczepa-
niak. Fast, compact, and high quality LSTM-RNN based statistical parametric speech synthesizers
for mobile devices. Proceedings Interspeech , 2016.
10
  Grounded Compositional Semantics
for Finding and Describing Images with Sentences
Richard Socher, Andrej Karpathy, Quoc V . Le*, Christopher D. Manning, Andrew Y. Ng
Stanford University, Computer Science Department, *Google Inc.
richard@socher.org, karpathy@cs.stanford.edu ,
qvl@google.com, manning@stanford.edu, ang@cs.stanford.edu
Abstract
Previous work on Recursive Neural Networks
(RNNs) shows that these models can produce
compositional feature vectors for accurately
representing and classifying sentences or im-
ages. However, the sentence vectors of previ-
ous models cannot accurately represent visu-
ally grounded meaning. We introduce the DT-
RNN model which uses dependency trees to
embed sentences into a vector space in order
to retrieve images that are described by those
sentences. Unlike previous RNN-based mod-
els which use constituency trees, DT-RNNs
naturally focus on the action and agents in
a sentence. They are better able to abstract
from the details of word order and syntactic
expression. DT-RNNs outperform other re-
cursive and recurrent neural networks, kernel-
ized CCA and a bag-of-words baseline on the
tasks of ﬁnding an image that ﬁts a sentence
description and vice versa. They also give
more similar representations to sentences that
describe the same image.
1 Introduction
Single word vector spaces are widely used (Turney
and Pantel, 2010) and successful at classifying sin-
gle words and capturing their meaning (Collobert
and Weston, 2008; Huang et al., 2012; Mikolov et
al., 2013). Since words rarely appear in isolation,
the task of learning compositional meaning repre-
sentations for longer phrases has recently received a
lot of attention (Mitchell and Lapata, 2010; Socher
et al., 2010; Socher et al., 2012; Grefenstette et al.,
2013). Similarly, classifying whole images into aﬁxed set of classes also achieves very high perfor-
mance (Le et al., 2012; Krizhevsky et al., 2012).
However, similar to words, objects in images are of-
ten seen in relationships with other objects which are
not adequately described by a single label.
In this work, we introduce a model, illustrated in
Fig. 1, which learns to map sentences and images
into a common embedding space in order to be able
to retrieve one from the other. We assume word and
image representations are ﬁrst learned in their re-
spective single modalities but ﬁnally mapped into a
jointly learned multimodal embedding space.
Our model for mapping sentences into this space
is based on ideas from Recursive Neural Networks
(RNNs) (Pollack, 1990; Costa et al., 2003; Socher
et al., 2011b). However, unlike all previous RNN
models which are based on constituency trees (CT-
RNNs), our model computes compositional vector
representations inside dependency trees. The com-
positional vectors computed by this new dependency
tree RNN (DT-RNN) capture more of the meaning
of sentences, where we deﬁne meaning in terms of
similarity to a visual representation of the textual
description. DT-RNN induced vector representa-
tions of sentences are more robust to changes in the
syntactic structure or word order than related mod-
els such as CT-RNNs or Recurrent Neural Networks
since they naturally focus on a sentences action and
its agents.
We evaluate and compare DT-RNN induced rep-
resentations on their ability to use a sentence such as
A man wearing a helmet jumps on his bike near a
beach.  to ﬁnd images that show such a scene. The
goal is to learn sentence representations that captureA man wearing a helmet jumps on his bike near a beach .
Compositional Sentence Vectors
Two airplanes parked in an airport .
A man jumping his downhill bike .
Image Vector Representation
A small child sits on a cement wall near white flower .
Multi -Modal 
RepresentationsFigure 1: The DT-RNN learns vector representations for sentences based on their dependency trees. We learn to
map the outputs of convolutional neural networks applied to images into the same space and can then compare both
sentences and images. This allows us to query images with a sentence and give sentence descriptions to images.
the visual scene described and to ﬁnd appropriate
images in the learned, multi-modal sentence-image
space. Conversely, when given a query image, we
would like to ﬁnd a description that goes beyond a
single label by providing a correct sentence describ-
ing it, a task that has recently garnered a lot of at-
tention (Farhadi et al., 2010; Ordonez et al., 2011;
Kuznetsova et al., 2012). We use the dataset intro-
duced by (Rashtchian et al., 2010) which consists of
1000 images, each with 5 descriptions. On all tasks,
our model outperforms baselines and related mod-
els.
2 Related Work
The presented model is connected to several areas of
NLP and vision research, each with a large amount
of related work to which we can only do some justice
given space constraints.
Semantic Vector Spaces and Their Composition-
ality. The dominant approach in semantic vec-
tor spaces uses distributional similarities of single
words. Often, co-occurrence statistics of a word and
its context are used to describe each word (Turney
and Pantel, 2010; Baroni and Lenci, 2010), such
as tf-idf. Most of the compositionality algorithms
and related datasets capture two-word compositions.
For instance, (Mitchell and Lapata, 2010) use two-
word phrases and analyze similarities computed by
vector addition, multiplication and others. Compo-
sitionality is an active ﬁeld of research with many
different models and representations being explored
(Grefenstette et al., 2013), among many others. We
compare to supervised compositional models thatcan learn task-speciﬁc vector representations such as
constituency tree recursive neural networks (Socher
et al., 2011b; Socher et al., 2011a), chain structured
recurrent neural networks and other baselines. An-
other alternative would be to use CCG trees as a
backbone for vector composition (K.M. Hermann,
2013).
Multimodal Embeddings. Multimodal embed-
ding methods project data from multiple sources
such as sound and video (Ngiam et al., 2011) or im-
ages and text. Socher et al. (Socher and Fei-Fei,
2010) project words and image regions into a com-
mon space using kernelized canonical correlation
analysis to obtain state of the art performance in an-
notation and segmentation. Similar to our work, they
use unsupervised large text corpora to learn seman-
tic word representations. Among other recent work
is that by Srivastava and Salakhutdinov (2012) who
developed multimodal Deep Boltzmann Machines.
Similar to their work, we use techniques from the
broad ﬁeld of deep learning to represent images and
words.
Recently, single word vector embeddings have
been used for zero shot learning (Socher et al.,
2013c). Mapping images to word vectors enabled
their system to classify images as depicting objects
such as cat without seeing any examples of this
class. Related work has also been presented at NIPS
(Socher et al., 2013b; Frome et al., 2013). This work
moves zero-shot learning beyond single categories
per image and extends it to unseen phrases and full
length sentences, making use of similar ideas of se-
mantic spaces grounded in visual knowledge.Detailed Image Annotation. Interactions be-
tween images and texts is a growing research ﬁeld.
Early work in this area includes generating single
words or ﬁxed phrases from images (Duygulu et al.,
2002; Barnard et al., 2003) or using contextual in-
formation to improve recognition (Gupta and Davis,
2008; Torralba et al., 2010).
Apart from a large body of work on single object
image classiﬁcation (Le et al., 2012), there is also
work on attribute classiﬁcation and other mid-level
elements (Kumar et al., 2009), some of which we
hope to capture with our approach as well.
Our work is close in spirit with recent work in de-
scribing images with more detailed, longer textual
descriptions. In particular, Yao et al. (2010) describe
images using hierarchical knowledge and humans in
the loop. In contrast, our work does not require hu-
man interactions. Farhadi et al. (2010) and Kulkarni
et al. (2011), on the other hand, use a more automatic
method to parse images. For instance, the former ap-
proach uses a single triple of objects estimated for an
image to retrieve sentences from a collection written
to describe similar images. It forms representations
to describe 1 object, 1 action, and 1 scene. Kulkarni
et al. (2011) extends their method to describe an im-
age with multiple objects. None of these approaches
have used a compositional sentence vector repre-
sentation and they require speciﬁc language gener-
ation techniques and sophisticated inference meth-
ods. Since our model is based on neural networks in-
ference is fast and simple. Kuznetsova et al. (2012)
use a very large parallel corpus to connect images
and sentences. Feng and Lapata (2013) use a large
dataset of captioned images and experiments with
both extractive (search) and abstractive (generation)
models.
Most related is the very recent work of Hodosh et
al. (2013). They too evaluate using a ranking mea-
sure. In our experiments, we compare to kernelized
Canonical Correlation Analysis which is the main
technique in their experiments.
3 Dependency-Tree Recursive Neural
Networks
In this section we ﬁrst focus on the DT-RNN model
that computes compositional vector representations
for phrases and sentences of variable length and syn-tactic type. In section 5 the resulting vectors will
then become multimodal features by mapping im-
ages that show what the sentence describes to the
same space and learning both the image and sen-
tence mapping jointly.
The most common way of building representa-
tions for longer phrases from single word vectors is
to simply linearly average the word vectors. While
this bag-of-words approach can yield reasonable
performance in some tasks, it gives all the words the
same weight and cannot distinguish important dif-
ferences in simple visual descriptions such as The
bike crashed into the standing car. vs.The car
crashed into the standing bike. .
RNN models (Pollack, 1990; Goller and K uchler,
1996; Socher et al., 2011b; Socher et al., 2011a) pro-
vided a novel way of combining word vectors for
longer phrases that moved beyond simple averag-
ing. They combine vectors with an RNN in binary
constituency trees which have potentially many hid-
den layers. While the induced vector representations
work very well on many tasks, they also inevitably
capture a lot of syntactic structure of the sentence.
However, the task of ﬁnding images from sentence
descriptions requires us to be more invariant to syn-
tactic differences. One such example are active-
passive constructions which can collapse words such
as by in some formalisms (de Marneffe et al.,
2006), relying instead on the semantic relationship
of agent. For instance, The mother hugged her
child. and The child was hugged by its mother.
should map to roughly the same visual space. Cur-
rent Recursive and Recurrent Neural Networks do
not exhibit this behavior and even bag of words rep-
resentations would be inﬂuenced by the words was
andby. The model we describe below focuses more
on recognizing actions and agents and has the po-
tential to learn representations that are invariant to
active-passive differences.
3.1 DT-RNN Inputs: Word Vectors and
Dependency Trees
In order for the DT-RNN to compute a vector repre-
sentation for an ordered list of mwords (a phrase or
sentence), we map the single words to a vector space
and then parse the sentence.
First, we map each word to a d-dimensional vec-
tor. We initialize these word vectors with the un-A man wearing a helmet jumps on his bike near a beachdetnsubj
partmod detdobjroot
prep posspobjprep
detpobj
Figure 2: Example of a full dependency tree for a longer sentence. The DT-RNN will compute vector representations
at every word that represents that word and an arbitrary number of child nodes. The ﬁnal representation is computed
at the root node, here at the verb jumps . Note that more important activity and object words are higher up in this tree
structure.
supervised model of Huang et al. (2012) which can
learn single word vector representations from both
local and global contexts. The idea is to construct a
neural network that outputs high scores for windows
and documents that occur in a large unlabeled corpus
and low scores for window-document pairs where
one word is replaced by a random word. When
such a network is optimized via gradient descent the
derivatives backpropagate into a word embedding
matrixAwhich stores word vectors as columns. In
order to predict correct scores the vectors in the ma-
trix capture co-occurrence statistics. We use d= 50
in all our experiments. The embedding matrix X
is then used by ﬁnding the column index iof each
word: [w] =iand retrieving the corresponding col-
umnxwfromX. Henceforth, we represent an input
sentencesas an ordered list of (word,vector) pairs:
s= ((w1;xw1);:::; (wm;xwm)).
Next, the sequence of words (w1;:::;wm)is
parsed by the dependency parser of de Marneffe
et al. (2006). Fig. 2 shows an example. We can
represent a dependency tree dof a sentence sas
an ordered list of (child,parent) indices: d(s) =
f(i;j)g, where every child word in the sequence
i= 1;:::;m is present and has any word j2
f1;:::;mg[f 0gas its parent. The root word has
as its parent 0and we notice that the same word can
be a parent between zero and mnumber of times.
Without loss of generality, we assume that these in-
dices form a tree structure. To summarize, the input
to the DT-RNN for each sentence is the pair (s;d):
the words and their vectors and the dependency tree.
3.2 Forward Propagation in DT-RNNs
Given these two inputs, we now illustrate how the
DT-RNN computes parent vectors. We will use the
following sentence as a running example: Students 1
ride 2bikes 3at4night 5.Fig. 3 shows its tree
and computed vector representations. The depen-
Students                 bikes           nightride 
at          x1x2
x3x4
x5h1h2
h3h4
h5Figure 3: Example of a DT-RNN tree structure for com-
puting a sentence representation in a bottom up fashion.
dency tree for this sentence can be summarized by
the following set of (child, parent) edges: d=
f(1;2);(2;0);(3;2);(4;2);(5;4)g.
The DT-RNN model will compute parent vectors
at each word that include all the dependent (chil-
dren) nodes in a bottom up fashion using a com-
positionality function gwhich is parameterized by
all the model parameters . To this end, the algo-
rithm searches for nodes in a tree that have either
(i) no children or (ii) whose children have already
been computed and then computes the correspond-
ing vector.
In our example, the words x1;x3;x5are leaf
nodes and hence, we can compute their correspond-
ing hidden nodes via:
hc=g(xc) =f(Wvxc)forc= 1;3;5;(1)
where we compute the hidden vector at position c
via our general composition function g. In the case
of leaf nodes, this composition function becomes
simply a linear layer, parameterized by Wv2Rnd,
followed by a nonlinearity. We cross-validate over
using no nonlinearity ( f= id ),tanh ,sigmoid or
rectiﬁed linear units ( f= max(0;x), but generally
ﬁndtanh to perform best.
The ﬁnal sentence representation we want to com-
pute is ath2, however, since we still do not have h4,we compute that one next:
h4=g(x4;h5) =f(Wvx4+Wr1h5);(2)
where we use the same Wvas before to map the
word vector into hidden space but we now also have
a linear layer that takes as input h5, the only child
of the fourth node. The matrix Wr12Rnnis used
because node 5 is the ﬁrst child node on the right
side of node 4. Generally, we have multiple matri-
ces for composing with hidden child vectors from
the right and left sides: Wr= (Wr1;:::;Wrkr)and
Wl= (Wl1;:::;Wlkl). The number of needed ma-
trices is determined by the data by simply ﬁnding
the maximum numbers of left kland rightkrchil-
dren any node has. If at test time a child appeared
at an even large distance (this does not happen in
our test set), the corresponding matrix would be the
identity matrix.
Now that all children of h2have their hidden vec-
tors, we can compute the ﬁnal sentence representa-
tion via:
h2=g(x2;h1;h3;h4) = (3)
f(Wvx2+Wl1h1+Wr1h3+Wr2h4):
Notice that the children are multiplied by matrices
that depend on their location relative to the current
node.
Another modiﬁcation that improves the mean
rank by approximately 6 in image search on the dev
set is to weight nodes by the number of words under-
neath them and normalize by the sum of words under
all children. This encourages the intuitive desidera-
tum that nodes describing longer phrases are more
important. Let (i)be the number of leaf nodes
(words) under node iandC(i;y)be the set of child
nodes of node iin dependency tree y. The ﬁnal com-
position function for a node vector hibecomes:
hi=f0
@1
(i)0
@Wvxi+X
j2C(i)(j)Wpos(i;j)hj1
A1
A;
(4)
where by deﬁnition (i) = 1 +P
j2C(i)(j)and
pos(i;j)is the relative position of child jwith re-
spect to node i, e.g.l1orr2in Eq. 3.
3.3 Semantic Dependency Tree RNNs
An alternative is to condition the weight matrices
on the semantic relations given by the dependencyparser. We use the collapsed tree formalism of
the Stanford dependency parser (de Marneffe et al.,
2006). With such a semantic untying of the weights,
the DT-RNN makes better use of the dependency
formalism and could give active-passive reversals
similar semantic vector representation. The equation
for this semantic DT-RNN ( SDT-RNN ) is the same
as the one above except that the matrices Wpos(i;j)
are replaced with matrices based on the dependency
relationship. There are a total of 141 unique such
relationships in the dataset. However, most are very
rare. For examples of semantic relationships, see
Fig. 2 and the model analysis section 6.7.
This forward propagation can be used for com-
puting compositional vectors and in Sec. 5 we will
explain the objective function in which these are
trained.
3.4 Comparison to Previous RNN Models
The DT-RNN has several important differences to
previous RNN models of Socher et al. (2011a) and
(Socher et al., 2011b; Socher et al., 2011c). These
constituency tree RNNs (CT-RNNs) use the follow-
ing composition function to compute a hidden par-
ent vectorhfrom exactly two child vectors (c1;c2)
in a binary tree: h=f
Wc1
c2
, whereW2
Rd2dis the main parameter to learn. This can be
rewritten to show the similarity to the DT-RNN as
h=f(Wl1c1+Wr1c2). However, there are several
important differences.
Note ﬁrst that in previous RNN models the par-
ent vectors were of the same dimensionality to be
recursively compatible and be used as input to the
next composition. In contrast, our new model ﬁrst
maps single words into a hidden space and then par-
ent nodes are composed from these hidden vectors.
This allows a higher capacity representation which
is especially helpful for nodes that have many chil-
dren.
Secondly, the DT-RNN allows for n-ary nodes in
the tree. This is an improvement that is possible even
for constituency tree CT-RNNs but it has not been
explored in previous models.
Third, due to computing parent nodes in con-
stituency trees, previous models had the problem
that words that are merged last in the tree have a
larger weight or importance in the ﬁnal sentence rep-Figure 4: The architecture of the visual model. This model has 3 sequences of ﬁltering, pooling and local contrast
normalization layers. The learnable parameters are the ﬁltering layer. The ﬁlters are not shared, i.e., the network is
nonconvolutional.
resentation. This can be problematic since these are
often simple non-content words, such as a leading
But,. While such single words can be important for
tasks such as sentiment analysis, we argue that for
describing visual scenes the DT-RNN captures the
more important effects: The dependency tree struc-
tures push the central content words such as the main
action or verb and its subject and object to be merged
last and hence, by construction, the ﬁnal sentence
representation is more robust to less important ad-
jectival modiﬁers, word order changes, etc.
Fourth, we allow some untying of weights de-
pending on either how far away a constituent is from
the current word or what its semantic relationship is.
Now that we can compute compositional vector
representations for sentences, the next section de-
scribes how we represent images.
4 Learning Image Representations with
Neural Networks
The image features that we use in our experiments
are extracted from a deep neural network, replicated
from the one described in (Le et al., 2012). The net-
work was trained using both unlabeled data (random
web images) and labeled data to classify 22,000 cat-
egories in ImageNet (Deng et al., 2009). We then
used the features at the last layer, before the classi-
ﬁer, as the feature representation in our experiments.
The dimension of the feature vector of the last layer
is 4,096. The details of the model and its training
procedures are as follows.
The architecture of the network can be seen in
Figure 4. The network takes 200x200 pixel images
as inputs and has 9 layers. The layers consist ofthree sequences of ﬁltering, pooling and local con-
trast normalization (Jarrett et al., 2009). The pooling
function is L2 pooling of the previous layer (taking
the square of the ﬁltering units, summing them up
in a small area in the image, and taking the square-
root). The local contrast normalization takes inputs
in a small area of the lower layer, subtracts the mean
and divides by the standard deviation.
The network was ﬁrst trained using an unsuper-
vised objective: trying to reconstruct the input while
keeping the neurons sparse. In this phase, the net-
work was trained on 20 million images randomly
sampled from the web. We resized a given image
so that its short dimension has 200 pixels. We then
cropped a ﬁxed size 200x200 pixel image right at the
center of the resized image. This means we may dis-
card a fraction of the long dimension of the image.
After unsupervised training, we used Ima-
geNet (Deng et al., 2009) to adjust the features in the
entire network. The ImageNet dataset has 22,000
categories and 14 million images. The number of
images in each category is equal across categories.
The 22,000 categories are extracted from WordNet.
To speed up the supervised training of this net-
work, we made a simple modiﬁcation to the algo-
rithm described in Le et al. (2012): adding a bottle-
neck layer in between the last layer and the classi-
ﬁer. to reduce the number of connections. We added
one bottleneck layer which has 4,096 units in be-
tween the last layer of the network and the softmax
layer. This newly-added layer is fully connected to
the previous layer and has a linear activation func-
tion. The total number of connections of this net-
work is approximately 1.36 billion.The network was trained again using the super-
vised objective of classifying the 22,000 classes in
ImageNet. Most features in the networks are local,
which allows model parallelism. Data parallelism
by asynchronous SGD was also employed as in Le
et al. (2012). The entire training, both unsupervised
and supervised, took 8 days on a large cluster of ma-
chines. This network achieves 18.3% precision@1
on the full ImageNet dataset (Release Fall 2011).
We will use the features at the bottleneck layer as
the feature vector zof an image. Each scaled and
cropped image is presented to our network. The net-
work then performs a feedforward computation to
compute the values of the bottleneck layer. This
means that every image is represented by a ﬁxed
length vector of 4,096 dimensions. Note that during
training, no aligned sentence-image data was used
and the ImageNet classes do not fully intersect with
the words used in our dataset.
5 Multimodal Mappings
The previous two sections described how we can
map sentences into a d= 50 -dimensional space and
how to extract high quality image feature vectors of
4096 dimensions. We now deﬁne our ﬁnal multi-
modal objective function for learning joint image-
sentence representations with these models. Our
training set consists of Nimages and their feature
vectorsziand each image has 5 sentence descrip-
tionssi1;:::;si5for which we use the DT-RNN to
compute vector representations. See Fig. 5 for ex-
amples from the dataset. For training, we use a max-
margin objective function which intuitively trains
pairs of correct image and sentence vectors to have
high inner products and incorrect pairs to have low
inner products. Let vi=WIzibe the mapped image
vector andyij=DTRNN (sij)the composed sen-
tence vector. We deﬁne Sto be the set of all sentence
indices andS(i)the set of sentence indices corre-
sponding to image i. Similarly,Iis the set of all im-
age indices andI(j)is the image index of sentence
j. The setPis the set of all correct image-sentence
training pairs (i;j). The ranking cost function to
minimize is then: J(WI;) =
X
(i;j)2PX
c2SnS (i)max(0;vT
iyj+vT
iyc)
+X
(i;j)2PX
c2InI (j)max(0;vT
iyj+vT
cyj);(5)whereare the language composition matrices,
and both second sums are over other sentences com-
ing from different images and vice versa. The hyper-
parameter is the margin. The margin is found via
cross validation on the dev set and usually around 1.
The ﬁnal objective also includes the regulariza-
tion term=left (kk2
2+kWIkF). Both the visual
model and the word vector learning require a very
large amount of training data and both have a huge
number of parameters. Hence, to prevent overﬁtting,
we assume their weights are ﬁxed and only train the
DT-RNN parameters WI. If larger training corpora
become available in the future, training both jointly
becomes feasible and would present a very promis-
ing direction. We use a modiﬁed version of Ada-
Grad (Duchi et al., 2011) for optimization of both
WIand the DT-RNN as well as the other baselines
(except kCCA). Adagrad has achieved good perfor-
mance previously in neural networks models (Dean
et al., 2012; Socher et al., 2013a). We modify it
by resetting all squared gradient sums to 1 every 5
epochs. With both images and sentences in the same
multimodal space, we can easily query the model for
similar images or sentences by ﬁnding the nearest
neighbors in terms of negative inner products.
An alternative objective function is based on the
squared loss J(WI;) =P
(i;j)2Pkviyjk2
2:This
requires an alternating minimization scheme that
ﬁrst trains only WI, then ﬁxes WIand trains the
DT-RNN weights and then repeats this several
times. We ﬁnd that the performance with this ob-
jective function (paired with ﬁnding similar images
using Euclidean distances) is worse for all models
than the margin loss of Eq. 5. In addition kCCA
also performs much better using inner products in
the multimodal space.
6 Experiments
We use the dataset of Rashtchian et al. (2010) which
consists of 1000 images, each with 5 sentences. See
Fig. 5 for examples.
We evaluate and compare the DT-RNN in three
different experiments. First, we analyze how well
the sentence vectors capture similarity in visual
meaning. Then we analyze Image Search with
Query Sentences : to query each model with a sen-
tence in order to ﬁnd an image showing that sen-1. A woman and her dog watch the cameraman in their living with wooden floors .
2. A woman sitting on the couch while a black faced dog runs across the floor .
3. A woman wearing a backpack sits on a couch while a small dog runs on the hardwood floor next to her .
4. A women sitting on a sofa while a small Jack Russell walks towards the camera .
5. White and black small dog walks toward the camera while woman sits on couch , desk and computer seen 
    in the background as well as a pillow , teddy bear and moggie toy on the wood floor .
1. A man in a cowboy hat check approaches a small red sports car .
2. The back and left side of a red Ferrari and two men admiring it .
3. The sporty car is admired by passer by .
4. Two men next to a red sports car in a parking lot .
5. Two men stand beside a red sports car .Figure 5: Examples from the dataset of images and their sentence descriptions (Rashtchian et al., 2010). Sentence
length varies greatly and different objects can be mentioned ﬁrst. Hence, models have to be invariant to word ordering.
tences visual meaning. The last experiment De-
scribing Images by Finding Suitable Sentences does
the reverse search where we query the model with an
image and try to ﬁnd the closest textual description
in the embedding space.
In our comparison to other methods we focus on
those models that can also compute ﬁxed, continu-
ous vectors for sentences. In particular, we compare
to the RNN model on constituency trees of Socher
et al. (2011a), a standard recurrent neural network;
a simple bag-of-words baseline which averages the
words. All models use the word vectors provided by
Huang et al. (2012) and do not update them as dis-
cussed above. Models are trained with their corre-
sponding gradients and backpropagation techniques.
A standard recurrent model is used where the hidden
vector at word index tis computed from the hidden
vector at the previous time step and the current word
vector:ht=f(Whht1+Wxxt). During training,
we take the last hidden vector of the sentence chain
and propagate the error into that. It is also this vector
that is used to represent the sentence.
Other possible comparisons are to the very differ-
ent models mentioned in the related work section.
These models use a lot more task-speciﬁc engineer-
ing, such as running object detectors with bounding
boxes, attribute classiﬁers, scene classiﬁers, CRFs
for composing the sentences, etc. Another line of
work uses large sentence-image aligned resources
(Kuznetsova et al., 2012), whereas we focus on eas-
ily obtainable training data of each modality sepa-
rately and a rather small multimodal corpus.
In our experiments we split the data into 800 train-
ing, 100 development and 100 test images. Since
there are 5 sentences describing each image, wehave 4000 training sentences and 500 testing sen-
tences. The dataset has 3020 unique words, half of
which only appear once. Hence, the unsupervised,
pre-trained semantic word vector representations are
crucial. Word vectors are not ﬁne tuned during train-
ing. Hence, the main parameters are the DT-RNNs
Wl;Wror the semantic matrices of which there are
141 and the image mapping WI. For both DT-RNNs
the weight matrices are initialized to block identity
matrices plus Gaussian noise. Word vectors and hid-
den vectors are set o length 50. Using the develop-
ment split, we found = 0:08and the learning rate
of AdaGrad to 0:0001 . The best model uses a mar-
gin of  = 3 .
Inspired by Socher and Fei-Fei (2010) and Ho-
dosh et al. (2013) we also compare to kernelized
Canonical Correlation Analysis (kCCA). We use the
average of word vectors for describing sentences and
the same powerful image vectors as before. We
use the code of Socher and Fei-Fei (2010). Tech-
nically, one could combine the recently introduced
deep CCA Andrew et al. (2013) and train the re-
cursive neural network architectures with the CCA
objective. We leave this to future work. With lin-
ear kernels, kCCA does well for image search but
is worse for sentence self similarity and describing
images with sentences close-by in embedding space.
All other models are trained by replacing the DT-
RNN function in Eq. 5.
6.1 Similarity of Sentences Describing the
Same Image
In this experiment, we ﬁrst map all 500 sentences
from the test set into the multi-modal space. Then
for each sentence, we ﬁnd the nearest neighbor sen-Sentences Similarity for Image
Model Mean Rank
Random 101.1
BoW 11.8
CT-RNN 15.8
Recurrent NN 18.5
kCCA 10.7
DT-RNN 11.1
SDT-RNN 10.5Image Search
Model Mean Rank
Random 52.1
BoW 14.6
CT-RNN 16.1
Recurrent NN 19.2
kCCA 15.9
DT-RNN 13.6
SDT-RNN 12.5Describing Images
Model Mean Rank
Random 92.1
BoW 21.1
CT-RNN 23.9
Recurrent NN 27.1
kCCA 18.0
DT-RNN 19.2
SDT-RNN 16.9
Table 1: Left: Comparison of methods for sentence similarity judgments. Lower numbers are better since they indicate
that sentences describing the same image rank more highly (are closer). The ranks are out of the 500 sentences in the
test set. Center: Comparison of methods for image search with query sentences. Shown is the average rank of the
single correct image that is being described. Right: Average rank of a correct sentence description for a query image.
tences in terms of inner products. We then sort
these neighbors and record the rank or position of
the nearest sentence that describes the same im-
age. If all the images were very unique and the vi-
sual descriptions close-paraphrases and consistent,
we would expect a very low rank. However, usually
a handful of images are quite similar (for instance,
there are various images of airplanes ﬂying, parking,
taxiing or waiting on the runway) and sentence de-
scriptions can vary greatly in detail and speciﬁcity
for the same image.
Table 1 (left) shows the results. We can see that
averaging the high quality word vectors already cap-
tures a lot of similarity. The chain structure of a
standard recurrent neural net performs worst since
its representation is dominated by the last words in
the sequence which may not be as important as ear-
lier words.
6.2 Image Search with Query Sentences
This experiment evaluates how well we can ﬁnd im-
ages that display the visual meaning of a given sen-
tence. We ﬁrst map a query sentence into the vector
space and then ﬁnd images in the same space using
simple inner products. As shown in Table 1 (center),
the new DT-RNN outperforms all other models.
6.3 Describing Images by Finding Suitable
Sentences
Lastly, we repeat the above experiments but with
roles reversed. For an image, we search for suitable
textual descriptions again simply by ﬁnding close-
by sentence vectors in the multi-modal embedding
space. Table 1 (right) shows that the DT-RNN again
outperforms related models. Fig. 2assigned to im-Image Search
Model mRank
BoW 24.7
CT-RNN 22.2
Recurrent NN 28.4
kCCA 13.7
DT-RNN 13.3
SDT-RNN 15.8Describing Images
Model mRank
BoW 30.7
CT-RNN 29.4
Recurrent NN 31.4
kCCA 38.0
DT-RNN 26.8
SDT-RNN 37.5
Table 2: Results of multimodal ranking when models are
trained with a squared error loss and using Euclidean dis-
tance in the multimodal space. Better performance is
reached for all models when trained in a max-margin loss
and using inner products as in the previous table.
ages. The average ranking of 25.3 for a correct sen-
tence description is out of 500 possible sentences. A
random assignment would give an average ranking
of 100.
6.4 Analysis: Squared Error Loss vs. Margin
Loss
We analyze the inﬂuence of the multimodal loss
function on the performance. In addition, we com-
pare using Euclidean distances instead of inner prod-
ucts. Table 2 shows that performance is worse for all
models in this setting.
6.5 Analysis: Recall at nvs Mean Rank
Hodosh et al. (2013) and other related work use re-
call atnas an evaluation measure. Recall at ncap-
tures how often one of the top nclosest vectors were
a correct image or sentence and gives a good intu-
ition of how a model would perform in a ranking
task that presents nsuch results to a user. Below, we
compare three commonly used and high performing
models: bag of words, kCCA and our SDT-RNN onA gray convertible sports car is parked in front of the trees .
A close -up view of the headlights of a blue old -fashioned car .
Black shiny sports car parked on concrete driveway .
Five cows grazing on a patch of grass between two roadways .
A jockey rides a brown and white horse in a dirt corral .
A young woman is riding a Bay hose in a dirt riding -ring.
A white bird pushes a miniature teal shopping cart .
A person rides a brown horse .
A motocross bike with rider flying through the air .
White propeller plane parked in middle of grassy field .
The white jet with its landing gear down flies in the blue sky .
An elderly woman catches a ride on the back of the bicycle .
A green steam train running down the tracks .
Steamy locomotive speeding thou the forest .
A steam engine comes down a train track near trees .
A double decker bus is driving by Big Ben in London .
People in an outrigger canoe sail on emerald green water .
Two people sailing a small white sail boat .
behind a cliff , a boat sails away
Tourist move in on Big Ben on a typical overcast London day .
A group of people sitting around a table on a porch .
A group of four people walking past a giant mushroom .
A man and women smiling for the camera in a kitchen .
A group of men sitting around a table drinking while a man behind 
stands pointing .
Figure 6: Images and their sentence descriptions assigned by the DT-RNN.
Image Search
Model mRank4 R@15 R@55 R@105
BoW 14.6 15.8 42.2 60.0
kCCA 15.9 16.4 41.4 58.0
SDT-RNN 12.5 16.4 46.6 65.6
Describing Images
BoW 21.1 19.0 38.0 57.0
kCCA 18.0 21.0 47.0 61.0
SDT-RNN 16.9 23.0 45.0 63.0
Table 3: Evaluation comparison between mean rank of
the closest correct image or sentence (lower is better 4)
with recall at different thresholds (higher is better, 5).
With one exception (R@5, bottom table), the SDT-RNN
outperforms the other two models and all other models
we did not include here.
this different metric. Table 3 shows that the mea-
sures do correlate well and the SDT-RNN also per-
forms best on the multimodal ranking tasks when
evaluated with this measure.
6.6 Error Analysis
In order to understand the main problems with the
composed sentence vectors, we analyze the sen-
tences that have the worst nearest neighbor rank be-
tween each other. We ﬁnd that the main failure mode
of the SDT-RNN occurs when a sentence that should
describe the same image does not use a verb but the
other sentences of that image do include a verb. For
example, the following sentence pair has vectors that
are very far apart from each other even though they
are supposed to describe the same image:
1. A blue and yellow airplane ﬂying straight down
while emitting white smoke
2. Airplane in dive positionGenerally, as long as both sentences either have a
verb or do not, the SDT-RNN is more robust to dif-
ferent sentence lengths than bag of words represen-
tations.
6.7 Model Analysis: Semantic Composition
Matrices
The best model uses composition matrices based on
semantic relationships from the dependency parser.
We give some insights into what the model learns
by listing the composition matrices with the largest
Frobenius norms. Intuitively, these matrices have
learned larger weights that are being multiplied with
the child vector in the tree and hence that child will
have more weight in the ﬁnal composed parent vec-
tor. In decreasing order of Frobenius norm, the re-
lationship matrices are: nominal subject, possession
modiﬁer (e.g. their), passive auxiliary, preposition
at, preposition in front of, passive auxiliary, passive
nominal subject, object of preposition, preposition
in and preposition on.
The model learns that nouns are very important as
well as their spatial prepositions and adjectives.
7 Conclusion
We introduced a new recursive neural network
model that is based on dependency trees. For eval-
uation, we use the challenging task of mapping sen-
tences and images into a common space for ﬁnding
one from the other. Our new model outperforms
baselines and other commonly used models that can
compute continuous vector representations for sen-
tences. In comparison to related models, the DT-
RNN is more invariant and robust to surface changes
such as word order.References
G. Andrew, R. Arora, K. Livescu, and J. Bilmes. 2013.
Deep canonical correlation analysis. In ICML , At-
lanta, Georgia.
K. Barnard, P. Duygulu, N. de Freitas, D. Forsyth,
D. Blei, and M. Jordan. 2003. Matching words and
pictures. JMLR .
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics , 36(4):673721.
R. Collobert and J. Weston. 2008. A uniﬁed archi-
tecture for natural language processing: deep neural
networks with multitask learning. In Proceedings of
ICML , pages 160167.
F. Costa, P. Frasconi, V . Lombardo, and G. Soda. 2003.
Towards incremental parsing of natural language using
recursive neural networks. Applied Intelligence .
M. de Marneffe, B. MacCartney, and C. D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC .
J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin,
Q. V . Le, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker,
K. Yang, and A.Y . Ng. 2012. Large scale distributed
deep networks. In NIPS .
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. 2009. ImageNet: A Large-Scale Hierarchical Im-
age Database. In CVPR .
J. Duchi, E. Hazan, and Y . Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic op-
timization. JMLR , 12, July.
P. Duygulu, K. Barnard, N. de Freitas, and D. Forsyth.
2002. Object recognition as machine translation. In
ECCV .
A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young,
C. Rashtchian, J. Hockenmaier, and D. Forsyth. 2010.
Every picture tells a story: Generating sentences from
images. In ECCV .
Y . Feng and M. Lapata. 2013. Automatic caption gen-
eration for news images. IEEE Trans. Pattern Anal.
Mach. Intell. , 35.
A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean,
M. Ranzato, and T. Mikolov. 2013. Devise: A deep
visual-semantic embedding model. In NIPS .
C. Goller and A. K uchler. 1996. Learning task-
dependent distributed representations by backpropaga-
tion through structure. In Proceedings of the Interna-
tional Conference on Neural Networks .
E. Grefenstette, G. Dinu, Y .-Z. Zhang, M. Sadrzadeh, and
M. Baroni. 2013. Multi-step regression learning for
compositional distributional semantics. In IWCS .
A. Gupta and L. S. Davis. 2008. Beyond nouns: Exploit-
ing prepositions and comparative adjectives for learn-
ing visual classiﬁers. In ECCV .M. Hodosh, P. Young, and J. Hockenmaier. 2013. Fram-
ing image description as a ranking task: Data, mod-
els and evaluation metrics. J. Artif. Intell. Res. (JAIR) ,
47:853899.
E. H. Huang, R. Socher, C. D. Manning, and A. Y . Ng.
2012. Improving Word Representations via Global
Context and Multiple Word Prototypes. In ACL.
K. Jarrett, K. Kavukcuoglu, M.A. Ranzato, and Y . Le-
Cun. 2009. What is the best multi-stage architecture
for object recognition? In ICCV .
P. Blunsom. K.M. Hermann. 2013. The role of syntax
in vector space models of compositional semantics. In
ACL.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. 2012.
Imagenet classiﬁcation with deep convolutional neural
networks. In NIPS .
G. Kulkarni, V . Premraj, S. Dhar, S. Li, Y . Choi, A. C.
Berg, and T. L. Berg. 2011. Baby talk: Understanding
and generating image descriptions. In CVPR .
N. Kumar, A. C. Berg, P. N. Belhumeur, , and S. K. Na-
yar. 2009. Attribute and simile classiﬁers for face ver-
iﬁcation. In ICCV .
P. Kuznetsova, V . Ordonez, A. C. Berg, T. L. Berg, and
Yejin Choi. 2012. Collective generation of natural
image descriptions. In ACL.
Q. V . Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen,
G.S. Corrado, J. Dean, and A. Y . Ng. 2012. Build-
ing high-level features using large scale unsupervised
learning. In ICML .
T. Mikolov, W. Yih, and G. Zweig. 2013. Linguistic
regularities in continuous spaceword representations.
InHLT-NAACL .
J. Mitchell and M. Lapata. 2010. Composition in dis-
tributional models of semantics. Cognitive Science ,
34(8):13881429.
J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A.Y .
Ng. 2011. Multimodal deep learning. In ICML .
V . Ordonez, G. Kulkarni, and T. L. Berg. 2011. Im2text:
Describing images using 1 million captioned pho-
tographs. In NIPS .
J. B. Pollack. 1990. Recursive distributed representa-
tions. Artiﬁcial Intelligence , 46, November.
C. Rashtchian, P. Young, M. Hodosh, and J. Hocken-
maier. 2010. Collecting image annotations using
Amazons Mechanical Turk. In Workshop on Creat-
ing Speech and Language Data with Amazons MTurk .
R. Socher and L. Fei-Fei. 2010. Connecting modalities:
Semi-supervised segmentation and annotation of im-
ages using unaligned text corpora. In CVPR .
R. Socher, C. D. Manning, and A. Y . Ng. 2010. Learning
continuous phrase representations and syntactic pars-
ing with recursive neural networks. In Proceedings of
the NIPS-2010 Deep Learning and Unsupervised Fea-
ture Learning Workshop .R. Socher, E. H. Huang, J. Pennington, A. Y . Ng, and
C. D. Manning. 2011a. Dynamic Pooling and Unfold-
ing Recursive Autoencoders for Paraphrase Detection.
InNIPS .
R. Socher, C. Lin, A. Y . Ng, and C.D. Manning. 2011b.
Parsing Natural Scenes and Natural Language with
Recursive Neural Networks. In ICML .
R. Socher, J. Pennington, E. H. Huang, A. Y . Ng, and
C. D. Manning. 2011c. Semi-Supervised Recursive
Autoencoders for Predicting Sentiment Distributions.
InEMNLP .
R. Socher, B. Huval, C. D. Manning, and A. Y . Ng.
2012. Semantic Compositionality Through Recursive
Matrix-Vector Spaces. In EMNLP .
R. Socher, J. Bauer, C. D. Manning, and A. Y . Ng. 2013a.
Parsing With Compositional Vector Grammars. In
ACL.
R. Socher, M. Ganjoo, C. D. Manning, and A. Y . Ng.
2013b. Zero-Shot Learning Through Cross-Modal
Transfer. In NIPS .
R. Socher, M. Ganjoo, H. Sridhar, O. Bastani, and
A. Y . Ng. C. D. Manning and. 2013c. Zero-shot learn-
ing through cross-modal transfer. In Proceedings of
the International Conference on Learning Representa-
tions (ICLR, Workshop Track) .
N. Srivastava and R. Salakhutdinov. 2012. Multimodal
learning with deep boltzmann machines. In NIPS .
A. Torralba, K. P. Murphy, and W. T. Freeman. 2010.
Using the forest to see the trees: exploiting context for
visual object detection and localization. Communica-
tions of the ACM .
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artiﬁcial Intelligence Research , 37:141188.
B. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu. 2010.
I2t:image parsing to text description. IEEE Xplore .
  Chameleon: Mixed-Modal Early-Fusion Foundation
Models
Chameleon Team1,
1FAIR at Meta
See Contributions section for full author list.
We present Chameleon, a family of early-fusion token-based mixed-modal models capable of under-
standing and generating images and text in any arbitrary sequence. We outline a stable training
approach from inception, an alignment recipe, and an architectural parameterization tailored for the
early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range
of tasks, including visual question answering, image captioning, text generation, image generation, and
long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including
state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while
being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image
generation, all in a single model. It also matches or exceeds the performance of much larger models,
including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal
generation evaluation, where either the prompt or outputs contain mixed sequences of both images and
text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.
Date:May 17, 2024
1 Introduction
Recent multimodal foundation models are very widely adopted but still model different modalities separately,
often using modality specific encoders or decoders. This can limit their ability to integrate information across
modalities and generate multimodal documents that can contain arbitrary sequences of images and text. In
this paper, we present Chameleon , a family of mixed-modal foundation models capable of generating and
reasoning with mixed sequences of arbitrarily interleaved textual and image content (Figures 2-4). This
allows for full multimodal document modeling, which is a direct generalization of standard multimodal tasks
such as image generation, understanding and reasoning over images, and text-only LLMs. Chameleon is
instead designed to be mixed-model from inception and uses a uniform architecture trained from scratch in an
end-to-end fashion on an interleaved mixture of all modalities, i.e., images, text, and code.
Our unified approach uses fully token-based representations for both image and textual modalities (Figure 1).
By quantizing images into discrete tokens, analogous to words in text, we can apply the same transformer
architecture to sequences of both image and text tokens, without the need for separate image/text encoders
(Alayrac et al., 2022; Liu et al., 2023b; Laurençon et al., 2023) or domain-specific decoders (Ramesh et al.,
2022; Jin et al., 2023; Betker et al., 2023). This early-fusion approach, where all modalities are projected into
a shared representational space from the start, allows for seamless reasoning and generation across modalities.
However, it also presents significant technical challenges, particularly in terms of optimization stability and
scaling.
We address these challenges through a combination of architectural innovations and training techniques. We
introduce novel modifications to the transformer architecture, such as query-key normalization and revised
placement of layer norms, which we find to be crucial for stable training in the mixed-modal setting (Section
2.3). We further show how to adapt the supervised finetuning approaches used for text-only LLMs to the
mixed-modal setting, enabling strong alignment at scale (Section 3). Using these techniques, we successfully
train Chameleon-34B on 5x the number of tokens as Llama-2  enabling new mixed-modal applications while
still matching or even outperforming existing LLMs on unimodal benchmarks.
1arXiv:2405.09818v1  [cs.CL]  16 May 2024Mixed-Modal Auto-Regressive LM 
TEXT PROMPT What can I bake 
with this? TEXT OUTPUT Here is a recipe for 
banana bread. 
Image Tokenizer Image De-Tokenizer 
IMAGE PROMPT 
IMAGE OUTPUT 
Start 
ImageStart 
Image End
Image
End
ImageStart 
Image 
Start 
ImageEnd
Image
Mixed Modal Auto-Regressive LM 
(a) Mixed-Modal Pre-Training (b) Mixed-Modal Generation Figure 1 Chameleon represents all modalities  images, text, and code, as discrete tokens and uses a uniform
transformer-based architecture that is trained from scratch in an end-to-end fashion on 10T tokens of interleaved
mixed-modal data. As a result, Chameleon can both reason over, as well as generate, arbitrary mixed-modal documents.
Text tokens are represented in green and image tokens are represented in blue.
Extensive evaluations demonstrate that Chameleon is a broadly capable model on a diverse set of tasks.
On visual question answering and image captioning benchmarks, Chameleon-34B achieves state-of-the-art
performance, outperforming models like Flamingo, IDEFICS and Llava-1.5 (Section 5.2). At the same time,
it maintains competitive performance on text-only benchmarks, matching models like Mixtral 8x7B and
Gemini-Pro on commonsense reasoning and reading comprehension tasks (Section 5.1). But perhaps most
impressively, Chameleon unlocks entirely new capabilities in terms of mixed-modal reasoning and generation.
As using only static, public benchmarks to evaluate model performance could be limited (Schaeffer, 2023), we
also conduct a carefully designed human evaluation experiment by measuring the quality of mixed-modal
long form responses to open-ended prompts. Chameleon-34B substantially outperforms strong baselines like
Gemini-Pro and GPT-4V (Section 4), achieving a 60.4% preference rate against Gemini-Pro and a 51.6%
preference rate against GPT-4V in pairwise comparisons.
In summary, we present the following contributions:
We present Chameleon , a family of early-fusion token-based mixed-modal models capable of reasoning
over and generating interleaved image-text documents, setting a new bar for open multimodal foundation
models.
We introduce architectural innovations and training techniques that enable the stable and scalable
training of early-fusion token-based models, addressing key challenges in mixed-modal learning.
Through extensive evaluations, we demonstrate state-of-the-art performance across a diverse set of
vision-language benchmarks, while maintaining competitive performance on text-only tasks, and high
quality image generation, all in the same model.
We conduct the first large-scale human evaluation on open-ended mixed-modal reasoning and generation,
demonstrating the unique capabilities of Chameleon in this new setting.
Chameleon represents a significant step towards realizing the vision of unified foundation models capable of
flexibly reasoning over and generating multimodal content.
2Prompt: Im bored. Could you show me some cool, quirky-looking birds? Also tell me about them in 23
sentences.
Response: Absolutely! Here are three birds with some unique features that
you might find interesting:
Keel-Billed Toucan The Keel-Billed Toucan is a brightly colored bird
native to South American rainforests and woodlands. Its most distinctive
feature is its colorful beak, which can grow up to seven inches long and has a
distinctive, curved shape. The beak is used for plucking fruit from trees and
for social displays. Despite its large size, the Keel-Billed Toucan is a skilled
flyer and can often be seen flitting through the treetops. Here is an image of
this vibrant bird: <img>
PuffinPuffins are small, aquatic birds that are found in the northern hemisphere.
They have distinctive black and white feathers, as well as a brightly colored
beak that is used for attracting mates and defending their territory. Puffins
are known for their distinctive puffin call, which sounds like a loud, guttural
chuckle. They feed on fish, which they catch by diving into the water from the
air. Puffins are colonial birds, and they breed in large colonies on rocky cliffs
and islands. Here is an image of this adorable bird: <img>
Golden Pheasant The Golden Pheasant is a brightly colored bird native to the
forests of China. It is a member of the pheasant family and is known for its
striking plumage, which features a golden-yellow body, red face, and green tail
feathers. The Golden Pheasant is a ground-dwelling bird, and it feeds on a diet
of insects, seeds, and fruits. It is a popular game bird, and is often hunted for
its meat and feathers. In captivity, the Golden Pheasant is sometimes kept as
an ornamental bird, due to its striking appearance. Here is an image of this
beautiful bird: <img>
I hope these images and descriptions have helped to brighten your day! If you have any other questions
or requests, please dont hesitate to ask.
Figure 2 Sample interleaved image and text generation from Chameleon. The corresponding images are generated in
locations marked by <img>.
32 Pre-Training
Chameleon represents images, in addition to text, as a series of discrete tokens and takes advantage of the
scaling properties of auto-regressive Transformers (Ramesh et al., 2021; Aghajanyan et al., 2022, 2023; Yu
et al., 2023). We present any ordering of images and text during training ranging from text-only, to single
text/image pairs to full interleaved text-image documents.
Prompt: <img> While chameleons are masters of camouflage in the natural
world, lets see how they might fare in a man-made setting.
1. Analyze the colors, patterns, and textures in this image and identify the
most challenging spot for a chameleon to blend in.
2. Then generate an image of a chameleon with the optimal pattern to
camouflage itself.
3. Then, rate and explain how successful each attempt is on a scale from 1-10.
Response: The image is of a stop sign in a forested area, with some sunlight filtering through the trees.
The sign has a red octagonal shape with a white border and the word STOP written in white letters.
In the background, there are green leaves and trees.
Challenging Spot for a Chameleon: The stop sign would be the most challenging spot for a chameleon
to blend in due to its bright red color and bold white letters, which would stand out in the forested
environment.
Optimal Pattern for Camouflage: To camouflage itself on the stop sign, a chameleon would need
to have a pattern that would break up its outline and make it less conspicuous. A pattern with shades
of brown and red would help the chameleon blend in more effectively.
Additionally, a mottled or dappled pattern would help to scatter
light and further reduce the chameleons visibility.
<img>
Rating and Explanation: A rating of 2/10 would be given for
the chameleons ability to blend in on the stop sign. While the
chameleons green and brown pattern helps to break up its outline,
the bright red color of the stop sign will still make it stand out in
the forested environment.
Figure 3 Sample generation from Chameleon.
2.1 Tokenization
Image Tokenization We train a new image tokenizer based on Gafni et al. (2022), which encodes a 512512
image into 1024discrete tokens from a codebook of size 8192. For training this tokenizer, we use only licensed
images. Given the importance of generating human faces, we up-sample the percentage of images with faces
during pre-training by 2 times. A core weakness of our tokenizer is in reconstructing images with a large
amount of text, therefore upper bounding the capability of our models, when it comes to heavy OCR-related
tasks.
Tokenizer We train a new BPE tokenizer (Sennrich et al., 2016) over a subset of the training data outlined
belowwithavocabularysizeof65,536, whichincludesthe8192imagecodebooktokens, usingthe sentencepiece
library (Kudo and Richardson, 2018).
4Prompt: What can I bake with this? Give me the full recipe and show me an
image with the final dish. <img>
Response: Bananas are a versatile fruit that can be used in various baked
goods, such as muffins, bread, cakes, and cookies. They add natural sweetness,
moisture, and a subtle banana flavor to the finished product. Additionally,
bananas are rich in potassium, fiber, and other nutrients, making them a
healthy addition to your baking repertoire.
Here is a simple recipe for banana bread that uses 3 ripe bananas:
Ingredients:
3 ripe bananas, mashed
1 1/2 cups all-purpose flour
1 tsp baking powder
1/2 tsp baking soda
1/2 tsp salt
1/4 cup unsalted butter, softened
1 cup granulated sugar
2 large eggs
1 tsp vanilla extract
1/2 cup chopped walnuts (optional)
Instructions:
Preheat your oven to 350 F (180 C). Grease a 9x5-inch loaf pan and set it aside.
In a medium bowl, whisk together the flour, baking powder, baking soda, and salt. Set aside.
In a large mixing bowl, use an electric mixer to cream together the butter and sugar until light and
fluffy, about 2-3 minutes.
Beat in the eggs one at a time, followed by the mashed bananas and vanilla extract.
Gradually mix in the dry ingredients until just combined, being careful
not to overmix.
If using walnuts, fold them into the batter.
Pour the batter into the prepared loaf pan and smooth the top.
Bake for 50-60 minutes, or until a toothpick inserted into the center of
the bread comes out clean.
Remove the bread from the oven and let it cool in the pan for 10 minutes
before transferring it to a wire rack to cool completely. <img>
Figure 4 Sample Chameleon outputs.
2.2 Pre-Training Data
We delineate the pre-training stage into two separate stages. The first stage takes up the first 80% of training
while the second stage takes the last 20%. For all Text-To-Image pairs we rotate so that 50% of the time the
image comes before the text (i.e., captioning).
2.2.1 First Stage
In the first stage we use a data mixture consisting of the following very large scale completely unsupervised
datasets.
Text-Only: We use a variety of textual datasets, including a combination of the pre-training data used to
train LLaMa-2 (Touvron et al., 2023) and CodeLLaMa (Roziere et al., 2023) for a total of 2.9 trillion text-only
tokens.
50k 5k 10k 15k 20k 25k 30k
Step0.05.010.015.020.025.030.035.0Output Normw/ QK-norm and dropout
w/o dropout
w/o QK-norm or dropout(a)Uncontrolled growth of output
norms is a strong indicator of future
training divergence.
0k 25k 50k 75k 100k 125k 150k 175k
Step3.43.53.53.63.63.73.7Training Lossw/o QK-norm
w/ QK-norm(b)An ablation with Chameleon -7B
with and without QK-Norm .
0k 20k 40k 60k 80k
Step3.43.53.63.73.83.9Training Lossw/o dropout
w/ dropout(c)An ablation with Chameleon -7B
with and without dropout.
Figure 5 Output norm and training loss curves for Chameleon models under various settings.
Text-Image: The text-image data for pre-training is a combination of publicly available data sources and
licensed data. The images are then resized and center cropped into 512512images for tokenization. In
total, we include 1.4 billion text-image pairs, which produces 1.5 trillion text-image tokens.
Text/Image Interleaved: We procure data from publicly available web sources, not including data from Metas
products or services, for a total of 400 billion tokens of interleaved text and image data similar to Laurençon
et al. (2023). We apply the same filtering for images, as was applied in Text-To-Image .
2.2.2 Second Stage
In the second stage, we lower the weight of the first stage data by 50% and mix in higher quality datasets
while maintaining a similar proportion of image text tokens.
We additionally include a filtered subset of the train sets from a large collection of instruction tuning sets.
2.3 Stability
It was challenging to maintain stable training when scaling the Chameleon models above 8B parameters
and 1T tokens, with instabilities often only arising very late in training. We adopted to following recipe for
architecture and optimization to achieve stability.
Architecture Our architecture largely follows LLaMa-2 (Touvron et al., 2023). For normalization, we continue
to use RMSNorm (Zhang and Sennrich, 2019); we use the SwiGLU (Shazeer, 2020) activation function and
rotary positional embeddings (RoPE) (Su et al., 2021).
We found that the standard LLaMa architecture showed complex divergences due to slow norm growth in the
mid-to-late stages of training. We narrowed down the cause of the divergence to the softmax operation being
problematic when training with multiple modalities of significantly varying entropy due to the translation
invariant property of softmax (i.e., softmax (z) =softmax (z+c)). Because we share all weights of the model
across modalities, each modality will try to compete with the other by increasing its norms slightly; while
not problematic at the beginning of training, it manifests in divergences once we get outside the effective
representation range of bf16 (In Figure 6b, we show that ablations without image generation did not diverge).
In a unimodal setting, this problem has also been named the logit drift problem (Wortsman et al., 2023). In
Figure 5a, we plot the norms of the output of the last transformer layer as training progresses and we find
that although training divergences can manifest after as much as even 20-30% of training progress, monitoring
uncontrolled growth of output norms is strongly correlated with predicting future loss divergence.
The softmax operation appears in two places in transformers: the core attention mechanism and the softmax
over the logits. As inspired by Dehghani et al. (2023) and Wortsman et al. (2023), we first deviate from
the Llama architecture by using query-key normalization (QK-Norm). QK-Norm directly controls the norm
growth of input to the softmax by applying layer norm to the query and key vectors within the attention.
60k 100k 200k 300k 400k 500k 600k
Step2.82.93.03.13.2Training Loss7b
34b(a)Training Curves for 600k steps for
Chameleon-7B and Chameleon-34B
over Mixed-Modal Data.
0k 50k 100k 150k 200k 250k
Step0.951.001.051.101.15Training Loss7B w/o image generation(b)Training loss curve with image gen-
eration disabled does not suffer from
instability issues.
0k 2k 4k 6k 8k 10k
Step3.54.04.55.05.56.0Training Lossw/o norm reordering
w/ norm reordering(c)For Chameleon-34B , using
dropout does not fix divergences,
both with and without norm-
reordering.
Figure 6 Training loss curves for Chameleon models under various settings.
In Figure 5b, we show training loss curves for Chameleon-7B with and without QK-Norm, and the latter
diverges after approximately 20% of a training epoch.
We found that to stabilize Chameleon-7B by controlling norm growth, it was necessary to introduce dropout
after the attention and feed-forward layers, in addition to QK-norm (see Figure 5c). However, this recipe was
not enough to stabilitize, Chameleon-34B , which required an additional re-ordering of the norms. Specifically,
we use the strategy of normalization proposed in Liu et al. (2021), within the transformer block. The benefit
of the Swin transformer normalization strategy is that it bounds the norm growth of the feedforward block,
which can become additionally problematic given the multiplicate nature of the SwiGLU activation function.
Ifhrepresents the hidden vector at time-step tafter self-attention is applied to input x,
Chameleon-34B: h=x+attention_norm (attention (x))
output =h+ffn_norm (feed_forward (h))
Llama2: h=x+attention (attention_norm (x))
output =h+feed_forward (ffn_norm (h))
There was no difference in perplexity when training a model from scratch with and without the normalization
re-ordering until the divergence of the LLaMa-2 parameterization. Additionally, we found that this type of
normalization did not work well in combination with dropout and therefore, we train Chameleon-34B without
dropout (Figure 6c). Furthermore, we retroactively found that Chameleon-7B can also be stably trained
without dropout, when using norm-reordering, but QK-norm is essential in both cases. We plot training
curves for the first 600k steps for both Chameleon-7B andChameleon-34B in Figure 6a.
Optimization Our training process uses the AdamW optimizer (Loshchilov and Hutter, 2017), with β1set
to 0.9 and β2to 0.95, with an ϵ= 105. We use a linear warm-up of 4000 steps with an exponential decay
schedule of the learning rate to 0. Additionally, we apply a weight decay of 0.1 and global gradient clipping at
a threshold of 1.0. We use a dropout of 0.1 (Srivastava et al., 2014) for Chameleon-7B for training stability,
but not for Chameleon-34B (see Figure 5c and 6c).
The application of QK-Norm while helping the inner softmaxes within the Transformer does not solve the
problem of logit shift in the final softmax. Following Chowdhery et al. (2022); Wortsman et al. (2023),
we apply z-lossregularization. Specifically, we regularize the partition function Zof the softmax function
σ(x)i=exi
Zwhere Z=P
iexiby adding 105log2Zto our loss function.
ForChameleon -7B it was important to use both dropout and z-lossto achieve stability, while Chameleon -34B
only required z-loss(Figure 6c).
Chameleon-7B was trained with a global batch size of 223(8M) tokens and Chameleon-34B was trained
with a global batch size of 3222(12M) tokens. We do 2.1 epochs over our full training dataset for a total
7of 9.2 trillion tokens seen during training. We show the first 600k steps of training (55% for Chameleon-7B
and 80% for Chameleon-34B ) in Figure 6a.
Table 1Summary of core architecture and optimization decisions made in Chameleon in contrast to LLaMa-1 and
LLaMa-2.
Model Params Context Length GQA Tokens LR Epochs Dropout Zloss Qknorm
LLaMa-1 7B 2k 1.0T 3.01041.0 0.0 0.0 
33B 2k 1.4T 1.51041.0 0.0 0.0 
LLaMa-2 7B 4k 2.0T 3.01041.0 0.0 0.0 
34B 4k 2.0T 1.51041.0 0.0 0.0 
Chameleon 7B 4k 4.4T 1.01042.1 0.1 105
34B 4k 4.4T 1.01042.1 0.0 105
Pre-Training Hardware Our model pretraining was conducted on Metas Research Super Cluster (RSC) (Lee
and Sengupta, 2022), and our alignment was done on other internal research clusters. NVIDIA A100 80
GB GPUs power both environments. The primary distinction is the interconnect technology: RSC employs
NVIDIA Quantum InfiniBand, whereas our research cluster utilizes Elastic Fabric. We report our GPU usage
for pre-training in Table 2.
Table 2 Chameleon Model Pre-Training Resource Usage
Chameleon Concurrent GPUs GPU Hours
7B 1024 856481
34B 3072 4282407
2.4 Inference
To support alignment and evaluation, both automated and human, and to demonstrate the application-
readiness of our approach, we augment the inference strategy with respect to interleaved generation to improve
throughput and reduce latency.
Autoregressive, mixed-modal generation introduces unique performance-related challenges at inference time.
These include:
Data-dependencies per-step  given that our decoding formulation changes depending on whether the
model is generating images or text at a particular step, tokens must be inspected at each step (i.e.
copied from the GPU to the CPU in a blocking fashion) to guide control flow.
Masking for modality-constrained generation  to facilitate exclusive generation for a particular modality
(e.g. image-only generation), tokens that do not fall in a particular modality space must be masked and
ignored when de-tokenizing.
Fixed-sized text units  unlike text-only generation, which is inherently variable-length, token-based
image generation produces fixed-size blocks of tokens corresponding to an image.
Given these unique challenges, we built a standalone inference pipeline based on PyTorch (Paszke et al., 2019)
supported with GPU kernels from xformers (Lefaudeux et al., 2022).
Our inference implementation supports streaming for both text and images. When generating in a streaming
fashion, token-dependent conditional logic is needed at each generation step. Without streaming, however,
blocks of image tokens can be generated in a fused fashion without conditional computation. In all cases,
token masking removes branching on the GPU. Even in the non-streaming setting, however, while generating
text, each output token must be inspected for image-start tokens to condition image-specific decoding
augmentations.
8Table 3Supervised Fine-Tuning Dataset Statistics
Category # of Samples # of Tokens # of Images
Chameleon-SFTText 1.6M 940.0M -
Code 14.1K 1.1M -
Visual Chat 15.6K 19.4M 16.7K
Image Generation 64.3K 68.0M 64.3K
Interleaved Generation 16.9K 35.8M 30.7K
Safety 95.3K 38.6M 1.6K
3 Alignment
We follow recent work in using a light weight alignment stage based on supervised fine tuning on carefully
curated high quality datasets (Zhou et al., 2023). We include a range of different types of data, targeting
both exposing model capabilities and improving safety.
3.1 Data
We separate our supervised fine-tuning (SFT) dataset into the following categories: Text,Code,Visual
Chat,Image Generation ,Interleaved Text/Image Generation , and Safety. We include examples from each
category from the Chameleon-SFT dataset in Figure 7.
We inherit the TextSFT dataset from LLaMa-2 (Touvron et al., 2023) and the CodeSFT from CodeLLaMa
(Roziere et al., 2023). For the Image Generation SFT dataset, we curate highly aesthetic images by applying
and filtering each image in our licensed data, with an aesthetic classifier from Schuhmann et al. (2022). We
first select images rated as at least six from the aesthetic classifier and then select the top 64K images closest
in size and aspect ratio to 512512(the native resolution of our image tokenizer).
For both Visual Chat andInterleaved Text/Image Generation SFT data, we focused on very high-quality
data collection using third-party vendors following a similar strategy recommended by Touvron et al. (2023);
Zhou et al. (2023). We do not include any Meta user data. We present our datasets statistics in Table 3.
Safety Data We include a collection of prompts that can potentially provoke the model to produce unsafe
content, and match them with a refusal response (e.g. I cant help with that.). These prompts cover a
wide variety of sensitive topics, such as violence, controlled substances, privacy, and sexual content. Our
collection of safety tuning data includes examples from LLaMa-2-Chat (Touvron et al., 2023), synthetic
text-based examples generated with Rainbow Teaming (Samvelyan et al., 2024), image generation prompts
manually selected from Pick-A-Pic (Kirstain et al., 2023) for safety testing, examples for cyber security
safety (Roziere et al., 2023), as well as mixed-modal prompts collected internally through manual annotation
and automatic expansion (Honovich et al., 2022). Collecting mixed-modal prompts is of particular importance,
since it addresses potential multi-modal attack vectors, which are outside the distribution of text-only and
text-to-image safety tuning datasets.
3.2 Fine-Tuning Strategy
Data Balancing We found that balancing modalities within the SFT stage is important for high quality
alignment. Specifically during the SFT stage, if there is a severe imbalance between pairings of modalities (or
when a specific modality should trigger), the model learns an unconditional prior of generating that modality
which can either mute or over exaggerate the generation of a single modality.
Optimization Our supervised fine-tuning strategy incorporates a cosine learning rate schedule, starting at an
initial rate of 1e-5, combined with a weight decay set at 0.1. We maintain a batch size of 128, accommodating
sequences up to 4096 tokens. During fine-tuning, each dataset instance comprises a paired prompt and its
corresponding answer. To improve efficiency, we pack as many prompts and answers as possible into each
9Prompt: Can you write a poem using
rhyming couplets?
Response: Sure, heres a short poem
using rhyming couplets:
The sun sets in the sky,
A beautiful sight to the eye.
The stars come out to play,
A magical sight to behold.
The wind whispers through the
trees,
A gentle breeze that brings ease.
The birds sing their sweet melodies,
A soothing sound that calms the soul.
The world is full of wonder,
A place of beauty and thunder.
So let us cherish every moment,
And bask in the beauty all around us.
(a)TextPrompt: Given a list of integers,
find the first number that ap-
pears at least twice.
Response:
def find_duplicate(my_list):
seen = set()
for num in my_list:
if num in seen:
return num
seen.add(num)
return None
(b)CodePrompt: Renaissance square of
Stepanakert
Response:
(c)Image Generation
Prompt: Which elements contribute to this flower
arrangements distinctiveness, and what sets its sur-
roundings apart?
Response: The image features an autumnal-themed
floral arrangement in a classically designed vase.
The arrangement stands out with its striking com-
bination of red maple leaves, purple flowers, and
pink-red roses. The vases rosettes give it an antique
design. In the background, blurry green vegetation
is visible, creating a contrast to the autumnal theme
of the bouquet. The red maple leaves, both in the
arrangement and scattered around the vase, evoke
autumn, while the green vegetation suggests a more
summer-like setting. The outdoor setting is bright
and sunny, adding to the contrast between the ar-
rangement and its settings.
(d)Visual ChatGyumri is an urban municipal community and the
second-largest city in Armenia, serving as the ad-
ministrative center of Shirak Province in the north-
western part of the country.
Archaeological excavations conducted throughout
the Soviet period have shown that the area of
modern-day Gyumri has been populated since at
least the third millennium BC.
(e)Interleaved Text/Image Generation
Figure 7 Example alignment data for different categories.
sequence, inserting a distinct token to delineate the end of a prompt and the beginning of an answer. We
use an autoregressive training objective, selectively masking the loss for the prompt tokens. This targeted
approach allows us to optimize the model exclusively based on the answer tokens, which provides slight gains
overall. We also apply a dropout of 0.05. Additionally, we maintain the same zloss that was used during
10Advice: 10.2% 
How-to: 12.5% 
How do I properly clean my TV 
screen? I used Windex and now 
there are towel fibers and wipe 
marks all over. Show me some 
reference photos. 
Explanation: 14.4% 
I've been studying 
classical French art, 
and my favorite 
so far is his painting 
seen here: <img>  
Could you please 
give me a few images of other 
contemporary artworks that have 
this same aesthetic? 
Hypothetical: 5.6% 
What would the modern-day 
vehicle look like if oil had never 
been discovered? Brainstorming: 18.6% 
Show me a Middle Eastern alternative to 
these dishes. <img1> <img2> 
Reasoning: 2.1% Identification: 9.3 % 
Is the below image a 
Shetland Pony? If 
not, what is it, and 
can you show me a 
Shetland Pony? 
<img> 
Article: 3.1% 
Write me an introduction to a story about 
knick-knacks, and finish the story by 
shifting the focus with an image. Report: 5.4% 
Who designed the church in the image below, 
and what's the name of the 
Church? <img> Can you 
please provide me with 
additional photos of famous 
landmarks designed 
by the same architect? Story: 3.9% 
Can you create and illustrate a short story 
for children about an octopus that can't 
stop eating pizza? 
Other: 5.2% 
Create a decal for my truck that features 
running horses as well as the TRD insignia. Use 
black to gray gradients. Comparison: 9.6% 
Please tell me what the difference between 
these two creatures is, and show me some 
more examples. <img1> <img2> 
What does a meningitis rash look 
like? What are the other 
symptoms I should be on the 
lookout for? 
What is typically found at a construction site? 
Show me a construction site that has a crane. 
Figure 8 Task categories and examples of prompts. Image attributions: Seguin (2010); Agriflanders (2009); Tuszyński
(2015); Sokolov (2022).
pre-training. During supervised fine-tuning, images in the prompt are resized with border padding to ensure
that all the information is available in the image, whereas images in the answer are center-cropped to ensure
visually good image generation quality.
4 Human Evaluations and Safety Testing
Chameleon has significant new mixed modal understanding and generation abilities that cannot be measured
with existing benchmarks. In this section, we detail how we conduct human evaluations on large multi-modal
language models responses to a set of diverse prompts that regular users may ask daily. We first introduce
how we collect the prompts and then describe our baselines and evaluation methods, along with the evaluation
results and analysis. A safety study is also included in this section.
4.1 Prompts for Evaluation
We work with a third-party crowdsourcing vendor to collect a set of diverse and natural prompts from human
annotators. Specifically, we ask annotators to creatively think about what they want a multi-modal model
to generate for different real-life scenarios. For example, for the scenario of imagine you are in a kitchen,
annotators may come up with prompts like How to cook pasta? or How should I design the layout of my
island? Show me some examples. The prompts can be text-only or text with some images, and the expected
responses should be mixed-modal, containing both text and images.
After collecting an initial set of prompts, we ask three random annotators to evaluate whether the prompts
are clear and whether they expect the responses to contain images. We use a majority vote to filter unclear
prompts and prompts that dont expect mixed-modal responses. In the end, our final evaluation set contains
1,048 prompts: 441 (42.1%) are mixed-modal (i.e., containing both text and images), and the remaining 607
(57.9%) are text-only.
To better understand the tasks users would like a multi-modal AI system to fulfill, we manually examine
11Fulfills Partially fulfills Does not fulfill
T ask Fulfillment Rate010203040506070Percentage (%)Model
Chameleon
Gemini+
GPT-4V+
Gemini
GPT-4V(a)The prompt task fulfillment rates.
0 20 40 60 80 100
Percent (%)GPT-4VGeminiGPT-4V+Gemini+
46.053.535.841.5
31.431.231.634.5
22.615.332.624.0Wins Ties Loses (b)Chameleon vs. the baselines: Gemini+, GPT-4V+,
Gemini, GPT-4V.
Figure 9 Performance of Chameleon vs baselines, on mixed-modal understanding and generation on a set of diverse
and natural prompts from human annotators.
the prompts and classify them into 12 categories. The description of these task categories1, as well as their
example prompts, can be found in Figure 8.
4.2 Baselines and Evaluations
We compare Chameleon 34B with OpenAI GPT-4V and Google Gemini Pro by calling their APIs. While these
models can take mixed-modal prompts as input, their responses are text-only. We create additional baselines
by augmenting GPT-4V and Gemini responses with images to have even stronger baselines. Specifically, we
instruct these models to generate image captions by adding the following sentence at the end of each original
input prompt: If the question requires an image to be generated, then generate an image caption instead
and enclose the caption in a pair of caption  /caption tags. We then use OpenAI DALL-E 3 to generate
images conditioned on these captions and replace the captions in the original responses with those generated
images. We denote the enhanced responses as GPT-4V+ and Gemini+ in this section. Working with the same
third-party crowdsourcing vendor, we conduct two types of evaluations to measure the model performance:
absolute andrelative.
4.2.1 Absolute Evaluation
For absolute evaluations, the output of each model is judged separately by asking three different annotators
a set of questions regarding the relevance and quality of the responses. Below, we give detailed results and
analysis on the most critical question, whether the response fulfills the task described in the prompt .
On task fulfillment, we ask annotators whether the response fulfills,partially fulfills , ordoes not fulfill the
task described in the prompt. As shown in Figure 9a, much more of Chameleon s responses are considered
to have completely fulfilled the tasks: 55.2% for Chameleon vs. 37.6% of Gemini+ and 44.7% of GPT-4V+.
When judging the original responses of Gemini and GPT-4V, the annotators consider much fewer prompts to
be fully fulfilled: Gemini completely fulfills 17.6% of the tasks and GPT-4V 23.1%. We suspect that because
all the prompts expect mixed-modal output, the text-only responses from Gemini and GPT-4V might be
viewed as only partially completing the tasks by the annotators.
The task fulfillment rates in each category and in each input modality can be found in Appendix B. The
task categories that Chameleon performs well include Brainstorming ,Comparison , andHypothetical , and the
1While not instructed specifically, certain image understanding tasks that require identifying the text in an image, such as
OCR (Optical character recognition), do not appear in our evaluation set of prompts.
12categories Chameleon needs to improve include Identification andReasoning . On the other hand, we dont
see that the model performance differs a lot when comparing mixed-modality and text-only prompts, although
Chameleon seems to perform slightly better on text-only prompts, while Gemini+ and GPT-4V+ are slightly
better on mixed-modal ones. Figure 2 shows an example of Chameleon s response to a brainstorming prompt.
4.2.2 Relative Evaluation
For relative evaluations, we directly compare Chameleon with each baseline model by presenting their
responses to the same prompt in random order and asking human annotators which response they prefer. The
options include the firstresponse, the secondresponse, and about the same . Figure 9b shows Chameleon s
win rates2over the baselines. Compared with Gemini+, Chameleon s responses are better in 41.5% of the
cases, 34.5% are tie, and 24.0% are inferior. Annotators also think that Chameleon s responses are slightly
more often better than GPT-4V+, with 35.8% win, 31.6% tie, and 32.6% loss. Overall, Chameleon has win
rates of 60.4% and 51.6% over Gemini+ and GPT-4V+, respectively. When compared with the original
responses from Gemini without the augmented images, Chameleon s responses are considered better in 53.5%
of the cases, 31.2% are tied, and 15.3% are inferior. Chameleon s responses are also considered better than
GPT-4V more frequently, with 46.0% win, 31.4% tie, and 22.6% loss. Chameleon s win rates over Gemini
and GPT-4V are 69.1% and 61.7%, respectively.
4.3 Inter-annotator Agreement
Every question in our evaluation is answered by three different human annotators, and we take the majority
votes as the final answer. To understand the quality of the human annotators and whether the questions we
asked are reasonably designed, we examine the level of agreement between different annotators.
The levels of agreement on each question in the absolute evaluation are shown in Figure 10.
0
5001000 1500 2000 2500 3000
CountContaining images
Image quality
Image relevance
Language quality
Objectionable content
Relevance
T ask fulfillment
AccuracyAgreement
All
T wo
None
Figure 10 The inter-annotator agreement on the questions in the absolute evaluation.
For questions about simple, objective properties of the responses, we very rarely see three annotators disagree
with each other. For example, annotators have unanimous judgments on whether the model responses contain
objectionable content (e.g., hate speech); in this case, all models produce safe responses. For some questions,
such as whether the response fulfills the task or whether the model interprets the prompt correctly, when one
2The win rate is calculated by adding 1 point for a win and 0.5 points for a tie.
13annotators judgment differs from the other twos, the decision is usually still close (e.g., fulfillsvs.partially
fulfills) rather than opposite (e.g., fulfillsvs.does not fulfill ).3
Table 4The inter-annotator agreement on relative evaluations.
All 3 annotators agree 2 of 3 annotators agree No Agreement
Chameleon vs. Gemini+ 331 (31.5%) 609 (58.1%) 108 (10.3%)
Chameleon vs. GPT-4V+ 371 (35.4%) 579 (55.2%) 98 (9.3%)
Chameleon vs. Gemini 317 (30.2%) 621 (59.3%) 110 (10.5%)
Chameleon vs. GPT-4V 300 (28.6%) 611 (58.3%) 137 (13.1%)
For the relative evaluation, Table 4 shows the numbers of cases where all three annotators agree, two annotators
agree, and there is no agreement. For each model pair, we have a bit higher than 10% of the cases where there
is no agreement among the three annotators (considered as a tie in our evaluation.) On about 28% to 35% of
the pairs, all annotators have unanimous judgments, and in about 55% to 60% of the pairs, one annotator
differs from other two. This may be interpreted as Chameleon performing similarly to other baselines in
many cases, making the relative evaluation challenging.4
4.4 Safety Testing
We crowdsource prompts that provoke the model to create unsafe content in predefined categories such as
self-harm, violence and hate, and criminal planning. These prompts cover both text and mixed-modal inputs,
as well as intents to produce unsafe text, images, or mixed-modal outputs. We generate the models response
to each prompt, and ask annotators to label whether the response is safeorunsafewith respect to each
categorys definition of safety; an unsureoption is also provided for borderline responses. Table 5 shows
that an overwhelming majority of Chameleon s responses are considered safe, with only 78 (0.39%) unsafe
responses for the 7B model and 19 (0.095%) for the 30B model.
We also evaluate the models ability to withstand adversarial prompting in an interactive session. For
that purpose, an internal red team probed the 30B model over 445 prompt-response interactions, including
multi-turn interactions. Table 5 shows that of those responses, 7 (1.6%) were considered unsafe and 20 (4.5%)
were labeled as unsure. While further safety tuning using RLHF/RLAIF has been shown to further harden
the model against jailbreaking and intentional malicious attacks, these results demonstrate that our current
safety tuning approach provides significant protection for reasonable, benign usage of this research artifact.
4.5 Discussion
Compared to Gemini and GPT-4V, Chameleon is very competitive when handling prompts that expect
interleaving, mixed-modal responses. The images generated by Chameleon are usually relevant to the context,
making the documents with interleaving text and images very appealing to users. However, readers should
be aware of the limitations of human evaluation. First, the prompts used in the evaluation came from
crowdsourcing instead of real users who interact with a model. While we certainly have a diverse set of
prompts, the coverage may still be limited, given the size of the dataset. Second, partially because our
prompts focus on the mixed-modal output, certain visual understanding tasks, such as OCR or Infographics
(i.e., interpreting a given chart or plot), are naturally excluded from our evaluation. Finally, at this moment,
the APIs of existing multi-modal LLMs provide only textual responses. While we strengthen the baselines by
augmenting their output with separately generated images, it is still preferred if we can compare Chameleon
to other native mixed-modal models.
3For the question of task fulfillment, the inter-rater reliability derived by Krippendorffs Alpha (Krippendorff, 2018; Marzi
et al., 2024) is 0.338; the 95% confidence interval is [0.319 ,0.356], based on bootstrap sampling of 1,000 iterations.
4When comparing Chameleon with Gemini+ and GPT-4V+, the Krippendorffs Alpha values are 0.337 [0 .293 ,0.378]and
0.396 [0 .353 ,0.435], respectively.
14Table 5Safety testing on 20,000 crowdsourced prompts and 445 red team interactions provoking the model to produce
unsafe content.
Dataset Params Safe Unsafe Unsure
Crowdsourced7B 99.2% 0.4% 0.4%
34B 99.7% 0.1% 0.2%
Red Team 34B 93.9% 1.6% 4.5%
5 Benchmark Evaluations
Given the general capabilities of Chameleon , there is not a single model that we can directly evaluate against;
therefore, we evaluate against the best models in every category within our capabilities.
5.1 Text
We evaluate the general text-only capabilities of our pre-trained (not SFTd) model against other state-of-the-
art text-only large language models. We follow the evaluation protocol outlined by Touvron et al. (2023).
Specifically we evaluate all models, using an in-house evaluation platform on the areas of commonsense
reasoning, reading comprehension, math problems, and world knowledge. We report our results in Table 6.
Table 6Comparison of overall performance on collective academic benchmarks against open-source foundational models.
Evaluated using our framework/using API. For GSM8k/MATH, we report maj@1 unless mentioned otherwise.
From Gemini et al. (2023).
Chameleon Llama-2 Mistral Gemini
ProGPT-
4
7B 34B 7B 34B 70B 7B 8x7B  
Commonsense Reasoning and Reading Comprehension
PIQA 79.6 83.3 78.8 81.9 82.8 83.0 83.6  
SIQA 57.0 63.3 48.3 50.9 50.7    
HellaSwag 74.2 82.7 77.2 83.3 85.3 81.3 84.4  
75.6
10-shot85.1
10-shot  87.1
10-shot83.9
10-shot86.7
10-shot84.7
10-shot95.3
10-shot
WinoGrande 70.4 78.5 69.2 76.7 80.2 75.3 77.2  
Arc-E 76.1 84.1 75.2 79.4 80.2 80.0 83.1  
Arc-C 46.5 59.7 45.9 54.5 57.4 55.5 59.7  
OBQA 51.0 54.0 58.6 58.2 60.2    
BoolQ 81.4 86.0 77.4 83.7 85.0 84.7  
Math and World Knowledge
GSM8k 41.6 61.4 14.6 42.2 56.8 52.1
maj@874.4
maj@886.5
maj@32
CoT92.0
SFT
CoT50.9
maj@877.0
maj@32    75.1
maj@32 
MATH 11.5
maj@122.5
maj@12.5 6.24 13.5 13.1
maj@428.4
maj@432.6 52.9
12.9
maj@424.7
maj@4      
MMLU 52.1 65.8 45.3 62.6 68.9 60.1 70.6 71.8 86.4
Commonsense Reasoning and Reading Comprehension: We report 0-shot performance on the following
benchmarks that measure commonsense reasoning and reading comprehension capabilities: PIQA(Bisk
et al., 2020), SIQA(Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al.,
2021), ARC-Easy (Clark et al., 2018), ARC-Challenge (Clark et al., 2018), OpenBookQA (Mihaylov
et al., 2018), and BoolQ(Clark et al., 2019). We score the prompt with each candidate answer and
compute accuracy using the candidate with the highest score. All baseline model performances except a
few are taken directly from the reported sources. We observe that Chameleon-7B andChameleon-34B
15are competitive with the corresponding Llama-2 models, with Chameleon-34B even outperforming
Llama-2 70B on 5/8tasks and performing on par with Mixtral 8x7B.
MATH and World Knowledge We report 8-shot performance on GSM8K (Cobbe et al., 2021) i.e., grade
school math word problems and 4-shot performance on the MATH(Hendrycks et al., 2021) benchmark.
We report maj@N exact match accuracy for both benchmarks by sampling N generations from the model
(greedy sampling for N=1) and choosing the answer via majority voting. Despite training for additional
modalities, both Chameleon models demonstrate strong math capabilities. On GSM8k,Chameleon-7B
outperforms the corresponding Llama-2 models, with performance comparable to Mistral 7B (50.9 vs
52.1 maj@8). Furthermore, Chameleon-34B can outperform Llama2-70B on maj@1 (61.4 vs 56.8) and
Mixtral 8x7B on maj@32 (77.0 vs 75.1). Similarly, on MATH, Chameleon-7B outperforms Llama-2
and matches Mistral 7B on maj@4, while Chameleon-34B outperforms Llama2-70B, approaching the
performance of Mixtral 8x7B on maj@4 (24.7 vs 28.4).
We also report performance on MMLU (Hendrycks et al., 2020), which measures world/in-domain
knowledge and problem-solving abilities using 57 subjects, including elementary mathematics, US
history, computer science, and law. Both Chameleon models outperform their Llama-2 counterparts
with Chameleon-34B approaching the performance of Mixtral 8x7B/Gemini-Pro (65.8 vs 70.6/71.8).
Overall, Chameleon outperformsLLaMa-2acrosstheboard, withperformanceapproachingMistral7B/Mixtral
8x7B (Jiang et al., 2023, 2024) on some tasks. These gains are likely due to multiple factors. First, we do
two epochs over the LLaMa-2 pre-training data, and in general use more compute for pretraining. Second,
including code data significantly improves performance on text-only reasoning tasks. Lastly, having higher
quality data in the last 20% of pre-training significantly improves performance.
5.2 Image-To-Text
We next evaluate Chameleon on the segment of tasks that requires text generation conditioned on an image,
specifically on image captioning and visual question-answering tasks, and present results of Chameleon-34B
in Table 7. Together with our pre-trained model, we also present results with a model fine-tuned on all tasks
together ( Chameleon-34B -MultiTask), as well as models exclusively fine-tuned for the specific evaluation
tasks ( Chameleon-34B -SFT).
We evaluate against available open-source late-fusion models: specifically Flamingo 80B (Alayrac et al., 2022),
IDEFICS 80B (Laurençon et al., 2023), and Llava-1.5 (Liu et al., 2023a), as well as recent closed-source
models, such as Gemini (Gemini et al., 2023) and GPT4-V (OpenAI, 2023). We note that we did not take
any special care when formatting the pre-training data to ensure that 0-shot inference can be effectively done.
Therefore, we augment the input images or questions with the published prompts used by other models. This
was purposefully done to maintain the fidelity of the pre-training data.
Image Captioning: For image captioning evaluations we report CiDER (Vedantam et al., 2015) scores
on the Karpathy test split of MS-COCO (Lin et al., 2014), and the Karpathy test split of Flickr30k
(Plummer et al., 2015) using the pycocoevalcap (Chen et al., 2020) package. For Chameleon models,
we restrict captions to 30tokens. We evaluated GPT-4V and Gemini models using several prompts and
generation lengths via their APIs and report the best performance that we were able to achieve.
In the open-source pre-trained category, Chameleon-34B (2-shot) outperforms the larger 80B models of
both Flamingo and IDEFICS on COCO with 32-shots, while matching their performance on Flickr30k.
With respect to fine-tuned/closed-source models, both multi-task and SFT variants of Chameleon-34B
outperform all other models on COCO, while for Flickr30k, the SFT model outperforms other models
with the multitask model being a close competitor.
Visual Question Answering: For visual question answering (VQA) we report performance on the test-
dev split of VQA-v2 (Goyal et al., 2017). For VQA-v2, the pre-trained Chameleon-34B model with
2-shots matches the 32-shot performance of the larger Flamingo and IDEFICS models, while for fine-
tuned/closed models, Chameleon-34B -Multitask approaches the performance of IDEFICS-80B-Instruct
and Gemini Pro, but trails larger models such as Flamingo-80B-FT, GPT-4V, and Gemini Ultra.
Llava-1.5 outperforms Chameleon-34B on VQAv2 potentially owing to its additional fine-tuning on
16Table 7Model Performances on Image-to-Text Capabilities.Evaluated using API.
Model Model Size COCO Flickr30k VQAv2
Pre-trainedFlamingo-80B 80B 113.8
32-shot75.1
4-shot67.6
32-shot
IDEFICS-80B 80B 116.6
32-shot73.7
4-shot65.9
32-shot
ChameleonChameleon 34B 120.2
2-shot74.7
2-shot66.0
2-shot
Chameleon-SFT 34B 140.8
0-shot82.3
2-shot
Chameleon-MultiTask 34B 139.1
2-shot76.2
2-shot69.6
Fine-tunedFlamingo-80B-FT 80B 138.1  82.0
IDEFICS-80B-Instruct 80B 123.2
32-shot78.4
32-shot68.8
32-shot
Closed Source
(finetuning
status unknown)GPT-4V  78.5
8-shot55.3
8-shot77.2
Gemini Nano 2    67.5
Gemini Pro  99.8
2-shot82.2
4-shot71.2
Gemini Ultra    77.8
conversations from GPT-4, ShareGPT (ShareGPT, 2023), GQA (Hudson and Manning, 2019), and
region-level VQA datasets, but significantly trails behind on the other tasks.
In general, we find Chameleon is fairly competitive on both image captioning and VQA tasks. It rivals other
models by using much fewer in-context training examples and with smaller model sizes, in both pre-trained
and fine-tuned model evaluations.
6 Related Work
Chameleon builds upon the lineage of works exploring token-based approaches for multimodal learning. The
idea of using discrete tokens to represent continuous modalities like images was first explored in works like
BEiT (Bao et al., 2021), which proposed a self-supervised vision representation learning method based on
tokenized image patches. Aghajanyan et al. (2022) extended this idea to learning from mixed-modal documents
through interleaved image and text tokens, allowing for joint reasoning over both modalities within a unified
architecture. CM3Leon (Yu et al., 2023) further scaled up this approach to autoregressive text-to-image
generation, building on the initial proposal of token-based image generation in DALL-E (Ramesh et al., 2021).
As a fully token-based early-fusion model, Chameleon differs from late-fusion approaches like Flamingo
(Alayrac et al., 2022) which encode images and text separately before combining them at a later stage.
Other models like LLaVA (Liu et al., 2023a), IDEFICS (Laurençon et al., 2023), and VisualGPT (Chen
et al., 2022) also maintain separate image and text encoders. In contrast, Chameleons unified token space
allows it to seamlessly reason over and generate interleaved image and text sequences, without the need for
modality-specific components. This early-fusion approach, however, comes with significant challenges in terms
of representation learning and alignment, as discussed in Baltrušaitis et al. (2018).
The most similar model to Chameleon is Gemini (Gemini et al., 2023), which also uses an early-fusion
token-based approach. However, a key difference is that Gemini uses separate image decoders, whereas
Chameleon is an end-to-end dense model without any routing components. This makes Chameleon a more
general-purpose model for both multimodal understanding and generation tasks, similar in spirit to the
Perceiver (Jaegle et al., 2021) architecture which also aims for a unified model across modalities and tasks.
In summary, Chameleon builds on a rich history of work in multimodal learning and token-based architectures,
while pushing the boundaries in terms of model scale and architecture design. By demonstrating strong
performance across a wide range of vision-language tasks and enabling new capabilities in mixed-modal
reasoning and generation, Chameleon represents a significant step towards realizing the vision of general-
17purpose multimodal foundation models.
7 Conclusion
In this paper, we introduced Chameleon , a new family of early-fusion token-based foundation models that
set a new bar for multimodal machine learning. By learning a unified representation space over interleaved
image and text tokens, Chameleon is a single model that achieves strong performance across a wide range of
vision-language benchmarks while enabling new mixed-modal reasoning and generation capabilities.
The key to Chameleon s success is its fully token-based architecture, which allows for seamless information
integration across modalities. By quantizing images into discrete tokens and training on mixed-modal data
from scratch, Chameleon learns to jointly reason over image and text in a way that is impossible with
late-fusion architectures or models that maintain separate encoders for each modality. At the same time,
Chameleon introduces novel techniques for stable and scalable training of early-fusion models, addressing key
optimization and architectural design challenges that have previously limited the scale of such approaches. On
tasks such as image captioning and visual question answering, Chameleon-34B outperforms models such as
Flamingo and IDEFICS, while maintaining competitive performance on text-only benchmarks. Chameleon
also unlocks entirely new possibilities for multimodal interaction, as demonstrated by its strong performance
on our new benchmark for mixed-modal open-ended QA.
Acknowledgements
We thank Naren Briar for her invaluable contribution to manually curating safety prompts, which were crucial
for our safety tuning efforts. We also thank Pierre Fernandez for his indispensable support with the Chameleon
release, Shelly Sheynin for her work on the Chameleon image tokenizer, Puxin Xu and David for helping us
with datasets. Additionally, we thank Mitchell Wortsman for engaging in insightful discussions about stability
in large-scale language models and Mike Lewis for general discussions and advice throughout the project. We
thank Aaron Grattafiori, Firat Ozgenel, Divya Shah, Danny Livshits, Cristian Canton Ferrer, Saghar Hosseini,
Ramon Calderer, Joshua Saxe, Daniel Song and Manish Bhatt for their help with the safety and red teaming
efforts.
Contributors
We attribute credit separated by bucket of work. Additionally,indicates joint first authors,indicates key
contributors,indicates workstream leads, andindicates project leads.
Pre-Training: Srinivasan Iyer, Bernie Huang, Lili Yu, Arun Babu, Chunting Zhou, Kushal Tirumala, Xi
Victoria Lin, Hu Xu, Xian Li, Akshat Shrivastava, Omer Levy, Armen Aghajanyan
Alignment and Safety: Ram Pasunuru, Andrew Cohen, Aram H. Markosyan, Koustuv Sinha, Xiaoqing
Ellen Tan, Ivan Evtimov, Ping Yu, Tianlu Wang, Olga Golovneva, Asli Celikyilmaz
Inference and Evaluation: Pedro Rodriguez, Leonid Shamis, Vasu Sharma, Christine Jou, Karthik Padthe,
Ching-Feng Yeh, Mingda Chen, Bapi Akula, Jacob Kahn, Daniel Li, Scott Yih
Overall Project: Barlas Oguz, Morteza Behrooz, Benjamin Muller, Carleigh Wood, Mary Williamson, Ramya
Raghavendra, Barbara Usher, William Ngan, Nikolay Bashlykov, Lukas Blecher, Sony Theakanath (Lead
PM), Ammar Rizvi (Lead TPM), Gargi Ghosh, Luke Zettlemoyer
18References
Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko,
Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked multimodal model of the internet. arXiv
preprint arXiv:2201.07520 , 2022.
Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller,
Naman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling laws for generative mixed-modal language models. arXiv
preprint arXiv:2301.03728 , 2023.
Agriflanders. Miniatuurpaardjes prijskamp - Agriflanders 2009, 2009. https://en.wikipedia.org/wiki/File:
Miniatuurpaardje.jpg . CC-BY-SA 2.0, https://creativecommons.org/licenses/by/2.0/deed.en .
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in
Neural Information Processing Systems , 35:2371623736, 2022.
Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning: A survey and
taxonomy. IEEE transactions on pattern analysis and machine intelligence , 41(2):423443, 2018.
Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint
arXiv:2106.08254 , 2021.
James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce
Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai.
com/papers/dall-e-3. pdf , 2(3):8, 2023.
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural
language. In Proceedings of the AAAI conference on artificial intelligence , pages 74327439, 2020.
Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed Elhoseiny. Visualgpt: Data-efficient adaptation of pretrained
language models for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 1803018040, 2022.
Xinlei Chen, Hao Fang, Tsung-Yi Lin, and Ramakrishna Vedantam. https://github.com/salaniz/pycocoevalcap , 2020.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,
Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways.
arXiv preprint arXiv:2204.02311 , 2022.
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq:
Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044 , 2019.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457 ,
2018.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,
Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint
arXiv:2110.14168 , 2021.
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter
Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion
parameters. In International Conference on Machine Learning , pages 74807512. PMLR, 2023.
Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based
text-to-image generation with human priors. arXiv preprint arXiv:2203.13131 , 2022.
Gemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan
Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv
preprint arXiv:2312.11805 , 2023.
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter:
Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 69046913, 2017.
19Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring
massive multitask language understanding. In International Conference on Learning Representations , 2020.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob
Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874 , 2021.
Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with
(almost) no human labor, 2022.
Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional
question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages
67006709, 2019.
Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda
Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A general architecture for structured
inputs & outputs. arXiv preprint arXiv:2107.14795 , 2021.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las
Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint
arXiv:2310.06825 , 2023.
Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh
Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint
arXiv:2401.04088 , 2024.
Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Bin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang Lei,
et al. Unified language-vision pretraining with dynamic discrete visual tokenization. arXiv preprint arXiv:2309.04669 ,
2023.
Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open
dataset of user preferences for text-to-image generation, 2023.
Klaus Krippendorff. Content analysis: An introduction to its methodology . Sage publications, 2018.
Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer
for neural text processing. arXiv preprint arXiv:1808.06226 , 2018.
Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang,
Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al. Obelisc: An open web-scale filtered dataset of
interleaved image-text documents. arXiv preprint arXiv:2306.16527 , 2023.
Kevin Lee and Shubho Sengupta. Introducing the ai research supercluster  metas cutting-edge ai supercomputer for
ai research. https://ai.facebook.com/blog/ai-rsc/ , 2022.
Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru
Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza, Luca Wehrstedt, Jeremy Reizenstein, and Grigory
Sizov. xformers: A modular and hackable transformer modelling library. https://github.com/facebookresearch/
xformers , 2022.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision , pages 740755.
Springer, 2014.
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv
preprint arXiv:2310.03744 , 2023a.
HaotianLiu, ChunyuanLi, QingyangWu, andYongJaeLee. Visualinstructiontuning. arXiv preprint arXiv:2304.08485 ,
2023b.
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on
computer vision , pages 1001210022, 2021.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 , 2017.
Giacomo Marzi, Marco Balzano, and Davide Marchiori. K-alpha calculatorkrippendorffs alpha calculator: A user-
friendly tool for computing krippendorffs alpha inter-rater reliability coefficient. MethodsX , 12:102545, 2024. ISSN
202215-0161. doi: https://doi.org/10.1016/j.mex.2023.102545. https://www.sciencedirect.com/science/article/pii/
S2215016123005411 .
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new
dataset for open book question answering. arXiv preprint arXiv:1809.02789 , 2018.
OpenAI. GPTV System Card. https://cdn.openai.com/papers/GPTV_System_Card.pdf , 2023.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performance deep learning library.
InNeurIPS , 2019.
Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.
Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings
of the IEEE international conference on computer vision , pages 26412649, 2015.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092 , 2021.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image
generation with clip latents. arXiv preprint arXiv:2204.06125 , 2022.
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu,
Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 ,
2023.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd
schema challenge at scale. Communications of the ACM , 64(9):99106, 2021.
Mikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram H. Markosyan, Manish Bhatt,
Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, Tim Rocktäschel, and Roberta Raileanu. Rainbow
teaming: Open-ended generation of diverse adversarial prompts, 2024.
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about
social interactions. arXiv preprint arXiv:1904.09728 , 2019.
Rylan Schaeffer. Pretraining on the test set is all you need. arXiv preprint arXiv:2309.08632 , 2023.
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for
training next generation image-text models. arXiv preprint arXiv:2210.08402 , 2022.
Georges Seguin. Mille-feuille, 2010. https://en.wikipedia.org/wiki/File:Mille-feuille_20100916.jpg . CC-BY-SA 3.0,
https://creativecommons.org/licenses/by-sa/3.0/deed.en .
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In
ACL, Berlin, Germany, 2016. https://aclanthology.org/P16-1162 .
ShareGPT. GPTV System Card. https://sharegpt.com/ , 2023.
Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 , 2020.
Maksim Sokolov. Sagrada Familia July 2022, 2022. https://en.wikipedia.org/wiki/File:Sagrada_Familia_%28July_
2022%29_08.jpg . CC-BY-SA-4.0, https://creativecommons.org/licenses/by-sa/4.0/deed.en .
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple
way to prevent neural networks from overfitting. The journal of machine learning research , 15(1):19291958, 2014.
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer
with rotary position embedding. arxiv e-prints, art. arXiv preprint arXiv:2104.09864 , 2021.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.
arXiv preprint arXiv:2307.09288 , 2023.
JarekTuszyński. Americantoad(Bufoamericanus)foundinFairfax, Virginia, 2015. https://en.wikipedia.org/wiki/File:
Miniatuurpaardje.jpg . CC-BY-SA-4.0, https://creativecommons.org/licenses/by-sa/4.0/deed.en .
Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation.
InProceedings of the IEEE conference on computer vision and pattern recognition , pages 45664575, 2015.
21Mitchell Wortsman, Peter J Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John D Co-Reyes, Izzeddin
Gur, Abhishek Kumar, Roman Novak, et al. Small-scale proxies for large-scale transformer training instabilities.
arXiv preprint arXiv:2309.14322 , 2023.
Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang,
Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models: Pretraining and instruction tuning.
arXiv preprint arXiv:2309.02591 , 2023.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your
sentence? arXiv preprint arXiv:1905.07830 , 2019.
Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing
Systems, 32, 2019.
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu,
et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206 , 2023.
22Appendix
A Samples
Prompt: My mom just adopted a dog called a mountain cur. I thought I
knew a lot about dogs but I have never heard of this breed. Can you give
me some basic information as well as show me a few pictures of what they
look like?
Response: Sure! Mountain Cur dogs are a type of hunting dog
that originated in the Appalachian Mountains. They are known for
their intelligence, loyalty, and strong prey drive. They are typically
medium-sized dogs, with a muscular build and a short, easy-to-maintain
coat that comes in a variety of colors including black, brown, tan, red,
and white. Here is an image of a black and white mountain cur dog: <img>
They have a long muzzle, floppy ears, and a bushy tail. They are generally
friendly and outgoing, but can be wary of strangers if not socialized
properly. They are highly energetic dogs that require regular exercise and
mental stimulation to prevent boredom and destructive behavior.
Here is another image of a mountain cur dog in the forest: <img>
Mountain Cur dogs are often used for hunting, particularly for small game
such as squirrels, raccoons, and rabbits. They are also popular family
pets due to their loyal and affectionate nature. However, they do require
regular exercise and training to ensure they remain happy and healthy.
With proper care and attention, Mountain Cur dogs can make wonderful
companions for active families or individuals who enjoy the outdoors.
Prompt: What color is a polar bears fur? Show me a photograph of the
polar bear in the wild.
Response: Polar bears have white fur, which helps them blend in with
their snowy and icy surroundings. This camouflage allows them to
hunt more effectively, as they can sneak up on their prey without being
detected. Additionally, the white fur reflects sunlight, keeping the polar
bear cool in the harsh Arctic environment. Here is an image of a polar
bear in the wild: <img>
23Prompt: <img> Myplantlookslikethis... Isthisaproblem? Ifso, howdoIfixit?
Response: The image you provided shows a pothos plant with yellowing leaves,
which can be caused by a variety of factors, including overwatering, underwater-
ing, lack of nutrients, too much direct sunlight, or pests. Here are some steps
you can take to troubleshoot and potentially fix the issue:
Check the soil: Pothos plants prefer well-draining soil that is neither too
wet nor too dry. If the soil feels soggy or waterlogged, stop watering for a
while. Conversely, if the soil feels dry to the touch, its time to water.
Adjust watering: Overwatering is a common cause of yellowing leaves on
pothos plants. Reduce the frequency of watering and allow the soil to dry
out slightly between waterings.
Provide adequate light: Pothos plants prefer bright, indirect light but can tolerate low light
conditions. If your plant is not getting enough light, move it to a brighter location.
Fertilize: Pothos plants benefit from regular fertilization. Use a balanced, water-soluble fertilizer
and follow the instructions on the label.
Prune: Yellow leaves can be removed to improve the appearance of the plant. Cut back any yellow
leaves to the nearest healthy stem using clean pruning shears.
Check for pests: Inspect the plant for pests such as mealybugs, spider mites, or scale. If present,
treat the plant with insecticidal soap or neem oil according to the products instructions.
Repot: If the plant is pot-bound (meaning the roots have filled the container), consider repotting it
in a larger container with fresh, well-draining soil.
By following these steps, you may be able to identify and address the underlying cause of the yellowing
leaves on your pothos plant.
B Additional Information of Human Evaluations
Table 8Descriptions of the prompt task categories.
Task category Description
How-to Instructions on how to complete a task or accomplish a goal.
Advice Informed problem-solving.
Explanation A more detailed exposition of a topic, e.g., how batteries work, why animals hibernate, or
how to apply rules of composition to photography.
Hypothetical Responses to imaginative, what if questions.
Brainstorming Generating ideas, options, or possibilities.
Reasoning Deducing the answer to a question using commonsense or information provided in the prompt.
Comparison Describes the similarities / differences between multiple things, like products, places, foods,
etc.
Identification Identifying objects in the input image.
Article Asking for the creation of content such as blog posts.
Report Generating a summary of real events.
Story Creating fictional narratives.
Other Other miscellaneous requests.
For the twelve task categories of the prompts we collected for human evaluation, a short description of each
category can be found in Table 8.
The task fulfillment rates, broken down by each task category and modality are shown in Table 9 and Table 10.
Chameleon s win rates, broken down by task category and modality, are shown in Table 11, Table 12, Table 13
and Table 14.
24Table 9Task fulfillment breakdown.
Chameleon Gemini+ GPT-4V+
Task Type Fulfills Partially
fulfillsDoes not
fulfillFulfills Partially
fulfillsDoes not
fulfillFulfills Partially
fulfillsDoes not
fulfill
Advice 69.2% 26.2% 4.7% 42.1% 56.1% 1.9% 43.9% 48.6% 7.5%
Article 59.4% 37.5% 3.1% 40.6% 53.1% 6.3% 62.5% 37.5% 0.0%
Brainstorming 57.9% 36.4% 5.6% 33.3% 61.5% 5.1% 47.7% 47.2% 5.1%
Comparison 60.4% 34.7% 5.0% 47.5% 46.5% 5.9% 43.6% 44.6% 11.9%
Explanation 53.0% 37.7% 9.3% 33.8% 61.6% 4.6% 41.7% 50.3% 7.9%
How-to 52.7% 40.5% 6.9% 43.5% 52.7% 3.8% 48.1% 41.2% 10.7%
Hypothetical 55.9% 39.0% 5.1% 39.0% 47.5% 13.6% 42.4% 44.1% 13.6%
Identification 55.7% 33.0% 11.3% 33.0% 66.0% 1.0% 35.1% 55.7% 9.3%
Other 41.8% 40.0% 18.2% 38.2% 41.8% 20.0% 50.9% 40.0% 9.1%
Reasoning 50.0% 13.6% 36.4% 27.3% 59.1% 13.6% 31.8% 54.5% 13.6%
Report 49.1% 40.4% 10.5% 29.8% 61.4% 8.8% 38.6% 47.4% 14.0%
Story 31.7% 63.4% 4.9% 39.0% 56.1% 4.9% 53.7% 43.9% 2.4%
Gemini GPT-4V
Task Type Fulfills Partially
fulfillsDoes not
fulfillFulfills Partially
fulfillsDoes not
fulfill
Advice 21.5% 70.1% 8.4% 23.4% 75.7% 0.9%
Article 12.5% 84.4% 3.1% 9.4% 90.6% 0.0%
Brainstorming 18.5% 71.8% 9.7% 27.2% 66.7% 6.2%
Comparison 14.9% 76.2% 8.9% 19.8% 72.3% 7.9%
Explanation 15.2% 78.1% 6.6% 19.9% 77.5% 2.6%
How-to 19.8% 74.0% 6.1% 31.3% 67.2% 1.5%
Hypothetical 30.5% 49.2% 20.3% 32.2% 61.0% 6.8%
Identification 18.6% 75.3% 6.2% 22.7% 68.0% 9.3%
Other 14.5% 60.0% 25.5% 18.2% 67.3% 14.5%
Reasoning 9.1% 77.3% 13.6% 13.6% 81.8% 4.5%
Report 12.3% 77.2% 10.5% 22.8% 68.4% 8.8%
Story 9.8% 82.9% 7.3% 7.3% 90.2% 2.4%
Table 10 Modality fulfillment breakdown.
Chameleon Gemini+ GPT-4V+
Fulfills Partially
fulfillsDoes not
fulfillFulfills Partially
fulfillsDoes not
fulfillFulfills Partially
fulfillsDoes not
fulfill
Mixed-modality 55.3% 36.7% 7.9% 39.2% 57.8% 2.9% 42.6% 52.4% 5.0%
Text-only 57.7% 38.4% 4.0% 36.4% 55.5% 8.1% 46.1% 42.7% 11.2%
Gemini GPT-4V
Fulfills Partially
fulfillsDoes not
fulfillFulfills Partially
fulfillsDoes not
fulfill
Mixed-modality 19.7% 76.0% 4.3% 24.3% 72.6% 3.2%
Text-only 18.3% 72.7% 9.1% 23.6% 72.0% 4.4%
25Table 11Complete Win Rates: Chameleon vs. Gemini+.
Wins Ties Loses Win rate
Overall 435 362 251 58.8%
Advice 48 35 24 61.2%
Article 14 14 4 65.6%
Brainstorming 101 60 34 67.2%
Comparison 41 38 22 59.4%
Explanation 65 46 40 58.3%
How-to 53 51 27 59.9%
Hypothetical 17 24 18 49.2%
Identification 39 33 25 57.2%
Other 24 17 14 59.1%
Reasoning 7 8 7 50.0%
Report 16 22 19 47.4%
Story 10 14 17 41.5%
Mixed-modal Prompts 194 145 102 60.4%
Text-only Prompts 241 217 149 57.6%
Table 12 Complete Win Rates: Chameleon vs. GPT-4V+.
Wins Ties Loses Win rate
Overall 375 331 342 51.6%
Advice 54 27 26 63.1%
Article 9 11 12 45.3%
Brainstorming 78 57 60 54.6%
Comparison 35 35 31 52.0%
Explanation 53 56 42 53.6%
How-to 49 46 36 55.0%
Hypothetical 23 19 17 55.1%
Identification 31 26 40 45.4%
Other 16 13 26 40.9%
Reasoning 11 5 6 61.4%
Report 16 21 20 46.5%
Story 0 15 26 18.3%
Mixed-modal Prompts 149 119 173 47.3%
Text-only Prompts 226 212 169 54.7%
26Table 13 Complete Win Rates: Chameleon vs. Gemini.
Wins Ties Loses Win rate
Overall 561 327 160 69.1%
Advice 59 25 23 66.8%
Article 18 11 3 73.4%
Brainstorming 133 42 20 79.0%
Comparison 54 29 18 67.8%
Explanation 78 51 22 68.5%
How-to 65 42 24 65.6%
Hypothetical 27 26 6 67.8%
Identification 45 30 22 61.9%
Other 27 23 5 70.0%
Reasoning 11 6 5 63.6%
Report 30 21 6 71.1%
Story 14 21 6 59.8%
Mixed-modal Prompts 240 123 78 68.4%
Text-only Prompts 321 204 82 69.7%
Table 14 Complete Win Rates: Chameleon vs. GPT-4V.
Wins Ties Loses Win rate
Overall 482 329 237 61.7%
Advice 53 30 24 63.6%
Article 18 9 5 70.3%
Brainstorming 107 53 35 68.5%
Comparison 44 35 22 60.9%
Explanation 75 36 40 61.6%
How-to 51 49 31 57.6%
Hypothetical 20 25 14 55.1%
Identification 40 29 28 56.2%
Other 20 22 13 56.4%
Reasoning 10 6 6 59.1%
Report 25 18 14 59.6%
Story 19 17 5 67.1%
Mixed-modal Prompts 191 125 125 57.5%
Text-only Prompts 291 204 112 64.7%
27
  Jukebox: A Generative Model for Music
Prafulla Dhariwal* 1Heewoo Jun* 1Christine Payne* 1Jong Wook Kim1Alec Radford1Ilya Sutskever1
Abstract
We introduce Jukebox, a model that generates
music with singing in the raw audio domain. We
tackle the long context of raw audio using a multi-
scale VQ-V AE to compress it to discrete codes,
and modeling those using autoregressive Trans-
formers. We show that the combined model at
scale can generate high-ﬁdelity and diverse songs
with coherence up to multiple minutes. We can
condition on artist and genre to steer the musical
and vocal style, and on unaligned lyrics to make
the singing more controllable. We are releasing
thousands of non cherry-picked samples, along
with model weights and code.
1. Introduction
Music is an integral part of human culture, existing from the
earliest periods of human civilization and evolving into a
wide diversity of forms. It evokes a unique human spirit in
its creation, and the question of whether computers can ever
capture this creative process has fascinated computer scien-
tists for decades. We have had algorithms generating piano
sheet music (Hiller Jr & Isaacson, 1957; Moorer, 1972;
Hadjeres et al., 2017; Huang et al., 2017), digital vocoders
generating a singers voice (Bonada & Serra, 2007; Saino
et al., 2006; Blaauw & Bonada, 2017) and also synthesizers
producing timbres for various musical instruments (Engel
et al., 2017; 2019). Each captures a speciﬁc aspect of music
generation: melody, composition, timbre, and the human
voice singing. However, a single system to do it all remains
elusive.
The ﬁeld of generative models has made tremendous
progress in the last few years. One of the aims of gen-
erative modeling is to capture the salient aspects of the data
and to generate new instances indistinguishable from the
true data The hypothesis is that by learning to produce the
data we can learn the best features of the data1. We are
surrounded by highly complex distributions in the visual,
audio, and text domain, and in recent years we have devel-
*Equal contribution1OpenAI, San Francisco. Correspondence
to: <jukebox@openai.com>.oped advances in text generation (Radford et al.), speech
generation (Xie et al., 2017) and image generation (Brock
et al., 2019; Razavi et al., 2019). The rate of progress in
this ﬁeld has been rapid, where only a few years ago we
had algorithms producing blurry faces (Kingma & Welling,
2014; Goodfellow et al., 2014) but now we now can gener-
ate high-resolution faces indistinguishable from real ones
(Zhang et al., 2019b).
Generative models have been applied to the music genera-
tion task too. Earlier models generated music symbolically
in the form of a pianoroll, which speciﬁes the timing, pitch,
velocity, and instrument of each note to be played. (Yang
et al., 2017; Dong et al., 2018; Huang et al., 2019a; Payne,
2019; Roberts et al., 2018; Wu et al., 2019). The symbolic
approach makes the modeling problem easier by working
on the problem in the lower-dimensional space. However, it
constrains the music that can be generated to being a speciﬁc
sequence of notes and a ﬁxed set of instruments to render
with. In parallel, researchers have been pursuing the non-
symbolic approach, where they try to produce music directly
as a piece of audio. This makes the problem more challeng-
ing, as the space of raw audio is extremely high dimensional
with a high amount of information content to model. There
has been some success, with models producing piano pieces
either in the raw audio domain (Oord et al., 2016; Mehri
et al., 2017; Yamamoto et al., 2020) or in the spectrogram
domain (Vasquez & Lewis, 2019). The key bottleneck is
that modeling the raw audio directly introduces extremely
long-range dependencies, making it computationally chal-
lenging to learn the high-level semantics of music. A way to
reduce the difﬁculty is to learn a lower-dimensional encod-
ing of the audio with the goal of losing the less important
information but retaining most of the musical information.
This approach has demonstrated some success in generat-
ing short instrumental pieces restricted to a set of a few
instruments (Oord et al., 2017; Dieleman et al., 2018).
In this work, we show that we can use state-of-the-art deep
generative models to produce a single system capable of gen-
erating diverse high-ﬁdelity music in the raw audio domain,
with long-range coherence spanning multiple minutes. Our
approach uses a hierarchical VQ-V AE architecture (Razavi
1Richard Feynmann famously said, What I cannot create, I
do not understandarXiv:2005.00341v1  [eess.AS]  30 Apr 2020Jukebox: A Generative Model for Music
et al., 2019) to compress audio into a discrete space, with
a loss function designed to retain the maximum amount of
musical information, while doing so at increasing levels
of compression. We use an autoregressive Sparse Trans-
former (Child et al., 2019; Vaswani et al., 2017) trained with
maximum-likelihood estimation over this compressed space,
and also train autoregressive upsamplers to recreate the lost
information at each level of compression.
We show that our models can produce songs from highly
diverse genres of music like rock, hip-hop, and jazz. They
can capture melody, rhythm, long-range composition, and
timbres for a wide variety of instruments, as well as the
styles and voices of singers to be produced with the mu-
sic. We can also generate novel completions of existing
songs. Our approach allows the option to inﬂuence the
generation process: by swapping the top prior with a con-
ditional prior, we can condition on lyrics to tell the singer
what to sing, or on midi to control the composition. We
release our model weights and training and sampling code
at https://github.com/openai/jukebox.
2. Background
We consider music in the raw audio domain represented as
a continuous waveform x2[1;1]T, where the number
of samplesTis the product of the audio duration and the
sampling rate typically ranging from 16 kHz to 48 kHz. For
music, CD quality audio, 44.1 kHz samples stored in 16
bit precision, is typically enough to capture the range of
frequencies perceptible to humans. As an example, a four-
minute-long audio segment will have an input length of 10
million, where each position can have 16 bits of information.
In comparison, a high-resolution RGB image with 1024
1024 pixels has an input length of 3million, and each
position has 24 bits of information. This makes learning
a generative model for music extremely computationally
demanding with increasingly longer durations; we have to
capture a wide range of musical structures from timbre to
global coherence while simultaneously modeling a large
amount of diversity.
2.1. VQ-V AE
To make this task feasible, we use the VQ-V AE (Oord et al.,
2017; Dieleman et al., 2018; Razavi et al., 2019) to compress
raw audio to a lower-dimensional space. A one-dimensional
VQ-V AE learns to encode an input sequence x=hxtiT
t=1
using a sequence of discrete tokens z=hzs2[K]iS
s=1,
whereKdenotes the vocabulary size and we call the ratio
T=S the hop length. It consists of an encoder E(x)which
encodes xinto a sequence of latent vectors h=hhsiS
s=1,
a bottleneck that quantizes hs7!ezsby mapping each hs
to its nearest vector ezsfrom a codebook C=fekgK
k=1,
and a decoder D(e)that decodes the embedding vectorsback to the input space. It is thus an auto-encoder with a
discretization bottleneck. The VQ-V AE is trained using the
following objective:
L=Lrecons +Lcodebook +Lcommit (1)
Lrecons =1
TP
tkxtD(ezt)k2
2 (2)
Lcodebook =1
SP
sksg [hs]ezsk2
2 (3)
Lcommit =1
SP
skhssg [ezs]k2
2 (4)
where sgdenotes the stop-gradient operation, which passes
zero gradient during backpropagation. The reconstruction
lossLrecons penalizes for the distance between the input x
and the reconstructed output bx=D(ez), andLcodebook pe-
nalizes the codebook for the distance between the encodings
hand their nearest neighbors ezfrom the codebook. To
stabilize the encoder, we also add Lcommit to prevent the
encodings from ﬂuctuating too much, where the weight 
controls the amount of contribution of this loss. To speed up
training, the codebook loss Lcodebook instead uses EMA up-
dates over the codebook variables. Razavi et al. (2019)
extends this to a hierarchical model where they train a sin-
gle encoder and decoder but break up the latent sequence
hinto a multi-level representation [h(1);;h(L)]with de-
creasing sequence lengths, each learning its own codebook
C(l). They use non-autoregressive encoder-decoders and
jointly train all levels with a simple mean-squared loss.
3. Music VQ-V AE
Inspired by the results from the hierarchical VQ-V AE model
(Razavi et al., 2019) for images, we consider applying the
same technique to model raw audio using three different
levels of abstraction, as illustrated in Figure 1. At each level,
we use residual networks consisting of WaveNet-style non-
causal 1-D dilated convolutions, interleaved with downsam-
pling and upsampling 1-D convolutions to match different
hop lengths. A detailed description of the architecture is
provided in Appendix B.1. We make a number of modiﬁca-
tions to our VQ-V AE compared to the ones in (Oord et al.,
2017; Razavi et al., 2019), as described in the following
subsections.
3.1. Random restarts for embeddings
VQ-V AEs are known to suffer from codebook collapse,
wherein all encodings get mapped to a single or few em-
bedding vectors while the other embedding vectors in the
codebook are not used, reducing the information capacity
of the bottleneck. To prevent this, we use random restarts:
when the mean usage of a codebook vector falls below a
threshold, we randomly reset it to one of the encoder out-
puts from the current batch. This ensures all vectors in theJukebox: A Generative Model for Music
V ector
QuantizationV ector
QuantizationV ector
Quantization
EncodeEncode
Encode
ht	=	E ( xt ) xt zt	=	 ar gmink	ǁ	 ht		 ek 	ǁ
DecodeDecodeDecode
ez t x t	=	D ( ez t )
Codebook
LookupCodebook
LookupCodebook
LookupCodebook  ek
Figure 1. We ﬁrst train three separate VQ-V AE models with different temporal resolutions. At each level, the input audio is segmented
and encoded into latent vectors ht, which are then quantized to the closest codebook vectors ezt. The code ztis a discrete representation
of the audio that we later train our prior on. The decoder takes the sequence of codebook vectors and reconstructs the audio. The top
level learns the highest degree of abstraction, since it is encoding longer audio per token while keeping the codebook size the same.
Audio can be reconstructed using the codes at any one of the abstraction levels, where the least abstract bottom-level codes result in the
highest-quality audio, as shown in Figure 4. For the detailed structure of each component, see Figure 7.
codebook are being used and thus have a gradient to learn
from, mitigating codebook collapse.
3.2. Separated Autoencoders
When using the hierarchical VQ-V AE from (Razavi et al.,
2019) for raw audio, we observed that the bottlenecked top
level is utilized very little and sometimes experiences a com-
plete collapse, as the model decides to pass all information
through the less bottlenecked lower levels. To maximize
the amount of information stored at each level, we simply
train separate autoencoders with varying hop lengths. Dis-
crete codes from each level can be treated as independent
encodings of the input at different levels of compression.
3.3. Spectral Loss
When using only the sample-level reconstruction loss, the
model learns to reconstruct low frequencies only. To capture
mid-to-high frequencies, we add a spectral loss which is
deﬁned as
Lspec=kjSTFT (x)jjSTFT (bx)jk2
It encourages the model to match the spectral components
without paying attention to phase which is more difﬁcult
to learn. This is similar to the use of power loss (Oord
et al., 2018) and spectral convergence (Arık et al., 2018b)
when training parallel decoders for raw audio. One differ-
ence between the latter approach and ours is that we are no
longer optimizing the spectral signal-to-noise ratio; dividing
by the magnitude of the signal results in numerical insta-
bility for mostly silent inputs. To prevent the model from
overﬁtting to a particular choice of the STFT parameters,we use the sum of the spectral losses Lspeccalculated over
multiple STFT parameters that trade-off time and frequency
resolutions (Yamamoto et al., 2020).
4. Music Priors and Upsamplers
After training the VQ-V AE, we need to learn a prior p(z)
over the compressed space to generate samples. We break
up the prior model as
p(z) =p(ztop;zmiddle;zbottom) (5)
=p(ztop)p(zmiddlejztop)p(zbottomjzmiddle;ztop)(6)
and train separate models for the top-level prior p(ztop), and
upsamplers p(zmiddlejztop)andp(zbottomjzmiddle;ztop). Each
of these is an autoregressive modeling problem in the dis-
crete token space produced by the VQ-V AE. We use Trans-
formers with sparse attention (Vaswani et al., 2017; Child
et al., 2019) as they are currently the SOTA in autoregressive
modeling. We propose a simpliﬁed version which we call
the Scalable Transformer, that is easier to implement and
scale (see Appendix A for details).
For the upsamplers, we need to provide the autoregres-
sive Transformers with conditioning information from the
codes of the upper levels. To do so, we use a deep resid-
ual WaveNet (Xie et al., 2017) followed by an upsampling
strided convolution and a layer norm (Ba et al., 2016), and
add the output as extra positional information to the embed-
dings of the current level. We condition the lower levels
only on the chunk of upper level codes that correspond to
the same segment of raw audio.Jukebox: A Generative Model for Music
At each level, we use Transformers over the same context
length of discrete codes, which correspond to increasing
the raw audio length with larger hop lengths, and modeling
longer temporal dependencies at the higher levels while
keeping the same computational footprint for training each
level. As our VQ-V AE is convolutional, we can use the
same VQ-V AE to produce codes for arbitrary lengths of
audio.
4.1. Artist, Genre, and Timing Conditioning
Our generative model can be made more controllable by
providing additional conditioning signals while training. For
our ﬁrst models, we provide artist and genre labels for the
songs. This has two advantages: ﬁrst, it reduces the entropy
of the audio prediction, so the model is able to achieve
better quality in any particular style. Second, at generation
time, we are able to steer the model to generate in a style
of our choosing. Additionally, we attach a timing signal
for each segment at training time. This signal includes the
total duration of the piece, the start time of that particular
sample and how much fraction of the song that has elapsed.
This allows the model to learn audio patterns that depend
on the overall structure, such as spoken or instrumental
introductions and applause at the end of a piece.
4.2. Lyrics Conditioning
While the conditioned models above are able to generate
songs of diverse genres and artistic styles, singing voices
generated by those models, while often sung in a compelling
melody, are mostly composed of babbling, rarely producing
recognizable English words. In order to be able to control
the generative model with lyrics, we provide more context
at training time by conditioning the model on the lyrics
corresponding to each audio segment, allowing the model
to produce singing simultaneosly with the music.
Lyrics-to-singing (LTS) task : The conditioning signal
only includes the text of the lyrics, without timing or vocal-
isation information. We thus have to model the temporal
alignment of lyrics and singing, the artists voice and also
the diversity of ways one can sing a phrase depending on the
pitch, melody, rhythm and even genre of the song. The con-
ditioning data isnt precise as the lyrics data often contains
textual references to repeated sections like chorus or mis-
matching portions of lyrics with the corresponding music.
There is also no separation between lead vocals, accompa-
nying vocals and the background music in target audio. This
makes the Lyrics-to-singing (LTS) task signiﬁcantly more
challenging than the corresponding Text-to-speech (TTS)
task.
Providing lyrics for chunks of audio : Our dataset includes
song-level lyrics, but to make the task easier we train on
shorter (24 sec) chunks of audio. To provide the lyrics cor-
Middle Upsampler
Bottom Upsampler
VQ-V AE DecoderTop-Level Prior
Conditioning
Information(a)Ancestral sampling : Priors for the VQ-V AE codes are trained
using a cascade of Transformer models, shown in blue. Each model
takes conditioning information such as genre, artist, timing, and
lyrics, and the upsampler models are also conditioned on the codes
from the upper levels. To generate music, the VQ-V AE codes are
sampled from top to bottom using the conditioning information
for control, after which the VQ-V AE decoder can convert the
bottom-level codes to audio.
time
new samples
(b)Windowed sampling : To generate music longer than the
models context length (12 in this ﬁgure), we repeatedly sample
continuations at each level, using overlapping windows of previous
codes as the context. The overlap amount is a hyperparameter, and
the ﬁgure shows an example of 75% overlap with hop length 3.
Primed Audio Generated AudioEncodeGenerateDecode
(c)Primed sampling : The model can generate continuations of
an existing audio signal by converting it into the VQ-V AE codes
and sampling the subsequent codes in each level.
Figure 2. Sampling methods for generating musicJukebox: A Generative Model for Music
responding to the audio during training, we began with a
simple heuristics of aligning the characters of the lyrics to
linearly span the duration of each song, and pass a ﬁxed-side
window of characters centered around the current segment
during training. While this simple strategy of linear align-
ment worked surprisingly well, we found that it fails for
certain genres such as hip-hop with fast lyrics. To address
this, we use Spleeter (Hennequin et al., 2019) to extract vo-
cals from each song and run NUS AutoLyricsAlign (Gupta
et al., 2020) on the extracted vocals to obtain a word-level
alignments of the lyrics, allowing us to more accurately
provide the lyrics for a given chunk of audio. We choose a
large enough window so that the actual lyrics have a high
probability of being inside the window.
Encoder-decoder model : We use an encoder-decoder style
model to condition on the characters of the lyrics, with
the encoder producing features from the lyrics which are
attended to by the decoder which produces the top level
music tokens. The lyrics encoder is a Transformer with an
autoregressive modeling loss for lyrics, and its last level is
used as features of the lyrics. In the music decoder, we inter-
leave a few additional layers with encoder-decoder attention
where the queries from the music tokens are only allowed
to attend to keys and values from the lyrics tokens. These
layers attend on the activation from the last layer of the
lyrics encoder (see Figure 8c). In Figure 3, we see that the
attention pattern learned by one of these layers corresponds
to the alignment between the lyrics and the singing.
4.3. Decoder Pretraining
To reduce computation required to train the lyrics condi-
tional model, we use a pretrained unconditional top-level
prior as our decoder and introduce the lyrics encoder using
model surgery (Berner et al., 2019). We initialize the output
projection weights in the MLP and the attention layers of
these residual blocks to zeros (Zhang et al., 2019a), so that
the added layers perform the identity function at initializa-
tion. Thus, at initialization the model behaves identically
as the pretrained decoder, but there is still a gradient with
respect to the encoder state and parameters2, allowing the
model to learn to use the encoder.
4.4. Sampling
After we have trained our VQ-V AE, upsamplers, and top
level priors, we can then use them to sample novel songs.
Ancestral sampling : We ﬁrst generate the top level codes
one token at a time by the usual ancestral sampling process
(see Figure 2a): generating the ﬁrst token, then passing all
2The gradient also needs to break symmetry with the encoder
output features, which is the case here since the weights of the
input projections in the attention are not zero.
0 1600 3200 4800 6400 8000
Music token position0100200300400500Lyrics token position
0.00.20.40.60.81.0Figure 3. Lyrics-singing alignment learned by one of the encoder-
decoder attention layers. The x-axis is the position of music
queries, and the y-axis is the position of lyric keys. The positions
attended to by the decoder correspond to the characters being sung.
previously generated tokens into the model as inputs and
outputting the next token conditioned on all previous tokens.
We then run our conditioning wavenet on the top level codes
to produce the conditioning information for the middle level
and sample ancestrally from it too, and do the same for the
bottom level.
Windowed sampling : To sample segments longer than the
context length, we use windowed sampling, where we move
ahead our sampling window by half our context and con-
tinue sampling conditioned on this half context (See Figure
2b). We can trade off speed for quality by using a smaller
hop length here.
Primed sampling : Instead of sampling the entire token
sequence from the model, we can also run a forward pass
of the VQ-V AE to obtain the top, middle, and bottom level
codes corresponding to a segment from an actual song, as
shown in Figure 2c. We can use these as the initial tokens in
our ancestral sampling process and continue sampling from
these to produce novel completions of the song.
5. Experiments
5.1. Dataset
We scraped a new dataset of 1.2 million songs (600k of
which in English), paired with the lyrics and metadata from
LyricWiki (LyricWiki). The metadata includes artist, album,
genre, and year of the release, along with common moods or
playlist keywords associated with each song. We train on 32
bit, 44.1 kHz raw audio and perform data augmentation by
randomly downmixing the right and left channels to produce
mono channel audio.
5.2. Training Details
For the music VQ-V AE, we use 3 levels of bottlenecks com-
pressing 44 kHz audio in dimensionality by 8x, 32x, andJukebox: A Generative Model for Music
128x respectively, with a codebook size of 2048 for each
level. The VQ-V AE has 2 million parameters and is trained
on 9-second audio clips on 256 V100 for 3 days. We used
exponential moving average to update the codebook fol-
lowing Razavi et al. (2019). For our prior and upsampler
models, we use a context of 8192 tokens of VQ-V AE codes,
which corresponds to approximately 24, 6, and 1.5 seconds
of raw audio at the top, middle, and bottom level, respec-
tively. The upsamplers have one billion parameters and are
trained on 128 V100s for 2 weeks, and the top-level prior
has 5 billion parameters and is trained on 512 V100s for 4
weeks. We use Adam with learning rate 0:00015 and weight
decay of 0:002. For lyrics conditioning, we reuse the prior
and add a small encoder, after which we train the model on
512 V100s for 2 weeks. The detailed hyperparameters for
our models and training are provided in Appendix B.3.
5.3. Samples
We trained a sequence of models with increasing sample
quality. Our ﬁrst model was trained on the MAESTRO
dataset using 22 kHz VQ-V AE codes and relatively small
prior models. We observed that this could generate high
ﬁdelity classical music samples with piano and occasional
violin. We then collected a larger and more diverse dataset
of songs with genre and artist labels. The same model when
trained on this new dataset was able to produce diverse sam-
ples other than classical music, and demonstrated musicality
and coherence over more than a minute.
Despite the novelty of being able to generate generally high
ﬁdelity and coherent songs, sample quality was still limited
by a number of factors. First, the use of 22 kHz sampling
rate along with small upsamplers introduced noise both in
the upsampling and decoding steps, which we hear as grainy
texture. We improved ﬁdelity by using 44 kHz VQ-V AE
and 1B parameter upsamplers in all subsequent experiments
at the expense of longer rendering time.
Second, the 1B top-level prior was not big enough to pro-
duce singing and diverse musical timbres. We ﬁrst explored
increasing the model size to 5 billion parameters. Larger
capacity allowed better modeling of the broader distribu-
tion of songs, resulting in samples with better musicality,
longer coherence and initial singing. While there is an over-
all qualitative improvement, the unconditional model still
struggled to sing recognizable words. Training a seq2seq
model with lyric conditioning and limiting the dataset only
to songs primarily in English made singing both intelligible
and controllable.
The ﬁnal model, which we call Jukebox, uses all these
improvements. Because everyone experiences music dif-
ferently, it is generally tricky and not very meaningful to
evaluate samples by the mean opinion score or FID-like
metrics. We manually evaluate coherence, musicality, diver-sity, and novelty of generated samples. The links to curated
examples are embedded in text.
Coherence: We ﬁnd the samples stay very coherent musi-
cally through the context length of the top-level prior (ap-
proximately 24 seconds), and they maintain similar har-
monies and textures as we slide the window to generate
longer samples. However, because the top-level does not
have the context of the entire song, we do not hear long
term musical patterns, and we would never hear choruses or
melodies that repeat.
The generations progress through beginnings of songs (for
example applause or slow instrumental warm-ups), through
sections that sound chorus-like, through instrumental inter-
ludes, and then fading or otherwise wrapping up at the end.
The top-level prior always knows what fraction of the song
is complete time-wise, so it is able to imitate appropriate
beginnings, middles and ends.
Musicality: The samples frequently imitate familiar mu-
sical harmonies and the lyrics are usually set in ways that
are very natural. Frequently the highest or longest notes of
the melody match words that a human singer would choose
to emphasize, and the lyrics are almost always rendered
in ways that capture the prosody of the phrases. This is
noticeable in hip hop generations, where the model reliably
captures the rhythm of spoken text. We do ﬁnd that the
generated melodies are usually less interesting than human
composed melodies. In particular, we do not hear the an-
tecedent and consequent pattern familiar to many human
melodies, and we rarely hear choruses that are melodically
memorable.
Diversity: Likelihood training encourages covering of all
modes, so we expect the model to produce diverse samples.
 Re-renditions: We generate multiple samples conditioned
on artist and lyrics combinations that exist in our training
data. While occasionally drum and bass lines or melodic
intervals echo the original versions, we ﬁnd that none of
the generated samples is noticeably similar to the original
songs.
We also generate multiple songs conditioned on the same
artist and lyrics as Sample 1 to obtain Samples 912. All ﬁve
sound interesting in their own ways with different moods
and melodies with Sample 10 playing a harmonic at 00:14
as part of a blues riff, showing that the model has learned a
wide range of singing and playing styles.
 Completions: We prime the model with 12 seconds of
existing songs and ask it to complete them in the same
styles. When the priming samples include singing, the con-
tinuations are more likely to imitate the original tunes and
rhythms. Songs primed with more generic or common intros
tend to be more diverse. Even generated samples that areJukebox: A Generative Model for Music
close to the originals early on deviate completely into new
musical material after about 30 seconds.
Re-renditions and completions are interesting and diverse,
but overall, there is still room for improvement in music
quality compared to the original songs.
 Full tree: To understand diversity in a more systematic
way, we generate multiple continuations from the same seg-
ment. We start with a one-minute sample and independently
sample four times per one-minute extension. By the three
minute mark, there are 16 completions. We can think of this
branching tree as exploring different possibilities obtained
by ancestral sampling. In the generated songs in the link,
we hear diversity in singing and development even when the
same initial segment is used. We note that this particular
sample follows the lyrics more successfully than many. For
certain genres like hip hop and rap, where linearly moving
the window does not yield good lyrics alignment, the chance
of obtaining plausible singing is lower.
Novelty: With the ability to condition on various styles,
lyrics, and raw audio, we would like Jukebox to be a useful
tool for both professional musicians and music enthusiasts
alike. In this section, we are interested in exploring capabil-
ities and applications of Jukebox.
 Novel styles: We generate songs in an unusual genre typi-
cally not associated with an artist. In general, we ﬁnd that
it is fairly difﬁcult to generalize to a novel style of singing
while using the same voice as the artist embedding overpow-
ers other information. In Joe Bonamassa and Frank Sinatra
samples, we hear a modest variation in instrumentation,
energy, and ambience depending on the genre embedding.
However, our attempts to mix country singer Alan Jackson
with unusual genres like hip hop and punk did not seem to
move the samples away from a country style in meaningful
ways.
 Novel voices: We pick artists whose voices are reproduced
reasonably well by the model, and interpolate their style
embeddings to synthesize new voices. Some blending, for
instance, between Frank Sinatra and Alan Jackson in Sample
4, still sounds similar to Frank Sinatra. In most cases, the
model renders in a vaguely recognizable but distinct voice
that preserves different vocal attributes. Samples 1 and
2 conditioned on the Céline Dion embeddings divided by
two have slightly different timbre and tone but capture her
unique vibrato.
We also experiment with changing the style embedding in
the middle of a song to create a duet (Sample 7). This is
another way of guiding generation during sampling. Con-
tinuing in another voice works best when the segment ends
in an interlude; otherwise, the model blends voices in the
middle of a word or a sentence. Novel lyrics: We ask Jukebox to sing poems and novel
verses generated by GPT-2 (Radford et al.) to demonstrate
that it can indeed sing new lyrics. While the training data
consists of song lyrics with limited vocabulary and con-
strained structure, the model has learned to follow along
most prompts and sing even new words that are reasonably
pronounceable (including technical terms from the deep
learning literature). To get the best results, however, we ﬁnd
that it is useful to spell out difﬁcult words or acronyms as
they are spoken. The generations are noticeably higher qual-
ity if the text matches the distribution of lyrics for the given
artist, both in terms of length, and of rhyming or rhythmic
qualities. For example, hip hop lyrics tend to be longer than
most other genres, and the commonly emphasized syllables
easily form clear rhythms.
 Novel riffs: Another useful application of Jukebox is the
ability to record an incomplete idea and explore various
continuations without ever needing to tabulate in symbolic
representations, which would lose details of timbre and
mood. We curate recordings of novel riffs by our in-house
musicians and prime the model during sampling. Sample 6
starts with a musical style not widely used in Elton Johns
songs. The model still carries out the tune and develops
it further. Similarly, the beginning of Sample 1 is a pro-
gressive jazz piece with a 5/4 polymeter, which has never
been used in hip hop. Despite this novelty, the rhythm per-
sists throughout the song and is incorporated naturally with
rapping.
5.4. VQ-V AE Ablations
Spectral convergence (dB)
Level Hop length Without restart With restart
Bottom 8 21:123:0
Middle 32 12:412:4
Top 128 8:38:3
Table 1. Reconstruction ﬁdelity degrades with higher compression.
Restarting dead codes near random encoder outputs mitigates learn-
ing suboptimal codes.
Codebook size Spectral convergence (dB)
256 15:9
2048 23:0
No quantization 40:5
Table 2. Bottom-level VQ-V AE reconstruction results with differ-
ent codebook sizes. Using larger codebooks helps reconstruction
because it allows more information to be encoded at the bottleneck
layers. Removing the bottleneck entirely yields almost perfect
reconstruction.Jukebox: A Generative Model for Music
5001k2k4k8k16k
5001k2k4k8k16k
5001k2k4k8k16k
5001k2k4k8k16k
5001k2k4k8k16k
5001k2k4k8k16k
Figure 4. Comparison of reconstructions from different VQ-V AEs, x-axis is time and y-axis is frequency. The columns from left to
right are bottom-, middle-, and top-level reconstructions at hop lengths 8, 32, and 128 respectively, visualized as Mel spectrograms.
The ﬁrst row is the ground-truth, and the second row shows the spectrograms of audio outputs from our VQ-V AE. In the third row, we
remove the spectral loss, and see that the middle and top level lose high-frequency information. In the fourth row, we use a hierarchical
VQ-V AE (Razavi et al., 2019) instead of separate auto-encoders (Figure 1), and we see the middle and top levels are not used for encoding
pertinent information. Finally, the ﬁfth row shows a baseline with the Opus codec that encodes audio at constant bitrates comparable to
our VQ-V AE. It also fails to capture higher frequencies and adds noticeable artifacts at the highest level of compression.
Ablation Spectral convergence (dB)
None 8:3
Without spectral loss 6:3
With single autoencoder 2:9
Table 3. Top-level codes are generally difﬁcult to train well without
spectral loss or with a single hierarchical autoencoder. Resulting
reconstructions may lose some to most of information.
We compare raw audio VQ-V AEs when trained with varying
compression ratios, objectives, and architectures. As we
use nonautoregressive decoders with continuous represen-
tation for output, we report spectral convergence (Sturmel& Daudet, 2011), which measures the amount of spectral
error relative to signal, as test error and proxy for reconstruc-
tion ﬁdelity. We evaluate on 5000 held-out 3-second audio
segments and report the average in decibels. All models in
this section are trained with a batch size of 32, 3-second
audio clips sampled at 44 kHz. As before, we use hop
lengths of 8, 32, and 128 for the bottom, middle and top
level respectively.
In Table 1, we see that increasing the hop size results in
higher reconstruction error. Figure 4 indeed shows that a
signiﬁcant amount of information, especially higher frequen-
cies, is missing at middle and top levels across all ablations
we ran. This is expected as audio is compressed more withJukebox: A Generative Model for Music
0 100k 200k 300k 400k 500k
Number of training steps8910Codebook entropy (bits)with restart
without restart
Figure 5. Entropy of codebook with 2048 codes, i.e 11 bits, over
training. Reviving dead codes near random encoder outputs en-
sures good codebook utilization from the start of training.
larger hop sizes. To mitigate codebook collapse, we restart
dead codes near random encoder embeddings. In Figure 5,
we see that this yields higher codebook usage even from
early on in training. Models trained without random restarts
can converge to the same test error and codebook usage but
require more training steps. With poor initialization, these
models sometimes end up with suboptimal codes hurting
reconstruction ﬁdelity.
Codebook size also matters, as it sets a limit on channel ca-
pacity through the bottleneck layers. In Table 2, we ﬁnd that
reconstruction error increases considerably when the code-
book size is reduced from 2048 to 256. We also compare
with a model that uses continuous representations without
vector quantization. We can think of this model as using a
vastly large codebook with all encoder embeddings. This
achieves almost perfect reconstruction with negligible spec-
tral error.
When the model is trained with L2 loss only, reconstruc-
tions tend to sound muddy from missing high frequencies,
and this problem is exacerbated as hop size is increased. In
Figure 4, we see that top-level codes trained without spec-
tral loss do not capture much information beyond 2 kHz,
and obtain worse reconstructions (Table 3). However, we
observe that while spectral loss helps encode more infor-
mation, it also adds distortion artifacts which we hear as
scratchy noise.
Lastly, we train a raw audio hierarchical VQ-V AE (Razavi
et al., 2019) and ﬁnd that it is generally difﬁcult to push
information to higher levels. This model is trained twice as
long as the previous models, but middle and top-level recon-
structions as shown in Figure 4 are not capturing much. It is
possible that higher level codes may have collapsed before
bottom level starts to reconstruct the audio well. Making
the bottom layers explicitly model residuals pushed more
information to the top. But, we found separate autoencoders
to be cleaner and more effective.6. Related Work
Generative modeling in deep learning: Generative mod-
els aim to learn the distribution of data by either explicitly
by modeling the distribution or implicitly by constructing
means to sample from it (Goodfellow, 2016). Modeling
the interdependency within high-dimensional data was tra-
ditionally considered extremely difﬁcult, but starting with
Deep Boltzmann Machines (Salakhutdinov & Hinton, 2009),
various kinds of deep generative models have been intro-
duced. Generative Adversarial Networks (GANs) (Good-
fellow et al., 2014) use generator and discriminator net-
works that contest each other to make the generated samples
as indistinguishable as possible from the data, and they
are renowned for their ability to generate high-quality pic-
tures (Zhang et al., 2019b; Brock et al., 2019). Autoregres-
sive generative models such as NADE (Uria et al., 2016),
PixelCNN (Van den Oord et al., 2016), and Transformers
(Vaswani et al., 2017) use the chain rule of probability to
factorize the joint distribution of data into a product of
simpler distributions, and ﬂow-based models (Dinh et al.,
2015; 2017; Rezende & Mohamed, 2015; Kingma & Dhari-
wal, 2018) learn a series of invertible transformations that
maps the data distribution with a simpler one such as a
Gaussian distribution. Autoregressive ﬂows (Papamakarios
et al., 2017; Kingma et al., 2016) combine the two ideas to
achieve faster density estimation or data generation. Varia-
tional autoencoders (V AEs) (Rezende et al., 2014; Kingma
& Welling, 2014) impose a Gaussian prior on the latent
code in an encoder-decoder setup from which data can be
sampled.
Generative models for music: Generative modeling of
symbolic music dates back to more than half a century, when
Hiller Jr & Isaacson (1957) introduced the ﬁrst computer-
generated music based on Markov chains. There exists
a variety of earlier approaches using rule-based systems
(Moorer, 1972), chaos and self-similarity (Pressing, 1988),
cellular automata (Beyls, 1989), concatenative synthesis
(Jehan, 2005), and constraint programming (Anders & Mi-
randa, 2011). More recent data-driven approaches include
DeepBach (Hadjeres et al., 2017) and Coconet (Huang et al.,
2017) which use Gibbs sampling to produce notes in the
style of Bach chorals, MidiNet (Yang et al., 2017) and
MuseGAN (Dong et al., 2018) which use generative ad-
versarial networks, MusicV AE (Roberts et al., 2018) and
HRNN (Wu et al., 2019) which use hierarchical recurrent
networks, and Music Transformer (Huang et al., 2019a)
and MuseNet (Payne, 2019) which use Transformers to au-
toregressively predict MIDI note events. There also have
been a number of approaches for synthesizing music con-
ditioned on symbolic music information, such as NSynth
(Engel et al., 2017) which uses WaveNet-style autoen-
coder, Mel2Mel (Kim et al., 2019) and Wave2Midi2Wave
(Hawthorne et al., 2019) which synthesize music usingJukebox: A Generative Model for Music
WaveNet conditioned on a piano roll representation, and
GanSynth (Engel et al., 2019) which uses generative adver-
sarial networks to produce magnitude spectrograms together
with instananeous frequencies for easier spectrogram inver-
sion. Generative models for music can also be used for
music style transfer, as seen in Midi-V AE (Brunner et al.,
2018) which uses a variational autoencoder to transfer styles
between classical and jazz music, LakhNES (Donahue et al.,
2019) which uses a Transformer architecture to generate
chiptune music, and Universal Music Translator Network
(Mor et al., 2019) which uses a denoising autoencoder that
can disentangle musical style and content.
Sample-level generation of audio: In recent years, a vari-
ety of generative models for raw audio have been introduced.
WaveNet (Oord et al., 2016) performs autoregressive sample-
by-sample probabilistic modeling of raw waveform using a
series of dilated convolutions to exponentially increase the
context length. It can produce realistic audio either uncon-
ditionally or by conditioning on acoustic features or spec-
trograms. The autoregressive nature of WaveNet makes the
sampling notoriously slow, and it uses a categorical distribu-
tion for audio samples which introduces quantization noise.
Parallel WaveNet (Oord et al., 2018) improves upon this
by instead using a mixture of logistics distribution, a con-
tinuous probability distribution, and performing probabil-
ity density distillation which learns a parallel feed-forward
network from a pre-trained autoregressive model, allow-
ing faster sampling of high ﬁdelity audio. ClariNet (Ping
et al., 2019) achieves similar audio quality using a simple
Gaussian distribution instead and thus having a closed-form
loss function, eliminating the need for Monte-Carlo sam-
pling. SampleRNN (Mehri et al., 2017) uses a multi-scale,
hierarchical recurrent neural network with convolutional
upsampling to model long-range complex structures. Wa-
veRNN (Kalchbrenner et al., 2018) uses recurrent neural
networks that operate separately on the most signiﬁcant and
the least signiﬁcant bytes, which can be efﬁciently deployed
in mobile devices while having comparable audio quality to
WaveNet. WaveGlow (Prenger et al., 2019) is a ﬂow-based
model for parallel sample-level audio synthesis, which can
be trained with a straightforward maximum-likelihood esti-
mation and thus is advantageous to the two-stage training
process needed for distillation. Parallel WaveGAN (Ya-
mamoto et al., 2020) and MelGAN (Kumar et al., 2019)
are GAN-based approaches directly modeling audio wave-
forms, achieving similar quality as WaveNet and WaveGlow
models with signiﬁcantly fewer parameters. While the ap-
proaches above serve as sophisticated generative models for
raw audio to be conditioned on a compact and controllable
representation of audio such as Mel spectrograms, Mel-
Net (Vasquez & Lewis, 2019) takes a different approach of
hierarchically generating accurate high-resolution Mel spec-trograms, after which a simple gradient-based optimization
can produce high-ﬁdelity audio.
VQ-V AE: Oord et al. (2017) introduced VQ-V AE, an ap-
proach of downsampling extremely long context inputs to a
shorter-length discrete latent encoding using a vector quan-
tization, and they showed that it can generate both high-
quality images and audio, as well as learn unsupervized
representations of phonemes. Razavi et al. (2019) extended
the above model by introducing a hierarchy of discrete rep-
resentations for images and showed that the resulting model
can learn to separate high-level semantics into the highest
level of discrete codes which have the largest receptive ﬁeld,
while capturing local features like textures in the lower lev-
els with smaller receptive ﬁelds. They used the hierarchical
model to generate high-diversity and high-ﬁdelity images
for the conditional ImageNet and FFHQ datasets. Dieleman
et al. (2018) tried variants of this approach where instead
of a single encoder there are successive encoders that each
further compress the lossy discrete encodings from the previ-
ous levels. A downside of this approach is that information
is lost at each step and requires separate training for each
VQ-V AE level, and it leads to a hierarchy collapse problem.
De Fauw et al. (2019) used AR decoders which are known to
cause the problem of ignoring the latent variables, and they
suggested ways to mitigate it. The feed-forward decoders
from (Razavi et al., 2019) do not suffer from this issue, and
thus we use their approach.
Speech synthesis: Producing natural human voice entails
an understanding of linguistic features, mapping of sounds,
and steerability of expression. Many text-to-speech (TTS)
systems rely on highly engineered features (Klatt, 1980),
carefully curated sound segments (Hunt & Black, 1996),
statistical parametric modeling (Zen et al., 2009), and of-
ten complex pipelines as described in (Arık et al., 2017).
These approaches are fairly involved and produce unnatural
or inarticulate voices. More recent works like Deep V oice
3 (Ping et al., 2018), Tacotron 2 (Shen et al., 2018), and
Char2Wav (Sotelo et al., 2017) learn speech synthesis end-
to-end using sequence-to-sequence architecture (Sutskever
et al., 2014). The design space is vast, but in general, typical
approaches comprise of a bidirectional encoder, a decoder,
and a vocoder to build text representations, audio features,
and the ﬁnal raw waveforms. To generate multiple voices,
text-to-speech models can also condition on the speaker
identity (Oord et al., 2016; Gibiansky et al., 2017; Jia et al.,
2018) as well as text prompt. By learning and manipulat-
ing auxiliary embeddings, models can mimic a new voice
(Arık et al., 2018a; Taigman et al., 2018) at test time. These
methods, however, require labeled data. Ideas like clus-
tering (Dehak et al., 2011), priming (Wang et al., 2018),
and variational autoencoders (Hsu et al., 2019; Akuzawa
et al., 2018) have been used to learn broader styles of speech
and control expressivity in an unsupervised way. There areJukebox: A Generative Model for Music
also works on synthesizing singing by additionally con-
trolling pitch and timbre. Similar to TTS literature, early
works use concatenative methods (Bonada & Serra, 2007)
that join short segments of curated singing, and statistical
parametric methods (Saino et al., 2006; Oura et al., 2010)
which allow modeling of timbre from training data. Both
approaches impose fairly strong assumptions resulting in
noticeable artifacts. (Blaauw & Bonada, 2017) train a neural
TTS model with a parametric vocoder to separate pitch and
timbre which can be controlled at generation time.
7. Future work
While our approach represents a step forward in the ability
to generate coherent long raw audio music samples, we rec-
ognize several directions for future work. Great music gen-
eration should be high quality over all time scales: it should
have a developing musical and emotional structure across
the entire piece, local notes and harmonies that always make
sense, nuanced and appropriate small timbral and textural
details, and audio recording quality that balances and blends
the multiple voices well, and without unwanted noise. We
view our current model as stronger on the mid-range time
scales: often the model generates samples that locally sound
very good, with interesting and diverse harmonies, rhythms,
instruments, and voices. We have frequently been very
impressed how the melody and rhythm generated suits a
particular lyric extremely well. However, while the samples
stay consistent over longer time scales, we notice they dont
have traditional larger music structures (such as choruses
that repeat, or melodies that have a question and answer
form). Additionally, on the smallest scale, we sometimes
hear audio noise or scratchiness.
Beyond the quality of the samples, we also would look
to diversify the languages and styles the model is able to
generate. Our current model has been trained only on songs
whose primary language as detected by (Sites, 2013) is
English. In the future, we would look to include other
languages and artists. We believe this will be of interest
both for generating strictly in those styles, and because
historically we have seen much creativity and development
coming from unusual blends of existing musical styles.
Finally, we consider it very important that computer music
generation also serves as a tool for human musicians, and
increasingly those interested in music but without formal
training. While we are able to steer our current model some-
what through lyric and midi conditioning, we can imagine
many other possible ways for humans to inﬂuence the gener-
ations, including indicating the mood or dynamic at various
sections, or controlling when drums, singers, or other instru-
ments should play.The current model takes around an hour to generate 1 minute
of top level tokens. The upsampling process is very slow,
as it proceeds sequentially through the sample. Currently it
takes around 8 hours to upsample one minute of top level
tokens. We can create a human-in-the-loop co-composition
process at the top level only, using the VQ-V AE decoders
to get a fast upsampling of the top level tokens to hear a
very rough sense of what the model generates. The top-level
model generates multiple samples, the person picks a fa-
vorite (listening to the rough VQ-V AE decoding), and then
the model continues generating multiple samples continuing
the favorite. This process would be signiﬁcantly improved
with faster generation and Transformer upsampling steps.
Our models have fast parallel evaluation of likelihood but
slow autoregressive sampling. We can instead use a model
with fast parallel sampling but slow autoregressive likeli-
hood evaluation (Kingma et al., 2016), and distill the infor-
mation from our current model into it (Oord et al., 2018).
The distillation works by generating samples from the paral-
lel sampler and evaluating it likelihood and entropy using
the parallel likelihood evaluator, and then optimising the
sampler by minimising the KL divergence of it from our
current model.
8. Conclusion
We have introduced Jukebox, a model that generates raw
audio music imitating many different styles and artists. We
can condition this music on speciﬁc artists and genres, and
can optionally specify the lyrics for the sample. We laid
out the details necessary to train a Hierarchical VQ-V AE to
compress the music effectively into tokens. While previous
work has generated raw audio music in the 2030 second
range, our model is capable of generating pieces that are
multiple minutes long, and with recognizable singing in
natural-sounding voices.
9. Acknowledgement
We would like to thank John Schulman and Will Guss for
producing and performing novel riffs for our sampling ex-
periments, and Rewon Child, Aditya Ramesh, Ryan Lowe
and Jack Clark for providing feedback for initial drafts of
this paper.
References
Akuzawa, K., Iwasawa, Y ., and Matsuo, Y . Expressive
speech synthesis via modeling expressions with varia-
tional autoencoder. In INTERSPEECH , 2018.
Anders, T. and Miranda, E. R. Constraint programming
systems for modeling music theories and composition.
ACM Computing Surveys (CSUR) , 43(4):138, 2011.Jukebox: A Generative Model for Music
Arık, S. Ö., Chrzanowski, M., Coates, A., Diamos, G., Gib-
iansky, A., Kang, Y ., Li, X., Miller, J., Ng, A., Raiman,
J., Sengupta, S., and Shoeybi, M. Deep Voice: Real-time
neural text-to-speech. In International Conference on
Machine Learning , pp. 195204, 2017.
Arık, S. Ö., Chen, J., Peng, K., Ping, W., and Zhou, Y .
Neural voice cloning with a few samples. In Advances
in Neural Information Processing Systems , pp. 10019
10029. 2018a.
Arık, S. Ö., Jun, H., and Diamos, G. Fast spectrogram
inversion using multi-head convolutional neural networks.
IEEE Signal Processing Letters , 26(1):9498, 2018b.
Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.
arXiv preprint arXiv:1607.06450 , 2016.
Berner, C., Brockman, G., Chan, B., Cheung, V ., D ebiak, P.,
Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,
C., et al. Dota 2 with large scale deep reinforcement
learning. arXiv preprint arXiv:1912.06680 , 2019.
Beyls, P. The musical universe of cellular automata. In
International Computer Music Conference , pp. 3441,
1989.
Blaauw, M. and Bonada, J. A neural parametric singing
synthesizer. In INTERSPEECH , 2017.
Bonada, J. and Serra, X. Synthesis of the singing voice by
performance sampling and spectral models. IEEE signal
processing magazine , 24(2):6779, 2007.
Brock, A., Donahue, J., and Simonyan, K. Large scale
GAN training for high ﬁdelity natural image synthesis. In
International Conference on Learning Representations ,
2019.
Brunner, G., Konrad, A., Wang, Y ., and Wattenhofer, R.
MIDI-V AE: modeling dynamics and instrumentation of
music with applications to style transfer. In International
Society for Music Information Retrieval Conference , pp.
747754, 2018.
Child, R., Gray, S., Radford, A., and Sutskever, I. Gen-
erating long sequences with sparse transformers. arXiv
preprint arXiv:1904.10509 , 2019.
De Fauw, J., Dieleman, S., and Simonyan, K. Hierarchi-
cal autoregressive image models with auxiliary decoders.
arXiv preprint arXiv:1903.04933 , 2019.
Dehak, N., Kenny, P. J., Dehak, R., Dumouchel, P., and
Ouellet, P. Front-end factor analysis for speaker veriﬁca-
tion. IEEE Transactions on Audio, Speech, and Language
Processing , 19(4):788798, 2011.Dieleman, S., van den Oord, A., and Simonyan, K. The chal-
lenge of realistic music generation: modelling raw audio
at scale. In Advances in Neural Information Processing
Systems , pp. 79897999, 2018.
Dinh, L., Krueger, D., and Bengio, Y . NICE: Non-linear in-
dependent components estimation. In International Con-
ference in Learning Representations , Workshop, 2015.
Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density esti-
mation using Real NVP. In International Conference in
Learning Representations , 2017.
Donahue, C., Mao, H. H., Li, Y . E., Cottrell, G. W., and
McAuley, J. J. LakhNES: Improving multi-instrumental
music generation with cross-domain pre-training. In In-
ternational Society for Music Information Retrieval Con-
ference , pp. 685692, 2019.
Dong, H.-W., Hsiao, W.-Y ., Yang, L.-C., and Yang, Y .-H.
MuseGAN: Multi-track sequential generative adversarial
networks for symbolic music generation and accompani-
ment. In Thirty-Second AAAI Conference on Artiﬁcial
Intelligence , 2018.
Engel, J., Resnick, C., Roberts, A., Dieleman, S., Norouzi,
M., Eck, D., and Simonyan, K. Neural audio synthesis
of musical notes with wavenet autoencoders. In Interna-
tional Conference on Machine Learning , pp. 10681077,
2017.
Engel, J., Agrawal, K. K., Chen, S., Gulrajani, I., Donahue,
C., and Roberts, A. GANSynth: Adversarial neural au-
dio synthesis. In International Conference on Learning
Representations , 2019.
Gibiansky, A., Arık, S. Ö., Diamos, G., Miller, J., Peng, K.,
Ping, W., Raiman, J., and Zhou, Y . Deep Voice 2: Multi-
speaker neural text-to-speech. In Advances in neural
information processing systems , pp. 29622970, 2017.
Goodfellow, I. NIPS 2016 tutorial: Generative adversarial
networks. In Neural Information Processing Systems ,
Tutorial, 2016.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S., Courville, A., and Bengio,
Y . Generative adversarial nets. In Advances in neural
information processing systems , pp. 26722680, 2014.
Gupta, C., Yılmaz, E., and Li, H. Automatic lyrics tran-
scription in polyphonic music: Does background music
help? In International Conference on Acoustics, Speech,
and Signal Processing , 2020.
Hadjeres, G., Pachet, F., and Nielsen, F. Deepbach: a steer-
able model for bach chorales generation. In International
Conference on Machine Learning , pp. 13621371. JMLR.
org, 2017.Jukebox: A Generative Model for Music
Hawthorne, C., Stasyuk, A., Roberts, A., Simon, I., Huang,
C.-Z. A., Dieleman, S., Elsen, E., Engel, J., and Eck, D.
Enabling factorized piano music modeling and generation
with the MAESTRO dataset. In International Conference
on Learning Representations , 2019.
Hennequin, R., Khlif, A., V oituret, F., and Moussallam, M.
Spleeter: A fast and state-of-the art music source separa-
tion tool with pre-trained models. Late-Breaking/Demo
ISMIR 2019, November 2019. Deezer Research.
Hiller Jr, L. A. and Isaacson, L. M. Musical composition
with a high speed digital computer. In Audio Engineering
Society Convention 9 . Audio Engineering Society, 1957.
Ho, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T.
Axial attention in multidimensional transformers. arXiv
preprint arXiv:1912.12180 , 2019.
Hsu, W.-N., Zhang, Y ., Weiss, R. J., Zen, H., Wu, Y ., Wang,
Y ., Cao, Y ., Jia, Y ., Chen, Z., Shen, J., Nguyen, P., and
Pang, R. Hierarchical generative modeling for control-
lable speech synthesis. In International Conference on
Learning Representations , 2019.
Huang, C. A., Cooijmans, T., Roberts, A., Courville, A. C.,
and Eck, D. Counterpoint by convolution. In Interna-
tional Society for Music Information Retrieval Confer-
ence, pp. 211218, 2017.
Huang, C.-Z. A., Vaswani, A., Uszkoreit, J., Shazeer, N.,
Simon, I., Hawthorne, C., Dai, A. M., Hoffman, M. D.,
Dinculescu, M., and Eck, D. Music Transformer: Gen-
erating music with long-term structure. In International
Conference on Learning Representations , 2019a.
Huang, Y ., Cheng, Y ., Bapna, A., Firat, O., Chen, D., Chen,
M., Lee, H., Ngiam, J., Le, Q. V ., Wu, Y ., et al. Gpipe:
Efﬁcient training of giant neural networks using pipeline
parallelism. In Advances in Neural Information Process-
ing Systems , pp. 103112, 2019b.
Hunt, A. J. and Black, A. W. Unit selection in a con-
catenative speech synthesis system using a large speech
database. In IEEE International Conference on Acoustics,
Speech, and Signal Processing Conference , pp. 373376,
1996.
Jehan, T. Creating music by listening . PhD thesis, Mas-
sachusetts Institute of Technology, School of Architecture
and Planning, Program in Media Arts and Sciences, 2005.
Jia, Y ., Zhang, Y ., Weiss, R., Wang, Q., Shen, J., Ren, F.,
Chen, z., Nguyen, P., Pang, R., Lopez Moreno, I., and
Wu, Y . Transfer learning from speaker veriﬁcation to
multispeaker text-to-speech synthesis. In Advances in
Neural Information Processing Systems , pp. 44804490.
2018.Kalchbrenner, N., Elsen, E., Simonyan, K., Noury, S.,
Casagrande, N., Lockhart, E., Stimberg, F., Oord, A.,
Dieleman, S., and Kavukcuoglu, K. Efﬁcient neural au-
dio synthesis. In International Conference on Machine
Learning , pp. 24102419, 2018.
Kim, J. W., Bittner, R., Kumar, A., and Bello, J. P. Neural
music synthesis for ﬂexible timbre control. In IEEE In-
ternational Conference on Acoustics, Speech and Signal
Processing , pp. 176180, 2019.
Kingma, D. P. and Dhariwal, P. Glow: Generative ﬂow
with invertible 1x1 convolutions. In Advances in Neural
Information Processing Systems , pp. 1021510224, 2018.
Kingma, D. P. and Welling, M. Auto-encoding variational
bayes. In International Conference on Learning Repre-
sentations , 2014.
Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X.,
Sutskever, I., and Welling, M. Improved variational in-
ference with inverse autoregressive ﬂow. In Advances in
neural information processing systems , pp. 47434751,
2016.
Klatt, D. H. Software for a cascade/parallel formant synthe-
sizer. Journal of the Acoustical Society of America , 67
(3):971995, 1980.
Kumar, K., Kumar, R., de Boissiere, T., Gestin, L., Teoh,
W. Z., Sotelo, J., de Brébisson, A., Bengio, Y ., and
Courville, A. C. MelGAN: Generative adversarial net-
works for conditional waveform synthesis. In Advances
in Neural Information Processing Systems , pp. 14881
14892, 2019.
LyricWiki. URL https://lyrics.fandom.com/
wiki/LyricWiki .
Mehri, S., Kumar, K., Gulrajani, I., Kumar, R., Jain, S.,
Sotelo, J., Courville, A., and Bengio, Y . SampleRNN: An
unconditional end-to-end neural audio generation model.
InInternational Conference on Learning Representations ,
2017.
Moorer, J. A. Music and computer composition. Communi-
cations of the ACM , 15(2):104113, 1972.
Mor, N., Wolf, L., Polyak, A., and Taigman, Y . Autoencoder-
based music translation. In International Conference on
Learning Representations , 2019.
Oord, A. v. d., Dieleman, S., Zen, H., Simonyan, K.,
Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A.,
and Kavukcuoglu, K. WaveNet: A generative model for
raw audio. arXiv preprint arXiv:1609.03499 , 2016.Jukebox: A Generative Model for Music
Oord, A. v. d., Vinyals, O., and Kavukcuoglu, K. Neural
discrete representation learning. In Neural Information
Processing Systems , 2017.
Oord, A. v. d., Li, Y ., Babuschkin, I., Simonyan, K., Vinyals,
O., Kavukcuoglu, K., van den Driessche, G., Lockhart, E.,
Cobo, L., Stimberg, F., Casagrande, N., Grewe, D., Noury,
S., Dieleman, S., Elsen, E., Kalchbrenner, N., Zen, H.,
Graves, A., King, H., Walters, T., Belov, D., and Hassabis,
D. Parallel WaveNet: Fast high-ﬁdelity speech synthesis.
InInternational Conference on Machine Learning , pp.
39183926, 2018.
Oura, K., Mase, A., Yamada, T., Muto, S., Nankaku, Y .,
and Tokuda, K. Recent development of the HMM-based
singing voice synthesis system  Sinsy. 2010.
Papamakarios, G., Pavlakou, T., and Murray, I. Masked
autoregressive ﬂow for density estimation. In Advances in
Neural Information Processing Systems , pp. 23382347,
2017.
Payne, C. Musenet. OpenAI blog , 2019. URL https:
//openai.com/blog/musenet .
Ping, W., Peng, K., Gibiansky, A., Arik, S. O., Kannan,
A., Narang, S., Raiman, J., and Miller, J. Deep Voice
3: 2000-speaker neural text-to-speech. In International
Conference on Learning Representations , 2018.
Ping, W., Peng, K., and Chen, J. Clarinet: Parallel wave
generation in end-to-end text-to-speech. In International
Conference on Learning Representations , 2019.
Prenger, R., Valle, R., and Catanzaro, B. WaveGlow: A
ﬂow-based generative network for speech synthesis. In
IEEE International Conference on Acoustics, Speech and
Signal Processing , pp. 36173621, 2019.
Pressing, J. Nonlinear maps as generators of musical design.
Computer Music Journal , 12(2):3546, 1988.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and
Sutskever, I. Language models are unsupervised multitask
learners.
Razavi, A., van den Oord, A., and Vinyals, O. Generating
diverse high-ﬁdelity images with vq-vae-2. In Advances
in Neural Information Processing Systems , pp. 14837
14847, 2019.
Rezende, D. and Mohamed, S. Variational inference with
normalizing ﬂows. In International Conference on Ma-
chine Learning , pp. 15301538, 2015.
Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic
backpropagation and approximate inference in deep gen-
erative models. In International Conference on Machine
Learning , pp. 12781286, 2014.Roberts, A., Engel, J., Raffel, C., Hawthorne, C., and Eck,
D. A hierarchical latent vector model for learning long-
term structure in music. In International Conference on
Machine Learning , pp. 43644373, 2018.
Saino, K., Zen, H., Nankaku, Y ., Lee, A., and Tokuda,
K. An HMM-based singing voice synthesis system. In
INTERSPEECH , 2006.
Salakhutdinov, R. and Hinton, G. Deep boltzmann machines.
InArtiﬁcial intelligence and statistics , pp. 448455, 2009.
Shen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N.,
Yang, Z., Chen, Z., Zhang, Y ., Wang, Y ., Skerrv-Ryan,
R., Saurous, R. A., Agiomvrgiannakis, Y ., and Wu, Y .
Natural TTS synthesis by conditioning wavenet on mel
spectrogram predictions. In IEEE International Confer-
ence on Acoustics, Speech and Signal Processing , pp.
47794783, 2018.
Sites, D. Compact language detector 2. 2013. URL https:
//github.com/CLD2Owners/cld2 .
Sotelo, J., Mehri, S., Kumar, K., Santos, J. F., Kastner, K.,
Courville, A. C., and Bengio, Y . Char2Wav: End-to-
end speech synthesis. In International Conference on
Learning Representations , 2017.
Sturmel, N. and Daudet, L. Signal reconstruction from stft
magnitude: A state of the art. International Conference
on Digital Audio Effects, DAFx , 2011.
Sutskever, I., Vinyals, O., and Le, Q. V . Sequence to se-
quence learning with neural networks. In Advances in
neural information processing systems , pp. 31043112,
2014.
Taigman, Y ., Wolf, L., Polyak, A., and Nachmani, E.
V oiceLoop: V oice ﬁtting and synthesis via a phonological
loop. In International Conference on Learning Represen-
tations , 2018.
Uria, B., Côté, M.-A., Gregor, K., Murray, I., and
Larochelle, H. Neural autoregressive distribution esti-
mation. The Journal of Machine Learning Research , 17
(1):71847220, 2016.
Van den Oord, A., Kalchbrenner, N., Espeholt, L., Vinyals,
O., Graves, A., et al. Conditional image generation with
pixelcnn decoders. In Advances in neural information
processing systems , pp. 47904798, 2016.
Vasquez, S. and Lewis, M. MelNet: A generative model
for audio in the frequency domain. arXiv preprint
arXiv:1906.01083 , 2019.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-
tion is all you need. In Advances in Neural Information
Processing Systems , pp. 59986008, 2017.Jukebox: A Generative Model for Music
Wang, Y ., Stanton, D., Zhang, Y ., Skerry-Ryan, R., Batten-
berg, E., Shor, J., Xiao, Y ., Ren, F., Jia, Y ., and Saurous,
R. A. Style Tokens: Unsupervised style modeling, control
and transfer in end-to-end speech synthesis. In Interna-
tional Conference on Machine Learning , 2018.
Wu, J., Hu, C., Wang, Y ., Hu, X., and Zhu, J. A hierarchical
recurrent neural network for symbolic melody generation.
IEEE Transactions on Cybernetics , 2019.
Xie, S., Girshick, R., Dollár, P., Tu, Z., and He, K. Aggre-
gated residual transformations for deep neural networks.
InIEEE Conference on Computer Vision and Pattern
Recognition , pp. 14921500, 2017.
Yamamoto, R., Song, E., and Kim, J.-M. Parallel Wave-
GAN: A fast waveform generation model based on gener-
ative adversarial networks with multi-resolution spectro-
gram. In International Conference on Acoustics, Speech,
and Signal Processing , 2020.
Yang, L., Chou, S., and Yang, Y . Midinet: A convolutional
generative adversarial network for symbolic-domain mu-
sic generation. In International Society for Music Infor-
mation Retrieval Conference , pp. 324331, 2017.
Zen, H., Tokuda, K., and Black, A. W. Review: Statistical
parametric speech synthesis. Speech Communication , 51
(11):10391064, 2009.
Zhang, H., Dauphin, Y . N., and Ma, T. Fixup initialization:
Residual learning without normalization. In International
Conference on Machine Learning , 2019a.
Zhang, H., Goodfellow, I., Metaxas, D., and Odena, A.
Self-attention generative adversarial networks. In Inter-
national Conference on Machine Learning , 2019b.Jukebox: A Generative Model for Music
A. Scalable Transformer
We make the Sparse Transformer (Child et al., 2019) more
scalable and easier to implement by a few small changes.
We implement a simpler attention pattern that has the same
performance without needing custom kernels to implement.
We simplify the initialization by using the same initalization
scale in the whole model without rescaling the weights
based on fan-in and depth, and we optimize the memory
footprint with fully half-precision training, i.e. storing the
model weights, gradients and the optimizer states in half
precision and performing computations in half precision as
well. To cope with the narrower dynamic range of the fp16
format, we use dynamic scaling of the gradient and Adam
optimizer states.
Axis-aligned attention patterns: The Sparse Transformer
(Child et al., 2019) sparsiﬁes the attention pattern by
reshaping the input sequence into a 2-D sequence of
shape (blocks, block length )to use factorized attention.
They observe that the strided attention pattern works
best for images and audio because it does not have the
state bottleneck of the ﬁxed attention. However, their
implementation require specialized CUDA kernels. We
can obtain a similar pattern by doing masked row, masked
column, and unmasked previous-row attention. While
the masked row captures the local context, the masked
column and unmasked previous-row attention captures
the context of all previous rows. We observe the same
computational speed as well as training loss with this
pattern. Each of these can be implemented directly as a
dense attention by transposing or slicing the input sequence
along appropriate axes, and thus do not require special
CUDA kernels to implement. This can be easily extended
to video too. Complementary to our work, a similar
pattern was introduced in (Ho et al., 2019) where they also
used axis-aligned attention but instead used a two-stream
architecture.
Half-precision parameters and optimizer state with dy-
namic scaling: To allow training large models, (Child et al.,
2019) uses recompute with gradient checkpointing, per-
forms computations using half precision activations and
gradients, and uses dynamic loss scaling. While this speeds
up training on V olta cores, one still has a high memory us-
age from storing the parameters and Adam state in full ﬂoat
precision. To scale our models further, we store our matmul
parameters and their Adam state in half precision, thus halv-
ing our memory usage. We use a single parameter sto set the
scale of all weights and initialize all matmul and input/out-
put embeddings3toN(0;s), and position embeddings to
N(0;2s). The initialization ensures all parameters are in a
similar dynamic range, and allows us to train in half preci-
3We share the input and output embedding
Masked
Row AttentionMasked
Column AttentionUnmasked
Previous-Row Attention(a) Three axis-aligned attention patterns are sparse attention pat-
terns that allow autoregressive generative modeling while only
using simple Python-level array manipulation. Masked row and
column attention patterns use autoregressive masks, whereas un-
masked previous-row attention is fully visible.
        
(b) Combining two of the attention patterns, each position can
attend to any of the previous positions, while not causing a state
bottleneck as in ﬁxed sparse attention (Child et al., 2019).
Figure 6. Axis-aligned attention patterns
sion completely without loss in training performance. For
the Adam state tensors (m_t, v_t) we do dynamic scal-
ing. For each iteration and for every parameter, we rescale
its state tensors before casting so that their maximum corre-
sponds to the maximum value of the ﬂoat16 range, thus max-
imizing the use of the ﬂoat16 range. Thus, we store the state
m_t as the tuple (scale, (m_t/scale).half()) ,
where scale = m_t.max()/float16.max() , and
similarly for v_t. The above lets us ﬁt models of size 1B
parameters into memory for our large context of 8192 to-
kens. To train even larger models, we use GPipe (Huang
et al., 2019b).Jukebox: A Generative Model for Music
B. Experimental details
B.1. Music VQ-V AE
We have three separate raw audio VQ-V AEs to produce dis-
crete codes at varying hop sizes for the bottom, middle, and
top priors. All autoencoders comprise non-causal, dilated
1-D convolutions, and are trained independently using non-
autoregressive reconstruction losses. Basic building blocks
in these networks share the same architecture, as shown in
Figure 7. Each encoder block consists of a downsampling
convolution, a residual network, and a 1D convolution with
a kernel size of 3. Dilation is grown by a factor of 3 in
these residual networks to increase the receptive ﬁeld. The
decoder block mirrors this exactly with a 1D convolution
with the kernel size of 3, a residual network with dilation
contracting across depth, and an upsampling transposed con-
volution. Here, all resampling convolutions use a kernel size
of 4 and stride 2 so that each building block changes the
hop length by a factor of 2. To get higher compression in
time, we simply stack more of these blocks. For example,
using seven blocks yields a hop length of 128 for the top
level autoencoder.
Each residual network has four residual blocks in the mid-
dle and top VQ-V AEs resulting in a receptive ﬁeld of 120
ms and 480 ms for the respective discrete tokens. Because
increasing the residual depth helped improve reconstruction
quality slightly, we doubled the number of residual blocks
for the bottom level. This dramatically increases the recep-
tive ﬁeld to about 2 seconds per code but the actual receptive
ﬁeld is mostly local.
We also experimented with having a single decoder and
modeling the residuals to separate out learned representa-
tions as in (Razavi et al., 2019), hoping upsampling priors
would simply ﬁll in local musical structure. However, push-
ing information to the top level was quite challenging as the
bottommost level reconstructs almost perfectly early on in
training. When we add auxiliary objectives to encourage
the top to be used more, the top-level codes add serious
distortions to the ﬁnal output. A similar challenge is shown
in (Dieleman et al., 2018).
B.2. Music Priors and Upsamplers
Architectural details of our music prior and upsampler mod-
els are depicted in Figure 8. They perform autoregressive
modeling of tokens at each level, conditioned on informa-
tion such as artist and genre, as well as the tokens from the
upper level in the case of the upsamplers (Figure 8a). Each
artist and genre are learned as embedding vectors, whose
sum is provided as the very ﬁrst token in each sequence.
In addition, positional embedding is learned as a function
of each positions absolute and relative timing in the dura-
tion of the song. In upsampler models, upper-level tokens
L
Conv1DDDilated
Conv1DConv1D
+ x t h t(a) The encoder compresses the raw audio input into a sequence
of embeddings. The length of this latent representation relative
to the raw audio duration determines the amount of compression,
and is an important factor for the trade-off between ﬁdelity and
coherence.
Gradient PassthroughNearest-Neighbor
Searchz tCodebook
h t e z tCodebook
Lookup
(b) The bottleneck takes the sequence of embeddings from the
encoder and maps it into a sequence of code vectors from the
codebook. This sequence of code indices is used as a discrete
representation to be modeled by the priors. Larger codebooks
improve ﬁdelity but may be more difﬁcult to compress.
Conv1DL
DDilated
Conv1DConv1D
+T ransposed
Conv1De z t x t
(c) The decoder reconstructs the raw audio from latent represen-
tations. It is a mirror of the encoder where dilations constracts
by a factor of 3 down to 1 at the last block. The ﬁnal Conv1D
projects to the desired number of audio channels and also acts as a
smoothing operation after a sequence of transposed convolutions.
Figure 7. Components of the VQ-V AE model
are upsampled by the conditioner network, using WaveNet-
style dilated convolutions followed by a transposed 1-D
convolutional layer (Figure 8b).
When the model is trained on lyrics, the top-level prior takes
lyrics data corresponding to each audio segment and uses
them to train an encoder-decoder Transformer as shown in
Figure 8c. All transformer stacks use sparse self-attention
layers with the three factorized attention types (row, column,
and previous-row) repeating, and encoder-decoder attention
layers, when present, are interleaved with the other attention
types. Each layer consists of residual connections of an
attention and an MLP feedforward network, each prepended
by layer normalization (see Figure 8d).Jukebox: A Generative Model for Music
Artist & GenreConditioner
z 1: TScalable T ransformerUpper-Level T okens
Time Embedding 
LyricsTiming DataNot Used in the T op Level
							z 1: T 1
(a) The structure of our prior models, performing next-token prediction at each
level. The Transformer takes the embeddings of the tokens z1:T1prepended by
the sum of the artist and genre embeddings, in addition to the time embedding
that encodes relative and absolute timing of the segments in the duration of the
song. The upsampler priors additionally take the tokens from the upper level,
which are fed to the conditioner network and added to the input sequence. The
top-level prior takes lyrics as conditioning information as well (see Figure 8c).
D
Dilated Conv1D
Conv1D
+
Transposed Conv1DToken Embedding(b) The conditioner network takes the tokens from
the upper level, and their embedding vectors go
through non-causal WaveNet-like layers with in-
creasingly dilated convolutions. The transposed 1-D
convolution upsamples the sequence to the higher
temporal resolution of the current level.
Lyrics
Row Attention Layer
Column Attention Layer
Previous-Row Attention Layer
Row Attention Layer
Column Attention Layer
Previous-Row Attention Layer

Row Attention Layer
Column Attention Layer
Previous-Row Attention LayerLyrics T oken Embedding
Next-T oken PredictionLyrics T oken EmbeddingRow Attention Layer
Column Attention Layer
Previous-Row Attention LayerVQ Code Embedding
Next-T oken PredictionVQ Code EmbeddingVQ Codes
6
Row Attention Layer
Column Attention Layer
Previous-Row Attention Layer3
Encoder-Decoder Attention Layer
Encoder-Decoder Attention Layer
Row Attention Layer
Column Attention Layer
Previous-Row Attention LayerEncoder-Decoder Attention Layer
3Only in the T op Level
Encoder-Decoder Attention Layer
(c) The Scalable Transformer architecture, shown with the lyrics Transformer used in the
top-level prior. The Transformer layers use the three factorized attention types alternatingly,
i.e. repeating row, column, and previous-row attentions. In the top-level prior, the VQ
Transformer additionally includes interleaved encoder-decoder attention layers that apply
lyrics conditioning by attending on the activation of the last encoder layer.
Layer Norm
Attention
Layer Norm
MLP+
+Encoder
Features(d) Each Transformer layer is a resid-
ual attention block, which performs
two residual operations, attention and
MLP, each prepended with layer nor-
malization. Depending on the layers
type, it uses either one of the three fac-
torized attentions or encoder-decoder
attention taking the lyrics features
from the encoder.
Figure 8. Detailed architecture of the music prior and upsampler modelsJukebox: A Generative Model for Music
B.3. Hyperparameters
For all Transformers residual blocks, we use MLP blocks
with the same width as the model width, and attention blocks
with queries, keys, and values with width 0.25 times the
model width. For all convolutional residual blocks, we use
convolutions with same channels as the model width.
Sample rate 44100
Sample length 393216
Hop lengths 8, 32, 128
Embedding width 64
Residual block width 64, 32, 32
Residual blocks (per 2x downsample) 8, 4, 4
Conv ﬁlter size 3
Conv channels 32
Dilation growth rate 3
Commit weight  0.02
Codebook EMA 
 0.99
Codebook size 2048
Spectral loss STFT bins 2048, 1024, 512
Spectral loss STFT hop length 240, 120, 50
Spectral loss STFT window size 1200, 600, 240
Initialization scale 0.02
Batch size 256
Training steps 384618
Learning rate 0.0003
Table 4. VQ-V AE hyperparameters
1B upsamplers
Sample length 262144, 65536
Context length 8192
Transformer width 1920
Transformer layers 72
Attention heads 1
Factorized attention shape (128, 64)
Conditioner residual block width 1024
Conditioner residual blocks 16
Conditioner conv ﬁlter size 3
Conditioner conv channels 1024
Conditioner dilation growth rate 3
Conditioner dilation cycle 8
Initialization scale 0.004, 0.008
Batch size 192, 184
Training steps 265000, 279000
Learning rate 0.0003
Adam2 0.95
Weight decay 0.01
Table 5. Middle- and bottom-level upsampler hyperparameters5B prior
Sample length 1048576
Context length 8192
Transformer width 4800
Transformer self-attention layers 72
Attention heads 8
Factorized attention shape (128, 64)
Lyrics encoder tokens 512
Lyrics encoder width 1280
Lyrics encoder layers 18
Lyrics encoder attention heads 4
Lyrics encoder factored attention shape (32, 16)
Encoder-Decoder attention layers 7
Initialization scale 0.002
Encoder initialization scale 0.014
Batch size 512
Training steps 310500
Learning rate 0.00015
Adam2 0.925
Weight decay 0.002
Table 6. Top-level prior hyperparametersJukebox: A Generative Model for Music
B.4.t-SNE Plot of Artists
The Beatles
George Harrison
Paul McCartney
John Lennon
Ringo Starr
Cheap Trick
Aerosmith
QueenStatus QuoELORod StewartFleetwood Mac
Neil Young
Eagles
Pearl JamThe Beatles
George Harrison
Paul McCartney
John Lennon
Ringo Starr
Cheap Trick
Aerosmith
QueenStatus QuoELORod StewartFleetwood Mac
Neil Young
Eagles
Pearl JamArianaGrande
P!nkEdSheeranTheWeeknd
TheBeatles
PinkFloyd
LinkinParkTheBeachBoysBuckOwens
EddyArnold
BillyRayCyrusShaggySeanPaulJimmyCliffBarringtonLevy
DeanMartin
MartyRobbinsHowardShoreTonyBennett
NeilDiamondCabCalloway
HenryMancini
VanMorrisonDinahWashingtonFatsDomino
HankSnow
TheSmashingPumpkinsRaminDjawadiNinaSimoneDonnaSummer
TheCureLouRawls
MigosVangelisNatalieCole
T.Pain
KanyeWestBessieSmithRayNoble
BobbyBlandTheMillsBrothers
AkonLouisPrima
O.S.T.R.LonnieJohnson
AndreaBocelliThePlattersBarryWhite
LutherVandross
YannTiersen
FranzSchubert
JohannSebastianBachGlennGould
Yo-YoMaGarrickOhlssonWalterGieseking
PopRock
ReggaeR&BR&B Soul
Hip Hop
 CountryBlues
ClassicalJazz
Soundtrack
Figure 9. t-SNE of (artist, genre) embedding. The overall clustering shows very clearly how genres are related to one another. The
broadest of all, pop, is situated in the middle of rock, country, blues, hip hop, and many more. Soundtrack and classical form their own
island. Within a genre, we see a similar trend among artists. John Lennon, Paul McCartney, George Harrison and Ringo Starr are clustered
around The Beatles. Cheap Trick which has a number of Beatles covers is also found near. Because we are showing only about 400 artists
here, not all neighboring artists may be related. For an interactive version, we point to our blog post.
  Preprint. Under review.
Leave No Context Behind:
Efficient Infinite Context Transformers with Infini-attention
Tsendsuren Munkhdalai, Manaal Faruqui and Siddharth Gopal
Google
tsendsuren@google.com
Abstract
This work introduces an efficient method to scale Transformer-based Large
Language Models (LLMs) to infinitely long inputs with bounded memory
and computation. A key component in our proposed approach is a new at-
tention technique dubbed Infini-attention. The Infini-attention incorporates
a compressive memory into the vanilla attention mechanism and builds
in both masked local attention and long-term linear attention mechanisms
in a single Transformer block. We demonstrate the effectiveness of our
approach on long-context language modeling benchmarks, 1M sequence
length passkey context block retrieval and 500K length book summarization
tasks with 1B and 8B LLMs. Our approach introduces minimal bounded
memory parameters and enables fast streaming inference for LLMs.
1 Introduction
Memory serves as a cornerstone of intelligence, as it enables efficient computations tailored
to specific contexts. However, Transformers (Vaswani et al., 2017) and Transformer-based
LLMs (Brown et al., 2020; Touvron et al., 2023; Anil et al., 2023; Groeneveld et al., 2024) have
a constrained context-dependent memory, due to the nature of the attention mechanism.
Update 
VVConcat Concat 
Q V
Q V
Qs{KV}sCompressive memory & 
Linear attention Causal scaled dot-product 
attention & PE Linear 
projection 
{KV}s-1Retrieve 
Figure 1: Infini-attention has an addi-
tional compressive memory with linear
attention for processing infinitely long
contexts. {KV}s1and{KV}sare atten-
tion key and values for current and previ-
ous input segments, respectively and Qs
the attention queries. PE denotes position
embeddings.The attention mechanism in Transformers ex-
hibits quadratic complexity in both memory
footprint and computation time. For example,
the attention Key-Value (KV) states have 3TB
memory footprint for a 500B model with batch
size 512 and context length 2048 (Pope et al.,
2023). Indeed, scaling LLMs to longer sequences
(i.e. 1M tokens) is challenging with the standard
Transformer architectures and serving longer
and longer context models becomes costly finan-
cially.
Compressive memory systems promise to be
more scalable and efficient than the attention
mechanism for extremely long sequences (Kan-
erva, 1988; Munkhdalai et al., 2019). Instead
of using an array that grows with the input se-
quence length, a compressive memory primarily
maintains a fixed number of parameters to store
and recall information with a bounded storage
and computation costs. In the compressive mem-
ory, new information is added to the memory
by changing its parameters with an objective
that this information can be recovered back later
on. However, the LLMs in their current state
have yet to see an effective, practical compres-
sive memory technique that balances simplicity along with quality.
1arXiv:2404.07143v1  [cs.CL]  10 Apr 2024Preprint. Under review.
In this work, we introduce a novel approach that enables Transformer LLMs to effectively
process infinitely long inputs with bounded memory footprint and computation. A key
component in our proposed approach is a new attention technique dubbed Infini-attention
(Figure 1). The Infini-attention incorporates a compressive memory into the vanilla attention
mechanism (Bahdanau et al., 2014; Vaswani et al., 2017) and builds in both masked local
attention and long-term linear attention mechanisms in a single Transformer block.
Such a subtle but critical modification to the Transformer attention layer enables a natural
extension of existing LLMs to infinitely long contexts via continual pre-training and fine-
tuning.
Our Infini-attention reuses all the key, value and query states of the standard attention
computation for long-term memory consolidation and retrieval. We store old KV states of
the attention in the compressive memory, instead of discarding them like in the standard
attention mechanism. We then retrieve the values from the memory by using the attention
query states when processing subsequent sequences. To compute the final contextual
output, the Infini-attention aggregates the long-term memory-retrieved values and the local
attention contexts.
In our experiments, we show that our approach outperforms baseline models on long-
context language modeling benchmarks while having 114x comprehension ratio in terms of
memory size. The model achieves even better perplexity when trained with 100K sequence
length. A 1B LLM naturally scales to 1M sequence length and solves the passkey retrieval
task when injected with Infini-attention. Finally, we show that a 8B model with Infini-
attention reaches a new SOTA result on a 500K length book summarization task after
continual pre-training and task fine-tuning.
In summary, our work makes the following contributions:
1.We introduce a practical and yet powerful attention mechanism  Infini-attention
with long-term compressive memory and local causal attention for efficiently mod-
eling both long and short-range contextual dependencies.
2.Infini-attention introduces minimal change to the standard scaled dot-product atten-
tion and supports plug-and-play continual pre-training and long-context adaptation
by design.
3.Our approach enables Transformer LLMs to scale to infinitely long context with a
bounded memory and compute resource by processing extremely long inputs in a
streaming fashion.
2 Method
Figure 2 compares our model, Infini-Transformer, and Transformer-XL (Dai et al., 2019).
Similar to Transformer-XL, Infini-Transformer operates on a sequence of segments. We
compute the standard causal dot-product attention context within each segment. So the
dot-product attention computation is local in a sense that it covers a total Nnumber of
tokens of the current segment with index S(Nis the segment length).
The local attention (Dai et al., 2019), however, discards the attention states of the previous
segment when processing the next one. In Infini-Transformers, instead of leaving out the
old KV attention states, we propose to reuse them to maintain the entire context history
with a compressive memory. So each attention layer of Infini-Transformers has both global
compressive and local fine-grained states. We call such an efficient attention mechanism
Infini-attention, which is illustrated in Figure 1 and described formally in the following
sections.
2.1 Infini-attention
As shown Figure 1, our Infini-attention computes both local and global context states and
combine them for its output. Similar to multi-head attention (MHA), it maintains Hnumber
2Preprint. Under review.
Segment 1 Segment 2 Segment 3 
Segment 1 Segment 2 Segment 3 Transformer block: Infini-T ransformer 
T ransformer-XL Compressive memory: 
Memory update: 
Memory retrieval: 
Effective context: 
Input segment: Segment 1 
Figure 2: Infini-Transformer (top) has an entire context history whereas Transformer-XL
(bottom) discards old contexts since it caches the KV states for the last segment only.
of parallel compressive memory per attention layer ( His the number of attention heads) in
addition to the dot-product attention.
2.1.1 Scaled Dot-product Attention
The multi-head scaled dot-product attention (Vaswani et al., 2017), specially its self-attention
variant (Munkhdalai et al., 2016; Cheng et al., 2016), has been the main building block in
LLMs. The MHAs strong capability to model context-dependent dynamic computation and
its conveniences of temporal masking have been leveraged extensively in the autoregressive
generative models.
A single head in the vanilla MHA computes its attention context AdotI RNdvaluefrom
sequence of input segments XI RNdmodel as follows. First, it computes attention query,
key, and value states:
K=XW K,V=XW Vand Q=XW Q. (1)
Here, WKI Rdmodeldkey,WVI Rdmodeldvalueand WQI Rdmodeldkeyare trainable projection
matrices. Then, the attention context is calculated as a weighted average of all other values
as
Adot=softmaxQKT
dmodel
V. (2)
For MHA, we compute Hnumber of attention context vectors for each sequence element
in parallel, concatenate them along the second dimension and then finally project the
concatenated vector to the model space to obtain attention the output.
2.1.2 Compressive Memory
In Infini-attention, instead of computing new memory entries for compressive memory, we
reuse the query, key and value states ( Q,Kand V) from the dot-product attention compu-
tation. The state sharing and reusing between the dot-product attention and compressive
memory not only enables efficient plug-in-play long-context adaptation but also speeds up
training and inference. Similar to the prior work (Munkhdalai et al., 2019), our goal is to
store bindings of key and value states in the compressive memory and retrieve by using the
query vectors.
3Preprint. Under review.
While there are different forms of compressive memory proposed in the literature (Hop-
field, 1982; Kanerva, 1988; Schlag et al., 2019; Munkhdalai et al., 2019), for simplicity and
computational efficiency, in this work we parameterize the memory with an associative
matrix (Schlag et al., 2020). This approach further allows us to cast the memory update
and retrieval process as linear attention mechanism (Shen et al., 2018) and to leverage
stable training techniques from the related methods. Specially, we adopt the update rule
and retrieval mechanism by Katharopoulos et al. (2020) mainly due to its simplicity and
competitive performance.
Memory retrieval. In Infini-attention, we retrieve new content AmemI RNdvaluefrom the
memory Ms1I Rdkeydvalueby using the query QI RNdkeyas:
Amem=σ(Q)Ms1
σ(Q)zs1. (3)
Here, σand zs1I Rdkeyare a nonlinear activation function and a normalization term,
respectively. As the choice of the non-linearity and the norm method is crucial for training
stability, following Katharopoulos et al. (2020) we record a sum over all keys as the normal-
ization term zs1and use element-wise ELU + 1 as the activation function (Clevert et al.,
2015).
Memory update. Once the retrieval is done, we update the memory and the normalization
term with the new KV entries and obtain the next states as
MsMs1+σ(K)TVand zszs1+N

t=1σ(Kt). (4)
The new memory states Msand zsare then passed to the next segment S+1, building in
a recurrence in each attention layer. The right side term σ(K)TVin Eq. (4)is known as an
associative binding operator (Smolensky, 1990; Hebb, 2005; Schlag et al., 2020).
Inspired by the success of delta rule (Munkhdalai et al., 2019; Schlag et al., 2020; 2021),
we have also incorporated it into our Infini-attention. The delta rule attempts a slightly
improved memory update by first retrieving existing value entries and subtracting them
from the new values before applying the associative bindings as new update.
MsMs1+σ(K)T(Vσ(K)Ms1
σ(K)zs1). (5)
This update rule ( Linear +Delta ) leaves the associative matrix unmodified if the KV binding
already exists in the memory while still tracking the same normalization term as the former
one ( Linear ) for numerical stability.
Long-term context injection. We aggregate the local attention state Adotand memory
retrieved content Amemvia a learned gating scalar β:
A=sigmoid (β)Amem+ (1sigmoid (β))Adot. (6)
This adds only a single scalar value as training parameter per head while allowing a
learnable trade-off between the long-term and local information flows in the model (Wu
et al., 2022).
Similar to the standard MHA, for the multi-head Infini-attention we compute Hnumber of
context states in parallel, and concatenate and project them for the final attention output
OI RNdmodel:
O= [A1; . . .AH]WO (7)
where WOI RHdvaluedmodelis trainable weights.
2.2 Memory and Effective Context Window
Our Infini-Transformer enables an unbounded context window with a bounded memory
footprint. To illustrate this, Table 1 lists the previous segment-level memory models with
4Preprint. Under review.
Model Memory (cache) footprint Context length Memory update Memory retrieval
Transformer-XL (dkey+dvalue)HNl N l Discarded Dot-product attention
Compressive Transformer dmodel(c+N)l (cr+N)l Discarded Dot-product attention
Memorizing Transformers (dkey+dvalue)HNS N S None kNN + dot-product attention
RMT dmodelpl2 NS Discarded Soft-prompt input
AutoCompressors dmodelp(m+1)l N S Discarded Soft-prompt input
Infini-Transformers dkey(dvalue+1)Hl N S Incremental Linear attention
Table 1: Transformer models with segment-level memory are compared. For each model, the
memory size and effective context length are defined in terms of their model parameters ( N:
input segment length, S: the number of segments, l: the number of layers, H: the number
of attention heads, c: Compressive Transformer memory size, r: compression ratio, p: the
number of soft-prompt summary vectors and m: summary vector accumulation steps).
their context-memory footprint and effective context length defined in terms of model
parameters and input segment length. Infini-Transformer has a constant memory complexity
ofdkeydvalue+dkeyfor storing compressed context in Msand zsfor each head in single
layer while for the other models, the complexity grows along with the sequence dimension
- the memory complexity depends either on the cache size for Transformer-XL (Dai et al.,
2019), Compressive Transformer (Rae et al., 2019) and Memorizing Transformers (Wu et al.,
2022) or on the soft-prompt size for RTM (Bulatov et al., 2022) and AutoCompressors (Ge
et al., 2023).
Early 
layers 
Attention heads 
Figure 3: There are two types of heads
emerged in Infini-attention after training: spe-
cialized heads with gating score near 0 or
1 and mixer heads with score close to 0.5.
The specialized heads either process contex-
tual information via the local attention mech-
anism or retrieve from the compressive mem-
ory whereas the mixer heads aggregate both
current contextual information and long-term
memory content together into single output.Transformer-XL computes attention over KV
states cached from the last segment in addition
to the current states. Since this is done for each
layer, Transformer-XL extends the context win-
dow from NtoNltokens with an additional
memory footprint of (dkey+dvalue)HNl.
Compressive Transformer adds a second cache
to Transformer-XL and stores compressed rep-
resentations of past segment activations. So it
extends the Transformer-XLs context window
bycrlbut still has a large context-memory
complexity. Taking the idea further, Memoriz-
ing Transformers opt to store the entire KV states
as context for input sequences. Since the stor-
age becomes prohibitively expensive in this case,
they restrict the contextual computation to a sin-
gle layer only. By utilizing a fast kNN retriever,
Memorizing Transformers then build a context
window covering the entire sequence history of
length NSat an increased cost of storage. Our
experiments show that Infini-Transformer LM
can achieve more than 100x compression rate on
top of Memorizing Transformers while further
improving the perplexity score.
RMT and AutoCompressors allow for a poten-
tially infinite context length since they compress
the input into summary vectors and then pass
them as extra soft-prompt inputs for the subsequent segments. However, in practice the
success of those techniques highly depends on the size of soft-prompt vectors. Namely, it
is necessary to increase the number of soft-prompt (summary) vectors to achieve a better
performance with AutoCompressors (Chevalier et al., 2023) and with that, the memory and
compute complexity grow quickly resulting in diminished efficiency. It was also observed in
AutoCompressors (Chevalier et al., 2023) that an efficient compression objective is needed
for training such prompt compression techniques (Ge et al., 2023).
5Preprint. Under review.
Model Memory size (comp.) XL cache Segment length PG19 Arxiv-math
Transformer-XL 50M (3.7x) 2048 2048 11.88 2.42
Memorizing Transformers 183M (1x) 2048 2048 11.37 2.26
RMT 2.5M (73x) None 2048 13.27 2.55
Infini-Transformer (Linear) 1.6M (114x) None 2048 9.65 2.24
Infini-Transformer (Linear + Delta) 1.6M (114x) None 2048 9.67 2.23
Table 2: Long-context language modeling results are compared in terms of average token-
level perplexity. Comp. denotes compression ratio. Infini-Transformer outperforms memo-
rizing transformers with memory length of 65K and achieves 114x compression ratio.
3 Experiments
We evaluated our Infini-Transformer models on benchmarks involving extremely long input
sequences: long-context language modeling, 1M length passkey context block retrieval
and 500K length book summarization tasks. For the language modeling benchmark, we
train our models from scratch while for the passkey and book summarization tasks, we
continually pre-train existing LLMs in order to highlight a plug-and-play long-context
adaptation capability of our approach.
3.1 Long-context Language Modeling
We trained and evaluated small Infini-Transformer models on PG19 (Rae et al., 2019) and
Arxiv-math (Wu et al., 2022) benchmarks. Our setup closely resembles that of Memorizing
Transformers (Wu et al., 2022). Namely, all our models have 12 layers and 8 attention heads
of dimension 128 each and FFNs with hidden layer 4096.
We set the Infini-attention segment length Nto 2048 for all attention layers and the input
sequence length to 32768 for training. This allows the Infini-attention to unroll over 16 steps
w.r.t its compressive memory states. For the RMT baseline, we performed several runs with
summary prompt lengths 50, 100 and 150 and sequence lengths 4096, 8196 and 32768. RMT
with 100 summary vectors gave the best result when trained on 8196 length sequences.
The main results from the language modeling experiments are summarized in Table 2. Our
Infini-Transformer outperforms both Transformer-XL (Dai et al., 2019) and Memorizing
Transformers (Wu et al., 2022) baselines while maintaining 114x less memory parameters
than the Memorizing Transformer model with a vector retrieval-based KV memory with
length of 65K at its 9thlayer.
100K length training. We further increased the training sequence length to 100K from
32K and trained the models on Arxiv-math dataset. 100K training further decreased the
perplexity score to 2.21 and 2.20 forLinear and Linear +Delta models.
Gating score visualization. Figure 3 visualizes the gating score, sigmoid (β)for the compres-
sive memory for all attention heads in each layer. There are two types of heads emerged in
Infini-attention after training: specialized heads with a gating score near 0 or 1 and mixer
heads with a score close to 0.5. The specialized heads either process contextual information
via the local attention computation or retrieve from the compressive memory whereas the
Zero-shot
32K 128K 256K 512K 1M
Infini-Transformer (Linear) 14/13/98 11/14/100 6/3/100 6/7/99 8/6/98
Infini-Transformer (Linear + Delta) 13/11/99 6/9/99 7/5/99 6/8/97 7/6/97
FT (400 steps)
Infini-Transformer (Linear) 100/100/100 100/100/100 100/100/100 97/99/100 96/94/100
Infini-Transformer (Linear + Delta) 100/100/100 100/100/99 100/100/99 100/100/100 100/100/100
Table 3: Infini-Transformers solved the passkey task with up to 1M context length when
fine-tuned on 5K length inputs. We report token-level retrieval accuracy for passkeys hidden
in a different part ( start/middle/end ) of long inputs with lengths 32K to 1M.
6Preprint. Under review.
Model Rouge-1 Rouge-2 Rouge-L Overall
BART 36.4 7.6 15.3 16.2
BART + Unlimiformer 36.8 8.3 15.7 16.9
PRIMERA 38.6 7.2 15.6 16.3
PRIMERA + Unlimiformer 37.9 8.2 16.3 17.2
Infini-Transformers (Linear) 37.9 8.7 17.6 18.0
Infini-Transformers (Linear + Delta) 40.0 8.8 17.9 18.5
Table 4: 500K length book summarization (BookSum) results. The BART, PRIMERA and
Unlimiformer results are from Bertsch et al. (2024).
mixer heads aggregate both current contextual information and long-term memory content
together into a single output. Interestingly, each layer has at least a single short-range
head, allowing a forward-propagation of input signal up until the output layer. We also
observed an interleaving of long and short-term content retrievals throughout the forward
computation.
3.2 LLM Continual Pre-training
We performed a lightweight continual pre-training for long-context adaptation of existing
LLMs. The pre-training data includes the PG19 and Arxiv-math corpus as well as C4
text (Raffel et al., 2020) with length more than 4K tokens. The segment length Nwas set to
2K throughout our experiments.
1M passkey retrieval benchmark. We replaced the vanilla MHA in a 1B LLM with Infini-
attention and continued to pre-train on inputs with length of 4K. The model was trained for
30K steps with batch size of 64 before fine-tuning on the passkey retrieval task (Mohtashami
& Jaggi, 2024).
The passkey task hides a random number into a long text and asks it back at the model
output. The length of the distraction text is varied by repeating a text chunk multiple times.
The previous work (Chen et al., 2023a) showed that a 8B LLaMA model can solve the task up
to 32K length when fine-tuned with the same 32K length inputs with Position Interpolation.
We take this challenge further and fine-tune on only 5K length inputs to test on 1M length
regime.
Input lengthRouge overall score
17181920
16K 32K 64K 128K 256K 500K
Figure 4: Infini-Transformers obtain better
Rouge overall scores with more book text pro-
vided as input.Table 3 reports the token-level accuracy for
test subsets with input lengths ranging from
32K to 1M. For each test subset, we con-
trolled the position of the passkey so that it
is either located around the beginning, mid-
dle or the end of the input sequence. We
reported both zero-shot accuracy and fine-
tuning accuracy. Infini-Transformers solved
the task with up to 1M context length af-
ter fine-tuning on 5K length inputs for 400
steps.
500K length book summarization (Book-
Sum). We further scaled our approach by
continuously pre-training a 8B LLM model
with 8K input length for 30K steps. We then
fine-tuned on a book summarization task,
BookSum (Kry scinski et al., 2021) where the
goal is to generate a summary of an entire
book text.
We set the input length to 32K for fine-tuning and increase to 500K for evaluating. We use a
generation temperature of 0.5 and top p=0.95 and set the number of decoding steps to 1024
to generate a summary of each book.
7Preprint. Under review.
Table 4 compares our model against the encoder-decoder models that were built particularly
for the summarization task (Lewis et al., 2019; Xiao et al., 2021) and their retrieval-based
long-context extension (Bertsch et al., 2024). Our model outperforms the previous best
results and achieves a new SOTA on BookSum by processing the entire text from book. We
have also plotted the overall Rouge score on validation split of BookSum data in Figure 4.
There is a clear trend showing that with more text provided as input from books, Our
Infini-Transformers improves its summarization performance metric.
4 Related Work
Compressive memory. Inspired by the plasticity in biological neurons (Munkhdalai & Yu,
2017a; Miconi et al., 2018), compressive memory approaches cast parameterized functions
as memory to store and retrieve information (Hinton & Plaut, 1987; Schmidhuber, 1992; Ba
et al., 2016; Munkhdalai et al., 2019). Unlike the Transformer KV memory array (Vaswani
et al., 2017; Wu et al., 2022), which grows with input sequence length, compressive memory
systems maintain a constant number of memory parameters for computational efficiency.
The parameters are modified with an update rule to store information, which is then
retrieved via a memory reading mechanism (Graves et al., 2014; Sukhbaatar et al., 2015;
Munkhdalai & Yu, 2017b).
Compressed input representations can be viewed as a summary of past sequence seg-
ments (Rae et al., 2019; Chevalier et al., 2023). Along this direction, more recent works
have been utilizing a Transformer LLM itself to compress input sequence for efficient long-
context modeling (Bulatov et al., 2022; Chevalier et al., 2023; Ge et al., 2023; Mu et al.,
2024). However, the previous segment-level compression methods, including Compressive
Transformers (Rae et al., 2019) still discard the memory entries of old segments in order to
free up space for the new ones, limiting their context window to the most recent segments.
This is in contrast to our Infini-attention that computes incremental memory updates to a
fixed amount of memory parameters in a recurrent fashion.
Long-context continual pre-training. There is a line of work that extends the do-product
attention layers and continues to train LLMs for long-context (Xiong et al., 2023; Fu et al.,
2024). The attention extensions include incorporating sparsity into the attention layer (Chen
et al., 2023b; Ratner et al., 2022; Mohtashami & Jaggi, 2024) as well as manipulating the
position encodings (Chen et al., 2023a; Peng et al., 2023) Although the position encoding-
based methods such as position interpolation techniques (Chen et al., 2023a) can be data
efficient as they only adjust the positional bias in the attention layer, they are still costly for
inference.
The attention mechanism is also prone to the issues of attention sink (Xiao et al., 2023) and
lost-in-the-middle (Liu et al., 2024). Consequently, they struggle in a regime where context
length is longer than what was observed during training (Press et al., 2021; Kazemnejad
et al., 2024). The proposed Infini-attention addresses those issues by enabling a segment-
level streaming computation over long sequences with a fixed local attention window. Our
Infini-Transformers successfully extrapolate to 1M input length regimes when trained on
32K and even 5K length sequences.
Efficient attention. The efficient attention techniques attempt to improve the efficiency of
the dot-product attention with an approximation or a system-level optimization. Multiple
directions have been explored for different forms of efficient attention approximation,
including sparsity-based (Child et al., 2019; Beltagy et al., 2020; Sukhbaatar et al., 2021;
Ding et al., 2023) and linear attention approximation (Shen et al., 2018; Katharopoulos et al.,
2020; Schlag et al., 2021). Among those, the linear attention variants are closely related
to the associative memory matrix (Schlag et al., 2020; 2021) and the metalearned neural
memory (Munkhdalai et al., 2019), where KV bindings (Smolensky, 1990) are stored in
Fast-Weights (Hinton & Plaut, 1987; Schmidhuber, 1992; Ba et al., 2016) that are modified
in with respect to new contextual information. More recently, system-level optimization
techniques have been proposed by leveraging specific hardware architecture to make the
exact attention computation more efficient (Dao et al., 2022; Liu et al., 2023).
8Preprint. Under review.
5 Conclusion
An effective memory system is crucial not just for comprehending long contexts with LLMs,
but also for reasoning, planning, continual adaptation for fresh knowledge, and even for
learning how to learn. This work introduces a close integration of compressive memory mod-
ule into the vanilla dot-product attention layer. This subtle but critical modification to the
attention layer enables LLMs to process infinitely long contexts with bounded memory and
computation resources. We show that our approach can naturally scale to a million length
regime of input sequences, while outperforming the baselines on long-context language
modeling benchmark and book summarization tasks. We also demonstrate a promising
length generalization capability of our approach. 1B model that was fine-tuned on up to 5K
sequence length passkey instances solved the 1M length problem.
References
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre
Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2
technical report. arXiv preprint arXiv:2305.10403 , 2023.
Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using
fast weights to attend to the recent past. Advances in neural information processing systems ,
29, 2016.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by
jointly learning to align and translate. arXiv preprint arXiv:1409.0473 , 2014.
Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document trans-
former. arXiv preprint arXiv:2004.05150 , 2020.
Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew Gormley. Unlimiformer: Long-
range transformers with unlimited length input. Advances in Neural Information Processing
Systems , 36, 2024.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. Advances in neural information processing systems ,
33:18771901, 2020.
Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer. Advances
in Neural Information Processing Systems , 35:1107911091, 2022.
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending con-
text window of large language models via positional interpolation. arXiv preprint
arXiv:2306.15595 , 2023a.
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.
Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint
arXiv:2309.12307 , 2023b.
Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for
machine reading. arXiv preprint arXiv:1601.06733 , 2016.
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language
models to compress contexts. arXiv preprint arXiv:2305.14788 , 2023.
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with
sparse transformers. arXiv preprint arXiv:1904.10509 , 2019.
Djork-Arn e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep
network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289 , 2015.
9Preprint. Under review.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhut-
dinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv
preprint arXiv:1901.02860 , 2019.
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R e. Flashattention: Fast
and memory-efficient exact attention with io-awareness. Advances in Neural Information
Processing Systems , 35:1634416359, 2022.
Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang,
Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens.
arXiv preprint arXiv:2307.02486 , 2023.
Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and
Hao Peng. Data engineering for scaling language models to 128k context. arXiv preprint
arXiv:2402.10171 , 2024.
Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context
compression in a large language model. arXiv preprint arXiv:2307.06945 , 2023.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint
arXiv:1410.5401 , 2014.
Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord,
Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Acceler-
ating the science of language models. arXiv preprint arXiv:2402.00838 , 2024.
Donald Olding Hebb. The organization of behavior: A neuropsychological theory . Psychology
press, 2005.
Geoffrey E Hinton and David C Plaut. Using fast weights to deblur old memories. In
Proceedings of the ninth annual conference of the Cognitive Science Society , pp. 177186, 1987.
John J Hopfield. Neural networks and physical systems with emergent collective computa-
tional abilities. Proceedings of the national academy of sciences , 79(8):25542558, 1982.
Pentti Kanerva. Sparse distributed memory . MIT press, 1988.
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran c ois Fleuret. Transformers
are rnns: Fast autoregressive transformers with linear attention. In International conference
on machine learning , pp. 51565165. PMLR, 2020.
Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and
Siva Reddy. The impact of positional encoding on length generalization in transformers.
Advances in Neural Information Processing Systems , 36, 2024.
Wojciech Kry scinski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir
Radev. Booksum: A collection of datasets for long-form narrative summarization. arXiv
preprint arXiv:2105.08209 , 2021.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence
pre-training for natural language generation, translation, and comprehension. arXiv
preprint arXiv:1910.13461 , 2019.
Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for
near-infinite context. arXiv preprint arXiv:2310.01889 , 2023.
Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,
and Percy Liang. Lost in the middle: How language models use long contexts. Transactions
of the Association for Computational Linguistics , 12:157173, 2024.
Thomas Miconi, Kenneth Stanley, and Jeff Clune. Differentiable plasticity: training plastic
neural networks with backpropagation. In International Conference on Machine Learning ,
pp. 35593568. PMLR, 2018.
10Preprint. Under review.
Amirkeivan Mohtashami and Martin Jaggi. Random-access infinite context length for
transformers. Advances in Neural Information Processing Systems , 36, 2024.
Jesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with gist tokens.
Advances in Neural Information Processing Systems , 36, 2024.
Tsendsuren Munkhdalai and Hong Yu. Meta networks. In International conference on machine
learning , pp. 25542563. PMLR, 2017a.
Tsendsuren Munkhdalai and Hong Yu. Neural semantic encoders. In Proceedings of the
conference. Association for Computational Linguistics. Meeting , volume 1, pp. 397. NIH Public
Access, 2017b.
Tsendsuren Munkhdalai, John P Lalor, and Hong Yu. Citation analysis with neural attention
models. In Proceedings of the Seventh International Workshop on Health Text Mining and
Information Analysis , pp. 6977, 2016.
Tsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang, and Adam Trischler. Metalearned
neural memory. Advances in Neural Information Processing Systems , 32, 2019.
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context
window extension of large language models. arXiv preprint arXiv:2309.00071 , 2023.
Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury,
Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling trans-
former inference. Proceedings of Machine Learning and Systems , 5, 2023.
Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear
biases enables input length extrapolation. arXiv preprint arXiv:2108.12409 , 2021.
Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive
transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507 , 2019.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a
unified text-to-text transformer. The Journal of Machine Learning Research , 21(1):54855551,
2020.
Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Omri Abend, Ehud Karpas, Amnon
Shashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows improve
in-context learning of large language models. arXiv preprint arXiv:2212.10947 , 2022.
Imanol Schlag, Paul Smolensky, Roland Fernandez, Nebojsa Jojic, J urgen Schmidhuber,
and Jianfeng Gao. Enhancing the transformer with explicit relational encoding for math
problem solving. arXiv preprint arXiv:1910.06611 , 2019.
Imanol Schlag, Tsendsuren Munkhdalai, and J urgen Schmidhuber. Learning associative
inference using fast weight memory. arXiv preprint arXiv:2011.07831 , 2020.
Imanol Schlag, Kazuki Irie, and J urgen Schmidhuber. Linear transformers are secretly
fast weight programmers. In International Conference on Machine Learning , pp. 93559366.
PMLR, 2021.
Jurgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic
recurrent networks. Neural Computation , 4(1):131139, 1992.
Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear
memory cost. In International Conference on Machine Learning , pp. 45964604. PMLR, 2018.
Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient
attention: Attention with linear complexities. arXiv preprint arXiv:1812.01243 , 2018.
Paul Smolensky. Tensor product variable binding and the representation of symbolic
structures in connectionist systems. Artificial intelligence , 46(1-2):159216, 1990.
11Preprint. Under review.
Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks.
Advances in neural information processing systems , 28, 2015.
Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston,
and Angela Fan. Not all memories are created equal: Learning to forget by expiring. In
International Conference on Machine Learning , pp. 99029912. PMLR, 2021.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:
Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-
tion processing systems , 30, 2017.
Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing
transformers. arXiv preprint arXiv:2203.08913 , 2022.
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient stream-
ing language models with attention sinks. arXiv preprint arXiv:2309.17453 , 2023.
Wen Xiao, Iz Beltagy, Giuseppe Carenini, and Arman Cohan. Primera: Pyramid-
based masked sentence pre-training for multi-document summarization. arXiv preprint
arXiv:2110.08499 , 2021.
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis
Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective
long-context scaling of foundation models. arXiv preprint arXiv:2309.16039 , 2023.
A Additional Training Details
For the long-context language modeling task, we set the learning rate to 0.01 by perform-
ing small search over values of 0.003, 0.005, 0.01 and 0.03. We used the Adafactor opti-
mizer (Shazeer & Stern, 2018) with linear warmup with 1000 steps, followed by cosine
decay. We applied gradient checkpointing after each segment to save to save memory. The
batch size was set to 64. For the LLM experiments, we set the learning rate to 0.0001 during
continual pre-training and task fine-tuning.
B Passkey Retrieval Task
Below we showed the input format of the passkey task.
There is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I
will quiz you about the important information there. The grass is green. The sky is blue. The sun
is yellow. Here we go. There and back again. (repeat x times) The pass key is 9054 . Remember
it.9054 is the pass key. The grass is green. The sky is blue. The sun is yellow. Here we go.
There and ack again. (repeat y times) What is the pass key? The pass key is
12
  NEURAL AUDIO FINGERPRINT FOR HIGH-SPECIFIC AUDIO RETRIEV AL
BASED ON CONTRASTIVE LEARNING
Sungkyun Chang1, Donmoon Lee1,2, Jeongsoo Park1, Hyungui Lim1,
Kyogu Lee2, Karam Ko3, and Yoonchang Han1
1Cochlear.ai,2Seoul National University,3SK Telecom
ABSTRACT
Most of existing audio ﬁngerprinting systems have limitations to be
used for high-speciﬁc audio retrieval at scale. In this work, we gen-
erate a low-dimensional representation from a short unit segment of
audio, and couple this ﬁngerprint with a fast maximum inner-product
search. To this end, we present a contrastive learning framework
that derives from the segment-level search objective. Each update in
training uses a batch consisting of a set of pseudo labels, randomly
selected original samples, and their augmented replicas. These repli-
cas can simulate the degrading effects on original audio signals by
applying small time offsets and various types of distortions, such
as background noise and room/microphone impulse responses. In
the segment-level search task, where the conventional audio ﬁnger-
printing systems used to fail, our system using 10x smaller storage
has shown promising results. Our code and dataset are available at
https://mimbres.github.io/neural-audio-fp/ .
Index Terms acoustic ﬁngerprint, self-supervised learning,
data augmentation, music information retrieval
1. INTRODUCTION
Audio ﬁngerprinting is a content summarization technique that links
short snippets of unlabeled audio contents to the same contents in the
database [1]. The most well-known application is the music ﬁnger-
printing system [17] that enables users to identify unknown songs
from the microphone or streaming audio input. Other applications
include detecting copyrights [3], deleting duplicated contents [8],
monitoring broadcasts [1, 9], and tracking advertisements [10].
General requirements for audio ﬁngerprinting system are dis-
criminability over a huge number of other ﬁngerprints, robustness
against various types of acoustic distortions, and computational efﬁ-
ciency for processing large-scale database. To achieve these require-
ments, most of conventional approaches [16, 11] employed a nov-
elty function to extract sparse representations of spectro-temporal
features from a pre-deﬁned audio window. These sparse represen-
tations, or acoustic landmarks [5], used to be coupled with binary
hashing algorithms [1, 2, 12] for scalable search in hamming space.
Still, the representation learning approach to audio ﬁngerprint-
ing has not been discovered well. Now-playing [7] has been pio-
neering work in the direction. They trained a neural network using
semi-hard triplet loss, which derived from face recognition [13]. In
their setup [7], Now-playing could identify songs within 44 h long
audio database. In our benchmark, we replicate this semi-hard triplet
approach and compare it with our work in a new setup: high-speciﬁc
audio retrieval in a 180 times larger database.
We present a neural audio ﬁngerprinter for robust high-speciﬁc
audio retrieval based on contrastive learning. Our ﬁngerprinting
model in Figure 1 differs from the prior works in three key aspects:
Fig. 1 . Overview of the neural audio ﬁngerprinter. We generate
segment-wise embeddings zt2Z that can represent a unit segment
of audio from the acoustic features Sat time step t. In our frame-
work, each segment can be searched by maximum inner-product.
 Prior works [17, 11] have focused on song-level audio retrieval
from a music excerpt; we challenge a high-speciﬁc audio search
by allowing miss-match less than 250 ms from a few seconds in-
put.
 We introduce the contrastive learning framework for simulating
maximum inner-product search (MIPS) in mini-batch.
 We employ various types of data augmentation methods for gen-
erating acoustic distractors and show their beneﬁts to training a
robust neural audio ﬁngerprinter.
2. NEURAL AUDIO FINGERPRINTER
Our neural audio ﬁngerprinter in Figure 1 transforms and maps the
segment-level acoustic features into L2-normalized space, where the
inner-product can measure similarities between segments. It consists
of a pre-processor and neural networks.
As a ﬁrst step, input audio Xis converted to time-frequency
representationS. It is then fed into convolutional encoder f(:)
which is based on the previous study [7]. Finally, L2-normalization
is applied to its output through a linear projection layer g(:). Thus,
we employgf:S7!Zdas a segment-wise encoder that trans-
formsSinto d-dimensional ﬁngerprint embedding space Zd. The
d-dimensional output space Zdalways belongs to Hilbert space
L2(Rd): the cosine similarity of a pair unit such as cos (za;zb)be-arXiv:2010.11910v4  [cs.SD]  10 Feb 2021Fig. 2 . Illustration of the contrastive prediction task in Section 2.1.
(left) Batch size N= 6. We prepare N=2pairs of original/replica.
The same shapes with solid/dashed lines represent the positive pair
of original/replica, respectively. (right) Each element in the matrix
represents pairwise similarity. In each row, a prediction task can
be deﬁned as classifying a positive pair (one of the orange squares)
against the negative pairs (green or purple squares) in the same row.
comes inner-product zT
azb, and due to its simplicity, L2projection
has been widely adopted in metric learning studies [7, 13, 14].
Thegf(:)described here can be interpreted as a reorganization
of the previous audio ﬁngerprinting networks [7] into the common
form employed in self-supervised learning (SSL) [1417]. However,
our approach differs from the typical SSL that throws g(:)away be-
fore ﬁne-tuning for the target task: we maintain the self-supervised
g(:)up to the ﬁnal target task.
2.1. Contrastive learning framework
As mentioned earlier, we can use the inner-product as a measure of
similarity between zt2 Zdfor any time step t. Without losing
generality, searching the most similar point (*) of database V = fvig
for a given query qinZdspace can be formulated as maximum inner
product search (MIPS), v
i:=arg maxi(q>vi).
We simulate MIPS in a mini-batch setup that takes into account
various acoustic distortions and input frame mismatches occurring
in the ﬁngerprint task. A mini-batch with the size of Nconsists of
N=2pairs offsorg;srepg.sorgis the time-frequency representation
of sampled audio and srepis the augmented replica of sorg, where
srep=M(sorg).Mis an ordered augmentation chain that con-
sists of multiple augmentors with the random parameter set for
each replica. In this conﬁguration, the indices of original examples
are always odd, and that of replicas are even. Therefore, the batch-
wise output of fg(s)can befzorg
2k1;zrep
2kg2=N
k=1.
We give each k-th example a chance to be an anchor (or a query
in MIPS) to be compared with all other examples excluding itself in
the batch. We calculate the pairwise inner-product matrix between
all elements in the batch fzigN
i=1asa(i;j) =zT
izjfor8i;j2
f1;2;:::;Ngas Figure 2. Then, we deﬁne the contrastive prediction
task for a positive pair of examples (i;j)as:
(i;j) =logexp(ai;j=)PN
k=11(k6=i)exp(ai;j=): (1)
1(:)2f0;1gis an indicator function that returns 1iff(:)is true,
and >0denotes the temperature [18] parameter for softmax. We
employ Equation 1 to replace MIPS from the property: computing
the top-k(k=1 in our setup) predictions in the softmax function isAlgorithm 1: Training of neural audio ﬁngerprinter
Conﬁg: even number of batch size N, temperature 
Variables: inputs, representation z2Rd
Augmentor:M(:)with parameters 
Nets: encoderf(:),L2projection layer g(:)
1foreach sampled mini-batch fskgN=2
k=1do
2 for8k2f1;:::;N= 2gdo
3zorg
k=gf(sk)
4zrep
k=gf(M(sk))
5z=fzorg
1;zrep
1;:::;zorg
N=2;zrep
N=2g
6 for8i2f1;:::;Ngand8j2f1;:::;Ngdo
7ai;j=z>
izj /*Pairwise similarity */
8(i;j) =NTxent (ai;j;) /*Eq.(1)*/
9 Updatef;gto minimizeL1
NNX
i=1/*Eq.(2)*/
10return ﬁngerprinter gf(:)
equivalent to the MIPS. A similar approach is found in [19]. The
total lossLaverageslacross all positive pairs, both (i;j)and(j;i):
L=1
NNX
k=1[(2k1;2k);(2k;2k1)]: (2)
Updating rules are summarized in Algorithm 1.
It is worth comparing our approach to SimCLR [14] for visual
representation. Our approach differs from SimCLR on how to con-
struct positive pairs. We use foriginal, replicag, whereas SimCLR
usesfreplica, replicagfrom the same original source. In our case,
the anchor is already given because the database will always store
the clean source, so it can be more important to learn the consistent
relation between the original and its replica over all other negatives.
2.2. Sequence search
Our model trained by simulating MIPS is optimized for segment-
level search. In the case of searching for a query sequence fQL
i=0g
consisting of Lconsecutive segments: We ﬁrst gather the top k
segment-level search results indices Iqifor eachqifrom the DB.
The offset is then compensated by I0
qi=Iqii. The set of candi-
date indices c2Cis determined by taking unique elements of I0
qi.
The sequence-level similarity score is the sum of all segment-level
similarities from the segment index range [c;c+L], and the index
with the highest score is the output of the system.
3. EXPERIMENTAL SETUP
3.1. Dataset
The main experiment in Table 3 is reproduceable with the following
three data sets, which are isolated from each other.
 Train (10K-30s): A subset of the fmamedium [20] consisting of
30 s audio clips from a total of 10K songs.
 Test-Dummy-DB (100K-full-db): a subset of the fmafull [20]
consisting of about 278 s audio clips from a total of 100K songs.
We scale the search experiment with this.
 Test-Query/DB (500-30s): Test-DB is another subset of the
fmamedium , which is 500 audio clips of 30 s each. Test-Query
was synthesized using Test-DB as directed in Section 3.5.Table 1 . Fingerprinter (FP) network structure in Section 3.3.
SCo i
ks(:) :=ReLUCLNCCo i
k0s0CReLUCLNCCo i
ks(:)
f(:) :=SCh h
32CSCh 4d
32CSC4d 4d
32CSC4d 2d
32C
SC2d 2d
32CSC2d d
32CSCd d
32CSCd 1
32(:)
g(:) :=L2CConcat CC1 u
11CELUCCu v
11CSplith=d(:)
FP:=gCf(input :=st)
3.2. Data pipeline with augmentation chain
A batch consists of fxorg;xrepgpairs. Eachxrepis generated from its
corresponding xorgthrough augmentation steps as following order:
 Time offset modulation: To simulate possible discrepancies in real
world search scenarios, we deﬁne positive examples as 1 s audio
clips with an offset of up to 200 ms. We ﬁrst sample 1.2 s of
audio and thenfxorg;xrepgare chosen by random start positions.
 Background mixing: A randomly selected noise in the SNR range
of [0, 10] dB is added to the audio to reﬂect the actual noise.
The noise dataset consists of 4.3 h of a subset of AudioSet [21]
and 2.3 h of pub and cafe noise recorded by us. The AudioSet
was crawled within subway ,metro , and underground tags with no
music-related tags. Each dataset is split into 8:2 for train/test.
 IR ﬁlters: To simulate the effect of diverse spatial and microphone
environments, microphone and room impulse response (IR) are
sequentially applied by convolution operation. Public microphone
[22] and spacial [23] IR dataset are split into 8:2 for train/test.
 Cutout [24] and Spec-augment [25] are applied after extracting
log-power Mel-spectrogram features, such that fsorg;srepg. Un-
like other augmentations, we uniformly apply a batch-wise ran-
dom mask to all examples in a batch including sorg. The size and
position of each rectangle/vertical/horizontal mask is random in
the range [1/10, 1/2] the length of each time/frequency axis.
3.3. Network structure
In Table 1, a space-saving notation Co i
ksdenotes Conv2d with input
channeli, output channel o, kernel size 1k, and stride 1s. The
k0ands0denote rotation as k1ands1.Splith=dsplits input
dimensionhintodparts of each output dimension v=h=d.gCf(:)
isg(f(:)). The network parameters fd;h;u;vgare in Table 2.
Convolutional encoder f(:):f(:)takes as input a log-power
Mel-spectrogram stwith a time step trepresnting 1s audio cap-
tured by 50% overlapping window. f(:)consists of several blocks
containing spatially separable convolution (SC) [26] followed by
a layer normalization (LN) [27] and a ReLU activation.
L2projection layer g(:): We take the split-head from the input
embeddings and pass it through the separate Linear-ELU-Linear
layers for each split as in previous studies [7, 28]. After concate-
nating the multi-head outputs, we apply L2-normalization.
3.4. Implementation details
The replication of Now-playing and our work shared the short-time
Fourier transform (STFT) settings listed in in Table 2. Note that, due
to ambiguity in the previous study [7], the STFT parameters were
set by us. We trained Now-playing using online semi-hard triplet
loss [13] with the margin m= 0:4and batch size N= 320 .Table 2 . Shared conﬁgurations for experiments
Parameter Value
Sampling rate 8,000 Hz
STFT window function Hann
STFT window length and hop 1024, 256
STFT spectrogram size FT 512T(T= 32)
log-power Mel-spectrogram size F0T 256T(T= 32)
Dynamic range 80 dB
Frequencyfmin, maxg f 300, 4,000gHz
Fingerprintfwindow length, hopg f 1s;0:5sgorf2s;1sg
Fingerprint dimension d 64 or 128
Network parameters fh; u; vg f 1024;32; h=dg
Batch size N 120 or 320 or 640
We trained our model using LAMB [29] optimizer, which per-
formed 2 pp better than Adam [30] with the 3 s query sequence for
batch sizeN320. In practice, Adam worked better only for
N240. The learning rate had an initial value of 1e-4 N=640with
cosine decay without warmup [31] or restarts [32], then it reached a
minimum value of 1-e7 in 100 epochs. The temperature in Eq.1 was
= 0:05, and we did not observe a meaningful performance change
in the range [0:01;0:1]. The training ﬁnished in about 30 h with a
single NVIDIA RTX 6000 GPU or v3-8 Cloud TPUs.
The search algorithm in Section 2.2 was implemented using an
open library [33]. We used the inverted ﬁle (IVF) index structure
with product quantizer (PQ) as a non-exhaustive MIPS. The IVF-PQ
had 200 centroids with the code size of 26, and 8-bits per index. In
this setting, the loss of recall remained below 0.1% compared to the
exhaustive search of 100K songs ( 56M segments) database.
3.5. Evaluation protocol
 Evaluation metric: To measure the performance in segment/song-
level search in Section 4, we use Top-1 hit rate(%) :
100(n of hits @Top-1)
(n of hits @Top-1) + (n of miss @Top-1 ); (3)
which is equivalent to recall . In Table 3, exact match is the case
when the system ﬁnds the correct index in database. We further
deﬁne the tolerance range for near match as500 ms.
 Test-Query generation: 2K query-sources for each f1, 2, 3, 5, 6,
10gs length are randomly cropped from Test-DB containing 500
clips of 30s each. Each query is synthesized through the random
augmentation pipeline as described in Section 3.2. Note that we
exclude Cutout and Spec-augment. The default SNR range is [0,
10] dB. We make sure that the data used for background mixing
and IR as unseen to our model by isolating them from training set.
4. RESULTS AND DISCUSSION
4.1. Experimental results
The main results are listed in Table 3. Using the same augmentation
method, Now-playing [7] based on semi-hard triplet [13] took 2 s as
a unit audio segment. The modiﬁed Now-playing with 1 s unit audio
segment could be more fairly compared with our works.
VS.Now-playing (semi-hard triplet) Modiﬁed Now-playing con-
sistently performed better than the replicated Now-playing . While
cutting the dimension in half, this trend was maintained. Consider-
ing that the DB size was the same when the number of ﬁngerprint
dimensions was half, it could be seen that constructing DB with 1Table 3 . Top-1 hit rate (%) of large-scale (total of 100K songs)
segment-level search. ddenotes the dimension of ﬁngerprint em-
bedding. exact match means that our system ﬁnds the exact index.
near match means a mismatch within 1 index or500 ms.
Method d matchQuery length in seconds
1 s 2 s 3 s 5 s 6 s 10 s
Now-playing
(replicated)128exact - 44.3 60.1 73.6 81.0 86.1
near - 46.8 63.5 75.2 81.6 86.3
Now-playing
(modiﬁed
for 1 s unit)64exact 25.8 58.5 69.3 78.5 81.4 87.7
near 30.9 61.3 71.2 79.5 82.2 88.3
128exact 26.3 58.2 69.5 78.4 81.4 87.8
near 30.9 61.1 71.8 79.8 83.0 89.2
This work
(N=640)64exact 54.6 78.9 85.4 90.4 92.0 94.9
near 61.3 81.7 86.7 90.9 92.7 95.1
128exact 62.2 83.2 87.4 92.0 93.3 95.6
near 68.3 84.9 88.7 92.7 94.1 95.8
This work
(N=320)128exact 61.0 82.2 87.1 91.8 93.1 95.2
near 67.1 84.1 88.1 92.5 93.9 95.5
This work
(N=120)128exact 55.9 78.8 84.9 90.9 92.2 95.3
near 62.3 80.9 86.3 91.5 92.8 95.5
This work
(no aug.)128exact 0.0 0.0 0.0 0.0 0.0 0.0
near 0.0 0.0 0.0 0.0 0.0 0.0
Table 4 . Effect of ﬁngerprint dimension din 1 s segment search.
Embedding dimension d=16 d=32 d=64 d=128
Top-1 hit rate@1 s (%) 11.6 40.2 54.6 62.2
s was more advantageous to segment search. The proposed model
with a 128-dimensional ﬁngerprint using batch size of 640 always
showed the best performance (highlighted in Table 3) for any query
length. This conﬁrmed that the proposed contrastive learning ap-
proach outperformed over the semi-hard triplet approach.
Embedding dimension In Table 2, increasing the embedding di-
mensiond: 64!128 for the modiﬁed Now-playing did not affect the
results signiﬁcantly. In contrast, increasing the embedding dimen-
siond: 64!128 for our best model gave us a larger improvement of
exact match performance as 7.6 (54.6!62.2%) pp for the 1 s query.
This reafﬁrmed the training beneﬁts of our contrastive learning over
the semi-hard triplet, fairly compared using the same network struc-
ture. In Table 4, we further investigated the effect of reducing dto
our model with 1 s query length. We could observed a rapid drop in
exact match performance while decreasing d: 64!32!16.
Performance of sequence search The longer the query sequence,
the better the performance in all experiments. In Table 3, segment-
level hit rate of our best model (highlighted) was increasing as
62.2!83.4!92.0!95.6% while increasing the query length by
almost double. Thus, the longer query length was useful. In Table 3,
the performance difference between near and exact match result of
our best model at 1 s query was 6.1 (62.2 and 68.3%) pp. This inter-
val decreased immediately as the query length became larger than 1.
These results showed that our sequence search method introduced
in Section 2.2 was quite effective.
Effect of batch size The larger the batch size, the better the per-
formance in all experiments. In Table 3, reducing the batch size
N: 640!120 from our best model degraded the exact match per-
formance by#6.3 (62.2!55.9%) pp at 1 s query length. Recent
works [14, 16, 17] on contrastive learning has been consistently re-
porting similar trends. Our result implicated that the diversity of
negative examples existing by large batch could play an important
role in the contrastive learning framework.VS.Dejavu We compared our work with the open-source project
Dejavu [34] based on the conventional method [1, 5] in the song-
level search task of smaller (10K-30s) scale. 69.6% of Top-1 hit rate
was achieved with Dejavu , a song-level search engine using a 6 s
query. Our best model achieved 99.5% for song-level hit rate, and
exact/near match was 98.9/99.1% at the 6 s query, respectively. Our
model also achieved f83.6, 95.4, 97.4g% exact match atf1,2,3gs
query. The capacity of ﬁngerprints from Dejavu was about 400
MB, while ours (quantized with 1/4 compression rate) was less than
40 MB ford=64. These results suggest that our method has advan-
tages over conventional methods in both performance and scalability.
4.2. Size of training set, search time and scalability
The models in Table 3 were trained with about 70 h dataset. This
size was less than 1% of the total 8K h DB for test. We assumed that
using the entire DB for training would be impracticala huge num-
ber of new songs are produced every day. In additional experiment,
we used 10% of the Test-dummy-DB for training a d=64 model. It
achievedf58.3, 81.1, 86.5, 92.4, 93.4, 96.0 g% of Top-1 hit rate for
the query sequence of f1, 2, 3, 5, 6 ,10gs. This improved3.7
(54.6!58.3%) pp at the 1 s query over the best model with d=64 in
Table 3, still lower than the result of d=128. Thus, both dand the
amount of training data were the factors affecting performance.
In our best model with d=128, the ﬁnal DB size was about 5.8
GB for 56M segments from total of 100K songs. We report about
1.5 s search time with i9-10980XE CPU (in-memory-search), and
0.02 s with GPU for parallel search of 19 segments (= 10 s query).
In case of using CPUs, we could observe on-disk-search using the
latest SSD with CPU was only twice as slow as in-memory-search.
We reserve the industry-level scalability issues for future work.
4.3. Transfer to down-stream task
We further investigated the generality of the learned embeddings by
performing a downstream task, as in the typical SSL [1417] set-
tings. By ﬁxing f(:)and ﬁne-tuning a linear classiﬁer, we tried audio
genre classiﬁcation in GTZAN dataset with stratiﬁed 10-fold cross-
validation. Fine-tuning on the pre-trained embeddings for ﬁngerprint
achieved 59.2% accuracy, while training from scratch achieved only
32.0%. This showed that the features encoded by f(:)were linearly
interpretable, consistent with other SSL reports [1417]. However,
our result was slightly lower than the baseline of 61.0% accuracy us-
ing MFCCs+GMM [35]. This might be due to the limitation of the
lightweight networks with the relatively short-time analysis window.
5. CONCLUSIONS AND FUTURE WORK
This study presented a neural audio ﬁngerprinter for high-speciﬁc
audio retrieval. Our model was trained to maximize the inner-
product between positive pairs of ﬁngerprints through a contrastive
prediction task. To this end, we explicitly sampled positive pairs to
have originalreplica relations by applying various augmentations to
clean signals. We evaluated our model in the segment-level search
task with a public database of 100K songs. In the experiment, our
model performed better than the model with triplet embeddings. It
was also shown that our work, using 10 times less memory than an
existing work, outperformed in song-level search task. So far, these
results have implied that the audio ﬁngerprinting task would inher-
ently have self-supervised learning potentials. The future direction
of this study is to test neural audio ﬁngerprints in industry-scale
database and queries from a variety of user devices.6. ACKNOWLEDGEMENT
We would like to thank the TensorFlow Research Cloud (TFRC) pro-
gram that gave us access to Google Cloud TPUs.
7. REFERENCES
[1] J. Haitsma and T. Kalker, A highly robust audio ﬁngerprinting
system., in Proc. of the Int. Society for Music Information
Retrieval (ISMIR) , 2002, vol. 2002, pp. 107115.
[2] A. Wang et al., An industrial strength audio search algo-
rithm., in Proc. of the Int. Society for Music Information Re-
trieval (ISMIR) , 2003, vol. 2003, pp. 713.
[3] P. Cano, E. Batlle, T. Kalker, et al., A review of audio ﬁnger-
printing, Journal of VLSI signal processing systems for signal,
image and video technology , vol. 41, no. 3, pp. 271284, 2005.
[4] S. Baluja and M. Covell, Waveprint: Efﬁcient wavelet-based
audio ﬁngerprinting, Pattern recognition , vol. 41, no. 11, pp.
34673480, 2008.
[5] C. V . Cotton and D. P. Ellis, Audio ﬁngerprinting to identify
multiple videos of an event, in Proc. of the IEEE Int. Conf.
on Acoustics, Speech and Signal Processing (ICASSP) . IEEE,
2010, pp. 23862389.
[6] T.-K. Hon, L. Wang, J. D. Reiss, and A. Cavallaro, Audio
ﬁngerprinting for multi-device self-localization, IEEE/ACM
Transactions on Audio, Speech, and language processing , vol.
23, no. 10, pp. 16231636, 2015.
[7] B. Gfeller et al., Now playing: Continuous low-power music
recognition, in NeurIPS 2017 Workshop on Machine Learning
on the Phone and other Consumer Devices , 2017.
[8] C. J. Burges, D. Plastina, J. C. Platt, E. Renshaw, and H. S.
Malvar, Using audio ﬁngerprinting for duplicate detection
and thumbnail generation, in Proc. of the IEEE Int. Conf.
on Acoustics, Speech, and Signal Processing (ICASSP) . IEEE,
2005, vol. 3, pp. iii9.
[9] E. Allamanche, Audioid: Towards content-based identiﬁca-
tion of audio material, in Proc. of the 100th AES Conv. , 2001.
[10] Y . Jiang, C. Wu, K. Deng, and Y . Wu, An audio ﬁngerprint-
ing extraction algorithm based on lifting wavelet packet and
improved optimal-basis selection, Multimedia Tools and Ap-
plications , vol. 78, no. 21, pp. 3001130025, 2019.
[11] J. Six and M. Leman, Panako - A Scalable Acoustic Finger-
printing System Handling Time-Scale and Pitch Modiﬁcation,
inProc. of the Int. Society for Music Information Retrieval (IS-
MIR) , 2014, pp. 259264.
[12] A. Gionis, P. Indyk, and R. Motwani, Similarity search in
high dimensions via hashing, in Proc. of the Int. Conf. on Very
Large Data Bases (VLDB) , 1999, VLDB 99, pp. 518529.
[13] F. Schroff, D. Kalenichenko, and J. Philbin, Facenet: A uni-
ﬁed embedding for face recognition and clustering, in Proc.
of the IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR) , 2015, pp. 815823.
[14] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, A simple
framework for contrastive learning of visual representations,
arXiv preprint arXiv:2002.05709 , 2020.
[15] A. v. d. Oord, Y . Li, and O. Vinyals, Representation
learning with contrastive predictive coding, arXiv preprint
arXiv:1807.03748 , 2018.[16] T. Chen, S. Kornblith, K. Swersky, M. Norouzi, and G. Hin-
ton, Big self-supervised models are strong semi-supervised
learners, arXiv preprint arXiv:2006.10029 , 2020.
[17] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, wav2vec
2.0: A framework for self-supervised learning of speech repre-
sentations, arXiv preprint arXiv:2006.11477 , 2020.
[18] G. Hinton, O. Vinyals, and J. Dean, Distilling the knowledge
in a neural network, arXiv preprint arXiv:1503.02531 , 2015.
[19] P. H. Chen, S. Si, S. Kumar, Y . Li, and C.-J. Hsieh, Learning
to screen for fast softmax inference on large vocabulary neural
networks, arXiv preprint arXiv:1810.12406 , 2018.
[20] M. Defferrard, K. Benzi, P. Vandergheynst, and X. Bresson,
Fma: A dataset for music analysis, in Proc. of the Int. Society
for Music Information Retrieval (ISMIR) , 2017.
[21] J. F. Gemmeke, D. P. Ellis, and et al., Audio set: An ontology
and human-labeled dataset for audio events, in Proc. of the
IEEE Int. Conf. on Acoustics, Speech and Signal Processing
(ICASSP) . IEEE, 2017, pp. 776780.
[22] Xaudia, Microphone impulse response project, 2017, [On-
line]. http://micirp.blogspot.com/.
[23] M. Jeub, M. Schafer, and P. Vary, A binaural room impulse
response database for the evaluation of dereverberation algo-
rithms, in Proc. of the Int. Conf. on Digital Signal Processing
(ICDSP) . IEEE, 2009, pp. 15.
[24] T. DeVries and G. W. Taylor, Improved regularization of
convolutional neural networks with cutout, arXiv preprint
arXiv:1708.04552 , 2017.
[25] D. S. Park, W. Chan, et al., Specaugment: A simple data
augmentation method for automatic speech recognition, in
Proc. of the Interspeech , 2019, pp. 26132617.
[26] F. Mamalet and C. Garcia, Simplifying convnets for fast
learning, in Proc. of the Int. Conf. on Artiﬁcial Neural Net-
works (ICANN) . Springer, 2012, pp. 5865.
[27] J. L. Ba, J. R. Kiros, and G. E. Hinton, Layer normalization,
arXiv preprint arXiv:1607.06450 , 2016.
[28] H. Lai, Y . Pan, Y . Liu, and S. Yan, Simultaneous feature learn-
ing and hash coding with deep neural networks, in Proc. of
the IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR) , 2015, pp. 32703278.
[29] Y . You, J. Li, et al., Large batch optimization for deep learn-
ing: Training bert in 76 minutes, in Proc. of the Int. Conf. on
Learning Representations (ICLR) , 2019.
[30] D. P. Kingma and J. Ba, Adam: A method for stochastic opti-
mization, arXiv preprint arXiv:1412.6980 , 2014.
[31] P. Goyal, P. Doll ar, R. Girshick, et al., Accurate, large mini-
batch sgd: Training imagenet in 1 hour, arXiv preprint
arXiv:1706.02677 , 2017.
[32] I. Loshchilov and F. Hutter, Sgdr: Stochastic gradient descent
with warm restarts, arXiv preprint arXiv:1608.03983 , 2016.
[33] J. Johnson, M. Douze, and H. J egou, Billion-scale similarity
search with gpus, IEEE Transactions on Big Data , 2019.
[34] W. Drevo, Dejavu: open-source audio ﬁngerprinting project,
2014, [Online]. https://pypi.org/project/PyDejavu/.
[35] G. Tzanetakis and P. Cook, Musical genre classiﬁcation of
audio signals, IEEE Transactions on speech and audio pro-
cessing , vol. 10, no. 5, pp. 293302, 2002.
  Semantic Segmentation using Vision Transformers:
A survey
Hans Thisankea, Chamli Deshana, Kavindu Chamitha, Sachith
Seneviratneb,c, Rajith Vidanaarachchib,c, Damayanthi Heratha,
aDepartment of Computer Engineering, University of
Peradeniya, Peradeniya, 20400, Sri Lanka
bMelbourne School of Design, University of Melbourne, Parkville, VIC 3010, Australia
cFaculty of Engineering and IT, University of Melbourne, Parkville, VIC 3010, Australia
Abstract
Semantic segmentation has a broad range of applications in a variety of do-
mains including land coverage analysis, autonomous driving, and medical
image analysis. Convolutional neural networks (CNN) and Vision Trans-
formers (ViTs) provide the architecture models for semantic segmentation.
Even though ViTs have proven success in image classication, they cannot
be directly applied to dense prediction tasks such as image segmentation
and object detection since ViT is not a general purpose backbone due to
its patch partitioning scheme. In this survey, we discuss some of the dier-
ent ViT architectures that can be used for semantic segmentation and how
their evolution managed the above-stated challenge. The rise of ViT and
its performance with a high success rate motivated the community to slowly
replace the traditional convolutional neural networks in various computer
vision tasks. This survey aims to review and compare the performances of
ViT architectures designed for semantic segmentation using benchmarking
datasets. This will be worthwhile for the community to yield knowledge re-
garding the implementations carried out in semantic segmentation and to
discover more ecient methodologies using ViTs.
Keywords: vision transformer, semantic segmentation, review, survey,
convolution neural networks, self-supervised learning, deep learning
Corresponding author
Email addresses: e16368@eng.pdn.ac.lk (Hans Thisanke), e16076@eng.pdn.ac.lk
(Chamli Deshan), e16057@eng.pdn.ac.lk (Kavindu Chamith),
sachith.seneviratne@unimelb.edu.au (Sachith Seneviratne),
rajith.vidanaarachchi@unimelb.edu.au (Rajith Vidanaarachchi),
damayanthiherath@eng.pdn.ac.lk (Damayanthi Herath)
Preprint submitted to Engineering Applications of Articial Intelligence May 8, 2023arXiv:2305.03273v1  [cs.CV]  5 May 20231. Introduction
Transformers became the new state-of-the-art in natural language pro-
cessing (NLP) [1] after the tremendous success it achieved. This led to the
development of ViT [2] which was later adapted into the computer vision
tasks such as image classication [2, 3], semantic segmentation [4, 5] and
object detection [6, 7]. A typical Transformer encoder consists of a multi-
head self-attention (MSA) layer, a multi-layer perceptron (MLP), and a layer
norm (LN). The main driving force behind the ViT is the multi-head self-
attention mechanism. It helps ViT to capture long-range dependencies with
less inductive bias [8]. When trained on a sucient amount of data, ViT
shows remarkable performance, beating the performance of state-of-the-art
CNNs [2]. However, ViTs still have some drawbacks compared to CNNs such
as the need for very large datasets. Strategies such as self-supervised based
approaches can be used to alleviate some of these drawbacks and further
enhance ViTs [9].
Semantic segmentation is the process of assigning a class label to each
and every pixel of an image. This requires accurate predictions at the pixel
level. For segmentation, there exist both CNN-based models and Trans-
former based models. However, plain ViT models cannot be directly used
for segmentation tasks because they do not consist of segmentation heads
[10]. Instead SETR [5] and Swin Transformer [4] based architectures can be
utilized for segmentation tasks. Unlike image classication, dense prediction
tasks such as semantic segmentation and object detection come with a few
diculties due to the rich intra-class variation, context variation, occlusion
ambiguities, and low image resolution [11]. There have been many improve-
ments in the ViT domain in the last few years to overcome these challenges
while further developments are still in progress to make them ecient.
The review focuses specically on semantic segmentation using Vision
Transformers. The comparison of the ViT models specialized for semantic
segmentation is discussed with architecture-wise and tabulated specic sets
of model variants that can be compared with the same set of benchmark
datasets. The current surveys performed on ViTs have been structured with
a detailed historical evolution from NLP to the Vision Transformer domain.
[12] focuses on self-attention and its varieties with advantages and limitations
with existing methods for segmentation, object detection, classication, and
action recognition. The comparison follows between CNN and ViT back-
bones on the ImageNet dataset. The survey done by [13] is also considering
various vision tasks and surpasses CNN-based models with experimental re-
2sults on benchmark datasets. Even though several surveys have been done
[12, 13, 14], a comparison between segmentation models with several bench-
mark datasets to identify the best-performing model has not been performed.
In our survey, we provide a set of segmentation models, for each of which we
dene the best variant in each benchmark dataset category. This is useful in
the sense of identifying the most optimal parameters such as patch size, iter-
ations count for each variant of the model. By providing mIoU (%) of model
performance results over several semantic segmentation-related benchmark
datasets, overall evaluation and highest-performing model variants for each
dataset can be identied.
In Section 2 we discuss the applications of semantic segmentation, ViTs,
their challenges, and loss functions. Section 3 describes benchmark datasets
used in semantic segmentation. Section 4 describes the existing work done
in semantic segmentation using ViTs and presents a quantitative analysis.
Finally, Section 5 provides the discussions and Section 6 concludes the paper
with future directions.
2. Semantic Segmentation using Vision Transformers
This section aims to provide an in-depth analysis of the applications in
semantic segmentation, with a focus on recent advancements in ViTs. We
begin by exploring the principles and architecture of ViTs and their potential
for improving semantic segmentation performance. We then delve into vari-
ous application domains of semantic segmentation. We also devote a section
to practical approaches for overcoming the data limitations that often arise
in ViT models. Finally, we discuss various loss functions used in semantic
segmentation and their eectiveness in dierent scenarios.
2.1. Vision Transformers
Automatic segmentation techniques have been evolving and improving
throughout the years with the advancements of deep learning approaches
and the application of semantic segmentation in practical usage. For seman-
tic segmentation, the requirement is to locally identify the dierent classes
in the image with spatial location. For that, the fully connected layers in
the conventional CNN architecture were replaced with fully convolutional
layers combined with feature extraction. This was introduced as Fully Con-
volutional Networks (FCN) [15] to identify high-level semantic features from
images. These networks have shown to be faster compared to previous CNN-
based techniques and are also capable of generating segmentation maps for
images of any resolution. Some of the commonly known architectures are
3U-Net (state-of-the-art FCN) and more improved architectures with higher
accuracy and eciency are developed by [16, 17, 18].
One of the limitations identied with the FCN architecture is the low
resolution of the nal output segmentation image of the feature map due
to going through several convolutional and pooling layers. Furthermore, the
locality property of the FCN-based methods caused limitations to the capture
of long-range dependencies of the feature maps. To solve this, researchers also
looked into attention mechanisms to merge or replace these models. This has
led to trying out Transformer architectures in the computer vision domain
which were successful in NLP.
Self-attention-based architectures have taken priority in NLP by avoiding
the drawbacks such as vanishing gradients in sequence modeling and trans-
duction tasks. Specially designed for sequence modeling and transduction
tasks, Transformers with attention were able to model long-range sequences
of data. When training a NLP model, one of the best ways is to pre-train
on a large text corpus and then ne-tune on a small set of data which is for
the related task. But with deep neural networks, this was a challenging task.
As Transformers have high computational eciency and scalability, it was
easier to train on a large set of data [19].
With the success of using self-attention to enhance the input-output in-
teraction in NLP, works have been proposed to combine convolutional ar-
chitectures with self-attention, especially in object detection and semantic
segmentation where input-output interaction is highly needed [20]. But ap-
plying attention to convolutional architectures demands high computation
power, even though they are theoretically ecient [1].
Considering images, calculating self-attention is quadratic to the image
size as each pixel attends to every other pixel therefore it is a quadratic cost of
the pixel count [2]. Thus [2] proposed to divide the image into a sequence of
patches and treat them as tokens as it was done in NLP. Instead of pixel-wise
attention, patch-wise attention was used in the architecture which helped to
reduce the computational complexity compared to applying self-attention to
convolutional architecture.
This architecture showed promising results by surpassing all the state-
of-the-art convolution-based methods by reaching an accuracy of 88.55% on
ImageNet, 90.72% on ImageNet-ReaL, and 94.55% on CIFAR-100 datasets
[2]. A major characteristic of the ViT is that it needs more data for model
training. Experiments carried out by [2] ensure that with increasing data
size, ViT performs well.
4Figure 1: Architecture of the Vision Transformer. The model splits an image into a
number of xed-size patches and linearly embeds them with position embeddings (left).
Then the result is fed into a standard transformer encoder (right). Adapted from [2].
2.2. Applications of Semantic Segmentation
In this section, we discuss various application domains of semantic seg-
mentation, including remote sensing, medical imaging, and video processing.
For each of these domains, we highlight the unique challenges and opportuni-
ties that arise, as well as the current state-of-the-art methods and techniques.
2.2.1. Semantic Segmentation of Remote Sensing Images
Remote sensing is the process of getting information and monitoring the
characteristics of an area without having any physical contact. The two
main types of remote sensing techniques are the use of active sensors such
as RADAR, LiDAR and the use of passive sensors such as satellite imagery
[21]. These high-resolution earth surface images provide a wide range of use
cases such as world mapping updates [22], forest degradation analysis [23],
monitoring changes to the surface [24], etc.
Remote sensing imagery is widely used in combination with computer vi-
sion and Articial Intelligence (AI) for analyzing and processing the earth's
surface over large areas with complex feature distributions. The images col-
lected by satellites or unmanned aerial vehicles (UAV) provide a wide range
of information for applications such as urban planning, disaster management,
trac management, climate change, wildlife conservation, crop monitoring,
etc. The use of datasets containing these high-resolution images and their
respective segmented masks [25] have provided a base for remote sensing
5image analysis using computer vision and AI. The use of neural networks
provides the ability to process large amounts of image data for object de-
tection, semantic segmentation, and change detection tasks. The evolution
in the remote sensing domain has further improved satellite sensors and the
introduction of drone technology for aerial imagery has been vital to getting
ner details on the earth's surface. This has resulted in precise and accurate
data for processing using AI techniques [26].
Remote sensing images of the earth's surface provide land cover areas that
can be categorized into dierent segmented classes. Each of these classes is
assigned a label for each pixel while preserving the spatial resolution of the
image. Many datasets containing these remote sensing images and their seg-
mented masks are available [25, 27, 28] to use for dierent applications such
as change detection, land cover segmentation, and classication. Examples
of common land cover classes covered by the pixel-level classication are
forests, crops, buildings, water resources, grasslands, roads, etc. Research
has been conducted using ViT architecture models by adding layers and at-
tention mechanisms eciently and improvements in performance to process
high-resolution remote sensing images for semantic segmentation such as Ef-
cient Transformer [10] and Wide-Context Transformer [29].
Manual segmentation of these dierent environmental areas from a com-
plex satellite or aerial images is a dicult task which is time-consuming,
error-prone, and requires expertise in the remote sensing domain.
2.2.2. Semantic Segmentation of Medical Images
Medical image analysis has developed and incorporated scanning and vi-
sualization techniques. Segmentation techniques have been vital as it has the
ability to identify and segment medical imagery to assist in further diagnosis
and interventions. By identifying each region of interest (ROI) highlighted,
various important diagnoses are happening such as brain tumor boundary
detection from MRI images, pneumonia aections in X-rays, cancer detec-
tion from biopsy sample images, etc. The demand for this type of analy-
sis through image segmentation has emerged in the recent past with much
research being done in the scope to develop more precise, ecient models
and algorithms. These medical images that are used in image segmentation
tasks can be grouped based on modalities such as MRI, CT scan, X-ray,
ultrasound, microscopy, dermoscopy, etc. Each of these categories contains
datasets that were collected under medical supervision and some are made
publicly available.
Since there exist several modalities as mentioned above, the technological
systems that are used for medical imagery dier. Medical imagery system
development vendors built them as per the doctor's requirements. There-
6fore, the images generated are bound to the limitations of the technology
available and require medical personal intervention to examine them [30].
Therefore the segmentation of these images in dierent biological domains
requires experts in each eld to cope with these systems and spend a vast
amount of time examining them. To overcome these diculties, the capabil-
ity of automatic feature extraction has been introduced with deep learning
based techniques, which have been valuable in the sense of medical imagery.
With the advancements in segmentation analysis, better-performing models
have been introduced with the use of medical images by many researchers.
One such famous architecture is the U-Net [31] which was initially intro-
duced for medical image analysis. Based on this, several improved versions
have been followed up using medical imagery datasets from heart, lesion, and
liver segmentation [32, 33, 18]. This proves how benecial the improvement
of segmentation has been in the medical environment. In recent years, the
emerging new architectures of ViTs have also been applied to the medical
domain with TransUNet [34] and Swin-Unet [35]. They are hybrid Trans-
former architectures with the advantages of the U-Net. They performed with
better accuracy in cardiac and multi-organ segmentation applications.
Some limitations of medical images are the relatively less number of im-
ages available compared to natural image datasets (landscapes, people, an-
imals, and automobiles) with millions of images. In the medical domain,
there are several image modalities. For annotating medical images, expertise
in each medical eld is a must. Among them, MRI and microscopy images
are quite dicult to annotate [36]. Typically, these datasets contain fewer
images compared to ultrasound, X-ray, and lesion datasets which are ob-
tained with the existing scanning systems and are easier to annotate with
less complex structures and ne boundaries. But still, limitations exist due
to restrictions on privacy and other medical policies to obtain these images in
large quantities. To overcome these limitations with some datasets, several
image segmentation challenge competitions are taking place every year which
provide publicly available well-annotated medical image datasets. Most of
the improvements made through research in semantic segmentation models
have been based on these challenge datasets and most are taken as bench-
mark datasets for segmentation [37, 38, 39].
2.2.3. Video Semantic Segmentation
Human-Machine interaction [40], augmented reality [41], autonomous ve-
hicles [42], image search engines [43] are some applications in complete scene
understanding and for these type of applications, semantic segmentation con-
tributes more on complete scene understanding on videos. Usually, the idea
is to apply semantic segmentation on frames of a high-resolution video where
7the video is considered as a set of uncorrelated xed images [44]. The com-
mon challenge with this type of semantic segmentation is the computational
complexity of scaling the spatial dimension of the video using the temporal
frame rate. Removal of temporal features and only focusing on spatial frame-
by-frame features doesn't make sense in video segmentation. Since there is
a combined 
ow among frames of a video, considering the temporal context
of a video is an essential factor in video semantic segmentation, even though
it is computationally expensive.
Research has been conducted to reduce this high computation cost on
videos. Feature reuse and feature warping [45] have been proposed as a
solution. Cityscapes [46] and CamVid [47], are some largest video segmen-
tation datasets available for frame-by-frame approach of video segmentation
[48]. Recent papers have proposed segmentation methods such as selective
re-execution of feature extraction layers [49], optical 
ow-based feature warp-
ing [50], and LSTM-based, xed-budget keyframe selection policies [51]. The
main key problem in these approaches is that they have less attention to the
temporal context of a video. Researchers have shown that to satisfy both
spatial and temporal contexts, using an optical 
ow of video as temporal in-
formation to speed up uncertainty estimation makes good sense [52]. VisTR
[53], TeViT [54] and SeqFormer [55] are some of the Transformer models that
are used for video segmentation tasks.
2.3. Practical approaches to overcome the data limitation
Deep neural networks have performed well with supervised learning in
computer vision and NLP. But when it comes to the real world, supervised
learning faces a bottleneck in training a neural network as it needs lots of
labeled data. Collecting labeled data or manual labeling is dicult in every
aspect. Training a network from scratch is a somewhat costly task; as a
remedy for this, transfer learning comes into play. But when considering
specied downstream tasks such as satellite imagery semantic segmentation,
using pre-trained datasets is dicult as most of the architectures have been
trained on benchmark datasets where the data domain is dierent. Therefore,
getting good accuracy has been tricky.
Specially when considering Transformer architectures, self-supervised learn-
ing plays a great role as a remedy for data-hungry problems in deep learning.
In human vision, humans are fed with dierent things in the environment and
then are able to distinguish those things from other objects in the environ-
ment. There are no labeling mechanisms for these scenarios. Therefore, this
is the technique used in SSL which actually trains a neural network using
an unlabeled dataset where the labels are automatically provided through
the dataset itself. As the rst step, the network is set to solve a pretext
8task as described in Figure 2. A pretext task is a pre-designed task from
which the network can learn features and then using those trained weights
for dierent features, the network can be applied to solve some downstream
tasks. A downstream task is a specied task. Common downstream tasks in
computer vision are semantic segmentation, object detection, etc.
Figure 2: The general pipeline of self-supervised learning. The trained weights from solving
a pretext task are applied to solve some downstream tasks.
Rotating an image by a given angle and predicting the rotation, solving
jigsaw puzzles, lling a cut patch on an image, predicting the relative position
of a patch of an image, and separating images belonging to dierent clusters
can be considered as some of the pretext tasks in SSL [56]. By using these
methods, the network can learn dierent features in the dataset under the
given scope. No labels are used here and automatic labeling is achieved via
the image itself.
SSL has three general categories based on how the training happens.
Generative: Train the encoder to encode the given input and using the
decoder get the input back
Contrastive: Train the encoder to encode the given input and nd the
similarities
9Generative-Contrastive (Adversarial): Train encoder to encode the given
input and create fake outputs and compare the features of the input
and output [57]
Semantic segmentation is one of the major downstream tasks that can
be performed using SSL. Pixel-wise labeling is essential in semantic segmen-
tation. If there are no properly annotated datasets, SSL is the best way to
train semantic segmentation architectures.
2.4. Loss functions in semantic segmentation
For segmentation, classication, and object detection models accuracy
improvement not only depends on the model architectures but also on the loss
functions used. The loss function calculates the overall error while training
batches and adjust the weights through back propagation. Numerous loss
functions have been created to cope with various domains, and some of them
are derived from existing loss functions. Additionally, these loss functions
take into account the imbalances in the dataset too.
In the case of semantic segmentation, the default choice and most com-
monly used is the cross-entropy loss which is applied pixel-wise. The loss
function independently evaluates the class predictions for each pixel and av-
erages over all the pixels.
CEloss(p;q) =nX
i=1pilog(qi) (1)
The equation 1 above computes the average loss for each pixel in an
image. Here in the equation piis the true probability of the ithclass and
qiis the predicted probability of the same class. This supports the model
to generate probability maps that closely resemble the actual segmentation
masks while penalizing inaccurate predictions more heavily. By minimizing
the cross-entropy loss function during training, the model becomes better at
precise image segmentation.
Even though the above method is widely used it can be biased with
dataset imbalance as the majority class will be dominant. To overcome this
when the dataset is skewed, a weighted cross entropy loss is introduced in
[31].
WCEloss(p;q) =nX
i=1piwilog(qi) (2)
Here as in equation 2, a weight factor as wifor theithclass is inserted to
the typical equation 1. But the issue was not signicantly solved as the cross
10entropy calculates the average per-pixel loss without considering the adjacent
pixels which can be boundaries.
As a further improvement for the cross-entropy loss, the focal loss tech-
nique [58] was introduced. This is implemented by altering the structure
of cross-entropy loss. When focal loss is applied to samples with accurate
classications, the scaling factor value is down-weighted. This ensures the
more harder samples are emphasized, therefore high class imbalance won't
bias toward the overall calculations.
Floss(pt) =t(1pt)
log(pt) (3)
In the equation 3, ptis the predicted probability of the true class, tis a
scaling factor that gives higher weight to the positive class, and 
is a focusing
parameter that controls how much the loss is focused on hard examples.
The cross-entropy loss is scaled in this loss function, with the scaling
factors decreasing to zero as the condence in the well-classied classes rises.
Therefore more attention is given to the pixel classes which are dicult to
predict.
Another set of loss calculation techniques is the overlapping between pre-
diction and actual segmentations. The models are trained to minimize the
loss such that the model outputs segmentations with higher overlaps.
Dice loss is one such widely used popular measure in computer vision
tasks to calculate the similarity between two images. It is based on the
dice coecient which was later developed as the dice loss function in the
segmentation domain. This loss was rst used in the computer vision domain
by [59] in medical image segmentation tasks.
Dloss(g;p) = 12Pn
i=1gipiPn
i=1gi+Pn
i=1pi+(4)
Here, in equation 4 gandpdescribes the ground truth and prediction
segmentations. The sum is calculated over the nnumber of pixels with 
small constant added to avoid division by zero. The dice coecient measures
the overlap between the samples (ground truth and prediction) and provides
a score ranging from 0 to 1, 1 means perfect overlap. Since this method
considered pixels in both global and local contexts, the accuracy is higher
than cross-entropy loss calculations.
Another similar method used to evaluate the metric of models is the
IoU (Intersection over Union) loss also known as the Jaccard index. It is
quite similar to the dice metric and measures the overlapping of the positive
instances between the considered samples. This method as shown in equation
115 diers from the dice loss with correctly classied segments relative to total
pixels in either the ground truth or predicted segments.
IoUloss(g;p) = 1Pn
i=1gipiPn
i=1gi+Pn
i=1piPn
i=1gipi+(5)
For multi-class segmentation, the mean IoU is considered by taking the
average of each individual class IoU. This is widely used for performance
comparison and evaluation of dense prediction models [60].
3. Datasets
In this section, the common datasets used for the training and testing
of semantic segmentation models are considered. Factors aecting the cre-
ation of real datasets are lighting conditions, weather, and season. Based
on these factors, datasets can be classied into dierent groups. When data
is collected under normal daytime environmental conditions, those data are
categorized under no cross-domain datasets. If data is collected under some
deviated environmental conditions including rainy, cloudy, nighttime, snowy,
etc then such data are categorized under cross-domain datasets. Another
category is synthetic data, where the data is articially created and col-
lected for training purposes. These synthetic datasets are mostly created as
a cost-eective supplement for training purposes. Following are some of the
benchmark datasets specially made for semantic segmentation tasks, with a
summary presented in Table 1.
PASCAL-Context [61] This dataset was created by manually labeling
every pixel of PASCAL-VOC 2010 [62] dataset with semantic categories. The
domain of this dataset is not limited and its data contains dierent objects.
The semantic categories of this dataset can be divided into three main classes.
(i) objects, (ii) stu, and (iii) hybrids. Objects have dened categories such
as cups, keyboards, etc. Stu has classes without a specic shape and has
regions such as sky, water, etc. Hybrid contains intermediate objects such
as roads where roads have a clear boundary but shape cannot be predicted
correctly.
ADE20K [63] Annotations of this dataset are done on scenes, objects,
parts of objects. Many of the objects in the dataset are annotated with their
parts. Annotations in this dataset are made continuously. Therefore, this is
a growing dataset.
KITTI [64] This dataset contains both 2D and 3D images which have
been collected from urban and rural expressway incidents and trac sce-
narios. It is useful for robotics and autonomous driving. This dataset has
12dierent variants namely KITTI-2012, KITTI-2015 and they have some dif-
ferences in the ground truth.
Cityscapes [46] This contains large-scale pixel-level and instance-level
semantic segmentation annotations recorded from a set of stereo video se-
quences. Compared to other datasets, quality, data size, and annotations in
this dataset have a good rank and data have been collected from 50 dierent
cities in Germany and neighboring countries.
IDD [65] This is specially designed for road scene understanding and data
have been collected from 182 Indian road scenes. As these are taken from
Indian roads, there are some variations in the weather and lighting conditions
because of dust and air quality on roads. One key feature of this dataset is,
this contains some special classes such as auto-rickshaws and animals on the
roads.
Virtual KITTI [66] Except for dierent weather and imaging conditions,
most of the virtual vision datasets such as Virtual KITTI are similar to the
real vision datasets. Therefore virtual datasets are useful for pre-training
purposes. This dataset is created from 5 dierent urban scene videos from
the real-world KITTI dataset. Data have been automatically labeled and can
be used for object detection, semantic segmentation, instance segmentation,
etc.
IDDA [67] This contains 1 million frames generated from simulator
CARLA oriented on dierent 7 city models. This dataset can be used to
do semantic segmentation for more than 100 dierent visual domains and is
specially designed for autonomous driving models.
Dataset Classes Size Train Validation Test Resolution (pixels) Category
PASCAL-Context 540 19740 4998 5105 9637 387 470 No cross-domain
ADE20K 150 25210 20210 2000 3000 - No cross-domain
KITTI 5 252 140 - 112 1392 512 No cross-domain
Cityscapes 30 5K ne, 20K coarse 2975 500 1525 1024 2048 Cross-domain
IDD 34 10004 7003 1000 2001 1678 968 Cross-domain
Virtual KITTI 14 21260 - - - 1242 375 Synthetic
IDDA 24 1M - - - 1920 1080 Synthetic
Table 1: Summary of the datasets
Note: Both cross-domain and no-cross domain falls into the non-synthetic
category
4. Meta - analysis
In this section, we discuss some of the ViT models specialized for the
task of semantic segmentation. The models are selected upon considering the
datasets that they benchmarked (ADE20K, Cityscapes, PASCAL-Context).
13The intuition behind that is to compare all the models on a common basis.
The benchmark results are summarized in Table 2.
4.1.SEgmentation TRansformer (SETR)
SETR [5] proposes semantic segmentation as a sequence-to-sequence pre-
diction task. They adopt a pure Transformer as the encoder part of their
segmentation model without utilizing any convolution layers. In this model,
they replace the prevalent stacked convolution layer based encoder with a
pure Transformer which gradually reduces the spatial resolution.
Figure 3: SETR architecture and its variants adapted from [5]. (a) SETR consists of a
standard Transformer. (b) SETR-PUP with a progressive up-sampling design. (c) SETR-
MLA with a multi-level feature aggregation.
The SETR encoder (Figure 3a) which is a standard Transformer treats
an image as a sequence of patches followed by a linear projection. Then it
embeds these projections with patch embedding + position embedding to
feed them into a set of Transformer layers. SETR has no down-sampling
in spatial resolution at each layer of the encoder transformer while it only
provides global context modeling. They classify SETR into a few variants
depending on the decoder part of the model; SETR-PUP (Figure 3b) which
has a progressive up-sampling design and the SETR-MLA (Figure 3)which
has a multi-level feature aggregation.
SETR achieved state-of-the-art semantic segmentation results on ADE20K,
Pascal Context by the time of submission [5]. It has also been tested on the
Cityscapes dataset and has shown impressive results.
4.2. Swin Transformer
To address the issue of not having a general purpose Transformer back-
bone for computer vision tasks, [4] proposed Swin Transformer (Hierarchical
14Vision Transformer using Shifted Win dows) which can be served as a gen-
eral purpose backbone for computer vision tasks such as image classication
and dense prediction.
Figure 4: An overview of the Swin Transformer adapted from [4]. (a) Hierarchical feature
maps for reducing computational complexity. (b) Shifted window approach which was
used when calculating self-attention. (c) Two successive Swin Transformer Blocks which
presented at each stage. (d) Core architecture of the Swin.
Swin Transformer was able to bring down the quadratic computational
complexity of calculating self-attention in Transformers to linear complex-
ity by constructing hierarchical feature maps (Figure 4a). Also, the shifted
window approach illustrated in Figure 4b has much lower latency than the
earlier sliding window based approaches which were used to calculate the self-
attention. Swin Transformer showed great success over the previous state-
of-the-art in image classication (87.3% top-1 accuracy on ImageNet-1K),
semantic segmentation (53.5% mIoU on ADE20Kval) and object detection
(58.7 box AP and 51.1 mask AP on COCO test-dev) [4].
According to the architecture of a Swin Transformer, in the beginning,
it splits the given image into a sequence of non-overlapping patches (tokens)
by using the patch partitioning module (Figure 4d). Then a linear embed-
ding is applied to this sequence of patches to project them into an arbitrary
dimension. It is followed by several Swin Transformer blocks to apply self-
attention. The main responsibility of the patch merging module is to reduce
the number of tokens in deeper layers. It is noteworthy that the feature map
resolutions in the hierarchical stages are similar to those in typical convo-
15lution architectures such as ResNet [68]. Therefore Swin Transformer can
eciently replace ResNet backbone networks in computer vision tasks.
4.3. Segmenter
Segmenter [11] is a purely transformer-based approach for semantic seg-
mentation which consist of a ViT backbone pre-trained on ImageNet and
introduces a mask transformer as the decoder (Figure 5). Even though the
model was built for segmentation tasks, they take advantage of the mod-
els made for image classication to pre-train and then ne-tune them on
moderate-sized segmentation datasets.
Figure 5: Segmenter architecture adapted from [11]. It basically has a ViT backbone with
a mask transformer as the decoder.
CNN-based models are generally inecient when processing global image
context and ultimately result in a sub-optimal segmentation. The reason for
the sub-optimal segmentation of the convolution-based approaches is that
convolution is a local operation which poorly accesses the global information
of the image. But the global information is crucial where the global image
context usually in
uences the local patch labeling. But modeling of global
interaction has a quadratic complexity to the image size because it needs
to model the interaction between each and every raw pixel of the image.
The architecture of the Segmenter especially captures the global context of
images, unlike the traditional CNN-based approaches.
Other than the semantic segmentation tasks, this Segmenter model also
can be applied to panoptic segmentation (semantic segmentation + instance
segmentation) tasks by altering the model architecture. The class embed-
dings of the model need to be replaced by object embeddings in such a case.
164.4. SegFormer
SegFormer [69] is an architecture for semantic segmentation which consist
of a hierarchical Transformer encoder with a lightweight multilayer percep-
tron (MLP) decoder (Figure 6). The MLP decoder is used for predicting the
nal mask. To obtain a precise segmentation, it uses a patch size of 4 4
in contrast to ViT which uses a patch size of 16 16. It has an overlapped
patch merging process to maintain the local continuity around the patches.
Figure 6: SegFormer architecture adapted from [69]. It has a hierarchical Transformer
encoder for feature extraction and a lightweight MLP decoder for predicting the nal
mask.
Generally, ViT has a xed resolution for positional encoding [70]. This
leads to a drop in accuracy since it needs to interpolate the positional en-
coding of testing images when they have a dierent resolution than training
images. Thus, SegFormer introduces a Positional-Encoding-Free design as a
key feature.
Moreover, the authors claim their architecture is more robust against
common corruptions and perturbations than current methods which make
SegFormer appropriate for safety-critical applications. SegFormer achieved
competitive results on ADE20K, Cityscapes, and COCO-Stu datasets as
shown in Table 2. SegFormer comes in several variants from SegFormer-B0
to SegFormer-B5, where the largest model is SegFormer-B5. This largest
model surpasses the SETR [5] on the ADE20K dataset achieving the highest
mIoU while being 4 faster than SETR. All of these SegFormer models have
trade-os between model size, accuracy, and runtime.
174.5. Pyramid Vision Transformer (PVT)
ViT couldn't be directly applicable to dense prediction tasks because its
output feature map is single scaled and it generally has a low resolution which
comes at a higher computational cost. PVT [71] overcomes the aforemen-
tioned concerns by introducing a progressive shrinking pyramid backbone
network to reduce the computational costs and simultaneously output more
ne-grained segmentation. PVT comes in two variants. PVT v1 [71] is the
rst work by the authors and PVT v2 [72] comes with some additional im-
provements to the previous version.
4.5.1. PVT v1
This initial version has some noteworthy changes compared to the ViT.
It takes 44 input patches in contrast to the 16 16 patches in ViT. This
improves the model's ability to learn high-resolution representations. It also
reduces the computational demand of traditional ViT by using a progressive
shrinking pyramid. This pyramid structure progressively shrinks the output
resolution from high to low in the stages which are responsible for generat-
ing the scaled feature maps (Figure 7). Another major dierence is that it
replaces the multi-head attention layer (MHA) in ViT with a novel spatial
reduction attention (SRA) layer which reduces the spatial scales before the
attention operation. This further reduces the computational and memory
demand because SRA has a low computational complexity than MHA.
Figure 7: PVT v1 architecture adapted from [71]. The pyramid structure of the stages
progressively shrinks the output resolution from high to low.
184.5.2. PVT v2
The former version has a few drawbacks. The computational demand
of the PVT v1 is relatively large when processing high-resolution images.
It loses the local continuity of the images when processing the image as a
sequence of non-overlapping patches. It cannot process variable-sized inputs
because of the xed-size position encoding. This new version has three major
improvements which circumvent the previous design issues. First one is linear
spatial reduction attention (LSRA) which reduces the spatial dimension of
the image to a xed size using average pooling (Figure 8). Unlike SRA
in the PVT v1, LSRA benets from linear complexity. Second one is the
overlapping patch embedding (Figure 9a). This is done by zero-padding the
border of the image and taking more enlarged patch windows which overlap
with the adjacent windows. It helps to capture more local continuity of the
images. The third one is the convolutional feed-forward network (Figure 9b)
which helps to process dierent sizes of input resolutions. With these major
improvements, PVT v2 was able to bring down the complexity of PVT v1
to linear complexity.
Figure 8: Comparison of spatial reduction attention (SRA) layers in PVT versions [72]
We can clearly see how the improvements of the PVT v2 contribute to
higher gains in the benchmark comparison in Table 2.
4.6. Twins
Twins [73] propose two modern Transformer designs for computer vision
named Twins-PCPVT and Twins-SVT by revisiting the work on the PVT
v1 [71] and Swin Transformer [4].
Twins-SVT uses a spatially separable self-attention (SSSA) mechanism
based on the depth-wise separable convolutions in neural networks. This
19Figure 9: Improved patch embedding and feed-forward networks in PVT v2 [72]
SSSA has two underlying attention mechanisms which are capable of cap-
turing local information as well as global information. Locally grouped self-
attention (LSA) and global sub-sampled attention (GSA) are the above-
mentioned attention mechanisms respectively. Those techniques greatly re-
duce the heavy computational demand in high-resolution image inputs while
keeping a ne-grained segmentation.
Figure 10: Twins-PCPVT architecture adapted from [73]. It uses conditional position
encoding with a positional encoding generator (PEG) to overcome some of the drawbacks
of xed-positional encoding.
As we discussed in the Pyramid Vision Transformer section, PVT v1
can only process xed-size image inputs due to its absolute positional en-
20coding. This hinders the performance of PVT. To alleviate this challenge
Twins-PCPVT uses a conditional position encoding (CPE) rst introduced
in Conditional Position encoding Vision Transformer (CPVT) [70]. This is
illustrated as the positional encoding generator (PEG) in Figure 10. It is
capable of alleviating some of the issues encountered in xed-position encod-
ing.
Twins architectures have shown outstanding performance on computer
vision tasks including image classication and semantic segmentation. The
semantic segmentation results achieved by the two Twins architectures are
highly competitive compared to the Swin Transformer [4] and PVT [71].
4.7. Dense Prediction Transformer (DPT)
DPT [74] architecture is introduced with a transformer backbone inside
the encoder-decoder design for ne-grained output segmentation predictions
compared to the fully convolutional networks. The transformer encoder
based on ViT [2] is capable of maintaining spatial resolution over all the
stages of the Transformer architecture which is important for dense predic-
tions.
Figure 11: DPT architecture adapted from [74]. (a) Non-overlapping image patches are fed
into the Transformer block. (b) Reassemble operation for assembling tokens into feature
maps. (c) Fusion blocks for combining feature maps.
In the paper, the authors have introduced several models based on the
used image embedding technique. The DPT-Base and DPT-Large models
use patch-based embedding where the input image is separated into non-
overlapping image patches. Then these are fed into the Transformer block
with a learnable position embedding to locate the spatial position of each in-
dividual token (Figure 11a). DPT-Base has 12 transformer layers compared
to the DPT-Large which has 24 layers with wide feature sizes. The other
model is the DPT-Hybrid, which uses the convolutional backbone ResNet-50
as a feature extractor and uses the pixel-based feature maps as token inputs
21to the 12-layer transformer block. The Transformer blocks reassemble the
tokens with multi-head self-attention (MSA) [1] sequential blocks for global
interaction between tokens. The tokens are reassembled into image-like fea-
ture representations in various resolutions (Figure 11b). Finally, these rep-
resentations are combined using residual convolutional units in the decoder
and fused together for the nal dense prediction (Figure 11c).
The experimental results of the dense prediction transformer have pro-
vided improved accuracy results over several benchmark dataset compar-
isons. The results show that for a large training dataset, the model has the
best performance. The comparisons were done for depth estimations and
semantic segmentation. ADE20K dataset is used for segmentation and the
DPT-Hybrid model has outperformed all the fully-convolutional models [74].
The DPT has the ability to identify precise boundaries of objects with less
distortion. The DPT model was also compared with the PASCAL-Context
dataset after ne-tuning.
4.8. High-Resolution Transformer (HRFormer)
HRFormer [75] is an architecture model that is built using a depth-wise
convolutional design with a Feed Forward Network (FFN) and a local win-
dow self-attention mechanism with a multi-resolution parallel transformer
module. This model is developed for dense prediction tasks focusing on pose
estimation and semantic segmentation. The model outperforms the conven-
tional ViT model which produces low-resolution outputs. The HRFormer is
designed to maintain the high-resolution using multi-resolution streams and
is more ecient in computational complexity and memory usage.
Figure 12: HRFormer architecture adapted from [75]. (a) Self-attention blocks. (b) FFN
with depth-wise convolutions.
HRFormer has been incorporated by using the HRNet [76], which is a
convolutional network consisting of a multi-scale parallel design. This ar-
chitecture helps to capture feature maps in variant resolutions while main-
taining high resolution. At each of these resolution blocks, partitioning is
22done by creating non-overlapping windows, and self-attention is performed
on each image window separately. This improved the eciency signicantly
compared to overlapping local window mechanisms introduced earlier in dif-
ferent studies [77]. The self-attention blocks (Figure 12a) are followed by
an FFN with depth-wise convolutions (Figure 12b) to increase the receptive
eld size by information exchange between local windows, which is vital in
dense prediction. By incorporating a multi-resolution parallel transformer
architecture with convolutional multi-scale fusions for the overall HRFormer
architecture, the information between dierent resolutions is exchanged re-
peatedly. This process creates a high-resolution output with both local and
global context information.
4.9. Masked-attention Mask Transformer (Mask2Former)
Mask2Former [78] is a new transformer architecture that can be leveraged
to do segmentation tasks including panoptic, instance, and semantic segmen-
tation. It is a successful attempt to introduce a universal architecture for the
segmentation tasks which outperforms the current specialized SOTA archi-
tectures for each of the segmentation tasks by the time of submission. Its key
components consist of a transformer decoder with masked attention. Gener-
ally, a standard Transformer attends to the full feature map. In contrast, the
masked attention operator in Mask2Former restricts the cross-attention to
the foreground region of the predicted mask and then extracts the localized
features. This makes the attention mechanism more ecient in this model.
Figure 13: Mask2Former architecture adapted from [78]. The model consists of a backbone
feature extractor, a pixel decoder, and a Transformer decoder.
23The architecture of Mask2Former is similar in design to the previous
MaskFormer [79] architecture. The main components are the backbone fea-
ture extractor, pixel decoder, and the Transformer decoder (Figure 13). The
backbone could be either a CNN-based model or a Transformer based model.
As the pixel decoder, they have used a more advanced multi-scale deformable
attention Transformer (MSDeformAttn) [6] in contrast to the feature pyra-
mid network [80] used in MaskFormer [79]. Masked attention has been used
to enhance the eectiveness of the Transformer decoder.
Despite being a universal architecture for segmentation, Mask2Former
still needs to be trained separately for each of the specic tasks. This is
a common limitation of the universal architectures for segmentation tasks.
Mask2Former has achieved new SOTA performance on all three segmentation
tasks (panoptic, instance, semantic) in popular datasets such as COCO and
ADE20K and Cityscapes. The semantic segmentation results are compared
for ADE20K and Cityscapes datasets in Table 2.
24Datasets
Model Variant Backbone #Params (M) ADE20K Cityscapes PASCAL-Context
SETR [5]SETR- Na ve (16,160k)ViT-Lz[2] 305.67 48.06 / 48.80 - -
SETR- PUP (16,160k) ViT-Lz318.31 48.58 / 50.09 - -
SETR- MLA (16,160k) ViT-Lz310.57 48.64 / 50.28 - -
SETR- PUP (16,40k) ViT-Lz318.31 - 78.39 / 81.57 -
SETR- PUP (16,80k) ViT-Lz318.31 - 79.34 / 82.15 -
SETR- Na ve (16,80k) ViT-Lz305.67 - - 52.89 / 53.61
SETR- PUP (16,80k) ViT-Lz318.31 - - 54.40 / 55.27
SETR- MLA (16,80k) ViT-Lz310.57 - - 54.87 / 55.83
Swin@[4]Swin-T 60 46.1 - -
Swin-S 81 49.3 - -
Swin-Bz121 51.6 - -
Swin-Lz234 53.5 - -
Segmenterx[11]Seg-B DeiT-By[81] 86 48.05 80.5 53.9
Seg-B/Mask DeiT-By86 50.08 80.6 55.0
Seg-L ViT-Lz307 52.25 80.7 56.5
Seg-L/Mask ViT-Lz307 53.63 81.3 59.0
SegFormer [69]MiT-B0y3.4 37.4 / 38.0 76.2 / 78.1 -
MiT-B1y13.1 42.2 / 43.1 78.5 / 80.0 -
MiT-B2y24.2 46.5 / 47.5 81.0 / 82.2 -
MiT-B3y44.0 49.4 / 50.0 81.7 / 83.3 -
MiT-B4y60.8 50.3 / 51.1 82.3 / 83.9 -
MiT-B5y81.4 51.0 / 51.8 82.4 /84.0 -
PVT@PVT-Tinyz17.0 35.7 - -
PVT-Smallz28.2 39.8 - -
PVT v1 [71] PVT-Mediumz48.0 41.6 - -
PVT-Largez65.1 42.1 - -
PVT-Largez* 65.1 44.8 - -
PVT v2-B0z7.6 37.2 - -
PVT v2-B1z17.8 42.5 - -
PVT v2 [72] PVT v2-B2z29.1 45.2 - -
PVT v2-B3z49.0 47.3 - -
PVT v2-B4z66.3 47.9 - -
PVT v2-B5z85.7 48.7 - -
Twins [73]Twins-PCPVT-Sy54.6 46.2 / 47.5 - -
Twins-PCPVT Twins-PCPVT-By74.3 47.1 / 48.4 - -
Twins-PCPVT-Ly91.5 48.6 / 49.8 - -
Twins-SVT-Sy54.4 46.2 / 47.1 - -
Twins-SVT Twins-SVT-By88.5 47.7 / 48.9 - -
Twins-SVT-Ly133 48.8 / 50.2 - -
DPTx[74]DPT-Hybrid ViT-Hybridz123 49.02 - 60.46
DPT-Large ViT-Lz343 47.63 - -
HRFormer [75]OCRNet(7,150k)HRFormer-S 13.5 44.0 / 45.1 - -
OCRNet(7,150k) HRFormer-B 50.3 46.3 / 47.6 - -
OCRNet(7,80k) HRFormer-S 13.5 - 80.0 / 81.0 -
OCRNet(7,80k) HRFormer-B 50.3 - 81.4 / 82.0 -
OCRNet(15,80k) HRFormer-B 50.3 - 81.9 / 82.6 57.6 / 58.5
OCRNet(7,60k) HRFormer-B 50.3 - - 56.3 / 57.1
OCRNet(7,60k) HRFormer-S 13.5 - - 53.8 / 54.6
Mask2Former [78]Swin-T - 47.7 / 49.6 - -
Swin-Lz216 56.1 / 57.3 - -
Swin-L-FaPNz- 56.4 / 57.7 - -
Swin-Lz216 - 83.3 / 84.3 -
Swin-Bz- - 83.3 / 84.5 -
Table 2: Comparison of the ViT models specialized for the task
of semantic segmentation according to mIoU (%) using dierent
benchmark datasets. The best-performing variant of each model
for a given dataset is highlighted. Overall top performing model
variant for each dataset is shaded in gray. SS / MS contains both
single-scale and multi-scale inferences.  @ - Single-scale inference only,  x
- Multi-scale inference only,   - (patch size, iterations),   - pre-trained
on ImageNet-1K,   - pre-trained on ImageNet-21K,   - 320K training
iterations and multi-scale 
ip testing
255. Discussion
In this survey, we discussed how ViTs became a powerful alternative to
classical CNNs in various computer vision applications, their strengths as
well as limitations, and how ViT contributed to the semantic segmentation
of images with their usage across dierent domains such as remote sensing,
medical and video processing. Even though we included some of the CNN ar-
chitectures widely used in prior mentioned domains to provide a comparison
between the ViT and CNNs, an in-depth discussion about CNN architectures
is beyond the scope of this paper. We have summarized the dierent statis-
tics regarding popular datasets used for semantic segmentation tasks and the
results of dierent ViT architectures used for semantic segmentation to give
a clear and high-level overview for the reader around the region of semantic
segmentation.
6. Conclusions and Future Directions
Unlike mature convolutional neural networks, ViTs are still in the early
stage of development. Nevertheless, we observed how powerful and compet-
itive they are with their CNN counterparts. ViTs are progressing towards
excellence and it is expected that they will replace traditional CNN-based
methods widely used in the deep learning domain in the near future. Dier-
ent variants of ViTs can be used for experiments with domains such as big
data analytics that require a vast amount of data for processing. Exploring
research areas with less adaptation to ViT usage can create more ecient,
performance-increased outcomes for current implementation methods.
Even though ViTs have proven successful, they can be challenging to ex-
periment with due to their high computational demand. Thus improvements
to the ViT architecture are needed to make it lightweight and more ecient.
This will inspire the community to open new pathways using ViTs.
We believe there is a plethora of new research areas that ViT, along with
semantic segmentation can be applied to solve real-world problems.
References
[1] A. Vaswani, G. Brain, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez,  Lukasz Kaiser, I. Polosukhin, Attention is all you need,
Advances in Neural Information Processing Systems 30 (2017).
[2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.,
26An image is worth 16x16 words: Transformers for image recognition at
scale, arXiv preprint arXiv:2010.11929 (2020).
[3] C.-F. R. Chen, Q. Fan, R. Panda, Crossvit: Cross-attention multi-
scale vision transformer for image classication, in: Proceedings of the
IEEE/CVF international conference on computer vision, 2021, pp. 357{
366.
[4] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, B. Guo, Swin
transformer: Hierarchical vision transformer using shifted windows, in:
Proceedings of the IEEE/CVF International Conference on Computer
Vision, 2021, pp. 10012{10022.
[5] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng,
T. Xiang, P. H. Torr, et al., Rethinking semantic segmentation from a
sequence-to-sequence perspective with transformers, in: Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition,
2021, pp. 6881{6890.
[6] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, J. Dai, Deformable detr: De-
formable transformers for end-to-end object detection, arXiv preprint
arXiv:2010.04159 (2020).
[7] X. Dai, Y. Chen, J. Yang, P. Zhang, L. Yuan, L. Zhang, Dynamic detr:
End-to-end object detection with dynamic attention, in: Proceedings
of the IEEE/CVF International Conference on Computer Vision, 2021,
pp. 2988{2997.
[8] N. Park, S. Kim, How do vision transformers work?, arXiv preprint
arXiv:2202.06709 (2022).
[9] M. Caron, H. Touvron, I. Misra, H. J egou, J. Mairal, P. Bojanowski,
A. Joulin, Emerging properties in self-supervised vision transformers, in:
Proceedings of the IEEE/CVF International Conference on Computer
Vision, 2021, pp. 9650{9660.
[10] Z. Xu, W. Zhang, T. Zhang, Z. Yang, J. Li, Ecient transformer for re-
mote sensing image segmentation, Remote Sensing 13 (18) (2021) 3585.
[11] R. Strudel, R. Garcia, I. Laptev, C. Schmid, Segmenter: Transformer for
semantic segmentation, in: Proceedings of the IEEE/CVF International
Conference on Computer Vision, 2021, pp. 7262{7272.
27[12] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, M. Shah,
Transformers in vision: A survey, ACM computing surveys (CSUR)
54 (10s) (2022) 1{41.
[13] Y. Liu, Y. Zhang, Y. Wang, F. Hou, J. Yuan, J. Tian, Y. Zhang, Z. Shi,
J. Fan, Z. He, A survey of visual transformers, IEEE Transactions on
Neural Networks and Learning Systems (2023).
[14] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao,
C. Xu, Y. Xu, et al., A survey on vision transformer, IEEE transactions
on pattern analysis and machine intelligence (2022).
[15] J. Long, E. Shelhamer, T. Darrell, Fully convolutional networks for se-
mantic segmentation, in: Proceedings of the IEEE conference on com-
puter vision and pattern recognition, 2015, pp. 3431{3440.
[16] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Mis-
awa, K. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz, et al., At-
tention u-net: Learning where to look for the pancreas, arXiv preprint
arXiv:1804.03999 (2018).
[17] F. I. Diakogiannis, F. Waldner, P. Caccetta, C. Wu, Resunet-a: A deep
learning framework for semantic segmentation of remotely sensed data,
ISPRS Journal of Photogrammetry and Remote Sensing 162 (2020) 94{
114.
[18] Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, J. Liang, Unet++:
A nested u-net architecture for medical image segmentation, in: Deep
learning in medical image analysis and multimodal learning for clinical
decision support, Springer, 2018, pp. 3{11.
[19] S. Hochreiter, The vanishing gradient problem during learning recurrent
neural nets and problem solutions, International Journal of Uncertainty,
Fuzziness and Knowledge-Based Systems 6 (02) (1998) 107{116.
[20] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya,
J. Shlens, Stand-alone self-attention in vision models, Advances in Neu-
ral Information Processing Systems 32 (2019).
[21] L. Zhu, J. Suomalainen, J. Liu, J. Hyypp a, H. Kaartinen, H. Haggren,
et al., A review: Remote sensing sensors, Multi-purposeful application
of geospatial data (2018) 19{42.
28[22] M. Schmitt, J. Prexl, P. Ebel, L. Liebel, X. X. Zhu, Weakly super-
vised semantic segmentation of satellite images for land cover mapping{
challenges and opportunities, arXiv preprint arXiv:2002.08254 (2020).
[23] L. P. Olander, H. K. Gibbs, M. Steininger, J. J. Swenson, B. C. Murray,
Reference scenarios for deforestation and forest degradation in support
of redd: a review of data and methods, Environmental Research Letters
3 (2) (2008) 025011.
[24] F. Pacici, F. Del Frate, C. Solimini, W. J. Emery, An innovative
neural-net method to detect temporal changes in high-resolution op-
tical satellite imagery, IEEE Transactions on Geoscience and Remote
Sensing 45 (9) (2007) 2940{2952.
[25] A. Boguszewski, D. Batorski, N. Ziemba-Jankowska, T. Dziedzic,
A. Zambrzycka, Landcover. ai: Dataset for automatic mapping of build-
ings, woodlands, water and roads from aerial imagery, in: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition, 2021, pp. 1102{1110.
[26] L. P. Osco, J. M. Junior, A. P. M. Ramos, L. A. de Castro Jorge, S. N.
Fatholahi, J. de Andrade Silva, E. T. Matsubara, H. Pistori, W. N.
Gon calves, J. Li, A review on deep learning in uav remote sensing,
International Journal of Applied Earth Observation and Geoinformation
102 (2021) 102456.
[27] J. Wang, Z. Zheng, A. Ma, X. Lu, Y. Zhong, Loveda: A remote sensing
land-cover dataset for domain adaptive semantic segmentation, arXiv
preprint arXiv:2110.08733 (2021).
[28] I. Demir, K. Koperski, D. Lindenbaum, G. Pang, J. Huang, S. Basu,
F. Hughes, D. Tuia, R. Raskar, Deepglobe 2018: A challenge to parse the
earth through satellite images, in: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition Workshops, 2018, pp. 172{
181.
[29] L. Ding, D. Lin, S. Lin, J. Zhang, X. Cui, Y. Wang, H. Tang, L. Bruz-
zone, Looking outside the window: Wide-context transformer for the
semantic segmentation of high-resolution remote sensing images, IEEE
Transactions on Geoscience and Remote Sensing 60 (2022) 1{13.
[30] S. D. Olabarriaga, A. W. Smeulders, Interaction in the segmentation of
medical images: A survey, Medical image analysis 5 (2) (2001) 127{142.
29[31] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for
biomedical image segmentation, in: International Conference on Medical
image computing and computer-assisted intervention, Springer, 2015,
pp. 234{241.
[32] Z. Gu, J. Cheng, H. Fu, K. Zhou, H. Hao, Y. Zhao, T. Zhang, S. Gao,
J. Liu, Ce-net: Context encoder network for 2d medical image segmen-
tation, IEEE transactions on medical imaging 38 (10) (2019) 2281{2292.
[33] H. Huang, L. Lin, R. Tong, H. Hu, Q. Zhang, Y. Iwamoto, X. Han, Y.-
W. Chen, J. Wu, Unet 3+: A full-scale connected unet for medical image
segmentation, in: ICASSP 2020-2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2020, pp.
1055{1059.
[34] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille,
Y. Zhou, Transunet: Transformers make strong encoders for medical
image segmentation, arXiv preprint arXiv:2102.04306 (2021).
[35] H. Cao, Y. Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, M. Wang,
Swin-unet: Unet-like pure transformer for medical image segmentation,
in: Computer Vision{ECCV 2022 Workshops: Tel Aviv, Israel, October
23{27, 2022, Proceedings, Part III, Springer, 2023, pp. 205{218.
[36] A. I sn, C. Direko glu, M. S ah, Review of mri-based brain tumor image
segmentation using deep learning methods, Procedia Computer Science
102 (2016) 317{324.
[37] N. C. Codella, D. Gutman, M. E. Celebi, B. Helba, M. A. Marchetti,
S. W. Dusza, A. Kalloo, K. Liopyris, N. Mishra, H. Kittler, et al., Skin
lesion analysis toward melanoma detection: A challenge at the 2017
international symposium on biomedical imaging (isbi), hosted by the
international skin imaging collaboration (isic), in: 2018 IEEE 15th in-
ternational symposium on biomedical imaging (ISBI 2018), IEEE, 2018,
pp. 168{172.
[38] P. Bilic, P. F. Christ, E. Vorontsov, G. Chlebus, H. Chen, Q. Dou, C.-W.
Fu, X. Han, P.-A. Heng, J. Hesser, et al., The liver tumor segmentation
benchmark (lits), arXiv preprint arXiv:1901.04056 (2019).
[39] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani,
J. Kirby, Y. Burren, N. Porz, J. Slotboom, R. Wiest, et al., The multi-
modal brain tumor image segmentation benchmark (brats), IEEE trans-
actions on medical imaging 34 (10) (2014) 1993{2024.
30[40] D. Gorecky, M. Schmitt, M. Loskyll, D. Z uhlke, Human-machine-
interaction in the industry 4.0 era, in: 2014 12th IEEE international
conference on industrial informatics (INDIN), Ieee, 2014, pp. 289{294.
[41] R. T. Azuma, A survey of augmented reality, Presence: teleoperators &
virtual environments 6 (4) (1997) 355{385.
[42] J. Janai, F. G uney, A. Behl, A. Geiger, et al., Computer vision for au-
tonomous vehicles: Problems, datasets and state of the art, Foundations
and Trends in Computer Graphics and Vision 12 (1{3) (2020) 1{308.
[43] T. Gevers, A. Smeulders, Image search engines: An overview, Emerging
Topics in Computer Vision (2004) 1{54.
[44] S. Jain, X. Wang, J. E. Gonzalez, Accel: A corrective fusion network
for ecient semantic segmentation on video, in: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2019, pp. 8866{8875.
[45] M. Ding, Z. Wang, B. Zhou, J. Shi, Z. Lu, P. Luo, Every frame counts:
Joint learning of video segmentation and optical 
ow, in: Proceedings
of the AAAI Conference on Articial Intelligence, Vol. 34, 2020, pp.
10713{10720.
[46] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benen-
son, U. Franke, S. Roth, B. Schiele, The cityscapes dataset for semantic
urban scene understanding, in: Proceedings of the IEEE conference on
computer vision and pattern recognition, 2016, pp. 3213{3223.
[47] G. J. Brostow, J. Fauqueur, R. Cipolla, Semantic object classes in video:
A high-denition ground truth database, Pattern Recognition Letters
30 (2) (2009) 88{97.
[48] S. R. Richter, V. Vineet, S. Roth, V. Koltun, Playing for data: Ground
truth from computer games, in: European conference on computer vi-
sion, Springer, 2016, pp. 102{118.
[49] E. Shelhamer, K. Rakelly, J. Homan, T. Darrell, Clockwork convnets
for video semantic segmentation, in: European Conference on Computer
Vision, Springer, 2016, pp. 852{868.
[50] X. Zhu, Y. Xiong, J. Dai, L. Yuan, Y. Wei, Deep feature 
ow for video
recognition, in: Proceedings of the IEEE conference on computer vision
and pattern recognition, 2017, pp. 2349{2358.
31[51] B. Mahasseni, S. Todorovic, A. Fern, Budget-aware deep semantic video
segmentation, in: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2017, pp. 1029{1038.
[52] A. Garcia-Garcia, S. Orts-Escolano, S. Oprea, V. Villena-Martinez,
P. Martinez-Gonzalez, J. Garcia-Rodriguez, A survey on deep learn-
ing techniques for image and video semantic segmentation, Applied Soft
Computing 70 (2018) 41{65.
[53] Y. Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, H. Xia, End-to-
end video instance segmentation with transformers, in: Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition,
2021, pp. 8741{8750.
[54] S. Yang, X. Wang, Y. Li, Y. Fang, J. Fang, W. Liu, X. Zhao, Y. Shan,
Temporally ecient vision transformer for video instance segmentation,
in: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2022, pp. 2885{2895.
[55] J. Wu, Y. Jiang, S. Bai, W. Zhang, X. Bai, Seqformer: Sequential trans-
former for video instance segmentation, in: Computer Vision{ECCV
2022: 17th European Conference, Tel Aviv, Israel, October 23{27, 2022,
Proceedings, Part XXVIII, Springer, 2022, pp. 553{569.
[56] S. Gustavsson, Object detection and semantic segmentation using self-
supervised learning (2021).
[57] X. Liu, F. Zhang, Z. Hou, L. Mian, Z. Wang, J. Zhang, J. Tang, Self-
supervised learning: Generative or contrastive, IEEE Transactions on
Knowledge and Data Engineering (2021).
[58] T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Doll ar, Focal loss for dense
object detection, in: Proceedings of the IEEE international conference
on computer vision, 2017, pp. 2980{2988.
[59] F. Milletari, N. Navab, S.-A. Ahmadi, V-net: Fully convolutional neural
networks for volumetric medical image segmentation, in: 2016 fourth
international conference on 3D vision (3DV), IEEE, 2016, pp. 565{571.
[60] S. Jadon, A survey of loss functions for semantic segmentation, in: 2020
IEEE Conference on Computational Intelligence in Bioinformatics and
Computational Biology (CIBCB), IEEE, 2020, pp. 1{7.
32[61] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler, R. Ur-
tasun, A. Yuille, The role of context for object detection and semantic
segmentation in the wild, in: Proceedings of the IEEE conference on
computer vision and pattern recognition, 2014, pp. 891{898.
[62] M. Everingham, J. Winn, The pascal visual object classes challenge 2012
(voc2012) development kit, Pattern Anal. Stat. Model. Comput. Learn.,
Tech. Rep 2007 (2012) 1{45.
[63] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, A. Torralba, Scene
parsing through ade20k dataset, in: Proceedings of the IEEE conference
on computer vision and pattern recognition, 2017, pp. 633{641.
[64] S. Kuutti, R. Bowden, Y. Jin, P. Barber, S. Fallah, A survey of deep
learning applications to autonomous vehicle control, IEEE Transactions
on Intelligent Transportation Systems 22 (2) (2020) 712{733.
[65] G. Varma, A. Subramanian, A. Namboodiri, M. Chandraker, C. Jawa-
har, Idd: A dataset for exploring problems of autonomous navigation
in unconstrained environments, in: 2019 IEEE Winter Conference on
Applications of Computer Vision (WACV), IEEE, 2019, pp. 1743{1751.
[66] A. Gaidon, Q. Wang, Y. Cabon, E. Vig, Virtual worlds as proxy for
multi-object tracking analysis, in: Proceedings of the IEEE conference
on computer vision and pattern recognition, 2016, pp. 4340{4349.
[67] E. Alberti, A. Tavera, C. Masone, B. Caputo, Idda: a large-scale multi-
domain dataset for autonomous driving, IEEE Robotics and Automation
Letters 5 (4) (2020) 5526{5533.
[68] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image
recognition, in: Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770{778.
[69] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, P. Luo,
Segformer: Simple and ecient design for semantic segmentation with
transformers, Advances in Neural Information Processing Systems 34
(2021) 12077{12090.
[70] X. Chu, Z. Tian, B. Zhang, X. Wang, X. Wei, H. Xia, C. Shen, Con-
ditional positional encodings for vision transformers, arXiv preprint
arXiv:2102.10882 (2021).
33[71] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo,
L. Shao, Pyramid vision transformer: A versatile backbone for dense
prediction without convolutions, in: Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision, 2021, pp. 568{578.
[72] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo,
L. Shao, Pvt v2: Improved baselines with pyramid vision transformer,
Computational Visual Media 8 (3) (2022) 415{424.
[73] X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, C. Shen,
Twins: Revisiting the design of spatial attention in vision transformers,
Advances in Neural Information Processing Systems 34 (2021) 9355{
9366.
[74] R. Ranftl, A. Bochkovskiy, V. Koltun, Vision transformers for dense
prediction, in: Proceedings of the IEEE/CVF International Conference
on Computer Vision, 2021, pp. 12179{12188.
[75] Y. Yuan, R. Fu, L. Huang, W. Lin, C. Zhang, X. Chen, J. Wang,
Hrformer: High-resolution vision transformer for dense predict, Ad-
vances in Neural Information Processing Systems 34 (2021) 7281{7293.
[76] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu,
M. Tan, X. Wang, et al., Deep high-resolution representation learning for
visual recognition, IEEE transactions on pattern analysis and machine
intelligence 43 (10) (2020) 3349{3364.
[77] H. Hu, Z. Zhang, Z. Xie, S. Lin, Local relation networks for image
recognition, in: Proceedings of the IEEE/CVF International Conference
on Computer Vision, 2019, pp. 3464{3473.
[78] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, R. Girdhar, Masked-
attention mask transformer for universal image segmentation, in: Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2022, pp. 1290{1299.
[79] B. Cheng, A. Schwing, A. Kirillov, Per-pixel classication is not all
you need for semantic segmentation, Advances in Neural Information
Processing Systems 34 (2021) 17864{17875.
[80] T.-Y. Lin, P. Doll ar, R. Girshick, K. He, B. Hariharan, S. Belongie,
Feature pyramid networks for object detection, in: Proceedings of the
IEEE conference on computer vision and pattern recognition, 2017, pp.
2117{2125.
34[81] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, H. J egou,
Training data-ecient image transformers & distillation through atten-
tion, in: International Conference on Machine Learning, PMLR, 2021,
pp. 10347{10357.
35
  Relational inductive biases, deep learning, and graph networks
Peter W. Battaglia1,Jessica B. Hamrick1,Victor Bapst1,
Alvaro Sanchez-Gonzalez1,Vinicius Zambaldi1,Mateusz Malinowski1,
Andrea Tacchetti1,David Raposo1,Adam Santoro1,Ryan Faulkner1,
Caglar Gulcehre1,Francis Song1,Andrew Ballard1,Justin Gilmer2,
George Dahl2,Ashish Vaswani2,Kelsey Allen3,Charles Nash4,
Victoria Langston1,Chris Dyer1,Nicolas Heess1,
Daan Wierstra1,Pushmeet Kohli1,Matt Botvinick1,
Oriol Vinyals1,Yujia Li1,Razvan Pascanu1
1DeepMind;2Google Brain;3MIT;4University of Edinburgh
Abstract
Articial intelligence (AI) has undergone a renaissance recently, making major progress in
key domains such as vision, language, control, and decision-making. This has been due, in
part, to cheap data and cheap compute resources, which have t the natural strengths of deep
learning. However, many dening characteristics of human intelligence, which developed under
much dierent pressures, remain out of reach for current approaches. In particular, generalizing
beyond one's experiences|a hallmark of human intelligence from infancy|remains a formidable
challenge for modern AI.
The following is part position paper, part review, and part unication. We argue that
combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that
structured representations and computations are key to realizing this objective. Just as biology
uses nature and nurture cooperatively, we reject the false choice between hand-engineering
and end-to-end learning, and instead advocate for an approach which benets from their
complementary strengths. We explore how using relational inductive biases within deep learning
architectures can facilitate learning about entities, relations, and rules for composing them. We
present a new building block for the AI toolkit with a strong relational inductive bias|the graph
network |which generalizes and extends various approaches for neural networks that operate
on graphs, and provides a straightforward interface for manipulating structured knowledge and
producing structured behaviors. We discuss how graph networks can support relational reasoning
and combinatorial generalization, laying the foundation for more sophisticated, interpretable,
and 
exible patterns of reasoning. As a companion to this paper, we have also released an
open-source software library for building graph networks, with demonstrations of how to use
them in practice.
1 Introduction
A key signature of human intelligence is the ability to make innite use of nite means (Humboldt,
1836; Chomsky, 1965), in which a small set of elements (such as words) can be productively
composed in limitless ways (such as into new sentences). This re
ects the principle of combinatorial
generalization , that is, constructing new inferences, predictions, and behaviors from known building
blocks. Here we explore how to improve modern AI's capacity for combinatorial generalization by
Corresponding author: peterbattaglia@google.com
1arXiv:1806.01261v3  [cs.LG]  17 Oct 2018biasing learning towards structured representations and computations, and in particular, systems
that operate on graphs.
Humans' capacity for combinatorial generalization depends critically on our cognitive mecha-
nisms for representing structure and reasoning about relations. We represent complex systems as
compositions of entities and their interactions1(Navon, 1977; McClelland and Rumelhart, 1981;
Plaut et al., 1996; Marcus, 2001; Goodwin and Johnson-Laird, 2005; Kemp and Tenenbaum, 2008),
such as judging whether a haphazard stack of objects is stable (Battaglia et al., 2013). We use
hierarchies to abstract away from ne-grained dierences, and capture more general commonalities
between representations and behaviors (Botvinick, 2008; Tenenbaum et al., 2011), such as parts of
an object, objects in a scene, neighborhoods in a town, and towns in a country. We solve novel
problems by composing familiar skills and routines (Anderson, 1982), for example traveling to a
new location by composing familiar procedures and objectives, such as travel by airplane, to
San Diego, eat at, and an Indian restaurant. We draw analogies by aligning the relational
structure between two domains and drawing inferences about one based on corresponding knowledge
about the other (Gentner and Markman, 1997; Hummel and Holyoak, 2003).
Kenneth Craik's The Nature of Explanation (1943), connects the compositional structure of
the world to how our internal mental models are organized:
...[a human mental model] has a similar relation-structure to that of the process it
imitates. By relation-structure' I do not mean some obscure non-physical entity which
attends the model, but the fact that it is a working physical model which works in the
same way as the process it parallels... physical reality is built up, apparently, from a few
fundamental types of units whose properties determine many of the properties of the
most complicated phenomena, and this seems to aord a sucient explanation of the
emergence of analogies between mechanisms and similarities of relation-structure among
these combinations without the necessity of any theory of objective universals. (Craik,
1943, page 51-55)
That is, the world is compositional, or at least, we understand it in compositional terms. When
learning, we either t new knowledge into our existing structured representations, or adjust the
structure itself to better accommodate (and make use of) the new and the old (Tenenbaum et al.,
2006; Griths et al., 2010; Ullman et al., 2017).
The question of how to build articial systems which exhibit combinatorial generalization has
been at the heart of AI since its origins, and was central to many structured approaches, including
logic, grammars, classic planning, graphical models, causal reasoning, Bayesian nonparametrics, and
probabilistic programming (Chomsky, 1957; Nilsson and Fikes, 1970; Pearl, 1986, 2009; Russell and
Norvig, 2009; Hjort et al., 2010; Goodman et al., 2012; Ghahramani, 2015). Entire sub-elds have
focused on explicit entity- and relation-centric learning, such as relational reinforcement learning
(D zeroski et al., 2001) and statistical relational learning (Getoor and Taskar, 2007). A key reason
why structured approaches were so vital to machine learning in previous eras was, in part, because
data and computing resources were expensive, and the improved sample complexity aorded by
structured approaches' strong inductive biases was very valuable.
In contrast with past approaches in AI, modern deep learning methods (LeCun et al., 2015;
Schmidhuber, 2015; Goodfellow et al., 2016) often follow an end-to-end design philosophy which
emphasizes minimal a priori representational and computational assumptions, and seeks to avoid
explicit structure and hand-engineering. This emphasis has t well with|and has perhaps been
armed by|the current abundance of cheap data and cheap computing resources, which make
1Whether this entails a language of thought (Fodor, 1975) is beyond the scope of this work.
2trading o sample eciency for more 
exible learning a rational choice. The remarkable and rapid
advances across many challenging domains, from image classication (Krizhevsky et al., 2012;
Szegedy et al., 2017), to natural language processing (Sutskever et al., 2014; Bahdanau et al., 2015),
to game play (Mnih et al., 2015; Silver et al., 2016; Morav c k et al., 2017), are a testament to this
minimalist principle. A prominent example is from language translation, where sequence-to-sequence
approaches (Sutskever et al., 2014; Bahdanau et al., 2015) have proven very eective without using
explicit parse trees or complex relationships between linguistic entities.
Despite deep learning's successes, however, important critiques (Marcus, 2001; Shalev-Shwartz
et al., 2017; Lake et al., 2017; Lake and Baroni, 2018; Marcus, 2018a,b; Pearl, 2018; Yuille and
Liu, 2018) have highlighted key challenges it faces in complex language and scene understanding,
reasoning about structured data, transferring learning beyond the training conditions, and learning
from small amounts of experience. These challenges demand combinatorial generalization, and so it
is perhaps not surprising that an approach which eschews compositionality and explicit structure
struggles to meet them.
When deep learning's connectionist (Rumelhart et al., 1987) forebears were faced with analogous
critiques from structured, symbolic positions (Fodor and Pylyshyn, 1988; Pinker and Prince, 1988),
there was a constructive eort (Bobrow and Hinton, 1990; Marcus, 2001) to address the challenges
directly and carefully. A variety of innovative sub-symbolic approaches for representing and reasoning
about structured objects were developed in domains such as analogy-making, linguistic analysis,
symbol manipulation, and other forms of relational reasoning (Smolensky, 1990; Hinton, 1990;
Pollack, 1990; Elman, 1991; Plate, 1995; Eliasmith, 2013), as well as more integrative theories for
how the mind works (Marcus, 2001). Such work also helped cultivate more recent deep learning
advances which use distributed, vector representations to capture rich semantic content in text
(Mikolov et al., 2013; Pennington et al., 2014), graphs (Narayanan et al., 2016, 2017), algebraic and
logical expressions (Allamanis et al., 2017; Evans et al., 2018), and programs (Devlin et al., 2017;
Chen et al., 2018b).
We suggest that a key path forward for modern AI is to commit to combinatorial generalization
as a top priority, and we advocate for integrative approaches to realize this goal. Just as biology does
not choose between nature versus nurture|it uses nature and nurture jointly , to build wholes which
are greater than the sums of their parts|we, too, reject the notion that structure and 
exibility are
somehow at odds or incompatible, and embrace both with the aim of reaping their complementary
strengths. In the spirit of numerous recent examples of principled hybrids of structure-based methods
and deep learning (e.g., Reed and De Freitas, 2016; Garnelo et al., 2016; Ritchie et al., 2016; Wu
et al., 2017; Denil et al., 2017; Hudson and Manning, 2018), we see great promise in synthesizing
new techniques by drawing on the full AI toolkit and marrying the best approaches from today
with those which were essential during times when data and computation were at a premium.
Recently, a class of models has arisen at the intersection of deep learning and structured
approaches, which focuses on approaches for reasoning about explicitly structured data, in particular
graphs (e.g. Scarselli et al., 2009b; Bronstein et al., 2017; Gilmer et al., 2017; Wang et al., 2018c; Li
et al., 2018; Kipf et al., 2018; Gulcehre et al., 2018). What these approaches all have in common
is a capacity for performing computation over discrete entities and the relations between them.
What sets them apart from classical approaches is how the representations and structure of the
entities and relations|and the corresponding computations|can be learned, relieving the burden
of needing to specify them in advance. Crucially, these methods carry strong relational inductive
biases , in the form of specic architectural assumptions, which guide these approaches towards
learning about entities and relations (Mitchell, 1980), which we, joining many others (Spelke et al.,
1992; Spelke and Kinzler, 2007; Marcus, 2001; Tenenbaum et al., 2011; Lake et al., 2017; Lake and
Baroni, 2018; Marcus, 2018b), suggest are an essential ingredient for human-like intelligence.
3Box 1: Relational reasoning
We dene structure as the product of composing a set of known building blocks. Structured
representations capture this composition (i.e., the arrangement of the elements) and structured
computations operate over the elements and their composition as a whole. Relational reasoning,
then, involves manipulating structured representations of entities and relations , using rules
for how they can be composed. We use these terms to capture notions from cognitive science,
theoretical computer science, and AI, as follows:
Anentity is an element with attributes, such as a physical object with a size and mass.
Arelation is a property between entities. Relations between two objects might include
same size as ,heavier than , and distance from . Relations can have attributes as
well. The relation more than Xtimes heavier than takes an attribute, X, which
determines the relative weight threshold for the relation to be true vs.false . Relations
can also be sensitive to the global context. For a stone and a feather, the relation falls
with greater acceleration than depends on whether the context is in air vs.in a
vacuum . Here we focus on pairwise relations between entities.
Aruleis a function (like a non-binary logical predicate) that maps entities and relations
to other entities and relations, such as a scale comparison like is entity X large? and
is entity X heavier than entity Y? . Here we consider rules which take one or two
arguments (unary and binary), and return a unary property value.
As an illustrative example of relational reasoning in machine learning, graphical models (Pearl,
1988; Koller and Friedman, 2009) can represent complex joint distributions by making explicit
random conditional independences among random variables. Such models have been very
successful because they capture the sparse structure which underlies many real-world generative
processes and because they support ecient algorithms for learning and reasoning. For example,
hidden Markov models constrain latent states to be conditionally independent of others given
the state at the previous time step, and observations to be conditionally independent given the
latent state at the current time step, which are well-matched to the relational structure of many
real-world causal processes. Explicitly expressing the sparse dependencies among variables
provides for various ecient inference and reasoning algorithms, such as message-passing, which
apply a common information propagation procedure across localities within a graphical model,
resulting in a composable, and partially parallelizable, reasoning procedure which can be applied
to graphical models of dierent sizes and shape.
In the remainder of the paper, we examine various deep learning methods through the lens of
their relational inductive biases, showing that existing methods often carry relational assumptions
which are not always explicit or immediately evident. We then present a general framework for
entity- and relation-based reasoning|which we term graph networks |for unifying and extending
existing methods which operate on graphs, and describe key design principles for building powerful
architectures using graph networks as building blocks. We have also released an open-source library
for building graph networks, which can be found here: github.com/deepmind/graph nets .
2 Relational inductive biases
Many approaches in machine learning and AI which have a capacity for relational reasoning
4Box 2: Inductive biases
Learning is the process of apprehending useful knowledge by observing and interacting with the
world. It involves searching a space of solutions for one expected to provide a better explanation
of the data or to achieve higher rewards. But in many cases, there are multiple solutions which
are equally good (Goodman, 1955). An inductive bias allows a learning algorithm to prioritize
one solution (or interpretation) over another, independent of the observed data (Mitchell,
1980). In a Bayesian model, inductive biases are typically expressed through the choice and
parameterization of the prior distribution (Griths et al., 2010). In other contexts, an inductive
bias might be a regularization term (McClelland, 1994) added to avoid overtting, or it might
be encoded in the architecture of the algorithm itself. Inductive biases often trade 
exibility
for improved sample complexity and can be understood in terms of the bias-variance tradeo
(Geman et al., 1992). Ideally, inductive biases both improve the search for solutions without
substantially diminishing performance, as well as help nd solutions which generalize in a
desirable way; however, mismatched inductive biases can also lead to suboptimal performance
by introducing constraints that are too strong.
Inductive biases can express assumptions about either the data-generating process or the space
of solutions. For example, when tting a 1D function to data, linear least squares follows
the constraint that the approximating function be a linear model, and approximation errors
should be minimal under a quadratic penalty. This re
ects an assumption that the data-
generating process can be explained simply, as a line process corrupted by additive Gaussian
noise. Similarly, L2 regularization prioritizes solutions whose parameters have small values,
and can induce unique solutions and global structure to otherwise ill-posed problems. This can
be interpreted as an assumption about the learning process: that searching for good solutions
is easier when there is less ambiguity among solutions. Note, these assumptions need not be
explicit|they re
ect interpretations of how a model or algorithm interfaces with the world.
(Box 1) use a relational inductive bias . While not a precise, formal denition, we use this term to
refer generally to inductive biases (Box 2) which impose constraints on relationships and interactions
among entities in a learning process.
Creative new machine learning architectures have rapidly proliferated in recent years, with
(perhaps not surprisingly given the thesis of this paper) practitioners often following a design pattern
of composing elementary building blocks to form more complex, deep2computational hierarchies and
graphs3. Building blocks such as fully connected layers are stacked into multilayer perceptrons
(MLPs), convolutional layers are stacked into convolutional neural networks (CNNs), and a
standard recipe for an image processing network is, generally, some variety of CNN composed with
a MLP. This composition of layers provides a particular type of relational inductive bias|that
of hierarchical processing|in which computations are performed in stages, typically resulting in
increasingly long range interactions among information in the input signal. As we explore below, the
building blocks themselves also carry various relational inductive biases (Table 1). Though beyond
the scope of this paper, various non-relational inductive biases are used in deep learning as well: for
example, activation non-linearities, weight decay, dropout (Srivastava et al., 2014), batch and layer
normalization (Ioe and Szegedy, 2015; Ba et al., 2016), data augmentation, training curricula, and
optimization algorithms all impose constraints on the trajectory and outcome of learning.
2This pattern of composition in depth is ubiquitous in deep learning, and is where the deep comes from.
3Recent methods (Liu et al., 2018) even automate architecture construction via learned graph editing procedures.
5Component Entities Relations Rel. inductive bias Invariance
Fully connected Units All-to-all Weak -
Convolutional Grid elements Local Locality Spatial translation
Recurrent Timesteps Sequential Sequentiality Time translation
Graph network Nodes Edges Arbitrary Node, edge permutations
Table 1: Various relational inductive biases in standard deep learning components. See also Section 2.
To explore the relational inductive biases expressed within various deep learning methods, we
must identify several key ingredients, analogous to those in Box 1: what are the entities , what are
therelations , and what are the rules for composing entities and relations, and computing their
implications? In deep learning, the entities and relations are typically expressed as distributed
representations, and the rules as neural network function approximators; however, the precise forms
of the entities, relations, and rules vary between architectures. To understand these dierences
between architectures, we can further ask how each supports relational reasoning by probing:
The arguments to the rule functions (e.g., which entities and relations are provided as input).
How the rule function is reused , orshared , across the computational graph (e.g., across dierent
entities and relations, across dierent time or processing steps, etc.).
How the architecture denes interactions versus isolation among representations (e.g., by
applying rules to draw conclusions about related entities, versus processing them separately).
2.1 Relational inductive biases in standard deep learning building blocks
2.1.1 Fully connected layers
Perhaps the most common building block is a fully connected layer (Rosenblatt, 1961). Typically
implemented as a non-linear vector-valued function of vector inputs, each element, or unit, of
the output vector is the dot product between a weight vector, followed by an added bias term, and
nally a non-linearity such as a rectied linear unit (ReLU). As such, the entities are the units in
the network, the relations are all-to-all (all units in layer iare connected to all units in layer j),
and the rules are specied by the weights and biases. The argument to the rule is the full input
signal, there is no reuse, and there is no isolation of information (Figure 1a). The implicit relational
inductive bias in a fully connected layer is thus very weak: all input units can interact to determine
any output unit's value, independently across outputs (Table 1).
2.1.2 Convolutional layers
Another common building block is a convolutional layer (Fukushima, 1980; LeCun et al., 1989). It is
implemented by convolving an input vector or tensor with a kernel of the same rank, adding a bias
term, and applying a point-wise non-linearity. The entities here are still individual units (or grid
elements, e.g. pixels), but the relations are sparser. The dierences between a fully connected layer
and a convolutional layer impose some important relational inductive biases: locality and translation
invariance (Figure 1b). Locality re
ects that the arguments to the relational rule are those entities in
close proximity with one another in the input signal's coordinate space, isolated from distal entities.
Translation invariance re
ects reuse of the same rule across localities in the input. These biases
are very eective for processing natural image data because there is high covariance within local
6(a) Fully connected
Sharing in space (b) Convolutional
Sharing in time (c) Recurrent
Figure 1: Reuse and sharing in common deep learning building blocks. (a) Fully connected layer,
in which all weights are independent, and there is no sharing. (b) Convolutional layer, in which
a local kernel function is reused multiple times across the input. Shared weights are indicated by
arrows with the same color. (c) Recurrent layer, in which the same function is reused across dierent
processing steps.
neighborhoods, which diminishes with distance, and because the statistics are mostly stationary
across an image (Table 1).
2.1.3 Recurrent layers
A third common building block is a recurrent layer (Elman, 1990), which is implemented over a
sequence of steps. Here, we can view the inputs and hidden states at each processing step as the
entities, and the Markov dependence of one step's hidden state on the previous hidden state and
the current input, as the relations. The rule for combining the entities takes a step's inputs and
hidden state as arguments to update the hidden state. The rule is reused over each step (Figure 1c),
which re
ects the relational inductive bias of temporal invariance (similar to a CNN's translational
invariance in space). For example, the outcome of some physical sequence of events should not
depend on the time of day. RNNs also carry a bias for locality in the sequence via their Markovian
structure (Table 1).
2.2 Computations over sets and graphs
While the standard deep learning toolkit contains methods with various forms of relational inductive
biases, there is no default deep learning component which operates on arbitrary relational structure.
We need models with explicit representations of entities and relations, and learning algorithms which
nd rules for computing their interactions, as well as ways of grounding them in data. Importantly,
entities in the world (such as objects and agents) do not have a natural order; rather, orderings
can be dened by the properties of their relations. For example, the relations between the sizes of
a set of objects can potentially be used to order them, as can their masses, ages, toxicities, and
prices. Invariance to ordering|except in the face of relations|is a property that should ideally be
re
ected by a deep learning component for relational reasoning.
Sets are a natural representation for systems which are described by entities whose order is
undened or irrelevant; in particular, their relational inductive bias does not come from the presence
of something, but rather from the absence . For illustration, consider the task of predicting the center
7H H
O
The brown 
dog jumped. The 
brown dog jumped (a) (b) 
(c) (d) 
(e) (f) 
Molecule Mass-Spring System 
n-body System Rigid Body System 
Sentence and Parse Tree Image and Fully-Connected Scene Graph Figure 2: Dierent graph representations. (a) A molecule, in which each atom is represented as a
node and edges correspond to bonds (e.g. Duvenaud et al., 2015). (b) A mass-spring system, in
which the rope is dened by a sequence of masses which are represented as nodes in the graph (e.g.
Battaglia et al., 2016; Chang et al., 2017). (c) A n-body system, in which the bodies are nodes and
the underlying graph is fully connected (e.g. Battaglia et al., 2016; Chang et al., 2017). (d) A rigid
body system, in which the balls and walls are nodes, and the underlying graph denes interactions
between the balls and between the balls and the walls (e.g. Battaglia et al., 2016; Chang et al., 2017).
(e) A sentence, in which the words correspond to leaves in a tree, and the other nodes and edges
could be provided by a parser (e.g. Socher et al., 2013). Alternately, a fully connected graph could
be used (e.g. Vaswani et al., 2017). (f) An image, which can be decomposed into image patches
corresponding to nodes in a fully connected graph (e.g. Santoro et al., 2017; Wang et al., 2018c).
of mass of a solar system comprised of nplanets, whose attributes (e.g., mass, position, velocity,
etc.) are denoted by fx1;x2;:::;xng. For such a computation, the order in which we consider the
planets does not matter because the state can be described solely in terms of aggregated, averaged
quantities. However, if we were to use a MLP for this task, having learned the prediction for a
particular input ( x1;x2;:::;xn) would not necessarily transfer to making a prediction for the same
inputs under a dierent ordering ( xn;x1;:::;x2). Since there are n! such possible permutations, in
the worst case, the MLP could consider each ordering as fundamentally dierent, and thus require
an exponential number of input/output training examples to learn an approximating function.
A natural way to handle such combinatorial explosion is to only allow the prediction to depend
on symmetric functions of the inputs' attributes. This might mean computing shared per-object
8featuresff(x1);:::;f (xn)gwhich are then aggregated in a symmetric way (for example, by taking
their mean). Such an approach is the essence of the Deep Sets and related models (Zaheer et al.,
2017; Edwards and Storkey, 2016; Pevn y and Somol, 2017), which we explore further in Section 4.2.3.
Of course, permutation invariance is not the only important form of underlying structure in
many problems. For example, each object in a set may be aected by pairwise interactions with
the other objects in the set (Hartford et al., 2018). In our planets scenario, consider now the
task of predicting each individual planet's position after a time interval,  t. In this case, using
aggregated, averaged information is not enough because the movement of each planet depends on
the forces the other planets are exerting on it. Instead, we could compute the state of each object
asx0
i=f(xi;P
jg(xi;xj)), wheregcould compute the force induced by the j-th planet on the
i-th planet, and fcould compute the future state of the i-th planet which results from the forces
and dynamics. The fact that we use the same geverywhere is again a consequence of the global
permutation invariance of the system; however, it also supports a dierent relational structure
becausegnow takes two arguments rather than one.4
The above solar system examples illustrate two relational structures: one in which there are
no relations, and one which consists of all pairwise relations. Many real-world systems (such as
in Figure 2) have a relational structure somewhere in between these two extremes, however, with
some pairs of entities possessing a relation and others lacking one. In our solar system example, if
the system instead consists of the planets and their moons, one may be tempted to approximate
it by neglecting the interactions between moons of dierent planets. In practice, this means
computing interactions only between some pairs of objects, i.e. x0
i=f(xi;P
j2(i)g(xi;xj)), where
(i)f1;:::;ngis a neighborhood around node i. This corresponds to a graph, in that the i-th
object only interacts with a subset of the other objects, described by its neighborhood. Note, the
updated states still do not depend in the order in which we describe the neighborhood.5
Graphs, generally, are a representation which supports arbitrary (pairwise) relational struc-
ture, and computations over graphs aord a strong relational inductive bias beyond that which
convolutional and recurrent layers can provide.
3 Graph networks
Neural networks that operate on graphs, and structure their computations accordingly, have been
developed and explored extensively for more than a decade under the umbrella of graph neural
networks (Gori et al., 2005; Scarselli et al., 2005, 2009a; Li et al., 2016), but have grown rapidly
in scope and popularity in recent years. We survey the literature on these methods in the next
sub-section (3.1). Then in the remaining sub-sections, we present our graph networks framework,
which generalizes and extends several lines of work in this area.
3.1 Background
Models in the graph neural network family (Gori et al., 2005; Scarselli et al., 2005, 2009a; Li et al.,
2016) have been explored in a diverse range of problem domains, across supervised, semi-supervised,
unsupervised, and reinforcement learning settings. They have been eective at tasks thought to
have rich relational structure, such as visual scene understanding tasks (Raposo et al., 2017; Santoro
4We could extend this same analysis to increasingly entangled structures that depend on relations among triplets
(i.e.,g(xi;xj;xk)), quartets, and so on. We note that if we restrict these functions to only operate on subsets of xi
which are spatially close, then we end back up with something resembling CNNs. In the most entangled sense, where
there is a single relation function g(x1; : : : ;xn), we end back up with a construction similar to a fully connected layer.
5The invariance which this model enforces is the invariance under isomorphism of the graph.
9et al., 2017) and few-shot learning (Garcia and Bruna, 2018). They have also been used to learn
the dynamics of physical systems (Battaglia et al., 2016; Chang et al., 2017; Watters et al., 2017;
van Steenkiste et al., 2018; Sanchez-Gonzalez et al., 2018) and multi-agent systems (Sukhbaatar
et al., 2016; Hoshen, 2017; Kipf et al., 2018), to reason about knowledge graphs (Bordes et al., 2013;
O~ noro-Rubio et al., 2017; Hamaguchi et al., 2017), to predict the chemical properties of molecules
(Duvenaud et al., 2015; Gilmer et al., 2017), to predict trac on roads (Li et al., 2017; Cui et al.,
2018), to classify and segment images and videos (Wang et al., 2018c; Hu et al., 2017) and 3D
meshes and point clouds (Wang et al., 2018d), to classify regions in images (Chen et al., 2018a), to
perform semi-supervised text classication (Kipf and Welling, 2017), and in machine translation
(Vaswani et al., 2017; Shaw et al., 2018; Gulcehre et al., 2018). They have been used within both
model-free (Wang et al., 2018b) and model-based (Hamrick et al., 2017; Pascanu et al., 2017;
Sanchez-Gonzalez et al., 2018) continuous control, for model-free reinforcement learning (Hamrick
et al., 2018; Zambaldi et al., 2018), and for more classical approaches to planning (Toyer et al.,
2017).
Many traditional computer science problems, which involve reasoning about discrete entities and
structure, have also been explored with graph neural networks, such as combinatorial optimization
(Bello et al., 2016; Nowak et al., 2017; Dai et al., 2017), boolean satisability (Selsam et al., 2018),
program representation and verication (Allamanis et al., 2018; Li et al., 2016), modeling cellular
automata and Turing machines (Johnson, 2017), and performing inference in graphical models
(Yoon et al., 2018). Recent work has also focused on building generative models of graphs (Li et al.,
2018; De Cao and Kipf, 2018; You et al., 2018; Bojchevski et al., 2018), and unsupervised learning of
graph embeddings (Perozzi et al., 2014; Tang et al., 2015; Grover and Leskovec, 2016; Garc a-Dur an
and Niepert, 2017).
The works cited above are by no means an exhaustive list, but provide a representative cross-
section of the breadth of domains for which graph neural networks have proven useful. We point
interested readers to a number of existing reviews which examine the body of work on graph neural
networks in more depth. In particular, Scarselli et al. (2009a) provides an authoritative overview of
early graph neural network approaches. Bronstein et al. (2017) provides an excellent survey of deep
learning on non-Euclidean data, and explores graph neural nets, graph convolution networks, and
related spectral approaches. Recently, Gilmer et al. (2017) introduced the message-passing neural
network (MPNN), which unied various graph neural network and graph convolutional network
approaches (Monti et al., 2017; Bruna et al., 2014; Hena et al., 2015; Deerrard et al., 2016;
Niepert et al., 2016; Kipf and Welling, 2017; Bronstein et al., 2017) by analogy to message-passing
in graphical models. In a similar vein, Wang et al. (2018c) introduced the non-local neural network
(NLNN), which unied various self-attention-style methods (Vaswani et al., 2017; Hoshen, 2017;
Veli ckovi c et al., 2018) by analogy to methods from computer vision and graphical models for
capturing long range dependencies in signals.
3.2 Graph network (GN) block
We now present our graph networks (GN) framework, which denes a class of functions for
relational reasoning over graph-structured representations. Our GN framework generalizes and
extends various graph neural network, MPNN, and NLNN approaches (Scarselli et al., 2009a; Gilmer
et al., 2017; Wang et al., 2018c), and supports constructing complex architectures from simple
building blocks. Note, we avoided using the term neural in the graph network label to re
ect
that they can be implemented with functions other than neural networks, though here our focus is
on neural network implementations.
The main unit of computation in the GN framework is the GN block , a graph-to-graph module
10Box 3: Our denition of graph
Attributesviek
<latexit sha1_base64=4ton1cC0/WpHTbJYOP5RCFkc+ww=>AAAB83icbVDLSsNAFL3xWeur6tLNYBG6KokIuiy4cVnBPqAJZTK9aYdOJmFmIpTQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJU8G1cd1vZ2Nza3tnt7JX3T84PDqunZx2dZIphh2WiET1Q6pRcIkdw43AfqqQxqHAXji9K/zeEyrNE/loZikGMR1LHnFGjZV8P6ZmEkY5zofTYa3uNt0FyDrxSlKHEu1h7csfJSyLURomqNYDz01NkFNlOBM4r/qZxpSyKR3jwFJJY9RBvsg8J5dWGZEoUfZJQxbq742cxlrP4tBOFhn1qleI/3mDzES3Qc5lmhmUbHkoygQxCSkKICOukBkxs4QyxW1WwiZUUWZsTVVbgrf65XXSvWp6btN7uK63GmUdFTiHC2iABzfQgntoQwcYpPAMr/DmZM6L8+58LEc3nHLnDP7A+fwBXXGRzQ==</latexit><latexit sha1_base64=4ton1cC0/WpHTbJYOP5RCFkc+ww=>AAAB83icbVDLSsNAFL3xWeur6tLNYBG6KokIuiy4cVnBPqAJZTK9aYdOJmFmIpTQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJU8G1cd1vZ2Nza3tnt7JX3T84PDqunZx2dZIphh2WiET1Q6pRcIkdw43AfqqQxqHAXji9K/zeEyrNE/loZikGMR1LHnFGjZV8P6ZmEkY5zofTYa3uNt0FyDrxSlKHEu1h7csfJSyLURomqNYDz01NkFNlOBM4r/qZxpSyKR3jwFJJY9RBvsg8J5dWGZEoUfZJQxbq742cxlrP4tBOFhn1qleI/3mDzES3Qc5lmhmUbHkoygQxCSkKICOukBkxs4QyxW1WwiZUUWZsTVVbgrf65XXSvWp6btN7uK63GmUdFTiHC2iABzfQgntoQwcYpPAMr/DmZM6L8+58LEc3nHLnDP7A+fwBXXGRzQ==</latexit><latexit sha1_base64=4ton1cC0/WpHTbJYOP5RCFkc+ww=>AAAB83icbVDLSsNAFL3xWeur6tLNYBG6KokIuiy4cVnBPqAJZTK9aYdOJmFmIpTQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJU8G1cd1vZ2Nza3tnt7JX3T84PDqunZx2dZIphh2WiET1Q6pRcIkdw43AfqqQxqHAXji9K/zeEyrNE/loZikGMR1LHnFGjZV8P6ZmEkY5zofTYa3uNt0FyDrxSlKHEu1h7csfJSyLURomqNYDz01NkFNlOBM4r/qZxpSyKR3jwFJJY9RBvsg8J5dWGZEoUfZJQxbq742cxlrP4tBOFhn1qleI/3mDzES3Qc5lmhmUbHkoygQxCSkKICOukBkxs4QyxW1WwiZUUWZsTVVbgrf65XXSvWp6btN7uK63GmUdFTiHC2iABzfQgntoQwcYpPAMr/DmZM6L8+58LEc3nHLnDP7A+fwBXXGRzQ==</latexit><latexit sha1_base64=4ton1cC0/WpHTbJYOP5RCFkc+ww=>AAAB83icbVDLSsNAFL3xWeur6tLNYBG6KokIuiy4cVnBPqAJZTK9aYdOJmFmIpTQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJU8G1cd1vZ2Nza3tnt7JX3T84PDqunZx2dZIphh2WiET1Q6pRcIkdw43AfqqQxqHAXji9K/zeEyrNE/loZikGMR1LHnFGjZV8P6ZmEkY5zofTYa3uNt0FyDrxSlKHEu1h7csfJSyLURomqNYDz01NkFNlOBM4r/qZxpSyKR3jwFJJY9RBvsg8J5dWGZEoUfZJQxbq742cxlrP4tBOFhn1qleI/3mDzES3Qc5lmhmUbHkoygQxCSkKICOukBkxs4QyxW1WwiZUUWZsTVVbgrf65XXSvWp6btN7uK63GmUdFTiHC2iABzfQgntoQwcYpPAMr/DmZM6L8+58LEc3nHLnDP7A+fwBXXGRzQ==</latexit>uvskvrkuviek
Here we use graph to mean a directed, attributed multi-graph with a global attribute. In our
terminology, a node is denoted as vi, an edge as ek, and the global attributes as u. We also use
skandrkto indicate the indices of the sender and receiver nodes (see below), respectively, for
edgek. To be more precise, we dene these terms as:
Directed : one-way edges, from a sender node to a receiver node.
Attribute : properties that can be encoded as a vector, set, or even another graph.
Attributed : edges and vertices have attributes associated with them.
Global attribute : a graph-level attribute.
Multi-graph : there can be more than one edge between vertices, including self-edges.
Figure 2 shows a variety of dierent types of graphs corresponding to real data that we may be
interested in modeling, including physical systems, molecules, images, and text.
which takes a graph as input, performs computations over the structure, and returns a graph as
output. As described in Box 3, entities are represented by the graph's nodes , relations by the edges ,
and system-level properties by global attributes. The GN framework's block organization emphasizes
customizability and synthesizing new architectures which express desired relational inductive biases.
The key design principles are: Flexible representations (see Section 4.1); Congurable within-block
structure (see Section 4.2); and Composable multi-block architectures (see Section 4.3).
We introduce a motivating example to help make the GN formalism more concrete. Consider
predicting the movements a set of rubber balls in an arbitrary gravitational eld, which, instead of
bouncing against one another, each have one or more springs which connect them to some (or all) of
the others. We will refer to this running example throughout the denitions below, to motivate the
graph representation and the computations operating over it. Figure 2 depicts some other common
scenarios that can be represented by graphs and reasoned over using graph networks.
3.2.1 Denition of graph
Within our GN framework, a graph is dened as a 3-tuple G=(u;V;E )(see Box 3 for details of
graph representations). The uis a global attribute; for example, umight represent the gravitational
eld. TheV=fvigi=1:Nvis the set of nodes (of cardinality Nv), where each viis a node's attribute.
For example, Vmight represent each ball, with attributes for position, velocity, and mass. The
E=f(ek;rk;sk)gk=1:Neis the set of edges (of cardinality Ne), where each ekis the edge's attribute,
rkis the index of the receiver node, and skis the index of the sender node. For example, Emight
represent the presence of springs between dierent balls, and their corresponding spring constants.
11Algorithm 1 Steps of computation in a full GN block.
function GraphNetwork (E,V,u)
fork2f1:::Negdo
e0
k e(ek;vrk;vsk;u) .1. Compute updated edge attributes
end for
fori2f1:::Nngdo
letE0
i=f(e0
k;rk;sk)grk=i;k=1:Ne
 e0
i e!v(E0
i) .2. Aggregate edge attributes per node
v0
i v( e0
i;vi;u) .3. Compute updated node attributes
end for
letV0=fv0gi=1:Nv
letE0=f(e0
k;rk;sk)gk=1:Ne
 e0 e!u(E0) .4. Aggregate edge attributes globally
 v0 v!u(V0) .5. Aggregate node attributes globally
u0 u( e0; v0;u) .6. Compute updated global attribute
return (E0;V0;u0)
end function
3.2.2 Internal structure of a GN block
A GN block contains three update functions, , and three aggregation functions, ,
e0
k=e(ek;vrk;vsk;u)
v0
i=v
 e0
i;vi;u
u0=u
 e0; v0;u e0
i=e!v
E0
i
 e0=e!u
E0
 v0=v!u
V0(1)
whereE0
i=f(e0
k;rk;sk)grk=i;k=1:Ne,V0=fv0
igi=1:Nv, andE0=S
iE0
i=f(e0
k;rk;sk)gk=1:Ne.
Theeis mapped across all edges to compute per-edge updates, the vis mapped across all
nodes to compute per-node updates, and the uis applied once as the global update. The 
functions each take a set as input, and reduce it to a single element which represents the aggregated
information. Crucially, the functions must be invariant to permutations of their inputs, and should
take variable numbers of arguments (e.g., elementwise summation, mean, maximum, etc.).
3.2.3 Computational steps within a GN block
When a graph, G, is provided as input to a GN block, the computations proceed from the edge, to
the node, to the global level. Figure 3 shows a depiction of which graph elements are involved in
each of these computations, and Figure 4a shows a full GN block, with its update and aggregation
functions. Algorithm 1 shows the following steps of computation:
1.eis applied per edge, with arguments ( ek;vrk;vsk;u), and returns e0
k. In our springs example,
this might correspond to the forces or potential energies between two connected balls. The
set of resulting per-edge outputs for each node, i, is,E0
i=f(e0
k;rk;sk)grk=i;k=1:Ne. And
E0=S
iE0
i=f(e0
k;rk;sk)gk=1:Neis the set of all per-edge outputs.
2.e!vis applied to E0
i, and aggregates the edge updates for edges that project to vertex i, into
 e0
i, which will be used in the next step's node update. In our running example, this might
correspond to summing all the forces or potential energies acting on the ithball.
12vi
<latexit sha1_base64=UuhsKP3lpHlY+K0A8uvGImQtNkI=>AAAB83icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFNKJPppB06mYSZm0IJ/Q03LhRx68+482+ctllo64GBwzn3cs+cMJXCoOt+O6Wt7Z3dvfJ+5eDw6PikenrWMUmmGW+zRCa6F1LDpVC8jQIl76Wa0ziUvBtO7hd+d8q1EYl6wlnKg5iOlIgEo2gl348pjsMon84HYlCtuQ13CbJJvILUoEBrUP3yhwnLYq6QSWpM33NTDHKqUTDJ5xU/MzylbEJHvG+pojE3Qb7MPCdXVhmSKNH2KSRL9fdGTmNjZnFoJxcZzbq3EP/z+hlGd0EuVJohV2x1KMokwYQsCiBDoTlDObOEMi1sVsLGVFOGtqaKLcFb//Im6Vw3PLfhPd7UmvWijjJcwCXUwYNbaMIDtKANDFJ4hld4czLnxXl3PlajJafYOYc/cD5/AHRgkdw=</latexit><latexit sha1_base64=UuhsKP3lpHlY+K0A8uvGImQtNkI=>AAAB83icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFNKJPppB06mYSZm0IJ/Q03LhRx68+482+ctllo64GBwzn3cs+cMJXCoOt+O6Wt7Z3dvfJ+5eDw6PikenrWMUmmGW+zRCa6F1LDpVC8jQIl76Wa0ziUvBtO7hd+d8q1EYl6wlnKg5iOlIgEo2gl348pjsMon84HYlCtuQ13CbJJvILUoEBrUP3yhwnLYq6QSWpM33NTDHKqUTDJ5xU/MzylbEJHvG+pojE3Qb7MPCdXVhmSKNH2KSRL9fdGTmNjZnFoJxcZzbq3EP/z+hlGd0EuVJohV2x1KMokwYQsCiBDoTlDObOEMi1sVsLGVFOGtqaKLcFb//Im6Vw3PLfhPd7UmvWijjJcwCXUwYNbaMIDtKANDFJ4hld4czLnxXl3PlajJafYOYc/cD5/AHRgkdw=</latexit><latexit sha1_base64=UuhsKP3lpHlY+K0A8uvGImQtNkI=>AAAB83icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFNKJPppB06mYSZm0IJ/Q03LhRx68+482+ctllo64GBwzn3cs+cMJXCoOt+O6Wt7Z3dvfJ+5eDw6PikenrWMUmmGW+zRCa6F1LDpVC8jQIl76Wa0ziUvBtO7hd+d8q1EYl6wlnKg5iOlIgEo2gl348pjsMon84HYlCtuQ13CbJJvILUoEBrUP3yhwnLYq6QSWpM33NTDHKqUTDJ5xU/MzylbEJHvG+pojE3Qb7MPCdXVhmSKNH2KSRL9fdGTmNjZnFoJxcZzbq3EP/z+hlGd0EuVJohV2x1KMokwYQsCiBDoTlDObOEMi1sVsLGVFOGtqaKLcFb//Im6Vw3PLfhPd7UmvWijjJcwCXUwYNbaMIDtKANDFJ4hld4czLnxXl3PlajJafYOYc/cD5/AHRgkdw=</latexit><latexit sha1_base64=UuhsKP3lpHlY+K0A8uvGImQtNkI=>AAAB83icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFNKJPppB06mYSZm0IJ/Q03LhRx68+482+ctllo64GBwzn3cs+cMJXCoOt+O6Wt7Z3dvfJ+5eDw6PikenrWMUmmGW+zRCa6F1LDpVC8jQIl76Wa0ziUvBtO7hd+d8q1EYl6wlnKg5iOlIgEo2gl348pjsMon84HYlCtuQ13CbJJvILUoEBrUP3yhwnLYq6QSWpM33NTDHKqUTDJ5xU/MzylbEJHvG+pojE3Qb7MPCdXVhmSKNH2KSRL9fdGTmNjZnFoJxcZzbq3EP/z+hlGd0EuVJohV2x1KMokwYQsCiBDoTlDObOEMi1sVsLGVFOGtqaKLcFb//Im6Vw3PLfhPd7UmvWijjJcwCXUwYNbaMIDtKANDFJ4hld4czLnxXl3PlajJafYOYc/cD5/AHRgkdw=</latexit>u
<latexit sha1_base64=Wl/NKcf+4FQq41kPZqpr8GSpKP8=>AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GE/oUbF4q49W/c+TdO2iy09cDA4Zx7mXNPkAiujet+O6WNza3tnfJuZW//4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0j+WDmSXoR3QsecgZNVZ6HETUTIIwS+fDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzF+RpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslB/b2Q00noWBXYyT6hXvVz8z+unJrzxMy6T1KBky4/CVBATk/x8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZL+oowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucU/sD5/AHw2JD/</latexit><latexit sha1_base64=Wl/NKcf+4FQq41kPZqpr8GSpKP8=>AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GE/oUbF4q49W/c+TdO2iy09cDA4Zx7mXNPkAiujet+O6WNza3tnfJuZW//4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0j+WDmSXoR3QsecgZNVZ6HETUTIIwS+fDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzF+RpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslB/b2Q00noWBXYyT6hXvVz8z+unJrzxMy6T1KBky4/CVBATk/x8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZL+oowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucU/sD5/AHw2JD/</latexit><latexit sha1_base64=Wl/NKcf+4FQq41kPZqpr8GSpKP8=>AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GE/oUbF4q49W/c+TdO2iy09cDA4Zx7mXNPkAiujet+O6WNza3tnfJuZW//4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0j+WDmSXoR3QsecgZNVZ6HETUTIIwS+fDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzF+RpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslB/b2Q00noWBXYyT6hXvVz8z+unJrzxMy6T1KBky4/CVBATk/x8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZL+oowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucU/sD5/AHw2JD/</latexit><latexit sha1_base64=Wl/NKcf+4FQq41kPZqpr8GSpKP8=>AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GE/oUbF4q49W/c+TdO2iy09cDA4Zx7mXNPkAiujet+O6WNza3tnfJuZW//4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0j+WDmSXoR3QsecgZNVZ6HETUTIIwS+fDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzF+RpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslB/b2Q00noWBXYyT6hXvVz8z+unJrzxMy6T1KBky4/CVBATk/x8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZL+oowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucU/sD5/AHw2JD/</latexit>e0k
<latexit sha1_base64=a1hco1MShws4KpmpFnenOcfEqyc=>AAAB9HicdVDLSgMxFM34rPVVdekmWMSuhkxtsd0V3LisYB/QDiWT3mlDMw+TTKEM/Q43LhRx68e482/MtBVU9EDgcM693JPjxYIrTciHtba+sbm1ndvJ7+7tHxwWjo7bKkokgxaLRCS7HlUgeAgtzbWAbiyBBp6Ajje5zvzOFKTiUXinZzG4AR2F3OeMaiO5/YDqseenML8YTAaFIrGdCilXy5jY1XqdVOqG1KqXpEywY5MFimiF5qDw3h9GLAkg1ExQpXoOibWbUqk5EzDP9xMFMWUTOoKeoSENQLnpIvQcnxtliP1ImhdqvFC/b6Q0UGoWeGYyC6l+e5n4l9dLtF9zUx7GiYaQLQ/5icA6wlkDeMglMC1mhlAmucmK2ZhKyrTpKW9K+Pop/p+0y7ZjurqtFBulVR05dIrOUAk56Ao10A1qohZi6B49oCf0bE2tR+vFel2OrlmrnRP0A9bbJyRskkI=</latexit><latexit sha1_base64=a1hco1MShws4KpmpFnenOcfEqyc=>AAAB9HicdVDLSgMxFM34rPVVdekmWMSuhkxtsd0V3LisYB/QDiWT3mlDMw+TTKEM/Q43LhRx68e482/MtBVU9EDgcM693JPjxYIrTciHtba+sbm1ndvJ7+7tHxwWjo7bKkokgxaLRCS7HlUgeAgtzbWAbiyBBp6Ajje5zvzOFKTiUXinZzG4AR2F3OeMaiO5/YDqseenML8YTAaFIrGdCilXy5jY1XqdVOqG1KqXpEywY5MFimiF5qDw3h9GLAkg1ExQpXoOibWbUqk5EzDP9xMFMWUTOoKeoSENQLnpIvQcnxtliP1ImhdqvFC/b6Q0UGoWeGYyC6l+e5n4l9dLtF9zUx7GiYaQLQ/5icA6wlkDeMglMC1mhlAmucmK2ZhKyrTpKW9K+Pop/p+0y7ZjurqtFBulVR05dIrOUAk56Ao10A1qohZi6B49oCf0bE2tR+vFel2OrlmrnRP0A9bbJyRskkI=</latexit><latexit sha1_base64=a1hco1MShws4KpmpFnenOcfEqyc=>AAAB9HicdVDLSgMxFM34rPVVdekmWMSuhkxtsd0V3LisYB/QDiWT3mlDMw+TTKEM/Q43LhRx68e482/MtBVU9EDgcM693JPjxYIrTciHtba+sbm1ndvJ7+7tHxwWjo7bKkokgxaLRCS7HlUgeAgtzbWAbiyBBp6Ajje5zvzOFKTiUXinZzG4AR2F3OeMaiO5/YDqseenML8YTAaFIrGdCilXy5jY1XqdVOqG1KqXpEywY5MFimiF5qDw3h9GLAkg1ExQpXoOibWbUqk5EzDP9xMFMWUTOoKeoSENQLnpIvQcnxtliP1ImhdqvFC/b6Q0UGoWeGYyC6l+e5n4l9dLtF9zUx7GiYaQLQ/5icA6wlkDeMglMC1mhlAmucmK2ZhKyrTpKW9K+Pop/p+0y7ZjurqtFBulVR05dIrOUAk56Ao10A1qohZi6B49oCf0bE2tR+vFel2OrlmrnRP0A9bbJyRskkI=</latexit><latexit sha1_base64=a1hco1MShws4KpmpFnenOcfEqyc=>AAAB9HicdVDLSgMxFM34rPVVdekmWMSuhkxtsd0V3LisYB/QDiWT3mlDMw+TTKEM/Q43LhRx68e482/MtBVU9EDgcM693JPjxYIrTciHtba+sbm1ndvJ7+7tHxwWjo7bKkokgxaLRCS7HlUgeAgtzbWAbiyBBp6Ajje5zvzOFKTiUXinZzG4AR2F3OeMaiO5/YDqseenML8YTAaFIrGdCilXy5jY1XqdVOqG1KqXpEywY5MFimiF5qDw3h9GLAkg1ExQpXoOibWbUqk5EzDP9xMFMWUTOoKeoSENQLnpIvQcnxtliP1ImhdqvFC/b6Q0UGoWeGYyC6l+e5n4l9dLtF9zUx7GiYaQLQ/5icA6wlkDeMglMC1mhlAmucmK2ZhKyrTpKW9K+Pop/p+0y7ZjurqtFBulVR05dIrOUAk56Ao10A1qohZi6B49oCf0bE2tR+vFel2OrlmrnRP0A9bbJyRskkI=</latexit>(a) Edge update
u
<latexit sha1_base64=Wl/NKcf+4FQq41kPZqpr8GSpKP8=>AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GE/oUbF4q49W/c+TdO2iy09cDA4Zx7mXNPkAiujet+O6WNza3tnfJuZW//4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0j+WDmSXoR3QsecgZNVZ6HETUTIIwS+fDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzF+RpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslB/b2Q00noWBXYyT6hXvVz8z+unJrzxMy6T1KBky4/CVBATk/x8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZL+oowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucU/sD5/AHw2JD/</latexit><latexit sha1_base64=Wl/NKcf+4FQq41kPZqpr8GSpKP8=>AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GE/oUbF4q49W/c+TdO2iy09cDA4Zx7mXNPkAiujet+O6WNza3tnfJuZW//4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0j+WDmSXoR3QsecgZNVZ6HETUTIIwS+fDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzF+RpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslB/b2Q00noWBXYyT6hXvVz8z+unJrzxMy6T1KBky4/CVBATk/x8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZL+oowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucU/sD5/AHw2JD/</latexit><latexit sha1_base64=Wl/NKcf+4FQq41kPZqpr8GSpKP8=>AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GE/oUbF4q49W/c+TdO2iy09cDA4Zx7mXNPkAiujet+O6WNza3tnfJuZW//4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0j+WDmSXoR3QsecgZNVZ6HETUTIIwS+fDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzF+RpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslB/b2Q00noWBXYyT6hXvVz8z+unJrzxMy6T1KBky4/CVBATk/x8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZL+oowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucU/sD5/AHw2JD/</latexit><latexit sha1_base64=Wl/NKcf+4FQq41kPZqpr8GSpKP8=>AAAB8XicbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9oFtKJPpTTt0MgkzE6GE/oUbF4q49W/c+TdO2iy09cDA4Zx7mXNPkAiujet+O6WNza3tnfJuZW//4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0j+WDmSXoR3QsecgZNVZ6HETUTIIwS+fDas1tuAuQdeIVpAYFWsPq12AUszRCaZigWvc9NzF+RpXhTOC8Mkg1JpRN6Rj7lkoaofazReI5ubDKiISxsk8aslB/b2Q00noWBXYyT6hXvVz8z+unJrzxMy6T1KBky4/CVBATk/x8MuIKmREzSyhT3GYlbEIVZcaWVLEleKsnr5POZcNzG979Va1ZL+oowxmcQx08uIYm3EEL2sBAwjO8wpujnRfn3flYjpacYucU/sD5/AHw2JD/</latexit>e0k
<latexit sha1_base64=TmBm7ikN3ChoJpDcsfwhm1T5rLk=>AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvYVZkRQZcFNy4r2Ae0Q8mkd9rQTGZMMoUy9DvcuFDErR/jzr8x03ahrQcCh3Pu5Z6cIBFcG9f9dgobm1vbO8Xd0t7+weFR+fikpeNUMWyyWMSqE1CNgktsGm4EdhKFNAoEtoPxXe63J6g0j+WjmSboR3QoecgZNVbyexE1oyDMcHbZH/fLFbfmzkHWibckFVii0S9/9QYxSyOUhgmqdddzE+NnVBnOBM5KvVRjQtmYDrFrqaQRaj+bh56RC6sMSBgr+6Qhc/X3RkYjradRYCfzkHrVy8X/vG5qwls/4zJJDUq2OBSmgpiY5A2QAVfIjJhaQpniNithI6ooM7anki3BW/3yOmld1Ty35j1cV+rVZR1FOINzqIIHN1CHe2hAExg8wTO8wpszcV6cd+djMVpwljun8AfO5w/CAJH+</latexit><latexit sha1_base64=TmBm7ikN3ChoJpDcsfwhm1T5rLk=>AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvYVZkRQZcFNy4r2Ae0Q8mkd9rQTGZMMoUy9DvcuFDErR/jzr8x03ahrQcCh3Pu5Z6cIBFcG9f9dgobm1vbO8Xd0t7+weFR+fikpeNUMWyyWMSqE1CNgktsGm4EdhKFNAoEtoPxXe63J6g0j+WjmSboR3QoecgZNVbyexE1oyDMcHbZH/fLFbfmzkHWibckFVii0S9/9QYxSyOUhgmqdddzE+NnVBnOBM5KvVRjQtmYDrFrqaQRaj+bh56RC6sMSBgr+6Qhc/X3RkYjradRYCfzkHrVy8X/vG5qwls/4zJJDUq2OBSmgpiY5A2QAVfIjJhaQpniNithI6ooM7anki3BW/3yOmld1Ty35j1cV+rVZR1FOINzqIIHN1CHe2hAExg8wTO8wpszcV6cd+djMVpwljun8AfO5w/CAJH+</latexit><latexit sha1_base64=TmBm7ikN3ChoJpDcsfwhm1T5rLk=>AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvYVZkRQZcFNy4r2Ae0Q8mkd9rQTGZMMoUy9DvcuFDErR/jzr8x03ahrQcCh3Pu5Z6cIBFcG9f9dgobm1vbO8Xd0t7+weFR+fikpeNUMWyyWMSqE1CNgktsGm4EdhKFNAoEtoPxXe63J6g0j+WjmSboR3QoecgZNVbyexE1oyDMcHbZH/fLFbfmzkHWibckFVii0S9/9QYxSyOUhgmqdddzE+NnVBnOBM5KvVRjQtmYDrFrqaQRaj+bh56RC6sMSBgr+6Qhc/X3RkYjradRYCfzkHrVy8X/vG5qwls/4zJJDUq2OBSmgpiY5A2QAVfIjJhaQpniNithI6ooM7anki3BW/3yOmld1Ty35j1cV+rVZR1FOINzqIIHN1CHe2hAExg8wTO8wpszcV6cd+djMVpwljun8AfO5w/CAJH+</latexit><latexit sha1_base64=TmBm7ikN3ChoJpDcsfwhm1T5rLk=>AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvYVZkRQZcFNy4r2Ae0Q8mkd9rQTGZMMoUy9DvcuFDErR/jzr8x03ahrQcCh3Pu5Z6cIBFcG9f9dgobm1vbO8Xd0t7+weFR+fikpeNUMWyyWMSqE1CNgktsGm4EdhKFNAoEtoPxXe63J6g0j+WjmSboR3QoecgZNVbyexE1oyDMcHbZH/fLFbfmzkHWibckFVii0S9/9QYxSyOUhgmqdddzE+NnVBnOBM5KvVRjQtmYDrFrqaQRaj+bh56RC6sMSBgr+6Qhc/X3RkYjradRYCfzkHrVy8X/vG5qwls/4zJJDUq2OBSmgpiY5A2QAVfIjJhaQpniNithI6ooM7anki3BW/3yOmld1Ty35j1cV+rVZR1FOINzqIIHN1CHe2hAExg8wTO8wpszcV6cd+djMVpwljun8AfO5w/CAJH+</latexit>v0i
<latexit sha1_base64=eeLXdOBZMDToGpT2JKCAlGanLL8=>AAAB9HicdVDLSgMxFL3js9ZX1aWbYBG7GjK1xXZXcOOygn1AO5RMmmlDMw+TTKEM/Q43LhRx68e482/MtBVU9EDgcM693JPjxYIrjfGHtba+sbm1ndvJ7+7tHxwWjo7bKkokZS0aiUh2PaKY4CFraa4F68aSkcATrONNrjO/M2VS8Si807OYuQEZhdznlGgjuf2A6LHnp9P5xYAPCkVsOxVcrpYRtqv1Oq7UDalVL3EZI8fGCxRhheag8N4fRjQJWKipIEr1HBxrNyVScyrYPN9PFIsJnZAR6xkakoApN12EnqNzowyRH0nzQo0W6veNlARKzQLPTGYh1W8vE//yeon2a27KwzjRLKTLQ34ikI5Q1gAacsmoFjNDCJXcZEV0TCSh2vSUNyV8/RT9T9pl2zFd3VaKjdKqjhycwhmUwIEraMANNKEFFO7hAZ7g2Zpaj9aL9bocXbNWOyfwA9bbJztsklE=</latexit><latexit sha1_base64=eeLXdOBZMDToGpT2JKCAlGanLL8=>AAAB9HicdVDLSgMxFL3js9ZX1aWbYBG7GjK1xXZXcOOygn1AO5RMmmlDMw+TTKEM/Q43LhRx68e482/MtBVU9EDgcM693JPjxYIrjfGHtba+sbm1ndvJ7+7tHxwWjo7bKkokZS0aiUh2PaKY4CFraa4F68aSkcATrONNrjO/M2VS8Si807OYuQEZhdznlGgjuf2A6LHnp9P5xYAPCkVsOxVcrpYRtqv1Oq7UDalVL3EZI8fGCxRhheag8N4fRjQJWKipIEr1HBxrNyVScyrYPN9PFIsJnZAR6xkakoApN12EnqNzowyRH0nzQo0W6veNlARKzQLPTGYh1W8vE//yeon2a27KwzjRLKTLQ34ikI5Q1gAacsmoFjNDCJXcZEV0TCSh2vSUNyV8/RT9T9pl2zFd3VaKjdKqjhycwhmUwIEraMANNKEFFO7hAZ7g2Zpaj9aL9bocXbNWOyfwA9bbJztsklE=</latexit><latexit sha1_base64=eeLXdOBZMDToGpT2JKCAlGanLL8=>AAAB9HicdVDLSgMxFL3js9ZX1aWbYBG7GjK1xXZXcOOygn1AO5RMmmlDMw+TTKEM/Q43LhRx68e482/MtBVU9EDgcM693JPjxYIrjfGHtba+sbm1ndvJ7+7tHxwWjo7bKkokZS0aiUh2PaKY4CFraa4F68aSkcATrONNrjO/M2VS8Si807OYuQEZhdznlGgjuf2A6LHnp9P5xYAPCkVsOxVcrpYRtqv1Oq7UDalVL3EZI8fGCxRhheag8N4fRjQJWKipIEr1HBxrNyVScyrYPN9PFIsJnZAR6xkakoApN12EnqNzowyRH0nzQo0W6veNlARKzQLPTGYh1W8vE//yeon2a27KwzjRLKTLQ34ikI5Q1gAacsmoFjNDCJXcZEV0TCSh2vSUNyV8/RT9T9pl2zFd3VaKjdKqjhycwhmUwIEraMANNKEFFO7hAZ7g2Zpaj9aL9bocXbNWOyfwA9bbJztsklE=</latexit><latexit sha1_base64=eeLXdOBZMDToGpT2JKCAlGanLL8=>AAAB9HicdVDLSgMxFL3js9ZX1aWbYBG7GjK1xXZXcOOygn1AO5RMmmlDMw+TTKEM/Q43LhRx68e482/MtBVU9EDgcM693JPjxYIrjfGHtba+sbm1ndvJ7+7tHxwWjo7bKkokZS0aiUh2PaKY4CFraa4F68aSkcATrONNrjO/M2VS8Si807OYuQEZhdznlGgjuf2A6LHnp9P5xYAPCkVsOxVcrpYRtqv1Oq7UDalVL3EZI8fGCxRhheag8N4fRjQJWKipIEr1HBxrNyVScyrYPN9PFIsJnZAR6xkakoApN12EnqNzowyRH0nzQo0W6veNlARKzQLPTGYh1W8vE//yeon2a27KwzjRLKTLQ34ikI5Q1gAacsmoFjNDCJXcZEV0TCSh2vSUNyV8/RT9T9pl2zFd3VaKjdKqjhycwhmUwIEraMANNKEFFO7hAZ7g2Zpaj9aL9bocXbNWOyfwA9bbJztsklE=</latexit> (b) Node update
u0
<latexit sha1_base64=RuJ/WOWmv0qWsx0aAsZGj4qvbr4=>AAAB8nicdVDJSgNBEO2JW4xb1KOXxiDmNMxk0cwt4MVjBLPAZAg9nZ6kSc9Cd40YhnyGFw+KePVrvPk3dhZBRR8UPN6roqqenwiuwLI+jNza+sbmVn67sLO7t39QPDzqqDiVlLVpLGLZ84ligkesDRwE6yWSkdAXrOtPruZ+945JxePoFqYJ80IyinjAKQEtuX1g9+AHWTo7HxRLllm5aFTqFrbMulOrNmqaNCpO1XGwbVoLlNAKrUHxvT+MaRqyCKggSrm2lYCXEQmcCjYr9FPFEkInZMRcTSMSMuVli5Nn+EwrQxzEUlcEeKF+n8hIqNQ09HVnSGCsfntz8S/PTSFoeBmPkhRYRJeLglRgiPH8fzzkklEQU00IlVzfiumYSEJBp1TQIXx9iv8nnYppW6Z9Uys1y6s48ugEnaIystElaqJr1EJtRFGMHtATejbAeDRejNdla85YzRyjHzDePgEJTZGr</latexit><latexit sha1_base64=RuJ/WOWmv0qWsx0aAsZGj4qvbr4=>AAAB8nicdVDJSgNBEO2JW4xb1KOXxiDmNMxk0cwt4MVjBLPAZAg9nZ6kSc9Cd40YhnyGFw+KePVrvPk3dhZBRR8UPN6roqqenwiuwLI+jNza+sbmVn67sLO7t39QPDzqqDiVlLVpLGLZ84ligkesDRwE6yWSkdAXrOtPruZ+945JxePoFqYJ80IyinjAKQEtuX1g9+AHWTo7HxRLllm5aFTqFrbMulOrNmqaNCpO1XGwbVoLlNAKrUHxvT+MaRqyCKggSrm2lYCXEQmcCjYr9FPFEkInZMRcTSMSMuVli5Nn+EwrQxzEUlcEeKF+n8hIqNQ09HVnSGCsfntz8S/PTSFoeBmPkhRYRJeLglRgiPH8fzzkklEQU00IlVzfiumYSEJBp1TQIXx9iv8nnYppW6Z9Uys1y6s48ugEnaIystElaqJr1EJtRFGMHtATejbAeDRejNdla85YzRyjHzDePgEJTZGr</latexit><latexit sha1_base64=RuJ/WOWmv0qWsx0aAsZGj4qvbr4=>AAAB8nicdVDJSgNBEO2JW4xb1KOXxiDmNMxk0cwt4MVjBLPAZAg9nZ6kSc9Cd40YhnyGFw+KePVrvPk3dhZBRR8UPN6roqqenwiuwLI+jNza+sbmVn67sLO7t39QPDzqqDiVlLVpLGLZ84ligkesDRwE6yWSkdAXrOtPruZ+945JxePoFqYJ80IyinjAKQEtuX1g9+AHWTo7HxRLllm5aFTqFrbMulOrNmqaNCpO1XGwbVoLlNAKrUHxvT+MaRqyCKggSrm2lYCXEQmcCjYr9FPFEkInZMRcTSMSMuVli5Nn+EwrQxzEUlcEeKF+n8hIqNQ09HVnSGCsfntz8S/PTSFoeBmPkhRYRJeLglRgiPH8fzzkklEQU00IlVzfiumYSEJBp1TQIXx9iv8nnYppW6Z9Uys1y6s48ugEnaIystElaqJr1EJtRFGMHtATejbAeDRejNdla85YzRyjHzDePgEJTZGr</latexit><latexit sha1_base64=RuJ/WOWmv0qWsx0aAsZGj4qvbr4=>AAAB8nicdVDJSgNBEO2JW4xb1KOXxiDmNMxk0cwt4MVjBLPAZAg9nZ6kSc9Cd40YhnyGFw+KePVrvPk3dhZBRR8UPN6roqqenwiuwLI+jNza+sbmVn67sLO7t39QPDzqqDiVlLVpLGLZ84ligkesDRwE6yWSkdAXrOtPruZ+945JxePoFqYJ80IyinjAKQEtuX1g9+AHWTo7HxRLllm5aFTqFrbMulOrNmqaNCpO1XGwbVoLlNAKrUHxvT+MaRqyCKggSrm2lYCXEQmcCjYr9FPFEkInZMRcTSMSMuVli5Nn+EwrQxzEUlcEeKF+n8hIqNQ09HVnSGCsfntz8S/PTSFoeBmPkhRYRJeLglRgiPH8fzzkklEQU00IlVzfiumYSEJBp1TQIXx9iv8nnYppW6Z9Uys1y6s48ugEnaIystElaqJr1EJtRFGMHtATejbAeDRejNdla85YzRyjHzDePgEJTZGr</latexit>e0k
<latexit sha1_base64=Iztn6Umi7rLG5lNF0JpW0x6J+s0=>AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120i7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAE/YgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3Sl/dfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23Sp+GsbalkM7V3xMZi4yZRIHtjBgOzbI3E//zOimGN34mVJIiKL5YFKaSYkxnCdC+0MBRTixhXAt7K+VDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IX/gfP4A6+WSGQ==</latexit><latexit sha1_base64=Iztn6Umi7rLG5lNF0JpW0x6J+s0=>AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120i7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAE/YgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3Sl/dfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23Sp+GsbalkM7V3xMZi4yZRIHtjBgOzbI3E//zOimGN34mVJIiKL5YFKaSYkxnCdC+0MBRTixhXAt7K+VDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IX/gfP4A6+WSGQ==</latexit><latexit sha1_base64=Iztn6Umi7rLG5lNF0JpW0x6J+s0=>AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120i7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAE/YgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3Sl/dfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23Sp+GsbalkM7V3xMZi4yZRIHtjBgOzbI3E//zOimGN34mVJIiKL5YFKaSYkxnCdC+0MBRTixhXAt7K+VDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IX/gfP4A6+WSGQ==</latexit><latexit sha1_base64=hP+6LrUf2d3tZaldqaQQvEKMXyw=>AAAB2XicbZDNSgMxFIXv1L86Vq1rN8EiuCozbnQpuHFZwbZCO5RM5k4bmskMyR2hDH0BF25EfC93vo3pz0JbDwQ+zknIvSculLQUBN9ebWd3b/+gfugfNfzjk9Nmo2fz0gjsilzl5jnmFpXU2CVJCp8LgzyLFfbj6f0i77+gsTLXTzQrMMr4WMtUCk7O6oyaraAdLMW2IVxDC9YaNb+GSS7KDDUJxa0dhEFBUcUNSaFw7g9LiwUXUz7GgUPNM7RRtRxzzi6dk7A0N+5oYkv394uKZ9bOstjdzDhN7Ga2MP/LBiWlt1EldVESarH6KC0Vo5wtdmaJNChIzRxwYaSblYkJN1yQa8Z3HYSbG29D77odBu3wMYA6nMMFXEEIN3AHD9CBLghI4BXevYn35n2suqp569LO4I+8zx84xIo4</latexit><latexit sha1_base64=ywz7v1q7Yrl4nBX/+QcnkaM0kGo=>AAAB6XicbZBLSwMxFIXv+Kz1Vd26CRbRVZlxo0vBjcsK9gHtUDLpnRqayYzJnWIZ+jvcuFDEP+TOf2P6WGjrgcDHOQn35kSZkpZ8/9tbW9/Y3Nou7ZR39/YPDitHe02b5kZgQ6QqNe2IW1RSY4MkKWxnBnkSKWxFw9tp3hqhsTLVDzTOMEz4QMtYCk7OCruEzxTFBU7Oe8NeperX/JnYKgQLqMJC9V7lq9tPRZ6gJqG4tZ3AzygsuCEpFE7K3dxixsWQD7DjUPMEbVjMlp6wM+f0WZwadzSxmfv7RcETa8dJ5G4mnB7tcjY1/8s6OcXXYSF1lhNqMR8U54pRyqYNsL40KEiNHXBhpNuViUduuCDXU9mVECx/eRWal7XArwX3PpTgBE7hAgK4ghu4gzo0QMATvMAbvHsj79X7mNe15i16O4Y/8j5/AKfckNQ=</latexit><latexit sha1_base64=ywz7v1q7Yrl4nBX/+QcnkaM0kGo=>AAAB6XicbZBLSwMxFIXv+Kz1Vd26CRbRVZlxo0vBjcsK9gHtUDLpnRqayYzJnWIZ+jvcuFDEP+TOf2P6WGjrgcDHOQn35kSZkpZ8/9tbW9/Y3Nou7ZR39/YPDitHe02b5kZgQ6QqNe2IW1RSY4MkKWxnBnkSKWxFw9tp3hqhsTLVDzTOMEz4QMtYCk7OCruEzxTFBU7Oe8NeperX/JnYKgQLqMJC9V7lq9tPRZ6gJqG4tZ3AzygsuCEpFE7K3dxixsWQD7DjUPMEbVjMlp6wM+f0WZwadzSxmfv7RcETa8dJ5G4mnB7tcjY1/8s6OcXXYSF1lhNqMR8U54pRyqYNsL40KEiNHXBhpNuViUduuCDXU9mVECx/eRWal7XArwX3PpTgBE7hAgK4ghu4gzo0QMATvMAbvHsj79X7mNe15i16O4Y/8j5/AKfckNQ=</latexit><latexit sha1_base64=Wxt2EGaSkqmyg6rX9KQvpR9rldE=>AAAB9HicbVA9TwJBEN3DL8Qv1NJmIzFSkTsbLUlsLDGRjwQuZG+Zgw17e+fuHJFc+B02Fhpj64+x89+4wBUKvmSSl/dmMjMvSKQw6LrfTmFjc2t7p7hb2ts/ODwqH5+0TJxqDk0ey1h3AmZACgVNFCihk2hgUSChHYxv5357AtqIWD3gNAE/YkMlQsEZWsnvITxhEGYwu+yP++WKW3MXoOvEy0mF5Gj0y1+9QczTCBRyyYzpem6CfsY0Ci5hVuqlBhLGx2wIXUsVi8D42eLoGb2wyoCGsbalkC7U3xMZi4yZRoHtjBiOzKo3F//zuimGN34mVJIiKL5cFKaSYkznCdCB0MBRTi1hXAt7K+UjphlHm1PJhuCtvrxOWlc1z615926lXs3jKJIzck6qxCPXpE7uSIM0CSeP5Jm8kjdn4rw4787HsrXg5DOn5A+czx/qpZIV</latexit><latexit sha1_base64=Iztn6Umi7rLG5lNF0JpW0x6J+s0=>AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120i7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAE/YgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3Sl/dfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23Sp+GsbalkM7V3xMZi4yZRIHtjBgOzbI3E//zOimGN34mVJIiKL5YFKaSYkxnCdC+0MBRTixhXAt7K+VDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IX/gfP4A6+WSGQ==</latexit><latexit sha1_base64=Iztn6Umi7rLG5lNF0JpW0x6J+s0=>AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120i7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAE/YgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3Sl/dfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23Sp+GsbalkM7V3xMZi4yZRIHtjBgOzbI3E//zOimGN34mVJIiKL5YFKaSYkxnCdC+0MBRTixhXAt7K+VDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IX/gfP4A6+WSGQ==</latexit><latexit sha1_base64=Iztn6Umi7rLG5lNF0JpW0x6J+s0=>AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120i7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAE/YgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3Sl/dfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23Sp+GsbalkM7V3xMZi4yZRIHtjBgOzbI3E//zOimGN34mVJIiKL5YFKaSYkxnCdC+0MBRTixhXAt7K+VDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IX/gfP4A6+WSGQ==</latexit><latexit sha1_base64=Iztn6Umi7rLG5lNF0JpW0x6J+s0=>AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120i7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAE/YgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3Sl/dfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23Sp+GsbalkM7V3xMZi4yZRIHtjBgOzbI3E//zOimGN34mVJIiKL5YFKaSYkxnCdC+0MBRTixhXAt7K+VDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IX/gfP4A6+WSGQ==</latexit><latexit sha1_base64=Iztn6Umi7rLG5lNF0JpW0x6J+s0=>AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120i7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAE/YgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3Sl/dfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23Sp+GsbalkM7V3xMZi4yZRIHtjBgOzbI3E//zOimGN34mVJIiKL5YFKaSYkxnCdC+0MBRTixhXAt7K+VDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IX/gfP4A6+WSGQ==</latexit><latexit sha1_base64=Iztn6Umi7rLG5lNF0JpW0x6J+s0=>AAAB9HicbVBNS8NAEN34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120i7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqDg0ey1i3A2ZACgUNFCihnWhgUSChFYxuZ35rDNqIWD3gJAE/YgMlQsEZWsnvIjxhEGYwveiNeqWyW3XnoKvEy0mZ5Kj3Sl/dfszTCBRyyYzpeG6CfsY0Ci5hWuymBhLGR2wAHUsVi8D42fzoKT23Sp+GsbalkM7V3xMZi4yZRIHtjBgOzbI3E//zOimGN34mVJIiKL5YFKaSYkxnCdC+0MBRTixhXAt7K+VDphlHm1PRhuAtv7xKmpdVz61691flWiWPo0BOyRmpEI9ckxq5I3XSIJw8kmfySt6csfPivDsfi9Y1J585IX/gfP4A6+WSGQ==</latexit>v0i
<latexit sha1_base64=PT7VlVtIO1b4RdkSG9z8jpkhSqk=>AAAB9HicbVBNS8NAEJ34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120y7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqxhsslrFuB9RwKRRvoEDJ24nmNAokbwWj25nfGnNtRKwecJJwP6IDJULBKFrJ7yJ/wiDMxtOLnuiVym7VnYOsEi8nZchR75W+uv2YpRFXyCQ1puO5CfoZ1SiY5NNiNzU8oWxEB7xjqaIRN342P3pKzq3SJ2GsbSkkc/X3REYjYyZRYDsjikOz7M3E/7xOiuGNnwmVpMgVWywKU0kwJrMESF9ozlBOLKFMC3srYUOqKUObU9GG4C2/vEqal1XPrXr3V+VaJY+jAKdwBhXw4BpqcAd1aACDR3iGV3hzxs6L8+58LFrXnHzmBP7A+fwBAvSSKA==</latexit><latexit sha1_base64=PT7VlVtIO1b4RdkSG9z8jpkhSqk=>AAAB9HicbVBNS8NAEJ34WetX1aOXxSL2VBIR9Fjw4rGC/YA2lM120y7dbOLupFhCf4cXD4p49cd489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqxhsslrFuB9RwKRRvoEDJ24nmNAokbwWj25nfGnNtRKwecJJwP6IDJULBKFrJ7yJ/wiDMxtOLnuiVym7VnYOsEi8nZchR75W+uv2YpRFXyCQ1puO5CfoZ1SiY5NNiNzU8oWxEB7xjqaIRN342P3pKzq3SJ2GsbSkkc/X3REYjYyZRYDsjikOz7M3E/7xOiuGNnwmVpMgVWywKU0kwJrMESF9ozlBOLKFMC3srYUOqKUObU9GG4C2/vEqal1XPrXr3V+VaJY+jAKdwBhXw4BpqcAd1aACDR3iGV3hzxs6L8+58LFrXnHzmBP7A+fwBAvSSKA==</latexit><latexit sha1_base64=6WjtAQy1eEki3DeLmUkkI9Sk/Os=>AAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0Xc+kvu/BunaRbaemDgcM65zL0nSAXXxnW/ncrG5tb2TnW3trd/cHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOB/WB6u/D7T6g0T+SDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzV+TpXhTOC8Nso0ppRN6RiHlkoao/bzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQ//OGmYlu/JzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3v/qrRbpZ1VOEMzqEJHlxDG+6gA11gEMIzvMKbw50X5935WEYrTjlzCn/gfP4AA6WMYA==</latexit><latexit sha1_base64=6WjtAQy1eEki3DeLmUkkI9Sk/Os=>AAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0Xc+kvu/BunaRbaemDgcM65zL0nSAXXxnW/ncrG5tb2TnW3trd/cHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOB/WB6u/D7T6g0T+SDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzV+TpXhTOC8Nso0ppRN6RiHlkoao/bzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQ//OGmYlu/JzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3v/qrRbpZ1VOEMzqEJHlxDG+6gA11gEMIzvMKbw50X5935WEYrTjlzCn/gfP4AA6WMYA==</latexit><latexit sha1_base64=hP+6LrUf2d3tZaldqaQQvEKMXyw=>AAAB2XicbZDNSgMxFIXv1L86Vq1rN8EiuCozbnQpuHFZwbZCO5RM5k4bmskMyR2hDH0BF25EfC93vo3pz0JbDwQ+zknIvSculLQUBN9ebWd3b/+gfugfNfzjk9Nmo2fz0gjsilzl5jnmFpXU2CVJCp8LgzyLFfbj6f0i77+gsTLXTzQrMMr4WMtUCk7O6oyaraAdLMW2IVxDC9YaNb+GSS7KDDUJxa0dhEFBUcUNSaFw7g9LiwUXUz7GgUPNM7RRtRxzzi6dk7A0N+5oYkv394uKZ9bOstjdzDhN7Ga2MP/LBiWlt1EldVESarH6KC0Vo5wtdmaJNChIzRxwYaSblYkJN1yQa8Z3HYSbG29D77odBu3wMYA6nMMFXEEIN3AHD9CBLghI4BXevYn35n2suqp569LO4I+8zx84xIo4</latexit><latexit sha1_base64=OfCCjkcIiyDbxmNKxve032U7QH4=>AAAB3HicbVBNS8NAEJ3Ur1qrVq9eFovgqSRe9Ch48VjBfkAbZLOZtEs3m7A7EUroH/DiQRF/lzf/jduPg7Y+GHi8N8PMvChX0pLvf3uVre2d3b3qfu2gfnh03Dipd21WGIEdkanM9CNuUUmNHZKksJ8b5GmksBdN7uZ+7xmNlZl+pGmOYcpHWiZScHJS+6nR9Fv+AmyTBCvShBVc/9cwzkSRoiahuLWDwM8pLLkhKRTOasPCYs7FhI9w4KjmKdqwXNw5YxdOiVmSGVea2EL9PVHy1NppGrnOlNPYrntz8T9vUFByE5ZS5wWhFstFSaEYZWz+NIulQUFq6ggXRrpbmRhzwwW5aGouhGD95U3SvWoFfit48KEKZ3AOlxDANdzCPbShAwJieIE3ePek9+p9LOOqeKvcTuEPvM8f/CqLKA==</latexit><latexit sha1_base64=OfCCjkcIiyDbxmNKxve032U7QH4=>AAAB3HicbVBNS8NAEJ3Ur1qrVq9eFovgqSRe9Ch48VjBfkAbZLOZtEs3m7A7EUroH/DiQRF/lzf/jduPg7Y+GHi8N8PMvChX0pLvf3uVre2d3b3qfu2gfnh03Dipd21WGIEdkanM9CNuUUmNHZKksJ8b5GmksBdN7uZ+7xmNlZl+pGmOYcpHWiZScHJS+6nR9Fv+AmyTBCvShBVc/9cwzkSRoiahuLWDwM8pLLkhKRTOasPCYs7FhI9w4KjmKdqwXNw5YxdOiVmSGVea2EL9PVHy1NppGrnOlNPYrntz8T9vUFByE5ZS5wWhFstFSaEYZWz+NIulQUFq6ggXRrpbmRhzwwW5aGouhGD95U3SvWoFfit48KEKZ3AOlxDANdzCPbShAwJieIE3ePek9+p9LOOqeKvcTuEPvM8f/CqLKA==</latexit><latexit sha1_base64=PwZ8GjhNs4EFPNOrNCQnWexiUCQ=>AAAB53icbVDLSgMxFL1TX7W+qi7dBIvQVZlxo8uCG5cV7APaQTKZTBuaZIbkjlCG/oAbF4q49Zfc+Tem7Sy0eiBwOOdccu+JMiks+v6XV9nY3Nreqe7W9vYPDo/qxyc9m+aG8S5LZWoGEbVcCs27KFDyQWY4VZHk/Wh6s/D7j9xYkep7nGU8VHSsRSIYRSd1HuoNv+UvQf6SoCQNKOHyn6M4ZbniGpmk1g4DP8OwoAYFk3xeG+WWZ5RN6ZgPHdVUcRsWyz3n5MIpMUlS455GslR/ThRUWTtTkUsqihO77i3E/7xhjsl1WAid5cg1W32U5JJgShZHk1gYzlDOHKHMCLcrYRNqKENXTc2VEKyf/Jf0LluB3wru/Ea7WdZRhTM4hyYEcAVtuIUOdIFBDE/wAq+e8J69N+99Fa145cwp/IL38Q0CZYxc</latexit><latexit sha1_base64=6WjtAQy1eEki3DeLmUkkI9Sk/Os=>AAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0Xc+kvu/BunaRbaemDgcM65zL0nSAXXxnW/ncrG5tb2TnW3trd/cHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOB/WB6u/D7T6g0T+SDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzV+TpXhTOC8Nso0ppRN6RiHlkoao/bzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQ//OGmYlu/JzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3v/qrRbpZ1VOEMzqEJHlxDG+6gA11gEMIzvMKbw50X5935WEYrTjlzCn/gfP4AA6WMYA==</latexit><latexit sha1_base64=6WjtAQy1eEki3DeLmUkkI9Sk/Os=>AAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0Xc+kvu/BunaRbaemDgcM65zL0nSAXXxnW/ncrG5tb2TnW3trd/cHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOB/WB6u/D7T6g0T+SDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzV+TpXhTOC8Nso0ppRN6RiHlkoao/bzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQ//OGmYlu/JzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3v/qrRbpZ1VOEMzqEJHlxDG+6gA11gEMIzvMKbw50X5935WEYrTjlzCn/gfP4AA6WMYA==</latexit><latexit sha1_base64=6WjtAQy1eEki3DeLmUkkI9Sk/Os=>AAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0Xc+kvu/BunaRbaemDgcM65zL0nSAXXxnW/ncrG5tb2TnW3trd/cHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOB/WB6u/D7T6g0T+SDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzV+TpXhTOC8Nso0ppRN6RiHlkoao/bzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQ//OGmYlu/JzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3v/qrRbpZ1VOEMzqEJHlxDG+6gA11gEMIzvMKbw50X5935WEYrTjlzCn/gfP4AA6WMYA==</latexit><latexit sha1_base64=6WjtAQy1eEki3DeLmUkkI9Sk/Os=>AAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0Xc+kvu/BunaRbaemDgcM65zL0nSAXXxnW/ncrG5tb2TnW3trd/cHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOB/WB6u/D7T6g0T+SDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzV+TpXhTOC8Nso0ppRN6RiHlkoao/bzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQ//OGmYlu/JzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3v/qrRbpZ1VOEMzqEJHlxDG+6gA11gEMIzvMKbw50X5935WEYrTjlzCn/gfP4AA6WMYA==</latexit><latexit sha1_base64=6WjtAQy1eEki3DeLmUkkI9Sk/Os=>AAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0Xc+kvu/BunaRbaemDgcM65zL0nSAXXxnW/ncrG5tb2TnW3trd/cHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOB/WB6u/D7T6g0T+SDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzV+TpXhTOC8Nso0ppRN6RiHlkoao/bzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQ//OGmYlu/JzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3v/qrRbpZ1VOEMzqEJHlxDG+6gA11gEMIzvMKbw50X5935WEYrTjlzCn/gfP4AA6WMYA==</latexit><latexit sha1_base64=6WjtAQy1eEki3DeLmUkkI9Sk/Os=>AAAB53icbVDLSsNAFL2pr1pfVZduBovQVUlE0GXBjcsK9gFtkMnkph06mYSZiVBCf8CNC0Xc+kvu/BunaRbaemDgcM65zL0nSAXXxnW/ncrG5tb2TnW3trd/cHhUPz7p6SRTDLssEYkaBFSj4BK7hhuBg1QhjQOB/WB6u/D7T6g0T+SDmaXox3QsecQZNVbqPNYbbsstQNaJV5IGlLD5r1GYsCxGaZigWg89NzV+TpXhTOC8Nso0ppRN6RiHlkoao/bzYs85ubBKSKJE2ScNKdTfEzmNtZ7FgU3G1Ez0qrcQ//OGmYlu/JzLNDMo2fKjKBPEJGRxNAm5QmbEzBLKFLe7EjahijJjq6nZErzVk9dJ77LluS3v/qrRbpZ1VOEMzqEJHlxDG+6gA11gEMIzvMKbw50X5935WEYrTjlzCn/gfP4AA6WMYA==</latexit> (c) Global update
Figure 3: Updates in a GN block. Blue indicates the element that is being updated, and black
indicates other elements which are involved in the update (note that the pre-update value of the
blue element is also used in the update). See Equation 1 for details on the notation.
3.vis applied to each node i, to compute an updated node attribute, v0
i. In our running
example,vmay compute something analogous to the updated position, velocity, and kinetic
energy of each ball. The set of resulting per-node outputs is, V0=fv0
igi=1:Nv.
4.e!uis applied to E0, and aggregates all edge updates, into  e0, which will then be used in the
next step's global update. In our running example, e!umay compute the summed forces
(which should be zero, in this case, due to Newton's third law) and the springs' potential
energies.
5.v!uis applied to V0, and aggregates all node updates, into  v0, which will then be used in
the next step's global update. In our running example, v!umight compute the total kinetic
energy of the system.
6.uis applied once per graph, and computes an update for the global attribute, u0. In our
running example, umight compute something analogous to the net forces and total energy
of the physical system.
Note, though we assume this sequence of steps here, the order is not strictly enforced: it is possible
to reverse the update functions to proceed from global, to per-node, to per-edge updates, for example.
Kearnes et al. (2016) computes edge updates from nodes in a similar manner.
3.2.4 Relational inductive biases in graph networks
Our GN framework imposes several strong relational inductive biases when used as components in
a learning process. First, graphs can express arbitrary relationships among entities, which means
the GN's input determines how representations interact and are isolated, rather than those choices
being determined by the xed architecture. For example, the assumption that two entities have a
relationship|and thus should interact|is expressed by an edge between the entities' corresponding
nodes. Similarly, the absence of an edge expresses the assumption that the nodes have no relationship
and should not in
uence each other directly.
Second, graphs represent entities and their relations as sets, which are invariant to permutations.
This means GNs are invariant to the order of these elements6, which is often desirable. For example,
the objects in a scene do not have a natural ordering (see Sec. 2.2).
Third, a GN's per-edge and per-node functions are reused across all edges and nodes, respectively.
This means GNs automatically support a form of combinatorial generalization (see Section 5.1):
because graphs are composed of edges, nodes, and global features, a single GN can operate on
graphs of dierent sizes (numbers of edges and nodes) and shapes (edge connectivity).
6Note, an ordering can be imposed by encoding the indices in the node or edge attributes, or via the edges
themselves (e.g. by encoding a chain or partial ordering).
134 Design principles for graph network architectures
The GN framework can be used to implement a wide variety of architectures, in accordance with
the design principles listed above in Section 3.2, which also correspond to the sub-sections (4.1,
4.2, and 4.3) below. In general, the framework is agnostic to specic attribute representations and
functional forms. Here, however, we focus mainly on deep learning architectures, which allow GNs
to act as learnable graph-to-graph function approximators.
4.1 Flexible representations
Graph networks support highly 
exible graph representations in two ways: rst, in terms of the
representation of the attributes; and second, in terms of the structure of the graph itself.
4.1.1 Attributes
The global, node, and edge attributes of a GN block can use arbitrary representational formats. In
deep learning implementations, real-valued vectors and tensors are most common. However, other
data structures such as sequences, sets, or even graphs could also be used.
The requirements of the problem will often determine what representations should be used for
the attributes. For example, when the input data is an image, the attributes might be represented
as tensors of image patches; however, when the input data is a text document, the attributes might
be sequences of words corresponding to sentences.
For each GN block within a broader architecture, the edge and node outputs typically correspond
to lists of vectors or tensors, one per edge or node, and the global outputs correspond to a single
vector or tensor. This allows a GN's output to be passed to other deep learning building blocks
such as MLPs, CNNs, and RNNs.
The output of a GN block can also be tailored to the demands of the task. In particular,
Anedge-focused GN uses the edges as output, for example to make decisions about interactions
among entities (Kipf et al., 2018; Hamrick et al., 2018).
Anode-focused GN uses the nodes as output, for example to reason about physical systems
(Battaglia et al., 2016; Chang et al., 2017; Wang et al., 2018b; Sanchez-Gonzalez et al., 2018).
Agraph-focused GN uses the globals as output, for example to predict the potential energy of
a physical system (Battaglia et al., 2016), the properties of a molecule (Gilmer et al., 2017),
or answers to questions about a visual scene (Santoro et al., 2017).
The nodes, edges, and global outputs can also be mixed-and-matched depending on the task. For
example, Hamrick et al. (2018) used both the output edge and global attributes to compute a policy
over actions.
4.1.2 Graph structure
When dening how the input data will be represented as a graph, there are generally two scenarios:
rst, the input explicitly species the relational structure; and second, the relational structure must
be inferred or assumed. These are not hard distinctions, but extremes along a continuum.
Examples of data with more explicitly specied entities and relations include knowledge graphs,
social networks, parse trees, optimization problems, chemical graphs, road networks, and physical
systems with known interactions. Figures 2a-d illustrate how such data can be expressed as graphs.
Examples of data where the relational structure is not made explicit, and must be inferred or
assumed, include visual scenes, text corpora, programming language source code, and multi-agent
14systems. In these types of settings, the data may be formatted as a set of entities without relations,
or even just a vector or tensor (e.g., an image). If the entities are not specied explicitly, they might
be assumed, for instance, by treating each word in a sentence (Vaswani et al., 2017) or each local
feature vector in a CNN's output feature map, as a node (Watters et al., 2017; Santoro et al., 2017;
Wang et al., 2018c) (Figures 2e-f). Or, it might be possible to use a separate learned mechanism to
infer entities from an unstructured signal (Luong et al., 2015; Mnih et al., 2014; Eslami et al., 2016;
van Steenkiste et al., 2018). If relations are not available, the simplest approach is to instantiate all
possible directed edges between entities (Figure 2f). This can be prohibitive for large numbers of
entities, however, because the number of possible edges grows quadratically with the number of
nodes. Thus developing more sophisticated ways of inferring sparse structure from unstructured
data (Kipf et al., 2018) is an important future direction.
4.2 Congurable within-block structure
The structure and functions within a GN block can be congured in dierent ways, which oers

exibility in what information is made available as inputs to its functions, as well as how output edge,
node, and global updates are produced. In particular, each in Equation 1 must be implemented
with some function, f, wheref's argument signature determines what information it requires as
input; in Figure 4, the incoming arrows to each depict whether u,V, andEare taken as inputs.
Hamrick et al. (2018) and Sanchez-Gonzalez et al. (2018) used the full GN block shown in Figure 4a.
Theirimplementations used neural networks (denoted NNe,NNv, and NNubelow, to indicate that
they are dierent functions with dierent parameters). Their implementations used elementwise
summation, but averages and max/min could also be used,
e(ek;vrk;vsk;u):=fe(ek;vrk;vsk;u) = NNe([ek;vrk;vsk;u]) (2)
v
 e0
i;vi;u:=fv
 e0
i;vi;u
= NNv
[ e0
i;vi;u]
u
 e0; v0;u:=fu
 e0; v0;u
= NNu
[ e0; v0;u]
e!v
E0
i:= =X
fk:rk=ige0
k
v!u
V0:= =X
iv0
i
e!u
E0:= =X
ke0
k
where [ x;y;z] indicates vector/tensor concatenation. For vector attributes, a MLP is often used for
, while for tensors such as image feature maps, CNNs may be more suitable.
Thefunctions can also use RNNs, which requires an additional hidden state as input and
output. Figure 4b shows a very simple version of a GN block with RNNs as functions: there is no
message-passing in this formulation, and this type of block might be used for recurrent smoothing of
some dynamic graph states. Of course, RNNs as functions could also be used in a full GN block
(Figure 4a).
A variety of other architectures can be expressed in the GN framework, often as dierent
function choices and within-block congurations. The remaining sub-sections explore how a GN's
within-block structure can be congured in dierent ways, with examples of published work which
uses such congurations. See the Appendix for details.
15(a) Full GN block
u0,u0hid
<latexit sha1_base64=W2hu4ghYojV0SmOcBbcdBDITOSo=>AAACDnicbVC7TsMwFHV4lvIKMLJYVBUMqEoQEgwMlVgYi0QfUhtFjuu0Vm0nsh2kKsoXsPArLAwgxMrMxt/gpBlKy5EsHZ9zr+69J4gZVdpxfqyV1bX1jc3KVnV7Z3dv3z447KgokZi0ccQi2QuQIowK0tZUM9KLJUE8YKQbTG5zv/tIpKKReNDTmHgcjQQNKUbaSL5dH3Ckx0GYJtnpOZz7+AWXPB3TYebbNafhFIDLxC1JDZRo+fb3YBjhhBOhMUNK9V0n1l6KpKaYkaw6SBSJEZ6gEekbKhAnykuLczJYN8oQhpE0T2hYqPMdKeJKTXlgKvMd1aKXi/95/USH115KRZxoIvBsUJgwqCOYZwOHVBKs2dQQhCU1u0I8RhJhbRKsmhDcxZOXSeei4ToN9/6y1rwp46iAY3ACzoALrkAT3IEWaAMMnsALeAPv1rP1an1Yn7PSFavsOQJ/YH39AtnQnJg=</latexit><latexit sha1_base64=W2hu4ghYojV0SmOcBbcdBDITOSo=>AAACDnicbVC7TsMwFHV4lvIKMLJYVBUMqEoQEgwMlVgYi0QfUhtFjuu0Vm0nsh2kKsoXsPArLAwgxMrMxt/gpBlKy5EsHZ9zr+69J4gZVdpxfqyV1bX1jc3KVnV7Z3dv3z447KgokZi0ccQi2QuQIowK0tZUM9KLJUE8YKQbTG5zv/tIpKKReNDTmHgcjQQNKUbaSL5dH3Ckx0GYJtnpOZz7+AWXPB3TYebbNafhFIDLxC1JDZRo+fb3YBjhhBOhMUNK9V0n1l6KpKaYkaw6SBSJEZ6gEekbKhAnykuLczJYN8oQhpE0T2hYqPMdKeJKTXlgKvMd1aKXi/95/USH115KRZxoIvBsUJgwqCOYZwOHVBKs2dQQhCU1u0I8RhJhbRKsmhDcxZOXSeei4ToN9/6y1rwp46iAY3ACzoALrkAT3IEWaAMMnsALeAPv1rP1an1Yn7PSFavsOQJ/YH39AtnQnJg=</latexit><latexit sha1_base64=W2hu4ghYojV0SmOcBbcdBDITOSo=>AAACDnicbVC7TsMwFHV4lvIKMLJYVBUMqEoQEgwMlVgYi0QfUhtFjuu0Vm0nsh2kKsoXsPArLAwgxMrMxt/gpBlKy5EsHZ9zr+69J4gZVdpxfqyV1bX1jc3KVnV7Z3dv3z447KgokZi0ccQi2QuQIowK0tZUM9KLJUE8YKQbTG5zv/tIpKKReNDTmHgcjQQNKUbaSL5dH3Ckx0GYJtnpOZz7+AWXPB3TYebbNafhFIDLxC1JDZRo+fb3YBjhhBOhMUNK9V0n1l6KpKaYkaw6SBSJEZ6gEekbKhAnykuLczJYN8oQhpE0T2hYqPMdKeJKTXlgKvMd1aKXi/95/USH115KRZxoIvBsUJgwqCOYZwOHVBKs2dQQhCU1u0I8RhJhbRKsmhDcxZOXSeei4ToN9/6y1rwp46iAY3ACzoALrkAT3IEWaAMMnsALeAPv1rP1an1Yn7PSFavsOQJ/YH39AtnQnJg=</latexit><latexit sha1_base64=W2hu4ghYojV0SmOcBbcdBDITOSo=>AAACDnicbVC7TsMwFHV4lvIKMLJYVBUMqEoQEgwMlVgYi0QfUhtFjuu0Vm0nsh2kKsoXsPArLAwgxMrMxt/gpBlKy5EsHZ9zr+69J4gZVdpxfqyV1bX1jc3KVnV7Z3dv3z447KgokZi0ccQi2QuQIowK0tZUM9KLJUE8YKQbTG5zv/tIpKKReNDTmHgcjQQNKUbaSL5dH3Ckx0GYJtnpOZz7+AWXPB3TYebbNafhFIDLxC1JDZRo+fb3YBjhhBOhMUNK9V0n1l6KpKaYkaw6SBSJEZ6gEekbKhAnykuLczJYN8oQhpE0T2hYqPMdKeJKTXlgKvMd1aKXi/95/USH115KRZxoIvBsUJgwqCOYZwOHVBKs2dQQhCU1u0I8RhJhbRKsmhDcxZOXSeei4ToN9/6y1rwp46iAY3ACzoALrkAT3IEWaAMMnsALeAPv1rP1an1Yn7PSFavsOQJ/YH39AtnQnJg=</latexit>E0,E0hid
<latexit sha1_base64=ZYHbDTDbKJNm66GNrt+dUMK6558=>AAAB/HicbVDLSgMxFM3UV62v0S7dBIvUhZQZEXThoiAFlxXsA9phyGQybWiSGZKMUIb6K25cKOLWD3Hn35hpZ6GtBwKHc+7lnpwgYVRpx/m2SmvrG5tb5e3Kzu7e/oF9eNRVcSox6eCYxbIfIEUYFaSjqWakn0iCeMBIL5jc5n7vkUhFY/GgpwnxOBoJGlGMtJF8u9qqn8NW3R9ypMeSZ2Mazny75jScOeAqcQtSAwXavv01DGOcciI0Zkipgesk2suQ1BQzMqsMU0UShCdoRAaGCsSJ8rJ5+Bk8NUoIo1iaJzScq783MsSVmvLATOYZ1bKXi/95g1RH115GRZJqIvDiUJQyqGOYNwFDKgnWbGoIwpKarBCPkURYm74qpgR3+curpHvRcJ2Ge39Za94UdZTBMTgBZ8AFV6AJ7kAbdAAGU/AMXsGb9WS9WO/Wx2K0ZBU7VfAH1ucPbBGT+A==</latexit><latexit sha1_base64=ZYHbDTDbKJNm66GNrt+dUMK6558=>AAAB/HicbVDLSgMxFM3UV62v0S7dBIvUhZQZEXThoiAFlxXsA9phyGQybWiSGZKMUIb6K25cKOLWD3Hn35hpZ6GtBwKHc+7lnpwgYVRpx/m2SmvrG5tb5e3Kzu7e/oF9eNRVcSox6eCYxbIfIEUYFaSjqWakn0iCeMBIL5jc5n7vkUhFY/GgpwnxOBoJGlGMtJF8u9qqn8NW3R9ypMeSZ2Mazny75jScOeAqcQtSAwXavv01DGOcciI0Zkipgesk2suQ1BQzMqsMU0UShCdoRAaGCsSJ8rJ5+Bk8NUoIo1iaJzScq783MsSVmvLATOYZ1bKXi/95g1RH115GRZJqIvDiUJQyqGOYNwFDKgnWbGoIwpKarBCPkURYm74qpgR3+curpHvRcJ2Ge39Za94UdZTBMTgBZ8AFV6AJ7kAbdAAGU/AMXsGb9WS9WO/Wx2K0ZBU7VfAH1ucPbBGT+A==</latexit><latexit sha1_base64=ZYHbDTDbKJNm66GNrt+dUMK6558=>AAAB/HicbVDLSgMxFM3UV62v0S7dBIvUhZQZEXThoiAFlxXsA9phyGQybWiSGZKMUIb6K25cKOLWD3Hn35hpZ6GtBwKHc+7lnpwgYVRpx/m2SmvrG5tb5e3Kzu7e/oF9eNRVcSox6eCYxbIfIEUYFaSjqWakn0iCeMBIL5jc5n7vkUhFY/GgpwnxOBoJGlGMtJF8u9qqn8NW3R9ypMeSZ2Mazny75jScOeAqcQtSAwXavv01DGOcciI0Zkipgesk2suQ1BQzMqsMU0UShCdoRAaGCsSJ8rJ5+Bk8NUoIo1iaJzScq783MsSVmvLATOYZ1bKXi/95g1RH115GRZJqIvDiUJQyqGOYNwFDKgnWbGoIwpKarBCPkURYm74qpgR3+curpHvRcJ2Ge39Za94UdZTBMTgBZ8AFV6AJ7kAbdAAGU/AMXsGb9WS9WO/Wx2K0ZBU7VfAH1ucPbBGT+A==</latexit><latexit sha1_base64=ZYHbDTDbKJNm66GNrt+dUMK6558=>AAAB/HicbVDLSgMxFM3UV62v0S7dBIvUhZQZEXThoiAFlxXsA9phyGQybWiSGZKMUIb6K25cKOLWD3Hn35hpZ6GtBwKHc+7lnpwgYVRpx/m2SmvrG5tb5e3Kzu7e/oF9eNRVcSox6eCYxbIfIEUYFaSjqWakn0iCeMBIL5jc5n7vkUhFY/GgpwnxOBoJGlGMtJF8u9qqn8NW3R9ypMeSZ2Mazny75jScOeAqcQtSAwXavv01DGOcciI0Zkipgesk2suQ1BQzMqsMU0UShCdoRAaGCsSJ8rJ5+Bk8NUoIo1iaJzScq783MsSVmvLATOYZ1bKXi/95g1RH115GRZJqIvDiUJQyqGOYNwFDKgnWbGoIwpKarBCPkURYm74qpgR3+curpHvRcJ2Ge39Za94UdZTBMTgBZ8AFV6AJ7kAbdAAGU/AMXsGb9WS9WO/Wx2K0ZBU7VfAH1ucPbBGT+A==</latexit>V0,V0hid
<latexit sha1_base64=WhBBytL7AOLCOPFKqkdbfTZ2xq4=>AAAB/HicbVBPS8MwHE3nvzn/VXf0EhwyDzJaEfTgYeDF4wTXDbZS0jTdwpK0JKkwyvwqXjwo4tUP4s1vY7r1oJsPAo/3fj9+Ly9MGVXacb6tytr6xuZWdbu2s7u3f2AfHnkqySQmXZywRPZDpAijgnQ11Yz0U0kQDxnphZPbwu89EqloIh70NCU+RyNBY4qRNlJg173mOfSawZAjPZY8H9NoFtgNp+XMAVeJW5IGKNEJ7K9hlOCME6ExQ0oNXCfVfo6kppiRWW2YKZIiPEEjMjBUIE6Un8/Dz+CpUSIYJ9I8oeFc/b2RI67UlIdmssiolr1C/M8bZDq+9nMq0kwTgReH4oxBncCiCRhRSbBmU0MQltRkhXiMJMLa9FUzJbjLX14l3kXLdVru/WWjfVPWUQXH4AScARdcgTa4Ax3QBRhMwTN4BW/Wk/VivVsfi9GKVe7UwR9Ynz+huZQa</latexit><latexit sha1_base64=WhBBytL7AOLCOPFKqkdbfTZ2xq4=>AAAB/HicbVBPS8MwHE3nvzn/VXf0EhwyDzJaEfTgYeDF4wTXDbZS0jTdwpK0JKkwyvwqXjwo4tUP4s1vY7r1oJsPAo/3fj9+Ly9MGVXacb6tytr6xuZWdbu2s7u3f2AfHnkqySQmXZywRPZDpAijgnQ11Yz0U0kQDxnphZPbwu89EqloIh70NCU+RyNBY4qRNlJg173mOfSawZAjPZY8H9NoFtgNp+XMAVeJW5IGKNEJ7K9hlOCME6ExQ0oNXCfVfo6kppiRWW2YKZIiPEEjMjBUIE6Un8/Dz+CpUSIYJ9I8oeFc/b2RI67UlIdmssiolr1C/M8bZDq+9nMq0kwTgReH4oxBncCiCRhRSbBmU0MQltRkhXiMJMLa9FUzJbjLX14l3kXLdVru/WWjfVPWUQXH4AScARdcgTa4Ax3QBRhMwTN4BW/Wk/VivVsfi9GKVe7UwR9Ynz+huZQa</latexit><latexit sha1_base64=WhBBytL7AOLCOPFKqkdbfTZ2xq4=>AAAB/HicbVBPS8MwHE3nvzn/VXf0EhwyDzJaEfTgYeDF4wTXDbZS0jTdwpK0JKkwyvwqXjwo4tUP4s1vY7r1oJsPAo/3fj9+Ly9MGVXacb6tytr6xuZWdbu2s7u3f2AfHnkqySQmXZywRPZDpAijgnQ11Yz0U0kQDxnphZPbwu89EqloIh70NCU+RyNBY4qRNlJg173mOfSawZAjPZY8H9NoFtgNp+XMAVeJW5IGKNEJ7K9hlOCME6ExQ0oNXCfVfo6kppiRWW2YKZIiPEEjMjBUIE6Un8/Dz+CpUSIYJ9I8oeFc/b2RI67UlIdmssiolr1C/M8bZDq+9nMq0kwTgReH4oxBncCiCRhRSbBmU0MQltRkhXiMJMLa9FUzJbjLX14l3kXLdVru/WWjfVPWUQXH4AScARdcgTa4Ax3QBRhMwTN4BW/Wk/VivVsfi9GKVe7UwR9Ynz+huZQa</latexit><latexit sha1_base64=WhBBytL7AOLCOPFKqkdbfTZ2xq4=>AAAB/HicbVBPS8MwHE3nvzn/VXf0EhwyDzJaEfTgYeDF4wTXDbZS0jTdwpK0JKkwyvwqXjwo4tUP4s1vY7r1oJsPAo/3fj9+Ly9MGVXacb6tytr6xuZWdbu2s7u3f2AfHnkqySQmXZywRPZDpAijgnQ11Yz0U0kQDxnphZPbwu89EqloIh70NCU+RyNBY4qRNlJg173mOfSawZAjPZY8H9NoFtgNp+XMAVeJW5IGKNEJ7K9hlOCME6ExQ0oNXCfVfo6kppiRWW2YKZIiPEEjMjBUIE6Un8/Dz+CpUSIYJ9I8oeFc/b2RI67UlIdmssiolr1C/M8bZDq+9nMq0kwTgReH4oxBncCiCRhRSbBmU0MQltRkhXiMJMLa9FUzJbjLX14l3kXLdVru/WWjfVPWUQXH4AScARdcgTa4Ax3QBRhMwTN4BW/Wk/VivVsfi9GKVe7UwR9Ynz+huZQa</latexit>E,Ehid
<latexit sha1_base64=DYZek5SmevKS8py25dx0aIqUbBY=>AAAB+nicbVDLSsNAFL3xWesr1aWbwSK4kJKIoAsXBSm4rGAf0IYwmUzboTNJmJkoJfZT3LhQxK1f4s6/cdJmoa0HBg7n3Ms9c4KEM6Ud59taWV1b39gsbZW3d3b39u3KQVvFqSS0RWIey26AFeUsoi3NNKfdRFIsAk47wfgm9zsPVCoWR/d6klBP4GHEBoxgbSTfrjTOUMPvC6xHUmQjFk59u+rUnBnQMnELUoUCTd/+6ocxSQWNNOFYqZ7rJNrLsNSMcDot91NFE0zGeEh7hkZYUOVls+hTdGKUEA1iaV6k0Uz9vZFhodREBGYyz6gWvVz8z+ulenDlZSxKUk0jMj80SDnSMcp7QCGTlGg+MQQTyUxWREZYYqJNW2VTgrv45WXSPq+5Ts29u6jWr4s6SnAEx3AKLlxCHW6hCS0g8AjP8Apv1pP1Yr1bH/PRFavYOYQ/sD5/AKXVk5Y=</latexit><latexit sha1_base64=DYZek5SmevKS8py25dx0aIqUbBY=>AAAB+nicbVDLSsNAFL3xWesr1aWbwSK4kJKIoAsXBSm4rGAf0IYwmUzboTNJmJkoJfZT3LhQxK1f4s6/cdJmoa0HBg7n3Ms9c4KEM6Ud59taWV1b39gsbZW3d3b39u3KQVvFqSS0RWIey26AFeUsoi3NNKfdRFIsAk47wfgm9zsPVCoWR/d6klBP4GHEBoxgbSTfrjTOUMPvC6xHUmQjFk59u+rUnBnQMnELUoUCTd/+6ocxSQWNNOFYqZ7rJNrLsNSMcDot91NFE0zGeEh7hkZYUOVls+hTdGKUEA1iaV6k0Uz9vZFhodREBGYyz6gWvVz8z+ulenDlZSxKUk0jMj80SDnSMcp7QCGTlGg+MQQTyUxWREZYYqJNW2VTgrv45WXSPq+5Ts29u6jWr4s6SnAEx3AKLlxCHW6hCS0g8AjP8Apv1pP1Yr1bH/PRFavYOYQ/sD5/AKXVk5Y=</latexit><latexit sha1_base64=DYZek5SmevKS8py25dx0aIqUbBY=>AAAB+nicbVDLSsNAFL3xWesr1aWbwSK4kJKIoAsXBSm4rGAf0IYwmUzboTNJmJkoJfZT3LhQxK1f4s6/cdJmoa0HBg7n3Ms9c4KEM6Ud59taWV1b39gsbZW3d3b39u3KQVvFqSS0RWIey26AFeUsoi3NNKfdRFIsAk47wfgm9zsPVCoWR/d6klBP4GHEBoxgbSTfrjTOUMPvC6xHUmQjFk59u+rUnBnQMnELUoUCTd/+6ocxSQWNNOFYqZ7rJNrLsNSMcDot91NFE0zGeEh7hkZYUOVls+hTdGKUEA1iaV6k0Uz9vZFhodREBGYyz6gWvVz8z+ulenDlZSxKUk0jMj80SDnSMcp7QCGTlGg+MQQTyUxWREZYYqJNW2VTgrv45WXSPq+5Ts29u6jWr4s6SnAEx3AKLlxCHW6hCS0g8AjP8Apv1pP1Yr1bH/PRFavYOYQ/sD5/AKXVk5Y=</latexit><latexit sha1_base64=DYZek5SmevKS8py25dx0aIqUbBY=>AAAB+nicbVDLSsNAFL3xWesr1aWbwSK4kJKIoAsXBSm4rGAf0IYwmUzboTNJmJkoJfZT3LhQxK1f4s6/cdJmoa0HBg7n3Ms9c4KEM6Ud59taWV1b39gsbZW3d3b39u3KQVvFqSS0RWIey26AFeUsoi3NNKfdRFIsAk47wfgm9zsPVCoWR/d6klBP4GHEBoxgbSTfrjTOUMPvC6xHUmQjFk59u+rUnBnQMnELUoUCTd/+6ocxSQWNNOFYqZ7rJNrLsNSMcDot91NFE0zGeEh7hkZYUOVls+hTdGKUEA1iaV6k0Uz9vZFhodREBGYyz6gWvVz8z+ulenDlZSxKUk0jMj80SDnSMcp7QCGTlGg+MQQTyUxWREZYYqJNW2VTgrv45WXSPq+5Ts29u6jWr4s6SnAEx3AKLlxCHW6hCS0g8AjP8Apv1pP1Yr1bH/PRFavYOYQ/sD5/AKXVk5Y=</latexit>V, Vhid
<latexit sha1_base64=brwuxF74R6OEOykh378as/RBfzE=>AAAB+nicbVBNS8NAFHzxs9avVI9eFovgQUoigh48FLx4rGDTQhvCZrNtl+4mYXejlNif4sWDIl79Jd78N27aHLR1YGGYeY83O2HKmdKO822trK6tb2xWtqrbO7t7+3btwFNJJgltk4QnshtiRTmLaVszzWk3lRSLkNNOOL4p/M4DlYol8b2epNQXeBizASNYGymwa94Z8oK+wHokRT5i0TSw607DmQEtE7ckdSjRCuyvfpSQTNBYE46V6rlOqv0cS80Ip9NqP1M0xWSMh7RnaIwFVX4+iz5FJ0aJ0CCR5sUazdTfGzkWSk1EaCaLjGrRK8T/vF6mB1d+zuI00zQm80ODjCOdoKIHFDFJieYTQzCRzGRFZIQlJtq0VTUluItfXibeecN1Gu7dRb15XdZRgSM4hlNw4RKacAstaAOBR3iGV3iznqwX6936mI+uWOXOIfyB9fkD20qTuA==</latexit><latexit sha1_base64=brwuxF74R6OEOykh378as/RBfzE=>AAAB+nicbVBNS8NAFHzxs9avVI9eFovgQUoigh48FLx4rGDTQhvCZrNtl+4mYXejlNif4sWDIl79Jd78N27aHLR1YGGYeY83O2HKmdKO822trK6tb2xWtqrbO7t7+3btwFNJJgltk4QnshtiRTmLaVszzWk3lRSLkNNOOL4p/M4DlYol8b2epNQXeBizASNYGymwa94Z8oK+wHokRT5i0TSw607DmQEtE7ckdSjRCuyvfpSQTNBYE46V6rlOqv0cS80Ip9NqP1M0xWSMh7RnaIwFVX4+iz5FJ0aJ0CCR5sUazdTfGzkWSk1EaCaLjGrRK8T/vF6mB1d+zuI00zQm80ODjCOdoKIHFDFJieYTQzCRzGRFZIQlJtq0VTUluItfXibeecN1Gu7dRb15XdZRgSM4hlNw4RKacAstaAOBR3iGV3iznqwX6936mI+uWOXOIfyB9fkD20qTuA==</latexit><latexit sha1_base64=brwuxF74R6OEOykh378as/RBfzE=>AAAB+nicbVBNS8NAFHzxs9avVI9eFovgQUoigh48FLx4rGDTQhvCZrNtl+4mYXejlNif4sWDIl79Jd78N27aHLR1YGGYeY83O2HKmdKO822trK6tb2xWtqrbO7t7+3btwFNJJgltk4QnshtiRTmLaVszzWk3lRSLkNNOOL4p/M4DlYol8b2epNQXeBizASNYGymwa94Z8oK+wHokRT5i0TSw607DmQEtE7ckdSjRCuyvfpSQTNBYE46V6rlOqv0cS80Ip9NqP1M0xWSMh7RnaIwFVX4+iz5FJ0aJ0CCR5sUazdTfGzkWSk1EaCaLjGrRK8T/vF6mB1d+zuI00zQm80ODjCOdoKIHFDFJieYTQzCRzGRFZIQlJtq0VTUluItfXibeecN1Gu7dRb15XdZRgSM4hlNw4RKacAstaAOBR3iGV3iznqwX6936mI+uWOXOIfyB9fkD20qTuA==</latexit><latexit sha1_base64=brwuxF74R6OEOykh378as/RBfzE=>AAAB+nicbVBNS8NAFHzxs9avVI9eFovgQUoigh48FLx4rGDTQhvCZrNtl+4mYXejlNif4sWDIl79Jd78N27aHLR1YGGYeY83O2HKmdKO822trK6tb2xWtqrbO7t7+3btwFNJJgltk4QnshtiRTmLaVszzWk3lRSLkNNOOL4p/M4DlYol8b2epNQXeBizASNYGymwa94Z8oK+wHokRT5i0TSw607DmQEtE7ckdSjRCuyvfpSQTNBYE46V6rlOqv0cS80Ip9NqP1M0xWSMh7RnaIwFVX4+iz5FJ0aJ0CCR5sUazdTfGzkWSk1EaCaLjGrRK8T/vF6mB1d+zuI00zQm80ODjCOdoKIHFDFJieYTQzCRzGRFZIQlJtq0VTUluItfXibeecN1Gu7dRb15XdZRgSM4hlNw4RKacAstaAOBR3iGV3iznqwX6936mI+uWOXOIfyB9fkD20qTuA==</latexit>u,uhid
<latexit sha1_base64=UO6spgXZarocGO1zBKohmEwCj+c=>AAACDHicbZDNSsNAFIVv6l+tf1WXbgaL4EJKIoIuXBTcuKxgW6ENZTKZtENnkjAzEUrIA7jxVdy4UMStD+DOt3GSBtTWAwMf597L3Hu8mDOlbfvLqiwtr6yuVddrG5tb2zv13b2uihJJaIdEPJJ3HlaUs5B2NNOc3sWSYuFx2vMmV3m9d0+lYlF4q6cxdQUehSxgBGtjDeuNgcB67AVpkp2gHx4WKEU6Zn5muuymXQgtglNCA0q1h/XPgR+RRNBQE46V6jt2rN0US80Ip1ltkCgaYzLBI9o3GGJBlZsWx2ToyDg+CiJpXqhR4f6eSLFQaio805nvqOZruflfrZ/o4MJNWRgnmoZk9lGQcKQjlCeDfCYp0XxqABPJzK6IjLHERJv8aiYEZ/7kReieNh276dycNVqXZRxVOIBDOAYHzqEF19CGDhB4gCd4gVfr0Xq23qz3WWvFKmf24Y+sj28GGpw2</latexit><latexit sha1_base64=UO6spgXZarocGO1zBKohmEwCj+c=>AAACDHicbZDNSsNAFIVv6l+tf1WXbgaL4EJKIoIuXBTcuKxgW6ENZTKZtENnkjAzEUrIA7jxVdy4UMStD+DOt3GSBtTWAwMf597L3Hu8mDOlbfvLqiwtr6yuVddrG5tb2zv13b2uihJJaIdEPJJ3HlaUs5B2NNOc3sWSYuFx2vMmV3m9d0+lYlF4q6cxdQUehSxgBGtjDeuNgcB67AVpkp2gHx4WKEU6Zn5muuymXQgtglNCA0q1h/XPgR+RRNBQE46V6jt2rN0US80Ip1ltkCgaYzLBI9o3GGJBlZsWx2ToyDg+CiJpXqhR4f6eSLFQaio805nvqOZruflfrZ/o4MJNWRgnmoZk9lGQcKQjlCeDfCYp0XxqABPJzK6IjLHERJv8aiYEZ/7kReieNh276dycNVqXZRxVOIBDOAYHzqEF19CGDhB4gCd4gVfr0Xq23qz3WWvFKmf24Y+sj28GGpw2</latexit><latexit sha1_base64=UO6spgXZarocGO1zBKohmEwCj+c=>AAACDHicbZDNSsNAFIVv6l+tf1WXbgaL4EJKIoIuXBTcuKxgW6ENZTKZtENnkjAzEUrIA7jxVdy4UMStD+DOt3GSBtTWAwMf597L3Hu8mDOlbfvLqiwtr6yuVddrG5tb2zv13b2uihJJaIdEPJJ3HlaUs5B2NNOc3sWSYuFx2vMmV3m9d0+lYlF4q6cxdQUehSxgBGtjDeuNgcB67AVpkp2gHx4WKEU6Zn5muuymXQgtglNCA0q1h/XPgR+RRNBQE46V6jt2rN0US80Ip1ltkCgaYzLBI9o3GGJBlZsWx2ToyDg+CiJpXqhR4f6eSLFQaio805nvqOZruflfrZ/o4MJNWRgnmoZk9lGQcKQjlCeDfCYp0XxqABPJzK6IjLHERJv8aiYEZ/7kReieNh276dycNVqXZRxVOIBDOAYHzqEF19CGDhB4gCd4gVfr0Xq23qz3WWvFKmf24Y+sj28GGpw2</latexit><latexit sha1_base64=UO6spgXZarocGO1zBKohmEwCj+c=>AAACDHicbZDNSsNAFIVv6l+tf1WXbgaL4EJKIoIuXBTcuKxgW6ENZTKZtENnkjAzEUrIA7jxVdy4UMStD+DOt3GSBtTWAwMf597L3Hu8mDOlbfvLqiwtr6yuVddrG5tb2zv13b2uihJJaIdEPJJ3HlaUs5B2NNOc3sWSYuFx2vMmV3m9d0+lYlF4q6cxdQUehSxgBGtjDeuNgcB67AVpkp2gHx4WKEU6Zn5muuymXQgtglNCA0q1h/XPgR+RRNBQE46V6jt2rN0US80Ip1ltkCgaYzLBI9o3GGJBlZsWx2ToyDg+CiJpXqhR4f6eSLFQaio805nvqOZruflfrZ/o4MJNWRgnmoZk9lGQcKQjlCeDfCYp0XxqABPJzK6IjLHERJv8aiYEZ/7kReieNh276dycNVqXZRxVOIBDOAYHzqEF19CGDhB4gCd4gVfr0Xq23qz3WWvFKmf24Y+sj28GGpw2</latexit>
Edge blockNode blockGlobal blocku
<latexit sha1_base64=znt8hwWv6wryqwCugrweUa+jkM8=>AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubfaSNXu7y+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvFhxZqzvf3tLyyura+uFjeLm1vbObmlvPzQy04TWieRSN2NsKGeC1i2znDaVpjiNOW3Eg6ux33ik2jApbu1Q0SjFPcESRrB1UthWfXaXdUplv+JPgBZJMCPl6mH4dA8AtU7pq92VJEupsIRjY1qBr2yUY20Z4XRUbGeGKkwGuEdbjgqcUhPlk2tH6MQpXZRI7UpYNFF/T+Q4NWaYxq4zxbZv5r2x+J/XymxyEeVMqMxSQaaLkowjK9H4ddRlmhLLh45gopm7FZE+1phYF1DRhRDMv7xIwrNK4FeCG5fGJUxRgCM4hlMI4ByqcA01qAOBB3iGV3jzpPfivXsf09YlbzZzAH/gff4AqE6Qnw==</latexit><latexit sha1_base64=Nc0DXje6uYB+/0fHlXL99yCq0no=>AAAB7XicbVC7SgNBFL0bXzG+ooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0/kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/cD5+AGXTkqw=</latexit><latexit sha1_base64=Nc0DXje6uYB+/0fHlXL99yCq0no=>AAAB7XicbVC7SgNBFL0bXzG+ooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0/kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/cD5+AGXTkqw=</latexit><latexit sha1_base64=S5XnA5iYIAgqxiI+i0ptSwAiKP4=>AAAB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/CHrlSt+1Z+DrJIgJxXIUe+Vv7p9xbIEpWWCGtMJ/NSGE6otZwKnpW5mMKVsRAfYcVTSBE04mV87JWdO6ZNYaVfSkrn6e2JCE2PGSeQ6E2qHZtmbif95nczGV+GEyzSzKNliUZwJYhWZvU76XCOzYuwIZZq7WwkbUk2ZdQGVXAjB8surpHlRDfxqcOdXatd5HEU4gVM4hwAuoQa3UIcGMHiEZ3iFN095L96797FoLXj5zDH8gff5A53Djxw=</latexit>
<latexit sha1_base64=m8MJ1M94ujO0d0COo5n2Dsol6rc=>AAAB53icbVC7SgNBFL0bXzG+opY2g0GwCrs2phAM2FhGMA9IFpmdvZsMmZ1dZmaFsKS0sbFQxNZP8Rfs/AZ/wsmj0MQDFw7nnMt9BKng2rjul1NYWV1b3yhulra2d3b3yvsHLZ1kimGTJSJRnYBqFFxi03AjsJMqpHEgsB0MryZ++x6V5om8NaMU/Zj2JY84o8ZKjbtyxa26U5Bl4s1J5fLj+4EAgM1/9sKEZTFKwwTVuuu5qfFzqgxnAselXqYxpWxI+9i1VNIYtZ9P9xyTE6uEJEqULWnIVP3dkdNY61Ec2GRMzUAvehPxP6+bmajm51ymmUHJZoOiTBCTkMnRJOQKmREjSyhT3O5K2IAqyox9Tck+wVs8eZm0zqqeW/Vu3Er9AmYowhEcwyl4cA51uIYGNIFBCI/wDC8Od56cV+dtFi04855D+APn/Qd/sI8A</latexit><latexit sha1_base64=q1zM5ZsCNMJAtNUZI97B98ATlaA=>AAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo+1Si4xJrhRmAzUUgjX2DD71+O/MYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL96+7g7fqt81/tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhY/V3R0YjrQeRb5MRNT09643E/7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1+TtE7zZk+dJ/aTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvk+iCM+3Zgz9wXn8AGkeQ8w==</latexit><latexit sha1_base64=q1zM5ZsCNMJAtNUZI97B98ATlaA=>AAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo+1Si4xJrhRmAzUUgjX2DD71+O/MYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL96+7g7fqt81/tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhY/V3R0YjrQeRb5MRNT09643E/7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1+TtE7zZk+dJ/aTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvk+iCM+3Zgz9wXn8AGkeQ8w==</latexit><latexit sha1_base64=ioxb3woZF1oAlTScqds23PrgiMY=>AAAB53icbVC7SgNBFL3rM8ZX1NJmMAhWYdZGC4uAjWUE84BkkdnZ2WTI7Owyc1cIS37AxkIRW3/Jzr9xkmyhiQcGDuecy9x7wkxJi5R+e2vrG5tb25Wd6u7e/sFh7ei4Y9PccNHmqUpNL2RWKKlFGyUq0cuMYEmoRDcc38787pMwVqb6ASeZCBI21DKWnKGTWo+1Om3QOcgq8UtShxIu/zWIUp4nQiNXzNq+TzMMCmZQciWm1UFuRcb4mA1F31HNEmGDYr7nlJw7JSJxatzTSObq74mCJdZOktAlE4Yju+zNxP+8fo7xdVBIneUoNF98FOeKYEpmR5NIGsFRTRxh3Ei3K+EjZhhHV03VleAvn7xKOpcNnzb8e1pv3pR1VOAUzuACfLiCJtxBC9rAIYJneIU3T3ov3rv3sYiueeXMCfyB9/kDCGmMcA==</latexit>v
<latexit sha1_base64=HCiXjOq04H3f4Ed7vqyiRfd+2dI=>AAAB7XicbVA9SwNBEJ2LXzF+nQo2NotBsAp3NlpYBGwsI5hLIDnj3mYvWbO3e+zuRcKR/2BjoYit/8fOf+Pmo9DEBwOP92aYmRelnGnjed9OYWV1bX2juFna2t7Z3XP3DwItM0VonUguVTPCmnImaN0ww2kzVRQnEaeNaHA98RtDqjST4s6MUhomuCdYzAg2VgraaZ/dDztu2at4U6Bl4s9JuXoUPD0AQK3jfrW7kmQJFYZwrHXL91IT5lgZRjgdl9qZpikmA9yjLUsFTqgO8+m1Y3RqlS6KpbIlDJqqvydynGg9SiLbmWDT14veRPzPa2UmvgxzJtLMUEFmi+KMIyPR5HXUZYoSw0eWYKKYvRWRPlaYGBtQyYbgL768TILziu9V/FubxhXMUIRjOIEz8OECqnADNagDgUd4hld4c6Tz4rw7H7PWgjOfOYQ/cD5/AKnSkKA=</latexit><latexit sha1_base64=/voSHBGyFE5xYPVKPYM/GTIarLQ=>AAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQW/NZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMMp+HAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit><latexit sha1_base64=/voSHBGyFE5xYPVKPYM/GTIarLQ=>AAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQW/NZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMMp+HAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit><latexit sha1_base64=Fc8T4ygtia14k1z/CDji4ezWDqY=>AAAB7XicbVA9SwNBEJ3zM8avqKXNYhCswp2NFhYBG8sI5gOSM+xtNsmavd1jdy4QjvwHGwtFbP0/dv4bN8kVmvhg4PHeDDPzokQKi77/7a2tb2xubRd2irt7+weHpaPjhtWpYbzOtNSmFVHLpVC8jgIlbyWG0ziSvBmNbmd+c8yNFVo94CThYUwHSvQFo+ikRicZisdxt1T2K/4cZJUEOSlDjlq39NXpaZbGXCGT1Np24CcYZtSgYJJPi53U8oSyER3wtqOKxtyG2fzaKTl3So/0tXGlkMzV3xMZja2dxJHrjCkO7bI3E//z2in2r8NMqCRFrthiUT+VBDWZvU56wnCGcuIIZUa4WwkbUkMZuoCKLoRg+eVV0risBH4luPfL1Zs8jgKcwhlcQABXUIU7qEEdGDzBM7zCm6e9F+/d+1i0rnn5zAn8gff5A59Hjx0=</latexit>e
<latexit sha1_base64=gRKFy+QFytmwqWy0cvo5FmmPz8I=>AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubeaSNXu3x+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvDAVXBvX/XaWlldW19YLG8XNre2d3dLevq9lphjWmRRSNUOqUfAE64Ybgc1UIY1DgY1wcDX2G4+oNJfJrRmmGMS0l/CIM2qs5LfTPr/DTqnsVtwJyCLxZqRcPfSf7gGg1il9tbuSZTEmhgmqdctzUxPkVBnOBI6K7UxjStmA9rBlaUJj1EE+uXZETqzSJZFUthJDJurviZzGWg/j0HbG1PT1vDcW//NamYkugpwnaWYwYdNFUSaIkWT8OulyhcyIoSWUKW5vJaxPFWXGBlS0IXjzLy8S/6ziuRXvxqZxCVMU4AiO4RQ8OIcqXEMN6sDgAZ7hFd4c6bw4787HtHXJmc0cwB84nz+QDpCP</latexit><latexit sha1_base64=74MJShuZzxGyM2nLY3EY87InuhI=>AAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA==</latexit><latexit sha1_base64=74MJShuZzxGyM2nLY3EY87InuhI=>AAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA==</latexit><latexit sha1_base64=pLq6KB/1S9uyUeWp/G4byg43mK0=>AAAB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/AF75Ypf9ecgqyTISQVy1Hvlr25fsSxBaZmgxnQCP7XhhGrLmcBpqZsZTCkb0QF2HJU0QRNO5tdOyZlT+iRW2pW0ZK7+npjQxJhxErnOhNqhWfZm4n9eJ7PxVTjhMs0sSrZYFGeCWEVmr5M+18isGDtCmebuVsKGVFNmXUAlF0Kw/PIqaV5UA78a3PmV2nUeRxFO4BTOIYBLqMEt1KEBDB7hGV7hzVPei/fufSxaC14+cwx/4H3+AIWDjww=</latexit> (b) Independent recurrent block
Edge blockNode blockGlobal blockV0
<latexit sha1_base64=gAQ7qdt3IKvK5oBqK3uN1PHYi1k=>AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit><latexit sha1_base64=gAQ7qdt3IKvK5oBqK3uN1PHYi1k=>AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit><latexit sha1_base64=gAQ7qdt3IKvK5oBqK3uN1PHYi1k=>AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit><latexit sha1_base64=gAQ7qdt3IKvK5oBqK3uN1PHYi1k=>AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit>u0
<latexit sha1_base64=Z/n1gIms2/ONBt0R58c8NGdBbqU=>AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit><latexit sha1_base64=Z/n1gIms2/ONBt0R58c8NGdBbqU=>AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit><latexit sha1_base64=Z/n1gIms2/ONBt0R58c8NGdBbqU=>AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit><latexit sha1_base64=Z/n1gIms2/ONBt0R58c8NGdBbqU=>AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit>u
<latexit sha1_base64=znt8hwWv6wryqwCugrweUa+jkM8=>AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubfaSNXu7y+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvFhxZqzvf3tLyyura+uFjeLm1vbObmlvPzQy04TWieRSN2NsKGeC1i2znDaVpjiNOW3Eg6ux33ik2jApbu1Q0SjFPcESRrB1UthWfXaXdUplv+JPgBZJMCPl6mH4dA8AtU7pq92VJEupsIRjY1qBr2yUY20Z4XRUbGeGKkwGuEdbjgqcUhPlk2tH6MQpXZRI7UpYNFF/T+Q4NWaYxq4zxbZv5r2x+J/XymxyEeVMqMxSQaaLkowjK9H4ddRlmhLLh45gopm7FZE+1phYF1DRhRDMv7xIwrNK4FeCG5fGJUxRgCM4hlMI4ByqcA01qAOBB3iGV3jzpPfivXsf09YlbzZzAH/gff4AqE6Qnw==</latexit><latexit sha1_base64=Nc0DXje6uYB+/0fHlXL99yCq0no=>AAAB7XicbVC7SgNBFL0bXzG+ooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0/kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/cD5+AGXTkqw=</latexit><latexit sha1_base64=Nc0DXje6uYB+/0fHlXL99yCq0no=>AAAB7XicbVC7SgNBFL0bXzG+ooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0/kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/cD5+AGXTkqw=</latexit><latexit sha1_base64=S5XnA5iYIAgqxiI+i0ptSwAiKP4=>AAAB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/CHrlSt+1Z+DrJIgJxXIUe+Vv7p9xbIEpWWCGtMJ/NSGE6otZwKnpW5mMKVsRAfYcVTSBE04mV87JWdO6ZNYaVfSkrn6e2JCE2PGSeQ6E2qHZtmbif95nczGV+GEyzSzKNliUZwJYhWZvU76XCOzYuwIZZq7WwkbUk2ZdQGVXAjB8surpHlRDfxqcOdXatd5HEU4gVM4hwAuoQa3UIcGMHiEZ3iFN095L96797FoLXj5zDH8gff5A53Djxw=</latexit>v!u
<latexit sha1_base64=8QVocR3pGD0i/QL+G1OhPZDl9fE=>AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWz3TRLN9mwO6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUEquAbH+bZKK6tr6xvlzcrW9s7unr1/0NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs8+8lQkH/KRp/ggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zop+DlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJ/XjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bH/PWklXMHKI/sD5/AAdVlig=</latexit><latexit sha1_base64=8QVocR3pGD0i/QL+G1OhPZDl9fE=>AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWz3TRLN9mwO6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUEquAbH+bZKK6tr6xvlzcrW9s7unr1/0NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs8+8lQkH/KRp/ggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zop+DlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJ/XjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bH/PWklXMHKI/sD5/AAdVlig=</latexit><latexit sha1_base64=8QVocR3pGD0i/QL+G1OhPZDl9fE=>AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWz3TRLN9mwO6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUEquAbH+bZKK6tr6xvlzcrW9s7unr1/0NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs8+8lQkH/KRp/ggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zop+DlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJ/XjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bH/PWklXMHKI/sD5/AAdVlig=</latexit><latexit sha1_base64=8QVocR3pGD0i/QL+G1OhPZDl9fE=>AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWz3TRLN9mwO6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUEquAbH+bZKK6tr6xvlzcrW9s7unr1/0NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs8+8lQkH/KRp/ggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zop+DlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJ/XjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bH/PWklXMHKI/sD5/AAdVlig=</latexit>
<latexit sha1_base64=m8MJ1M94ujO0d0COo5n2Dsol6rc=>AAAB53icbVC7SgNBFL0bXzG+opY2g0GwCrs2phAM2FhGMA9IFpmdvZsMmZ1dZmaFsKS0sbFQxNZP8Rfs/AZ/wsmj0MQDFw7nnMt9BKng2rjul1NYWV1b3yhulra2d3b3yvsHLZ1kimGTJSJRnYBqFFxi03AjsJMqpHEgsB0MryZ++x6V5om8NaMU/Zj2JY84o8ZKjbtyxa26U5Bl4s1J5fLj+4EAgM1/9sKEZTFKwwTVuuu5qfFzqgxnAselXqYxpWxI+9i1VNIYtZ9P9xyTE6uEJEqULWnIVP3dkdNY61Ec2GRMzUAvehPxP6+bmajm51ymmUHJZoOiTBCTkMnRJOQKmREjSyhT3O5K2IAqyox9Tck+wVs8eZm0zqqeW/Vu3Er9AmYowhEcwyl4cA51uIYGNIFBCI/wDC8Od56cV+dtFi04855D+APn/Qd/sI8A</latexit><latexit sha1_base64=q1zM5ZsCNMJAtNUZI97B98ATlaA=>AAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo+1Si4xJrhRmAzUUgjX2DD71+O/MYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL96+7g7fqt81/tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhY/V3R0YjrQeRb5MRNT09643E/7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1+TtE7zZk+dJ/aTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvk+iCM+3Zgz9wXn8AGkeQ8w==</latexit><latexit sha1_base64=q1zM5ZsCNMJAtNUZI97B98ATlaA=>AAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo+1Si4xJrhRmAzUUgjX2DD71+O/MYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL96+7g7fqt81/tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhY/V3R0YjrQeRb5MRNT09643E/7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1+TtE7zZk+dJ/aTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvk+iCM+3Zgz9wXn8AGkeQ8w==</latexit><latexit sha1_base64=ioxb3woZF1oAlTScqds23PrgiMY=>AAAB53icbVC7SgNBFL3rM8ZX1NJmMAhWYdZGC4uAjWUE84BkkdnZ2WTI7Owyc1cIS37AxkIRW3/Jzr9xkmyhiQcGDuecy9x7wkxJi5R+e2vrG5tb25Wd6u7e/sFh7ei4Y9PccNHmqUpNL2RWKKlFGyUq0cuMYEmoRDcc38787pMwVqb6ASeZCBI21DKWnKGTWo+1Om3QOcgq8UtShxIu/zWIUp4nQiNXzNq+TzMMCmZQciWm1UFuRcb4mA1F31HNEmGDYr7nlJw7JSJxatzTSObq74mCJdZOktAlE4Yju+zNxP+8fo7xdVBIneUoNF98FOeKYEpmR5NIGsFRTRxh3Ei3K+EjZhhHV03VleAvn7xKOpcNnzb8e1pv3pR1VOAUzuACfLiCJtxBC9rAIYJneIU3T3ov3rv3sYiueeXMCfyB9/kDCGmMcA==</latexit>v
<latexit sha1_base64=HCiXjOq04H3f4Ed7vqyiRfd+2dI=>AAAB7XicbVA9SwNBEJ2LXzF+nQo2NotBsAp3NlpYBGwsI5hLIDnj3mYvWbO3e+zuRcKR/2BjoYit/8fOf+Pmo9DEBwOP92aYmRelnGnjed9OYWV1bX2juFna2t7Z3XP3DwItM0VonUguVTPCmnImaN0ww2kzVRQnEaeNaHA98RtDqjST4s6MUhomuCdYzAg2VgraaZ/dDztu2at4U6Bl4s9JuXoUPD0AQK3jfrW7kmQJFYZwrHXL91IT5lgZRjgdl9qZpikmA9yjLUsFTqgO8+m1Y3RqlS6KpbIlDJqqvydynGg9SiLbmWDT14veRPzPa2UmvgxzJtLMUEFmi+KMIyPR5HXUZYoSw0eWYKKYvRWRPlaYGBtQyYbgL768TILziu9V/FubxhXMUIRjOIEz8OECqnADNagDgUd4hld4c6Tz4rw7H7PWgjOfOYQ/cD5/AKnSkKA=</latexit><latexit sha1_base64=/voSHBGyFE5xYPVKPYM/GTIarLQ=>AAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQW/NZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMMp+HAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit><latexit sha1_base64=/voSHBGyFE5xYPVKPYM/GTIarLQ=>AAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQW/NZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMMp+HAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit><latexit sha1_base64=Fc8T4ygtia14k1z/CDji4ezWDqY=>AAAB7XicbVA9SwNBEJ3zM8avqKXNYhCswp2NFhYBG8sI5gOSM+xtNsmavd1jdy4QjvwHGwtFbP0/dv4bN8kVmvhg4PHeDDPzokQKi77/7a2tb2xubRd2irt7+weHpaPjhtWpYbzOtNSmFVHLpVC8jgIlbyWG0ziSvBmNbmd+c8yNFVo94CThYUwHSvQFo+ikRicZisdxt1T2K/4cZJUEOSlDjlq39NXpaZbGXCGT1Np24CcYZtSgYJJPi53U8oSyER3wtqOKxtyG2fzaKTl3So/0tXGlkMzV3xMZja2dxJHrjCkO7bI3E//z2in2r8NMqCRFrthiUT+VBDWZvU56wnCGcuIIZUa4WwkbUkMZuoCKLoRg+eVV0risBH4luPfL1Zs8jgKcwhlcQABXUIU7qEEdGDzBM7zCm6e9F+/d+1i0rnn5zAn8gff5A59Hjx0=</latexit>e!v
<latexit sha1_base64=s3/Cw/iD/Ic9TAit26LWmPV1hK0=>AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit><latexit sha1_base64=s3/Cw/iD/Ic9TAit26LWmPV1hK0=>AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit><latexit sha1_base64=s3/Cw/iD/Ic9TAit26LWmPV1hK0=>AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit><latexit sha1_base64=s3/Cw/iD/Ic9TAit26LWmPV1hK0=>AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit>e
<latexit sha1_base64=gRKFy+QFytmwqWy0cvo5FmmPz8I=>AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubeaSNXu3x+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvDAVXBvX/XaWlldW19YLG8XNre2d3dLevq9lphjWmRRSNUOqUfAE64Ybgc1UIY1DgY1wcDX2G4+oNJfJrRmmGMS0l/CIM2qs5LfTPr/DTqnsVtwJyCLxZqRcPfSf7gGg1il9tbuSZTEmhgmqdctzUxPkVBnOBI6K7UxjStmA9rBlaUJj1EE+uXZETqzSJZFUthJDJurviZzGWg/j0HbG1PT1vDcW//NamYkugpwnaWYwYdNFUSaIkWT8OulyhcyIoSWUKW5vJaxPFWXGBlS0IXjzLy8S/6ziuRXvxqZxCVMU4AiO4RQ8OIcqXEMN6sDgAZ7hFd4c6bw4787HtHXJmc0cwB84nz+QDpCP</latexit><latexit sha1_base64=74MJShuZzxGyM2nLY3EY87InuhI=>AAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA==</latexit><latexit sha1_base64=74MJShuZzxGyM2nLY3EY87InuhI=>AAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA==</latexit><latexit sha1_base64=pLq6KB/1S9uyUeWp/G4byg43mK0=>AAAB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/AF75Ypf9ecgqyTISQVy1Hvlr25fsSxBaZmgxnQCP7XhhGrLmcBpqZsZTCkb0QF2HJU0QRNO5tdOyZlT+iRW2pW0ZK7+npjQxJhxErnOhNqhWfZm4n9eJ7PxVTjhMs0sSrZYFGeCWEVmr5M+18isGDtCmebuVsKGVFNmXUAlF0Kw/PIqaV5UA78a3PmV2nUeRxFO4BTOIYBLqMEt1KEBDB7hGV7hzVPei/fufSxaC14+cwx/4H3+AIWDjww=</latexit>E<latexit sha1_base64=iJ/x8cSgmmYNbMN8WtCvsNrlH/U=>AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit><latexit sha1_base64=iJ/x8cSgmmYNbMN8WtCvsNrlH/U=>AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit><latexit sha1_base64=iJ/x8cSgmmYNbMN8WtCvsNrlH/U=>AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit><latexit sha1_base64=iJ/x8cSgmmYNbMN8WtCvsNrlH/U=>AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit>V<latexit sha1_base64=xc4uzoZiBSxZUZkArgltxczS6nM=>AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64=xc4uzoZiBSxZUZkArgltxczS6nM=>AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64=xc4uzoZiBSxZUZkArgltxczS6nM=>AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64=xc4uzoZiBSxZUZkArgltxczS6nM=>AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit>
(c) Message-passing neural network
Edge blockNode blockGlobal blockV0
<latexit sha1_base64=gAQ7qdt3IKvK5oBqK3uN1PHYi1k=>AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit><latexit sha1_base64=gAQ7qdt3IKvK5oBqK3uN1PHYi1k=>AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit><latexit sha1_base64=gAQ7qdt3IKvK5oBqK3uN1PHYi1k=>AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit><latexit sha1_base64=gAQ7qdt3IKvK5oBqK3uN1PHYi1k=>AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit>
<latexit sha1_base64=m8MJ1M94ujO0d0COo5n2Dsol6rc=>AAAB53icbVC7SgNBFL0bXzG+opY2g0GwCrs2phAM2FhGMA9IFpmdvZsMmZ1dZmaFsKS0sbFQxNZP8Rfs/AZ/wsmj0MQDFw7nnMt9BKng2rjul1NYWV1b3yhulra2d3b3yvsHLZ1kimGTJSJRnYBqFFxi03AjsJMqpHEgsB0MryZ++x6V5om8NaMU/Zj2JY84o8ZKjbtyxa26U5Bl4s1J5fLj+4EAgM1/9sKEZTFKwwTVuuu5qfFzqgxnAselXqYxpWxI+9i1VNIYtZ9P9xyTE6uEJEqULWnIVP3dkdNY61Ec2GRMzUAvehPxP6+bmajm51ymmUHJZoOiTBCTkMnRJOQKmREjSyhT3O5K2IAqyox9Tck+wVs8eZm0zqqeW/Vu3Er9AmYowhEcwyl4cA51uIYGNIFBCI/wDC8Od56cV+dtFi04855D+APn/Qd/sI8A</latexit><latexit sha1_base64=q1zM5ZsCNMJAtNUZI97B98ATlaA=>AAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo+1Si4xJrhRmAzUUgjX2DD71+O/MYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL96+7g7fqt81/tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhY/V3R0YjrQeRb5MRNT09643E/7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1+TtE7zZk+dJ/aTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvk+iCM+3Zgz9wXn8AGkeQ8w==</latexit><latexit sha1_base64=q1zM5ZsCNMJAtNUZI97B98ATlaA=>AAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo+1Si4xJrhRmAzUUgjX2DD71+O/MYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL96+7g7fqt81/tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhY/V3R0YjrQeRb5MRNT09643E/7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1+TtE7zZk+dJ/aTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvk+iCM+3Zgz9wXn8AGkeQ8w==</latexit><latexit sha1_base64=ioxb3woZF1oAlTScqds23PrgiMY=>AAAB53icbVC7SgNBFL3rM8ZX1NJmMAhWYdZGC4uAjWUE84BkkdnZ2WTI7Owyc1cIS37AxkIRW3/Jzr9xkmyhiQcGDuecy9x7wkxJi5R+e2vrG5tb25Wd6u7e/sFh7ei4Y9PccNHmqUpNL2RWKKlFGyUq0cuMYEmoRDcc38787pMwVqb6ASeZCBI21DKWnKGTWo+1Om3QOcgq8UtShxIu/zWIUp4nQiNXzNq+TzMMCmZQciWm1UFuRcb4mA1F31HNEmGDYr7nlJw7JSJxatzTSObq74mCJdZOktAlE4Yju+zNxP+8fo7xdVBIneUoNF98FOeKYEpmR5NIGsFRTRxh3Ei3K+EjZhhHV03VleAvn7xKOpcNnzb8e1pv3pR1VOAUzuACfLiCJtxBC9rAIYJneIU3T3ov3rv3sYiueeXMCfyB9/kDCGmMcA==</latexit>v
<latexit sha1_base64=HCiXjOq04H3f4Ed7vqyiRfd+2dI=>AAAB7XicbVA9SwNBEJ2LXzF+nQo2NotBsAp3NlpYBGwsI5hLIDnj3mYvWbO3e+zuRcKR/2BjoYit/8fOf+Pmo9DEBwOP92aYmRelnGnjed9OYWV1bX2juFna2t7Z3XP3DwItM0VonUguVTPCmnImaN0ww2kzVRQnEaeNaHA98RtDqjST4s6MUhomuCdYzAg2VgraaZ/dDztu2at4U6Bl4s9JuXoUPD0AQK3jfrW7kmQJFYZwrHXL91IT5lgZRjgdl9qZpikmA9yjLUsFTqgO8+m1Y3RqlS6KpbIlDJqqvydynGg9SiLbmWDT14veRPzPa2UmvgxzJtLMUEFmi+KMIyPR5HXUZYoSw0eWYKKYvRWRPlaYGBtQyYbgL768TILziu9V/FubxhXMUIRjOIEz8OECqnADNagDgUd4hld4c6Tz4rw7H7PWgjOfOYQ/cD5/AKnSkKA=</latexit><latexit sha1_base64=/voSHBGyFE5xYPVKPYM/GTIarLQ=>AAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQW/NZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMMp+HAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit><latexit sha1_base64=/voSHBGyFE5xYPVKPYM/GTIarLQ=>AAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQW/NZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMMp+HAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit><latexit sha1_base64=Fc8T4ygtia14k1z/CDji4ezWDqY=>AAAB7XicbVA9SwNBEJ3zM8avqKXNYhCswp2NFhYBG8sI5gOSM+xtNsmavd1jdy4QjvwHGwtFbP0/dv4bN8kVmvhg4PHeDDPzokQKi77/7a2tb2xubRd2irt7+weHpaPjhtWpYbzOtNSmFVHLpVC8jgIlbyWG0ziSvBmNbmd+c8yNFVo94CThYUwHSvQFo+ikRicZisdxt1T2K/4cZJUEOSlDjlq39NXpaZbGXCGT1Np24CcYZtSgYJJPi53U8oSyER3wtqOKxtyG2fzaKTl3So/0tXGlkMzV3xMZja2dxJHrjCkO7bI3E//z2in2r8NMqCRFrthiUT+VBDWZvU56wnCGcuIIZUa4WwkbUkMZuoCKLoRg+eVV0risBH4luPfL1Zs8jgKcwhlcQABXUIU7qEEdGDzBM7zCm6e9F+/d+1i0rnn5zAn8gff5A59Hjx0=</latexit>e!v
<latexit sha1_base64=s3/Cw/iD/Ic9TAit26LWmPV1hK0=>AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit><latexit sha1_base64=s3/Cw/iD/Ic9TAit26LWmPV1hK0=>AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit><latexit sha1_base64=s3/Cw/iD/Ic9TAit26LWmPV1hK0=>AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit><latexit sha1_base64=s3/Cw/iD/Ic9TAit26LWmPV1hK0=>AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit>e
<latexit sha1_base64=gRKFy+QFytmwqWy0cvo5FmmPz8I=>AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubeaSNXu3x+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvDAVXBvX/XaWlldW19YLG8XNre2d3dLevq9lphjWmRRSNUOqUfAE64Ybgc1UIY1DgY1wcDX2G4+oNJfJrRmmGMS0l/CIM2qs5LfTPr/DTqnsVtwJyCLxZqRcPfSf7gGg1il9tbuSZTEmhgmqdctzUxPkVBnOBI6K7UxjStmA9rBlaUJj1EE+uXZETqzSJZFUthJDJurviZzGWg/j0HbG1PT1vDcW//NamYkugpwnaWYwYdNFUSaIkWT8OulyhcyIoSWUKW5vJaxPFWXGBlS0IXjzLy8S/6ziuRXvxqZxCVMU4AiO4RQ8OIcqXEMN6sDgAZ7hFd4c6bw4787HtHXJmc0cwB84nz+QDpCP</latexit><latexit sha1_base64=74MJShuZzxGyM2nLY3EY87InuhI=>AAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA==</latexit><latexit sha1_base64=74MJShuZzxGyM2nLY3EY87InuhI=>AAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA==</latexit><latexit sha1_base64=pLq6KB/1S9uyUeWp/G4byg43mK0=>AAAB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/AF75Ypf9ecgqyTISQVy1Hvlr25fsSxBaZmgxnQCP7XhhGrLmcBpqZsZTCkb0QF2HJU0QRNO5tdOyZlT+iRW2pW0ZK7+npjQxJhxErnOhNqhWfZm4n9eJ7PxVTjhMs0sSrZYFGeCWEVmr5M+18isGDtCmebuVsKGVFNmXUAlF0Kw/PIqaV5UA78a3PmV2nUeRxFO4BTOIYBLqMEt1KEBDB7hGV7hzVPei/fufSxaC14+cwx/4H3+AIWDjww=</latexit>E<latexit sha1_base64=iJ/x8cSgmmYNbMN8WtCvsNrlH/U=>AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit><latexit sha1_base64=iJ/x8cSgmmYNbMN8WtCvsNrlH/U=>AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit><latexit sha1_base64=iJ/x8cSgmmYNbMN8WtCvsNrlH/U=>AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit><latexit sha1_base64=iJ/x8cSgmmYNbMN8WtCvsNrlH/U=>AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit>V<latexit sha1_base64=xc4uzoZiBSxZUZkArgltxczS6nM=>AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64=xc4uzoZiBSxZUZkArgltxczS6nM=>AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64=xc4uzoZiBSxZUZkArgltxczS6nM=>AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64=xc4uzoZiBSxZUZkArgltxczS6nM=>AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit> (d) Non-local neural network
Edge blockNode blockGlobal blocku0
<latexit sha1_base64=Z/n1gIms2/ONBt0R58c8NGdBbqU=>AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit><latexit sha1_base64=Z/n1gIms2/ONBt0R58c8NGdBbqU=>AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit><latexit sha1_base64=Z/n1gIms2/ONBt0R58c8NGdBbqU=>AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit><latexit sha1_base64=Z/n1gIms2/ONBt0R58c8NGdBbqU=>AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit>u
<latexit sha1_base64=znt8hwWv6wryqwCugrweUa+jkM8=>AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubfaSNXu7y+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvFhxZqzvf3tLyyura+uFjeLm1vbObmlvPzQy04TWieRSN2NsKGeC1i2znDaVpjiNOW3Eg6ux33ik2jApbu1Q0SjFPcESRrB1UthWfXaXdUplv+JPgBZJMCPl6mH4dA8AtU7pq92VJEupsIRjY1qBr2yUY20Z4XRUbGeGKkwGuEdbjgqcUhPlk2tH6MQpXZRI7UpYNFF/T+Q4NWaYxq4zxbZv5r2x+J/XymxyEeVMqMxSQaaLkowjK9H4ddRlmhLLh45gopm7FZE+1phYF1DRhRDMv7xIwrNK4FeCG5fGJUxRgCM4hlMI4ByqcA01qAOBB3iGV3jzpPfivXsf09YlbzZzAH/gff4AqE6Qnw==</latexit><latexit sha1_base64=Nc0DXje6uYB+/0fHlXL99yCq0no=>AAAB7XicbVC7SgNBFL0bXzG+ooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0/kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/cD5+AGXTkqw=</latexit><latexit sha1_base64=Nc0DXje6uYB+/0fHlXL99yCq0no=>AAAB7XicbVC7SgNBFL0bXzG+ooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0/kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/cD5+AGXTkqw=</latexit><latexit sha1_base64=S5XnA5iYIAgqxiI+i0ptSwAiKP4=>AAAB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/CHrlSt+1Z+DrJIgJxXIUe+Vv7p9xbIEpWWCGtMJ/NSGE6otZwKnpW5mMKVsRAfYcVTSBE04mV87JWdO6ZNYaVfSkrn6e2JCE2PGSeQ6E2qHZtmbif95nczGV+GEyzSzKNliUZwJYhWZvU76XCOzYuwIZZq7WwkbUk2ZdQGVXAjB8surpHlRDfxqcOdXatd5HEU4gVM4hwAuoQa3UIcGMHiEZ3iFN095L96797FoLXj5zDH8gff5A53Djxw=</latexit>e!u
<latexit sha1_base64=2suSYs2KtjHJeb1CIts1JrhYPII=>AAAB/nicbVBNS8NAEN34WetXVDx5WSyCp5KIoMeiF48V7Ac0sWy2m3bpZjfsTpQSCv4VLx4U8erv8Oa/cdvmoK0PBh7vzTAzL0oFN+B5387S8srq2nppo7y5tb2z6+7tN43KNGUNqoTS7YgYJrhkDeAgWDvVjCSRYK1oeD3xWw9MG67kHYxSFiakL3nMKQErdd3DQA/Ufc4CzfsDIFqrR5yNu27Fq3pT4EXiF6SCCtS77lfQUzRLmAQqiDEd30shzIkGTgUbl4PMsJTQIemzjqWSJMyE+fT8MT6xSg/HStuSgKfq74mcJMaMksh2JgQGZt6biP95nQziyzDnMs2ASTpbFGcCg8KTLHCPa0ZBjCwhVHN7K6YDogkFm1jZhuDPv7xImmdV36v6t+eV2lURRwkdoWN0inx0gWroBtVRA1GUo2f0it6cJ+fFeXc+Zq1LTjFzgP7A+fwB7JSWFw==</latexit><latexit sha1_base64=2suSYs2KtjHJeb1CIts1JrhYPII=>AAAB/nicbVBNS8NAEN34WetXVDx5WSyCp5KIoMeiF48V7Ac0sWy2m3bpZjfsTpQSCv4VLx4U8erv8Oa/cdvmoK0PBh7vzTAzL0oFN+B5387S8srq2nppo7y5tb2z6+7tN43KNGUNqoTS7YgYJrhkDeAgWDvVjCSRYK1oeD3xWw9MG67kHYxSFiakL3nMKQErdd3DQA/Ufc4CzfsDIFqrR5yNu27Fq3pT4EXiF6SCCtS77lfQUzRLmAQqiDEd30shzIkGTgUbl4PMsJTQIemzjqWSJMyE+fT8MT6xSg/HStuSgKfq74mcJMaMksh2JgQGZt6biP95nQziyzDnMs2ASTpbFGcCg8KTLHCPa0ZBjCwhVHN7K6YDogkFm1jZhuDPv7xImmdV36v6t+eV2lURRwkdoWN0inx0gWroBtVRA1GUo2f0it6cJ+fFeXc+Zq1LTjFzgP7A+fwB7JSWFw==</latexit><latexit sha1_base64=2suSYs2KtjHJeb1CIts1JrhYPII=>AAAB/nicbVBNS8NAEN34WetXVDx5WSyCp5KIoMeiF48V7Ac0sWy2m3bpZjfsTpQSCv4VLx4U8erv8Oa/cdvmoK0PBh7vzTAzL0oFN+B5387S8srq2nppo7y5tb2z6+7tN43KNGUNqoTS7YgYJrhkDeAgWDvVjCSRYK1oeD3xWw9MG67kHYxSFiakL3nMKQErdd3DQA/Ufc4CzfsDIFqrR5yNu27Fq3pT4EXiF6SCCtS77lfQUzRLmAQqiDEd30shzIkGTgUbl4PMsJTQIemzjqWSJMyE+fT8MT6xSg/HStuSgKfq74mcJMaMksh2JgQGZt6biP95nQziyzDnMs2ASTpbFGcCg8KTLHCPa0ZBjCwhVHN7K6YDogkFm1jZhuDPv7xImmdV36v6t+eV2lURRwkdoWN0inx0gWroBtVRA1GUo2f0it6cJ+fFeXc+Zq1LTjFzgP7A+fwB7JSWFw==</latexit><latexit sha1_base64=2suSYs2KtjHJeb1CIts1JrhYPII=>AAAB/nicbVBNS8NAEN34WetXVDx5WSyCp5KIoMeiF48V7Ac0sWy2m3bpZjfsTpQSCv4VLx4U8erv8Oa/cdvmoK0PBh7vzTAzL0oFN+B5387S8srq2nppo7y5tb2z6+7tN43KNGUNqoTS7YgYJrhkDeAgWDvVjCSRYK1oeD3xWw9MG67kHYxSFiakL3nMKQErdd3DQA/Ufc4CzfsDIFqrR5yNu27Fq3pT4EXiF6SCCtS77lfQUzRLmAQqiDEd30shzIkGTgUbl4PMsJTQIemzjqWSJMyE+fT8MT6xSg/HStuSgKfq74mcJMaMksh2JgQGZt6biP95nQziyzDnMs2ASTpbFGcCg8KTLHCPa0ZBjCwhVHN7K6YDogkFm1jZhuDPv7xImmdV36v6t+eV2lURRwkdoWN0inx0gWroBtVRA1GUo2f0it6cJ+fFeXc+Zq1LTjFzgP7A+fwB7JSWFw==</latexit>e
<latexit sha1_base64=gRKFy+QFytmwqWy0cvo5FmmPz8I=>AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubeaSNXu3x+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvDAVXBvX/XaWlldW19YLG8XNre2d3dLevq9lphjWmRRSNUOqUfAE64Ybgc1UIY1DgY1wcDX2G4+oNJfJrRmmGMS0l/CIM2qs5LfTPr/DTqnsVtwJyCLxZqRcPfSf7gGg1il9tbuSZTEmhgmqdctzUxPkVBnOBI6K7UxjStmA9rBlaUJj1EE+uXZETqzSJZFUthJDJurviZzGWg/j0HbG1PT1vDcW//NamYkugpwnaWYwYdNFUSaIkWT8OulyhcyIoSWUKW5vJaxPFWXGBlS0IXjzLy8S/6ziuRXvxqZxCVMU4AiO4RQ8OIcqXEMN6sDgAZ7hFd4c6bw4787HtHXJmc0cwB84nz+QDpCP</latexit><latexit sha1_base64=74MJShuZzxGyM2nLY3EY87InuhI=>AAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA==</latexit><latexit sha1_base64=74MJShuZzxGyM2nLY3EY87InuhI=>AAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA==</latexit><latexit sha1_base64=pLq6KB/1S9uyUeWp/G4byg43mK0=>AAAB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/AF75Ypf9ecgqyTISQVy1Hvlr25fsSxBaZmgxnQCP7XhhGrLmcBpqZsZTCkb0QF2HJU0QRNO5tdOyZlT+iRW2pW0ZK7+npjQxJhxErnOhNqhWfZm4n9eJ7PxVTjhMs0sSrZYFGeCWEVmr5M+18isGDtCmebuVsKGVFNmXUAlF0Kw/PIqaV5UA78a3PmV2nUeRxFO4BTOIYBLqMEt1KEBDB7hGV7hzVPei/fufSxaC14+cwx/4H3+AIWDjww=</latexit>E<latexit sha1_base64=iJ/x8cSgmmYNbMN8WtCvsNrlH/U=>AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit><latexit sha1_base64=iJ/x8cSgmmYNbMN8WtCvsNrlH/U=>AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit><latexit sha1_base64=iJ/x8cSgmmYNbMN8WtCvsNrlH/U=>AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit><latexit sha1_base64=iJ/x8cSgmmYNbMN8WtCvsNrlH/U=>AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0IOHgggeW7Af0Iay2U7atZtN2N0IJfQXePGgiFd/kjf/jds2B219MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH49uZ335CpXksH8wkQT+iQ8lDzqixUuOuX664VXcOskq8nFQgR71f/uoNYpZGKA0TVOuu5ybGz6gynAmclnqpxoSyMR1i11JJI9R+Nj90Ss6sMiBhrGxJQ+bq74mMRlpPosB2RtSM9LI3E//zuqkJr/2MyyQ1KNliUZgKYmIy+5oMuEJmxMQSyhS3txI2oooyY7Mp2RC85ZdXSeui6rlVr3FZqd3kcRThBE7hHDy4ghrcQx2awADhGV7hzXl0Xpx352PRWnDymWP4A+fzB5cfjMM=</latexit>V<latexit sha1_base64=xc4uzoZiBSxZUZkArgltxczS6nM=>AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64=xc4uzoZiBSxZUZkArgltxczS6nM=>AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64=xc4uzoZiBSxZUZkArgltxczS6nM=>AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64=xc4uzoZiBSxZUZkArgltxczS6nM=>AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit>
(e) Relation network
Edge blockNode blockGlobal blocku0
<latexit sha1_base64=Z/n1gIms2/ONBt0R58c8NGdBbqU=>AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit><latexit sha1_base64=Z/n1gIms2/ONBt0R58c8NGdBbqU=>AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit><latexit sha1_base64=Z/n1gIms2/ONBt0R58c8NGdBbqU=>AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit><latexit sha1_base64=Z/n1gIms2/ONBt0R58c8NGdBbqU=>AAAB8nicbVDLSsNAFL2pr1pfVZduBovoqiQi6MJFwY3LCvYBbSiT6aQdOpmEmRuhhH6GGxeKuPVr3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJne533ni2ohYPeI04X5ER0qEglG0Uq8fURwHYZbOzgfVmlt35yCrxCtIDQo0B9Wv/jBmacQVMkmN6Xlugn5GNQom+azSTw1PKJvQEe9ZqmjEjZ/NI8/ImVWGJIy1fQrJXP29kdHImGkU2Mk8oln2cvE/r5dieONnQiUpcsUWH4WpJBiT/H4yFJozlFNLKNPCZiVsTDVlaFuq2BK85ZNXSfuy7rl17+Gq1rgt6ijDCZzCBXhwDQ24hya0gEEMz/AKbw46L86787EYLTnFzjH8gfP5A1s4kUQ=</latexit>u
<latexit sha1_base64=znt8hwWv6wryqwCugrweUa+jkM8=>AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubfaSNXu7y+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvFhxZqzvf3tLyyura+uFjeLm1vbObmlvPzQy04TWieRSN2NsKGeC1i2znDaVpjiNOW3Eg6ux33ik2jApbu1Q0SjFPcESRrB1UthWfXaXdUplv+JPgBZJMCPl6mH4dA8AtU7pq92VJEupsIRjY1qBr2yUY20Z4XRUbGeGKkwGuEdbjgqcUhPlk2tH6MQpXZRI7UpYNFF/T+Q4NWaYxq4zxbZv5r2x+J/XymxyEeVMqMxSQaaLkowjK9H4ddRlmhLLh45gopm7FZE+1phYF1DRhRDMv7xIwrNK4FeCG5fGJUxRgCM4hlMI4ByqcA01qAOBB3iGV3jzpPfivXsf09YlbzZzAH/gff4AqE6Qnw==</latexit><latexit sha1_base64=Nc0DXje6uYB+/0fHlXL99yCq0no=>AAAB7XicbVC7SgNBFL0bXzG+ooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0/kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/cD5+AGXTkqw=</latexit><latexit sha1_base64=Nc0DXje6uYB+/0fHlXL99yCq0no=>AAAB7XicbVC7SgNBFL0bXzG+ooKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kitAGSXiiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRRf0/kONY6i0PbGWMz0PPeWPzPa6cmOg9yJmRqqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/cD5+AGXTkqw=</latexit><latexit sha1_base64=S5XnA5iYIAgqxiI+i0ptSwAiKP4=>AAAB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/CHrlSt+1Z+DrJIgJxXIUe+Vv7p9xbIEpWWCGtMJ/NSGE6otZwKnpW5mMKVsRAfYcVTSBE04mV87JWdO6ZNYaVfSkrn6e2JCE2PGSeQ6E2qHZtmbif95nczGV+GEyzSzKNliUZwJYhWZvU76XCOzYuwIZZq7WwkbUk2ZdQGVXAjB8surpHlRDfxqcOdXatd5HEU4gVM4hwAuoQa3UIcGMHiEZ3iFN095L96797FoLXj5zDH8gff5A53Djxw=</latexit>v!u
<latexit sha1_base64=8QVocR3pGD0i/QL+G1OhPZDl9fE=>AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWz3TRLN9mwO6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUEquAbH+bZKK6tr6xvlzcrW9s7unr1/0NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs8+8lQkH/KRp/ggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zop+DlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJ/XjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bH/PWklXMHKI/sD5/AAdVlig=</latexit><latexit sha1_base64=8QVocR3pGD0i/QL+G1OhPZDl9fE=>AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWz3TRLN9mwO6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUEquAbH+bZKK6tr6xvlzcrW9s7unr1/0NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs8+8lQkH/KRp/ggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zop+DlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJ/XjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bH/PWklXMHKI/sD5/AAdVlig=</latexit><latexit sha1_base64=8QVocR3pGD0i/QL+G1OhPZDl9fE=>AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWz3TRLN9mwO6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUEquAbH+bZKK6tr6xvlzcrW9s7unr1/0NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs8+8lQkH/KRp/ggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zop+DlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJ/XjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bH/PWklXMHKI/sD5/AAdVlig=</latexit><latexit sha1_base64=8QVocR3pGD0i/QL+G1OhPZDl9fE=>AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWz3TRLN9mwO6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUEquAbH+bZKK6tr6xvlzcrW9s7unr1/0NIyU5Q1qRRSdQKimeAJawIHwTqpYiQOBGsHw5up3x4xpblM7mGcMj8mg4SHnBIwUs8+8lQkH/KRp/ggAqKUfMTZpGdXnZozA14mbkGqqECjZ395fUmzmCVABdG66zop+DlRwKlgk4qXaZYSOiQD1jU0ITHTfj47f4JPjdLHoVSmEsAz9fdETmKtx3FgOmMCkV70puJ/XjeD8MrPeZJmwBI6XxRmAoPE0yxwnytGQYwNIVRxcyumEVGEgkmsYkJwF19eJq3zmuvU3LuLav26iKOMjtEJOkMuukR1dIsaqIkoytEzekVv1pP1Yr1bH/PWklXMHKI/sD5/AAdVlig=</latexit>
<latexit sha1_base64=m8MJ1M94ujO0d0COo5n2Dsol6rc=>AAAB53icbVC7SgNBFL0bXzG+opY2g0GwCrs2phAM2FhGMA9IFpmdvZsMmZ1dZmaFsKS0sbFQxNZP8Rfs/AZ/wsmj0MQDFw7nnMt9BKng2rjul1NYWV1b3yhulra2d3b3yvsHLZ1kimGTJSJRnYBqFFxi03AjsJMqpHEgsB0MryZ++x6V5om8NaMU/Zj2JY84o8ZKjbtyxa26U5Bl4s1J5fLj+4EAgM1/9sKEZTFKwwTVuuu5qfFzqgxnAselXqYxpWxI+9i1VNIYtZ9P9xyTE6uEJEqULWnIVP3dkdNY61Ec2GRMzUAvehPxP6+bmajm51ymmUHJZoOiTBCTkMnRJOQKmREjSyhT3O5K2IAqyox9Tck+wVs8eZm0zqqeW/Vu3Er9AmYowhEcwyl4cA51uIYGNIFBCI/wDC8Od56cV+dtFi04855D+APn/Qd/sI8A</latexit><latexit sha1_base64=q1zM5ZsCNMJAtNUZI97B98ATlaA=>AAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo+1Si4xJrhRmAzUUgjX2DD71+O/MYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL96+7g7fqt81/tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhY/V3R0YjrQeRb5MRNT09643E/7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1+TtE7zZk+dJ/aTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvk+iCM+3Zgz9wXn8AGkeQ8w==</latexit><latexit sha1_base64=q1zM5ZsCNMJAtNUZI97B98ATlaA=>AAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo+1Si4xJrhRmAzUUgjX2DD71+O/MYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL96+7g7fqt81/tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhY/V3R0YjrQeRb5MRNT09643E/7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1+TtE7zZk+dJ/aTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvk+iCM+3Zgz9wXn8AGkeQ8w==</latexit><latexit sha1_base64=ioxb3woZF1oAlTScqds23PrgiMY=>AAAB53icbVC7SgNBFL3rM8ZX1NJmMAhWYdZGC4uAjWUE84BkkdnZ2WTI7Owyc1cIS37AxkIRW3/Jzr9xkmyhiQcGDuecy9x7wkxJi5R+e2vrG5tb25Wd6u7e/sFh7ei4Y9PccNHmqUpNL2RWKKlFGyUq0cuMYEmoRDcc38787pMwVqb6ASeZCBI21DKWnKGTWo+1Om3QOcgq8UtShxIu/zWIUp4nQiNXzNq+TzMMCmZQciWm1UFuRcb4mA1F31HNEmGDYr7nlJw7JSJxatzTSObq74mCJdZOktAlE4Yju+zNxP+8fo7xdVBIneUoNF98FOeKYEpmR5NIGsFRTRxh3Ei3K+EjZhhHV03VleAvn7xKOpcNnzb8e1pv3pR1VOAUzuACfLiCJtxBC9rAIYJneIU3T3ov3rv3sYiueeXMCfyB9/kDCGmMcA==</latexit>v
<latexit sha1_base64=HCiXjOq04H3f4Ed7vqyiRfd+2dI=>AAAB7XicbVA9SwNBEJ2LXzF+nQo2NotBsAp3NlpYBGwsI5hLIDnj3mYvWbO3e+zuRcKR/2BjoYit/8fOf+Pmo9DEBwOP92aYmRelnGnjed9OYWV1bX2juFna2t7Z3XP3DwItM0VonUguVTPCmnImaN0ww2kzVRQnEaeNaHA98RtDqjST4s6MUhomuCdYzAg2VgraaZ/dDztu2at4U6Bl4s9JuXoUPD0AQK3jfrW7kmQJFYZwrHXL91IT5lgZRjgdl9qZpikmA9yjLUsFTqgO8+m1Y3RqlS6KpbIlDJqqvydynGg9SiLbmWDT14veRPzPa2UmvgxzJtLMUEFmi+KMIyPR5HXUZYoSw0eWYKKYvRWRPlaYGBtQyYbgL768TILziu9V/FubxhXMUIRjOIEz8OECqnADNagDgUd4hld4c6Tz4rw7H7PWgjOfOYQ/cD5/AKnSkKA=</latexit><latexit sha1_base64=/voSHBGyFE5xYPVKPYM/GTIarLQ=>AAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQW/NZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMMp+HAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit><latexit sha1_base64=/voSHBGyFE5xYPVKPYM/GTIarLQ=>AAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQW/NZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMMp+HAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit><latexit sha1_base64=Fc8T4ygtia14k1z/CDji4ezWDqY=>AAAB7XicbVA9SwNBEJ3zM8avqKXNYhCswp2NFhYBG8sI5gOSM+xtNsmavd1jdy4QjvwHGwtFbP0/dv4bN8kVmvhg4PHeDDPzokQKi77/7a2tb2xubRd2irt7+weHpaPjhtWpYbzOtNSmFVHLpVC8jgIlbyWG0ziSvBmNbmd+c8yNFVo94CThYUwHSvQFo+ikRicZisdxt1T2K/4cZJUEOSlDjlq39NXpaZbGXCGT1Np24CcYZtSgYJJPi53U8oSyER3wtqOKxtyG2fzaKTl3So/0tXGlkMzV3xMZja2dxJHrjCkO7bI3E//z2in2r8NMqCRFrthiUT+VBDWZvU56wnCGcuIIZUa4WwkbUkMZuoCKLoRg+eVV0risBH4luPfL1Zs8jgKcwhlcQABXUIU7qEEdGDzBM7zCm6e9F+/d+1i0rnn5zAn8gff5A59Hjx0=</latexit>V<latexit sha1_base64=xc4uzoZiBSxZUZkArgltxczS6nM=>AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64=xc4uzoZiBSxZUZkArgltxczS6nM=>AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64=xc4uzoZiBSxZUZkArgltxczS6nM=>AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64=xc4uzoZiBSxZUZkArgltxczS6nM=>AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit>u<latexit sha1_base64=0coyYP26hzTYQyo/d27+M3N3DnU=>AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6MJFwY3LCvaBbSiT6aQdOpmEmRuhhP6FGxeKuPVv3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJre533ni2ohYPeA04X5ER0qEglG00mM/ojgOwiydDao1t+7OQVaJV5AaFGgOql/9YczSiCtkkhrT89wE/YxqFEzyWaWfGp5QNqEj3rNU0YgbP5snnpEzqwxJGGv7FJK5+nsjo5Ex0yiwk3lCs+zl4n9eL8Xw2s+ESlLkii0+ClNJMCb5+WQoNGcop5ZQpoXNStiYasrQllSxJXjLJ6+S9kXdc+ve/WWtcVPUUYYTOIVz8OAKGnAHTWgBAwXP8ApvjnFenHfnYzFacoqdY/gD5/MH9tyREw==</latexit><latexit sha1_base64=0coyYP26hzTYQyo/d27+M3N3DnU=>AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6MJFwY3LCvaBbSiT6aQdOpmEmRuhhP6FGxeKuPVv3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJre533ni2ohYPeA04X5ER0qEglG00mM/ojgOwiydDao1t+7OQVaJV5AaFGgOql/9YczSiCtkkhrT89wE/YxqFEzyWaWfGp5QNqEj3rNU0YgbP5snnpEzqwxJGGv7FJK5+nsjo5Ex0yiwk3lCs+zl4n9eL8Xw2s+ESlLkii0+ClNJMCb5+WQoNGcop5ZQpoXNStiYasrQllSxJXjLJ6+S9kXdc+ve/WWtcVPUUYYTOIVz8OAKGnAHTWgBAwXP8ApvjnFenHfnYzFacoqdY/gD5/MH9tyREw==</latexit><latexit sha1_base64=0coyYP26hzTYQyo/d27+M3N3DnU=>AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6MJFwY3LCvaBbSiT6aQdOpmEmRuhhP6FGxeKuPVv3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJre533ni2ohYPeA04X5ER0qEglG00mM/ojgOwiydDao1t+7OQVaJV5AaFGgOql/9YczSiCtkkhrT89wE/YxqFEzyWaWfGp5QNqEj3rNU0YgbP5snnpEzqwxJGGv7FJK5+nsjo5Ex0yiwk3lCs+zl4n9eL8Xw2s+ESlLkii0+ClNJMCb5+WQoNGcop5ZQpoXNStiYasrQllSxJXjLJ6+S9kXdc+ve/WWtcVPUUYYTOIVz8OAKGnAHTWgBAwXP8ApvjnFenHfnYzFacoqdY/gD5/MH9tyREw==</latexit><latexit sha1_base64=0coyYP26hzTYQyo/d27+M3N3DnU=>AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6MJFwY3LCvaBbSiT6aQdOpmEmRuhhP6FGxeKuPVv3Pk3TtostPXAwOGce5lzT5BIYdB1v53S2vrG5lZ5u7Kzu7d/UD08aps41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJre533ni2ohYPeA04X5ER0qEglG00mM/ojgOwiydDao1t+7OQVaJV5AaFGgOql/9YczSiCtkkhrT89wE/YxqFEzyWaWfGp5QNqEj3rNU0YgbP5snnpEzqwxJGGv7FJK5+nsjo5Ex0yiwk3lCs+zl4n9eL8Xw2s+ESlLkii0+ClNJMCb5+WQoNGcop5ZQpoXNStiYasrQllSxJXjLJ6+S9kXdc+ve/WWtcVPUUYYTOIVz8OAKGnAHTWgBAwXP8ApvjnFenHfnYzFacoqdY/gD5/MH9tyREw==</latexit> (f) Deep set
Figure 4: Dierent internal GN block congurations. See Section 3.2 for details on the notation,
and Section 4 for details about each variant. (a) A full GN predicts node, edge, and global output
attributes based on incoming node, edge, and global attributes. (b) An independent, recurrent
update block takes input and hidden graphs, and the functions are RNNs (Sanchez-Gonzalez
et al., 2018). (c) An MPNN (Gilmer et al., 2017) predicts node, edge, and global output attributes
based on incoming node, edge, and global attributes. Note that the global prediction does not
include aggregated edges. (d) A NLNN (Wang et al., 2018c) only predicts node output attributes.
(e) A relation network (Raposo et al., 2017; Santoro et al., 2017) only uses the edge predictions
to predict global attributes. (f) A Deep Set (Zaheer et al., 2017) bypasses the edge update and
predicts updated global attributes.
4.2.1 Message-passing neural network (MPNN)
Gilmer et al. (2017)'s MPNN generalizes a number of previous architectures and can be translated
naturally into the GN formalism. Following the MPNN paper's terminology (see Gilmer et al.
(2017), pages 2-4):
the message function, Mt, plays the role of the GN's e, but does not take uas input,
elementwise summation is used for the GN's e!v,
the update function, Ut, plays the role of the GN's v,
16V<latexit sha1_base64=xc4uzoZiBSxZUZkArgltxczS6nM=>AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64=xc4uzoZiBSxZUZkArgltxczS6nM=>AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64=xc4uzoZiBSxZUZkArgltxczS6nM=>AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit><latexit sha1_base64=xc4uzoZiBSxZUZkArgltxczS6nM=>AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyAfMByRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0UNja3tneKu6W9/YPDo/LxSVvHqWLYYrGIVTegGgWX2DLcCOwmCmkUCOwEk7u533lCpXksH8w0QT+iI8lDzqixUrM9KFfcqrsAWSdeTiqQozEof/WHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ6mkEWo/Wxw6IxdWGZIwVrakIQv190RGI62nUWA7I2rGetWbi/95vdSEN37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdmUbAje6svrpH1V9dyq17yu1G/zOIpwBudwCR7UoA730IAWMEB4hld4cx6dF+fd+Vi2Fpx85hT+wPn8AbDjjNQ=</latexit>e
<latexit sha1_base64=mjs48M94BmT64MLe6tKpQUPkNJM=>AAAB73icbVA9SwNBEJ3zM8avqKXNYRCswp0IWlgEbCwjmA9IzjC3mUuW7O2du3tCCPkTNhaK2Pp37Pw3bpIrNPHBwOO9GWbmhang2njet7Oyura+sVnYKm7v7O7tlw4OGzrJFKM6S0SiWiFqElxS3XAjqJUqwjgU1AyHN1O/+URK80Tem1FKQYx9ySPO0Fip1UGRDvCBuqWyV/FmcJeJn5My5Kh1S1+dXsKymKRhArVu+15qgjEqw5mgSbGTaUqRDbFPbUslxqSD8ezeiXtqlZ4bJcqWNO5M/T0xxljrURzazhjNQC96U/E/r52Z6CoYc5lmhiSbL4oy4ZrEnT7v9rgiZsTIEmSK21tdNkCFzNiIijYEf/HlZdI4r/hexb+7KFev8zgKcAwncAY+XEIVbqEGdWAg4Ble4c15dF6cd+dj3rri5DNH8AfO5w8AXY/p</latexit><latexit sha1_base64=mjs48M94BmT64MLe6tKpQUPkNJM=>AAAB73icbVA9SwNBEJ3zM8avqKXNYRCswp0IWlgEbCwjmA9IzjC3mUuW7O2du3tCCPkTNhaK2Pp37Pw3bpIrNPHBwOO9GWbmhang2njet7Oyura+sVnYKm7v7O7tlw4OGzrJFKM6S0SiWiFqElxS3XAjqJUqwjgU1AyHN1O/+URK80Tem1FKQYx9ySPO0Fip1UGRDvCBuqWyV/FmcJeJn5My5Kh1S1+dXsKymKRhArVu+15qgjEqw5mgSbGTaUqRDbFPbUslxqSD8ezeiXtqlZ4bJcqWNO5M/T0xxljrURzazhjNQC96U/E/r52Z6CoYc5lmhiSbL4oy4ZrEnT7v9rgiZsTIEmSK21tdNkCFzNiIijYEf/HlZdI4r/hexb+7KFev8zgKcAwncAY+XEIVbqEGdWAg4Ble4c15dF6cd+dj3rri5DNH8AfO5w8AXY/p</latexit><latexit sha1_base64=mjs48M94BmT64MLe6tKpQUPkNJM=>AAAB73icbVA9SwNBEJ3zM8avqKXNYRCswp0IWlgEbCwjmA9IzjC3mUuW7O2du3tCCPkTNhaK2Pp37Pw3bpIrNPHBwOO9GWbmhang2njet7Oyura+sVnYKm7v7O7tlw4OGzrJFKM6S0SiWiFqElxS3XAjqJUqwjgU1AyHN1O/+URK80Tem1FKQYx9ySPO0Fip1UGRDvCBuqWyV/FmcJeJn5My5Kh1S1+dXsKymKRhArVu+15qgjEqw5mgSbGTaUqRDbFPbUslxqSD8ezeiXtqlZ4bJcqWNO5M/T0xxljrURzazhjNQC96U/E/r52Z6CoYc5lmhiSbL4oy4ZrEnT7v9rgiZsTIEmSK21tdNkCFzNiIijYEf/HlZdI4r/hexb+7KFev8zgKcAwncAY+XEIVbqEGdWAg4Ble4c15dF6cd+dj3rri5DNH8AfO5w8AXY/p</latexit><latexit sha1_base64=mjs48M94BmT64MLe6tKpQUPkNJM=>AAAB73icbVA9SwNBEJ3zM8avqKXNYRCswp0IWlgEbCwjmA9IzjC3mUuW7O2du3tCCPkTNhaK2Pp37Pw3bpIrNPHBwOO9GWbmhang2njet7Oyura+sVnYKm7v7O7tlw4OGzrJFKM6S0SiWiFqElxS3XAjqJUqwjgU1AyHN1O/+URK80Tem1FKQYx9ySPO0Fip1UGRDvCBuqWyV/FmcJeJn5My5Kh1S1+dXsKymKRhArVu+15qgjEqw5mgSbGTaUqRDbFPbUslxqSD8ezeiXtqlZ4bJcqWNO5M/T0xxljrURzazhjNQC96U/E/r52Z6CoYc5lmhiSbL4oy4ZrEnT7v9rgiZsTIEmSK21tdNkCFzNiIijYEf/HlZdI4r/hexb+7KFev8zgKcAwncAY+XEIVbqEGdWAg4Ble4c15dF6cd+dj3rri5DNH8AfO5w8AXY/p</latexit>e
<latexit sha1_base64=EsSika+7GeuHZg/021QVL5D37Tg=>AAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoAcPAS8eI5gHJGuYnfQmQ2YfzPQKIeQjvHhQxKvf482/cZLsQRMLGoqqbrq7glRJQ6777RTW1jc2t4rbpZ3dvf2D8uFR0ySZFtgQiUp0O+AGlYyxQZIUtlONPAoUtoLR7cxvPaE2MokfaJyiH/FBLEMpOFmp1Q2Q+CP2yhW36s7BVomXkwrkqPfKX91+IrIIYxKKG9Px3JT8CdckhcJpqZsZTLkY8QF2LI15hMafzM+dsjOr9FmYaFsxsbn6e2LCI2PGUWA7I05Ds+zNxP+8TkbhtT+RcZoRxmKxKMwUo4TNfmd9qVGQGlvChZb2ViaGXHNBNqGSDcFbfnmVNC+qnlv17i8rtZs8jiKcwCmcgwdXUIM7qEMDBIzgGV7hzUmdF+fd+Vi0Fpx85hj+wPn8ATfaj3U=</latexit><latexit sha1_base64=EsSika+7GeuHZg/021QVL5D37Tg=>AAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoAcPAS8eI5gHJGuYnfQmQ2YfzPQKIeQjvHhQxKvf482/cZLsQRMLGoqqbrq7glRJQ6777RTW1jc2t4rbpZ3dvf2D8uFR0ySZFtgQiUp0O+AGlYyxQZIUtlONPAoUtoLR7cxvPaE2MokfaJyiH/FBLEMpOFmp1Q2Q+CP2yhW36s7BVomXkwrkqPfKX91+IrIIYxKKG9Px3JT8CdckhcJpqZsZTLkY8QF2LI15hMafzM+dsjOr9FmYaFsxsbn6e2LCI2PGUWA7I05Ds+zNxP+8TkbhtT+RcZoRxmKxKMwUo4TNfmd9qVGQGlvChZb2ViaGXHNBNqGSDcFbfnmVNC+qnlv17i8rtZs8jiKcwCmcgwdXUIM7qEMDBIzgGV7hzUmdF+fd+Vi0Fpx85hj+wPn8ATfaj3U=</latexit><latexit sha1_base64=EsSika+7GeuHZg/021QVL5D37Tg=>AAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoAcPAS8eI5gHJGuYnfQmQ2YfzPQKIeQjvHhQxKvf482/cZLsQRMLGoqqbrq7glRJQ6777RTW1jc2t4rbpZ3dvf2D8uFR0ySZFtgQiUp0O+AGlYyxQZIUtlONPAoUtoLR7cxvPaE2MokfaJyiH/FBLEMpOFmp1Q2Q+CP2yhW36s7BVomXkwrkqPfKX91+IrIIYxKKG9Px3JT8CdckhcJpqZsZTLkY8QF2LI15hMafzM+dsjOr9FmYaFsxsbn6e2LCI2PGUWA7I05Ds+zNxP+8TkbhtT+RcZoRxmKxKMwUo4TNfmd9qVGQGlvChZb2ViaGXHNBNqGSDcFbfnmVNC+qnlv17i8rtZs8jiKcwCmcgwdXUIM7qEMDBIzgGV7hzUmdF+fd+Vi0Fpx85hj+wPn8ATfaj3U=</latexit><latexit sha1_base64=EsSika+7GeuHZg/021QVL5D37Tg=>AAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoAcPAS8eI5gHJGuYnfQmQ2YfzPQKIeQjvHhQxKvf482/cZLsQRMLGoqqbrq7glRJQ6777RTW1jc2t4rbpZ3dvf2D8uFR0ySZFtgQiUp0O+AGlYyxQZIUtlONPAoUtoLR7cxvPaE2MokfaJyiH/FBLEMpOFmp1Q2Q+CP2yhW36s7BVomXkwrkqPfKX91+IrIIYxKKG9Px3JT8CdckhcJpqZsZTLkY8QF2LI15hMafzM+dsjOr9FmYaFsxsbn6e2LCI2PGUWA7I05Ds+zNxP+8TkbhtT+RcZoRxmKxKMwUo4TNfmd9qVGQGlvChZb2ViaGXHNBNqGSDcFbfnmVNC+qnlv17i8rtZs8jiKcwCmcgwdXUIM7qEMDBIzgGV7hzUmdF+fd+Vi0Fpx85hj+wPn8ATfaj3U=</latexit>e!v
<latexit sha1_base64=s3/Cw/iD/Ic9TAit26LWmPV1hK0=>AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit><latexit sha1_base64=s3/Cw/iD/Ic9TAit26LWmPV1hK0=>AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit><latexit sha1_base64=s3/Cw/iD/Ic9TAit26LWmPV1hK0=>AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit><latexit sha1_base64=s3/Cw/iD/Ic9TAit26LWmPV1hK0=>AAAB/nicbVBNS8NAEN3Ur1q/ouLJy2IRPJVEBD0WvXisYD+giWWznTRLN9mwu6mUUPCvePGgiFd/hzf/jds2B219MPB4b4aZeUHKmdKO822VVlbX1jfKm5Wt7Z3dPXv/oKVEJik0qeBCdgKigLMEmpppDp1UAokDDu1geDP12yOQionkXo9T8GMySFjIKNFG6tlHnozEQw6eZINIEynFIx5NenbVqTkz4GXiFqSKCjR69pfXFzSLIdGUE6W6rpNqPydSM8phUvEyBSmhQzKArqEJiUH5+ez8CT41Sh+HQppKNJ6pvydyEis1jgPTGRMdqUVvKv7ndTMdXvk5S9JMQ0Lni8KMYy3wNAvcZxKo5mNDCJXM3IppRCSh2iRWMSG4iy8vk9Z5zXVq7t1FtX5dxFFGx+gEnSEXXaI6ukUN1EQU5egZvaI368l6sd6tj3lrySpmDtEfWJ8/7hmWGA==</latexit>e
<latexit sha1_base64=gRKFy+QFytmwqWy0cvo5FmmPz8I=>AAAB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubeaSNXu3x+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvDAVXBvX/XaWlldW19YLG8XNre2d3dLevq9lphjWmRRSNUOqUfAE64Ybgc1UIY1DgY1wcDX2G4+oNJfJrRmmGMS0l/CIM2qs5LfTPr/DTqnsVtwJyCLxZqRcPfSf7gGg1il9tbuSZTEmhgmqdctzUxPkVBnOBI6K7UxjStmA9rBlaUJj1EE+uXZETqzSJZFUthJDJurviZzGWg/j0HbG1PT1vDcW//NamYkugpwnaWYwYdNFUSaIkWT8OulyhcyIoSWUKW5vJaxPFWXGBlS0IXjzLy8S/6ziuRXvxqZxCVMU4AiO4RQ8OIcqXEMN6sDgAZ7hFd4c6bw4787HtHXJmc0cwB84nz+QDpCP</latexit><latexit sha1_base64=74MJShuZzxGyM2nLY3EY87InuhI=>AAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA==</latexit><latexit sha1_base64=74MJShuZzxGyM2nLY3EY87InuhI=>AAAB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHHdL6ewtLyyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VuueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRuu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg00VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiiiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA==</latexit><latexit sha1_base64=pLq6KB/1S9uyUeWp/G4byg43mK0=>AAAB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmmGDKaF0O6IGBZfYsNwKbKcaaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/AF75Ypf9ecgqyTISQVy1Hvlr25fsSxBaZmgxnQCP7XhhGrLmcBpqZsZTCkb0QF2HJU0QRNO5tdOyZlT+iRW2pW0ZK7+npjQxJhxErnOhNqhWfZm4n9eJ7PxVTjhMs0sSrZYFGeCWEVmr5M+18isGDtCmebuVsKGVFNmXUAlF0Kw/PIqaV5UA78a3PmV2nUeRxFO4BTOIYBLqMEt1KEBDB7hGV7hzVPei/fufSxaC14+cwx/4H3+AIWDjww=</latexit><latexit sha1_base64=m8MJ1M94ujO0d0COo5n2Dsol6rc=>AAAB53icbVC7SgNBFL0bXzG+opY2g0GwCrs2phAM2FhGMA9IFpmdvZsMmZ1dZmaFsKS0sbFQxNZP8Rfs/AZ/wsmj0MQDFw7nnMt9BKng2rjul1NYWV1b3yhulra2d3b3yvsHLZ1kimGTJSJRnYBqFFxi03AjsJMqpHEgsB0MryZ++x6V5om8NaMU/Zj2JY84o8ZKjbtyxa26U5Bl4s1J5fLj+4EAgM1/9sKEZTFKwwTVuuu5qfFzqgxnAselXqYxpWxI+9i1VNIYtZ9P9xyTE6uEJEqULWnIVP3dkdNY61Ec2GRMzUAvehPxP6+bmajm51ymmUHJZoOiTBCTkMnRJOQKmREjSyhT3O5K2IAqyox9Tck+wVs8eZm0zqqeW/Vu3Er9AmYowhEcwyl4cA51uIYGNIFBCI/wDC8Od56cV+dtFi04855D+APn/Qd/sI8A</latexit><latexit sha1_base64=q1zM5ZsCNMJAtNUZI97B98ATlaA=>AAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo+1Si4xJrhRmAzUUgjX2DD71+O/MYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL96+7g7fqt81/tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhY/V3R0YjrQeRb5MRNT09643E/7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1+TtE7zZk+dJ/aTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvk+iCM+3Zgz9wXn8AGkeQ8w==</latexit><latexit sha1_base64=q1zM5ZsCNMJAtNUZI97B98ATlaA=>AAAB53icbVC7SgNBFL3rM8ZX1FKQwSBYhV0bLQQDNpYJmAckQWZn7yZDZmeXmVkhLCltbCwUsfUvbP0FO79BP8LJo9DEAxcO55zLffiJ4Nq47qezsLi0vLKaW8uvb2xubRd2dus6ThXDGotFrJo+1Si4xJrhRmAzUUgjX2DD71+O/MYtKs1jeW0GCXYi2pU85IwaK1VuCkW35I5B5ok3JcWL96+7g7fqt81/tIOYpRFKwwTVuuW5ielkVBnOBA7z7VRjQlmfdrFlqaQR6k423nNIjqwSkDBWtqQhY/V3R0YjrQeRb5MRNT09643E/7xWasKzTsZlkhqUbDIoTAUxMRkdTQKukBkxsIQyxe2uhPWooszY1+TtE7zZk+dJ/aTkuSWv6hbL5zBBDvbhEI7Bg1MowxVUoAYMAriHR3hyuPPgPDsvk+iCM+3Zgz9wXn8AGkeQ8w==</latexit><latexit sha1_base64=ioxb3woZF1oAlTScqds23PrgiMY=>AAAB53icbVC7SgNBFL3rM8ZX1NJmMAhWYdZGC4uAjWUE84BkkdnZ2WTI7Owyc1cIS37AxkIRW3/Jzr9xkmyhiQcGDuecy9x7wkxJi5R+e2vrG5tb25Wd6u7e/sFh7ei4Y9PccNHmqUpNL2RWKKlFGyUq0cuMYEmoRDcc38787pMwVqb6ASeZCBI21DKWnKGTWo+1Om3QOcgq8UtShxIu/zWIUp4nQiNXzNq+TzMMCmZQciWm1UFuRcb4mA1F31HNEmGDYr7nlJw7JSJxatzTSObq74mCJdZOktAlE4Yju+zNxP+8fo7xdVBIneUoNF98FOeKYEpmR5NIGsFRTRxh3Ei3K+EjZhhHV03VleAvn7xKOpcNnzb8e1pv3pR1VOAUzuACfLiCJtxBC9rAIYJneIU3T3ov3rv3sYiueeXMCfyB9/kDCGmMcA==</latexit>v
<latexit sha1_base64=HCiXjOq04H3f4Ed7vqyiRfd+2dI=>AAAB7XicbVA9SwNBEJ2LXzF+nQo2NotBsAp3NlpYBGwsI5hLIDnj3mYvWbO3e+zuRcKR/2BjoYit/8fOf+Pmo9DEBwOP92aYmRelnGnjed9OYWV1bX2juFna2t7Z3XP3DwItM0VonUguVTPCmnImaN0ww2kzVRQnEaeNaHA98RtDqjST4s6MUhomuCdYzAg2VgraaZ/dDztu2at4U6Bl4s9JuXoUPD0AQK3jfrW7kmQJFYZwrHXL91IT5lgZRjgdl9qZpikmA9yjLUsFTqgO8+m1Y3RqlS6KpbIlDJqqvydynGg9SiLbmWDT14veRPzPa2UmvgxzJtLMUEFmi+KMIyPR5HXUZYoSw0eWYKKYvRWRPlaYGBtQyYbgL768TILziu9V/FubxhXMUIRjOIEz8OECqnADNagDgUd4hld4c6Tz4rw7H7PWgjOfOYQ/cD5/AKnSkKA=</latexit><latexit sha1_base64=/voSHBGyFE5xYPVKPYM/GTIarLQ=>AAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQW/NZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMMp+HAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit><latexit sha1_base64=/voSHBGyFE5xYPVKPYM/GTIarLQ=>AAAB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJmNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy733RJJRbTzvy1laXlldWy9sFDe3tnd2S3v7gRapwqSOBROqGSFNGOWkbqhhpCkVQUnESCMaXI39xpAoTQW/NZkkYYJ6nMYUI2OloC379G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6Za4Q7ft3tUkWwYZklCCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMMp+HAOVbiGGtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit><latexit sha1_base64=Fc8T4ygtia14k1z/CDji4ezWDqY=>AAAB7XicbVA9SwNBEJ3zM8avqKXNYhCswp2NFhYBG8sI5gOSM+xtNsmavd1jdy4QjvwHGwtFbP0/dv4bN8kVmvhg4PHeDDPzokQKi77/7a2tb2xubRd2irt7+weHpaPjhtWpYbzOtNSmFVHLpVC8jgIlbyWG0ziSvBmNbmd+c8yNFVo94CThYUwHSvQFo+ikRicZisdxt1T2K/4cZJUEOSlDjlq39NXpaZbGXCGT1Np24CcYZtSgYJJPi53U8oSyER3wtqOKxtyG2fzaKTl3So/0tXGlkMzV3xMZja2dxJHrjCkO7bI3E//z2in2r8NMqCRFrthiUT+VBDWZvU56wnCGcuIIZUa4WwkbUkMZuoCKLoRg+eVV0risBH4luPfL1Zs8jgKcwhlcQABXUIU7qEEdGDzBM7zCm6e9F+/d+1i0rnn5zAn8gff5A59Hjx0=</latexit>V0
<latexit sha1_base64=gAQ7qdt3IKvK5oBqK3uN1PHYi1k=>AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit><latexit sha1_base64=gAQ7qdt3IKvK5oBqK3uN1PHYi1k=>AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit><latexit sha1_base64=gAQ7qdt3IKvK5oBqK3uN1PHYi1k=>AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit><latexit sha1_base64=gAQ7qdt3IKvK5oBqK3uN1PHYi1k=>AAAB6XicbVA9SwNBEJ2LXzF+RS1tFoNoFe4koIVFwMYyivmA5Ah7m7lkyd7esbsnhCP/wMZCEVv/kZ3/xk1yhSY+GHi8N8PMvCARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e3Mbz+h0jyWj2aSoB/RoeQhZ9RY6aF13i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5tfOiVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmb1NBlwhM2JiCWWK21sJG1FFmbHhlGwI3vLLq6R1WfXcqndfq9Rv8jiKcAKncAEeXEEd7qABTWAQwjO8wpszdl6cd+dj0Vpw8plj+APn8wcRSI0F</latexit>Figure 5: NLNNs as GNs. A schematic showing how NLNNs (Wang et al., 2018c) are implemented
by theeande!vunder the GN framework. Typically, NLNNs assume that dierent regions of
an image (or words in a sentence) correspond to nodes in a fully connected graph, and the attention
mechanism denes a weighted sum over nodes during the aggregation step.
the readout function, R, plays the role of the GN's u, but does not take uorE0as input,
and thus an analog to the GN's e!uis not required;
dmaster serves a roughly similar purpose to the GN's u, but is dened as an extra node
connected to all others, and thus does not in
uence the edge and global updates directly. It
can then be represented in the GN's V.
Figure 4c shows how an MPNN is structured, according to the GN framework. For details and
various MPNN architectures, see the Appendix.
4.2.2 Non-local neural networks (NLNN)
Wang et al. (2018c)'s NLNN, which unies various intra-/self-/vertex-/graph-attention approaches
(Lin et al., 2017; Vaswani et al., 2017; Hoshen, 2017; Veli ckovi c et al., 2018; Shaw et al., 2018),
can also be translated into the GN formalism. The label attention refers to how the nodes are
updated: each node update is based on a weighted sum of (some function of) the node attributes of
its neighbors, where the weight between a node and one of its neighbors is computed by a scalar
pairwise function between their attributes (and then normalized across neighbors). The published
NLNN formalism does not explicitly include edges, and instead computes pairwise attention weights
between all nodes. But various NLNN-compliant models, such as the vertex attention interaction
network (Hoshen, 2017) and graph attention network (Veli ckovi c et al., 2018), are able to handle
explicit edges by eectively setting to zero the weights between nodes which do not share an edge.
As Figures 4d and 5 illustrate, the eis factored into the scalar pairwise-interaction function
which returns the unnormalized attention term, denoted e(vrk;vsk)=a0
k, and a vector-valued
non-pairwise term, denoted e(vsk)=b0
k. In thee!vaggregation, the a0
kterms are normalized
across each receiver's edges, b0
k, and elementwise summed:
e(ek;vrk;vsk;u):=fe(vrk;vsk) = ( e(vrk;vsk); e(vsk)) = (a0
k;b0
k) =e0
k
ve0
i;vi;u:=fv(e0
i)
e!v
E0
i:=1P
fk:rk=iga0
kX
fk:rk=iga0
kb0
k
In the NLNN paper's terminology (see Wang et al. (2018c), pages 2-4):
theirfplays the role of the above ,
17theirgplays the role of the above .
This formulation may be helpful for focusing only on those interactions which are most relevant for
the downstream task, especially when the input entities were a set, from which a graph was formed
by adding all possible edges between them.
Vaswani et al. (2017)'s multi-headed self-attention mechanism adds an interesting feature, where
theeande!vare implemented by a parallel set of functions, whose results are concatenated
together as the nal step of e!v. This can be interpreted as using typed edges, where the dierent
types index into dierent ecomponent functions, analogous to Li et al. (2016).
For details and various NLNN architectures, see the Appendix.
4.2.3 Other graph network variants
The full GN (Equation 2) can be used to predict a full graph, or any subset of (u0;V0;E0), as
outlined in Section 4.1.1. For example, to predict a global property of a graph, V0andE0can just
be ignored. Similarly, if global, node, or edge attributes are unspecied in the inputs, those vectors
can be zero-length, i.e., not taken as explicit input arguments. The same idea applies for other GN
variants which do not use the full set of mapping ( ) and reduction ( ) functions. For instance,
Interaction Networks (Battaglia et al., 2016; Watters et al., 2017) and the Neural Physics Engine
(Chang et al., 2017) use a full GN but for the absence of the global to update the edge properties
(see Appendix for details).
Various models, including CommNet (Sukhbaatar et al., 2016), structure2vec (Dai et al., 2016)
(in the version of (Dai et al., 2017)), and Gated Graph Sequence Neural Networks (Li et al., 2016)
have used a ewhich does not directly compute pairwise interactions, but instead ignore the receiver
node, operating only on the sender node and in some cases an edge attribute. This can be expressed
by implementations of ewith the following signatures, such as:
e(ek;vrk;vsk;u):=fe(vsk)
ore(ek;vrk;vsk;u):=vsk+fe(ek)
ore(ek;vrk;vsk;u):=fe(ek;vsk):
See the Appendix for further details.
Relation Networks (Raposo et al., 2017; Santoro et al., 2017) bypass the node update entirely
and predict the global output from pooled edge information directly (see also Figure 4e),
e(ek;vrk;vsk;u):=fe(vrk;vsk) = NNe([vrk;vsk])
u
 e0; v0;u:=fu
 e0
= NNu
 e0
e!u
E0:= =X
ke0
k
Deep Sets (Zaheer et al., 2017) bypass the edges update completely and predict the global output
from pooled nodes information directly (Figure 4f),
v(ei;vi;u):=fv(vi;u) = NNv([vi;u])
u
 e0; v0;u:=fu
 v0
= NNu
 v0
v!u
V0:= =X
iv0
i
PointNet (Qi et al., 2017) use similar update rule, with a max-aggregation for v!uand a two-step
node update.
18GM
<latexit sha1_base64=vjTCpRgsPEJfhljVzwQb7AFhV5c=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOf+MmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd6+8f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUe+Vv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf+8TmrCSz/jMkkNSjZfFKaCmJhM/yZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuz+v1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5A+mdjYU=</latexit><latexit sha1_base64=vjTCpRgsPEJfhljVzwQb7AFhV5c=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOf+MmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd6+8f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUe+Vv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf+8TmrCSz/jMkkNSjZfFKaCmJhM/yZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuz+v1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5A+mdjYU=</latexit><latexit sha1_base64=vjTCpRgsPEJfhljVzwQb7AFhV5c=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOf+MmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd6+8f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUe+Vv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf+8TmrCSz/jMkkNSjZfFKaCmJhM/yZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuz+v1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5A+mdjYU=</latexit><latexit sha1_base64=vjTCpRgsPEJfhljVzwQb7AFhV5c=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOf+MmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd6+8f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUe+Vv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf+8TmrCSz/jMkkNSjZfFKaCmJhM/yZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuz+v1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5A+mdjYU=</latexit>GN1
<latexit sha1_base64=oAmr7/S238q10w2wEvXkfEGmAr8=>AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIUBcuCi50JRXsA9qhZNJMG5pkxiRTKEO/w40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3md+eUKVZJB/NNKa+wEPJQkawsZLfE9iMlEhv72d9r1+uuFV3DrRKvJxUIEejX/7qDSKSCCoN4VjrrufGxk+xMoxwOiv1Ek1jTMZ4SLuWSiyo9tN56Bk6s8oAhZGyTxo0V39vpFhoPRWBncxC6mUvE//zuokJr/yUyTgxVJLFoTDhyEQoawANmKLE8KklmChmsyIywgoTY3sq2RK85S+vktZF1XOr3sNlpX6d11GEEziFc/CgBnW4gwY0gcATPMMrvDkT58V5dz4WowUn3zmGP3A+fwCg3ZH4</latexit><latexit sha1_base64=oAmr7/S238q10w2wEvXkfEGmAr8=>AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIUBcuCi50JRXsA9qhZNJMG5pkxiRTKEO/w40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3md+eUKVZJB/NNKa+wEPJQkawsZLfE9iMlEhv72d9r1+uuFV3DrRKvJxUIEejX/7qDSKSCCoN4VjrrufGxk+xMoxwOiv1Ek1jTMZ4SLuWSiyo9tN56Bk6s8oAhZGyTxo0V39vpFhoPRWBncxC6mUvE//zuokJr/yUyTgxVJLFoTDhyEQoawANmKLE8KklmChmsyIywgoTY3sq2RK85S+vktZF1XOr3sNlpX6d11GEEziFc/CgBnW4gwY0gcATPMMrvDkT58V5dz4WowUn3zmGP3A+fwCg3ZH4</latexit><latexit sha1_base64=oAmr7/S238q10w2wEvXkfEGmAr8=>AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIUBcuCi50JRXsA9qhZNJMG5pkxiRTKEO/w40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3md+eUKVZJB/NNKa+wEPJQkawsZLfE9iMlEhv72d9r1+uuFV3DrRKvJxUIEejX/7qDSKSCCoN4VjrrufGxk+xMoxwOiv1Ek1jTMZ4SLuWSiyo9tN56Bk6s8oAhZGyTxo0V39vpFhoPRWBncxC6mUvE//zuokJr/yUyTgxVJLFoTDhyEQoawANmKLE8KklmChmsyIywgoTY3sq2RK85S+vktZF1XOr3sNlpX6d11GEEziFc/CgBnW4gwY0gcATPMMrvDkT58V5dz4WowUn3zmGP3A+fwCg3ZH4</latexit><latexit sha1_base64=oAmr7/S238q10w2wEvXkfEGmAr8=>AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIUBcuCi50JRXsA9qhZNJMG5pkxiRTKEO/w40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3md+eUKVZJB/NNKa+wEPJQkawsZLfE9iMlEhv72d9r1+uuFV3DrRKvJxUIEejX/7qDSKSCCoN4VjrrufGxk+xMoxwOiv1Ek1jTMZ4SLuWSiyo9tN56Bk6s8oAhZGyTxo0V39vpFhoPRWBncxC6mUvE//zuokJr/yUyTgxVJLFoTDhyEQoawANmKLE8KklmChmsyIywgoTY3sq2RK85S+vktZF1XOr3sNlpX6d11GEEziFc/CgBnW4gwY0gcATPMMrvDkT58V5dz4WowUn3zmGP3A+fwCg3ZH4</latexit>GN2
<latexit sha1_base64=pet508CCa1uIZM8cv8xqNGylB9w=>AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqswUQRcuCi50JRXsA9qhZNJMG5pkxiRTKEO/w40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3md+eUKVZJB/NNKa+wEPJQkawsZLfE9iMlEhv72f9Wr9ccavuHGiVeDmpQI5Gv/zVG0QkEVQawrHWXc+NjZ9iZRjhdFbqJZrGmIzxkHYtlVhQ7afz0DN0ZpUBCiNlnzRorv7eSLHQeioCO5mF1MteJv7ndRMTXvkpk3FiqCSLQ2HCkYlQ1gAaMEWJ4VNLMFHMZkVkhBUmxvZUsiV4y19eJa1a1XOr3sNFpX6d11GEEziFc/DgEupwBw1oAoEneIZXeHMmzovz7nwsRgtOvnMMf+B8/gCiYZH5</latexit><latexit sha1_base64=pet508CCa1uIZM8cv8xqNGylB9w=>AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqswUQRcuCi50JRXsA9qhZNJMG5pkxiRTKEO/w40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3md+eUKVZJB/NNKa+wEPJQkawsZLfE9iMlEhv72f9Wr9ccavuHGiVeDmpQI5Gv/zVG0QkEVQawrHWXc+NjZ9iZRjhdFbqJZrGmIzxkHYtlVhQ7afz0DN0ZpUBCiNlnzRorv7eSLHQeioCO5mF1MteJv7ndRMTXvkpk3FiqCSLQ2HCkYlQ1gAaMEWJ4VNLMFHMZkVkhBUmxvZUsiV4y19eJa1a1XOr3sNFpX6d11GEEziFc/DgEupwBw1oAoEneIZXeHMmzovz7nwsRgtOvnMMf+B8/gCiYZH5</latexit><latexit sha1_base64=pet508CCa1uIZM8cv8xqNGylB9w=>AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqswUQRcuCi50JRXsA9qhZNJMG5pkxiRTKEO/w40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3md+eUKVZJB/NNKa+wEPJQkawsZLfE9iMlEhv72f9Wr9ccavuHGiVeDmpQI5Gv/zVG0QkEVQawrHWXc+NjZ9iZRjhdFbqJZrGmIzxkHYtlVhQ7afz0DN0ZpUBCiNlnzRorv7eSLHQeioCO5mF1MteJv7ndRMTXvkpk3FiqCSLQ2HCkYlQ1gAaMEWJ4VNLMFHMZkVkhBUmxvZUsiV4y19eJa1a1XOr3sNFpX6d11GEEziFc/DgEupwBw1oAoEneIZXeHMmzovz7nwsRgtOvnMMf+B8/gCiYZH5</latexit><latexit sha1_base64=pet508CCa1uIZM8cv8xqNGylB9w=>AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqswUQRcuCi50JRXsA9qhZNJMG5pkxiRTKEO/w40LRdz6Me78GzPtLLT1QOBwzr3ckxPEnGnjut9OYW19Y3OruF3a2d3bPygfHrV0lChCmyTikeoEWFPOJG0aZjjtxIpiEXDaDsY3md+eUKVZJB/NNKa+wEPJQkawsZLfE9iMlEhv72f9Wr9ccavuHGiVeDmpQI5Gv/zVG0QkEVQawrHWXc+NjZ9iZRjhdFbqJZrGmIzxkHYtlVhQ7afz0DN0ZpUBCiNlnzRorv7eSLHQeioCO5mF1MteJv7ndRMTXvkpk3FiqCSLQ2HCkYlQ1gAaMEWJ4VNLMFHMZkVkhBUmxvZUsiV4y19eJa1a1XOr3sNFpX6d11GEEziFc/DgEupwBw1oAoEneIZXeHMmzovz7nwsRgtOvnMMf+B8/gCiYZH5</latexit>GNM
<latexit sha1_base64=1uUQuLnXmq2FrQq5fvsHBbzm7v8=>AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoAsXBRe6USrYB7RDyaSZNjTJjEmmUIZ+hxsXirj1Y9z5N2baWWjrgcDhnHu5JyeIOdPGdb+dwsrq2vpGcbO0tb2zu1feP2jqKFGENkjEI9UOsKacSdowzHDajhXFIuC0FYyuM781pkqzSD6aSUx9gQeShYxgYyW/K7AZKpHe3E97d71yxa26M6Bl4uWkAjnqvfJXtx+RRFBpCMdadzw3Nn6KlWGE02mpm2gaYzLCA9qxVGJBtZ/OQk/RiVX6KIyUfdKgmfp7I8VC64kI7GQWUi96mfif10lMeOmnTMaJoZLMD4UJRyZCWQOozxQlhk8swUQxmxWRIVaYGNtTyZbgLX55mTTPqp5b9R7OK7WrvI4iHMExnIIHF1CDW6hDAwg8wTO8wpszdl6cd+djPlpw8p1D+APn8wfLTZIU</latexit><latexit sha1_base64=1uUQuLnXmq2FrQq5fvsHBbzm7v8=>AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoAsXBRe6USrYB7RDyaSZNjTJjEmmUIZ+hxsXirj1Y9z5N2baWWjrgcDhnHu5JyeIOdPGdb+dwsrq2vpGcbO0tb2zu1feP2jqKFGENkjEI9UOsKacSdowzHDajhXFIuC0FYyuM781pkqzSD6aSUx9gQeShYxgYyW/K7AZKpHe3E97d71yxa26M6Bl4uWkAjnqvfJXtx+RRFBpCMdadzw3Nn6KlWGE02mpm2gaYzLCA9qxVGJBtZ/OQk/RiVX6KIyUfdKgmfp7I8VC64kI7GQWUi96mfif10lMeOmnTMaJoZLMD4UJRyZCWQOozxQlhk8swUQxmxWRIVaYGNtTyZbgLX55mTTPqp5b9R7OK7WrvI4iHMExnIIHF1CDW6hDAwg8wTO8wpszdl6cd+djPlpw8p1D+APn8wfLTZIU</latexit><latexit sha1_base64=1uUQuLnXmq2FrQq5fvsHBbzm7v8=>AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoAsXBRe6USrYB7RDyaSZNjTJjEmmUIZ+hxsXirj1Y9z5N2baWWjrgcDhnHu5JyeIOdPGdb+dwsrq2vpGcbO0tb2zu1feP2jqKFGENkjEI9UOsKacSdowzHDajhXFIuC0FYyuM781pkqzSD6aSUx9gQeShYxgYyW/K7AZKpHe3E97d71yxa26M6Bl4uWkAjnqvfJXtx+RRFBpCMdadzw3Nn6KlWGE02mpm2gaYzLCA9qxVGJBtZ/OQk/RiVX6KIyUfdKgmfp7I8VC64kI7GQWUi96mfif10lMeOmnTMaJoZLMD4UJRyZCWQOozxQlhk8swUQxmxWRIVaYGNtTyZbgLX55mTTPqp5b9R7OK7WrvI4iHMExnIIHF1CDW6hDAwg8wTO8wpszdl6cd+djPlpw8p1D+APn8wfLTZIU</latexit><latexit sha1_base64=1uUQuLnXmq2FrQq5fvsHBbzm7v8=>AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoAsXBRe6USrYB7RDyaSZNjTJjEmmUIZ+hxsXirj1Y9z5N2baWWjrgcDhnHu5JyeIOdPGdb+dwsrq2vpGcbO0tb2zu1feP2jqKFGENkjEI9UOsKacSdowzHDajhXFIuC0FYyuM781pkqzSD6aSUx9gQeShYxgYyW/K7AZKpHe3E97d71yxa26M6Bl4uWkAjnqvfJXtx+RRFBpCMdadzw3Nn6KlWGE02mpm2gaYzLCA9qxVGJBtZ/OQk/RiVX6KIyUfdKgmfp7I8VC64kI7GQWUi96mfif10lMeOmnTMaJoZLMD4UJRyZCWQOozxQlhk8swUQxmxWRIVaYGNtTyZbgLX55mTTPqp5b9R7OK7WrvI4iHMExnIIHF1CDW6hDAwg8wTO8wpszdl6cd+djPlpw8p1D+APn8wfLTZIU</latexit>...<latexit sha1_base64=Gj7yv98SlyD93Ghofp+NnyXvd2c=>AAAB7HicbVBNS8NAFHypX7V+VT16WSyCp5KIUA8eCl48VjBtoQ1ls920SzebsPsilNDf4MWDIl79Qd78N27bHLR1YGGYecO+N2EqhUHX/XZKG5tb2zvl3cre/sHhUfX4pG2STDPus0QmuhtSw6VQ3EeBkndTzWkcSt4JJ3dzv/PEtRGJesRpyoOYjpSIBKNoJb8/TNAMqjW37i5A1olXkBoUaA2qXzbHspgrZJIa0/PcFIOcahRM8lmlnxmeUjahI96zVNGYmyBfLDsjF1YZkijR9ikkC/V3IqexMdM4tJMxxbFZ9ebif14vw+gmyIVKM+SKLT+KMkkwIfPLyVBozlBOLaFMC7srYWOqKUPbT8WW4K2evE7aV3XPrXsP17XmbVFHGc7gHC7BgwY04R5a4AMDAc/wCm+Ocl6cd+djOVpyiswp/IHz+QPvmo68</latexit><latexit sha1_base64=Gj7yv98SlyD93Ghofp+NnyXvd2c=>AAAB7HicbVBNS8NAFHypX7V+VT16WSyCp5KIUA8eCl48VjBtoQ1ls920SzebsPsilNDf4MWDIl79Qd78N27bHLR1YGGYecO+N2EqhUHX/XZKG5tb2zvl3cre/sHhUfX4pG2STDPus0QmuhtSw6VQ3EeBkndTzWkcSt4JJ3dzv/PEtRGJesRpyoOYjpSIBKNoJb8/TNAMqjW37i5A1olXkBoUaA2qXzbHspgrZJIa0/PcFIOcahRM8lmlnxmeUjahI96zVNGYmyBfLDsjF1YZkijR9ikkC/V3IqexMdM4tJMxxbFZ9ebif14vw+gmyIVKM+SKLT+KMkkwIfPLyVBozlBOLaFMC7srYWOqKUPbT8WW4K2evE7aV3XPrXsP17XmbVFHGc7gHC7BgwY04R5a4AMDAc/wCm+Ocl6cd+djOVpyiswp/IHz+QPvmo68</latexit><latexit sha1_base64=Gj7yv98SlyD93Ghofp+NnyXvd2c=>AAAB7HicbVBNS8NAFHypX7V+VT16WSyCp5KIUA8eCl48VjBtoQ1ls920SzebsPsilNDf4MWDIl79Qd78N27bHLR1YGGYecO+N2EqhUHX/XZKG5tb2zvl3cre/sHhUfX4pG2STDPus0QmuhtSw6VQ3EeBkndTzWkcSt4JJ3dzv/PEtRGJesRpyoOYjpSIBKNoJb8/TNAMqjW37i5A1olXkBoUaA2qXzbHspgrZJIa0/PcFIOcahRM8lmlnxmeUjahI96zVNGYmyBfLDsjF1YZkijR9ikkC/V3IqexMdM4tJMxxbFZ9ebif14vw+gmyIVKM+SKLT+KMkkwIfPLyVBozlBOLaFMC7srYWOqKUPbT8WW4K2evE7aV3XPrXsP17XmbVFHGc7gHC7BgwY04R5a4AMDAc/wCm+Ocl6cd+djOVpyiswp/IHz+QPvmo68</latexit><latexit sha1_base64=Gj7yv98SlyD93Ghofp+NnyXvd2c=>AAAB7HicbVBNS8NAFHypX7V+VT16WSyCp5KIUA8eCl48VjBtoQ1ls920SzebsPsilNDf4MWDIl79Qd78N27bHLR1YGGYecO+N2EqhUHX/XZKG5tb2zvl3cre/sHhUfX4pG2STDPus0QmuhtSw6VQ3EeBkndTzWkcSt4JJ3dzv/PEtRGJesRpyoOYjpSIBKNoJb8/TNAMqjW37i5A1olXkBoUaA2qXzbHspgrZJIa0/PcFIOcahRM8lmlnxmeUjahI96zVNGYmyBfLDsjF1YZkijR9ikkC/V3IqexMdM4tJMxxbFZ9ebif14vw+gmyIVKM+SKLT+KMkkwIfPLyVBozlBOLaFMC7srYWOqKUPbT8WW4K2evE7aV3XPrXsP17XmbVFHGc7gHC7BgwY04R5a4AMDAc/wCm+Ocl6cd+djOVpyiswp/IHz+QPvmo68</latexit>G1
<latexit sha1_base64=YNShseMoKm2HdChKvcjMRmoBu5o=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9r1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvy2NaQ==</latexit><latexit sha1_base64=YNShseMoKm2HdChKvcjMRmoBu5o=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9r1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvy2NaQ==</latexit><latexit sha1_base64=YNShseMoKm2HdChKvcjMRmoBu5o=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9r1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvy2NaQ==</latexit><latexit sha1_base64=YNShseMoKm2HdChKvcjMRmoBu5o=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9r1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvy2NaQ==</latexit>G0
<latexit sha1_base64=vj48jMMQe2f55rU6zb6RZp+K9y4=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9t1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvamNaA==</latexit><latexit sha1_base64=vj48jMMQe2f55rU6zb6RZp+K9y4=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9t1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvamNaA==</latexit><latexit sha1_base64=vj48jMMQe2f55rU6zb6RZp+K9y4=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9t1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvamNaA==</latexit><latexit sha1_base64=vj48jMMQe2f55rU6zb6RZp+K9y4=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9t1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvamNaA==</latexit>GM
<latexit sha1_base64=vjTCpRgsPEJfhljVzwQb7AFhV5c=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOf+MmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd6+8f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUe+Vv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf+8TmrCSz/jMkkNSjZfFKaCmJhM/yZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuz+v1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5A+mdjYU=</latexit><latexit sha1_base64=vjTCpRgsPEJfhljVzwQb7AFhV5c=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOf+MmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd6+8f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUe+Vv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf+8TmrCSz/jMkkNSjZfFKaCmJhM/yZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuz+v1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5A+mdjYU=</latexit><latexit sha1_base64=vjTCpRgsPEJfhljVzwQb7AFhV5c=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOf+MmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd6+8f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUe+Vv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf+8TmrCSz/jMkkNSjZfFKaCmJhM/yZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuz+v1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5A+mdjYU=</latexit><latexit sha1_base64=vjTCpRgsPEJfhljVzwQb7AFhV5c=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYKGNENF8QHKEvc1csmRv79jdE8KRn2BjoYitv8jOf+MmuUITHww83pthZl6QCK6N6347hZXVtfWN4mZpa3tnd6+8f9DUcaoYNlgsYtUOqEbBJTYMNwLbiUIaBQJbweh66reeUGkey0czTtCP6EDykDNqrPRw07vrlStu1Z2BLBMvJxXIUe+Vv7r9mKURSsME1brjuYnxM6oMZwInpW6qMaFsRAfYsVTSCLWfzU6dkBOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf+8TmrCSz/jMkkNSjZfFKaCmJhM/yZ9rpAZMbaEMsXtrYQNqaLM2HRKNgRv8eVl0jyrem7Vuz+v1K7yOIpwBMdwCh5cQA1uoQ4NYDCAZ3iFN0c4L8678zFvLTj5zCH8gfP5A+mdjYU=</latexit>G0
<latexit sha1_base64=vj48jMMQe2f55rU6zb6RZp+K9y4=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9t1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvamNaA==</latexit><latexit sha1_base64=vj48jMMQe2f55rU6zb6RZp+K9y4=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9t1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvamNaA==</latexit><latexit sha1_base64=vj48jMMQe2f55rU6zb6RZp+K9y4=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9t1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvamNaA==</latexit><latexit sha1_base64=vj48jMMQe2f55rU6zb6RZp+K9y4=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwELLiOYDkiPsbfaSJXt7x+6cEI78BBsLRWz9RXb+GzfJFZr4YODx3gwz84JECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6Z+e0nro2I1SNOEu5HdKhEKBhFKz3c9t1+ueJW3TnIKvFyUoEcjX75qzeIWRpxhUxSY7qem6CfUY2CST4t9VLDE8rGdMi7lioaceNn81On5MwqAxLG2pZCMld/T2Q0MmYSBbYzojgyy95M/M/rphhe+ZlQSYpcscWiMJUEYzL7mwyE5gzlxBLKtLC3EjaimjK06ZRsCN7yy6ukdVH13Kp3f1mpX+dxFOEETuEcPKhBHe6gAU1gMIRneIU3RzovzrvzsWgtOPnMMfyB8/kDvamNaA==</latexit>GNcore
<latexit sha1_base64=sfcetjjriA53KVhP8LRkSGs9KNA=>AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit><latexit sha1_base64=sfcetjjriA53KVhP8LRkSGs9KNA=>AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit><latexit sha1_base64=sfcetjjriA53KVhP8LRkSGs9KNA=>AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit><latexit sha1_base64=sfcetjjriA53KVhP8LRkSGs9KNA=>AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit>M
<latexit sha1_base64=xCEPSgjeJaAOppNxwTZXrwRukIg=>AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit><latexit sha1_base64=xCEPSgjeJaAOppNxwTZXrwRukIg=>AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit><latexit sha1_base64=xCEPSgjeJaAOppNxwTZXrwRukIg=>AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit><latexit sha1_base64=xCEPSgjeJaAOppNxwTZXrwRukIg=>AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit>(a) Composition of GN blocks
GNenc
<latexit sha1_base64=KZY5NxgXVEC/q8QikcRvAbEAxBs=>AAAB+nicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6/cRKz0NYDA4dz7mXOPWHMqNKu+2UtLa+srq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3M/e49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfc+NtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De/2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa9/4wuWeXOPvyB9fENyBqUTg==</latexit><latexit sha1_base64=KZY5NxgXVEC/q8QikcRvAbEAxBs=>AAAB+nicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6/cRKz0NYDA4dz7mXOPWHMqNKu+2UtLa+srq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3M/e49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfc+NtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De/2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa9/4wuWeXOPvyB9fENyBqUTg==</latexit><latexit sha1_base64=KZY5NxgXVEC/q8QikcRvAbEAxBs=>AAAB+nicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6/cRKz0NYDA4dz7mXOPWHMqNKu+2UtLa+srq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3M/e49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfc+NtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De/2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa9/4wuWeXOPvyB9fENyBqUTg==</latexit><latexit sha1_base64=KZY5NxgXVEC/q8QikcRvAbEAxBs=>AAAB+nicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6/cRKz0NYDA4dz7mXOPWHMqNKu+2UtLa+srq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3M/e49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfc+NtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De/2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa9/4wuWeXOPvyB9fENyBqUTg==</latexit>GNdec
<latexit sha1_base64=79QHnx4t/4kSfeuqQfRiz+zDMfI=>AAAB+nicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79Enf+jZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr+10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk+5p03Wa7t1Zo3VZ1lEFh+AInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQB/YH3+ALjdlEQ=</latexit><latexit sha1_base64=79QHnx4t/4kSfeuqQfRiz+zDMfI=>AAAB+nicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79Enf+jZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr+10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk+5p03Wa7t1Zo3VZ1lEFh+AInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQB/YH3+ALjdlEQ=</latexit><latexit sha1_base64=79QHnx4t/4kSfeuqQfRiz+zDMfI=>AAAB+nicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79Enf+jZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr+10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk+5p03Wa7t1Zo3VZ1lEFh+AInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQB/YH3+ALjdlEQ=</latexit><latexit sha1_base64=79QHnx4t/4kSfeuqQfRiz+zDMfI=>AAAB+nicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79Enf+jZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr+10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk+5p03Wa7t1Zo3VZ1lEFh+AInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQB/YH3+ALjdlEQ=</latexit>GNcore
<latexit sha1_base64=sfcetjjriA53KVhP8LRkSGs9KNA=>AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit><latexit sha1_base64=sfcetjjriA53KVhP8LRkSGs9KNA=>AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit><latexit sha1_base64=sfcetjjriA53KVhP8LRkSGs9KNA=>AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit><latexit sha1_base64=sfcetjjriA53KVhP8LRkSGs9KNA=>AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit>M
<latexit sha1_base64=xCEPSgjeJaAOppNxwTZXrwRukIg=>AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit><latexit sha1_base64=xCEPSgjeJaAOppNxwTZXrwRukIg=>AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit><latexit sha1_base64=xCEPSgjeJaAOppNxwTZXrwRukIg=>AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit><latexit sha1_base64=xCEPSgjeJaAOppNxwTZXrwRukIg=>AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit>Gout
<latexit sha1_base64=TKn8tu9S9/KYM9INqkcELhgYcuA=>AAAB+XicbVDLSsNAFJ3UV62vqEs3wSK4KokIunBRcKHLCvYBbQiT6aQdOo8wc1MooX/ixoUibv0Td/6NkzYLrR4YOJxzL/fMiVPODPj+l1NZW9/Y3Kpu13Z29/YP3MOjjlGZJrRNFFe6F2NDOZO0DQw47aWaYhFz2o0nt4XfnVJtmJKPMEtpKPBIsoQRDFaKXPcuygcCw1iLXGUwn0du3W/4C3h/SVCSOirRitzPwVCRTFAJhGNj+oGfQphjDYxwOq8NMkNTTCZ4RPuWSiyoCfNF8rl3ZpWhlyhtnwRvof7cyLEwZiZiO1mENKteIf7n9TNIrsOcyTQDKsnyUJJxD5RX1OANmaYE+MwSTDSzWT0yxhoTsGXVbAnB6pf/ks5FI/AbwcNlvXlT1lFFJ+gUnaMAXaEmukct1EYETdETekGvTu48O2/O+3K04pQ7x+gXnI9vWWaUGA==</latexit><latexit sha1_base64=TKn8tu9S9/KYM9INqkcELhgYcuA=>AAAB+XicbVDLSsNAFJ3UV62vqEs3wSK4KokIunBRcKHLCvYBbQiT6aQdOo8wc1MooX/ixoUibv0Td/6NkzYLrR4YOJxzL/fMiVPODPj+l1NZW9/Y3Kpu13Z29/YP3MOjjlGZJrRNFFe6F2NDOZO0DQw47aWaYhFz2o0nt4XfnVJtmJKPMEtpKPBIsoQRDFaKXPcuygcCw1iLXGUwn0du3W/4C3h/SVCSOirRitzPwVCRTFAJhGNj+oGfQphjDYxwOq8NMkNTTCZ4RPuWSiyoCfNF8rl3ZpWhlyhtnwRvof7cyLEwZiZiO1mENKteIf7n9TNIrsOcyTQDKsnyUJJxD5RX1OANmaYE+MwSTDSzWT0yxhoTsGXVbAnB6pf/ks5FI/AbwcNlvXlT1lFFJ+gUnaMAXaEmukct1EYETdETekGvTu48O2/O+3K04pQ7x+gXnI9vWWaUGA==</latexit><latexit sha1_base64=TKn8tu9S9/KYM9INqkcELhgYcuA=>AAAB+XicbVDLSsNAFJ3UV62vqEs3wSK4KokIunBRcKHLCvYBbQiT6aQdOo8wc1MooX/ixoUibv0Td/6NkzYLrR4YOJxzL/fMiVPODPj+l1NZW9/Y3Kpu13Z29/YP3MOjjlGZJrRNFFe6F2NDOZO0DQw47aWaYhFz2o0nt4XfnVJtmJKPMEtpKPBIsoQRDFaKXPcuygcCw1iLXGUwn0du3W/4C3h/SVCSOirRitzPwVCRTFAJhGNj+oGfQphjDYxwOq8NMkNTTCZ4RPuWSiyoCfNF8rl3ZpWhlyhtnwRvof7cyLEwZiZiO1mENKteIf7n9TNIrsOcyTQDKsnyUJJxD5RX1OANmaYE+MwSTDSzWT0yxhoTsGXVbAnB6pf/ks5FI/AbwcNlvXlT1lFFJ+gUnaMAXaEmukct1EYETdETekGvTu48O2/O+3K04pQ7x+gXnI9vWWaUGA==</latexit><latexit sha1_base64=TKn8tu9S9/KYM9INqkcELhgYcuA=>AAAB+XicbVDLSsNAFJ3UV62vqEs3wSK4KokIunBRcKHLCvYBbQiT6aQdOo8wc1MooX/ixoUibv0Td/6NkzYLrR4YOJxzL/fMiVPODPj+l1NZW9/Y3Kpu13Z29/YP3MOjjlGZJrRNFFe6F2NDOZO0DQw47aWaYhFz2o0nt4XfnVJtmJKPMEtpKPBIsoQRDFaKXPcuygcCw1iLXGUwn0du3W/4C3h/SVCSOirRitzPwVCRTFAJhGNj+oGfQphjDYxwOq8NMkNTTCZ4RPuWSiyoCfNF8rl3ZpWhlyhtnwRvof7cyLEwZiZiO1mENKteIf7n9TNIrsOcyTQDKsnyUJJxD5RX1OANmaYE+MwSTDSzWT0yxhoTsGXVbAnB6pf/ks5FI/AbwcNlvXlT1lFFJ+gUnaMAXaEmukct1EYETdETekGvTu48O2/O+3K04pQ7x+gXnI9vWWaUGA==</latexit>Ginp
<latexit sha1_base64=mvYjY6mgtt6w2Efm7YMP3XaOQ7I=>AAAB+XicbVDLSgMxFL1TX7W+Rl26CRbBVZkRQRcuCi50WcE+oB2GTJppQ5PMkGQKZeifuHGhiFv/xJ1/Y6adhVYPBA7n3EvOPVHKmTae9+VU1tY3Nreq27Wd3b39A/fwqKOTTBHaJglPVC/CmnImadsww2kvVRSLiNNuNLkt/O6UKs0S+WhmKQ0EHkkWM4KNlULXvQvzgcBmrETOZDqfh27da3gLoL/EL0kdSrRC93MwTEgmqDSEY637vpeaIMfKMMLpvDbINE0xmeAR7VsqsaA6yBfJ5+jMKkMUJ8o+adBC/bmRY6H1TER2sgipV71C/M/rZya+DoqLMkMlWX4UZxyZBBU1oCFTlBg+swQTxWxWRMZYYWJsWTVbgr968l/SuWj4XsN/uKw3b8o6qnACp3AOPlxBE+6hBW0gMIUneIFXJ3eenTfnfTlaccqdY/gF5+MbP22UBw==</latexit><latexit sha1_base64=mvYjY6mgtt6w2Efm7YMP3XaOQ7I=>AAAB+XicbVDLSgMxFL1TX7W+Rl26CRbBVZkRQRcuCi50WcE+oB2GTJppQ5PMkGQKZeifuHGhiFv/xJ1/Y6adhVYPBA7n3EvOPVHKmTae9+VU1tY3Nreq27Wd3b39A/fwqKOTTBHaJglPVC/CmnImadsww2kvVRSLiNNuNLkt/O6UKs0S+WhmKQ0EHkkWM4KNlULXvQvzgcBmrETOZDqfh27da3gLoL/EL0kdSrRC93MwTEgmqDSEY637vpeaIMfKMMLpvDbINE0xmeAR7VsqsaA6yBfJ5+jMKkMUJ8o+adBC/bmRY6H1TER2sgipV71C/M/rZya+DoqLMkMlWX4UZxyZBBU1oCFTlBg+swQTxWxWRMZYYWJsWTVbgr968l/SuWj4XsN/uKw3b8o6qnACp3AOPlxBE+6hBW0gMIUneIFXJ3eenTfnfTlaccqdY/gF5+MbP22UBw==</latexit><latexit sha1_base64=mvYjY6mgtt6w2Efm7YMP3XaOQ7I=>AAAB+XicbVDLSgMxFL1TX7W+Rl26CRbBVZkRQRcuCi50WcE+oB2GTJppQ5PMkGQKZeifuHGhiFv/xJ1/Y6adhVYPBA7n3EvOPVHKmTae9+VU1tY3Nreq27Wd3b39A/fwqKOTTBHaJglPVC/CmnImadsww2kvVRSLiNNuNLkt/O6UKs0S+WhmKQ0EHkkWM4KNlULXvQvzgcBmrETOZDqfh27da3gLoL/EL0kdSrRC93MwTEgmqDSEY637vpeaIMfKMMLpvDbINE0xmeAR7VsqsaA6yBfJ5+jMKkMUJ8o+adBC/bmRY6H1TER2sgipV71C/M/rZya+DoqLMkMlWX4UZxyZBBU1oCFTlBg+swQTxWxWRMZYYWJsWTVbgr968l/SuWj4XsN/uKw3b8o6qnACp3AOPlxBE+6hBW0gMIUneIFXJ3eenTfnfTlaccqdY/gF5+MbP22UBw==</latexit><latexit sha1_base64=mvYjY6mgtt6w2Efm7YMP3XaOQ7I=>AAAB+XicbVDLSgMxFL1TX7W+Rl26CRbBVZkRQRcuCi50WcE+oB2GTJppQ5PMkGQKZeifuHGhiFv/xJ1/Y6adhVYPBA7n3EvOPVHKmTae9+VU1tY3Nreq27Wd3b39A/fwqKOTTBHaJglPVC/CmnImadsww2kvVRSLiNNuNLkt/O6UKs0S+WhmKQ0EHkkWM4KNlULXvQvzgcBmrETOZDqfh27da3gLoL/EL0kdSrRC93MwTEgmqDSEY637vpeaIMfKMMLpvDbINE0xmeAR7VsqsaA6yBfJ5+jMKkMUJ8o+adBC/bmRY6H1TER2sgipV71C/M/rZya+DoqLMkMlWX4UZxyZBBU1oCFTlBg+swQTxWxWRMZYYWJsWTVbgr968l/SuWj4XsN/uKw3b8o6qnACp3AOPlxBE+6hBW0gMIUneIFXJ3eenTfnfTlaccqdY/gF5+MbP22UBw==</latexit> (b) Encode-process-decode
GNenc
<latexit sha1_base64=KZY5NxgXVEC/q8QikcRvAbEAxBs=>AAAB+nicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6/cRKz0NYDA4dz7mXOPWHMqNKu+2UtLa+srq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3M/e49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfc+NtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De/2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa9/4wuWeXOPvyB9fENyBqUTg==</latexit><latexit sha1_base64=KZY5NxgXVEC/q8QikcRvAbEAxBs=>AAAB+nicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6/cRKz0NYDA4dz7mXOPWHMqNKu+2UtLa+srq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3M/e49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfc+NtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De/2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa9/4wuWeXOPvyB9fENyBqUTg==</latexit><latexit sha1_base64=KZY5NxgXVEC/q8QikcRvAbEAxBs=>AAAB+nicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6/cRKz0NYDA4dz7mXOPWHMqNKu+2UtLa+srq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3M/e49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfc+NtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De/2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa9/4wuWeXOPvyB9fENyBqUTg==</latexit><latexit sha1_base64=KZY5NxgXVEC/q8QikcRvAbEAxBs=>AAAB+nicbVDLSsNAFL3xWesr1aWbYBFclUQEXbgouNCVVLAPaEOYTKft0JlJmJkoJeZT3LhQxK1f4s6/cRKz0NYDA4dz7mXOPWHMqNKu+2UtLa+srq1XNqqbW9s7u3Ztr6OiRGLSxhGLZC9EijAqSFtTzUgvlgTxkJFuOL3M/e49kYpG4k7PYuJzNBZ0RDHSRgrs2oAjPZE8vbrJgpQInAV23W24BZxF4pWkDiVagf05GEY44URozJBSfc+NtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De/2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa9/4wuWeXOPvyB9fENyBqUTg==</latexit>GNdec
<latexit sha1_base64=79QHnx4t/4kSfeuqQfRiz+zDMfI=>AAAB+nicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79Enf+jZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr+10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk+5p03Wa7t1Zo3VZ1lEFh+AInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQB/YH3+ALjdlEQ=</latexit><latexit sha1_base64=79QHnx4t/4kSfeuqQfRiz+zDMfI=>AAAB+nicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79Enf+jZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr+10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk+5p03Wa7t1Zo3VZ1lEFh+AInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQB/YH3+ALjdlEQ=</latexit><latexit sha1_base64=79QHnx4t/4kSfeuqQfRiz+zDMfI=>AAAB+nicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79Enf+jZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr+10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk+5p03Wa7t1Zo3VZ1lEFh+AInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQB/YH3+ALjdlEQ=</latexit><latexit sha1_base64=79QHnx4t/4kSfeuqQfRiz+zDMfI=>AAAB+nicbVDLSsNAFJ3UV62vVJduBovgqiQi6MJFwYWupIJ9QBvCZDJph85MwsxEKTGf4saFIm79Enf+jZM2C209MHA4517umRMkjCrtON9WZWV1bX2julnb2t7Z3bPr+10VpxKTDo5ZLPsBUoRRQTqaakb6iSSIB4z0gslV4fceiFQ0Fvd6mhCPo5GgEcVIG8m360OO9Fjy7Po297OQ4Ny3G07TmQEuE7ckDVCi7dtfwzDGKSdCY4aUGrhOor0MSU0xI3ltmCqSIDxBIzIwVCBOlJfNoufw2CghjGJpntBwpv7eyBBXasoDM1kEVYteIf7nDVIdXXgZFUmqicDzQ1HKoI5h0QMMqSRYs6khCEtqskI8RhJhbdqqmRLcxS8vk+5p03Wa7t1Zo3VZ1lEFh+AInAAXnIMWuAFt0AEYPIJn8ArerCfrxXq3PuajFavcOQB/YH3+ALjdlEQ=</latexit>GNcore
<latexit sha1_base64=sfcetjjriA53KVhP8LRkSGs9KNA=>AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit><latexit sha1_base64=sfcetjjriA53KVhP8LRkSGs9KNA=>AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit><latexit sha1_base64=sfcetjjriA53KVhP8LRkSGs9KNA=>AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit><latexit sha1_base64=sfcetjjriA53KVhP8LRkSGs9KNA=>AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNCVVLAPaEOYTCft0HmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0oY1cbzvp2V1bX1jc3KVnV7Z3dv3z2odbRMFSZtLJlUvQhpwqggbUMNI71EEcQjRrrR5Lrwu49EaSrFg5kmJOBoJGhMMTJWCt3agCMzVjy7ucvDDEtF8tCtew1vBrhM/JLUQYlW6H4NhhKnnAiDGdK673uJCTKkDMWM5NVBqkmC8ASNSN9SgTjRQTbLnsMTqwxhLJV9wsCZ+nsjQ1zrKY/sZJFUL3qF+J/XT018GWRUJKkhAs8PxSmDRsKiCDikimDDppYgrKjNCvEYKYSNratqS/AXv7xMOmcN32v49+f15lVZRwUcgWNwCnxwAZrgFrRAG2DwBJ7BK3hzcufFeXc+5qMrTrlzCP7A+fwBopiUyw==</latexit>M
<latexit sha1_base64=xCEPSgjeJaAOppNxwTZXrwRukIg=>AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit><latexit sha1_base64=xCEPSgjeJaAOppNxwTZXrwRukIg=>AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit><latexit sha1_base64=xCEPSgjeJaAOppNxwTZXrwRukIg=>AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit><latexit sha1_base64=xCEPSgjeJaAOppNxwTZXrwRukIg=>AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0MIiYGMjRDAfkBxhb7OXLNnbu+zOCeHIn7CxUMTWv2Pnv3GTXKGJDwYe780wMy9IpDDout9OYW19Y3OruF3a2d3bPygfHjVNnGrGGyyWsW4H1HApFG+gQMnbieY0CiRvBaPbmd964tqIWD3iJOF+RAdKhIJRtFK7iyLihtz3yhW36s5BVomXkwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit>Gthid
<latexit sha1_base64=Vr1kZJh7jApVU4cGmtPimK4PPic=>AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIoAsXBRe6rGAf0MYwmUzaoTOTMDMRagj+ihsXirj1P9z5N07aLLT1wMDhnHu5Z06QMKq043xblaXlldW16nptY3Nre8fe3euoOJWYtHHMYtkLkCKMCtLWVDPSSyRBPGCkG4yvCr/7QKSisbjTk4R4HA0FjShG2ki+fXB9n+nczwYc6ZHk2YiGee7bdafhTAEXiVuSOijR8u2vQRjjlBOhMUNK9V0n0V6GpKaYkbw2SBVJEB6jIekbKhAnysum6XN4bJQQRrE0T2g4VX9vZIgrNeGBmSxCqnmvEP/z+qmOLryMiiTVRODZoShlUMewqAKGVBKs2cQQhCU1WSEeIYmwNoXVTAnu/JcXSee04ToN9/as3rws66iCQ3AEToALzkET3IAWaAMMHsEzeAVv1pP1Yr1bH7PRilXu7IM/sD5/AI1Vlec=</latexit><latexit sha1_base64=Vr1kZJh7jApVU4cGmtPimK4PPic=>AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIoAsXBRe6rGAf0MYwmUzaoTOTMDMRagj+ihsXirj1P9z5N07aLLT1wMDhnHu5Z06QMKq043xblaXlldW16nptY3Nre8fe3euoOJWYtHHMYtkLkCKMCtLWVDPSSyRBPGCkG4yvCr/7QKSisbjTk4R4HA0FjShG2ki+fXB9n+nczwYc6ZHk2YiGee7bdafhTAEXiVuSOijR8u2vQRjjlBOhMUNK9V0n0V6GpKaYkbw2SBVJEB6jIekbKhAnysum6XN4bJQQRrE0T2g4VX9vZIgrNeGBmSxCqnmvEP/z+qmOLryMiiTVRODZoShlUMewqAKGVBKs2cQQhCU1WSEeIYmwNoXVTAnu/JcXSee04ToN9/as3rws66iCQ3AEToALzkET3IAWaAMMHsEzeAVv1pP1Yr1bH7PRilXu7IM/sD5/AI1Vlec=</latexit><latexit sha1_base64=Vr1kZJh7jApVU4cGmtPimK4PPic=>AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIoAsXBRe6rGAf0MYwmUzaoTOTMDMRagj+ihsXirj1P9z5N07aLLT1wMDhnHu5Z06QMKq043xblaXlldW16nptY3Nre8fe3euoOJWYtHHMYtkLkCKMCtLWVDPSSyRBPGCkG4yvCr/7QKSisbjTk4R4HA0FjShG2ki+fXB9n+nczwYc6ZHk2YiGee7bdafhTAEXiVuSOijR8u2vQRjjlBOhMUNK9V0n0V6GpKaYkbw2SBVJEB6jIekbKhAnysum6XN4bJQQRrE0T2g4VX9vZIgrNeGBmSxCqnmvEP/z+qmOLryMiiTVRODZoShlUMewqAKGVBKs2cQQhCU1WSEeIYmwNoXVTAnu/JcXSee04ToN9/as3rws66iCQ3AEToALzkET3IAWaAMMHsEzeAVv1pP1Yr1bH7PRilXu7IM/sD5/AI1Vlec=</latexit><latexit sha1_base64=Vr1kZJh7jApVU4cGmtPimK4PPic=>AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIoAsXBRe6rGAf0MYwmUzaoTOTMDMRagj+ihsXirj1P9z5N07aLLT1wMDhnHu5Z06QMKq043xblaXlldW16nptY3Nre8fe3euoOJWYtHHMYtkLkCKMCtLWVDPSSyRBPGCkG4yvCr/7QKSisbjTk4R4HA0FjShG2ki+fXB9n+nczwYc6ZHk2YiGee7bdafhTAEXiVuSOijR8u2vQRjjlBOhMUNK9V0n0V6GpKaYkbw2SBVJEB6jIekbKhAnysum6XN4bJQQRrE0T2g4VX9vZIgrNeGBmSxCqnmvEP/z+qmOLryMiiTVRODZoShlUMewqAKGVBKs2cQQhCU1WSEeIYmwNoXVTAnu/JcXSee04ToN9/as3rws66iCQ3AEToALzkET3IAWaAMMHsEzeAVv1pP1Yr1bH7PRilXu7IM/sD5/AI1Vlec=</latexit>Gt1hid
<latexit sha1_base64=TTYaLKTJF7FnWkmBKBRepTsjYHc=>AAAB/3icbVDLSsNAFJ34rPUVFdy4GSyCG0sigi5cFFzosoJ9QBvDZDJph85MwsxEKDELf8WNC0Xc+hvu/BsnbRbaemDgcM693DMnSBhV2nG+rYXFpeWV1cpadX1jc2vb3tltqziVmLRwzGLZDZAijArS0lQz0k0kQTxgpBOMrgq/80CkorG40+OEeBwNBI0oRtpIvr1/fZ/pEzf3sz5Heih5NqRhnvt2zak7E8B54pakBko0ffurH8Y45URozJBSPddJtJchqSlmJK/2U0UShEdoQHqGCsSJ8rJJ/hweGSWEUSzNExpO1N8bGeJKjXlgJouQatYrxP+8XqqjCy+jIkk1EXh6KEoZ1DEsyoAhlQRrNjYEYUlNVoiHSCKsTWVVU4I7++V50j6tu07dvT2rNS7LOirgAByCY+CCc9AAN6AJWgCDR/AMXsGb9WS9WO/Wx3R0wSp39sAfWJ8/b2WWWQ==</latexit><latexit sha1_base64=TTYaLKTJF7FnWkmBKBRepTsjYHc=>AAAB/3icbVDLSsNAFJ34rPUVFdy4GSyCG0sigi5cFFzosoJ9QBvDZDJph85MwsxEKDELf8WNC0Xc+hvu/BsnbRbaemDgcM693DMnSBhV2nG+rYXFpeWV1cpadX1jc2vb3tltqziVmLRwzGLZDZAijArS0lQz0k0kQTxgpBOMrgq/80CkorG40+OEeBwNBI0oRtpIvr1/fZ/pEzf3sz5Heih5NqRhnvt2zak7E8B54pakBko0ffurH8Y45URozJBSPddJtJchqSlmJK/2U0UShEdoQHqGCsSJ8rJJ/hweGSWEUSzNExpO1N8bGeJKjXlgJouQatYrxP+8XqqjCy+jIkk1EXh6KEoZ1DEsyoAhlQRrNjYEYUlNVoiHSCKsTWVVU4I7++V50j6tu07dvT2rNS7LOirgAByCY+CCc9AAN6AJWgCDR/AMXsGb9WS9WO/Wx3R0wSp39sAfWJ8/b2WWWQ==</latexit><latexit sha1_base64=TTYaLKTJF7FnWkmBKBRepTsjYHc=>AAAB/3icbVDLSsNAFJ34rPUVFdy4GSyCG0sigi5cFFzosoJ9QBvDZDJph85MwsxEKDELf8WNC0Xc+hvu/BsnbRbaemDgcM693DMnSBhV2nG+rYXFpeWV1cpadX1jc2vb3tltqziVmLRwzGLZDZAijArS0lQz0k0kQTxgpBOMrgq/80CkorG40+OEeBwNBI0oRtpIvr1/fZ/pEzf3sz5Heih5NqRhnvt2zak7E8B54pakBko0ffurH8Y45URozJBSPddJtJchqSlmJK/2U0UShEdoQHqGCsSJ8rJJ/hweGSWEUSzNExpO1N8bGeJKjXlgJouQatYrxP+8XqqjCy+jIkk1EXh6KEoZ1DEsyoAhlQRrNjYEYUlNVoiHSCKsTWVVU4I7++V50j6tu07dvT2rNS7LOirgAByCY+CCc9AAN6AJWgCDR/AMXsGb9WS9WO/Wx3R0wSp39sAfWJ8/b2WWWQ==</latexit><latexit sha1_base64=TTYaLKTJF7FnWkmBKBRepTsjYHc=>AAAB/3icbVDLSsNAFJ34rPUVFdy4GSyCG0sigi5cFFzosoJ9QBvDZDJph85MwsxEKDELf8WNC0Xc+hvu/BsnbRbaemDgcM693DMnSBhV2nG+rYXFpeWV1cpadX1jc2vb3tltqziVmLRwzGLZDZAijArS0lQz0k0kQTxgpBOMrgq/80CkorG40+OEeBwNBI0oRtpIvr1/fZ/pEzf3sz5Heih5NqRhnvt2zak7E8B54pakBko0ffurH8Y45URozJBSPddJtJchqSlmJK/2U0UShEdoQHqGCsSJ8rJJ/hweGSWEUSzNExpO1N8bGeJKjXlgJouQatYrxP+8XqqjCy+jIkk1EXh6KEoZ1DEsyoAhlQRrNjYEYUlNVoiHSCKsTWVVU4I7++V50j6tu07dvT2rNS7LOirgAByCY+CCc9AAN6AJWgCDR/AMXsGb9WS9WO/Wx3R0wSp39sAfWJ8/b2WWWQ==</latexit>Gtout
<latexit sha1_base64=Fm4Iz64LvWn082kf2+Dq6sd1g9I=>AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNBlBfuANobJdNIOnWTCzI1YQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0gE1+A439bK6tr6xmZlq7q9s7u3bx/UOlqmirI2lUKqXkA0EzxmbeAgWC9RjESBYN1gcl343UemNJfxPUwT5kVkFPOQUwJG8u3azQP42SAiMFZRJlPIc9+uOw1nBrxM3JLUUYmWb38NhpKmEYuBCqJ133US8DKigFPB8uog1SwhdEJGrG9oTCKmvWyWPccnRhniUCrzYsAz9fdGRiKtp1FgJouQetErxP+8fgrhpZfxOEmBxXR+KEwFBomLIvCQK0ZBTA0hVHGTFdMxUYSCqatqSnAXv7xMOmcN12m4d+f15lVZRwUdoWN0ilx0gZroFrVQG1H0hJ7RK3qzcuvFerc+5qMrVrlziP7A+vwB7zuU/g==</latexit><latexit sha1_base64=Fm4Iz64LvWn082kf2+Dq6sd1g9I=>AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNBlBfuANobJdNIOnWTCzI1YQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0gE1+A439bK6tr6xmZlq7q9s7u3bx/UOlqmirI2lUKqXkA0EzxmbeAgWC9RjESBYN1gcl343UemNJfxPUwT5kVkFPOQUwJG8u3azQP42SAiMFZRJlPIc9+uOw1nBrxM3JLUUYmWb38NhpKmEYuBCqJ133US8DKigFPB8uog1SwhdEJGrG9oTCKmvWyWPccnRhniUCrzYsAz9fdGRiKtp1FgJouQetErxP+8fgrhpZfxOEmBxXR+KEwFBomLIvCQK0ZBTA0hVHGTFdMxUYSCqatqSnAXv7xMOmcN12m4d+f15lVZRwUdoWN0ilx0gZroFrVQG1H0hJ7RK3qzcuvFerc+5qMrVrlziP7A+vwB7zuU/g==</latexit><latexit sha1_base64=Fm4Iz64LvWn082kf2+Dq6sd1g9I=>AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNBlBfuANobJdNIOnWTCzI1YQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0gE1+A439bK6tr6xmZlq7q9s7u3bx/UOlqmirI2lUKqXkA0EzxmbeAgWC9RjESBYN1gcl343UemNJfxPUwT5kVkFPOQUwJG8u3azQP42SAiMFZRJlPIc9+uOw1nBrxM3JLUUYmWb38NhpKmEYuBCqJ133US8DKigFPB8uog1SwhdEJGrG9oTCKmvWyWPccnRhniUCrzYsAz9fdGRiKtp1FgJouQetErxP+8fgrhpZfxOEmBxXR+KEwFBomLIvCQK0ZBTA0hVHGTFdMxUYSCqatqSnAXv7xMOmcN12m4d+f15lVZRwUdoWN0ilx0gZroFrVQG1H0hJ7RK3qzcuvFerc+5qMrVrlziP7A+vwB7zuU/g==</latexit><latexit sha1_base64=Fm4Iz64LvWn082kf2+Dq6sd1g9I=>AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXbgouNBlBfuANobJdNIOnWTCzI1YQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzJ0gE1+A439bK6tr6xmZlq7q9s7u3bx/UOlqmirI2lUKqXkA0EzxmbeAgWC9RjESBYN1gcl343UemNJfxPUwT5kVkFPOQUwJG8u3azQP42SAiMFZRJlPIc9+uOw1nBrxM3JLUUYmWb38NhpKmEYuBCqJ133US8DKigFPB8uog1SwhdEJGrG9oTCKmvWyWPccnRhniUCrzYsAz9fdGRiKtp1FgJouQetErxP+8fgrhpZfxOEmBxXR+KEwFBomLIvCQK0ZBTA0hVHGTFdMxUYSCqatqSnAXv7xMOmcN12m4d+f15lVZRwUdoWN0ilx0gZroFrVQG1H0hJ7RK3qzcuvFerc+5qMrVrlziP7A+vwB7zuU/g==</latexit>Gtinp
<latexit sha1_base64=KNSYXpe+JERsrbP/+QxnbJ2F1T0=>AAAB+3icbVDLSsNAFL3xWesr1qWbYBFclUQEXbgouNBlBfuANobJdNIOnZmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvcy5J0wYVdp1v62V1bX1jc3KVnV7Z3dv3z6odVScSkzaOGax7IVIEUYFaWuqGeklkiAeMtINJ9eF330kUtFY3OtpQnyORoJGFCNtpMCu3QTZgCM9ljyjIsnzBx3YdbfhzuAsE68kdSjRCuyvwTDGKSdCY4aU6ntuov0MSU0xI3l1kCqSIDxBI9I3VCBOlJ/NsufOiVGGThRL84R2ZurvjQxxpaY8NJNFTLXoFeJ/Xj/V0aVf3JRqIvD8oyhljo6doghnSCXBmk0NQVhSk9XBYyQR1qauqinBWzx5mXTOGp7b8O7O682rso4KHMExnIIHF9CEW2hBGzA8wTO8wpuVWy/Wu/UxH12xyp1D+APr8wfV6pTt</latexit><latexit sha1_base64=KNSYXpe+JERsrbP/+QxnbJ2F1T0=>AAAB+3icbVDLSsNAFL3xWesr1qWbYBFclUQEXbgouNBlBfuANobJdNIOnZmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvcy5J0wYVdp1v62V1bX1jc3KVnV7Z3dv3z6odVScSkzaOGax7IVIEUYFaWuqGeklkiAeMtINJ9eF330kUtFY3OtpQnyORoJGFCNtpMCu3QTZgCM9ljyjIsnzBx3YdbfhzuAsE68kdSjRCuyvwTDGKSdCY4aU6ntuov0MSU0xI3l1kCqSIDxBI9I3VCBOlJ/NsufOiVGGThRL84R2ZurvjQxxpaY8NJNFTLXoFeJ/Xj/V0aVf3JRqIvD8oyhljo6doghnSCXBmk0NQVhSk9XBYyQR1qauqinBWzx5mXTOGp7b8O7O682rso4KHMExnIIHF9CEW2hBGzA8wTO8wpuVWy/Wu/UxH12xyp1D+APr8wfV6pTt</latexit><latexit sha1_base64=KNSYXpe+JERsrbP/+QxnbJ2F1T0=>AAAB+3icbVDLSsNAFL3xWesr1qWbYBFclUQEXbgouNBlBfuANobJdNIOnZmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvcy5J0wYVdp1v62V1bX1jc3KVnV7Z3dv3z6odVScSkzaOGax7IVIEUYFaWuqGeklkiAeMtINJ9eF330kUtFY3OtpQnyORoJGFCNtpMCu3QTZgCM9ljyjIsnzBx3YdbfhzuAsE68kdSjRCuyvwTDGKSdCY4aU6ntuov0MSU0xI3l1kCqSIDxBI9I3VCBOlJ/NsufOiVGGThRL84R2ZurvjQxxpaY8NJNFTLXoFeJ/Xj/V0aVf3JRqIvD8oyhljo6doghnSCXBmk0NQVhSk9XBYyQR1qauqinBWzx5mXTOGp7b8O7O682rso4KHMExnIIHF9CEW2hBGzA8wTO8wpuVWy/Wu/UxH12xyp1D+APr8wfV6pTt</latexit><latexit sha1_base64=KNSYXpe+JERsrbP/+QxnbJ2F1T0=>AAAB+3icbVDLSsNAFL3xWesr1qWbYBFclUQEXbgouNBlBfuANobJdNIOnZmEmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvcy5J0wYVdp1v62V1bX1jc3KVnV7Z3dv3z6odVScSkzaOGax7IVIEUYFaWuqGeklkiAeMtINJ9eF330kUtFY3OtpQnyORoJGFCNtpMCu3QTZgCM9ljyjIsnzBx3YdbfhzuAsE68kdSjRCuyvwTDGKSdCY4aU6ntuov0MSU0xI3l1kCqSIDxBI9I3VCBOlJ/NsufOiVGGThRL84R2ZurvjQxxpaY8NJNFTLXoFeJ/Xj/V0aVf3JRqIvD8oyhljo6doghnSCXBmk0NQVhSk9XBYyQR1qauqinBWzx5mXTOGp7b8O7O682rso4KHMExnIIHF9CEW2hBGzA8wTO8wpuVWy/Wu/UxH12xyp1D+APr8wfV6pTt</latexit> (c) Recurrent GN architecture
Figure 6: (a) An example composing multiple GN blocks in sequence to form a GN core. Here,
the GN blocks can use shared weights, or they could be independent. (b) The encode-process-decode
architecture, which is a common choice for composing GN blocks (see Section 4.3). Here, a GN
encodes an input graph, which is then processed by a GN core. The output of the core is decoded
by a third GN block into an output graph, whose nodes, edges, and/or global attributes would be
used for task-specic purposes. (c) The encode-process-decode architecture applied in a sequential
setting in which the core is also unrolled over time (potentially using a GRU or LSTM architecture),
in addition to being repeated within each time step. Here, merged lines indicate concatenation, and
split lines indicate copying.
4.3 Composable multi-block architectures
A key design principle of graph networks is constructing complex architectures by composing GN
blocks. We dened a GN block as always taking a graph comprised of edge, node, and global
elements as input, and returning a graph with the same constituent elements as output (simply
passing through the input elements to the output when those elements are not explicitly updated).
This graph-to-graph input/output interface ensures that the output of one GN block can be passed
as input to another, even if their internal congurations are dierent, similar to the tensor-to-tensor
interface of the standard deep learning toolkit. In the most basic form, two GN blocks, GN1and
GN2, can be composed as GN1GN2by passing the output of the rst as input to the second:
G0= GN 2(GN 1(G)).
Arbitrary numbers of GN blocks can be composed, as show in Figure 6a. The blocks can
be unshared (dierent functions and/or parameters, analogous to layers of a CNN), GN16=
GN26=6=GNM, or shared (reused functions and parameters, analogous to an unrolled RNN),
GN1=GN2==GNM. The white box around the GNcorein Figure 6a represents Mrepeated
internal processing sub-steps, with either shared or unshared GN blocks. Shared congurations
are analogous to message-passing (Gilmer et al., 2017), where the same local update procedure is
applied iteratively to propagate information across the structure (Figure 7). If we exclude the global
u(which aggregates information from across the nodes and edges), the information that a node
has access to after msteps of propagation is determined by the set of nodes and edges that are at
mostmhops away. This can be interpreted as breaking down a complex computation into smaller
elementary steps. The steps can also be used to capture sequentiality in time. In our ball-spring
example, if each propagation step predicts the physical dynamics over one time step of duration  t,
then theMpropagation steps result in a total simulation time of, Mt.
A common architecture design is what we call the encode-process-decode conguration (Hamrick
et al. (2018); also see Figure 6ba): an input graph, Ginpis transformed into a latent representation,
G0, by an encoder, GNenc; a shared core block, GNcore, is applied Mtimes to return GM; and
nally an output graph, Gout, is decoded by GNdec. For example, in our running example, the
encoder might compute the initial forces and interaction energies between the balls, the core might
19m=0
<latexit sha1_base64=ic6WezPV7TWar890N1QnpVOPjNA=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79Im/+jZNkDxotaCiquunuilIpLPr+l1dYW9/Y3Cpul3Z29/YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324/cWJHoB5ykPFR0qEUsGEUn3atrv1+u+DV/AfKXBDmpQI5Gv/zZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4v/ed0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMv+B9fAO9341Y</latexit><latexit sha1_base64=ic6WezPV7TWar890N1QnpVOPjNA=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79Im/+jZNkDxotaCiquunuilIpLPr+l1dYW9/Y3Cpul3Z29/YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324/cWJHoB5ykPFR0qEUsGEUn3atrv1+u+DV/AfKXBDmpQI5Gv/zZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4v/ed0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMv+B9fAO9341Y</latexit><latexit sha1_base64=ic6WezPV7TWar890N1QnpVOPjNA=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79Im/+jZNkDxotaCiquunuilIpLPr+l1dYW9/Y3Cpul3Z29/YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324/cWJHoB5ykPFR0qEUsGEUn3atrv1+u+DV/AfKXBDmpQI5Gv/zZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4v/ed0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMv+B9fAO9341Y</latexit><latexit sha1_base64=ic6WezPV7TWar890N1QnpVOPjNA=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79Im/+jZNkDxotaCiquunuilIpLPr+l1dYW9/Y3Cpul3Z29/YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324/cWJHoB5ykPFR0qEUsGEUn3atrv1+u+DV/AfKXBDmpQI5Gv/zZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4v/ed0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMv+B9fAO9341Y</latexit>m=1
<latexit sha1_base64=Zv5ybfKo/H8QvXQaOWqQVyOFtg4=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64=Zv5ybfKo/H8QvXQaOWqQVyOFtg4=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64=Zv5ybfKo/H8QvXQaOWqQVyOFtg4=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64=hP+6LrUf2d3tZaldqaQQvEKMXyw=>AAAB2XicbZDNSgMxFIXv1L86Vq1rN8EiuCozbnQpuHFZwbZCO5RM5k4bmskMyR2hDH0BF25EfC93vo3pz0JbDwQ+zknIvSculLQUBN9ebWd3b/+gfugfNfzjk9Nmo2fz0gjsilzl5jnmFpXU2CVJCp8LgzyLFfbj6f0i77+gsTLXTzQrMMr4WMtUCk7O6oyaraAdLMW2IVxDC9YaNb+GSS7KDDUJxa0dhEFBUcUNSaFw7g9LiwUXUz7GgUPNM7RRtRxzzi6dk7A0N+5oYkv394uKZ9bOstjdzDhN7Ga2MP/LBiWlt1EldVESarH6KC0Vo5wtdmaJNChIzRxwYaSblYkJN1yQa8Z3HYSbG29D77odBu3wMYA6nMMFXEEIN3AHD9CBLghI4BXevYn35n2suqp569LO4I+8zx84xIo4</latexit><latexit sha1_base64=41wuc+jmacSzA8ipzMzk3JQ30Cs=>AAAB33icbZDNSgMxFIXv1L9aq1a3boJFcFUybnQjCG5cVrS10A4lk95pQ5PMkGSEMvQR3LhQxLdy59uY/iy09UDg45yE3HviTArrKP0OShubW9s75d3KXnX/4LB2VG3bNDccWzyVqenEzKIUGltOOImdzCBTscSneHw7y5+e0ViR6kc3yTBSbKhFIjhz3npQ12G/VqcNOhdZh3AJdViq2a999QYpzxVqxyWzthvSzEUFM05widNKL7eYMT5mQ+x61EyhjYr5qFNy5p0BSVLjj3Zk7v5+UTBl7UTF/qZibmRXs5n5X9bNXXIVFUJnuUPNFx8luSQuJbO9yUAY5E5OPDBuhJ+V8BEzjDvfTsWXEK6uvA7ti0ZIG+E9hTKcwCmcQwiXcAN30IQWcBjCC7zBeyCD1+BjUVcpWPZ2DH8UfP4Arc6MHg==</latexit><latexit sha1_base64=41wuc+jmacSzA8ipzMzk3JQ30Cs=>AAAB33icbZDNSgMxFIXv1L9aq1a3boJFcFUybnQjCG5cVrS10A4lk95pQ5PMkGSEMvQR3LhQxLdy59uY/iy09UDg45yE3HviTArrKP0OShubW9s75d3KXnX/4LB2VG3bNDccWzyVqenEzKIUGltOOImdzCBTscSneHw7y5+e0ViR6kc3yTBSbKhFIjhz3npQ12G/VqcNOhdZh3AJdViq2a999QYpzxVqxyWzthvSzEUFM05widNKL7eYMT5mQ+x61EyhjYr5qFNy5p0BSVLjj3Zk7v5+UTBl7UTF/qZibmRXs5n5X9bNXXIVFUJnuUPNFx8luSQuJbO9yUAY5E5OPDBuhJ+V8BEzjDvfTsWXEK6uvA7ti0ZIG+E9hTKcwCmcQwiXcAN30IQWcBjCC7zBeyCD1+BjUVcpWPZ2DH8UfP4Arc6MHg==</latexit><latexit sha1_base64=c5O5JuZLnHr2YYnYcAr/a2hwAn0=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoOQKtzZaCMEbCwjmg9IjrC3mSRLdveO3T0hHPkJNhaK2PqL7Pw3bpIrNPHBwOO9GWbmRYngxvr+t1fY2Nza3inulvb2Dw6PyscnLROnmmGTxSLWnYgaFFxh03IrsJNopDIS2I4mt3O//YTa8Fg92mmCoaQjxYecUeukB3kT9MsVv+YvQNZJkJMK5Gj0y1+9QcxSicoyQY3pBn5iw4xqy5nAWamXGkwom9ARdh1VVKIJs8WpM3LhlAEZxtqVsmSh/p7IqDRmKiPXKakdm1VvLv7ndVM7vA4zrpLUomLLRcNUEBuT+d9kwDUyK6aOUKa5u5WwMdWUWZdOyYUQrL68TlqXtcCvBfd+pV7N4yjCGZxDFQK4gjrcQQOawGAEz/AKb57wXrx372PZWvDymVP4A+/zB74jjVU=</latexit><latexit sha1_base64=Zv5ybfKo/H8QvXQaOWqQVyOFtg4=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64=Zv5ybfKo/H8QvXQaOWqQVyOFtg4=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64=Zv5ybfKo/H8QvXQaOWqQVyOFtg4=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64=Zv5ybfKo/H8QvXQaOWqQVyOFtg4=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64=Zv5ybfKo/H8QvXQaOWqQVyOFtg4=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64=Zv5ybfKo/H8QvXQaOWqQVyOFtg4=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit>m=2
<latexit sha1_base64=oVNcNejQAmVb9Nn6VIeeV7f50ls=>AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3+CFw+KePUXefPfmLZ70NYHA4/3ZpiZFyaCG+t536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5Ux+UK17NWwCvEz8nFcjRHJS/+sOYppIpSwUxpud7iQ0yoi2ngs1K/dSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F/7xeaqPrIOMqSS1TdLkoSgW2MZ7/jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZp/f1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541a</latexit><latexit sha1_base64=oVNcNejQAmVb9Nn6VIeeV7f50ls=>AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3+CFw+KePUXefPfmLZ70NYHA4/3ZpiZFyaCG+t536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5Ux+UK17NWwCvEz8nFcjRHJS/+sOYppIpSwUxpud7iQ0yoi2ngs1K/dSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F/7xeaqPrIOMqSS1TdLkoSgW2MZ7/jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZp/f1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541a</latexit><latexit sha1_base64=oVNcNejQAmVb9Nn6VIeeV7f50ls=>AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3+CFw+KePUXefPfmLZ70NYHA4/3ZpiZFyaCG+t536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5Ux+UK17NWwCvEz8nFcjRHJS/+sOYppIpSwUxpud7iQ0yoi2ngs1K/dSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F/7xeaqPrIOMqSS1TdLkoSgW2MZ7/jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZp/f1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541a</latexit><latexit sha1_base64=oVNcNejQAmVb9Nn6VIeeV7f50ls=>AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3+CFw+KePUXefPfmLZ70NYHA4/3ZpiZFyaCG+t536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5Ux+UK17NWwCvEz8nFcjRHJS/+sOYppIpSwUxpud7iQ0yoi2ngs1K/dSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F/7xeaqPrIOMqSS1TdLkoSgW2MZ7/jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZp/f1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541a</latexit>m=3
<latexit sha1_base64=+jOsYdJ9SNJymWDVPaXJffNRSPM=>AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpT/BiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9/Y3Cpul3Z29/YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6Je/eoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E/7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezX//rJSr+ZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4Q/Q5w/Ca41b</latexit><latexit sha1_base64=+jOsYdJ9SNJymWDVPaXJffNRSPM=>AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpT/BiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9/Y3Cpul3Z29/YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6Je/eoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E/7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezX//rJSr+ZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4Q/Q5w/Ca41b</latexit><latexit sha1_base64=+jOsYdJ9SNJymWDVPaXJffNRSPM=>AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpT/BiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9/Y3Cpul3Z29/YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6Je/eoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E/7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezX//rJSr+ZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4Q/Q5w/Ca41b</latexit><latexit sha1_base64=+jOsYdJ9SNJymWDVPaXJffNRSPM=>AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpT/BiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9/Y3Cpul3Z29/YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6Je/eoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E/7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezX//rJSr+ZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4Q/Q5w/Ca41b</latexit>
m=0
<latexit sha1_base64=ic6WezPV7TWar890N1QnpVOPjNA=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79Im/+jZNkDxotaCiquunuilIpLPr+l1dYW9/Y3Cpul3Z29/YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324/cWJHoB5ykPFR0qEUsGEUn3atrv1+u+DV/AfKXBDmpQI5Gv/zZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4v/ed0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMv+B9fAO9341Y</latexit><latexit sha1_base64=ic6WezPV7TWar890N1QnpVOPjNA=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79Im/+jZNkDxotaCiquunuilIpLPr+l1dYW9/Y3Cpul3Z29/YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324/cWJHoB5ykPFR0qEUsGEUn3atrv1+u+DV/AfKXBDmpQI5Gv/zZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4v/ed0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMv+B9fAO9341Y</latexit><latexit sha1_base64=ic6WezPV7TWar890N1QnpVOPjNA=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79Im/+jZNkDxotaCiquunuilIpLPr+l1dYW9/Y3Cpul3Z29/YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324/cWJHoB5ykPFR0qEUsGEUn3atrv1+u+DV/AfKXBDmpQI5Gv/zZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4v/ed0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMv+B9fAO9341Y</latexit><latexit sha1_base64=ic6WezPV7TWar890N1QnpVOPjNA=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ7PJkJnZZaZXCCGf4MWDIl79Im/+jZNkDxotaCiquunuilIpLPr+l1dYW9/Y3Cpul3Z29/YPyodHLZtkhvEmS2RiOhG1XArNmyhQ8k5qOFWR5O1ofDP324/cWJHoB5ykPFR0qEUsGEUn3atrv1+u+DV/AfKXBDmpQI5Gv/zZGyQsU1wjk9TabuCnGE6pQcEkn5V6meUpZWM65F1HNVXchtPFqTNy5pQBiRPjSiNZqD8nplRZO1GR61QUR3bVm4v/ed0M46twKnSaIddsuSjOJMGEzP8mA2E4QzlxhDIj3K2EjaihDF06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NWT3rP35r0vWwtePnMMv+B9fAO9341Y</latexit>m=1
<latexit sha1_base64=Zv5ybfKo/H8QvXQaOWqQVyOFtg4=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64=Zv5ybfKo/H8QvXQaOWqQVyOFtg4=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64=Zv5ybfKo/H8QvXQaOWqQVyOFtg4=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64=hP+6LrUf2d3tZaldqaQQvEKMXyw=>AAAB2XicbZDNSgMxFIXv1L86Vq1rN8EiuCozbnQpuHFZwbZCO5RM5k4bmskMyR2hDH0BF25EfC93vo3pz0JbDwQ+zknIvSculLQUBN9ebWd3b/+gfugfNfzjk9Nmo2fz0gjsilzl5jnmFpXU2CVJCp8LgzyLFfbj6f0i77+gsTLXTzQrMMr4WMtUCk7O6oyaraAdLMW2IVxDC9YaNb+GSS7KDDUJxa0dhEFBUcUNSaFw7g9LiwUXUz7GgUPNM7RRtRxzzi6dk7A0N+5oYkv394uKZ9bOstjdzDhN7Ga2MP/LBiWlt1EldVESarH6KC0Vo5wtdmaJNChIzRxwYaSblYkJN1yQa8Z3HYSbG29D77odBu3wMYA6nMMFXEEIN3AHD9CBLghI4BXevYn35n2suqp569LO4I+8zx84xIo4</latexit><latexit sha1_base64=41wuc+jmacSzA8ipzMzk3JQ30Cs=>AAAB33icbZDNSgMxFIXv1L9aq1a3boJFcFUybnQjCG5cVrS10A4lk95pQ5PMkGSEMvQR3LhQxLdy59uY/iy09UDg45yE3HviTArrKP0OShubW9s75d3KXnX/4LB2VG3bNDccWzyVqenEzKIUGltOOImdzCBTscSneHw7y5+e0ViR6kc3yTBSbKhFIjhz3npQ12G/VqcNOhdZh3AJdViq2a999QYpzxVqxyWzthvSzEUFM05widNKL7eYMT5mQ+x61EyhjYr5qFNy5p0BSVLjj3Zk7v5+UTBl7UTF/qZibmRXs5n5X9bNXXIVFUJnuUPNFx8luSQuJbO9yUAY5E5OPDBuhJ+V8BEzjDvfTsWXEK6uvA7ti0ZIG+E9hTKcwCmcQwiXcAN30IQWcBjCC7zBeyCD1+BjUVcpWPZ2DH8UfP4Arc6MHg==</latexit><latexit sha1_base64=41wuc+jmacSzA8ipzMzk3JQ30Cs=>AAAB33icbZDNSgMxFIXv1L9aq1a3boJFcFUybnQjCG5cVrS10A4lk95pQ5PMkGSEMvQR3LhQxLdy59uY/iy09UDg45yE3HviTArrKP0OShubW9s75d3KXnX/4LB2VG3bNDccWzyVqenEzKIUGltOOImdzCBTscSneHw7y5+e0ViR6kc3yTBSbKhFIjhz3npQ12G/VqcNOhdZh3AJdViq2a999QYpzxVqxyWzthvSzEUFM05widNKL7eYMT5mQ+x61EyhjYr5qFNy5p0BSVLjj3Zk7v5+UTBl7UTF/qZibmRXs5n5X9bNXXIVFUJnuUPNFx8luSQuJbO9yUAY5E5OPDBuhJ+V8BEzjDvfTsWXEK6uvA7ti0ZIG+E9hTKcwCmcQwiXcAN30IQWcBjCC7zBeyCD1+BjUVcpWPZ2DH8UfP4Arc6MHg==</latexit><latexit sha1_base64=c5O5JuZLnHr2YYnYcAr/a2hwAn0=>AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoOQKtzZaCMEbCwjmg9IjrC3mSRLdveO3T0hHPkJNhaK2PqL7Pw3bpIrNPHBwOO9GWbmRYngxvr+t1fY2Nza3inulvb2Dw6PyscnLROnmmGTxSLWnYgaFFxh03IrsJNopDIS2I4mt3O//YTa8Fg92mmCoaQjxYecUeukB3kT9MsVv+YvQNZJkJMK5Gj0y1+9QcxSicoyQY3pBn5iw4xqy5nAWamXGkwom9ARdh1VVKIJs8WpM3LhlAEZxtqVsmSh/p7IqDRmKiPXKakdm1VvLv7ndVM7vA4zrpLUomLLRcNUEBuT+d9kwDUyK6aOUKa5u5WwMdWUWZdOyYUQrL68TlqXtcCvBfd+pV7N4yjCGZxDFQK4gjrcQQOawGAEz/AKb57wXrx372PZWvDymVP4A+/zB74jjVU=</latexit><latexit sha1_base64=Zv5ybfKo/H8QvXQaOWqQVyOFtg4=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64=Zv5ybfKo/H8QvXQaOWqQVyOFtg4=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64=Zv5ybfKo/H8QvXQaOWqQVyOFtg4=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64=Zv5ybfKo/H8QvXQaOWqQVyOFtg4=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64=Zv5ybfKo/H8QvXQaOWqQVyOFtg4=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit><latexit sha1_base64=Zv5ybfKo/H8QvXQaOWqQVyOFtg4=>AAAB6nicbVDLSgNBEOyNrxhfUY9eBoOQU9gVQS9CwIvHiOYByRJmJ73JkJnZZWZWCCGf4MWDIl79Im/+jZNkDxotaCiquunuilLBjfX9L6+wtr6xuVXcLu3s7u0flA+PWibJNMMmS0SiOxE1KLjCpuVWYCfVSGUksB2Nb+Z++xG14Yl6sJMUQ0mHisecUeuke3kd9MsVv+YvQP6SICcVyNHolz97g4RlEpVlghrTDfzUhlOqLWcCZ6VeZjClbEyH2HVUUYkmnC5OnZEzpwxInGhXypKF+nNiSqUxExm5TkntyKx6c/E/r5vZ+CqccpVmFhVbLoozQWxC5n+TAdfIrJg4Qpnm7lbCRlRTZl06JRdCsPryX9I6rwV+Lbi7qNSreRxFOIFTqEIAl1CHW2hAExgM4Qle4NUT3rP35r0vWwtePnMMv+B9fAO/Y41Z</latexit>m=2
<latexit sha1_base64=oVNcNejQAmVb9Nn6VIeeV7f50ls=>AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3+CFw+KePUXefPfmLZ70NYHA4/3ZpiZFyaCG+t536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5Ux+UK17NWwCvEz8nFcjRHJS/+sOYppIpSwUxpud7iQ0yoi2ngs1K/dSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F/7xeaqPrIOMqSS1TdLkoSgW2MZ7/jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZp/f1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541a</latexit><latexit sha1_base64=oVNcNejQAmVb9Nn6VIeeV7f50ls=>AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3+CFw+KePUXefPfmLZ70NYHA4/3ZpiZFyaCG+t536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5Ux+UK17NWwCvEz8nFcjRHJS/+sOYppIpSwUxpud7iQ0yoi2ngs1K/dSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F/7xeaqPrIOMqSS1TdLkoSgW2MZ7/jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZp/f1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541a</latexit><latexit sha1_base64=oVNcNejQAmVb9Nn6VIeeV7f50ls=>AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3+CFw+KePUXefPfmLZ70NYHA4/3ZpiZFyaCG+t536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5Ux+UK17NWwCvEz8nFcjRHJS/+sOYppIpSwUxpud7iQ0yoi2ngs1K/dSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F/7xeaqPrIOMqSS1TdLkoSgW2MZ7/jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZp/f1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541a</latexit><latexit sha1_base64=oVNcNejQAmVb9Nn6VIeeV7f50ls=>AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9ktgl6EghePFe0HtEvJptk2NMkuSVYoS3+CFw+KePUXefPfmLZ70NYHA4/3ZpiZFyaCG+t536iwsbm1vVPcLe3tHxwelY9P2iZONWUtGotYd0NimOCKtSy3gnUTzYgMBeuEk9u533li2vBYPdppwgJJRopHnBLrpAd5Ux+UK17NWwCvEz8nFcjRHJS/+sOYppIpSwUxpud7iQ0yoi2ngs1K/dSwhNAJGbGeo4pIZoJsceoMXzhliKNYu1IWL9TfExmRxkxl6DolsWOz6s3F/7xeaqPrIOMqSS1TdLkoSgW2MZ7/jYdcM2rF1BFCNXe3YjommlDr0im5EPzVl9dJu17zvZp/f1lpVPM4inAG51AFH66gAXfQhBZQGMEzvMIbEugFvaOPZWsB5TOn8Afo8wfA541a</latexit>m=3
<latexit sha1_base64=+jOsYdJ9SNJymWDVPaXJffNRSPM=>AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpT/BiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9/Y3Cpul3Z29/YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6Je/eoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E/7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezX//rJSr+ZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4Q/Q5w/Ca41b</latexit><latexit sha1_base64=+jOsYdJ9SNJymWDVPaXJffNRSPM=>AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpT/BiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9/Y3Cpul3Z29/YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6Je/eoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E/7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezX//rJSr+ZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4Q/Q5w/Ca41b</latexit><latexit sha1_base64=+jOsYdJ9SNJymWDVPaXJffNRSPM=>AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpT/BiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9/Y3Cpul3Z29/YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6Je/eoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E/7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezX//rJSr+ZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4Q/Q5w/Ca41b</latexit><latexit sha1_base64=+jOsYdJ9SNJymWDVPaXJffNRSPM=>AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvQU9lVQS9CwYvHivYD2qVk02wbmmSXJCuUpT/BiwdFvPqLvPlvTNs9aOuDgcd7M8zMCxPBjfW8b1RYW9/Y3Cpul3Z29/YPyodHLROnmrImjUWsOyExTHDFmpZbwTqJZkSGgrXD8e3Mbz8xbXisHu0kYYEkQ8UjTol10oO8ueiXK17NmwOvEj8nFcjR6Je/eoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E/7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezX//rJSr+ZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4Q/Q5w/Ca41b</latexit>Figure 7: Example of message passing. Each row highlights the information that diuses through
the graph starting from a particular node. In the top row, the node of interest is in the upper
right; in the bottom row, the node of interest is in the bottom right. Shaded nodes indicate how far
information from the original node can travel in msteps of message passing; bolded edges indicate
which edges that information has the potential to travel across. Note that during the full message
passing procedure, this propagation of information happens simultaneously for all nodes and edges
in the graph (not just the two shown here).
apply an elementary dynamics update, and the decoder might read out the nal positions from the
updated graph state.
Similar to the encode-process-decode design, recurrent GN-based architectures can be built by
maintaining a hidden graph, Gt
hid, taking as input an observed graph, Gt
inp, and returning an output
graph,Gt
out, on each step (see Figure 6c). This type of architecture can be particularly useful for
predicting sequences of graphs, such as predicting the trajectory of a dynamical system over time
(e.g. Sanchez-Gonzalez et al., 2018). The encoded graph, output by GNenc, must have the same
structure as Gt
hid, and they can be easily combined by concatenating their corresponding ek,vi,
anduvectors (where the upward arrow merges into the left-hand horizontal arrow in Figure 6c),
before being passed to GNcore. For the output, the Gt
hidis copied (where the right-hand horizontal
arrow splits into the downward arrow in Figure 6c) and decoded by GNdec. This design reuses GN
blocks in several ways: GNenc,GNdec, and GNcoreare shared across each step, t; and within each
step, GN coremay perform multiple shared sub-steps.
Various other techniques for designing GN-based architectures can be useful. Graph skip
connections, for example, would concatenate a GN block's input graph, Gm, with its output graph,
Gm+1, before proceeding to further computations. Merging and smoothing input and hidden graph
information, as in Figure 6c, can use LSTM- or GRU-style gating schemes, instead of simple
concatenation (Li et al., 2016). Or distinct, recurrent GN blocks (e.g. Figure 4b) can be composed
before and/or after other GN blocks, to improve stability in the representations over multiple
propagation steps (Sanchez-Gonzalez et al., 2018).
4.4 Implementing graph networks in code
Similar to CNNs (see Figure 1), which are naturally parallelizable (e.g. on GPUs), GNs have a
natural parallel structure: since the eandvfunctions in Equation 1 are shared over the edges
and nodes, respectively, they can be computed in parallel. In practice, this means that with respect
20Box 4: Graph Nets open-source software library: github.com/deepmind/graph nets
We have released an open-source library for building GNs in Tensor
ow/Sonnet. It includes
demos of how to create, manipulate, and train GNs to reason about graph-structured data, on
a shortest path-nding task, a sorting task, and a physical prediction task. Each demo uses the
same GN architecture, which highlights the 
exibility of the approach.
Shortest path demo: tinyurl.com/gn-shortest-path-demo
This demo creates random graphs, and trains a GN to label the nodes and edges on the shortest
path between any two nodes. Over a sequence of message-passing steps (as depicted by each
step's plot), the model renes its prediction of the shortest path.
Sort demo: tinyurl.com/gn-sort-demo
This demo creates lists of random numbers, and trains a GN to sort the list. After a sequence
of message-passing steps, the model makes an accurate prediction of which elements (columns
in the gure) come next after each other (rows).
Physics demo: tinyurl.com/gn-physics-demo
This demo creates random mass-spring physical systems, and trains a GN to predict the state of
the system on the next timestep. The model's next-step predictions can be fed back in as input
to create a rollout of a future trajectory. Each subplot below shows the true and predicted
mass-spring system states over 50 timesteps. This is similar to the model and experiments in
(Battaglia et al., 2016)'s interaction networks.
21toeandv, the nodes and edges can be treated like the batch dimension in typical mini-batch
training regimes. Moreover, several graphs can be naturally batched together by treating them as
disjoint components of a larger graph. With some additional bookkeeping, this allows batching
together the computations made on several independent graphs.
Reusingeandvalso improves GNs' sample eciency. Again, analogous to a convolutional
kernel, the number of samples which are used to optimize a GN's eandvfunctions is the number
of edges and nodes, respectively, across all training graphs. For example, in the balls example
from Sec. 3.2, a scene with four balls which are all connected by springs will provide twelve (4 3)
examples of the contact interaction between them.
We have released an open-source software library for building GNs, which can be found here:
github.com/deepmind/graph nets . See Box 4 for an overview.
4.5 Summary
In this section, we have discussed the design principles behind graph networks: 
exible representa-
tions, congurable within-block structure, and composable multi-block architectures. These three
design principles combine in our framework which is extremely 
exible and applicable to a wide range
of domains ranging from perception, language, and symbolic reasoning. And, as we will see in the
remainder of this paper, the strong relational inductive bias possessed by graph networks supports
combinatorial generalization, thus making it a powerful tool both in terms of implementation and
theory.
5 Discussion
In this paper, we analyzed the extent to which relational inductive bias exists in deep learning
architectures like MLPs, CNNs, and RNNs, and concluded that while CNNs and RNNs do contain
relational inductive biases, they cannot naturally handle more structured representations such as
sets or graphs. We advocated for building stronger relational inductive biases into deep learning
architectures by highlighting an underused deep learning building block called a graph network ,
which performs computations over graph-structured data. Our graph network framework unies
existing approaches that also operate over graphs, and provides a straightforward interface for
assembling graph networks into complex, sophisticated architectures.
5.1 Combinatorial generalization in graph networks
The structure of GNs naturally supports combinatorial generalization because they do not perform
computations strictly at the system level, but also apply shared computations across the entities and
across the relations as well. This allows never-before-seen systems to be reasoned about, because
they are built from familiar components, in a way that re
ects von Humboldt's innite use of nite
means (Humboldt, 1836; Chomsky, 1965).
A number of studies have explored GNs' capacity for combinatorial generalization. Battaglia
et al. (2016) found that GNs trained to make one-step physical state predictions could simulate
thousands of future time steps, and also exhibit accurate zero-shot transfer to physical systems
with double, or half, the number of entities experienced during training. Sanchez-Gonzalez et al.
(2018) found similar results in more complex physical control settings, including that GNs trained as
forward models on simulated multi-joint agents could generalize to agents with new numbers of joints.
Hamrick et al. (2018) and Wang et al. (2018b) each found that GN-based decision-making policies
could transfer to novel numbers of entities as well. In combinatorial optimization problems, Bello
22et al. (2016); Nowak et al. (2017); Dai et al. (2017); Kool and Welling (2018) showed that GNs could
generalize well to problems of much dierent sizes than they had been trained on. Similarly, Toyer
et al. (2017) showed generalization to dierent sizes of planning problems, and Hamilton et al. (2017)
showed generalization to producing useful node embeddings for previously unseen data. On boolean
SAT problems, Selsam et al. (2018) demonstrated generalization both to dierent problem sizes and
across problem distributions: their model retained good performance upon strongly modifying the
distribution of the input graphs and its typical local structure.
These striking examples of combinatorial generalization are not entirely surprising, given GNs'
entity- and relation-centric organization, but nonetheless provide important support for the view
that embracing explicit structure and 
exible learning is a viable approach toward realizing better
sample eciency and generalization in modern AI.
5.2 Limitations of graph networks
One limitation of GNs' and MPNNs' form of learned message-passing (Shervashidze et al., 2011)
is that it cannot be guaranteed to solve some classes of problems, such as discriminating between
certain non-isomorphic graphs. Kondor et al. (2018) suggested that covariance7(Cohen and Welling,
2016; Kondor and Trivedi, 2018), rather than invariance to permutations of the nodes and edges
is preferable, and proposed covariant compositional networks which can preserve structural
information, and allow it to be ignored only if desired.
More generally, while graphs are a powerful way of representing structure information, they
have limits. For example, notions like recursion, control 
ow, and conditional iteration are not
straightforward to represent with graphs, and, minimally, require additional assumptions (e.g., in
interpreting abstract syntax trees). Programs and more computer-like processing can oer greater
representational and computational expressivity with respect to these notions, and some have argued
they are an important component of human cognition (Tenenbaum et al., 2011; Lake et al., 2015;
Goodman et al., 2015).
5.3 Open questions
Although we are excited about the potential impacts that graph networks can have, we caution that
these models are only one step forward. Realizing the full potential of graph networks will likely be
far more challenging than organizing their behavior under one framework, and indeed, there are a
number of unanswered questions regarding the best ways to use graph networks.
One pressing question is: where do the graphs come from that graph networks operate over?
One of the hallmarks of deep learning has been its ability to perform complex computations over
raw sensory data, such as images and text, yet it is unclear the best ways to convert sensory data
into more structured representations like graphs. One approach (which we have already discussed)
assumes a fully connected graph structure between spatial or linguistic entities, such as in the
literature on self-attention (Vaswani et al., 2017; Wang et al., 2018c). However, such representations
may not correspond exactly to the true entities (e.g., convolutional features do not directly
correspond to objects in a scene). Moreover, many underlying graph structures are much more
sparse than a fully connected graph, and it is an open question how to induce this sparsity. Several
lines of active research are exploring these issues (Watters et al., 2017; van Steenkiste et al., 2018;
Li et al., 2018; Kipf et al., 2018) but as of yet there is no single method which can reliably extract
discrete entities from sensory data. Developing such a method is an exciting challenge for future
7Covariance means, roughly, that the activations vary in a predictable way as a function of the ordering of the
incoming edges.
23research, and once solved will likely open the door for much more powerful and 
exible reasoning
algorithms.
A related question is how to adaptively modify graph structures during the course of computation.
For example, if an object fractures into multiple pieces, a node representing that object also ought
to split into multiple nodes. Similarly, it might be useful to only represent edges between objects
that are in contact, thus requiring the ability to add or remove edges depending on context. The
question of how to support this type of adaptivity is also actively being researched, and in particular,
some of the methods used for identifying the underlying structure of a graph may be applicable (e.g.
Li et al., 2018; Kipf et al., 2018).
Human cognition makes the strong assumption that the world is composed of objects and
relations (Spelke and Kinzler, 2007), and because GNs make a similar assumption, their behavior
tends to be more interpretable. The entities and relations that GNs operate over often correspond
to things that humans understand (such as physical objects), thus supporting more interpretable
analysis and visualization (e.g., as in Selsam et al., 2018). An interesting direction for future work
is to further explore the interpretability of the behavior of graph networks.
5.4 Integrative approaches for learning and structure
While our focus here has been on graphs, one takeaway from this paper is less about graphs
themselves and more about the approach of blending powerful deep learning approaches with
structured representations. We are excited by related approaches which have explored this idea for
other types of structured representations and computations, such as linguistic trees (Socher et al.,
2011a,b, 2012, 2013; Tai et al., 2015; Andreas et al., 2016), partial tree traversals in a state-action
graph (Guez et al., 2018; Farquhar et al., 2018), hierarchical action policies (Andreas et al., 2017),
multi-agent communication channels (Foerster et al., 2016), capsules (Sabour et al., 2017), and
programs (Parisotto et al., 2017). Other methods have attempted to capture dierent types of
structure by mimicking key hardware and software components in computers and how they transfer
information between each other, such as persistent slotted storage, registers, memory I/O controllers,
stacks, and queues (e.g. Dyer et al., 2015; Grefenstette et al., 2015; Joulin and Mikolov, 2015;
Sukhbaatar et al., 2015; Kurach et al., 2016; Graves et al., 2016).
5.5 Conclusion
Recent advances in AI, propelled by deep learning, have been transformative across many important
domains. Despite this, a vast gap between human and machine intelligence remains, especially with
respect to ecient, generalizable learning. We argue for making combinatorial generalization a top
priority for AI, and advocate for embracing integrative approaches which draw on ideas from human
cognition, traditional computer science, standard engineering practice, and modern deep learning.
Here we explored 
exible learning-based approaches which implement strong relational inductive
biases to capitalize on explicitly structured representations and computations, and presented a
framework called graph networks , which generalize and extend various recent approaches for neural
networks applied to graphs. Graph networks are designed to promote building complex architectures
using customizable graph-to-graph building blocks, and their relational inductive biases promote
combinatorial generalization and improved sample eciency over other standard machine learning
building blocks.
Despite their benets and potential, however, learnable models which operate on graphs are
only a stepping stone on the path toward human-like intelligence. We are optimistic about a
number of other relevant, and perhaps underappreciated, research directions, including marrying
24learning-based approaches with programs (Ritchie et al., 2016; Andreas et al., 2016; Gaunt et al.,
2016; Evans and Grefenstette, 2018; Evans et al., 2018), developing model-based approaches with an
emphasis on abstraction (Kansky et al., 2017; Konidaris et al., 2018; Zhang et al., 2018; Hay et al.,
2018), investing more heavily in meta-learning (Wang et al., 2016, 2018a; Finn et al., 2017), and
exploring multi-agent learning and interaction as a key catalyst for advanced intelligence (Nowak,
2006; Ohtsuki et al., 2006). These directions each involve rich notions of entities, relations, and
combinatorial generalization, and can potentially benet, and benet from, greater interaction with
approaches for learning relational reasoning over explicitly structured representations.
Acknowledgements
We thank Tobias Pfa, Danilo Rezende, Nando de Freitas, Murray Shanahan, Thore Graepel, John
Jumper, Demis Hassabis, and the broader DeepMind and Google communities for valuable feedback
and support.
References
Allamanis, M., Brockschmidt, M., and Khademi, M. (2018). Learning to represent programs with
graphs. In Proceedings of the International Conference on Learning Representations (ICLR) .
Allamanis, M., Chanthirasegaran, P., Kohli, P., and Sutton, C. (2017). Learning continuous
semantic representations of symbolic expressions. In Proceedings of the International Conference
on Machine Learning (ICML) .
Anderson, J. R. (1982). Acquisition of cognitive skill. Psychological Review , 89(4):369.
Andreas, J., Klein, D., and Levine, S. (2017). Modular multitask reinforcement learning with policy
sketches. In Proceedings of the International Conference on Machine Learning (ICML) .
Andreas, J., Rohrbach, M., Darrell, T., and Klein, D. (2016). Neural module networks. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 39{48.
Ba, J. L., Kiros, J. R., and Hinton, G. E. (2016). Layer normalization. arXiv preprint
arXiv:1607.06450 .
Bahdanau, D., Cho, K., and Bengio, Y. (2015). Neural machine translation by jointly learning to
align and translate. In Proceedings of the International Conference on Learning Representations
(ICLR) .
Battaglia, P., Pascanu, R., Lai, M., Rezende, D. J., et al. (2016). Interaction networks for learning
about objects, relations and physics. In Advances in Neural Information Processing Systems ,
pages 4502{4510.
Battaglia, P. W., Hamrick, J. B., and Tenenbaum, J. B. (2013). Simulation as an engine of physical
scene understanding. Proceedings of the National Academy of Sciences , 110(45):18327{18332.
Bello, I., Pham, H., Le, Q. V., Norouzi, M., and Bengio, S. (2016). Neural combinatorial optimization
with reinforcement learning. arXiv preprint arXiv:1611.09940 .
Bobrow, D. G. and Hinton, G. E., editors (1990). Articial Intelligence , volume 46. Elsevier Science
Publishers Ltd., Essex, UK. Special Issue 1-2: On Connectionist Symbol Processing.
25Bojchevski, A., Shchur, O., Z ugner, D., and G unnemann, S. (2018). Netgan: Generating graphs via
random walks. arXiv preprint arXiv:1803.00816 .
Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., and Yakhnenko, O. (2013). Translating
embeddings for modeling multi-relational data. In Advances in Neural Information Processing
Systems , pages 2787{2795.
Botvinick, M. M. (2008). Hierarchical models of behavior and prefrontal function. Trends in
Cognitive Sciences , 12(5):201{208.
Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., and Vandergheynst, P. (2017). Geometric deep
learning: going beyond euclidean data. IEEE Signal Processing Magazine , 34(4):18{42.
Bruna, J., Zaremba, W., Szlam, A., and LeCun, Y. (2014). Spectral networks and locally connected
networks on graphs. In Proceedings of the International Conference on Learning Representations
(ICLR) .
Chang, M. B., Ullman, T., Torralba, A., and Tenenbaum, J. B. (2017). A compositional object-
based approach to learning physical dynamics. In Proceedings of the International Conference on
Learning Representations (ICLR) .
Chen, X., Li, L., Fei-Fei, L., and Gupta, A. (2018a). Iterative visual reasoning beyond convolutions.
InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .
Chen, X., Liu, C., and Song, D. (2018b). Tree-to-tree neural networks for program translation. In
Workshops of the International Conference on Learning Representations (ICLR) .
Cho, K., Van Merri enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio,
Y. (2014). Learning phrase representations using rnn encoder-decoder for statistical machine
translation. In Proceeding of the Conference on Empirical Methods in Natural Language Processing
(EMNLP) .
Chomsky, N. (1957). Syntactic Structures . Mouton & Co.
Chomsky, N. (1965). Aspects of the Theory of Syntax . MIT Press.
Cohen, T. and Welling, M. (2016). Group equivariant convolutional networks. In International
Conference on Machine Learning , pages 2990{2999.
Craik, K. J. W. (1943). The Nature of Explanation . Cambridge University Press.
Cui, Z., Henrickson, K., Ke, R., and Wang, Y. (2018). High-order graph convolutional recurrent
neural network: A deep learning framework for network-scale trac learning and forecasting.
arXiv preprint arXiv:1802.07007 .
Dai, H., Dai, B., and Song, L. (2016). Discriminative embeddings of latent variable models for
structured data. In Proceedings of the International Conference on Machine Learning (ICML) .
Dai, H., Khalil, E. B., Zhang, Y., Dilkina, B., and Song, L. (2017). Learning combinatorial
optimization algorithms over graphs. In Advances in Neural Information Processing Systems .
De Cao, N. and Kipf, T. (2018). MolGAN: An implicit generative model for small molecular graphs.
arXiv preprint arXiv:1805.11973 .
26Deerrard, M., Bresson, X., and Vandergheynst, P. (2016). Convolutional neural networks on graphs
with fast localized spectral ltering. In Advances in Neural Information Processing Systems , pages
3844{3852.
Denil, M., Colmenarejo, S. G., Cabi, S., Saxton, D., and de Freitas, N. (2017). Programmable
agents. arXiv preprint arXiv:1706.06383 .
Devlin, J., Uesato, J., Singh, R., and Kohli, P. (2017). Semantic code repair using neuro-symbolic
transformation networks. arXiv preprint arXiv:1710.11054 .
Duvenaud, D. K., Maclaurin, D., Iparraguirre, J., Bombarell, R., Hirzel, T., Aspuru-Guzik, A., and
Adams, R. P. (2015). Convolutional networks on graphs for learning molecular ngerprints. In
Advances in Neural Information Processing Systems , pages 2224{2232.
Dyer, C., Ballesteros, M., Ling, W., Matthews, A., and Smith, N. A. (2015). Transition-based
dependency parsing with stack long short-term memory. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics (ACL) .
D zeroski, S., De Raedt, L., and Driessens, K. (2001). Relational reinforcement learning. Machine
Learning , 43(1-2):7{52.
Edwards, H. and Storkey, A. (2016). Towards a neural statistician. arXiv preprint arXiv:1606.02185 .
Eliasmith, C. (2013). How to build a brain: A neural architecture for biological cognition . Oxford
University Press.
Elman, J. L. (1990). Finding structure in time. Cognitive Science , 14(2):179{211.
Elman, J. L. (1991). Distributed representations, simple recurrent networks, and grammatical
structure. Machine Learning , 7(2-3):195{225.
Eslami, S. A., Heess, N., Weber, T., Tassa, Y., Szepesvari, D., Hinton, G. E., et al. (2016). Attend,
infer, repeat: Fast scene understanding with generative models. In Advances in Neural Information
Processing Systems , pages 3225{3233.
Evans, R. and Grefenstette, E. (2018). Learning explanatory rules from noisy data. Journal of
Articial Intelligence Research , 61:1{64.
Evans, R., Saxton, D., Amos, D., Kohli, P., and Grefenstette, E. (2018). Can neural networks
understand logical entailment? In Proceedings of the International Conference on Learning
Representations (ICLR) .
Farquhar, G., Rockt aschel, T., Igl, M., and Whiteson, S. (2018). TreeQN and ATreeC: Dierentiable
tree planning for deep reinforcement learning. In Proceedings of the International Conference on
Learning Representations (ICLR) .
Finn, C., Abbeel, P., and Levine, S. (2017). Model-agnostic meta-learning for fast adaptation of
deep networks. arXiv preprint arXiv:1703.03400 .
Fodor, J. A. (1975). The Language of Thought . Harvard University Press.
Fodor, J. A. and Pylyshyn, Z. W. (1988). Connectionism and cognitive architecture: A critical
analysis. Cognition , 28(1-2):3{71.
27Foerster, J., Assael, I. A., de Freitas, N., and Whiteson, S. (2016). Learning to communicate with
deep multi-agent reinforcement learning. In Advances in Neural Information Processing Systems ,
pages 2137{2145.
Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of
pattern recognition unaected by shift in position. Biological Cybernetics , 36:193{202.
Garcia, V. and Bruna, J. (2018). Few-shot learning with graph neural networks. In Proceedings of
the International Conference on Learning Representations (ICLR) .
Garc a-Dur an, A. and Niepert, M. (2017). Learning graph representations with embedding propaga-
tion. arXiv preprint arXiv:1710.03059 .
Garnelo, M., Arulkumaran, K., and Shanahan, M. (2016). Towards deep symbolic reinforcement
learning. arXiv preprint arXiv:1609.05518 .
Gaunt, A. L., Brockschmidt, M., Kushman, N., and Tarlow, D. (2016). Dierentiable programs
with neural libraries. arXiv preprint arXiv:1611.02109 .
Geman, S., Bienenstock, E., and Doursat, R. (1992). Neural networks and the bias/variance dilemma.
Neural Computation , 4(1):1{58.
Gentner, D. and Markman, A. B. (1997). Structure mapping in analogy and similarity. American
Psychologist , 52(1):45.
Getoor, L. and Taskar, B. (2007). Introduction to Statistical Relational Learning . MIT press.
Ghahramani, Z. (2015). Probabilistic machine learning and articial intelligence. Nature ,
521(7553):452.
Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. (2017). Neural message
passing for quantum chemistry. arXiv preprint arXiv:1704.01212 .
Goodfellow, I., Bengio, Y., Courville, A., and Bengio, Y. (2016). Deep Learning . MIT Press.
Goodman, N. (1955). The new riddle of induction. In Fact, Fiction, and Forecast , pages 59{83.
Harvard University Press.
Goodman, N., Mansinghka, V., Roy, D. M., Bonawitz, K., and Tenenbaum, J. B. (2012). Church: a
language for generative models. arXiv preprint arXiv:1206.3255 .
Goodman, N. D., Tenenbaum, J. B., and Gerstenberg, T. (2015). Concepts in a probabilistic
language of thought. In Margolis, E. and Laurence, S., editors, The Conceptual Mind: New
Directions in the Study of Concepts . MIT Press.
Goodwin, G. P. and Johnson-Laird, P. (2005). Reasoning about relations. Psychological Review ,
112(2):468.
Gori, M., Monfardini, G., and Scarselli, F. (2005). A new model for learning in graph domains. In
Proceedings of the International Joint Conference on Neural Networks (IJCNN) , volume 2, pages
729{734. IEEE.
Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwi nska, A., Colmenarejo,
S. G., Grefenstette, E., Ramalho, T., Agapiou, J., et al. (2016). Hybrid computing using a neural
network with dynamic external memory. Nature , 538(7626):471.
28Grefenstette, E., Hermann, K. M., Suleyman, M., and Blunsom, P. (2015). Learning to transduce
with unbounded memory. In Advances in Neural Information Processing Systems , pages 1828{1836.
Griths, T. L., Chater, N., Kemp, C., Perfors, A., and Tenenbaum, J. B. (2010). Probabilistic
models of cognition: Exploring representations and inductive biases. Trends in Cognitive Sciences ,
14(8):357{364.
Grover, A. and Leskovec, J. (2016). node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining ,
pages 855{864. ACM.
Guez, A., Weber, T., Antonoglou, I., Simonyan, K., Vinyals, O., Wierstra, D., Munos, R., and
Silver, D. (2018). Learning to search with MCTSnets. arXiv preprint arXiv:1802.04697 .
Gulcehre, C., Denil, M., Malinowski, M., Razavi, A., Pascanu, R., Hermann, K. M., Battaglia, P.,
Bapst, V., Raposo, D., Santoro, A., and de Freitas, N. (2018). Hyperbolic attention networks.
arXiv preprint arXiv:1805.09786 .
Hamaguchi, T., Oiwa, H., Shimbo, M., and Matsumoto, Y. (2017). Knowledge transfer for out-of-
knowledge-base entities: A graph neural network approach. In Proceedings of the International
Joint Conference on Articial Intelligence (IJCAI) .
Hamilton, W., Ying, Z., and Leskovec, J. (2017). Inductive representation learning on large graphs.
InAdvances in Neural Information Processing Systems , pages 1025{1035.
Hamrick, J., Allen, K., Bapst, V., Zhu, T., McKee, K., Tenenbaum, J., and Battaglia, P. (2018).
Relational inductive bias for physical construction in humans and machines. In Proceedings of the
40th Annual Conference of the Cognitive Science Society .
Hamrick, J. B., Ballard, A. J., Pascanu, R., Vinyals, O., Heess, N., and Battaglia, P. W. (2017).
Metacontrol for adaptive imagination-based optimization. In Proceedings of the International
Conference on Learning Representations (ICLR) .
Hartford, J., Graham, D. R., Leyton-Brown, K., and Ravanbakhsh, S. (2018). Deep models of
interactions across sets. arXiv preprint arXiv:1803.02879 .
Hay, N., Stark, M., Schlegel, A., Wendelken, C., Park, D., Purdy, E., Silver, T., Phoenix, D. S.,
and George, D. (2018). Behavior is everything{towards representing concepts with sensorimotor
contingencies. In Proceedings of the AAAI Conference on Articial Intelligence (AAAI) .
Hena, M., Bruna, J., and LeCun, Y. (2015). Deep convolutional networks on graph-structured
data. arXiv preprint arXiv:1506.05163 .
Hinton, G. E. (1990). Mapping part-whole hierarchies into connectionist networks. Articial
Intelligence , 46(1-2):47{75.
Hjort, N. L., Holmes, C., M uller, P., and Walker, S. G. (2010). Bayesian Nonparametrics . Cambridge
University Press.
Hoshen, Y. (2017). Vain: Attentional multi-agent predictive modeling. In Advances in Neural
Information Processing Systems , pages 2698{2708.
Hu, H., Gu, J., Zhang, Z., Dai, J., and Wei, Y. (2017). Relation networks for object detection.
arXiv preprint arXiv:1711.11575 .
29Hudson, D. A. and Manning, C. D. (2018). Compositional attention networks for machine reasoning.
InProceedings of the International Conference on Learning Representations (ICLR) .
Humboldt, W. (1999/1836). On Language: On the diversity of human language construction and its
in
uence on the mental development of the human species . Cambridge University Press.
Hummel, J. E. and Holyoak, K. J. (2003). A symbolic-connectionist theory of relational inference
and generalization. Psychological Review , 110(2):220.
Ioe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In Proceedings of the 32nd International Conference on International
Conference on Machine Learning (ICML) .
Johnson, D. D. (2017). Learning graphical state transitions. Proceedings of the International
Conference on Learning Representations (ICLR) .
Joulin, A. and Mikolov, T. (2015). Inferring algorithmic patterns with stack-augmented recurrent
nets. In Advances in Neural Information Processing Systems , pages 190{198.
Kansky, K., Silver, T., M ely, D. A., Eldawy, M., L azaro-Gredilla, M., Lou, X., Dorfman, N., Sidor,
S., Phoenix, S., and George, D. (2017). Schema networks: Zero-shot transfer with a generative
causal model of intuitive physics. In Proceedings of the International Conference on Machine
Learning (ICML) .
Kearnes, S., McCloskey, K., Berndl, M., Pande, V., and Riley, P. (2016). Molecular graph
convolutions: moving beyond ngerprints. Journal of computer-aided molecular design , 30(8):595{
608.
Kemp, C. and Tenenbaum, J. B. (2008). The discovery of structural form. Proceedings of the
National Academy of Sciences , 105(31):10687{10692.
Kipf, T., Fetaya, E., Wang, K.-C., Welling, M., and Zemel, R. (2018). Neural relational inference
for interacting systems. In Proceedings of the International Conference on Machine Learning
(ICML) .
Kipf, T. N. and Welling, M. (2017). Semi-supervised classication with graph convolutional networks.
InProceedings of the International Conference on Learning Representations (ICLR) .
Koller, D. and Friedman, N. (2009). Probabilistic Graphical Models: Principles and Techniques .
MIT press.
Kondor, R., Son, H. T., Pan, H., Anderson, B., and Trivedi, S. (2018). Covariant compositional
networks for learning graphs. arXiv preprint arXiv:1801.02144 .
Kondor, R. and Trivedi, S. (2018). On the generalization of equivariance and convolution in neural
networks to the action of compact groups. arXiv preprint arXiv:1802.03690 .
Konidaris, G., Kaelbling, L. P., and Lozano-Perez, T. (2018). From skills to symbols: Learning
symbolic representations for abstract high-level planning. Journal of Articial Intelligence
Research , 61:215{289.
Kool, W. and Welling, M. (2018). Attention solves your TSP. arXiv preprint arXiv:1803.08475 .
30Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet classication with deep convolu-
tional neural networks. In Advances in Neural Information Processing Systems , pages 1097{1105.
Kurach, K., Andrychowicz, M., and Sutskever, I. (2016). Neural random-access machines. In
Proceedings of the International Conference on Learning Representations (ICLR) .
Lake, B. M. and Baroni, M. (2018). Still not systematic after all these years: On the compositional
skills of sequence-to-sequence recurrent networks. In Proceedings of the International Conference
on Machine Learning (ICML) .
Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning
through probabilistic program induction. Science , 350(6266):1332{1338.
Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. (2017). Building machines that
learn and think like people. Behavioral and Brain Sciences , 40.
LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature , 521(7553):436.
LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel,
L. D. (1989). Backpropagation applied to handwritten zip code recognition. Neural computation ,
1(4):541{551.
Li, Y., Tarlow, D., Brockschmidt, M., and Zemel, R. (2016). Gated graph sequence neural networks.
InProceedings of the International Conference on Learning Representations (ICLR) .
Li, Y., Vinyals, O., Dyer, C., Pascanu, R., and Battaglia, P. (2018). Learning deep generative
models of graphs. In Workshops at the International Conference on Learning Representations
(ICLR) .
Li, Y., Yu, R., Shahabi, C., and Liu, Y. (2017). Diusion convolutional recurrent neural network:
Data-driven trac forecasting. arXiv preprint arXiv:1707.01926 .
Lin, Z., Feng, M., Santos, C. N. d., Yu, M., Xiang, B., Zhou, B., and Bengio, Y. (2017). A structured
self-attentive sentence embedding. In Proceedings of the International Conference on Learning
Representations (ICLR) .
Liu, H., Simonyan, K., Vinyals, O., Fernando, C., and Kavukcuoglu, K. (2018). Hierarchical
representations for ecient architecture search. In Proceedings of the International Conference on
Learning Representations (ICLR) .
Luong, M.-T., Pham, H., and Manning, C. D. (2015). Eective approaches to attention-based neural
machine translation. arXiv preprint arXiv:1508.04025 .
Marcus, G. (2001). The algebraic mind.
Marcus, G. (2018a). Deep learning: A critical appraisal. arXiv preprint arXiv:1801.00631 .
Marcus, G. (2018b). Innateness, alphazero, and articial intelligence. arXiv preprint
arXiv:1801.05667 .
McClelland, J. L. (1994). The interaction of nature and nurture in development: A parallel
distributed processing perspective. International perspectives on psychological science , 1:57{88.
31McClelland, J. L. and Rumelhart, D. E. (1981). An interactive activation model of context eects
in letter perception: I. an account of basic ndings. Psychological Review , 88(5):375.
Mikolov, T., Yih, W.-t., and Zweig, G. (2013). Linguistic regularities in continuous space word
representations. In Proceedings of the 2013 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies , pages 746{751.
Mitchell, T. M. (1980). The need for biases in learning generalizations . Department of Computer
Science, Laboratory for Computer Science Research, Rutgers Univ. New Jersey.
Mnih, V., Heess, N., Graves, A., et al. (2014). Recurrent models of visual attention. In Advances in
neural information processing systems , pages 2204{2212.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A.,
Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. (2015). Human-level control through deep
reinforcement learning. Nature , 518(7540):529.
Monti, F., Boscaini, D., Masci, J., Rodola, E., Svoboda, J., and Bronstein, M. M. (2017). Geometric
deep learning on graphs and manifolds using mixture model cnns. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) .
Morav c k, M., Schmid, M., Burch, N., Lis y, V., Morrill, D., Bard, N., Davis, T., Waugh, K.,
Johanson, M., and Bowling, M. (2017). Deepstack: Expert-level articial intelligence in heads-up
no-limit poker. Science , 356(6337):508{513.
Narayanan, A., Chandramohan, M., Chen, L., Liu, Y., and Saminathan, S. (2016). subgraph2vec:
Learning distributed representations of rooted sub-graphs from large graphs. In Workshops at the
20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining .
Narayanan, A., Chandramohan, M., Venkatesan, R., Chen, L., Liu, Y., and Jaiswal, S. (2017).
graph2vec: Learning distributed representations of graphs. arXiv preprint arXiv:1707.05005 .
Navon, D. (1977). Forest before trees: The precedence of global features in visual perception.
Cognitive Psychology , 9(3):353{383.
Niepert, M., Ahmed, M., and Kutzkov, K. (2016). Learning convolutional neural networks for
graphs. In Proceedings of the International Conference on Machine Learning (ICML) , pages
2014{2023.
Nilsson, N. J. and Fikes, R. E. (1970). Strips: A new approach to the application of theorem proving
to problem solving. Technical report, SRI International, Menlo Park, CA Articial Intelligence
Center.
Nowak, A., Villar, S., Bandeira, A. S., and Bruna, J. (2017). A note on learning algorithms for
quadratic assignment with graph neural networks. In Proceedings of the Principled Approaches to
Deep Learning Workshop (PADL) at the International Conference of Machine Learning (ICML) .
Nowak, M. A. (2006). Five rules for the evolution of cooperation. science , 314(5805):1560{1563.
Ohtsuki, H., Hauert, C., Lieberman, E., and Nowak, M. A. (2006). A simple rule for the evolution
of cooperation on graphs and social networks. Nature , 441(7092):502.
32O~ noro-Rubio, D., Niepert, M., Garc a-Dur an, A., Gonz alez-S anchez, R., and L opez-Sastre,
R. J. (2017). Representation learning for visual-relational knowledge graphs. arXiv preprint
arXiv:1709.02314 .
Parisotto, E., Mohamed, A.-r., Singh, R., Li, L., Zhou, D., and Kohli, P. (2017). Neuro-symbolic
program synthesis. In Proceedings of the International Conference on Learning Representations
(ICLR) .
Pascanu, R., Li, Y., Vinyals, O., Heess, N., Buesing, L., Racani ere, S., Reichert, D., Weber, T.,
Wierstra, D., and Battaglia, P. (2017). Learning model-based planning from scratch. arXiv
preprint arXiv:1707.06170 .
Pearl, J. (1986). Fusion, propagation, and structuring in belief networks. Articial intelligence ,
29(3):241{288.
Pearl, J. (1988). Probabilistic reasoning in intelligent systems: Networks of plausible inference.
Morgan Kaufmann.
Pearl, J. (2009). Causality: Models, Reasoning and Inference . Cambridge University Press, New
York, NY, USA, 2nd edition.
Pearl, J. (2018). Theoretical impediments to machine learning with seven sparks from the causal
revolution. arXiv preprint arXiv:1801.04016 .
Pennington, J., Socher, R., and Manning, C. (2014). Glove: Global vectors for word representation.
InProceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) ,
pages 1532{1543.
Perozzi, B., Al-Rfou, R., and Skiena, S. (2014). Deepwalk: Online learning of social representations.
InProceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and
data mining , pages 701{710. ACM.
Pevn y, T. and Somol, P. (2017). Using neural network formalism to solve multiple-instance problems.
InInternational Symposium on Neural Networks , pages 135{142. Springer.
Pinker, S. and Prince, A. (1988). On language and connectionism: Analysis of a parallel distributed
processing model of language acquisition. Cognition , 28(1-2):73{193.
Plate, T. A. (1995). Holographic reduced representations. IEEE Transactions on Neural Networks ,
6(3):623{641.
Plaut, D. C., McClelland, J. L., Seidenberg, M. S., and Patterson, K. (1996). Understanding normal
and impaired word reading: computational principles in quasi-regular domains. Psychological
Review , 103(1):56.
Pollack, J. B. (1990). Recursive distributed representations. Articial Intelligence , 46(1-2):77{105.
Qi, C. R., Su, H., Mo, K., and Guibas, L. J. (2017). Pointnet: Deep learning on point sets for 3d
classication and segmentation. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) .
Raposo, D., Santoro, A., Barrett, D., Pascanu, R., Lillicrap, T., and Battaglia, P. (2017). Discovering
objects and their relations from entangled scene representations. In Workshops at the International
Conference on Learning Representations (ICLR) .
33Reed, S. and De Freitas, N. (2016). Neural programmer-interpreters. In Proceedings of the
International Conference on Learning Representations (ICLR) .
Ritchie, D., Horsfall, P., and Goodman, N. D. (2016). Deep amortized inference for probabilistic
programs. arXiv preprint arXiv:1610.05735 .
Rosenblatt, F. (1961). Principles of neurodynamics. perceptrons and the theory of brain mechanisms.
Technical report, Cornell Aeronautical Lab Inc., Bualo, NY.
Rumelhart, D. E., McClelland, J. L., Group, P. R., et al. (1987). Parallel Distributed Processing ,
volume 1. MIT Press.
Russell, S. J. and Norvig, P. (2009). Articial Intelligence: A Modern Approach (3rd Edition) .
Pearson.
Sabour, S., Frosst, N., and Hinton, G. E. (2017). Dynamic routing between capsules. In Advances
in Neural Information Processing Systems , pages 3859{3869.
Sanchez-Gonzalez, A., Heess, N., Springenberg, J. T., Merel, J., Riedmiller, M., Hadsell, R., and
Battaglia, P. (2018). Graph networks as learnable physics engines for inference and control. In
Proceedings of the 35th International Conference on Machine Learning (ICLR) .
Santoro, A., Raposo, D., Barrett, D. G., Malinowski, M., Pascanu, R., Battaglia, P., and Lillicrap,
T. (2017). A simple neural network module for relational reasoning. In Advances in Neural
Information Processing Systems .
Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G. (2009a). Computational
capabilities of graph neural networks. IEEE Transactions on Neural Networks , 20(1):81{102.
Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G. (2009b). The graph
neural network model. IEEE Transactions on Neural Networks , 20(1):61{80.
Scarselli, F., Yong, S. L., Gori, M., Hagenbuchner, M., Tsoi, A. C., and Maggini, M. (2005). Graph
neural networks for ranking web pages. In Proceedings of the 2005 IEEE/WIC/ACM International
Conference on Web Intelligence , pages 666{672. IEEE.
Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks , 61:85{117.
Selsam, D., Lamm, M., Bunz, B., Liang, P., de Moura, L., and Dill, D. L. (2018). Learning a sat
solver from single-bit supervision. arXiv preprint arXiv:1802.03685 .
Shalev-Shwartz, S., Shamir, O., and Shammah, S. (2017). Failures of gradient-based deep learning.
arXiv preprint arXiv:1703.07950 .
Shaw, P., Uszkoreit, J., and Vaswani, A. (2018). Self-attention with relative position representations.
InProceedings of the 16th Annual Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies .
Shervashidze, N., Schweitzer, P., Leeuwen, E. J. v., Mehlhorn, K., and Borgwardt, K. M. (2011).
Weisfeiler-lehman graph kernels. Journal of Machine Learning Research , 12(Sep):2539{2561.
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J.,
Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. (2016). Mastering the game of go with
deep neural networks and tree search. Nature , 529(7587):484{489.
34Smolensky, P. (1990). Tensor product variable binding and the representation of symbolic structures
in connectionist systems. Articial Intelligence , 46(1-2):159{216.
Socher, R., Huval, B., Manning, C. D., and Ng, A. Y. (2012). Semantic compositionality through
recursive matrix-vector spaces. In Proceedings of the Joint Conference on Empirical Methods in
Natural Language Processing (EMNLP) and Computational Natural Language Learning (CNLL) ,
pages 1201{1211. Association for Computational Linguistics.
Socher, R., Lin, C. C., Manning, C., and Ng, A. Y. (2011a). Parsing natural scenes and natural
language with recursive neural networks. In Proceedings of the 28th International Conference on
Machine Learning (ICML) , pages 129{136.
Socher, R., Pennington, J., Huang, E. H., Ng, A. Y., and Manning, C. D. (2011b). Semi-supervised
recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing (EMNLP) , pages 151{161. Association for
Computational Linguistics.
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., and Potts, C. (2013).
Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of
the Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 1631{1642.
Spelke, E. S., Breinlinger, K., Macomber, J., and Jacobson, K. (1992). Origins of knowledge.
Psychological review , 99(4):605.
Spelke, E. S. and Kinzler, K. D. (2007). Core knowledge. Developmental Science , 10(1):89{96.
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout:
A simple way to prevent neural networks from overtting. The Journal of Machine Learning
Research , 15(1):1929{1958.
Sukhbaatar, S., Fergus, R., et al. (2016). Learning multiagent communication with backpropagation.
InAdvances in Neural Information Processing Systems , pages 2244{2252.
Sukhbaatar, S., Weston, J., Fergus, R., et al. (2015). End-to-end memory networks. In Advances in
Neural Information Processing Systems , pages 2440{2448.
Sutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to sequence learning with neural networks.
InAdvances in Neural Information Processing Systems , pages 3104{3112.
Szegedy, C., Ioe, S., Vanhoucke, V., and Alemi, A. A. (2017). Inception-v4, inception-resnet
and the impact of residual connections on learning. In Proceedings of the AAAI Conference on
Articial Intelligence (AAAI) , volume 4, page 12.
Tai, K. S., Socher, R., and Manning, C. D. (2015). Improved semantic representations from
tree-structured long short-term memory networks. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL) .
Tang, J., Qu, M., Wang, M., Zhang, M., Yan, J., and Mei, Q. (2015). Line: Large-scale information
network embedding. In Proceedings of the 24th International Conference on World Wide Web ,
pages 1067{1077. International World Wide Web Conferences Steering Committee.
Tenenbaum, J. B., Griths, T. L., and Kemp, C. (2006). Theory-based bayesian models of inductive
learning and reasoning. Trends in Cognitive Sciences , 10(7):309{318.
35Tenenbaum, J. B., Kemp, C., Griths, T. L., and Goodman, N. D. (2011). How to grow a mind:
Statistics, structure, and abstraction. Science , 331(6022):1279{1285.
Toyer, S., Trevizan, F., Thiebaux, S., and Xie, L. (2017). Action schema networks: Generalised
policies with deep learning. In Proceedings of the AAAI Conference on Articial Intelligence
(AAAI) .
Ullman, T. D., Spelke, E., Battaglia, P., and Tenenbaum, J. B. (2017). Mind games: Game engines
as an architecture for intuitive physics. Trends in Cognitive Sciences , 21(9):649{665.
van Steenkiste, S., Chang, M., Gre, K., and Schmidhuber, J. (2018). Relational neural expectation
maximization: Unsupervised discovery of objects and their interactions. Proceedings of the
International Conference on Learning Representations (ICLR) .
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and
Polosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing
Systems .
Veli ckovi c, P., Cucurull, G., Casanova, A., Romero, A., Li o, P., and Bengio, Y. (2018). Graph
attention networks. In Proceedings of the International Conference on Learning Representations
(ICLR) .
Wang, J. X., Kurth-Nelson, Z., Kumaran, D., Tirumala, D., Soyer, H., Leibo, J. Z., Hassabis, D.,
and Botvinick, M. (2018a). Prefrontal cortex as a meta-reinforcement learning system. Nature
neuroscience , page 1.
Wang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z., Munos, R., Blundell, C.,
Kumaran, D., and Botvinick, M. (2016). Learning to reinforcement learn. arXiv preprint
arXiv:1611.05763 .
Wang, T., Liao, R., Ba, J., and Fidler, S. (2018b). Nervenet: Learning structured policy with graph
neural networks. In Proceedings of the International Conference on Learning Representations
(ICLR) .
Wang, X., Girshick, R., Gupta, A., and He, K. (2018c). Non-local neural networks. In Proceedings
of the Conference on Computer Vision and Pattern Recognition (CVPR) .
Wang, Y., Sun, Y., Liu, Z., Sarma, S. E., Bronstein, M. M., and Solomon, J. M. (2018d). Dynamic
graph cnn for learning on point clouds. arXiv preprint arXiv:1801.07829 .
Watters, N., Zoran, D., Weber, T., Battaglia, P., Pascanu, R., and Tacchetti, A. (2017). Visual
interaction networks: Learning a physics simulator from video. In Advances in Neural Information
Processing Systems , pages 4542{4550.
Wu, J., Lu, E., Kohli, P., Freeman, B., and Tenenbaum, J. (2017). Learning to see physics via
visual de-animation. In Advances in Neural Information Processing Systems , pages 152{163.
Yoon, K., Liao, R., Xiong, Y., Zhang, L., Fetaya, E., Urtasun, R., Zemel, R., and Pitkow, X.
(2018). Inference in probabilistic graphical models by graph neural networks. In Workshops at
the International Conference on Learning Representations (ICLR) .
You, J., Ying, R., Ren, X., Hamilton, W. L., and Leskovec, J. (2018). GraphRNN: A deep generative
model for graphs. arXiv preprint arXiv:1802.08773 .
36Yuille, A. L. and Liu, C. (2018). Deep nets: What have they ever done for vision? arXiv preprint
arXiv:1805.04025 .
Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., and Smola, A. J. (2017).
Deep sets. In Advances in Neural Information Processing Systems , pages 3394{3404.
Zambaldi, V., Raposo, D., Santoro, A., Bapst, V., Li, Y., Babuschkin, I., Tuyls, K., Reichert, D.,
Lillicrap, T., Lockhart, E., Shanahan, M., Langston, V., Pascanu, R., Botvinick, M., Vinyals, O.,
and Battaglia, P. (2018). Relational deep reinforcement learning. arXiv preprint arXiv .
Zhang, A., Lerer, A., Sukhbaatar, S., Fergus, R., and Szlam, A. (2018). Composable planning with
attributes. arXiv preprint arXiv:1803.00512 .
Z ugner, D., Akbarnejad, A., and G unnemann, S. (2018). Adversarial Attacks on Neural Networks
for Graph Data. arXiv preprint arXiv:1805.07984 .
37Appendix: Formulations of additional models
In this appendix we give more examples of how published networks can t in the frame dened by
Equation 1.
Interaction networks
Interaction Networks (Battaglia et al., 2016; Watters et al., 2017) and the Neural Physics Engine
Chang et al. (2017) use a full GN but for the absence of the global to update the edge properties:
e(ek;vrk;vsk;u):=fe(ek;vrk;vsk) = NNe([ek;vrk;vsk])
v
 e0
i;vi;u:=fv
 e0
i;vi;u
= NNv
[ e0
i;vi;u]
e!v
E0
i:= =X
fk:rk=ige0
k
That work also included an extension to the above formulation which output global, rather than
per-node, predictions:
e(ek;vrk;vsk;u):=fe(ek;vrk;vsk) = NNe([ek;vrk;vsk])
v
 e0
i;vi;u:=fv
 e0
i;vi;u
= NNv
[ e0
i;vi;u]
u
 e0; v0;u:=fu
 v0;u
= NNu
[ v0;u]
v!g
V0:= =X
iv0
i
Non-pairwise interactions
Gated Graph Sequence Neural Networks (GGS-NN) (Li et al., 2016) use a slightly generalized
formulation where each edge has an attached type tk2f1;::;Tg, and the updates are:
e((ek;tk);vrk;vsk;u):=fe(ek;vsk) = NNe;tk(vsk)
v
 e0
i;vi;u:=fv
 e0
i;vi
= NNv
[ e0
i;vi]
e!v
E0
i:= =X
fk:rk=ige0
k
These updates are applied recurrently (the NNvis a GRU (Cho et al., 2014)), followed by a global
decoder which computes a weighted sum of embedded nal node states. Here each NNe;tkis a neural
network with specic parameters.
CommNet (Sukhbaatar et al., 2016) (in the slightly more general form described by (Hoshen,
2017)) uses:
e(ek;vrk;vsk;u):=fe(vsk) = NN e(vsk)
v
 e0
i;vi;u:=fv
 e0
i;vi
= NNv
[ e0
i;NNv0(vi)]
e!v
E0
i:= =1
jE0
ijX
fk:rk=ige0
k
38Attention-based approaches
The various attention-based approaches use a ewhich is factored into a scalar pairwise-interaction
function which returns the unnormalized attention term, denoted e(vrk;vsk)=a0
k, and a vector-
valued non-pairwise term, denoted e(vsk) =b0
k,
e(ek;vrk;vsk;u):=fe(vrk;vsk) = (e(vrk;vsk); e(vsk)) = (a0
k;b0
k) =e0
k
The single-headed self-attention (SA) in the Transformer architecture (Vaswani et al., 2017),
implements the non-local formulation as:
e(vrk;vsk) = exp (NN query(vrk)|NNkey(vsk))
e(vsk) = NN (vsk)
v
 e0
i;vi;u:=fv
 e0
i
= NNv
 e0
i
where NNquery,NNkey, and NNare again neural network functions with dierent parameters and
possibly dierent architectures. They also use a multi-headed version which computes Nhparallel
 e0h
iusing dierent NNquery
h,NNkey
h,NNh, wherehindexes the dierent parameters. These are
passed tofvand concatenated:
fv
f e0h
igh=1:::Nh
= NNv
[ e01
i;:::; e0Nh
i]
Vertex Attention Interaction Networks (Hoshen, 2017) are very similar to single-headed SA,
but use Euclidean distance for the attentional similarity metric, with shared parameters across the
attention inputs' embeddings, and also use the input node feature in the node update function,
e(vrk;vsk) = exp
kNN(vrk)NN(vsk)k2
e(vsk) = NN (vsk)
v
 e0
i;vi;u:=fv
 e0
i
= NNv
[ e0
i;vi]
Graph Attention Networks (Veli ckovi c et al., 2018) are also similar to multi-headed SA, but use
a neural network as the attentional similarity metric, with shared parameters across the attention
inputs' embeddings:
e(vrk;vsk) = exp (NN 0([NN(vrk);NN(vsk)))
e(vsk) = NN (vsk)
v
 e0
i;vi;u:=fv
f e0h
igh=1:::Nh
= NNv
[ e01
i;:::; e0Nh
i]
Stretching beyond the specic non-local formulation, Shaw et al. (2018) extended multi-headed
SA with relative position encodings. Relative refers to an encoding of the spatial distance between
nodes in a sequence or other signal in a metric space. This can be expressed in GN language as an
edge attribute ek, and replacing the e(vsk) from multi-headed SA above with:
e(ek;vsk) = NNe(vsk) +ek
39Belief Propagation embeddings
Finally, we brie
y summarize how the general structure2vec algorithm of Dai et al. (2016) can t
into our framework. In order to do so, we need to slightly modify our main Equation 1, i.e.:
k=
felgsl=rk
rl6=sk
:=X
rl=sk
sl6=rkel
e0
k=e(k) :=f(k) = NN( k)
 e0
i=
fe0
kgrk=i:=X
fk:rk=igek
v0
i=v
 e0
i:=f( e0
i) = NN(  e0
i)
Edges' features now takes the meaning of message between their receiver and sender; note that
there is only one set of parameters to learn for both the edges and nodes updates.
40
  Diffusion Models Beat GANs on Image Synthesis
Prafulla Dhariwal
OpenAI
prafulla@openai.comAlex Nichol
OpenAI
alex@openai.com
Abstract
We show that diffusion models can achieve image sample quality superior to the
current state-of-the-art generative models. We achieve this on unconditional im-
age synthesis by ﬁnding a better architecture through a series of ablations. For
conditional image synthesis, we further improve sample quality with classiﬁer guid-
ance: a simple, compute-efﬁcient method for trading off diversity for ﬁdelity using
gradients from a classiﬁer. We achieve an FID of 2.97 on ImageNet 128 128,
4.59 on ImageNet 256 256, and 7.72 on ImageNet 512 512, and we match
BigGAN-deep even with as few as 25 forward passes per sample, all while main-
taining better coverage of the distribution. Finally, we ﬁnd that classiﬁer guidance
combines well with upsampling diffusion models, further improving FID to 3.94
on ImageNet 256256 and 3.85 on ImageNet 512 512. We release our code at
https://github.com/openai/guided-diffusion .
1 Introduction
Figure 1: Selected samples from our best ImageNet 512 512 model (FID 3.85)
Over the past few years, generative models have gained the ability to generate human-like natural
language [ 6], inﬁnite high-quality synthetic images [ 5,28,51] and highly diverse human speech and
music [ 64,13]. These models can be used in a variety of ways, such as generating images from text
prompts [ 72,50] or learning useful feature representations [ 14,7]. While these models are already
Equal contributionarXiv:2105.05233v4  [cs.LG]  1 Jun 2021capable of producing realistic images and sound, there is still much room for improvement beyond
the current state-of-the-art, and better generative models could have wide-ranging impacts on graphic
design, games, music production, and countless other ﬁelds.
GANs [ 19] currently hold the state-of-the-art on most image generation tasks [ 5,68,28] as measured
by sample quality metrics such as FID [ 23], Inception Score [ 54] and Precision [ 32]. However, some
of these metrics do not fully capture diversity, and it has been shown that GANs capture less diversity
than state-of-the-art likelihood-based models [ 51,43,42]. Furthermore, GANs are often difﬁcult to
train, collapsing without carefully selected hyperparameters and regularizers [5, 41, 4].
While GANs hold the state-of-the-art, their drawbacks make them difﬁcult to scale and apply to
new domains. As a result, much work has been done to achieve GAN-like sample quality with
likelihood-based models [ 51,25,42,9]. While these models capture more diversity and are typically
easier to scale and train than GANs, they still fall short in terms of visual sample quality. Furthermore,
except for V AEs, sampling from these models is slower than GANs in terms of wall-clock time.
Diffusion models are a class of likelihood-based models which have recently been shown to produce
high-quality images [ 56,59,25] while offering desirable properties such as distribution coverage,
a stationary training objective, and easy scalability. These models generate samples by gradually
removing noise from a signal, and their training objective can be expressed as a reweighted variational
lower-bound [ 25]. This class of models already holds the state-of-the-art [ 60] on CIFAR-10 [ 31], but
still lags behind GANs on difﬁcult generation datasets like LSUN and ImageNet. Nichol and Dhariwal
[43] found that these models improve reliably with increased compute, and can produce high-quality
samples even on the difﬁcult ImageNet 256 256 dataset using an upsampling stack. However, the
FID of this model is still not competitive with BigGAN-deep [5], the current state-of-the-art on this
dataset.
We hypothesize that the gap between diffusion models and GANs stems from at least two factors:
ﬁrst, that the model architectures used by recent GAN literature have been heavily explored and
reﬁned; second, that GANs are able to trade off diversity for ﬁdelity, producing high quality samples
but not covering the whole distribution. We aim to bring these beneﬁts to diffusion models, ﬁrst by
improving model architecture and then by devising a scheme for trading off diversity for ﬁdelity.
With these improvements, we achieve a new state-of-the-art, surpassing GANs on several different
metrics and datasets.
The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion
models based on Ho et al. [ 25] and the improvements from Nichol and Dhariwal [ 43] and Song
et al. [ 57], and we describe our evaluation setup. In Section 3, we introduce simple architecture
improvements that give a substantial boost to FID. In Section 4, we describe a method for using
gradients from a classiﬁer to guide a diffusion model during sampling. We ﬁnd that a single
hyperparameter, the scale of the classiﬁer gradients, can be tuned to trade off diversity for ﬁdelity,
and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial
examples [ 61]. Finally, in Section 5 we show that models with our improved architecture achieve
state-of-the-art on unconditional image synthesis tasks, and with classiﬁer guidance achieve state-of-
the-art on conditional image synthesis. When using classiﬁer guidance, we ﬁnd that we can sample
with as few as 25 forward passes while maintaining FIDs comparable to BigGAN. We also compare
our improved models to upsampling stacks, ﬁnding that the two approaches give complementary
improvements and that combining them gives the best results on ImageNet 256 256 and 512512.
2 Background
In this section, we provide a brief overview of diffusion models. For a more detailed mathematical
description, we refer the reader to Appendix B.
On a high level, diffusion models sample from a distribution by reversing a gradual noising process. In
particular, sampling starts with noise xTand produces gradually less-noisy samples xT1;xT2;:::
until reaching a ﬁnal sample x0. Each timestep tcorresponds to a certain noise level, and xtcan be
thought of as a mixture of a signal x0with some noise where the signal to noise ratio is determined
by the timestep t. For the remainder of this paper, we assume that the noise is drawn from a diagonal
Gaussian distribution, which works well for natural images and simpliﬁes various derivations.
2A diffusion model learns to produce a slightly more denoised xt1fromxt. Ho et al. [ 25]
parameterize this model as a function (xt;t)which predicts the noise component of a noisy sample
xt. To train these models, each sample in a minibatch is produced by randomly drawing a data sample
x0, a timestep t, and noise, which together give rise to a noised sample xt(Equation 17). The
training objective is then jj(xt;t)jj2, i.e. a simple mean-squared error loss between the true
noise and the predicted noise (Equation 26).
It is not immediately obvious how to sample from a noise predictor (xt;t). Recall that diffusion
sampling proceeds by repeatedly predicting xt1fromxt, starting from xT. Ho et al. [ 25] show
that, under reasonable assumptions, we can model the distribution p(xt1jxt)ofxt1givenxtas
a diagonal Gaussian N(xt1;(xt;t);(xt;t)), where the mean (xt;t)can be calculated as a
function of(xt;t)(Equation 27). The variance (xt;t)of this Gaussian distribution can be ﬁxed
to a known constant [ 25] or learned with a separate neural network head [ 43], and both approaches
yield high-quality samples when the total number of diffusion steps Tis large enough.
Ho et al. [ 25] observe that the simple mean-sqaured error objective, Lsimple , works better in practice
than the actual variational lower bound Lvlbthat can be derived from interpreting the denoising diffu-
sion model as a V AE. They also note that training with this objective and using their corresponding
sampling procedure is equivalent to the denoising score matching model from Song and Ermon [ 58],
who use Langevin dynamics to sample from a denoising model trained with multiple noise levels to
produce high quality image samples. We often use diffusion models as shorthand to refer to both
classes of models.
2.1 Improvements
Following the breakthrough work of Song and Ermon [ 58] and Ho et al. [ 25], several recent papers
have proposed improvements to diffusion models. Here we describe a few of these improvements,
which we employ for our models.
Nichol and Dhariwal [ 43] ﬁnd that ﬁxing the variance (xt;t)to a constant as done in Ho et al.
[25] is sub-optimal for sampling with fewer diffusion steps, and propose to parameterize (xt;t)as
a neural network whose output vis interpolated as:
(xt;t) = exp(vlogt+ (1v) log ~t) (1)
Here,tand~t(Equation 19) are the variances in Ho et al. [ 25] corresponding to upper and lower
bounds for the reverse process variances. Additionally, Nichol and Dhariwal [ 43] propose a hybrid
objective for training both (xt;t)and(xt;t)using the weighted sum Lsimple +Lvlb. Learning
the reverse process variances with their hybrid objective allows sampling with fewer steps without
much drop in sample quality. We adopt this objective and parameterization, and use it throughout our
experiments.
Song et al. [ 57] propose DDIM, which formulates an alternative non-Markovian noising process
that has the same forward marginals as DDPM, but allows producing different reverse samplers by
changing the variance of the reverse noise. By setting this noise to 0, they provide a way to turn any
model(xt;t)into a deterministic mapping from latents to images, and ﬁnd that this provides an
alternative way to sample with fewer steps. We adopt this sampling approach when using fewer than
50 sampling steps, since Nichol and Dhariwal [43] found it to be beneﬁcial in this regime.
2.2 Sample Quality Metrics
For comparing sample quality across models, we perform quantitative evaluations using the following
metrics. While these metrics are often used in practice and correspond well with human judgement,
they are not a perfect proxy, and ﬁnding better metrics for sample quality evaluation is still an open
problem.
Inception Score (IS) was proposed by Salimans et al. [ 54], and it measures how well a model captures
the full ImageNet class distribution while still producing individual samples that are convincing
examples of a single class. One drawback of this metric is that it does not reward covering the
whole distribution or capturing diversity within a class, and models which memorize a small subset
of the full dataset will still have high IS [ 3]. To better capture diversity than IS, Fréchet Inception
Distance (FID) was proposed by Heusel et al. [ 23], who argued that it is more consistent with human
3Channels Depth HeadsAttention BigGAN Rescale FID FID
resolutions up/downsample resblock 700K 1200K
160 2 1 16 7 7 15.33 13.21
128 4 -0.21 -0.48
4 -0.54 -0.82
32,16,8 -0.72 -0.66
3 -1.20 -1.21
3 0.16 0.25
160 2 4 32,16,8 3 7 -3.14 -3.00
Table 1: Ablation of various architecture changes, evaluated at 700K and 1200K iterations
judgement than Inception Score. FID provides a symmetric measure of the distance between two
image distributions in the Inception-V3 [ 62] latent space. Recently, sFID was proposed by Nash
et al. [ 42] as a version of FID that uses spatial features rather than the standard pooled features.
They ﬁnd that this metric better captures spatial relationships, rewarding image distributions with
coherent high-level structure. Finally, Kynkäänniemi et al. [ 32] proposed Improved Precision and
Recall metrics to separately measure sample ﬁdelity as the fraction of model samples which fall into
the data manifold (precision), and diversity as the fraction of data samples which fall into the sample
manifold (recall).
We use FID as our default metric for overall sample quality comparisons as it captures both diversity
and ﬁdelity and has been the de facto standard metric for state-of-the-art generative modeling work
[27,28,5,25]. We use Precision or IS to measure ﬁdelity, and Recall to measure diversity or
distribution coverage. When comparing against other methods, we re-compute these metrics using
public samples or models whenever possible. This is for two reasons: ﬁrst, some papers [ 27,28,
25] compare against arbitrary subsets of the training set which are not readily available; and second,
subtle implementation differences can affect the resulting FID values [ 45]. To ensure consistent
comparisons, we use the entire training set as the reference batch [ 23,5], and evaluate metrics for all
models using the same codebase.
3 Architecture Improvements
In this section we conduct several architecture ablations to ﬁnd the model architecture that provides
the best sample quality for diffusion models.
Ho et al. [ 25] introduced the UNet architecture for diffusion models, which Jolicoeur-Martineau
et al. [ 26] found to substantially improve sample quality over the previous architectures [ 58,33] used
for denoising score matching. The UNet model uses a stack of residual layers and downsampling
convolutions, followed by a stack of residual layers with upsampling colvolutions, with skip con-
nections connecting the layers with the same spatial size. In addition, they use a global attention
layer at the 1616 resolution with a single head, and add a projection of the timestep embedding into
each residual block. Song et al. [ 60] found that further changes to the UNet architecture improved
performance on the CIFAR-10 [ 31] and CelebA-64 [ 34] datasets. We show the same result on
ImageNet 128128, ﬁnding that architecture can indeed give a substantial boost to sample quality on
much larger and more diverse datasets at a higher resolution.
We explore the following architectural changes:
 Increasing depth versus width, holding model size relatively constant.
 Increasing the number of attention heads.
 Using attention at 32 32, 1616, and 88 resolutions rather than only at 16 16.
Using the BigGAN [ 5] residual block for upsampling and downsampling the activations,
following [60].
 Rescaling residual connections with1p
2, following [60, 27, 28].
For all comparisons in this section, we train models on ImageNet 128 128 with batch size 256, and
sample using 250 sampling steps. We train models with the above architecture changes and compare
4Number of heads Channels per head FID
1 14.08
2 -0.50
4 -0.97
8 -1.17
32 -1.36
64 -1.03
128 -1.08
Table 2: Ablation of various attention conﬁgurations. More heads or lower channels per heads both
lead to improved FID.
40 60 80 100 120 140 160 180
time (hrs)14161820222426FIDch=128, res=4
ch=160, res=2
ch=160, res=2, heads=4
ch=160, res=2, multi-res attn
ch=160, res=2, biggan up/down
ch=160, res=2, skip rescale
ch=160, res=2, heads=4, multi-res attn, biggan up/down
20 40 60 80 100
time (hrs)1416182022242628FID1 head
2 heads
4 heads
8 heads
32 head channels
64 head channels
128 head channels
Figure 2: Ablation of various architecture changes, showing FID as a function of wall-clock time.
FID evaluated over 10k samples instead of 50k for efﬁciency.
Operation FID
AdaGN 13.06
Addition + GroupNorm 15.08
Table 3: Ablating the element-wise operation used when projecting timestep and class embeddings
into each residual block. Replacing AdaGN with the Addition + GroupNorm layer from Ho et al.
[25] makes FID worse.
them on FID, evaluated at two different points of training, in Table 1. Aside from rescaling residual
connections, all of the other modiﬁcations improve performance and have a positive compounding
effect. We observe in Figure 2 that while increased depth helps performance, it increases training
time and takes longer to reach the same performance as a wider model, so we opt not to use this
change in further experiments.
We also study other attention conﬁgurations that better match the Transformer architecture [ 66]. To
this end, we experimented with either ﬁxing attention heads to a constant, or ﬁxing the number of
channels per head. For the rest of the architecture, we use 128 base channels, 2 residual blocks
per resolution, multi-resolution attention, and BigGAN up/downsampling, and we train the models
for 700K iterations. Table 2 shows our results, indicating that more heads or fewer channels per
head improves FID. In Figure 2, we see 64 channels is best for wall-clock time, so we opt to use 64
channels per head as our default. We note that this choice also better matches modern transformer
architectures, and is on par with our other conﬁgurations in terms of ﬁnal FID.
53.1 Adaptive Group Normalization
We also experiment with a layer [ 43] that we refer to as adaptive group normalization (AdaGN), which
incorporates the timestep and class embedding into each residual block after a group normalization
operation [ 69], similar to adaptive instance norm [ 27] and FiLM [ 48]. We deﬁne this layer as
AdaGN (h;y) =ysGroupNorm (h)+yb, wherehis the intermediate activations of the residual block
following the ﬁrst convolution, and y= [ys;yb]is obtained from a linear projection of the timestep
and class embedding.
We had already seen AdaGN improve our earliest diffusion models, and so had it included by
default in all our runs. In Table 3, we explicitly ablate this choice, and ﬁnd that the adaptive group
normalization layer indeed improved FID. Both models use 128 base channels and 2 residual blocks
per resolution, multi-resolution attention with 64 channels per head, and BigGAN up/downsampling,
and were trained for 700K iterations.
In the rest of the paper, we use this ﬁnal improved model architecture as our default: variable width
with 2 residual blocks per resolution, multiple heads with 64 channels per head, attention at 32, 16 and
8 resolutions, BigGAN residual blocks for up and downsampling, and adaptive group normalization
for injecting timestep and class embeddings into residual blocks.
4 Classiﬁer Guidance
In addition to employing well designed architectures, GANs for conditional image synthesis [ 39,5]
make heavy use of class labels. This often takes the form of class-conditional normalization statistics
[16,11] as well as discriminators with heads that are explicitly designed to behave like classiﬁers
p(yjx)[40]. As further evidence that class information is crucial to the success of these models,
Lucic et al. [ 36] ﬁnd that it is helpful to generate synthetic labels when working in a label-limited
regime.
Given this observation for GANs, it makes sense to explore different ways to condition diffusion
models on class labels. We already incorporate class information into normalization layers (Section
3.1). Here, we explore a different approach: exploiting a classiﬁer p(yjx)to improve a diffusion
generator. Sohl-Dickstein et al. [ 56] and Song et al. [ 60] show one way to achieve this, wherein a
pre-trained diffusion model can be conditioned using the gradients of a classiﬁer. In particular, we
can train a classiﬁer p(yjxt;t)on noisy images xt, and then use gradients rxtlogp(yjxt;t)to
guide the diffusion sampling process towards an arbitrary class label y.
In this section, we ﬁrst review two ways of deriving conditional sampling processes using classiﬁers.
We then describe how we use such classiﬁers in practice to improve sample quality. We choose the
notationp(yjxt;t) =p(yjxt)and(xt;t) =(xt)for brevity, noting that they refer to separate
functions for each timestep tand at training time the models must be conditioned on the input t.
4.1 Conditional Reverse Noising Process
We start with a diffusion model with an unconditional reverse noising process p(xtjxt+1). To
condition this on a label y, it sufﬁces to sample each transition2according to
p;(xtjxt+1;y) =Zp(xtjxt+1)p(yjxt) (2)
whereZis a normalizing constant (proof in Appendix H). It is typically intractable to sample from
this distribution exactly, but Sohl-Dickstein et al. [ 56] show that it can be approximated as a perturbed
Gaussian distribution. Here, we review this derivation.
Recall that our diffusion model predicts the previous timestep xtfrom timestep xt+1using a Gaussian
distribution:
p(xtjxt+1) =N(;) (3)
logp(xtjxt+1) =1
2(xt)T1(xt) +C (4)
2We must also sample xTconditioned on y, but a noisy enough diffusion process causes xTto be nearly
Gaussian even in the conditional case.
6Algorithm 1 Classiﬁer guided diffusion sampling, given a diffusion model ((xt);(xt)), classi-
ﬁerp(yjxt), and gradient scale s.
Input: class label y, gradient scale s
xT sample fromN(0;I)
for alltfromTto 1do
; (xt);(xt)
xt1 sample fromN(+srxtlogp(yjxt);)
end for
returnx0
Algorithm 2 Classiﬁer guided DDIM sampling, given a diffusion model (xt), classiﬁerp(yjxt),
and gradient scale s.
Input: class label y, gradient scale s
xT sample fromN(0;I)
for alltfromTto 1do
^ (xt)p1trxtlogp(yjxt)
xt1 pt1
xtp1t^pt
+p1t1^
end for
returnx0
We can assume that logp(yjxt)has low curvature compared to 1. This assumption is reasonable
in the limit of inﬁnite diffusion steps, where jjjj! 0. In this case, we can approximate logp(yjxt)
using a Taylor expansion around xt=as
logp(yjxt)logp(yjxt)jxt=+ (xt)rxtlogp(yjxt)jxt= (5)
= (xt)g+C1 (6)
Here,g=rxtlogp(yjxt)jxt=, andC1is a constant. This gives
log(p(xtjxt+1)p(yjxt))1
2(xt)T1(xt) + (xt)g+C2 (7)
=1
2(xtg)T1(xtg) +1
2gTg+C2 (8)
=1
2(xtg)T1(xtg) +C3 (9)
= logp(z) +C4;zN(+ g;) (10)
We can safely ignore the constant term C4, since it corresponds to the normalizing coefﬁcient Zin
Equation 2. We have thus found that the conditional transition operator can be approximated by a
Gaussian similar to the unconditional transition operator, but with its mean shifted by g. Algorithm
1 summaries the corresponding sampling algorithm. We include an optional scale factor sfor the
gradients, which we describe in more detail in Section 4.3.
4.2 Conditional Sampling for DDIM
The above derivation for conditional sampling is only valid for the stochastic diffusion sampling
process, and cannot be applied to deterministic sampling methods like DDIM [ 57]. To this end, we
use a score-based conditioning trick adapted from Song et al. [ 60], which leverages the connection
between diffusion models and score matching [ 59]. In particular, if we have a model (xt)that
predicts the noise added to a sample, then this can be used to derive a score function:
rxtlogp(xt) =1p1t(xt) (11)
7Figure 3: Samples from an unconditional diffusion model with classiﬁer guidance to condition
on the class Pembroke Welsh corgi. Using classiﬁer scale 1.0 (left; FID: 33.0) does not produce
convincing samples in this class, whereas classiﬁer scale 10.0 (right; FID: 12.0) produces much more
class-consistent images.
We can now substitute this into the score function for p(xt)p(yjxt):
rxtlog(p(xt)p(yjxt)) =rxtlogp(xt) +rxtlogp(yjxt) (12)
=1p1t(xt) +rxtlogp(yjxt) (13)
Finally, we can deﬁne a new epsilon prediction ^(xt)which corresponds to the score of the joint
distribution:
^(xt):=(xt)p
1trxtlogp(yjxt) (14)
We can then use the exact same sampling procedure as used for regular DDIM, but with the modiﬁed
noise predictions ^(xt)instead of(xt). Algorithm 2 summaries the corresponding sampling
algorithm.
4.3 Scaling Classiﬁer Gradients
To apply classiﬁer guidance to a large scale generative task, we train classiﬁcation models on
ImageNet. Our classiﬁer architecture is simply the downsampling trunk of the UNet model with
an attention pool [ 49] at the 8x8 layer to produce the ﬁnal output. We train these classiﬁers on the
same noising distribution as the corresponding diffusion model, and also add random crops to reduce
overﬁtting. After training, we incorporate the classiﬁer into the sampling process of the diffusion
model using Equation 10, as outlined by Algorithm 1.
In initial experiments with unconditional ImageNet models, we found it necessary to scale the
classiﬁer gradients by a constant factor larger than 1. When using a scale of 1, we observed that the
classiﬁer assigned reasonable probabilities (around 50%) to the desired classes for the ﬁnal samples,
but these samples did not match the intended classes upon visual inspection. Scaling up the classiﬁer
gradients remedied this problem, and the class probabilities from the classiﬁer increased to nearly
100%. Figure 3 shows an example of this effect.
To understand the effect of scaling classiﬁer gradients, note that srxlogp(yjx) =rxlog1
Zp(yjx)s,
whereZis an arbitrary constant. As a result, the conditioning process is still theoretically grounded
in a re-normalized classiﬁer distribution proportional to p(yjx)s. Whens > 1, this distribution
becomes sharper than p(yjx), since larger values are ampliﬁed by the exponent. In other words, using
a larger gradient scale focuses more on the modes of the classiﬁer, which is potentially desirable for
producing higher ﬁdelity (but less diverse) samples.
In the above derivations, we assumed that the underlying diffusion model was unconditional, modeling
p(x). It is also possible to train conditional diffusion models, p(xjy), and use classiﬁer guidance in
the exact same way. Table 4 shows that the sample quality of both unconditional and conditional
models can be greatly improved by classiﬁer guidance. We see that, with a high enough scale, the
guided unconditional model can get quite close to the FID of an unguided conditional model, although
training directly with the class labels still helps. Guiding a conditional model further improves FID.
Table 4 also shows that classiﬁer guidance improves precision at the cost of recall, thus introducing
a trade-off in sample ﬁdelity versus diversity. We explicitly evaluate how this trade-off varies with
8Conditional Guidance Scale FID sFID IS Precision Recall
7 7 26.21 6.35 39.70 0.61 0.63
7 3 1.0 33.03 6.99 32.92 0.56 0.65
7 3 10.0 12.00 10.40 95.41 0.76 0.44
3 7 10.94 6.02 100.98 0.69 0.63
3 3 1.0 4.59 5.25 186.70 0.82 0.52
3 3 10.0 9.11 10.93 283.92 0.88 0.32
Table 4: Effect of classiﬁer guidance on sample quality. Both conditional and unconditional models
were trained for 2M iterations on ImageNet 256 256 with batch size 256.
0 2 4 6 8 10
gradient scale46810121416FID sFID
0 2 4 6 8 10
gradient scale100150200250300IS
0 2 4 6 8 10
gradient scale0.30.40.50.60.70.80.9precision recall
Figure 4: Change in sample quality as we vary scale of the classiﬁer gradients for a class-conditional
ImageNet 128128 model.
0.70 0.75 0.80 0.85 0.90 0.95
Precision0.00.10.20.30.40.50.6RecallBigGAN-deep
Classifier  guidance (ours)
100125150175200225250275
IS51015202530FIDBigGAN-deep
Classifier guidance (ours)
Figure 5: Trade-offs when varying truncation for BigGAN-deep and gradient scale for classiﬁer
guidance. Models are evaluated on ImageNet 128 128. The BigGAN-deep results were produced
using the TFHub model [12] at truncation levels [0:1;0:2;0:3;:::;1:0].
the gradient scale in Figure 4. We see that scaling the gradients beyond 1.0 smoothly trades off
recall (a measure of diversity) for higher precision and IS (measures of ﬁdelity). Since FID and sFID
depend on both diversity and ﬁdelity, their best values are obtained at an intermediate point. We also
compare our guidance with the truncation trick from BigGAN in Figure 5. We ﬁnd that classiﬁer
guidance is strictly better than BigGAN-deep when trading off FID for Inception Score. Less clear
cut is the precision/recall trade-off, which shows that classiﬁer guidance is only a better choice up
until a certain precision threshold, after which point it cannot achieve better precision.
5 Results
To evaluate our improved model architecture on unconditional image generation, we train separate
diffusion models on three LSUN [ 71] classes: bedroom, horse, and cat. To evaluate classiﬁer
guidance, we train conditional diffusion models on the ImageNet [ 52] dataset at 128128, 256256,
and 512512 resolution.
9Model FID sFID Prec Rec
LSUN Bedrooms 256 256
DCTransformery[42] 6.40 6.66 0.44 0.56
DDPM [25] 4.89 9.07 0.60 0.45
IDDPM [43] 4.24 8.21 0.62 0.46
StyleGAN [27] 2.35 6.62 0.59 0.48
ADM (dropout) 1.90 5.59 0.66 0.51
LSUN Horses 256256
StyleGAN2 [28] 3.84 6.46 0.63 0.48
ADM 2.95 5.94 0.69 0.55
ADM (dropout) 2.57 6.81 0.71 0.55
LSUN Cats 256256
DDPM [25] 17.1 12.4 0.53 0.48
StyleGAN2 [28] 7.25 6.33 0.58 0.43
ADM (dropout) 5.57 6.69 0.63 0.52
ImageNet 6464
BigGAN-deep* [5] 4.06 3.96 0.79 0.48
IDDPM [43] 2.92 3.79 0.74 0.62
ADM 2.61 3.77 0.73 0.63
ADM (dropout) 2.07 4.29 0.74 0.63Model FID sFID Prec Rec
ImageNet 128128
BigGAN-deep [5] 6.02 7.18 0.86 0.35
LOGANy[68] 3.36
ADM 5.91 5.09 0.70 0.65
ADM-G (25 steps) 5.98 7.04 0.78 0.51
ADM-G 2.97 5.09 0.78 0.59
ImageNet 256256
DCTransformery[42] 36.51 8.24 0.36 0.67
VQ-V AE-2yz[51] 31.11 17.38 0.36 0.57
IDDPMz[43] 12.26 5.42 0.70 0.62
SR3yz[53] 11.30
BigGAN-deep [5] 6.95 7.36 0.87 0.28
ADM 10.94 6.02 0.69 0.63
ADM-G (25 steps) 5.44 5.32 0.81 0.49
ADM-G 4.59 5.25 0.82 0.52
ImageNet 512512
BigGAN-deep [5] 8.43 8.13 0.88 0.29
ADM 23.24 10.19 0.73 0.60
ADM-G (25 steps) 8.41 9.67 0.83 0.47
ADM-G 7.72 6.57 0.87 0.42
Table 5: Sample quality comparison with state-of-the-art generative models for each task. ADM
refers to our ablated diffusion model, and ADM-G additionally uses classiﬁer guidance. LSUN
diffusion models are sampled using 1000 steps (see Appendix J). ImageNet diffusion models are
sampled using 250 steps, except when we use the DDIM sampler with 25 steps. *No BigGAN-deep
model was available at this resolution, so we trained our own.yValues are taken from a previous
paper, due to lack of public models or samples.zResults use two-resolution stacks.
5.1 State-of-the-art Image Synthesis
Table 5 summarizes our results. Our diffusion models can obtain the best FID on each task, and
the best sFID on all but one task. With the improved architecture, we already obtain state-of-the-art
image generation on LSUN and ImageNet 64 64. For higher resolution ImageNet, we observe that
classiﬁer guidance allows our models to substantially outperform the best GANs. These models
obtain perceptual quality similar to GANs, while maintaining a higher coverage of the distribution as
measured by recall, and can even do so using only 25 diffusion steps.
Figure 6 compares random samples from the best BigGAN-deep model to our best diffusion model.
While the samples are of similar perceptual quality, the diffusion model contains more modes than the
GAN, such as zoomed ostrich heads, single ﬂamingos, different orientations of cheeseburgers, and a
tinca ﬁsh with no human holding it. We also check our generated samples for nearest neighbors in
the Inception-V3 feature space in Appendix C, and we show additional samples in Appendices K-M.
5.2 Comparison to Upsampling
We also compare guidance to using a two-stage upsampling stack. Nichol and Dhariwal [ 43] and
Saharia et al. [ 53] train two-stage diffusion models by combining a low-resolution diffusion model
with a corresponding upsampling diffusion model. In this approach, the upsampling model is
trained to upsample images from the training set, and conditions on low-resolution images that are
concatenated channel-wise to the model input using a simple interpolation (e.g. bilinear). During
sampling, the low-resolution model produces a sample, and then the upsampling model is conditioned
on this sample. This greatly improves FID on ImageNet 256 256, but does not reach the same
performance as state-of-the-art models like BigGAN-deep [43, 53], as seen in Table 5.
In Table 6, we show that guidance and upsampling improve sample quality along different axes.
While upsampling improves precision while keeping a high recall, guidance provides a knob to trade
10Figure 6: Samples from BigGAN-deep with truncation 1.0 (FID 6.95, left) vs samples from our
diffusion model with guidance (FID 4.59, middle) and samples from the training set (right).
Model Sbase Supsample FID sFID IS Precision Recall
ImageNet 256256
ADM 250 10.94 6.02 100.98 0.69 0.63
ADM-U 250 250 7.49 5.13 127.49 0.72 0.63
ADM-G 250 4.59 5.25 186.70 0.82 0.52
ADM-G, ADM-U 250 250 3.94 6.14 215.84 0.83 0.53
ImageNet 512512
ADM 250 23.24 10.19 58.06 0.73 0.60
ADM-U 250 250 9.96 5.62 121.78 0.75 0.64
ADM-G 250 7.72 6.57 172.71 0.87 0.42
ADM-G, ADM-U 25 25 5.96 12.10 187.87 0.81 0.54
ADM-G, ADM-U 250 25 4.11 9.57 219.29 0.83 0.55
ADM-G, ADM-U 250 250 3.85 5.86 221.72 0.84 0.53
Table 6: Comparing our single, upsampling and classiﬁer guided models. For upsampling, we use
theupsampling stack from Nichol and Dhariwal [ 43] combined with our architecture improvements,
which we refer to as ADM-U. The base resolution for the two-stage upsampling models is 64and
128for the 256and512models, respectively. When combining classiﬁer guidance with upsampling,
we only guide the lower resolution model.
off diversity for much higher precision. We achieve the best FIDs by using guidance at a lower
resolution before upsampling to a higher resolution, indicating that these approaches complement
one another.
6 Related Work
Score based generative models were introduced by Song and Ermon [ 59] as a way of modeling a
data distribution using its gradients, and then sampling using Langevin dynamics [ 67]. Ho et al. [ 25]
found a connection between this method and diffusion models [ 56], and achieved excellent sample
quality by leveraging this connection. After this breakthrough work, many works followed up with
more promising results: Kong et al. [ 30] and Chen et al. [ 8] demonstrated that diffusion models
11work well for audio; Jolicoeur-Martineau et al. [ 26] found that a GAN-like setup could improve
samples from these models; Song et al. [ 60] explored ways to leverage techniques from stochastic
differential equations to improve the sample quality obtained by score-based models; Song et al. [ 57]
and Nichol and Dhariwal [ 43] proposed methods to improve sampling speed; Nichol and Dhariwal
[43] and Saharia et al. [ 53] demonstrated promising results on the difﬁcult ImageNet generation task
using upsampling diffusion models. Also related to diffusion models, and following the work of
Sohl-Dickstein et al. [ 56], Goyal et al. [ 21] described a technique for learning a model with learned
iterative generation steps, and found that it could achieve good image samples when trained with a
likelihood objective.
One missing element from previous work on diffusion models is a way to trade off diversity for ﬁdelity.
Other generative techniques provide natural levers for this trade-off. Brock et al. [ 5] introduced the
truncation trick for GANs, wherein the latent vector is sampled from a truncated normal distribution.
They found that increasing truncation naturally led to a decrease in diversity but an increase in ﬁdelity.
More recently, Razavi et al. [ 51] proposed to use classiﬁer rejection sampling to ﬁlter out bad samples
from an autoregressive likelihood-based model, and found that this technique improved FID. Most
likelihood-based models also allow for low-temperature sampling [ 1], which provides a natural way
to emphasize modes of the data distribution (see Appendix G).
Other likelihood-based models have been shown to produce high-ﬁdelity image samples. VQ-V AE
[65] and VQ-V AE-2 [ 51] are autoregressive models trained on top of quantized latent codes, greatly
reducing the computational resources required to train these models on large images. These models
produce diverse and high quality images, but still fall short of GANs without expensive rejection
sampling and special metrics to compensate for blurriness. DCTransformer [ 42] is a related method
which relies on a more intelligent compression scheme. V AEs are another promising class of
likelihood-based models, and recent methods such as NV AE [ 63] and VDV AE [ 9] have successfully
been applied to difﬁcult image generation domains. Energy-based models are another class of
likelihood-based models with a rich history [ 1,10,24]. Sampling from the EBM distribution is
challenging, and Xie et al. [ 70] demonstrate that Langevin dynamics can be used to sample coherent
images from these models. Du and Mordatch [ 15] further improve upon this approach, obtaining
high quality images. More recently, Gao et al. [ 18] incorporate diffusion steps into an energy-based
model, and ﬁnd that doing so improves image samples from these models.
Other works have controlled generative models with a pre-trained classiﬁer. For example, an emerging
body of work [ 17,47,2] aims to optimize GAN latent spaces for text prompts using pre-trained CLIP
[49] models. More similar to our work, Song et al. [ 60] uses a classiﬁer to generate class-conditional
CIFAR-10 images with a diffusion model. In some cases, classiﬁers can act as stand-alone generative
models. For example, Santurkar et al. [ 55] demonstrate that a robust image classiﬁer can be used as a
stand-alone generative model, and Grathwohl et al. [ 22] train a model which is jointly a classiﬁer and
an energy-based model.
7 Limitations and Future Work
While we believe diffusion models are an extremely promising direction for generative modeling,
they are still slower than GANs at sampling time due to the use of multiple denoising steps (and
therefore forward passes). One promising work in this direction is from Luhman and Luhman [ 37],
who explore a way to distill the DDIM sampling process into a single step model. The samples
from the single step model are not yet competitive with GANs, but are much better than previous
single-step likelihood-based models. Future work in this direction might be able to completely close
the sampling speed gap between diffusion models and GANs without sacriﬁcing image quality.
Our proposed classiﬁer guidance technique is currently limited to labeled datasets, and we have
provided no effective strategy for trading off diversity for ﬁdelity on unlabeled datasets. In the future,
our method could be extended to unlabeled data by clustering samples to produce synthetic labels
[36] or by training discriminative models to predict when samples are in the true data distribution or
from the sampling distribution.
The effectiveness of classiﬁer guidance demonstrates that we can obtain powerful generative models
from the gradients of a classiﬁcation function. This could be used to condition pre-trained models
in a plethora of ways, for example by conditioning an image generator with a text caption using a
noisy version of CLIP [ 49], similar to recent methods that guide GANs using text prompts [ 17,47,
122]. It also suggests that large unlabeled datasets could be leveraged in the future to pre-train powerful
diffusion models that can later be improved by using a classiﬁer with desirable properties.
8 Conclusion
We have shown that diffusion models, a class of likelihood-based models with a stationary training
objective, can obtain better sample quality than state-of-the-art GANs. Our improved architecture
is sufﬁcient to achieve this on unconditional image generation tasks, and our classiﬁer guidance
technique allows us to do so on class-conditional tasks. In the latter case, we ﬁnd that the scale
of the classiﬁer gradients can be adjusted to trade off diversity for ﬁdelity. These guided diffusion
models can reduce the sampling time gap between GANs and diffusion models, although diffusion
models still require multiple forward passes during sampling. Finally, by combining guidance with
upsampling, we can further improve sample quality on high-resolution conditional image synthesis.
9 Acknowledgements
We thank Alec Radford, Mark Chen, Pranav Shyam and Raul Puri for providing feedback on this
work.
References
[1]David Ackley, Geoffrey Hinton, and Terrence Sejnowski. A learning algorithm for boltzmann
machines. Cognitive science, 9(1):147-169 , 1985.
[2]Adverb. The big sleep. https://twitter.com/advadnoun/status/
1351038053033406468 , 2021.
[3] Shane Barratt and Rishi Sharma. A note on the inception score. arXiv:1801.01973 , 2018.
[4]Andrew Brock, Theodore Lim, J. M. Ritchie, and Nick Weston. Neural photo editing with
introspective adversarial networks. arXiv:1609.07093 , 2016.
[5]Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high ﬁdelity
natural image synthesis. arXiv:1809.11096 , 2018.
[6]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners.
arXiv:2005.14165 , 2020.
[7]Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya
Sutskever. Generative pretraining from pixels. In International Conference on Machine
Learning , pages 16911703. PMLR, 2020.
[8]Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan.
Wavegrad: Estimating gradients for waveform generation. arXiv:2009.00713 , 2020.
[9]Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on
images. arXiv:2011.10650 , 2021.
[10] Peter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S Zemel. The helmholtz
machine. Neural computation , 7(5):889904, 1995.
[11] Harm de Vries, Florian Strub, Jérémie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron
Courville. Modulating early visual processing by language. arXiv:1707.00683 , 2017.
[12] DeepMind. Biggan-deep 128x128 on tensorﬂow hub. https://tfhub.dev/deepmind/
biggan-deep-128/1 , 2018.
13[13] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya
Sutskever. Jukebox: A generative model for music. arXiv:2005.00341 , 2020.
[14] Jeff Donahue and Karen Simonyan. Large scale adversarial representation learning.
arXiv:1907.02544 , 2019.
[15] Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models.
arXiv:1903.08689 , 2019.
[16] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. A learned representation for
artistic style. arXiv:1610.07629 , 2017.
[17] Federico A. Galatolo, Mario G. C. A. Cimino, and Gigliola Vaglini. Generating images from
caption and vice versa via clip-guided generative latent space search. arXiv:2102.01645 , 2021.
[18] Ruiqi Gao, Yang Song, Ben Poole, Ying Nian Wu, and Diederik P. Kingma. Learning energy-
based models by diffusion recovery likelihood. arXiv:2012.08125 , 2020.
[19] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv:1406.2661 ,
2014.
[20] Google. Cloud tpus. https://cloud.google.com/tpu/ , 2018.
[21] Anirudh Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. Variational walkback:
Learning a transition operator as a stochastic recurrent net. arXiv:1711.02282 , 2017.
[22] Will Grathwohl, Kuan-Chieh Wang, Jörn-Henrik Jacobsen, David Duvenaud, Mohammad
Norouzi, and Kevin Swersky. Your classiﬁer is secretly an energy based model and you should
treat it like one. arXiv:1912.03263 , 2019.
[23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in
Neural Information Processing Systems 30 (NIPS 2017) , 2017.
[24] Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural
computation , 14(8):17711800, 2002.
[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models.
arXiv:2006.11239 , 2020.
[26] Alexia Jolicoeur-Martineau, Rémi Piché-Taillefer, Rémi Tachet des Combes, and Ioan-
nis Mitliagkas. Adversarial score matching and improved sampling for image generation.
arXiv:2009.05475 , 2020.
[27] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. arXiv:arXiv:1812.04948 , 2019.
[28] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.
Analyzing and improving the image quality of stylegan. arXiv:1912.04958 , 2019.
[29] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
arXiv:1412.6980 , 2014.
[30] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile
diffusion model for audio synthesis. arXiv:2009.09761 , 2020.
[31] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. CIFAR-10 (Canadian Institute for Advanced
Research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html .
[32] Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved
precision and recall metric for assessing generative models. arXiv:1904.06991 , 2019.
[33] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian Reid. Reﬁnenet: Multi-path reﬁnement
networks for high-resolution semantic segmentation. arXiv:1611.06612 , 2016.
14[34] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the
wild. In Proceedings of International Conference on Computer Vision (ICCV) , December 2015.
[35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv:1711.05101 ,
2017.
[36] Mario Lucic, Michael Tschannen, Marvin Ritter, Xiaohua Zhai, Olivier Bachem, and Sylvain
Gelly. High-ﬁdelity image generation with fewer labels. arXiv:1903.02271 , 2019.
[37] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for
improved sampling speed. arXiv:2101.02388 , 2021.
[38] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,
Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed
precision training. arXiv:1710.03740 , 2017.
[39] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv:1411.1784 ,
2014.
[40] Takeru Miyato and Masanori Koyama. cgans with projection discriminator. arXiv:1802.05637 ,
2018.
[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. arXiv:1802.05957 , 2018.
[42] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W. Battaglia. Generating images with
sparse representations. arXiv:2103.03841 , 2021.
[43] Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.
arXiv:2102.09672 , 2021.
[44] NVIDIA. Stylegan2. https://github.com/NVlabs/stylegan2 , 2019.
[45] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On buggy resizing libraries and surprising
subtleties in ﬁd calculation. arXiv:2104.11222 , 2021.
[46] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
style, high-performance deep learning library. arXiv:1912.01703 , 2019.
[47] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip:
Text-driven manipulation of stylegan imagery. arXiv:2103.17249 , 2021.
[48] Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. Film:
Visual reasoning with a general conditioning layer. arXiv:1709.07871 , 2017.
[49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini
Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger,
and Ilya Sutskever. Learning transferable visual models from natural language supervision.
arXiv:2103.00020 , 2021.
[50] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark
Chen, and Ilya Sutskever. Zero-shot text-to-image generation. arXiv:2102.12092 , 2021.
[51] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-ﬁdelity images
with VQ-V AE-2. arXiv:1906.00446 , 2019.
[52] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
Imagenet large scale visual recognition challenge. arXiv:1409.0575 , 2014.
[53] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad
Norouzi. Image super-resolution via iterative reﬁnement. arXiv:arXiv:2104.07636 , 2021.
15[54] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. arXiv:1606.03498 , 2016.
[55] Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Andrew Ilyas, Logan Engstrom, and
Aleksander Madry. Image synthesis with a single (robust) classiﬁer. arXiv:1906.09453 , 2019.
[56] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep
unsupervised learning using nonequilibrium thermodynamics. arXiv:1503.03585 , 2015.
[57] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models.
arXiv:2010.02502 , 2020.
[58] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.
arXiv:2006.09011 , 2020.
[59] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data
distribution. arXiv:arXiv:1907.05600 , 2020.
[60] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon,
and Ben Poole. Score-based generative modeling through stochastic differential equations.
arXiv:2011.13456 , 2020.
[61] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Good-
fellow, and Rob Fergus. Intriguing properties of neural networks. arXiv:1312.6199 , 2013.
[62] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.
Rethinking the inception architecture for computer vision. arXiv:1512.00567 , 2015.
[63] Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder.
arXiv:2007.03898 , 2020.
[64] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex
Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative
model for raw audio. arXiv:1609.03499 , 2016.
[65] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation
learning. arXiv:1711.00937 , 2017.
[66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv:1706.03762 , 2017.
[67] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics.
InProceedings of the 28th international conference on machine learning (ICML-11) , pages
681688. Citeseer, 2011.
[68] Yan Wu, Jeff Donahue, David Balduzzi, Karen Simonyan, and Timothy Lillicrap. Logan: Latent
optimisation for generative adversarial networks. arXiv:1912.00953 , 2019.
[69] Yuxin Wu and Kaiming He. Group normalization. arXiv:1803.08494 , 2018.
[70] Jianwen Xie, Yang Lu, Song-Chun Zhu, and Ying Nian Wu. A theory of generative convnet.
arXiv:1602.03264 , 2016.
[71] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao.
Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop.
arXiv:1506.03365 , 2015.
[72] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and
Dimitris Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative
adversarial networks. arXiv:1612.03242 , 2016.
[73] Ligeng Zhu. Thop. https://github.com/Lyken17/pytorch-OpCounter , 2018.
16A Computational Requirements
Compute is essential to modern machine learning applications, and more compute typically yields
better results. It is thus important to compare our methods compute requirements to competing
methods. In this section, we demonstrate that we can achieve results better than StyleGAN2 and
BigGAN-deep with the same or lower compute budget.
A.1 Throughput
We ﬁrst benchmark the throughput of our models in Table 7. For the theoretical throughput, we
measure the theoretical FLOPs for our model using THOP [ 73], and assume 100% utilization of an
NVIDIA Tesla V100 (120 TFLOPs), while for the actual throughput we use measured wall-clock
time. We include communication time across two machines whenever our training batch size doesnt
ﬁt on a single machine, where each of our machines has 8 V100s.
We ﬁnd that a naive implementation of our models in PyTorch 1.7 is very inefﬁcient, utilizing only
20-30% of the hardware. We also benchmark our optimized version, which use larger per-GPU batch
sizes, fused GroupNorm-Swish and fused Adam CUDA ops. For our ImageNet 128 128 model in
particular, we ﬁnd that we can increase the per-GPU batch size from 4 to 32 while still ﬁtting in GPU
memory, and this makes a large utilization difference. Our implementation is still far from optimal,
and further optimizations should allow us to reach higher levels of utilization.
Model ImplementationBatch Size ThroughputUtilizationper GPU Imgs per V100-sec
6464Theoretical - 182.3 100%
Naive 32 37.0 20%
Optimized 96 74.1 41%
128128Theoretical - 65.2 100%
Naive 4 11.5 18%
Optimized 32 24.8 38%
256256Theoretical - 17.9 100%
Naive 4 4.4 25%
Optimized 8 6.4 36%
64!256Theoretical - 31.7 100%
Naive 4 6.3 20%
Optimized 12 9.5 30%
128!512Theoretical - 8.0 100%
Naive 2 1.9 24%
Optimized 2 2.3 29%
Table 7: Throughput of our ImageNet models, measured in Images per V100-sec.
A.2 Early stopping
In addition, we can train for many fewer iterations while maintaining sample quality superior to
BigGAN-deep. Table 8 and 9 evaluate our ImageNet 128 128 and 256256 models throughout
training. We can see that the ImageNet 128 128 model beats BigGAN-deeps FID (6.02) after 500K
training iterations, only one eighth of the way through training. Similarly, the ImageNet 256 256
model beats BigGAN-deep after 750K iterations, roughly a third of the way through training.
Iterations FID sFID Precision Recall
250K 7.97 6.48 0.80 0.50
500K 5.31 5.97 0.83 0.49
1000K 4.10 5.80 0.81 0.51
2000K 3.42 5.69 0.83 0.53
4360K 3.09 5.59 0.82 0.54
Table 8: Evaluating an ImageNet 128 128 model throughout training (classiﬁer scale 1.0).
17Iterations FID sFID Precision Recall
250K 12.21 6.15 0.78 0.50
500K 7.95 5.51 0.81 0.50
750K 6.49 5.39 0.81 0.50
1000K 5.74 5.29 0.81 0.52
1500K 5.01 5.20 0.82 0.52
1980K 4.59 5.25 0.82 0.52
Table 9: Evaluating an ImageNet 256 256 model throughout training (classiﬁer scale 1.0).
A.3 Compute comparison
Finally, in Table 10 we compare the compute of our models with StyleGAN2 and BigGAN-deep, and
show we can obtain better FIDs with a similar compute budget. For BigGAN-deep, Brock et al. [ 5] do
not explicitly describe the compute requirements for training their models, but rather provide rough
estimates in terms of days on a Google TPUv3 pod [ 20]. We convert their TPU-v3 estimates to V100
days according to 2 TPU-v3 day = 1 V100 day. For StyleGAN2, we use the reported throughput of
25M images over 32 days 13 hour on one V100 for conﬁg-f [ 44]. We note that our classiﬁer training
is relatively lightweight compared to training the generative model.
Model Generator Classiﬁer Total FID sFID Precision Recall
Compute Compute Compute
LSUN Horse 256256
StyleGAN2 [28] 130 3.84 6.46 0.63 0.48
ADM (250K) 116 - 116 2.95 5.94 0.69 0.55
ADM (dropout, 250K) 116 - 116 2.57 6.81 0.71 0.55
LSUN Cat 256256
StyleGAN2 [28] 115 7.25 6.33 0.58 0.43
ADM (dropout, 200K) 92 - 92 5.57 6.69 0.63 0.52
ImageNet 128128
BigGAN-deep [5] 64-128 6.02 7.18 0.86 0.35
ADM-G (4360K) 521 9 530 3.09 5.59 0.82 0.54
ADM-G (450K) 54 9 63 5.67 6.19 0.82 0.49
ImageNet 256256
BigGAN-deep [5] 128-256 6.95 7.36 0.87 0.28
ADM-G (1980K) 916 46 962 4.59 5.25 0.82 0.52
ADM-G (750K) 347 46 393 6.49 5.39 0.81 0.50
ADM-G (750K) 347 14y361 6.68 5.34 0.81 0.51
ADM-G (540K), ADM-U (500K) 329 30 359 3.85 5.86 0.84 0.53
ADM-G (540K), ADM-U (150K) 219 30 249 4.15 6.14 0.82 0.54
ADM-G (200K), ADM-U (150K) 110 10z126 4.93 5.82 0.82 0.52
ImageNet 512512
BigGAN-deep [5] 256-512 8.43 8.13 0.88 0.29
ADM-G (4360K), ADM-U (1050K) 1878 36 1914 3.85 5.86 0.84 0.53
ADM-G (500K), ADM-U (100K) 189 9* 198 7.59 6.84 0.84 0.53
Table 10: Training compute requirements for our diffusion models compared to StyleGAN2 and
BigGAN-deep. Training iterations for each diffusion model are mentioned in parenthesis. Compute
is measured in V100-days.yImageNet 256256 classiﬁer with 150K iterations (instead of 500K).
zImageNet 6464 classiﬁer with batch size 256 (instead of 1024). *ImageNet 128 128 classiﬁer
with batch size 256 (instead of 1024).
18B Detailed Formulation of DDPM
Here, we provide a detailed review of the formulation of Gaussian diffusion models from Ho et al.
[25]. We start by deﬁning our data distribution x0q(x0)and a Markovian noising process qwhich
gradually adds noise to the data to produce noised samples x1throughxT. In particular, each step of
the noising process adds Gaussian noise according to some variance schedule given by t:
q(xtjxt1):=N(xt;p
1txt1;tI) (15)
Ho et al. [ 25] note that we need not apply qrepeatedly to sample from xtq(xtjx0). Instead,
q(xtjx0)can be expressed as a Gaussian distribution. With t:= 1tandt:=Qt
s=0s
q(xtjx0) =N(xt;ptx0;(1t)I) (16)
=ptx0+p
1t; N(0;I) (17)
Here, 1ttells us the variance of the noise for an arbitrary timestep, and we could equivalently
use this to deﬁne the noise schedule instead of t.
Using Bayes theorem, one ﬁnds that the posterior q(xt1jxt;x0)is also a Gaussian with mean
~t(xt;x0)and variance ~tdeﬁned as follows:
~t(xt;x0):=pt1t
1tx0+pt(1t1)
1txt (18)
~t:=1t1
1tt (19)
q(xt1jxt;x0) =N(xt1; ~(xt;x0);~tI) (20)
If we wish to sample from the data distribution q(x0), we can ﬁrst sample from q(xT)and then sample
reverse steps q(xt1jxt)until we reach x0. Under reasonable settings for tandT, the distribution
q(xT)is nearly an isotropic Gaussian distribution, so sampling xTis trivial. All that is left is to
approximate q(xt1jxt)using a neural network, since it cannot be computed exactly when the data
distribution is unknown. To this end, Sohl-Dickstein et al. [ 56] note thatq(xt1jxt)approaches a
diagonal Gaussian distribution as T!1 and correspondingly t!0, so it is sufﬁcient to train a
neural network to predict a mean and a diagonal covariance matrix :
p(xt1jxt):=N(xt1;(xt;t);(xt;t)) (21)
To train this model such that p(x0)learns the true data distribution q(x0), we can optimize the
following variational lower-bound Lvlbforp(x0):
Lvlb:=L0+L1+:::+LT1+LT (22)
L0:=logp(x0jx1) (23)
Lt1:=DKL(q(xt1jxt;x0)jjp(xt1jxt)) (24)
LT:=DKL(q(xTjx0)jjp(xT)) (25)
While the above objective is well-justiﬁed, Ho et al. [ 25] found that a different objective produces
better samples in practice. In particular, they do not directly parameterize (xt;t)as a neural
network, but instead train a model (xt;t)to predictfrom Equation 17. This simpliﬁed objective
is deﬁned as follows:
Lsimple :=Et[1;T];x0q(x0);N(0;I)[jj(xt;t)jj2] (26)
During sampling, we can use substitution to derive (xt;t)from(xt;t):
(xt;t) =1pt
xt1tp1t(xt;t)
(27)
Note thatLsimple does not provide any learning signal for (xt;t). Ho et al. [ 25] ﬁnd that instead of
learning (xt;t), they can ﬁx it to a constant, choosing either tIor~tI. These values correspond
to upper and lower bounds for the true reverse step variance.
19C Nearest Neighbors for Samples
Figure 7: Nearest neighbors for samples from a classiﬁer guided model on ImageNet 256 256. For
each image, the top row is a sample, and the remaining rows are the top 3 nearest neighbors from the
dataset. The top samples were generated with classiﬁer scale 1 and 250 diffusion sampling steps (FID
4.59). The bottom samples were generated with classiﬁer scale 2.5 and 25 DDIM steps (FID 5.44).
Our models achieve their best FID when using a classiﬁer to reduce the diversity of the generations.
One might fear that such a process could cause the model to recall existing images from the training
dataset, especially as the classiﬁer scale is increased. To test this, we looked at the nearest neighbors
(in InceptionV3 [ 62] feature space) for a handful of samples. Figure 7 shows our results, revealing
that the samples are indeed unique and not stored in the training set.
D Effect of Varying the Classiﬁer Scale
Figure 8: Samples when increasing the classiﬁer scale from 0.0 (left) to 5.5 (right). Each row
corresponds to a ﬁxed noise seed. We observe that the classiﬁer drastically changes some images,
while leaving others relatively unaffected.
20E LSUN Diversity Comparison
Figure 9: Samples from StyleGAN2 (or StyleGAN for bedrooms) with truncation 1.0 (left) vs
samples from our diffusion models (middle) and samples from the training set (right).
21F Interpolating Between Dataset Images Using DDIM
The DDIM [ 57] sampling process is deterministic given the initial noise xT, thus giving rise to an
implicit latent space. It corresponds to integrating an ODE in the forward direction, and we can run
the process in reverse to get the latents that produce a given real image. Here, we experiment with
encoding real images into this latent space and then interpolating between them.
Equation 13 for the generative pass in DDIM looks like
xt1xt=pt1hp
1=tp
1=t1
xt+p
1=t11p
1=t1
(xt)i
Thus, in the limit of small steps, we can expect the reversal of this ODE in the forward direction
looks like
xt+1xt=pt+1hp
1=tp
1=t+1
xt+p
1=t+11p
1=t1
(xt)i
We found that this reverse ODE approximation gives latents with reasonable reconstructions, even
with as few as 250 reverse steps. However, we noticed some noise artifacts when reversing all 250
steps, and ﬁnd that reversing the ﬁrst 249 steps gives much better reconstructions. To interpolate
the latents, class embeddings, and classiﬁer log probabilities, we use cos()x0+sin()x1where
sweeps linearly from 0 to
2.
Figures 10athrough 10cshow DDIM latent space interpolations on a class-conditional 256 256
model, while varying the classiﬁer scale. The left and rightmost images are ground truth dataset
examples, and between them are reconstructed interpolations in DDIM latent space (including both
endpoints). We see that the model with no guidance has almost perfect reconstructions due to its high
recall, whereas raising the guidance scale to 2.5 only ﬁnds approximately similar reconstructions.
Figure 10a: DDIM latent reconstructions and interpolations on real images with no classiﬁer guidance.
22Figure 10b: DDIM latent reconstructions and interpolations on real images with classiﬁer scale 1.0.
Figure 10c: DDIM latent reconstructions and interpolations on real images with classiﬁer scale 2.5.
23G Reduced Temperature Sampling
We achieved our best ImageNet samples by reducing the diversity of our models using classiﬁer
guidance. For many classes of generative models, there is a much simpler way to reduce diversity:
reducing the temperature [ 1]. The temperature parameter is typically setup so that = 1:0corre-
sponds to standard sampling, and  <1:0focuses more on high-density samples. We experimented
with two ways of implementing this for diffusion models: ﬁrst, by scaling the Gaussian noise used
for each transition by , and second by dividing (xt)by. The latter implementation makes
sense when thinking about as a re-scaled score function (see Section 4.2), and scaling up the score
function is similar to scaling up classiﬁer gradients.
To measure how temperature scaling affects samples, we experimented with our ImageNet 128 128
model, evaluating FID, Precision, and Recall across different temperatures (Figure 11). We ﬁnd
that two techniques behave similarly, and neither technique provides any substantial improvement in
our evaluation metrics. We also ﬁnd that low temperatures have both low precision and low recall,
indicating that the model is not focusing on modes of the real data distribution. Figure 12 highlights
this effect, indicating that reducing temperature produces blurry, smooth images.
103102
1 - temperature05101520FIDnoise temperature
epsilon temperature
103102
1 - temperature0.40.50.60.7Precision
noise temperature
epsilon temperature
103102
1 - temperature0.450.500.550.600.65Recall
noise temperature
epsilon temperature
Figure 11: The effect of changing temperature for an ImageNet 128 128 model.
Figure 12: Samples at temperature 0.98 with epsilon scaling (left) and noise scaling (right).
24H Conditional Diffusion Process
In this section, we show that conditional sampling can be achieved with a transition operator
proportional to p(xtjxt+1)p(yjxt), wherep(xtjxt+1)approximates q(xtjxt+1)andp(yjxt)
approximates the label distribution for a noised sample xt.
We start by deﬁning a conditional Markovian noising process ^qsimilar toq, and assume that ^q(yjx0)
is a known and readily available label distribution for each sample.
^q(x0):=q(x0) (28)
^q(yjx0):=Known labels per sample (29)
^q(xt+1jxt;y):=q(xt+1jxt) (30)
^q(x1:Tjx0;y):=TY
t=1^q(xtjxt1;y) (31)
While we deﬁned the noising process ^qconditioned on y, we can prove that ^qbehaves exactly like
qwhen not conditioned on y. Along these lines, we ﬁrst derive the unconditional noising operator
^q(xt+1jxt):
^q(xt+1jxt) =Z
y^q(xt+1;yjxt)dy (32)
=Z
y^q(xt+1jxt;y)^q(yjxt)dy (33)
=Z
yq(xt+1jxt)^q(yjxt)dy (34)
=q(xt+1jxt)Z
y^q(yjxt)dy (35)
=q(xt+1jxt) (36)
= ^q(xt+1jxt;y) (37)
Following similar logic, we ﬁnd the joint distribution ^q(x1:Tjx0):
^q(x1:Tjx0) =Z
y^q(x1:T;yjx0)dy (38)
=Z
y^q(yjx0)^q(x1:Tjx0;y)dy (39)
=Z
y^q(yjx0)TY
t=1^q(xtjxt1;y)dy (40)
=Z
y^q(yjx0)TY
t=1q(xtjxt1)dy (41)
=TY
t=1q(xtjxt1)Z
y^q(yjx0)dy (42)
=TY
t=1q(xtjxt1) (43)
=q(x1:Tjx0) (44)
25Using Equation 44, we can now derive ^q(xt):
^q(xt) =Z
x0:t1^q(x0;:::;xt)dx0:t1 (45)
=Z
x0:t1^q(x0)^q(x1;:::;xtjx0)dx0:t1 (46)
=Z
x0:t1q(x0)q(x1;:::;xtjx0)dx0:t1 (47)
=Z
x0:t1q(x0;:::;xt)dx0:t1 (48)
=q(xt) (49)
(50)
Using the identities ^q(xt) =q(xt)and^q(xt+1jxt) =q(xt+1jxt), it is trivial to show via Bayes rule
that the unconditional reverse process ^q(xtjxt+1) =q(xtjxt+1).
One observation about ^qis that it gives rise to a noisy classiﬁcation function, ^q(yjxt). We can show
that this classiﬁcation distribution does not depend on xt+1(a noisier version of xt), a fact which we
will later use:
^q(yjxt;xt+1) = ^q(xt+1jxt;y)^q(yjxt)
^q(xt+1jxt)(51)
= ^q(xt+1jxt)^q(yjxt)
^q(xt+1jxt)(52)
= ^q(yjxt) (53)
(54)
We can now derive the conditional reverse process:
^q(xtjxt+1;y) =^q(xt;xt+1;y)
^q(xt+1;y)(55)
=^q(xt;xt+1;y)
^q(yjxt+1)^q(xt+1)(56)
=^q(xtjxt+1)^q(yjxt;xt+1)^q(xt+1)
^q(yjxt+1)^q(xt+1)(57)
=^q(xtjxt+1)^q(yjxt;xt+1)
^q(yjxt+1)(58)
=^q(xtjxt+1)^q(yjxt)
^q(yjxt+1)(59)
=q(xtjxt+1)^q(yjxt)
^q(yjxt+1)(60)
(61)
The^q(yjxt+1)term can be treated as a constant since it does not depend on xt. We thus want to
sample from the distribution Zq(xtjxt+1)^q(yjxt)whereZis a normalizing constant. We already
have a neural network approximation of q(xtjxt+1), calledp(xtjxt+1), so all that is left is an
approximation of ^q(yjxt). This can be obtained by training a classiﬁer p(yjxt)on noised images xt
derived by sampling from q(xt).
26I Hyperparameters
When choosing optimal classiﬁer scales for our sampler, we swept over [0:5;1;2]for ImageNet
128128 and ImageNet 256 256, and [1;2;3;3:5;4;4:5;5]for ImageNet 512512. For DDIM,
we swept over values [0:5;0:75;1:0;1:25;2]for ImageNet 128128,[0:5;1;1:5;2;2:5;3;3:5]for
ImageNet 256256, and [3;4;5;6;7;9;11]for ImageNet 512512.
Hyperparameters for training the diffusion and classiﬁcation models are in Table 11 and Table 12
respectively. Hyperparameters for guided sampling are in Table 14. Hyperparameters used to train
upsampling models are in Table 13. We train all of our models using Adam [ 29] or AdamW [ 35]
with1= 0:9and2= 0:999. We train in 16-bit precision using loss-scaling [ 38], but maintain
32-bit weights, EMA, and optimizer state. We use an EMA rate of 0.9999 for all experiments. We
use PyTorch [46], and train on NVIDIA Tesla V100s.
For all architecture ablations, we train with batch size 256, and sample using 250 sampling steps.
For our attention heads ablations, we use 128 base channels, 2 residual blocks per resolution, multi-
resolution attention, and BigGAN up/downsampling, and we train the models for 700K iterations. By
default, all of our experiments use adaptive group normalization, except when explicitly ablating for
it.
When sampling with 1000 timesteps, we use the same noise schedule as for training. On ImageNet,
we use the uniform stride from Nichol and Dhariwal [ 43] for 250 step samples and the slightly
different uniform stride from Song et al. [57] for 25 step DDIM.
LSUN ImageNet 64 ImageNet 128 ImageNet 256 ImageNet 512
Diffusion steps 1000 1000 1000 1000 1000
Noise Schedule linear cosine linear linear linear
Model size 552M 296M 422M 554M 559M
Channels 256 192 256 256 256
Depth 2 3 2 2 2
Channels multiple 1,1,2,2,4,4 1,2,3,4 1,1,2,3,4 1,1,2,2,4,4 0.5,1,1,2,2,4,4
Heads 4
Heads Channels 64 64 64 64
Attention resolution 32,16,8 32,16,8 32,16,8 32,16,8 32,16,8
BigGAN up/downsample 3 3 3 3 3
Dropout 0.1 0.1 0.0 0.0 0.0
Batch size 256 2048 256 256 256
Iterations varies* 540K 4360K 1980K 1940K
Learning Rate 1e-4 3e-4 1e-4 1e-4 1e-4
Table 11: Hyperparameters for diffusion models. *We used 200K iterations for LSUN cat, 250K for
LSUN horse, and 500K for LSUN bedroom.
ImageNet 64 ImageNet 128 ImageNet 256 ImageNet 512
Diffusion steps 1000 1000 1000 1000
Noise Schedule cosine linear linear linear
Model size 65M 43M 54M 54M
Channels 128 128 128 128
Depth 4 2 2 2
Channels multiple 1,2,3,4 1,1,2,3,4 1,1,2,2,4,4 0.5,1,1,2,2,4,4
Heads Channels 64 64 64 64
Attention resolution 32,16,8 32,16,8 32,16,8 32,16,8
BigGAN up/downsample 3 3 3 3
Attention pooling 3 3 3 3
Weight decay 0.2 0.05 0.05 0.05
Batch size 1024 256* 256 256
Iterations 300K 300K 500K 500K
Learning rate 6e-4 3e-4* 3e-4 3e-4
Table 12: Hyperparameters for classiﬁcation models. *For our ImageNet 128 128!512512
upsamples, we used a different classiﬁer for the base model, with batch size 1024 and learning rate
6e-5.
27ImageNet 64!256 ImageNet 128!512
Diffusion steps 1000 1000
Noise Schedule linear linear
Model size 312M 309M
Channels 192 192
Depth 2 2
Channels multiple 1,1,2,2,4,4 1,1,2,2,4,4*
Heads 4
Heads Channels 64
Attention resolution 32,16,8 32,16,8
BigGAN up/downsample 3 3
Dropout 0.0 0.0
Batch size 256 256
Iterations 500K 1050K
Learning Rate 1e-4 1e-4
Table 13: Hyperparameters for upsampling diffusion models. *We chose this as an optimization, with
the intuition that a lower-resolution path should be unnecessary for upsampling 128x128 images.
ImageNet 64 ImageNet 128 ImageNet 256 ImageNet 512
Gradient Scale (250 steps) 1.0 0.5 1.0 4.0
Gradient Scale (DDIM, 25 steps) - 1.25 2.5 9.0
Table 14: Hyperparameters for classiﬁer-guided sampling.
28J Using Fewer Sampling Steps on LSUN
We initially found that our LSUN models achieved much better results when sampling with 1000
steps rather than 250 steps, contrary to previous results from Nichol and Dhariwal [ 43]. To address
this, we conducted a sweep over sampling-time noise schedules, ﬁnding that an improved schedule
can largely close the gap. We swept over schedules on LSUN bedrooms, and selected the schedule
with the best FID for use on the other two datasets. Table 15 details the ﬁndings of this sweep, and
Table 16 applies this schedule to three LSUN datasets.
While sweeping over sampling schedules is not as expensive as re-training models from scratch, it
does require a signiﬁcant amount of sampling compute. As a result, we did not conduct an exhaustive
sweep, and superior schedules are likely to exist.
Schedule FID
50;50;50;50;50 2.31
70;60;50;40;30 2.17
90;50;40;40;30 2.10
90;60;50;30;20 2.09
80;60;50;30;30 2.09
90;50;50;30;30 2.07
100;50;40;30;30 2.03
90;60;60;20;20 2.02
Table 15: Results of sweeping over 250 step sampling schedules on LSUN bedrooms. The schedule
is expressed as a sequence of ﬁve integers, where each integer is the number of steps allocated to
one ﬁfth of the diffusion process. The ﬁrst integer corresponding to t2[0;199] and the last to
t2[T200;T1]. Thus, 50;50;50;50;50is a uniform schedule, and 250;0;0;0;0is a schedule
where all timesteps are spent near t= 0.
Schedule FID sFID Prec Rec
LSUN Bedrooms 256 256
1000 steps 1.90 5.59 0.66 0.51
250 steps (uniform) 2.31 6.12 0.65 0.50
250 steps (sweep) 2.02 6.12 0.67 0.50
LSUN Horses 256256
1000 steps 2.57 6.81 0.71 0.55
250 steps (uniform) 3.45 7.55 0.68 0.56
250 steps (sweep) 2.83 7.08 0.69 0.56
LSUN Cat 256256
1000 steps 5.57 6.69 0.63 0.52
250 steps (uniform) 7.03 8.24 0.60 0.53
250 steps (sweep) 5.94 7.43 0.62 0.52
Table 16: Evaluations on LSUN bedrooms, horses, and cats using different sampling schedules. We
ﬁnd that the sweep schedule produces better results than the uniform 250 step schedule on all three
datasets, and mostly bridges the gap to the 1000 step schedule.
29K Samples from ImageNet 512 512
Figure 13: Samples from our best 512 512 model (FID: 3.85). Classes are 1: goldﬁsh, 279: arctic
fox, 323: monarch butterﬂy, 386: african elephant, 130: ﬂamingo, 852: tennis ball.
30Figure 14: Samples from our best 512 512 model (FID: 3.85). Classes are 933: cheeseburger, 562:
fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric.
31Figure 15: Difﬁcult class samples from our best 512 512 model (FID: 3.85). Classes are 432:
bassoon, 468: cab, 424: barbershop, 444: bicycle-built-for-two, 981: ballplayer, 550: espresso maker.
32Figure 16: Samples from our guided 512 512 model using 250 steps with classiﬁer scale 4.0 (FID
7.72). Classes are 1: goldﬁsh, 279: arctic fox, 323: monarch butterﬂy, 386: african elephant, 130:
ﬂamingo, 852: tennis ball.
33Figure 17: Samples from our guided 512 512 model using 250 steps with classiﬁer scale 4.0 (FID
7.72). Classes are 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992:
agaric.
34Figure 18: Random samples from our best ImageNet 512 512 model (FID 3.85).
35Figure 19: Random samples from our guided 512 512 model using 250 steps with classiﬁer scale
4.0 (FID 7.72).
36L Samples from ImageNet 256 256
Figure 20: Samples using our best 256 256 model (FID 3.94). Classes are 1: goldﬁsh, 279: arctic
fox, 323: monarch butterﬂy, 386: african elephant, 130: ﬂamingo, 852: tennis ball, 933: cheeseburger,
562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric
37Figure 21: Samples from our guided 256 256 model using 250 steps with classiﬁer scale 1.0 (FID
4.59). Classes are 1: goldﬁsh, 279: arctic fox, 323: monarch butterﬂy, 386: african elephant, 130:
ﬂamingo, 852: tennis ball, 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90:
lorikeet, 992: agaric
38Figure 22: Samples from our guided 256 256 model using 25 DDIM steps with classiﬁer scale 2.5
(FID 5.44). Classes are 1: goldﬁsh, 279: arctic fox, 323: monarch butterﬂy, 386: african elephant,
130: ﬂamingo, 852: tennis ball, 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90:
lorikeet, 992: agaric
39Figure 23: Random samples from our best 256 256 model (FID 3.94).
40Figure 24: Random samples from our guided 256 256 model using 250 steps with classiﬁer scale
1.0 (FID 4.59).
41M Samples from LSUN
Figure 25: Random samples from our LSUN bedroom model using 1000 sampling steps. (FID 1.90)
42Figure 26: Random samples from our LSUN horse model using 1000 sampling steps. (FID 2.57)
43Figure 27: Random samples from our LSUN cat model using 1000 sampling steps. (FID 5.57)
44
  Mixtral of Experts
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch,
Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour,
Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux,
Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao,
Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed
Abstract
We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language
model. Mixtral has the same architecture as Mistral 7B, with the difference
that each layer is composed of 8 feedforward blocks (i.e. experts). For every
token, at each layer, a router network selects two experts to process the current
state and combine their outputs. Even though each token only sees two experts,
the selected experts can be different at each timestep. As a result, each token
has access to 47B parameters, but only uses 13B active parameters during
inference. Mixtral was trained with a context size of 32k tokens and it outperforms
or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In
particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code
generation, and multilingual benchmarks. We also provide a model fine-
tuned to follow instructions, Mixtral 8x7B  Instruct, that surpasses GPT-3.5
Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B  chat model on human bench-
marks. Both the base and instruct models are released under the Apache 2.0 license.
Code: https://github.com/mistralai/mistral-src
Webpage: https://mistral.ai/news/mixtral-of-experts/
1 Introduction
In this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights,
licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As
it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low
batch-sizes, and higher throughput at large batch-sizes.
Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward
block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router
network chooses two of these groups (the experts) to process the token and combine their output
additively. This technique increases the number of parameters of a model while controlling cost and
latency, as the model only uses a fraction of the total set of parameters per token.
Mixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches
or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,arXiv:2401.04088v1  [cs.LG]  8 Jan 2024Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The
layers output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard
feedforward block as in a vanilla transformer architecture.
Mixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require
multilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments
show that Mixtral is able to successfully retrieve information from its context window of 32k tokens,
regardless of the sequence length and the location of the information in the sequence.
We also present Mixtral 8x7B  Instruct, a chat model fine-tuned to follow instructions using
supervised fine-tuning and Direct Preference Optimization [ 25]. Its performance notably surpasses
that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B  chat model on human evaluation
benchmarks. Mixtral  Instruct also demonstrates reduced biases, and a more balanced sentiment
profile in benchmarks such as BBQ, and BOLD.
We release both Mixtral 8x7B and Mixtral 8x7B  Instruct under the Apache 2.0 license1, free for
academic and commercial usage, ensuring broad accessibility and potential for diverse applications.
To enable the community to run Mixtral with a fully open-source stack, we submitted changes to
the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also
allows the deployment of vLLM endpoints on any instance in the cloud.
2 Architectural details
Parameter Value
dim 4096
n_layers 32
head_dim 128
hidden_dim 14336
n_heads 32
n_kv_heads 8
context_len 32768
vocab_size 32000
num_experts 8
top_k_experts 2
Table 1: Model architecture.Mixtral is based on a transformer architecture [ 31] and uses the same
modifications as described in [ 18], with the notable exceptions that Mix-
tral supports a fully dense context length of 32k tokens, and the feed-
forward blocks are replaced by Mixture-of-Expert layers (Section 2.1).
The model architecture parameters are summarized in Table 1.
2.1 Sparse Mixture of Experts
We present a brief overview of the Mixture of Experts layer (Figure 1).
For a more in-depth overview, see [ 12]. The output of the MoE module
for a given input xis determined by the weighted sum of the outputs
of the expert networks, where the weights are given by the gating
networks output. i.e. given nexpert networks {E0, Ei, ..., E n1}, the
output of the expert layer is given by:
n1X
i=0G(x)iEi(x).
Here, G(x)idenotes the n-dimensional output of the gating network for the i-th expert, and Ei(x)
is the output of the i-th expert network. If the gating vector is sparse, we can avoid computing
the outputs of experts whose gates are zero. There are multiple alternative ways of implementing
G(x)[6,15,35], but a simple and performant one is implemented by taking the softmax over the
Top-K logits of a linear layer [28]. We use
G(x) := Softmax (TopK (xWg)),
where (TopK (ℓ))i:=ℓiifℓiis among the top-K coordinates of logits ℓRnand(TopK (ℓ))i:=
otherwise. The value of K  the number of experts used per token  is a hyper-parameter that modu-
lates the amount of compute used to process each token. If one increases nwhile keeping Kfixed, one
1https://mistral.ai/news/mixtral-of-experts/
2can increase the models parameter count while keeping its computational cost effectively constant.
This motivates a distinction between the models total parameter count (commonly referenced as the
sparse parameter count), which grows with n, and the number of parameters used for processing an
individual token (called the active parameter count), which grows with Kup to n.
MoE layers can be run efficiently on single GPUs with high performance specialized kernels. For
example, Megablocks [ 13] casts the feed-forward network (FFN) operations of the MoE layer as large
sparse matrix multiplications, significantly enhancing the execution speed and naturally handling
cases where different experts get a variable number of tokens assigned to them. Moreover, the
MoE layer can be distributed to multiple GPUs through standard Model Parallelism techniques, and
through a particular kind of partitioning strategy called Expert Parallelism (EP) [ 28]. During the MoE
layers execution, tokens meant to be processed by a specific expert are routed to the corresponding
GPU for processing, and the experts output is returned to the original token location. Note that EP
introduces challenges in load balancing, as it is essential to distribute the workload evenly across the
GPUs to prevent overloading individual GPUs or hitting computational bottlenecks.
In a Transformer model, the MoE layer is applied independently per token and replaces the
feed-forward (FFN) sub-block of the transformer block. For Mixtral we use the same SwiGLU
architecture as the expert function Ei(x)and set K= 2. This means each token is routed to two
SwiGLU sub-blocks with different sets of weights. Taking this all together, the output yfor an input
token xis computed as:
y=n1X
i=0Softmax (Top2 (xWg))iSwiGLU i(x).
This formulation is similar to the GShard architecture [ 21], with the exceptions that we replace all
FFN sub-blocks by MoE layers while GShard replaces every other block, and that GShard uses a
more elaborate gating strategy for the second expert assigned to each token.
3 Results
We compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair
comparison. We measure performance on a wide variety of tasks categorized as follow:
Commonsense Reasoning (0-shot): Hellaswag [ 32], Winogrande [ 26], PIQA [ 3], SIQA [ 27],
OpenbookQA [22], ARC-Easy, ARC-Challenge [8], CommonsenseQA [30]
World Knowledge (5-shot): NaturalQuestions [20], TriviaQA [19]
Reading Comprehension (0-shot): BoolQ [7], QuAC [5]
Math: GSM8K [9] (8-shot) with maj@8 and MATH [17] (4-shot) with maj@4
Code: Humaneval [4] (0-shot) and MBPP [1] (3-shot)
Popular aggregated results: MMLU [ 16] (5-shot), BBH [ 29] (3-shot), and AGI Eval [ 34]
(3-5-shot, English multiple-choice questions only)
Figure 2: Performance of Mixtral and different Llama models on a wide range of benchmarks . All models
were re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mixtral outperforms or
matches Llama 2 70B on all benchmarks. In particular, it is vastly superior in mathematics and code generation.
3ModelActive
ParamsMMLU HellaS WinoG PIQA Arc-e Arc-c NQ TriQA HumanE MBPP Math GSM8K
LLaMA 2 7B 7B 44.4% 77.1% 69.5% 77.9% 68.7% 43.2% 17.5% 56.6% 11.6% 26.1% 3.9% 16.0%
LLaMA 2 13B 13B 55.6% 80.7% 72.9% 80.8% 75.2% 48.8% 16.7% 64.0% 18.9% 35.4% 6.0% 34.3%
LLaMA 1 33B 33B 56.8% 83.7% 76.2% 82.2% 79.6% 54.4% 24.1% 68.5% 25.0% 40.9% 8.4% 44.1%
LLaMA 2 70B 70B 69.9% 85.4% 80.4% 82.6% 79.9% 56.5% 25.4% 73.0% 29.3% 49.8% 13.8% 69.6%
Mistral 7B 7B 62.5% 81.0% 74.2% 82.2% 80.5% 54.9% 23.2% 62.5% 26.2% 50.2% 12.7% 50.0%
Mixtral 8x7B 13B 70.6% 84.4% 77.2% 83.6% 83.1% 59.7% 30.6% 71.5% 40.2% 60.7% 28.4% 74.4%
Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on
almost all popular benchmarks while using 5x fewer active parameters during inference.
Figure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension,
math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B) . Mixtral largely outperforms Llama 2 70B
on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It
is also vastly superior to Llama 2 70B on code and math.
Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2are reported
in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different
categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a
superior performance in code and mathematics benchmarks.
Size and Efficiency. We compare our performance to the Llama 2 family, aiming to understand
Mixtral models efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture-
of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active
parameters, Mixtral is able to outperform Llama 2 70B across most categories.
Note that this analysis focuses on the active parameter count (see Section 2.1), which is directly
proportional to the inference compute cost, but does not consider the memory costs and hardware
utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count,
47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer
introduces additional overhead due to the routing mechanism and due to the increased memory loads
when running more than one expert per device. They are more suitable for batched workloads where
one can reach a good degree of arithmetic intensity.
Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B
compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the
two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller
capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest
GPT-3.5-Turbo model available, gpt-3.5-turbo-1106 .
2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.
4LLaMA 2 70B GPT-3.5 Mixtral 8x7B
MMLU
(MCQ in 57 subjects)69.9% 70.0% 70.6%
HellaSwag
(10-shot)87.1% 85.5% 86.7%
ARC Challenge
(25-shot)85.1% 85.2% 85.8%
WinoGrande
(5-shot)83.2% 81.6% 81.2%
MBPP
(pass@1)49.8% 52.2% 60.7%
GSM-8K
(5-shot)53.6% 57.1% 58.4%
MT Bench
(for Instruct Models)6.86 8.32 8.30
Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2
70B and GPT-3.5 performance on most metrics.
Evaluation Differences. On some benchmarks, there are some differences between our evaluation
protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2)
on TriviaQA, we do not provide Wikipedia contexts.
3.1 Multilingual benchmarks
Compared to Mistral 7B, we significantly upsample the proportion of multilingual data during
pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while
maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B
in French, German, Spanish, and Italian, as shown in Table 4.
Active
ParamsFrench German Spanish Italian
Model Arc-c HellaS MMLU Arc-c HellaS MMLU Arc-c HellaS MMLU Arc-c HellaS MMLU
LLaMA 1 33B 33B 39.3% 68.1% 49.9% 41.1% 63.3% 48.7% 45.7% 69.8% 52.3% 42.9% 65.4% 49.0%
LLaMA 2 70B 70B 49.9% 72.5% 64.3% 47.3% 68.7% 64.2% 50.5% 74.5% 66.0% 49.4% 70.9% 65.1%
Mixtral 8x7B 13B 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9%
Table 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On ARC Challenge, Hellaswag,
and MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and Italian.
3.2 Long range performance
To assess the capabilities of Mixtral to tackle long context, we evaluate it on the passkey retrieval
task introduced in [ 23], a synthetic task designed to measure the ability of the model to retrieve a
passkey inserted randomly in a long prompt. Results in Figure 4 (Left) show that Mixtral achieves a
100% retrieval accuracy regardless of the context length or the position of passkey in the sequence.
Figure 4 (Right) shows that the perplexity of Mixtral on a subset of the proof-pile dataset [ 2] decreases
monotonically as the size of the context increases.
Figure 4: Long range performance of Mixtral. (Left) Mixtral has 100% retrieval accuracy of the Passkey task
regardless of the location of the passkey and length of the input sequence. (Right) The perplexity of Mixtral on
the proof-pile dataset decreases monotonically as the context length increases.
53.3 Bias Benchmarks
Llama 2 70B Mixtral 8x7B
BBQ accuracy 51.5% 56.0%
BOLD sentiment score (avg std)
gender 0.293 0.073 0.323 0.045
profession 0.218 0.073 0.243 0.087
religious_ideology 0.188 0.133 0.144 0.089
political_ideology 0.149 0.140 0.186 0.146
race 0.232 0.049 0.232 0.052
Figure 5: Bias Benchmarks. Compared Llama 2 70B,
Mixtral presents less bias (higher accuracy on BBQ, lower
std on BOLD) and displays more positive sentiment (higher
avg on BOLD).To identify possible flaws to be corrected
by fine-tuning / preference modeling, we
measure the base model performance on
Bias Benchmark for QA (BBQ) [ 24] and
Bias in Open-Ended Language Generation
Dataset (BOLD) [ 10]. BBQ is a dataset
of hand-written question sets that target
attested social biases against nine differ-
ent socially-relevant categories: age, dis-
ability status, gender identity, nationality,
physical appearance, race/ethnicity, religion,
socio-economic status, sexual orientation.
BOLD is a large-scale dataset that consists
of 23,679 English text generation prompts
for bias benchmarking across five domains.
We benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report
the results in Table 5. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark
(56.0% vs 51.5%). For each group in BOLD, a higher average sentiment score means more positive
sentiments and a lower standard deviation indicates less bias within the group. Overall, Mixtral
displays more positive sentiments than Llama 2, with similar variances within each group.
4 Instruction Fine-tuning
We train Mixtral  Instruct using supervised fine-tuning (SFT) on an instruction dataset followed by
Direct Preference Optimization (DPO) [ 25] on a paired feedback dataset. Mixtral  Instruct reaches a
score of 8.30 on MT-Bench [ 33] (see Table 2), making it the best open-weights model as of December
2023. Independent human evaluation conducted by LMSys is reported in Figure 63and shows that
Mixtral  Instruct outperforms GPT-3.5-Turbo, Gemini Pro, Claude-2.1, and Llama 2 70B chat.
Figure 6: LMSys Leaderboard. (Screenshot from Dec 22, 2023) Mixtral 8x7B Instruct v0.1 achieves an Arena
Elo rating of 1121 outperforming Claude-2.1 (1117), all versions of GPT-3.5-Turbo (1117 best), Gemini Pro
(1111), and Llama-2-70b-chat (1077). Mixtral is currently the best open-weights model by a large margin.
3https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard
65 Routing analysis
In this section, we perform a small analysis on the expert selection by the router. In particular,
we are interested to see if during training some experts specialized to some specific domains (e.g.
mathematics, biology, philosophy, etc.).
To investigate this, we measure the distribution of selected experts on different subsets of The Pile
validation dataset [ 14]. Results are presented in Figure 7, for layers 0, 15, and 31 (layers 0 and 31
respectively being the first and the last layers of the model). Surprisingly, we do not observe obvious
patterns in the assignment of experts based on the topic. For instance, at all layers, the distribution of
expert assignment is very similar for ArXiv papers (written in Latex), for biology (PubMed Abstracts),
and for Philosophy (PhilPapers) documents.
Only for DM Mathematics we note a marginally different distribution of experts. This divergence is
likely a consequence of the datasets synthetic nature and its limited coverage of the natural language
spectrum, and is particularly noticeable at the first and last layers, where the hidden states are very
correlated to the input and output embeddings respectively.
This suggests that the router does exhibit some structured syntactic behavior. Figure 8 shows
examples of text from different domains (Python code, mathematics, and English), where each token
is highlighted with a background color corresponding to its selected expert. The figure shows that
words such as self in Python and Question in English often get routed through the same expert
even though they involve multiple tokens. Similarly, in code, the indentation tokens are always
assigned to the same experts, particularly at the first and last layers where the hidden states are more
correlated to the input and output of the model.
We also note from Figure 8 that consecutive tokens are often assigned the same experts. In fact, we
observe some degree of positional locality in The Pile datasets. Table 5 shows the proportion of con-
secutive tokens that get the same expert assignments per domain and layer. The proportion of repeated
00.050.100.150.20layer: 0
00.050.100.150.20layer: 15
0 1 2 3 4 5 6 700.050.100.150.20layer: 31
Expert IDSelection proportion
ArXiv
DM MathematicsGithub
GutenbergPhilPapers
PubMed AbstractsStackExchange
Wikipedia (en)
Figure 7: Proportion of tokens assigned to each expert on different domains from The Pile dataset for
layers 0, 15, and 31. The gray dashed vertical line marks 1/8, i.e. the proportion expected with uniform
sampling. Here, we consider experts that are either selected as a first or second choice by the router. A
breakdown of the proportion of assignments done in each case cane be seen in Figure 9 in the Appendix.
7First choice First or second choice
Layer 0 Layer 15 Layer 31 Layer 0 Layer 15 Layer 31
ArXiv 14.0% 27.9% 22.7% 46.5% 62.3% 52.9%
DM Mathematics 14.1% 28.4% 19.7% 44.9% 67.0% 44.5%
Github 14.9% 28.1% 19.7% 49.9% 66.9% 49.2%
Gutenberg 13.9% 26.1% 26.3% 49.5% 63.1% 52.2%
PhilPapers 13.6% 25.3% 22.1% 46.9% 61.9% 51.3%
PubMed Abstracts 14.2% 24.6% 22.0% 48.6% 61.6% 51.8%
StackExchange 13.6% 27.2% 23.6% 48.2% 64.6% 53.6%
Wikipedia (en) 14.4% 23.6% 25.3% 49.8% 62.1% 51.8%
Table 5: Percentage of expert assignment repetitions. We evaluate the proportion of times the same expert is
assigned to a token iand its following token i+1. We report whether the first chosen expert is the same, or whether
the same expert is observed as first or second choice in consecutive tokens. For reference, the expected proportion
of repetitions in the case of random assignments is1
8= 12.5%for First choice and 16
85
746% for First
and second choice. Repetitions at the first layer are close to random, but are significantly higher at layers 15
and 31. The high number of repetitions shows that expert choice exhibits high temporal locality at these layers.
consecutive assignments is significantly higher than random for higher layers. This has implications
in how one might optimize the model for fast training and inference. For example, cases with high
locality are more likely to cause over-subscription of certain experts when doing Expert Parallelism.
Conversely, this locality can be leveraged for caching, as is done in [ 11]. A more complete view of
these same expert frequency is provided for all layers and across datasets in Figure 10 in the Appendix.
6 Conclusion
In this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the-
art performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem-
ini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each
time step, Mixtral only uses 13B active parameters per token while outperforming the previous best
model using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned mod-
els publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de-
velopment of new techniques and applications that can benefit a wide range of industries and domains.
Figure 8: Text samples where each token is colored with the first expert choice. The selection of experts
appears to be more aligned with the syntax rather than the domain, especially at the initial and final layers.
8Acknowledgements
We thank the CoreWeave and Scaleway teams for technical support as we trained our models. We
are grateful to NVIDIA for supporting us in integrating TensorRT-LLM and Triton and working
alongside us to make a sparse mixture of experts compatible with TensorRT-LLM.
References
[1]Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David
Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large
language models. arXiv preprint arXiv:2108.07732 , 2021.
[2]Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,
Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language
model for mathematics. arXiv preprint arXiv:2310.10631 , 2023.
[3]Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys-
ical commonsense in natural language. In Proceedings of the AAAI conference on artificial
intelligence , pages 74327439, 2020.
[4]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374 , 2021.
[5]Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and
Luke Zettlemoyer. Quac: Question answering in context. arXiv preprint arXiv:1808.07036 ,
2018.
[6]Aidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan
Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified
scaling laws for routed language models. In International Conference on Machine Learning ,
pages 40574086. PMLR, 2022.
[7]Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and
Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.
arXiv preprint arXiv:1905.10044 , 2019.
[8]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,
and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning
challenge. arXiv preprint arXiv:1803.05457 , 2018.
[9]Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to
solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.
[10] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei
Chang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended
language generation. In Proceedings of the 2021 ACM conference on fairness, accountability,
and transparency , pages 862872, 2021.
[11] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with
offloading. arXiv preprint arXiv:2312.17238 , 2023.
[12] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning.
arXiv preprint arXiv:2209.01667 , 2022.
[13] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse
training with mixture-of-experts. arXiv preprint arXiv:2211.15841 , 2022.
[14] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse
text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.
[15] Hussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen,
Rahul Mazumder, Lichan Hong, and Ed Chi. Dselect-k: Differentiable selection in the mixture
of experts with applications to multi-task learning. Advances in Neural Information Processing
Systems , 34:2933529347, 2021.
9[16] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and
Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint
arXiv:2009.03300 , 2020.
[17] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn
Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.
arXiv preprint arXiv:2103.03874 , 2021.
[18] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile
Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825 , 2023.
[19] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large
scale distantly supervised challenge dataset for reading comprehension. arXiv preprint
arXiv:1705.03551 , 2017.
[20] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris
Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a
benchmark for question answering research. Transactions of the Association for Computational
Linguistics , pages 453466, 2019.
[21] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with condi-
tional computation and automatic sharding. arXiv preprint arXiv:2006.16668 , 2020.
[22] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789 ,
2018.
[23] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context
length for transformers. arXiv preprint arXiv:2305.16300 , 2023.
[24] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thomp-
son, Phu Mon Htut, and Samuel R Bowman. Bbq: A hand-built bias benchmark for question
answering. arXiv preprint arXiv:2110.08193 , 2021.
[25] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and
Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model.
arXiv preprint arXiv:2305.18290 , 2023.
[26] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An
adversarial winograd schema challenge at scale. Communications of the ACM , pages 99106,
2021.
[27] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Com-
monsense reasoning about social interactions. arXiv preprint arXiv:1904.09728 , 2019.
[28] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts
layer. arXiv preprint arXiv:1701.06538 , 2017.
[29] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won
Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei.
Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint
arXiv:2210.09261 , 2022.
[30] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A ques-
tion answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937 ,
2018.
[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems , 30, 2017.
[32] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a
machine really finish your sentence? arXiv preprint arXiv:1905.07830 , 2019.
[33] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and
chatbot arena. arXiv preprint arXiv:2306.05685 , 2023.
10[34] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,
Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation
models. arXiv preprint arXiv:2304.06364 , 2023.
[35] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai,
Quoc V Le, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in
Neural Information Processing Systems , 35:71037114, 2022.
1100.10.20.3Layer 0 -- Either choice
00.10.20.3Layer 0 -- First choice
00.10.20.3Layer 0 -- Second choice
00.10.20.3Layer 15 -- Either choice
00.10.20.3Layer 15 -- First choice
00.10.20.3Layer 15 -- Second choice
00.10.20.3Layer 31 -- Either choice
00.10.20.3Layer 31 -- First choice
0 1 2 3 4 5 6 700.10.20.3Layer 31 -- Second choice
Expert IDSelection proportion
ArXiv
DM MathematicsGithub
GutenbergPhilPapers
PubMed AbstractsStackExchange
Wikipedia (en)Figure 9: Proportion of tokens assigned to each expert on different subsets from The Pile dataset, separated
by whether the expert was selected as first or second choice, or either. The Either choice case is equivalent
to Figure 7. The gray dashed vertical line marks1
8, i.e. the proportion expected with uniform sampling.
120.150.200.250.300.35First choice
0 10 20 300.50.60.7First or second choice
LayerProportion of repeated assignmentssource
ArXiv
DM Mathematics
Github
Gutenberg
PhilPapers
PubMed Abstracts
StackExchange
Wikipedia (en)Figure 10: Repeated consecutive assignments per MoE layer. Repeated assignments occur a lot more
often than they would with uniform assignments (materialized by the dashed lines). Patterns are similar across
datasets with less repetitions for DM Mathematics.
13
  1
Long-term Recurrent Convolutional Networks for
Visual Recognition and Description
Jeff Donahue, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, Sergio Guadarrama,
Kate Saenko, Trevor Darrell
Abstract 
Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which
are also recurrent are effective for tasks involving sequences, visual and otherwise. We describe a class of recurrent convolutional
architectures which is end-to-end trainable and suitable for large-scale visual understanding tasks, and demonstrate the value of these
models for activity recognition, image captioning, and video description. In contrast to previous models which assume a ﬁxed visual
representation or perform simple temporal averaging for sequential processing, recurrent convolutional models are doubly deep in
that they learn compositional representations in space and time. Learning long-term dependencies is possible when nonlinearities are
incorporated into the network state updates. Differentiable recurrent models are appealing in that they can directly map variable-length
inputs (e.g., videos) to variable-length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be
optimized with backpropagation. Our recurrent sequence models are directly connected to modern visual convolutional network
models and can be jointly trained to learn temporal dynamics and convolutional perceptual representations. Our results show that such
models have distinct advantages over state-of-the-art models for recognition or generation which are separately deﬁned or optimized.
F
1 I NTRODUCTION
Recognition and description of images and videos is
a fundamental challenge of computer vision. Dramatic
progress has been achieved by supervised convolutional
neural network (CNN) models on image recognition tasks,
and a number of extensions to process video have been
recently proposed. Ideally, a video model should allow pro-
cessing of variable length input sequences, and also provide
for variable length outputs, including generation of full-
length sentence descriptions that go beyond conventional
one-versus-all prediction tasks. In this paper we propose
Long-term Recurrent Convolutional Networks (LRCNs), a class
of architectures for visual recognition and description which
combines convolutional layers and long-range temporal re-
cursion and is end-to-end trainable (Figure 1). We instanti-
ate our architecture for speciﬁc video activity recognition,
image caption generation, and video description tasks as
described below.
Research on CNN models for video processing has
considered learning 3D spatio-temporal ﬁlters over raw
sequence data [1], [2], and learning of frame-to-frame rep-
resentations which incorporate instantaneous optic ﬂow or
trajectory-based models aggregated over ﬁxed windows
or video shot segments [3], [4]. Such models explore two
extrema of perceptual time-series representation learning:
either learn a fully general time-varying weighting, or apply
J. Donahue, L. A. Hendricks, M. Rohrbach, S. Guadarrama, and T. Darrell
are with the Department of Electrical Engineering and Computer Science,
UC Berkeley, Berkeley, CA.
M. Rohrbach and T. Darrell are additionally afﬁliated with the Interna-
tional Computer Science Institute, Berkeley, CA.
S. Venugopalan is with the Department of Computer Science, UT Austin,
Austin, TX.
K. Saenko is with the Department of Computer Science, UMass Lowell,
Lowell, MA.
Manuscript received November 30, 2015.
y1
y2
yTOutput 
CNN 
CNN 
CNN Visual 
Features Input 
LSTM 
LSTM 
LSTM Sequence 
Learning Fig. 1. We propose Long-term Recurrent Convolutional Networks (LR-
CNs), a class of architectures leveraging the strengths of rapid progress
in CNNs for visual recognition problems, and the growing desire to
apply such models to time-varying inputs and outputs. LRCN processes
the (possibly) variable-length visual input (left) with a CNN (middle-
left), whose outputs are fed into a stack of recurrent sequence models
(LSTMs , middle-right), which ﬁnally produce a variable-length prediction
(right). Both the CNN and LSTM weights are shared across time, result-
ing in a representation that scales to arbitrarily long sequences.
simple temporal pooling. Following the same inspiration
that motivates current deep convolutional models, we ad-
vocate for video recognition and description models which
are also deep over temporal dimensions; i.e., have temporal
recurrence of latent variables. Recurrent Neural Network
(RNN) models are deep in time  explicitly so when
unrolled  and form implicit compositional representationsarXiv:1411.4389v4  [cs.CV]  31 May 20162
in the time domain. Such deep models predated deep
spatial convolution models in the literature [5], [6].
The use of RNNs in perceptual applications has been ex-
plored for many decades, with varying results. A signiﬁcant
limitation of simple RNN models which strictly integrate
state information over time is known as the vanishing
gradient effect: the ability to backpropagate an error signal
through a long-range temporal interval becomes increas-
ingly difﬁcult in practice. Long Short-Term Memory (LSTM)
units, ﬁrst proposed in [7], are recurrent modules which
enable long-range learning. LSTM units have hidden state
augmented with nonlinear mechanisms to allow state to
propagate without modiﬁcation, be updated, or be reset,
using simple learned gating functions. LSTMs have recently
been demonstrated to be capable of large-scale learning of
speech recognition [8] and language translation models [9],
[10].
We show here that convolutional networks with re-
current units are generally applicable to visual time-series
modeling, and argue that in visual tasks where static or ﬂat
temporal models have previously been employed, LSTM-
style RNNs can provide signiﬁcant improvement when
ample training data are available to learn or reﬁne the rep-
resentation. Speciﬁcally, we show that LSTM type models
provide for improved recognition on conventional video
activity challenges and enable a novel end-to-end optimiz-
able mapping from image pixels to sentence-level natural
language descriptions. We also show that these models
improve generation of descriptions from intermediate visual
representations derived from conventional visual models.
We instantiate our proposed architecture in three ex-
perimental settings (Figure 3). First, we show that directly
connecting a visual convolutional model to deep LSTM
networks, we are able to train video recognition models
that capture temporal state dependencies (Figure 3 left;
Section 4). While existing labeled video activity datasets
may not have actions or activities with particularly com-
plex temporal dynamics, we nonetheless observe signiﬁcant
improvements on conventional benchmarks.
Second, we explore end-to-end trainable image to sen-
tence mappings. Strong results for machine translation
tasks have recently been reported [9], [10]; such models
are encoder-decoder pairs based on LSTM networks. We
propose a multimodal analog of this model, and describe an
architecture which uses a visual convnet to encode a deep
state vector, and an LSTM to decode the vector into a natural
language string (Figure 3 middle; Section 5). The resulting
model can be trained end-to-end on large-scale image and
text datasets, and even with modest training provides com-
petitive generation results compared to existing methods.
Finally, we show that LSTM decoders can be driven
directly from conventional computer vision methods which
predict higher-level discriminative labels, such as the se-
mantic video role tuple predictors in [11] (Figure 3, right;
Section 6). While not end-to-end trainable, such models offer
architectural and performance advantages over previous
statistical machine translation-based approaches.
We have realized a generic framework for recurrent
models in the widely adopted deep learning framework
Caffe [12], including ready-to-use implementations of RNN
and LSTM units. (See http://jeffdonahue.com/lrcn/.)
+
σσ σ
xt
ht-1
ht = ztOutput 
Gate Input 
Gate 
Forget Gate Input Modulation Gate LSTM Unit 
ϕxt
ht-1ht
Output ztRNN Unit 
σσ
ϕ
ftit
gtot
ctct-1Fig. 2. A diagram of a basic RNN cell (left) and an LSTM memory
cell (right) used in this paper (from [13], a slight simpliﬁcation of the
architecture described in [14], which was derived from the LSTM initially
proposed in [7]).
2 B ACKGROUND : RECURRENT NETWORKS
Traditional recurrent neural networks (RNNs, Figure 2, left)
model temporal dynamics by mapping input sequences to
hidden states, and hidden states to outputs via the following
recurrence equations (Figure 2, left):
ht=g(Wxhxt+Whhht1+bh)
zt=g(Whzht+bz)
wheregis an element-wise non-linearity, such as a sigmoid
or hyperbolic tangent, xtis the input, ht2RNis the hidden
state withNhidden units, and ztis the output at time t.
For a length Tinput sequencehx1;x2;:::;x Ti, the updates
above are computed sequentially as h1(lettingh0= 0),z1,
h2,z2, ...,hT,zT.
Though RNNs have proven successful on tasks such
as speech recognition [15] and text generation [16], it can
be difﬁcult to train them to learn long-term dynamics,
likely due in part to the vanishing and exploding gradients
problem [7] that can result from propagating the gradients
down through the many layers of the recurrent network,
each corresponding to a particular time step. LSTMs provide
a solution by incorporating memory units that explicitly
allow the network to learn when to forget previous hid-
den states and when to update hidden states given new
information. As research on LSTMs has progressed, hidden
units with varying connections within the memory unit
have been proposed. We use the LSTM unit as described
in [13] (Figure 2, right), a slight simpliﬁcation of the one
described in [8], which was derived from the original LSTM
unit proposed in [7]. Letting (x) = (1 +ex)1be the
sigmoid non-linearity which squashes real-valued inputs to
a[0;1]range, and letting tanh(x) =exex
ex+ex= 2(2x)1
be the hyperbolic tangent non-linearity, similarly squashing
its inputs to a [1;1]range, the LSTM updates for time step
tgiven inputs xt,ht1, andct1are:
it=(Wxixt+Whiht1+bi)
ft=(Wxfxt+Whfht1+bf)
ot=(Wxoxt+Whoht1+bo)
gt= tanh(Wxcxt+Whcht1+bc)
ct=ftct1+itgt
ht=ottanh(ct)3
xydenotes the element-wise product of vectors xandy.
In addition to a hidden unit ht2RN, the LSTM includes
an input gate it2RN, forget gate ft2RN, output gate
ot2RN, input modulation gate gt2RN, and memory cell
ct2RN. The memory cell unit ctis a sum of two terms: the
previous memory cell unit ct1which is modulated by ft,
andgt, a function of the current input and previous hidden
state, modulated by the input gate it. Becauseitandftare
sigmoidal, their values lie within the range [0;1], andit
andftcan be thought of as knobs that the LSTM learns
to selectively forget its previous memory or consider its
current input. Likewise, the output gate otlearns how much
of the memory cell to transfer to the hidden state. These
additional cells seem to enable the LSTM to learn complex
and long-term temporal dynamics for a wide variety of
sequence learning and prediction tasks. Additional depth
can be added to LSTMs by stacking them on top of each
other, using the hidden state h(1)
t of the LSTM in layer
1as the input to the LSTM in layer .
Recently, LSTMs have achieved impressive results on
language tasks such as speech recognition [8] and ma-
chine translation [9], [10]. Analogous to CNNs, LSTMs are
attractive because they allow end-to-end ﬁne-tuning. For
example, [8] eliminates the need for complex multi-step
pipelines in speech recognition by training a deep bidirec-
tional LSTM which maps spectrogram inputs to text. Even
with no language model or pronunciation dictionary, the
model produces convincing text translations. [9] and [10]
translate sentences from English to French with a multi-
layer LSTM encoder and decoder. Sentences in the source
language are mapped to a hidden state using an encoding
LSTM, and then a decoding LSTM maps the hidden state to
a sequence in the target language. Such an encoder-decoder
scheme allows an input sequence of arbitrary length to
be mapped to an output sequence of different length. The
sequence-to-sequence architecture for machine translation
circumvents the need for language models.
The advantages of LSTMs for modeling sequential data
in vision problems are twofold. First, when integrated with
current vision systems, LSTM models are straightforward
to ﬁne-tune end-to-end. Second, LSTMs are not conﬁned to
ﬁxed length inputs or outputs allowing simple modeling
for sequential data of varying lengths, such as text or video.
We next describe a uniﬁed framework to combine recurrent
models such as LSTMs with deep convolutional networks
to form end-to-end trainable networks capable of complex
visual and sequence prediction tasks.
3 L ONG-TERM RECURRENT CONVOLUTIONAL
NETWORK (LRCN) MODEL
This work proposes a Long-term Recurrent Convolutional
Network (LRCN) model combining a deep hierarchical vi-
sual feature extractor (such as a CNN) with a model that can
learn to recognize and synthesize temporal dynamics for
tasks involving sequential data (inputs or outputs), visual,
linguistic, or otherwise. Figure 1 depicts the core of our
approach. LRCN works by passing each visual input xt
(an image in isolation, or a frame from a video) through
a feature transformation V(:)with parameters V, usually
a CNN, to produce a ﬁxed-length vector representationV(xt). The outputs of Vare then passed into a recurrent
sequence learning module.
In its most general form, a recurrent model has param-
etersW, and maps an input xtand a previous time step
hidden state ht1to an output ztand updated hidden state
ht. Therefore, inference must be run sequentially (i.e., from
top to bottom, in the Sequence Learning box of Figure 1), by
computing in order: h1=fW(x1;h0) =fW(x1;0), then
h2=fW(x2;h1), etc., up to hT. Some of our models stack
multiple LSTMs atop one another as described in Section 2.
To predict a distribution P(yt)over outcomes yt2C
(whereCis a discrete, ﬁnite set of outcomes) at time step
t, the outputs zt2Rdzof the sequential model are passed
through a linear prediction layer ^yt=Wzzt+bz, where
Wz2RjCjdzandbz2RjCjare learned parameters. Finally,
the predicted distribution P(yt)is computed by taking the
softmax of ^yt:P(yt=c) = softmax(^ yt) =exp(^yt;c)P
c02Cexp(^yt;c0).
The success of recent deep models for object recogni-
tion [17], [18], [19] suggests that strategically composing
many layers of non-linear functions can result in powerful
models for perceptual problems. For large T, the above
recurrence indicates that the last few predictions from a
recurrent network with Ttime steps are computed by a very
deep (Tlayer) non-linear function, suggesting that the
resulting recurrent model may have similar representational
power to a Tlayer deep network. Critically, however, the
sequence models weights Ware reused at every time step,
forcing the model to learn generic time step-to-time step
dynamics (as opposed to dynamics conditioned on t, the
sequence index) and preventing the parameter size from
growing in proportion to the maximum sequence length.
In most of our experiments, the visual feature transfor-
mationcorresponds to the activations in some layer of
a deep CNN. Using a visual transformation V(:)which
is time-invariant and independent at each time step has the
important advantage of making the expensive convolutional
inference and training parallelizable over all time steps of
the input, facilitating the use of fast contemporary CNN
implementations whose efﬁciency relies on independent
batch processing, and end-to-end optimization of the visual
and sequential model parameters VandW.
We consider three vision problems (activity recognition,
image description and video description), each of which
instantiates one of the following broad classes of sequential
learning tasks:
1) Sequential input, static output (Figure 3, left):
hx1;x2;:::;x Ti7!y. The visual activity recognition
problem can fall under this umbrella, with videos
of arbitrary length Tas input, but with the goal
of predicting a single label like running orjumping
drawn from a ﬁxed vocabulary.
2) Static input, sequential output (Figure 3, middle):
x7!hy1;y2;:::;y Ti. The image captioning problem
ﬁts in this category, with a static (non-time-varying)
image as input, but a much larger and richer label
space consisting of sentences of any length.
3) Sequential input and output (Figure 3, right):
hx1;x2;:::;x Ti7!hy1;y2;:::;y T0i. In tasks such as
video description, both the visual input and output
are time-varying, and in general the number of4
Activity Recognition 
Sequences in the Input 
Image Captioning 
Sequences in the Output Video Description 
Sequences in the Input and Output 
<BOS> 
 HighJump 
CNN 
CNN CNN 
LSTM LSTM LSTM LSTM LSTM LSTM LSTM 
LSTM LSTM LSTM LSTM 
A man runs <EOS> 
LSTM LSTM LSTM LSTM LSTM 
A man jumps high <BOS> <EOS> Average CNN CRF 
LSTM LSTM LSTM LSTM LSTM 
Fig. 3. Task-speciﬁc instantiations of our LRCN model for activity recognition, image description, and video description.
input and output time steps may differ (i.e., we may
haveT6=T0). In video description, for example, the
number of frames in the video should not constrain
the length of (number of words in) the natural
language description.
In the previously described generic formulation of re-
current models, each instance has Tinputshx1;x2;:::;x Ti
andToutputshy1;y2;:::;y Ti. Note that this formulation
does not align cleanly with any of the three problem classes
described above  in the ﬁrst two classes, either the input
or output is static, and in the third class, the input length
Tneed not match the output length T0. Hence, we describe
how we adapt this formulation in our hybrid model to each
of the above three problem settings.
With sequential inputs and static outputs (class 1), we
take a late-fusion approach to merging the per-time step
predictionshy1;y2;:::;y Tiinto a single prediction yfor the
full sequence. With static inputs xand sequential outputs
(class 2), we simply duplicate the input xat allTtime
steps:8t2f1;2;:::;Tg:xt:=x. Finally, for a sequence-
to-sequence problem with (in general) different input and
output lengths (class 3), we take an encoder-decoder
approach, as proposed for machine translation by [9], [20].
In this approach, one sequence model, the encoder , maps
the input sequence to a ﬁxed-length vector, and another se-
quence model, the decoder , unrolls this vector to a sequential
output of arbitrary length. Under this type of model, a run
of the full system on one instance occurs over T+T01time
steps. For the ﬁrst Ttime steps, the encoder processes the
inputx1;x2;:::;x T, and the decoder is inactive until time
stepT, when the encoders output is passed to the decoder,
which in turn predicts the ﬁrst output y1. For the latter T01
time steps, the decoder predicts the remainder of the out-
puty2;y3;:::;y T0with the encoder inactive. This encoder-
decoder approach, as applied to the video description task,
is depicted in Section 6, Figure 5 (left).
Under the proposed system, the parameters (V;W )
of the models visual and sequential components can
be jointly optimized by maximizing the likelihood of
the ground truth outputs ytat each time step t, con-
ditioned on the input data and labels up to that point
(x1:t;y1:t1). In particular, for a training set Dof labeled
sequences (xt;yt)T
t=12D , we optimize parameters (V;W )
to minimize the expected negative log likelihood of asequence sampled from the training set L(V;W;D) =
1
jDjP
(xt;yt)T
t=12DPT
t=1logP(ytjx1:t;y1:t1;V;W ).
One of the most appealing aspects of the described sys-
tem is the ability to learn the parameters end-to-end, such
that the parameters Vof the visual feature extractor learn
to pick out the aspects of the visual input that are relevant
to the sequential classiﬁcation problem. We train our LRCN
models using stochastic gradient descent, with backprop-
agation used to compute the gradient rV;WL(V;W; ~D)of
the objectiveLwith respect to all parameters (V;W )over
minibatches ~DD sampled from the training dataset D.
We next demonstrate the power of end-to-end trainable
hybrid convolutional and recurrent networks by exploring
three applications: activity recognition, image captioning,
and video description.
4 A CTIVITY RECOGNITION
Activity recognition is an instance of the ﬁrst class of se-
quential learning tasks described above: each frame in a
lengthTsequence is the input to a single convolutional
network (i.e., the convnet weights are tied across time). We
consider both RGB and ﬂow as inputs to our recognition
system. Flow is computed with [21] and transformed into a
ﬂow image by scaling and shifting xandyﬂow values to
a range of [128;+128] . A third channel for the ﬂow image
is created by calculating the ﬂow magnitude.
During training, videos are resized to 240320and we
augment our data by using 227227 crops and mirroring.
Additionally, we train the LRCN networks with video clips
of 16 frames, even though the UCF101 videos are generally
much longer (on the order of 100 frames when extracting
frames at 30 FPS). Training on shorter video clips can be
seen as analogous to training on image crops and is a useful
method of data augmentation. LRCN is trained to predict
the videos activity class at each time step. To produce a
single label prediction for an entire video clip, we average
the label probabilities  the outputs of the networks softmax
layer  across all frames and choose the most probable label.
At test time, we extract 16 frame clips with a stride of 8
frames from each video and average across all clips from a
single video.
The CNN base of LRCN in our activity recognition
experiments is a hybrid of the CaffeNet [12] reference model
(a minor variant of AlexNet [17]) and the network used5
by Zeiler & Fergus [22]. The network is pre-trained on
the 1.2M image ILSVRC-2012 [23] classiﬁcation training
subset of the ImageNet [24] dataset, giving the network a
strong initialization to facilitate faster training and avoid
overﬁtting to the relatively small video activity recognition
datasets. When classifying center crops, the top-1 classiﬁca-
tion accuracy is 60.2% and 57.4% for the hybrid and CaffeNet
reference models, respectively.
We compare LRCN to a single frame baseline model.
In our baseline model, Tvideo frames are individually
classiﬁed by a CNN. As in the LSTM model, whole video
classiﬁcation is done by averaging scores across all video
frames.
4.1 Evaluation
We evaluate our architecture on the UCF101 dataset [25]
which consists of over 12,000 videos categorized into 101
human action classes. The dataset is split into three splits,
with just under 8,000 videos in the training set for each split.
We explore various hyperparameters for the LRCN activ-
ity recognition architecture. To explore different variants, we
divide the ﬁrst training split of UCF101 into a smaller train-
ing set (6,000 videos) and a validation set ( 3,000 videos).
We ﬁnd that the most inﬂuential hyperparameters include
the number of hidden units in the LSTM and whether fc6
orfc7features are used as input to the LSTM. We compare
networks with 256,512, and 1024 LSTM hidden units. When
using ﬂow as an input, more hidden units leads to better
peformance with 1024 hidden units yielding a 1.7% boost in
accuracy in comparison to a network with 256 hidden units
on our validation set. In contrast, for networks with RGB
input, the number of hidden units has little impact on the
performance of the model. We thus use 1024 hidden units
for ﬂow inputs, and 256 for RGB inputs. We ﬁnd that using
fc6as opposed to fc7features improves accuracy when
using ﬂow as input on our validation set by 1%. When using
RGB images as input, the difference between using fc6or
fc7features is quite small; using fc6features only increases
accuracy by 0.2%. Because both models perform better with
fc6features, we train our ﬁnal models using fc6features
(denoted by LRCN- fc6). We also considered subsampling
the frames input to the LSTM, but found that this hurts
performance compared with using all frames. Additionally,
when training the LRCN network end-to-end, we found that
aggressive dropout ( 0:9) was needed to avoid overﬁtting.
Table 1 reports the average accuracy across the three
standard test splits of UCF101. Columns 2-3, compare video
classiﬁcation of LRCN against the baseline single frame
architecture for both RGB and ﬂow inputs. LRCN yields the
best results for both RGB and ﬂow and improves upon the
baseline network by 0.83% and 2.91% respectively. RGB and
ﬂow networks can be combined by computing a weighted
average of network scores as proposed in [4]. Like [4],
we report two weighted averages of the predictions from
the RGB and ﬂow networks in Table 1 (right). Since the
ﬂow network outperforms the RGB network, weighting the
ﬂow network higher unsurprisingly leads to better accuracy.
In this case, LRCN outperforms the baseline single-frame
model by 3.40%.
Table 2 compares LRCNs accuracy with the single frame
baseline model for individual classes on Split 1 of UCF101.Single Input Type Weighted Average
Model RGB Flow 1=2;1=2 1=3;2=3
Single frame 67.37 74.37 75.46 78.94
LRCN-fc 6 68.20 77.28 80.90 82.34
TABLE 1
Activity recognition: Comparing single frame models to LRCN networks
for activity recognition on the UCF101 [25] dataset, with RGB and ﬂow
inputs. Average values across all three splits are shown. LRCN
consistently and strongly outperforms a model based on predictions
from the underlying convolutional network architecture alone.
Label  Label 
BoxingPunchingBag 40.82 BoxingSpeedBag -16.22
HighJump 29.73 Mixing -15.56
JumpRope 28.95 Knitting -14.71
CricketShot 28.57 Typing -13.95
Basketball 28.57 Skiing -12.50
WallPushups 25.71 BaseballPitch -11.63
Nunchucks 22.86 BrushingTeeth -11.11
ApplyEyeMakeup 22.73 Skijet -10.71
HeadMassage 21.95 Haircut -9.10
Drumming 17.78 TennisSwing -8.16
TABLE 2
Activity recognition: comparison of improvement in LRCNs per-class
recognition accuracy versus the single-frame baseline. Here we report
results on all three splits of UCF101 (only results on the ﬁrst split were
presented in the paper). is the difference between LRCNs accuracy
and the single-frame models accuracy.
For the majority of classes, LRCN improves performance
over the single frame model. Though LRCN performs worse
on some classes including Knitting and Mixing , in general
when LRCN performs worse, the loss in accuracy is not
as substantial as the gain in accuracy for classes like Box-
ingPunchingBag and HighJump . Consequently, accuracy is
higher overall.
Table 3 compares accuracies for the LRCN ﬂow and
LRCN RGB models for individual classes on Split 1 of
UCF101. Note that for some classes the LRCN ﬂow model
outperforms the LRCN RGB model and vice versa. One
explanation is that activities which are better classiﬁed by
the LRCN RGB model are best determined by which objects
are present in the scene, while activities which are better
classiﬁed by the LRCN ﬂow model are best classiﬁed by the
kind of motion in the scene. For example, activity classes
likeTyping are highly correlated with the presence of certain
objects, such as a keyboard, and are thus best learned by the
LRCN RGB model. Other activities such as SoccerJuggling
include more generic objects which are frequently seen
in other activities (soccer balls, people) and are thus best
identiﬁed from class-speciﬁc motion cues. Because RGB and
ﬂow signals are complementary, the best models take both
into account.
LRCN shows clear improvement over the baseline
single-frame system and is comparable to accuracy achieved
by other deep models. [4] report the results on UCF101
by computing a weighted average between ﬂow and RGB
networks and achieve 87.6%. [3] reports 65.4% accuracy on
UCF101, which is substantially lower than LRCN.
5 I MAGE CAPTIONING
In contrast to activity recognition, the static image caption-
ing task requires only a single invocation of a convolutional
network since the input consists of a single image. At each
time step, both the image features and the previous word6
Label  Label 
BoxingPunchingBag 57.14 Typing -44.19
PushUps 53.33 TennisSwing -42.86
JumpRope 50.00 FieldHockeyPenalty -32.50
SoccerJuggling 48.72 BrushingTeeth -30.56
HandstandWalking 44.12 CuttingInKitchen -30.30
Basketball 40.00 Skijet -28.57
BodyWeightSquats 38.46 Mixing -26.67
Lunges 37.84 Skiing -25.00
Nunchucks 34.29 Knitting -20.59
WallPushups 34.29 FloorGymnastics -19.44
TABLE 3
Activity recognition: comparison of per-class recognition accuracy
between the ﬂow and RGB LRCN models. is the difference between
LRCN ﬂow accuracy and LRCN RGB accuracy.
are provided as inputs to the sequence model, in this case a
stack of LSTMs (each with 1000 hidden units), which is used
to learn the dynamics of the time-varying output sequence,
natural language.
At time step t, the input to the bottom-most LSTM is the
embedded word from the previous time step yt1. Input
words are encoded as one-hot vectors: vectors y2RK
with a single non-zero component yi= 1 denoting the ith
word in the vocabulary, where Kis the number of words
in the vocabulary, plus one additional entry for the <BOS>
(beginning of sequence) token which is always taken as y0,
the previous word at the ﬁrst time step ( t= 1). These
one-hot vectors are then projected into an embedding space
with dimension deby multiplication Weytwith a learned
parameter matrix We2RdeK. The result of a matrix-
vector multiplication with a one-hot vector is the column
of the matrix corresponding to the index of the single non-
zero component of the one-hot vector. Wecan therefore be
thought of as a lookup table, mapping each of the K
words in the vocabulary to a de-dimensional vector.
The visual feature representation V(x)of the image x
may be input to the sequence model  a stack of LLSTMs
 by concatenating it at each time step either with (1) the
embedded previous word Weyt1and fed into the ﬁrst
LSTM of the stack, or (2) the hidden state h(1)
t output
from LSTM 1and fed into LSTM , for some22;:::;L .
These choices are depicted in Figure 4. We refer to the
latter choice as factored, as it forces a sort of separation
of responsibilities by blinding the ﬁrst 1LSTMs and
forcing all of the capacity of their hidden states at time step
tto represent only the partial caption y1:t1independent
of the visual input, while the LSTMs starting from are
responsible for fusing the lower layers hidden state given
by the partial caption with the visual feature representation
V(x)to produce a joint hidden state representation h()
t
of the visual and language inputs up to time step tfrom
which the next word ytcan be predicted. In the factored
case, the hidden state htfor the lower layers is conditionally
independent of the image xgiven the partial caption y1:t1.
The outputs of the ﬁnal LSTM in the stack are the
inputs to a learned linear prediction layer with a softmax
producing a distribution P(ytjy1:t1;V(x))over words yt
in the models vocabulary, including the <EOS> token de-
noting the end of the caption, allowing the model to predict
captions of varying length. The visual model Vused for
our image captioning experiments is either the CaffeNet [12]
reference model, a variant of AlexNet [17], or the moreR@1 R@5 R@10 Med r
Caption to Image (Flickr30k)
DeViSE [30] 6.7 21.9 32.7 25
SDT-RNN [29] 8.9 29.8 41.1 16
DeFrag [28] 10.3 31.4 44.5 13
m-RNN [27] 12.6 31.2 41.5 16
ConvNet [31] 11.8 34.0 46.3 13
LRCN 2f(ours) 17.5 40.3 50.8 9
Image to Caption (Flickr30k)
DeViSE [30] 4.5 18.1 29.2 26
SDT-RNN [29] 9.6 29.8 41.1 16
DeFrag [28] 16.4 40.2 54.7 8
m-RNN [27] 18.4 40.2 50.9 10
ConvNet [31] 14.8 39.2 50.9 10
LRCN 2f(ours) 23.6 46.6 58.3 7
TABLE 4
Image description: retrieval results for the Flickr30k [32] datasets.
R@Kis the average recall at rank K(high is good). Med ris the
median rank (low is good).
modern and computationally expensive VGGNet [18] model
pre-trained for ILSVRC-2012 [23] classiﬁcation.
Without any explicit language modeling or impositions
on the structure of the generated captions, the described
LRCN system learns mappings from images input as pixel
intensity values to natural language descriptions that are
often semantically descriptive and grammatically correct.
At training time, the previous word inputs y1:t1at time
steptare from the ground truth caption. For inference of
captions on a novel image x, the input is a sample ~yt
P(ytj~y1:t1;V(x))from the models predicted distribution
at the previous time step, and generation continues until an
<EOS> (end of sequence) token is generated.
5.1 Evaluation
We evaluate our image description model for retrieval and
generation tasks. We ﬁrst demonstrate the effectiveness of
our model by quantitatively evaluating it on the image and
caption retrieval tasks proposed by [26] and seen in [27],
[28], [29], [30], [31]. We report results on Flickr30k [32], and
COCO 2014 [33] datasets, both with ﬁve captions annotated
per image.
5.1.1 Retrieval
Retrieval results on the Flickr30k [32] dataset are recorded in
Table 4. We report median rank, Med r, of the ﬁrst retrieved
ground truth image or caption and Recall@ K, the number
of images or captions for which a correct caption or image is
retrieved within the top Kresults. Our model consistently
outperforms the strong baselines from recent work [27],
[28], [29], [30], [31] as can be seen in Table 4. Here, we
note that the VGGNet model in [31] (called OxfordNet in
their work) outperforms our model on the retrieval task.
However, VGGNet is a stronger convolutional network [18]
than that used for our results on this task. The strength
of our sequence model (and integration of the sequence
and visual models) can be more directly measured against
the ConvNet [31] result, which uses a very similar base
CNN architecture (AlexNet [17], where we use CaffeNet)
pretrained on the same data.
We also ablate the models retrieval performance on a
randomly chosen subset of 1000 images (and 5000 cap-
tions) from the COCO 2014 [33] validation set. Results are7
<BOS> 
LSTM LSTM 
A man 
CNN 
<BOS> 
LSTM LSTM 
A man 
CNN 
LSTM LSTM 
<BOS> 
LSTM LSTM 
A man 
CNN 
LSTM LSTM 
Single Layer ( L= 1) Two Layers ( L= 2), Unfactored Two Layers ( L= 2), Factored
LRCN 1u LRCN 2u LRCN 2f
Fig. 4. Three variants of the LRCN image captioning architecture that we experimentally evaluate. We explore the effect of depth in the LSTM
stack, and the effect of the factorization of the modalities.
recorded in Table 5. The ﬁrst group of results for each
task examines the effectiveness of an LSTM compared with
a vanilla RNN as described in Section 2. These results
demonstrate that the use of the LSTM unit compared to
the simpler RNN architecture is an important element of
our models performance on this task, justifying the addi-
tional complexity and suggesting that the LSTMs gating
mechanisms allowing for long-term memory may be quite
useful, even for relatively simple sequences.
Within the second and third result groups, we compare
performance among the three sequence model architectural
variants depicted in Figure 4. For both tasks and under all
metrics, the two layer, unfactored variant (LRCN 2u) per-
forms worse than the other two. The fact that LRCN 1uout-
performs LRCN 2uindicates that stacking additional LSTM
layers alone is not beneﬁcial for this task. The other two
variants (LRCN 2fand LRCN 1u) perform similarly across
the board, with LRCN 2fappearing to have a slight edge
in the image to caption task under most metrics, but the
reverse for caption to image retrieval.
Unsurprisingly, ﬁnetuning the CNN (indicated by the
FT? column of Table 5) and using a more powerful CNN
(VGGNet [18] rather than CaffeNet) each improve results
substantially across the board. Finetuning boosts the R@ k
metrics by 3-5% for CaffeNet, and 5-8% for VGGNet. Switch-
ing from CaffeNet to VGGNet improves results by around
8-12% for the caption to image task, and by roughly 11-17%
for the image to caption task.
5.1.2 Generation
We evaluate LRCNs caption generation performance on
the COCO2014 [33] dataset using the ofﬁcial metrics on
which COCO image captioning submissions are evaluated.
The BLEU [34] and METEOR [36] metrics were designed
for automatic evaluation of machine translation methods.
ROUGE-L [37] was designed for evaluating summarization
performance. CIDEr-D [35] was designed speciﬁcally to
evaluate the image captioning task.
In Table 6 we evaluate variants of our model along the
same axes as done for the retrieval tasks in Table 5. In the
last of the three groups of results, we additionally explore
and evaluate various caption generation strategies that canVision Model Sequence Model Retrieval Performance
CNN FT? Unit L Factor? R@1 R@5 R@10 Med r
Caption to Image
CaffeNet - RNN 2 X 21.3 51.7 67.2 5
CaffeNet - LSTM 2 X 25.0 56.2 70.6 4
CaffeNet - LSTM 1 - 25.2 56.2 70.8 4
CaffeNet - LSTM 2 - 23.4 54.8 69.3 5
CaffeNet - LSTM 2 X 25.0 56.2 70.6 4
CaffeNet X LSTM 1 - 28.5 60.0 74.5 4
CaffeNet X LSTM 2 - 25.6 57.2 72.2 4
CaffeNet X LSTM 2 X 27.2 59.6 74.7 4
VGGNet - LSTM 2 X 33.5 68.1 80.8 3
VGGNet X LSTM 2 X 39.3 74.7 85.9 2
Image to Caption
CaffeNet - RNN 2 X 30.2 61.0 72.6 4
CaffeNet - LSTM 2 X 33.8 65.3 75.3 3
CaffeNet - LSTM 1 - 32.3 64.5 75.6 3
CaffeNet - LSTM 2 - 29.9 60.8 72.7 3
CaffeNet - LSTM 2 X 33.8 65.3 75.3 3
CaffeNet X LSTM 1 - 36.1 68.4 79.5 3
CaffeNet X LSTM 2 - 33.1 63.7 76.9 3
CaffeNet X LSTM 2 X 36.3 67.3 80.6 2
VGGNet - LSTM 2 X 46.0 77.4 88.3 2
VGGNet X LSTM 2 X 53.3 84.3 91.9 1
TABLE 5
Retrieval results (image to caption and caption to image) for a randomly
chosen subset (1000 images) of the COCO 2014 [33] validation set.
R@Kis the average recall at rank K(high is good). Med ris the
median rank (low is good).
be employed for a given network. The simplest strategy,
and the one employed for most of our generation results
in our prior work [43], is to generate captions greedily;
i.e., by simply choosing the most probable word at each
time step. This is equivalent to (and denoted in Table 6 by)
beam search with beam width 1. In general, beam search
with beam width Napproximates the most likely caption
by retaining and expanding only the Ncurrent most likely
partial captions, according to the model. We ﬁnd that of the
beam search strategies, a beam width of 3-5 gives the best
generation numbers  performance saturates quickly and
even degrades for larger beam width (e.g., 10).
An alternative, non-deterministic generation strategy is
to randomly sample Ncaptions from the models distri-
bution and choose the most probable among these. Under8
Generation Strategy Vision Model Sequence Model Generation Performance (COCO 2014 [33] Validation Set)
Beam Sample
Width N T CNN FT? Unit L Factor? B1 B2 B3 B4 C M R
1 - - CaffeNet - RNN 2 X 0.638 0.454 0.315 0.220 0.660 0.209 0.473
1 - - CaffeNet - LSTM 2 X 0.646 0.462 0.321 0.224 0.674 0.210 0.477
1 - - CaffeNet - LSTM 1 - 0.654 0.475 0.333 0.231 0.661 0.209 0.480
1 - - CaffeNet - LSTM 2 - 0.653 0.470 0.328 0.230 0.682 0.212 0.480
1 - - CaffeNet - LSTM 2 X 0.646 0.462 0.321 0.224 0.674 0.210 0.477
1 - - CaffeNet X LSTM 1 - 0.661 0.485 0.344 0.241 0.702 0.216 0.489
1 - - CaffeNet X LSTM 2 - 0.659 0.478 0.338 0.238 0.716 0.217 0.486
1 - - CaffeNet X LSTM 2 X 0.659 0.478 0.336 0.237 0.717 0.218 0.486
1 - - VGGNet - LSTM 2 X 0.674 0.494 0.351 0.248 0.773 0.227 0.497
1 - - VGGNet X LSTM 2 X 0.695 0.519 0.374 0.268 0.839 0.237 0.512
- 100 1.5 CaffeNet - RNN 2 X 0.647 0.466 0.334 0.244 0.703 0.212 0.479
- 100 1.5 CaffeNet - LSTM 2 X 0.657 0.478 0.344 0.251 0.720 0.215 0.485
- 100 1.5 CaffeNet - LSTM 1 - 0.664 0.490 0.354 0.254 0.704 0.211 0.488
- 100 1.5 CaffeNet - LSTM 2 - 0.664 0.486 0.352 0.257 0.732 0.216 0.489
- 100 1.5 CaffeNet - LSTM 2 X 0.657 0.478 0.344 0.251 0.720 0.215 0.485
- 100 1.5 CaffeNet X LSTM 1 - 0.679 0.507 0.370 0.268 0.753 0.219 0.499
- 100 1.5 CaffeNet X LSTM 2 - 0.672 0.495 0.361 0.265 0.762 0.222 0.495
- 100 1.5 CaffeNet X LSTM 2 X 0.670 0.493 0.358 0.264 0.764 0.222 0.495
- 100 1.5 VGGNet - LSTM 2 X 0.690 0.514 0.377 0.278 0.828 0.231 0.508
- 100 1.5 VGGNet X LSTM 2 X 0.711 0.541 0.402 0.300 0.896 0.242 0.524
1 - - VGGNet X LSTM 2 X 0.695 0.519 0.374 0.268 0.839 0.237 0.512
2 - - VGGNet X LSTM 2 X 0.707 0.533 0.394 0.291 0.879 0.242 0.520
3 - - VGGNet X LSTM 2 X 0.708 0.536 0.399 0.298 0.888 0.243 0.521
4 - - VGGNet X LSTM 2 X 0.706 0.534 0.398 0.299 0.888 0.243 0.521
5 - - VGGNet X LSTM 2 X 0.704 0.533 0.398 0.300 0.888 0.242 0.520
10 - - VGGNet X LSTM 2 X 0.699 0.528 0.395 0.298 0.886 0.241 0.518
- 1 2.0 VGGNet X LSTM 2 X 0.658 0.472 0.327 0.224 0.733 0.222 0.483
- 10 2.0 VGGNet X LSTM 2 X 0.708 0.534 0.391 0.286 0.868 0.239 0.519
- 25 2.0 VGGNet X LSTM 2 X 0.712 0.540 0.398 0.294 0.885 0.241 0.523
- 100 2.0 VGGNet X LSTM 2 X 0.714 0.543 0.402 0.297 0.889 0.242 0.524
- 100 1.0 VGGNet X LSTM 2 X 0.674 0.494 0.357 0.261 0.805 0.228 0.494
- 100 1.5 VGGNet X LSTM 2 X 0.711 0.541 0.402 0.300 0.896 0.242 0.524
- 100 2.0 VGGNet X LSTM 2 X 0.714 0.543 0.402 0.297 0.889 0.242 0.524
TABLE 6
Image caption generation performance (under the BLEU 1-4 [34] (B1-B4), CIDEr-D [35] (C), METEOR [36] (M), and ROUGE-L [37] (R) metrics)
across various network architectures and generation strategies. In the topmost set of results, we show performance across various CNN and
recurrent architectures for a simple generation strategy  beam search with beam width 1 (i.e., simply choosing the most probable word at each
time step). In the middle set of results, we show performance across the same set of architectures for a more sophisticated and computationally
intensive generation strategy found to be the best performing (in terms of performance under the CIDEr-D metric) among those explored in the
bottom-most set of results, which explores various generation strategies while ﬁxing the choice of network. In the ﬁrst two sets of results, we vary
the visual input CNN architecture (either CaffeNet [12], an architecture similar to AlexNet [17], or the more modern VGGNet [18]) and whether its
weights are ﬁnetuned (FT?). Keeping the visual input CNN ﬁxed with CaffeNet, we also vary the choice of recurrent architecture, comparing a
stack of vanilla RNNs with LSTMs [7], as well as the number of layers in the stack L, and (for L= 2) whether the layers are factored (i.e.,
whether the visual input is passed into the second layer). In the last set of results, we explore two generation strategies  beam search, and
choosing the best (highest log-likelihood) among Nsamples from the models predicted distribution. For beam search we vary the beam width
from 1-10. For the sampling strategy we explore the effect of sample size Nas well as the effect of applying various choices of scalar factor T
(inverse of the temperature) to the logits input to the softmax producing the distribution.
this strategy we also examine the effect of applying various
choices of scalar factors (inverse of the temperature) Tto
the real-valued predictions input to the softmax producing
the distribution. For larger values of Tthe samples are
greedier and less diverse, with T=1being equivalent to
beam search with beam width 1. Larger values of Nsuggest
using smaller values of T, and vice versa  for example,
with largeNand largeT, most of theO(N)computation is
wasted as many of the samples will be redundant. We assess
saturation as the number of samples Ngrows, and ﬁnd that
N= 100 samples with T= 2 improves little over N= 25 .
We also varied the temperature Tamong values 1, 1.5, and
2 (all withN= 100 ) and found T= 1:5to perform the best.
We adopt the best-performing generation strategy from
the bottom-most set of results in Table 6 (sampling with
T= 1:5,N= 100 ) as the strategy for the middle setof results in the table, which ablates LRCN architectures.
We also record generation performance for all architectures
(Table 6, top set of results) with the simpler generation
strategy used in our earlier work [43] for ease of comparison
with this work and for future researchers. For the remainder
of this discussion, we will focus on the middle set of
results, and particularly on the CIDEr-D [35] (C) metric,
as it was designed speciﬁcally for automatic evaluation of
image captioning systems. We see again that the LSTM unit
outperforms an RNN unit for generation, though not as
signiﬁcantly as for retrieval. Between the sequence model
architecture choices (depicted in Figure 4) of the number
of layersLand whether to factor, we see that in this
case the two-layer models (LRCN 2fand LRCN 2u) perform
similarly, outperforming the single layer model (LRCN 1u).
Interestingly, of the three variants, LRCN 2fis the only one9
Generation Performance (COCO 2014 [33] Test Set)
Method B1 B2 B3 B4 C M R
[38] NIC 0.895 0.802 0.694 0.587 0.946 0.346 0.682
[39] MSR Captivator 0.907 0.819 0.710 0.601 0.937 0.339 0.680
[40] m-RNN (2015) 0.890 0.798 0.687 0.575 0.935 0.325 0.666
(Ours) * LRCN, this work (sample) 0.895 0.804 0.695 0.585 0.934 0.335 0.678
[41] MSR 0.880 0.789 0.678 0.567 0.925 0.331 0.662
[42] Nearest Neighbor 0.872 0.770 0.655 0.542 0.916 0.318 0.648
[33] Human 0.880 0.744 0.603 0.471 0.910 0.335 0.626
[27] m-RNN (2014) 0.890 0.801 0.690 0.578 0.896 0.320 0.668
(Ours) [43] LRCN (greedy) 0.871 0.772 0.653 0.534 0.891 0.322 0.656
[44] Show, Attend, and Tell 0.872 0.768 0.644 0.523 0.878 0.323 0.651
[31] MLBL 0.848 0.747 0.633 0.517 0.752 0.294 0.635
[45] NeuralTalk 0.828 0.701 0.566 0.446 0.692 0.280 0.603
TABLE 7
Image caption generation results from top-performing methods in the 2015 COCO caption challenge competition, sorted by performance under the
CIDEr-D metric. (We omit submissions that did not provide a reference to a report describing their method; see full results at
http://mscoco.org/dataset/#captions-leaderboard.) All results except for our updated result (denoted by LRCN, this work ) were competition entries
(submitted by May 2015). Our updated result differs from our original competition entry only by generation strategy (sampling with N= 100 ,
T= 1:5, rather than beam search with width 1; i.e., greedy search); the visual and recurrent architectures (and trained weights) are the same.
to perform best for both retrieval and generation.
We see again that ﬁne-tuning (FT) the visual represen-
tation and using a stronger vision model (VGGNet [18])
improves results signiﬁcantly. Fine-tuning improves CIDEr-
D by roughly 0.04 points for CaffeNet, and by roughly 0.07
points for VGGNet. Switching from ﬁnetuned CaffeNet to
VGGNet improves CIDEr-D by 0.13 points.
In Table 7 we compare generation performance with
contemporaneous and recent work submitted to the 2015
COCO caption challenge using our best-performing method
(under the CIDEr-D metric) from the results on the valida-
tion set described above  generating a caption for a single
image by taking the best of N= 100 samples with a scalar
factor ofT= 1:5applied to the softmax inputs, using an
LRCN model which pairs a ﬁne-tuned VGGNet with our
LRCN 2f(two layer, factored) sequence model architecture.
Our results are competitive with the contemporary work,
performing 4th best in CIDEr-D (0.934, compared with the
best result of 0.946 from [38]), and 3rd best in METEOR
(0.335, compared with 0.346 from [38]).
In addition to standard quantitative evaluations, we also
employ Amazon Mechnical Turk workers (Turkers) to
evaluate the generated sentences. Given an image and a
set of descriptions from different models, we ask Turkers
to rank the sentences based on correctness, grammar and
relevance. We compared sentences from our model to the
ones made publicly available by [31]. As seen in Table 8,
our ﬁne-tuned (FT) LRCN model performs on par with the
Nearest Neighbour (NN) on correctness and relevance, and
better on grammar.
We show sample captions in Figure 6. We additionally
note some properties of the captions our model generates.
When using the VGG model to generate sentences in the
validation set, we ﬁnd that 33.7% of our generated setences
exactly match a sentence in the training set. Furthermore,
we ﬁnd that when using a beam size of one, our model
generates 42% of the vocabulary words used by human
annotators when describing images in the validation set.
Some words, such as lady and guy, are not generated
by our model but are commonly used by human annotators,
but synonyms such as woman and man are two of the
most common words generated by our model.Correctness Grammar Relevance
TreeTalk [46] 4.08 4.35 3.98
VGGNet [31] 3.71 3.46 3.70
NN [31] 3.44 3.20 3.49
LRCN fc 8(ours) 3.74 3.19 3.72
LRCN FT (ours) 3.47 3.01 3.50
Captions 2.55 3.72 2.59
TABLE 8
Image description: Human evaluator rankings from 1-6 (low is good)
averaged for each method and criterion. We evaluated on 785 Flickr
images selected by the authors of [31] for the purposes of comparison
against this similar contemporary approach.
6 V IDEO DESCRIPTION
In video description the LSTM framework allows us to
model the video as a variable length input stream. How-
ever, due to the limitations of available video description
datasets, we rely on more traditional activity and video
recognition processing for the input and use LSTMs for
generating a sentence. We ﬁrst distinguish the following
architectures for video description (see Figure 5). For each
architecture, we assume we have predictions of activity, tool,
object, and locations present in the video from a CRF based
on the full video input. In this way, we observe the video as
whole at each time step, not incrementally frame by frame.
(a) LSTM encoder & decoder with CRF max. (Fig-
ure 5(a)) This architecture is motivated by the video de-
scription approach presented in [11]. They ﬁrst recognize
a semantic representation of the video using the maximum
a posteriori (MAP) estimate of a CRF with video features
as unaries. This representation, e.g., hknife,cut,carrot,cutting
boardi, is concatenated into an input sequence ( knife cut
carrot cutting board ) which is translated to a natural language
sentence ( a person cuts a carrot on the board ) using statistical
machine translation (SMT) [47]. We replace SMT with an
encoder-decoder LSTM, which encodes the input sequence
as a ﬁxed length vector before decoding to a sentence.
(b) LSTM decoder with CRF max. (Figure 5(b)) In this
variant we provide the full visual input representation at
each time step to the LSTM, analogous to how an image is
provided as an input to the LSTM in image captioning.
(c) LSTM decoder with CRF probabilites. (Figure 5(c))
A beneﬁt of using LSTMs for machine translation compared10
LSTM 
LSTM CRFcutting 
board 
cut
knife board 
cutting 
cut
knife [0, 1, 0, 0] 
[1, 0, 0, 0] 
[0, 0, 1, 0...] 
[0, 0, 0, 1] LSTM 
LSTM 
A
man 
cuts 
<EOS> LSTM 
LSTM 
LSTM 
LSTM 
LSTM 
LSTM 
LSTM 
LSTM LSTM 
LSTM 
LSTM 
LSTM Decoder 
Encoder CRF-max Input 
Sentence One Hot Visual 
Input 
LSTM 
LSTM 
LSTM 
LSTM LSTM 
LSTM 
LSTM 
LSTM [0, 1, 0, 0] [0, 0, 1, 0...] [0, 0, 0, 1] 
[0, 1, 0, 0] [0, 0, 1, 0...] [0, 0, 0, 1] 
[0, 1, 0, 0] [0, 0, 1, 0...] [0, 0, 0, 1] 
[0, 1, 0, 0] [0, 0, 1, 0...] [0, 0, 0, 1] 
LSTM 
LSTM CRF
LSTM 
LSTM LSTM 
LSTM 
LSTM 
LSTM A
man 
cuts 
<EOS> [0, 0.8, 0.2, 0] [0.3, 0, 0.7, 0] [0, 0.1, 0.2, 0.7] 
[0, 0.8, 0.2, 0] [0.3, 0, 0.7, 0] [0, 0.1, 0.2, 0.7] 
[0, 0.8, 0.2, 0] [0.3, 0, 0.7, 0] [0, 0.1, 0.2, 0.7] 
[0, 0.8, 0.2, 0] [0.3, 0, 0.7, 0] [0, 0.1, 0.2, 0.7] CRF-prob Visual 
Input knife cutcutting 
board 
(a) (b) (c)
LSTM Encoder-Decoder LSTM Decoder (CRF-max) LSTM Decoder (CRF-prob)
Fig. 5. Our approaches to video description. (a) LSTM encoder & decoder with CRF max (b) LSTM decoder with CRF max (c) LSTM decoder with
CRF probabilities.
Architecture Input BLEU
SMT [11] CRF max 24.9
SMT [48] CRF prob 26.9
(a) LSTM Encoder-Decoder (ours) CRF max 25.3
(b) LSTM Decoder (ours) CRF max 27.4
(c) LSTM Decoder (ours) CRF prob 28.8
TABLE 9
Video description: Results on detailed description of TACoS multilevel
[48], in %, see Section 6 for details.
to phrase-based SMT [47] is that it can naturally incorporate
probability vectors during training and test time which
allows the LSTM to learn uncertainties in visual generation
rather than relying on MAP estimates. The architecture is
the the same as in (b), but we replace max predictions with
probability distributions.
6.1 Evaluation
We evaluate our approach on the TACoS multilevel [48]
dataset, which has 44,762 video/sentence pairs (about
40,000 for training/validation). We compare to [11] who use
max prediction as well as a variant presented in [48] which
takes CRF probabilities at test time and uses a word lattice
to ﬁnd an optimal sentence prediction. Since we use the
max prediction as well as the probability scores provided
by [48], we have an identical visual representation. [48]
uses dense trajectories [49] and SIFT features as well as
temporal context reasoning modeled in a CRF. In this set
of experiments we use the two-layered, unfactored version
of LRCN, as described for image description.
Table 9 shows the BLEU-4 score. The results show that
(1) the LSTM outperforms an SMT-based approach to video
description; (2) the simpler decoder architecture (b) and
(c) achieve better performance than (a), likely because the
input does not need to be memorized; and (3) our approach
achieves 28.8%, clearly outperforming the best reported
number of 26.9% on TACoS multilevel by [48].
More broadly, these results show that our architecture is
not restricted only to input from deep networks, but can be
cleanly integrated with ﬁxed or variable length inputs from
other vision systems.
7 R ELATED WORK
We present previous literature pertaining to the three tasks
discussed in this work. Additionally, we discuss subsequent
extensions which combine convolutional and recurrent net-
works to achieve improved results on activity recognition,image captioning, and video description as well as related
new tasks such as visual question answering.
7.1 Prior Work
Activity Recognition. State-of-the-art shallow models com-
bine spatio-temporal features along dense trajectories [50]
and encode features as bags of words or Fisher vectors
for classiﬁcation. Such shallow features track how low
level features change through time but cannot track higher
level features. Furthermore, by encoding features as bags of
words or Fisher vectors, temporal relationships are lost.
Many deep architectures proposed for activity recogni-
tion stack a ﬁxed number of video frames for input to a
deep network. [3] propose a fusion convolutional network
which fuses layers which correspond to different input
frames at various levels of a deep network. [4] proposes
a two stream CNN which combines one CNN trained
on RGB frames and one CNN trained on a stack of 10
ﬂow frames. When combining RGB and ﬂow by averaging
softmax scores, results are comparable to state-of-the-art
shallow models on UCF101 [25] and HMDB51 [51]. Results
are further improved by using an SVM to fuse RGB and
ﬂow as opposed to simply averaging scores. Alternatively,
[1] and [2] propose learning deep spatio-temporal features
with 3D convolutional neural networks. [2], [52] propose
extracting visual and motion features and modeling tempo-
ral dependencies with recurrent networks. This architecture
most closely resembles our proposed architecture for activ-
ity classiﬁcation, though it differs in two key ways. First, we
integrate 2D CNNs that can be pre-trained on large image
datasets. Second, we combine the CNN and LSTM into a
single model to enable end-to-end ﬁne-tuning.
Image Captioning. Several early works [53], [54], [55],
[56] on image captioning combine object and scene recog-
nition with template or tree based approaches to generate
captions. Such sentences are typically simple and are easily
distinguished from more ﬂuent human generated descrip-
tions. [46], [57] address this by composing new sentences
from existing caption fragments which, though more human
like, are not necessarily accurate or correct.
More recently, a variety of deep and multi-modal models
[27], [29], [30], [58] have been proposed for image and cap-
tion retrieval, as well as caption generation. Though some of
these models rely on deep convolutional nets for image fea-
ture extraction [30], [58], recently researchers have realized
the importance of also including temporally deep networks11
to model text. [29] propose an RNN to map sentences into
a multi-modal embedding space. By mapping images and
language into the same embedding space, they are able to
compare images and descriptions for image and annotation
retrieval tasks. [27] propose a model for caption generation
that is more similar to the model proposed in this work:
predictions for the next word are based on previous words
in a sentence and image features. [58] propose an encoder-
decoder model for image caption retrieval which relies on
both a CNN and LSTM encoder to learn an embedding of
image-caption pairs. Their model uses a neural language
decoder to enable sentence generation. As evidenced by the
rapid growth of image captioning, visual sequence models
like LRCN are increasingly important for describing the
visual world using natural language.
Video Description. Recent approaches to describing
video with natural language have made use of templates,
retrieval, or language models [11], [59], [60], [60], [61], [62],
[63], [64]. To our knowledge, we present the ﬁrst application
of deep models to the video description task. Most similar
to our work is [11], which use phrase-based SMT [47] to
generate a sentence. In Section 6 we show that phrase-based
SMT can be replaced with LSTMs for video description as
has been shown previously for language translation [9], [65].
7.2 Contemporaneous and Subsequent Work
Similar work in activity recognition and visual description
was conducted contemporaneously with our work, and a
variety of subsequent work has combined convolutional and
recurrent networks to both improve upon our results and
achieve exciting results on other sequential visual tasks.
Activity Recognition. Contemporaneous with our work,
[66] train a network which combines CNNs and LSTMs for
activity recognition. Because activity recognition datasets
like UCF101 are relatively small in comparison to image
recognition datasets, [66] pretrain their network using the
Sports-1M [3] dataset which includes over a million videos
mined from YouTube. By training a much larger network
(four stacked LSTMs) and pretraining on a large video
dataset, [66] achieve 88.6% on the UCF101 dataset.
[67] also combines a convolutional network with an
LSTM to predict multiple activities per frame. Unlike LRCN,
[67] focuses on frame-level (rather than video-level) predic-
tions, which allows their system to label multiple activities
that occur in different temporal locations of a video clip.
Like we show for activity recognition, [67] demonstrates
that including temporal information improves upon a sin-
gle frame baseline. Additionally, [67] employ an attention
mechanism to further improve results.
Image Captioning. [45] and [38] also propose models
which combine a CNN with a recurrent network for image
captioning. Though similar to LRCN, the architectures pro-
posed in [45] and [38] differ in how image features are input
into the sequence model. In contrast to our system, in which
image features are input at each time step, [45] and [38]
only input image features at the ﬁrst time step. Furthermore,
they do not explore a factored representation (Figure 4).
Subsequent work [44] has proposed attention to focus on
which portion of the image is observed during sequence
generation. By including attention, [44] aim to visuallyfocus on the current word generated by the model. Other
works aim to address speciﬁc limitations of captioning
models based on combining convolutional and recurrent
architectures. For example, methods have been proposed
to integrate new vocabulary with limited [40] or no [68]
examples of images and corresponding captions.
Video Description. In this work, we rely on intermedi-
ate features for video description, but end-to-end trainable
models for visual captioning have since been proposed. [69]
propose creating a video feature by pooling high level CNN
features across frames. The video feature is then used to
generate descriptions in the same way an image is used to
generate a description in LRCN. Though achieving good
results, by pooling CNN features, temporal information
from the video is lost. Consequently, [70] propose an LSTM
to encode video frames into a ﬁxed length vector before
sentence generation with an LSTM. Using an end-to-end
trainable sequence-to-sequence model which can exploit
temporal structure in video, [70] improve upon results for
video description. [71] propose a similar model, adding a
temporal attention mechanism which weights video frames
differently when generating each word in a sentence.
Visual Grounding. [72] combine CNNs with LSTMs for
visual grounding. The model ﬁrst encodes a phrase which
describes part of an image using an LSTM, then learns to
attend to the appropriate location in the image to accurately
reconstruct the phrase. In order to reconstruct the phrase,
the model must learn to visually ground the input phrase to
the appropriate location in the image.
Natural Language Object Retrieval. In this work, we
present methods for image retrieval based on a natural
language description. In contrast, [73] use a model based on
LRCN for object retrieval , which returns the bounding box
around a given object as opposed to an entire image. In or-
der to adapt LRCN to the task of object retrieval, [73] include
local convolutional features which are extracted from object
proposals and the spatial conﬁguration of object proposals
in addition to a global image feature. By including local
features, [73] effectively adapt LRCN for object retrieval.
8 C ONCLUSION
Weve presented LRCN, a class of models that is both
spatially and temporally deep, and ﬂexible enough to be
applied to a variety of vision tasks involving sequential
inputs and outputs. Our results consistently demonstrate
that by learning sequential dynamics with a deep sequence
model, we can improve upon previous methods which learn
a deep hierarchy of parameters only in the visual domain,
and on methods which take a ﬁxed visual representation
of the input and only learn the dynamics of the output
sequence.
As the ﬁeld of computer vision matures beyond tasks
with static input and predictions, deep sequence modeling
tools like LRCN are increasingly central to vision systems
for problems with sequential structure. The ease with which
these tools can be incorporated into existing visual recog-
nition pipelines makes them a natural choice for percep-
tual problems with time-varying visual input or sequential
outputs, which these methods are able to handle with little
input preprocessing and no hand-designed features.12
A female tennis player in action on
the court.A group of young men playing a
game of soccerA man riding a wave on top of a
surfboard.
A baseball game in progress with the
batter up to plate.A brown bear standing on top of a
lush green ﬁeld.A person holding a cell phone in
their hand.
A close up of a person brushing his
teeth.A woman laying on a bed in a bed-
room.A black and white cat is sitting on a
chair.
A large clock mounted to the side of
a building.A bunch of fruit that are sitting on a
table.A toothbrush holder sitting on top of
a white sink.
Fig. 6. Image description: images with corresponding captions generated by our ﬁnetuned LRCN model. These are images 1-12 of our randomly
chosen validation set from COCO 2014 [33]. We used beam search with a beam size of 5 to generate the sentences, and display the top (highest
likelihood) result above.13
ACKNOWLEDGMENTS
The authors thank Oriol Vinyals for valuable advice and
helpful discussion throughout this work. This work was
supported in part by DARPAs MSEE and SMISC programs,
NSF awards IIS-1427425 and IIS-1212798, and the Berkeley
Vision and Learning Center. The GPUs used for this research
were donated by NVIDIA. Marcus Rohrbach was supported
by a fellowship within the FITweltweit-Program of the
German Academic Exchange Service (DAAD). Lisa Anne
Hendricks was supported by the NDSEG.
REFERENCES
[1] S. Ji, W. Xu, M. Yang, and K. Yu, 3D convolutional neural
networks for human action recognition, in IEEE Trans. Pattern
Anal. Mach. Intell. , 2013.
[2] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, and A. Baskurt, Se-
quential deep learning for human action recognition, in Human
Behavior Understanding , 2011.
[3] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and
L. Fei-Fei, Large-scale video classiﬁcation with convolutional
neural networks, in CVPR , 2014.
[4] K. Simonyan and A. Zisserman, Two-stream convolutional net-
works for action recognition in videos, in NIPS , 2014.
[5] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, Learning
internal representations by error propagation, DTIC Document,
Tech. Rep., 1985.
[6] R. J. Williams and D. Zipser, A learning algorithm for continually
running fully recurrent neural networks, in Neural Computation ,
1989.
[7] S. Hochreiter and J. Schmidhuber, Long short-term memory, in
Neural Computation . MIT Press, 1997.
[8] A. Graves and N. Jaitly, Towards end-to-end speech recognition
with recurrent neural networks, in ICML , 2014.
[9] I. Sutskever, O. Vinyals, and Q. V . Le, Sequence to sequence
learning with neural networks, in NIPS , 2014.
[10] K. Cho, B. van Merri enboer, D. Bahdanau, and Y. Bengio, On
the properties of neural machine translation: Encoder-decoder
approaches, in SSST Workshop , 2014.
[11] M. Rohrbach, W. Qiu, I. Titov, S. Thater, M. Pinkal, and B. Schiele,
Translating video content to natural language descriptions, in
ICCV , 2013.
[12] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, Caffe: Convolutional architecture
for fast feature embedding, in ACM MM , 2014.
[13] W. Zaremba and I. Sutskever, Learning to execute, in arXiv
preprint arXiv:1410.4615 , 2014.
[14] A. Graves, Generating sequences with recurrent neural net-
works, in arXiv preprint arXiv:1308.0850 , 2013.
[15] O. Vinyals, S. V . Ravuri, and D. Povey, Revisiting recurrent neural
networks for robust ASR, in ICASSP , 2012.
[16] I. Sutskever, J. Martens, and G. E. Hinton, Generating text with
recurrent neural networks, in ICML , 2011.
[17] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ImageNet classiﬁ-
cation with deep convolutional neural networks, in NIPS , 2012.
[18] K. Simonyan and A. Zisserman, Very deep convolutional net-
works for large-scale image recognition, in ICLR , 2015.
[19] C. Szegedy, W. Liu, Y. Jia, P . Sermanet, S. Reed, D. Anguelov,
D. Erhan, V . Vanhoucke, and A. Rabinovich, Going deeper with
convolutions, in CVPR , 2015.
[20] K. Cho, B. van Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk,
and Y. Bengio, Learning phrase representations using rnn
encoder-decoder for statistical machine translation, in EMNLP ,
2014.
[21] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert, High accuracy
optical ﬂow estimation based on a theory for warping, in ECCV ,
2004.
[22] M. D. Zeiler and R. Fergus, Visualizing and understanding con-
volutional networks, in ECCV , 2014.
[23] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei, ImageNet Large Scale Visual Recognition Challenge,
inIJCV , vol. 115, no. 3, 2015.[24] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, Ima-
geNet: A large-scale hierarchical image database, in CVPR , 2009.
[25] K. Soomro, A. R. Zamir, and M. Shah, UCF101: A dataset of 101
human actions classes from videos in the wild, CRCV-TR-12-01,
Tech. Rep., 2012.
[26] P . Y. Micah Hodosh and J. Hockenmaier, Framing image descrip-
tion as a ranking task: Data, models and evaluation metrics, in
JAIR , vol. 47, 2013.
[27] J. Mao, W. Xu, Y. Yang, J. Wang, and A. Yuille, Deep captioning
with multimodal recurrent neural networks (m-RNN), in ICLR ,
2015.
[28] A. Karpathy, A. Joulin, and L. Fei-Fei, Deep fragment embed-
dings for bidirectional image sentence mapping, in NIPS , 2014.
[29] R. Socher, A. Karpathy, Q. V . Le, C. D. Manning, and A. Y. Ng,
Grounded compositional semantics for ﬁnding and describing
images with sentences, in TACL , vol. 2, 2014.
[30] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, T. Mikolov
et al. , Devise: A deep visual-semantic embedding model, in
NIPS , 2013.
[31] R. Kiros, R. Salakhuditnov, and R. S. Zemel, Unifying visual-
semantic embeddings with multimodal neural language models,
inTACL , 2015.
[32] M. H. Peter Young, Alice Lai and J. Hockenmaier, From image
descriptions to visual denotations: New similarity metrics for
semantic inference over event descriptions, in TACL , vol. 2, 2014.
[33] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ramanan,
P . Doll ar, and C. L. Zitnick, Microsoft COCO: Common objects in
context, arXiv preprint arXiv:1405.0312, Tech. Rep., 2014.
[34] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, BLEU: a method
for automatic evaluation of machine translation, in ACL , 2002.
[35] R. Vedantam, C. L. Zitnick, and D. Parikh, CIDEr: Consensus-
based image description evaluation, in CVPR , 2015.
[36] S. Banerjee and A. Lavie, METEOR: An automatic metric for MT
evaluation with improved correlation with human judgments, in
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summarization , 2005.
[37] C.-Y. Lin, Rouge: A package for automatic evaluation of sum-
maries, in Text Summarization Branches Out: Proceedings of the ACL-
04 Workshop , 2004.
[38] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, Show and tell: A
neural image caption generator, in CVPR , 2015.
[39] J. Devlin, H. Cheng, H. Fang, S. Gupta, L. Deng, X. He, G. Zweig,
and M. Mitchell, Language models for image captioning: The
quirks and what works, in ACL , 2015.
[40] J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille, Learning
like a child: Fast novel visual concept learning from sentence
descriptions of images, in ICCV , 2015.
[41] H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P . Doll ar,
J. Gao, X. He, M. Mitchell, J. Platt et al. , From captions to visual
concepts and back, in CVPR , 2015.
[42] J. Devlin, S. Gupta, R. Girshick, M. Mitchell, and C. L. Zitnick,
Exploring nearest neighbor approaches for image captioning,
arXiv preprint arXiv:1505.04467, Tech. Rep., 2015.
[43] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach,
S. Venugopalan, K. Saenko, and T. Darrell, Long-term recurrent
convolutional networks for visual recognition and description, in
CVPR , 2015.
[44] K. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov, R. Zemel, and
Y. Bengio, Show, attend and tell: Neural image caption generation
with visual attention, in ICML , 2015.
[45] A. Karpathy and L. Fei-Fei, Deep visual-semantic alignments for
generating image descriptions, in CVPR , 2015.
[46] P . Kuznetsova, V . Ordonez, T. L. Berg, U. C. Hill, and Y. Choi,
TreeTalk: Composition and compression of trees for image de-
scriptions, in TACL , vol. 2, no. 10, 2014.
[47] P . Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico,
N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer,
O. Bojar, A. Constantin, and E. Herbst, Moses: Open source
toolkit for statistical machine translation, in ACL , 2007.
[48] A. Rohrbach, M. Rohrbach, W. Qiu, A. Friedrich, M. Pinkal,
and B. Schiele, Coherent multi-sentence video description with
variable level of detail, in German Conference on Pattern Recognition
(GCPR) . Springer, 2014.
[49] H. Wang, A. Kl aser, C. Schmid, and C. Liu, Dense trajectories
and motion boundary descriptors for action recognition, in IJCV ,
2013.14
[50] H. Wang and C. Schmid, Action recognition with improved
trajectories, in ICCV , 2013.
[51] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, HMDB:
a large video database for human motion recognition, in ICCV ,
2011.
[52] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, and A. Baskurt, Ac-
tion classiﬁcation in soccer videos with long short-term memory
recurrent neural networks, in International Conference on Artiﬁcial
Neural Networks (ICANN) , 2010.
[53] A. Farhadi, M. Hejrati, M. Sadeghi, P . Young, C. Rashtchian,
J. Hockenmaier, and D. Forsyth, Every picture tells a story:
Generating sentences from images, in ECCV , 2010.
[54] G. Kulkarni, V . Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg, and T. L.
Berg, Baby talk: Understanding and generating simple image
descriptions, in CVPR , 2011.
[55] Y. Yang, C. L. Teo, H. Daum e III, and Y. Aloimonos, Corpus-
guided sentence generation of natural images, in EMNLP , 2011.
[56] M. Mitchell, X. Han, J. Dodge, A. Mensch, A. Goyal, A. Berg,
K. Yamaguchi, T. Berg, K. Stratos, and H. Daum e III, Midge:
Generating image descriptions from computer vision detections,
inProceedings of the 13th Conference of the European Chapter of the
Association for Computational Linguistics , 2012.
[57] P . Kuznetsova, V . Ordonez, A. C. Berg, T. L. Berg, and Y. Choi,
Collective generation of natural image descriptions, in ACL ,
2012.
[58] R. Kiros, R. Salakhutdinov, and R. Zemel, Multimodal neural
language models, in ICML , 2014.
[59] S. Guadarrama, N. Krishnamoorthy, G. Malkarnenkar, S. Venu-
gopalan, R. Mooney, T. Darrell, and K. Saenko, YouTube2Text:
Recognizing and describing arbitrary activities using semantic
hierarchies and zero-shoot recognition, in ICCV , 2013.
[60] M. U. G. Khan, L. Zhang, and Y. Gotoh, Human focused video
description, in ICCV Workshops , 2011.
[61] A. Barbu, A. Bridge, Z. Burchill, D. Coroian, S. Dickinson, S. Fidler,
A. Michaux, S. Mussman, S. Narayanaswamy, D. Salvi, L. Schmidt,
J. Shangguan, J. M. Siskind, J. Waggoner, S. Wang, J. Wei, Y. Yin,
and Z. Zhang, Video in sentences out, in The Conference on
Uncertainty in Artiﬁcial Intelligence (UAI) , 2012.
[62] P . Das, C. Xu, R. Doell, and J. Corso, Thousand frames in just
a few words: Lingual description of videos through latent topics
and sparse object stitching, in CVPR , 2013.
[63] C. C. Tan, Y.-G. Jiang, and C.-W. Ngo, Towards textually describ-
ing complex video contents with audio-visual concept classiﬁers,
inACM MM , 2011.
[64] J. Thomason, S. Venugopalan, S. Guadarrama, K. Saenko, and R. J.
Mooney, Integrating language and vision to generate natural
language descriptions of videos in the wild, in International
Conference on Computational Linguistics (COLING) , 2014.
[65] H. Sak, O. Vinyals, G. Heigold, A. Senior, E. McDermott, R. Monga,
and M. Mao, Sequence discriminative distributed training of long
short-term memory recurrent neural networks, in Interspeech ,
2014.
[66] J. Y.-H. Ng, M. Hausknecht, S. Vijayanarasimhan, O. Vinyals,
R. Monga, and G. Toderici, Beyond short snippets: Deep net-
works for video classiﬁcation, in CVPR , 2015.
[67] S. Yeung, O. Russakovsky, N. Jin, M. Andriluka, G. Mori, and
L. Fei-Fei, Every moment counts: Dense detailed labeling of
actions in complex videos, arXiv preprint arXiv:1507.05738, Tech.
Rep., 2015.
[68] L. A. Hendricks, S. Venugopalan, M. Rohrbach, R. Mooney,
K. Saenko, and T. Darrell, Deep compositional captioning: De-
scribing novel object categories without paired training data, in
CVPR , 2016.
[69] S. Venugopalan, H. Xu, J. Donahue, M. Rohrbach, R. Mooney, and
K. Saenko, Translating videos to natural language using deep
recurrent neural networks, in NAACL , 2015.
[70] S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney, T. Darrell,
and K. Saenko, Sequence to sequencevideo to text, in ICCV ,
2015.
[71] L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle, and
A. Courville, Describing videos by exploiting temporal struc-
ture, in CVPR , vol. 1050, 2015.
[72] A. Rohrbach, M. Rohrbach, R. Hu, T. Darrell, and B. Schiele,
Grounding of textual phrases in images by reconstruction, arXiv
preprint arXiv:1511.03745, Tech. Rep., 2015.
[73] R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Darrell,
Natural language object retrieval, in CVPR , 2016.Jeff Donahue is a PhD student at the University of California, Berkeley,
advised by Prof. Trevor Darrell. His research focuses on the use of deep
learning for computer vision applications. He graduated with a BS in
computer science from the University of Texas at Austin, where he was
advised by Prof. Kristen Grauman.
Lisa Anne Hendricks is a PhD student at the University of California,
Berkeley. Her research focuses on deep learning for sequential models
as well as applications at the intersection of language and vision. She is
advised by Prof. Trevor Darrell. Lisa Anne holds a Bachelors of Science
in Electrical Engineering (B.S.E.E.) from Rice University.
Marcus Rohrbach sresearch focuses on visual recognition, language
understanding, and machine learning. He received his BSc and MSc
degree in Computer Science from the University of Technology Darm-
stadt, Germany, in 2006 and 2009, respectively. From 2006-2007, he
spent one year at the University of British Columbia as a graduate
visiting student. During his PhD he worked at the Max Planck Institute
for Informatics, Saarbr ucken, Germany with Bernt Schiele and Manfred
Pinkal. He completed it in 2014 with summa cum laude at Saarland
University and received the DAGM MVTec Dissertation Award 2015 for
it. He currently works as a post-doc with Trevor Darrell at UC Berkeley.
Subhashini Venugopalan is a PhD student at the University of Texas at
Austin. Her research focuses on deep learning techniques to generate
descriptions for events in videos. She is advised by Prof. Raymond
Mooney. Subhashini holds a masters degree in Computer Science from
IIT Madras and a bachelors degree from NIT Karnataka, India.
Sergio Guadarrama is a Software Engineer at Google Research, where
he works in Machine Perception as a member of the Vale team. He
received his PhD from the Technical University of Madrid, followed by
postdoctoral work at the European Center for Soft Computing. After
that, he was ﬁrst a Visiting Scholar and then a Research Scientist at
UC Berkeley EECS. His research spans the areas of computer vision,
language and deep learning. Dr. Guadarramas current research focus
is on new network architectures for multi-task dense predictions, such
as object detection, instance segmentation, depth prediction and visual
question-answering. He has received research grants from the Govern-
ment of Spain, such as the Juan de la Cierva Award (Early Career Award
in Computer Science), and the Mobility Grant for Postdoctoral Research.
Kate Saenko is an Assistant Professor of Computer Science at the
University of Massachusetts Lowell, where she leads the Computer
Vision and Learning Group. She received her PhD from MIT, followed
by postdoctoral work at UC Berkeley EECS and Harvard SEAS. Her
research spans the areas of computer vision, machine learning, and
human-robot interfaces. Dr. Saenkos current research interests include
domain adaptation of machine learning models and joint modeling of
language and vision. She is the recipient of research grant awards from
the National Science Foundation, DARPA, and other government and
industry agencies.
Trevor Darrell is on the faculty of the CS Division of the EECS Depart-
ment at UC Berkeley and is also appointed at the UCB-afﬁliated Interna-
tional Computer Science Institute (ICSI). He is the director of the Berke-
ley Vision and Learning Center (BVLC) and is the faculty director of the
PATH center in the UCB Institute of Transportation Studies PATH. His
interests include computer vision, machine learning, computer graphics,
and perception-based human computer interfaces. Prof. Darrell received
the SM and PhD degrees from MIT in 1992 and 1996, respectively. He
was previously on the faculty of the MIT EECS department from 1999-
2008, where he directed the Vision Interface Group. He was a member
of the research staff at Interval Research Corporation from 1996-1999.
He obtained the BSE degree from the University of Pennsylvania in
1988, having started his career in computer vision as an undergraduate
researcher in Ruzena Bajcsys GRASP lab.
  Show and Tell: A Neural Image Caption Generator
Oriol Vinyals
Google
vinyals@google.comAlexander Toshev
Google
toshev@google.comSamy Bengio
Google
bengio@google.comDumitru Erhan
Google
dumitru@google.com
Abstract
Automatically describing the content of an image is a
fundamental problem in artiﬁcial intelligence that connects
computer vision and natural language processing. In this
paper, we present a generative model based on a deep re-
current architecture that combines recent advances in com-
puter vision and machine translation and that can be used
to generate natural sentences describing an image. The
model is trained to maximize the likelihood of the target de-
scription sentence given the training image. Experiments
on several datasets show the accuracy of the model and the
ﬂuency of the language it learns solely from image descrip-
tions. Our model is often quite accurate, which we verify
both qualitatively and quantitatively. For instance, while
the current state-of-the-art BLEU-1 score (the higher the
better) on the Pascal dataset is 25, our approach yields 59,
to be compared to human performance around 69. We also
show BLEU-1 score improvements on Flickr30k, from 56 to
66, and on SBU, from 19 to 28. Lastly, on the newly released
COCO dataset, we achieve a BLEU-4 of 27.7, which is the
current state-of-the-art.
1. Introduction
Being able to automatically describe the content of an
image using properly formed English sentences is a very
challenging task, but it could have great impact, for instance
by helping visually impaired people better understand the
content of images on the web. This task is signiﬁcantly
harder, for example, than the well-studied image classiﬁ-
cation or object recognition tasks, which have been a main
focus in the computer vision community [27]. Indeed, a
description must capture not only the objects contained in
an image, but it also must express how these objects relate
to each other as well as their attributes and the activities
they are involved in. Moreover, the above semantic knowl-
edge has to be expressed in a natural language like English,
which means that a language model is needed in addition to
visual understanding.
Most previous attempts have proposed to stitch together
A group of people shopping at an outdoor market. !There are many vegetables at the fruit stand.Vision!Deep CNNLanguage !Generating!RNN
Figure 1. NIC, our model, is based end-to-end on a neural net-
work consisting of a vision CNN followed by a language gener-
ating RNN. It generates complete sentences in natural language
from an input image, as shown on the example above.
existing solutions of the above sub-problems, in order to go
from an image to its description [6, 16]. In contrast, we
would like to present in this work a single joint model that
takes an image Ias input, and is trained to maximize the
likelihoodp(SjI)of producing a target sequence of words
S=fS1;S2;:::gwhere each word Stcomes from a given
dictionary, that describes the image adequately.
The main inspiration of our work comes from recent ad-
vances in machine translation, where the task is to transform
a sentenceSwritten in a source language, into its transla-
tionTin the target language, by maximizing p(TjS). For
many years, machine translation was also achieved by a se-
ries of separate tasks (translating words individually, align-
ing words, reordering, etc), but recent work has shown that
translation can be done in a much simpler way using Re-
current Neural Networks (RNNs) [3, 2, 30] and still reach
state-of-the-art performance. An encoder RNN reads the
source sentence and transforms it into a rich ﬁxed-length
vector representation, which in turn in used as the initial
hidden state of a decoder RNN that generates the target
sentence.
Here, we propose to follow this elegant recipe, replac-
ing the encoder RNN by a deep convolution neural network
(CNN). Over the last few years it has been convincingly
shown that CNNs can produce a rich representation of the
input image by embedding it to a ﬁxed-length vector, such
that this representation can be used for a variety of vision
1arXiv:1411.4555v2  [cs.CV]  20 Apr 2015tasks [28]. Hence, it is natural to use a CNN as an image
encoder, by ﬁrst pre-training it for an image classiﬁcation
task and using the last hidden layer as an input to the RNN
decoder that generates sentences (see Fig. 1). We call this
model the Neural Image Caption, or NIC.
Our contributions are as follows. First, we present an
end-to-end system for the problem. It is a neural net which
is fully trainable using stochastic gradient descent. Second,
our model combines state-of-art sub-networks for vision
and language models. These can be pre-trained on larger
corpora and thus can take advantage of additional data. Fi-
nally, it yields signiﬁcantly better performance compared
to state-of-the-art approaches; for instance, on the Pascal
dataset, NIC yielded a BLEU score of 59, to be compared to
the current state-of-the-art of 25, while human performance
reaches 69. On Flickr30k, we improve from 56 to 66, and
on SBU, from 19 to 28.
2. Related Work
The problem of generating natural language descriptions
from visual data has long been studied in computer vision,
but mainly for video [7, 32]. This has led to complex sys-
tems composed of visual primitive recognizers combined
with a structured formal language, e.g. And-Or Graphs or
logic systems, which are further converted to natural lan-
guage via rule-based systems. Such systems are heav-
ily hand-designed, relatively brittle and have been demon-
strated only on limited domains, e.g. trafﬁc scenes or sports.
The problem of still image description with natural text
has gained interest more recently. Leveraging recent ad-
vances in recognition of objects, their attributes and loca-
tions, allows us to drive natural language generation sys-
tems, though these are limited in their expressivity. Farhadi
et al. [6] use detections to infer a triplet of scene elements
which is converted to text using templates. Similarly, Li
et al. [19] start off with detections and piece together a ﬁ-
nal description using phrases containing detected objects
and relationships. A more complex graph of detections
beyond triplets is used by Kulkani et al. [16], but with
template-based text generation. More powerful language
models based on language parsing have been used as well
[23, 1, 17, 18, 5]. The above approaches have been able to
describe images in the wild, but they are heavily hand-
designed and rigid when it comes to text generation.
A large body of work has addressed the problem of rank-
ing descriptions for a given image [11, 8, 24]. Such ap-
proaches are based on the idea of co-embedding of images
and text in the same vector space. For an image query, de-
scriptions are retrieved which lie close to the image in the
embedding space. Most closely, neural networks are used to
co-embed images and sentences together [29] or even image
crops and subsentences [13] but do not attempt to generate
novel descriptions. In general, the above approaches cannotdescribe previously unseen compositions of objects, even
though the individual objects might have been observed in
the training data. Moreover, they avoid addressing the prob-
lem of evaluating how good a generated description is.
In this work we combine deep convolutional nets for im-
age classiﬁcation [12] with recurrent networks for sequence
modeling [10], to create a single network that generates de-
scriptions of images. The RNN is trained in the context of
this single end-to-end network. The model is inspired by
recent successes of sequence generation in machine trans-
lation [3, 2, 30], with the difference that instead of starting
with a sentence, we provide an image processed by a con-
volutional net. The closest works are by Kiros et al. [15]
who use a neural net, but a feedforward one, to predict the
next word given the image and previous words. A recent
work by Mao et al. [21] uses a recurrent NN for the same
prediction task. This is very similar to the present proposal
but there are a number of important differences: we use a
more powerful RNN model, and provide the visual input to
the RNN model directly, which makes it possible for the
RNN to keep track of the objects that have been explained
by the text. As a result of these seemingly insigniﬁcant dif-
ferences, our system achieves substantially better results on
the established benchmarks. Lastly, Kiros et al. [14] pro-
pose to construct a joint multimodal embedding space by
using a powerful computer vision model and an LSTM that
encodes text. In contrast to our approach, they use two sepa-
rate pathways (one for images, one for text) to deﬁne a joint
embedding, and, even though they can generate text, their
approach is highly tuned for ranking.
3. Model
In this paper, we propose a neural and probabilistic
framework to generate descriptions from images. Recent
advances in statistical machine translation have shown that,
given a powerful sequence model, it is possible to achieve
state-of-the-art results by directly maximizing the proba-
bility of the correct translation given an input sentence in
an end-to-end fashion  both for training and inference.
These models make use of a recurrent neural network which
encodes the variable length input into a ﬁxed dimensional
vector, and uses this representation to decode it to the de-
sired output sentence. Thus, it is natural to use the same ap-
proach where, given an image (instead of an input sentence
in the source language), one applies the same principle of
translating it into its description.
Thus, we propose to directly maximize the probability of
the correct description given the image by using the follow-
ing formulation:
?= arg max
X
(I;S)logp(SjI;) (1)
whereare the parameters of our model, Iis an image, andSits correct transcription. Since Srepresents any sentence,
its length is unbounded. Thus, it is common to apply the
chain rule to model the joint probability over S0;:::;SN,
whereNis the length of this particular example as
logp(SjI) =NX
t=0logp(StjI;S 0;:::;St1) (2)
where we dropped the dependency on for convenience.
At training time, (S;I)is a training example pair, and we
optimize the sum of the log probabilities as described in (2)
over the whole training set using stochastic gradient descent
(further training details are given in Section 4).
It is natural to model p(StjI;S 0;:::;St1)with a Re-
current Neural Network (RNN), where the variable number
of words we condition upon up to t1is expressed by a
ﬁxed length hidden state or memory ht. This memory is
updated after seeing a new input xtby using a non-linear
functionf:
ht+1=f(ht;xt): (3)
To make the above RNN more concrete two crucial design
choices are to be made: what is the exact form of fand
how are the images and words fed as inputs xt. Forfwe
use a Long-Short Term Memory (LSTM) net, which has
shown state-of-the art performance on sequence tasks such
as translation. This model is outlined in the next section.
For the representation of images, we use a Convolutional
Neural Network (CNN). They have been widely used and
studied for image tasks, and are currently state-of-the art
for object recognition and detection. Our particular choice
of CNN uses a novel approach to batch normalization and
yields the current best performance on the ILSVRC 2014
classiﬁcation competition [12]. Furthermore, they have
been shown to generalize to other tasks such as scene clas-
siﬁcation by means of transfer learning [4]. The words are
represented with an embedding model.
3.1. LSTM-based Sentence Generator
The choice of fin (3) is governed by its ability to deal
with vanishing and exploding gradients [10], the most com-
mon challenge in designing and training RNNs. To address
this challenge, a particular form of recurrent nets, called
LSTM, was introduced [10] and applied with great success
to translation [3, 30] and sequence generation [9].
The core of the LSTM model is a memory cell cencod-
ing knowledge at every time step of what inputs have been
observed up to this step (see Figure 2) . The behavior of the
cell is controlled by gates  layers which are applied mul-
tiplicatively and thus can either keep a value from the gated
layer if the gate is 1or zero this value if the gate is 0. In
particular, three gates are being used which control whether
to forget the current cell value (forget gate f), if it should
hσσσc
inputLSTMmemory blockword predictionsoftmax
inputgate ioutputgate fforgetgate f
updatingtermct-1ctmt
xFigure 2. LSTM: the memory block contains a cell cwhich is
controlled by three gates. In blue we show the recurrent connec-
tions  the output mat time t1is fed back to the memory at
time tvia the three gates; the cell value is fed back via the forget
gate; the predicted word at time t1is fed back in addition to the
memory output mat time tinto the Softmax for word prediction.
read its input (input gate i) and whether to output the new
cell value (output gate o). The deﬁnition of the gates and
cell update and output are as follows:
it=(Wixxt+Wimmt1) (4)
ft=(Wfxxt+Wfmmt1) (5)
ot=(Woxxt+Wommt1) (6)
ct=ftct1+ith(Wcxxt+Wcmmt1)(7)
mt=otct (8)
pt+1 =Softmax (mt) (9)
whererepresents the product with a gate value, and the
variousWmatrices are trained parameters. Such multi-
plicative gates make it possible to train the LSTM robustly
as these gates deal well with exploding and vanishing gra-
dients [10]. The nonlinearities are sigmoid ()and hyper-
bolic tangent h(). The last equation mtis what is used to
feed to a Softmax, which will produce a probability distri-
butionptover all words.
Training The LSTM model is trained to predict each
word of the sentence after it has seen the image as well
as all preceding words as deﬁned by p(StjI;S 0;:::;St1).
For this purpose, it is instructive to think of the LSTM in un-
rolled form  a copy of the LSTM memory is created for theLSTMLSTMLSTMWeS1WeSN-1p1pNp2log p1(S1) log p2(S2) log pN(SN) ...LSTMWeS0S1SN-1S0
imageFigure 3. LSTM model combined with a CNN image embedder
(as deﬁned in [12]) and word embeddings. The unrolled connec-
tions between the LSTM memories are in blue and they corre-
spond to the recurrent connections in Figure 2. All LSTMs share
the same parameters.
image and each sentence word such that all LSTMs share
the same parameters and the output mt1of the LSTM at
timet1is fed to the LSTM at time t(see Figure 3). All
recurrent connections are transformed to feed-forward con-
nections in the unrolled version. In more detail, if we denote
byIthe input image and by S= (S0;:::;SN)a true sen-
tence describing this image, the unrolling procedure reads:
x1=CNN (I) (10)
xt=WeSt; t2f0:::N1g (11)
pt+1 =LSTM (xt); t2f0:::N1g (12)
where we represent each word as a one-hot vector Stof
dimension equal to the size of the dictionary. Note that we
denote byS0a special start word and by SNa special stop
word which designates the start and end of the sentence. In
particular by emitting the stop word the LSTM signals that a
complete sentence has been generated. Both the image and
the words are mapped to the same space, the image by using
a vision CNN, the words by using word embedding We.
The imageIis only input once, at t=1, to inform the
LSTM about the image contents. We empirically veriﬁed
that feeding the image at each time step as an extra input
yields inferior results, as the network can explicitly exploit
noise in the image and overﬁts more easily.
Our loss is the sum of the negative log likelihood of the
correct word at each step as follows:
L(I;S) =NX
t=1logpt(St): (13)
The above loss is minimized w.r.t. all the parameters of the
LSTM, the top layer of the image embedder CNN and word
embeddings We.Inference There are multiple approaches that can be used
to generate a sentence given an image, with NIC. The ﬁrst
one is Sampling where we just sample the ﬁrst word ac-
cording top1, then provide the corresponding embedding
as input and sample p2, continuing like this until we sample
the special end-of-sentence token or some maximum length.
The second one is BeamSearch : iteratively consider the set
of thekbest sentences up to time tas candidates to generate
sentences of size t+ 1, and keep only the resulting best k
of them. This better approximates S= arg max S0p(S0jI).
We used the BeamSearch approach in the following experi-
ments, with a beam of size 20. Using a beam size of 1 (i.e.,
greedy search) did degrade our results by 2 BLEU points on
average.
4. Experiments
We performed an extensive set of experiments to assess
the effectiveness of our model using several metrics, data
sources, and model architectures, in order to compare to
prior art.
4.1. Evaluation Metrics
Although it is sometimes not clear whether a description
should be deemed successful or not given an image, prior
art has proposed several evaluation metrics. The most re-
liable (but time consuming) is to ask for raters to give a
subjective score on the usefulness of each description given
the image. In this paper, we used this to reinforce that some
of the automatic metrics indeed correlate with this subjec-
tive score, following the guidelines proposed in [11], which
asks the graders to evaluate each generated sentence with a
scale from 1 to 41.
For this metric, we set up an Amazon Mechanical Turk
experiment. Each image was rated by 2 workers. The typ-
ical level of agreement between workers is 65%. In case
of disagreement we simply average the scores and record
the average as the score. For variance analysis, we perform
bootstrapping (re-sampling the results with replacement and
computing means/standard deviation over the resampled re-
sults). Like [11] we report the fraction of scores which are
larger or equal than a set of predeﬁned thresholds.
The rest of the metrics can be computed automatically
assuming one has access to groundtruth, i.e. human gen-
erated descriptions. The most commonly used metric so
far in the image description literature has been the BLEU
score [25], which is a form of precision of word n-grams
between generated and reference sentences2. Even though
1The raters are asked whether the image is described without any er-
rors, described with minor errors, with a somewhat related description, or
with an unrelated description, with a score of 4 being the best and 1 being
the worst.
2In this literature, most previous work report BLEU-1, i.e., they only
compute precision at the unigram level, whereas BLEU-n is a geometric
average of precision over 1- to n-grams.this metric has some obvious drawbacks, it has been shown
to correlate well with human evaluations. In this work,
we corroborate this as well, as we show in Section 4.3.
An extensive evaluation protocol, as well as the generated
outputs of our system, can be found at http://nic.
droppages.com/ .
Besides BLEU, one can use the perplexity of the model
for a given transcription (which is closely related to our
objective function in (1)). The perplexity is the geometric
mean of the inverse probability for each predicted word. We
used this metric to perform choices regarding model selec-
tion and hyperparameter tuning in our held-out set, but we
do not report it since BLEU is always preferred3. A much
more detailed discussion regarding metrics can be found in
[31], and research groups working on this topic have been
reporting other metrics which are deemed more appropriate
for evaluating caption. We report two such metrics - ME-
TEOR and Cider - hoping for much more discussion and
research to arise regarding the choice of metric.
Lastly, the current literature on image description has
also been using the proxy task of ranking a set of avail-
able descriptions with respect to a given image (see for in-
stance [14]). Doing so has the advantage that one can use
known ranking metrics like recall@k. On the other hand,
transforming the description generation task into a ranking
task is unsatisfactory: as the complexity of images to de-
scribe grows, together with its dictionary, the number of
possible sentences grows exponentially with the size of the
dictionary, and the likelihood that a predeﬁned sentence will
ﬁt a new image will go down unless the number of such
sentences also grows exponentially, which is not realistic;
not to mention the underlying computational complexity
of evaluating efﬁciently such a large corpus of stored sen-
tences for each image. The same argument has been used in
speech recognition, where one has to produce the sentence
corresponding to a given acoustic sequence; while early at-
tempts concentrated on classiﬁcation of isolated phonemes
or words, state-of-the-art approaches for this task are now
generative and can produce sentences from a large dictio-
nary.
Now that our models can generate descriptions of rea-
sonable quality, and despite the ambiguities of evaluating
an image description (where there could be multiple valid
descriptions not in the groundtruth) we believe we should
concentrate on evaluation metrics for the generation task
rather than for ranking.
4.2. Datasets
For evaluation we use a number of datasets which consist
of images and sentences in English describing these images.
3Even though it would be more desirable, optimizing for BLEU score
yields a discrete optimization problem. In general, perplexity and BLEU
scores are fairly correlated.The statistics of the datasets are as follows:
Dataset namesize
train valid. test
Pascal VOC 2008 [6] - - 1000
Flickr8k [26] 6000 1000 1000
Flickr30k [33] 28000 1000 1000
MSCOCO [20] 82783 40504 40775
SBU [24] 1M - -
With the exception of SBU, each image has been annotated
by labelers with 5 sentences that are relatively visual and
unbiased. SBU consists of descriptions given by image
owners when they uploaded them to Flickr. As such they
are not guaranteed to be visual or unbiased and thus this
dataset has more noise.
The Pascal dataset is customary used for testing only af-
ter a system has been trained on different data such as any of
the other four dataset. In the case of SBU, we hold out 1000
images for testing and train on the rest as used by [18]. Sim-
ilarly, we reserve 4K random images from the MSCOCO
validation set as test, called COCO-4k, and use it to report
results in the following section.
4.3. Results
Since our model is data driven and trained end-to-end,
and given the abundance of datasets, we wanted to an-
swer questions such as how dataset size affects general-
ization, what kinds of transfer learning it would be able
to achieve, and how it would deal with weakly labeled
examples. As a result, we performed experiments on ﬁve
different datasets, explained in Section 4.2, which enabled
us to understand our model in depth.
4.3.1 Training Details
Many of the challenges that we faced when training our
models had to do with overﬁtting. Indeed, purely supervised
approaches require large amounts of data, but the datasets
that are of high quality have less than 100000 images. The
task of assigning a description is strictly harder than object
classiﬁcation and data driven approaches have only recently
become dominant thanks to datasets as large as ImageNet
(with ten times more data than the datasets we described
in this paper, with the exception of SBU). As a result, we
believe that, even with the results we obtained which are
quite good, the advantage of our method versus most cur-
rent human-engineered approaches will only increase in the
next few years as training set sizes will grow.
Nonetheless, we explored several techniques to deal with
overﬁtting. The most obvious way to not overﬁt is to ini-
tialize the weights of the CNN component of our system
to a pretrained model (e.g., on ImageNet). We did this in
all the experiments (similar to [8]), and it did help quite alot in terms of generalization. Another set of weights that
could be sensibly initialized are We, the word embeddings.
We tried initializing them from a large news corpus [22],
but no signiﬁcant gains were observed, and we decided to
just leave them uninitialized for simplicity. Lastly, we did
some model level overﬁtting-avoiding techniques. We tried
dropout [34] and ensembling models, as well as exploring
the size (i.e., capacity) of the model by trading off number
of hidden units versus depth. Dropout and ensembling gave
a few BLEU points improvement, and that is what we report
throughout the paper.
We trained all sets of weights using stochastic gradi-
ent descent with ﬁxed learning rate and no momentum.
All weights were randomly initialized except for the CNN
weights, which we left unchanged because changing them
had a negative impact. We used 512 dimensions for the em-
beddings and the size of the LSTM memory.
Descriptions were preprocessed with basic tokenization,
keeping all words that appeared at least 5 times in the train-
ing set.
4.3.2 Generation Results
We report our main results on all the relevant datasets in Ta-
bles 1 and 2. Since PASCAL does not have a training set,
we used the system trained using MSCOCO (arguably the
largest and highest quality dataset for this task). The state-
of-the-art results for PASCAL and SBU did not use image
features based on deep learning, so arguably a big improve-
ment on those scores comes from that change alone. The
Flickr datasets have been used recently [11, 21, 14], but
mostly evaluated in a retrieval framework. A notable ex-
ception is [21], where they did both retrieval and genera-
tion, and which yields the best performance on the Flickr
datasets up to now.
Human scores in Table 2 were computed by comparing
one of the human captions against the other four. We do this
for each of the ﬁve raters, and average their BLEU scores.
Since this gives a slight advantage to our system, given the
BLEU score is computed against ﬁve reference sentences
and not four, we add back to the human scores the average
difference of having ﬁve references instead of four.
Given that the ﬁeld has seen signiﬁcant advances in the
last years, we do think it is more meaningful to report
BLEU-4, which is the standard in machine translation mov-
ing forward. Additionally, we report metrics shown to cor-
relate better with human evaluations in Table 14. Despite
recent efforts on better evaluation metrics [31], our model
fares strongly versus human raters. However, when evalu-
ating our captions using human raters (see Section 4.3.6),
our model fares much more poorly, suggesting more work
4We used the implementation of these metrics kindly provided in
http://www.mscoco.org .Metric BLEU-4 METEOR CIDER
NIC 27.7 23.7 85.5
Random 4.6 9.0 5.1
Nearest Neighbor 9.9 15.7 36.5
Human 21.7 25.2 85.4
Table 1. Scores on the MSCOCO development set.
Approach PASCAL Flickr Flickr SBU
(xfer) 30k 8k
Im2Text [24] 11
TreeTalk [18] 19
BabyTalk [16] 25
Tri5Sem [11] 48
m-RNN [21] 55 58
MNLM [14]556 51
SOTA 25 56 58 19
NIC 59 66 63 28
Human 69 68 70
Table 2. BLEU-1 scores. We only report previous work results
when available. SOTA stands for the current state-of-the-art.
is needed towards better metrics. On the ofﬁcial test set for
which labels are only available through the ofﬁcial website,
our model had a 27.2 BLEU-4.
4.3.3 Transfer Learning, Data Size and Label Quality
Since we have trained many models and we have several
testing sets, we wanted to study whether we could transfer
a model to a different dataset, and how much the mismatch
in domain would be compensated with e.g. higher quality
labels or more training data.
The most obvious case for transfer learning and data size
is between Flickr30k and Flickr8k. The two datasets are
similarly labeled as they were created by the same group.
Indeed, when training on Flickr30k (with about 4 times
more training data), the results obtained are 4 BLEU points
better. It is clear that in this case, we see gains by adding
more training data since the whole process is data-driven
and overﬁtting prone. MSCOCO is even bigger (5 times
more training data than Flickr30k), but since the collection
process was done differently, there are likely more differ-
ences in vocabulary and a larger mismatch. Indeed, all the
BLEU scores degrade by 10 points. Nonetheless, the de-
scriptions are still reasonable.
Since PASCAL has no ofﬁcial training set and was
collected independently of Flickr and MSCOCO, we re-
port transfer learning from MSCOCO (in Table 2). Doing
transfer learning from Flickr30k yielded worse results with
BLEU-1 at 53 (cf. 59).
Lastly, even though SBU has weak labeling (i.e., the la-
bels were captions and not human generated descriptions),
5We computed these BLEU scores with the outputs that the authors of
[14] kindly provided for their OxfordNet system.the task is much harder with a much larger and noisier vo-
cabulary. However, much more data is available for train-
ing. When running the MSCOCO model on SBU, our per-
formance degrades from 28 down to 16.
4.3.4 Generation Diversity Discussion
Having trained a generative model that gives p(SjI), an ob-
vious question is whether the model generates novel cap-
tions, and whether the generated captions are both diverse
and high quality. Table 3 shows some samples when re-
turning the N-best list from our beam search decoder in-
stead of the best hypothesis. Notice how the samples are di-
verse and may show different aspects from the same image.
The agreement in BLEU score between the top 15 generated
sentences is 58, which is similar to that of humans among
them. This indicates the amount of diversity our model gen-
erates. In bold are the sentences that are not present in the
training set. If we take the best candidate, the sentence is
present in the training set 80% of the times. This is not
too surprising given that the amount of training data is quite
small, so it is relatively easy for the model to pick exem-
plar sentences and use them to generate descriptions. If
we instead analyze the top 15 generated sentences, about
half of the times we see a completely novel description, but
still with a similar BLEU score, indicating that they are of
enough quality, yet they provide a healthy diversity.
A man throwing a frisbee in a park.
A man holding a frisbee in his hand.
A man standing in the grass with a frisbee.
A close up of a sandwich on a plate.
A close up of a plate of food with french fries.
A white plate topped with a cut in half sandwich.
A display case ﬁlled with lots of donuts.
A display case ﬁlled with lots of cakes.
A bakery display case ﬁlled with lots of donuts.
Table 3. N-best examples from the MSCOCO test set. Bold lines
indicate a novel sentence not present in the training set.
4.3.5 Ranking Results
While we think ranking is an unsatisfactory way to evalu-
ate description generation from images, many papers report
ranking scores, using the set of testing captions as candi-
dates to rank given a test image. The approach that works
best on these metrics (MNLM), speciﬁcally implemented a
ranking-aware loss. Nevertheless, NIC is doing surprisingly
well on both ranking tasks (ranking descriptions given im-
ages, and ranking images given descriptions), as can be seen
in Tables 4 and 5. Note that for the Image Annotation task,
we normalized our scores similar to what [21] used.ApproachImage Annotation Image Search
R@1 R@10 Med rR@1 R@10 Med r
DeFrag [13] 13 44 14 10 43 15
m-RNN [21] 15 49 11 12 42 15
MNLM [14] 18 55 8 13 52 10
NIC 20 61 6 19 64 5
Table 4. Recall@k and median rank on Flickr8k.
ApproachImage Annotation Image Search
R@1 R@10 Med rR@1 R@10 Med r
DeFrag [13] 16 55 8 10 45 13
m-RNN [21] 18 51 10 13 42 16
MNLM [14] 23 63 5 17 57 8
NIC 17 56 7 17 57 7
Table 5. Recall@k and median rank on Flickr30k.
Figure 4. Flickr-8k: NIC : predictions produced by NIC on the
Flickr8k test set (average score: 2.37); Pascal: NIC : (average
score: 2.45); COCO-1k: NIC : A subset of 1000 images from the
MSCOCO test set with descriptions produced by NIC (average
score: 2.72); Flickr-8k: ref : these are results from [11] on Flickr8k
rated using the same protocol, as a baseline (average score: 2.08);
Flickr-8k: GT : we rated the groundtruth labels from Flickr8k us-
ing the same protocol. This provides us with a calibration of the
scores (average score: 3.89)
4.3.6 Human Evaluation
Figure 4 shows the result of the human evaluations of the
descriptions provided by NIC, as well as a reference system
and groundtruth on various datasets. We can see that NIC
is better than the reference system, but clearly worse than
the groundtruth, as expected. This shows that BLEU is not
a perfect metric, as it does not capture well the difference
between NIC and human descriptions assessed by raters.
Examples of rated images can be seen in Figure 5. It is
interesting to see, for instance in the second image of the
ﬁrst column, how the model was able to notice the frisbee
given its size.Figure 5. A selection of evaluation results, grouped by human rating.
4.3.7 Analysis of Embeddings
In order to represent the previous word St1as input to
the decoding LSTM producing St, we use word embedding
vectors [22], which have the advantage of being indepen-
dent of the size of the dictionary (contrary to a simpler one-
hot-encoding approach). Furthermore, these word embed-
dings can be jointly trained with the rest of the model. It
is remarkable to see how the learned representations have
captured some semantic from the statistics of the language.
Table 4.3.7 shows, for a few example words, the nearest
other words found in the learned embedding space.
Note how some of the relationships learned by the model
will help the vision component. Indeed, having horse,
pony, and donkey close to each other will encourage the
CNN to extract features that are relevant to horse-looking
animals. We hypothesize that, in the extreme case where
we see very few examples of a class (e.g., unicorn), its
proximity to other word embeddings (e.g., horse) should
provide a lot more information that would be completely
lost with more traditional bag-of-words based approaches.
5. Conclusion
We have presented NIC, an end-to-end neural network
system that can automatically view an image and generateWord Neighbors
car van, cab, suv, vehicule, jeep
boy toddler, gentleman, daughter, son
street road, streets, highway, freeway
horse pony, donkey, pig, goat, mule
computer computers, pc, crt, chip, compute
Table 6. Nearest neighbors of a few example words
a reasonable description in plain English. NIC is based on
a convolution neural network that encodes an image into a
compact representation, followed by a recurrent neural net-
work that generates a corresponding sentence. The model is
trained to maximize the likelihood of the sentence given the
image. Experiments on several datasets show the robust-
ness of NIC in terms of qualitative results (the generated
sentences are very reasonable) and quantitative evaluations,
using either ranking metrics or BLEU, a metric used in ma-
chine translation to evaluate the quality of generated sen-
tences. It is clear from these experiments that, as the size
of the available datasets for image description increases, so
will the performance of approaches like NIC. Furthermore,
it will be interesting to see how one can use unsupervised
data, both from images alone and text alone, to improve im-
age description approaches.Acknowledgement
We would like to thank Geoffrey Hinton, Ilya Sutskever,
Quoc Le, Vincent Vanhoucke, and Jeff Dean for useful dis-
cussions on the ideas behind the paper, and the write up.
References
[1] A. Aker and R. Gaizauskas. Generating image descriptions
using dependency relational patterns. In ACL, 2010.
[2] D. Bahdanau, K. Cho, and Y . Bengio. Neural ma-
chine translation by jointly learning to align and translate.
arXiv:1409.0473 , 2014.
[3] K. Cho, B. van Merrienboer, C. Gulcehre, F. Bougares,
H. Schwenk, and Y . Bengio. Learning phrase representations
using RNN encoder-decoder for statistical machine transla-
tion. In EMNLP , 2014.
[4] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-
vation feature for generic visual recognition. In ICML , 2014.
[5] D. Elliott and F. Keller. Image description using visual de-
pendency representations. In EMNLP , 2013.
[6] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young,
C. Rashtchian, J. Hockenmaier, and D. Forsyth. Every pic-
ture tells a story: Generating sentences from images. In
ECCV , 2010.
[7] R. Gerber and H.-H. Nagel. Knowledge representation for
the generation of quantiﬁed natural language descriptions of
vehicle trafﬁc in image sequences. In ICIP . IEEE, 1996.
[8] Y . Gong, L. Wang, M. Hodosh, J. Hockenmaier, and
S. Lazebnik. Improving image-sentence embeddings using
large weakly annotated photo collections. In ECCV , 2014.
[9] A. Graves. Generating sequences with recurrent neural net-
works. arXiv:1308.0850 , 2013.
[10] S. Hochreiter and J. Schmidhuber. Long short-term memory.
Neural Computation , 9(8), 1997.
[11] M. Hodosh, P. Young, and J. Hockenmaier. Framing image
description as a ranking task: Data, models and evaluation
metrics. JAIR , 47, 2013.
[12] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
arXiv:1502.03167 , 2015.
[13] A. Karpathy, A. Joulin, and L. Fei-Fei. Deep fragment em-
beddings for bidirectional image sentence mapping. NIPS ,
2014.
[14] R. Kiros, R. Salakhutdinov, and R. S. Zemel. Unifying
visual-semantic embeddings with multimodal neural lan-
guage models. In arXiv:1411.2539 , 2014.
[15] R. Kiros and R. Z. R. Salakhutdinov. Multimodal neural lan-
guage models. In NIPS Deep Learning Workshop , 2013.
[16] G. Kulkarni, V . Premraj, S. Dhar, S. Li, Y . Choi, A. C. Berg,
and T. L. Berg. Baby talk: Understanding and generating
simple image descriptions. In CVPR , 2011.
[17] P. Kuznetsova, V . Ordonez, A. C. Berg, T. L. Berg, and
Y . Choi. Collective generation of natural image descriptions.
InACL, 2012.[18] P. Kuznetsova, V . Ordonez, T. Berg, and Y . Choi. Treetalk:
Composition and compression of trees for image descrip-
tions. ACL, 2(10), 2014.
[19] S. Li, G. Kulkarni, T. L. Berg, A. C. Berg, and Y . Choi. Com-
posing simple image descriptions using web-scale n-grams.
InConference on Computational Natural Language Learn-
ing, 2011.
[20] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. arXiv:1405.0312 , 2014.
[21] J. Mao, W. Xu, Y . Yang, J. Wang, and A. Yuille. Ex-
plain images with multimodal recurrent neural networks. In
arXiv:1410.1090 , 2014.
[22] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient
estimation of word representations in vector space. In ICLR ,
2013.
[23] M. Mitchell, X. Han, J. Dodge, A. Mensch, A. Goyal, A. C.
Berg, K. Yamaguchi, T. L. Berg, K. Stratos, and H. D. III.
Midge: Generating image descriptions from computer vision
detections. In EACL , 2012.
[24] V . Ordonez, G. Kulkarni, and T. L. Berg. Im2text: Describ-
ing images using 1 million captioned photographs. In NIPS ,
2011.
[25] K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: A
method for automatic evaluation of machine translation. In
ACL, 2002.
[26] C. Rashtchian, P. Young, M. Hodosh, and J. Hockenmaier.
Collecting image annotations using amazons mechanical
turk. In NAACL HLT Workshop on Creating Speech and
Language Data with Amazons Mechanical Turk , pages 139
147, 2010.
[27] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual
Recognition Challenge, 2014.
[28] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,
and Y . LeCun. Overfeat: Integrated recognition, localization
and detection using convolutional networks. arXiv preprint
arXiv:1312.6229 , 2013.
[29] R. Socher, A. Karpathy, Q. V . Le, C. Manning, and A. Y . Ng.
Grounded compositional semantics for ﬁnding and describ-
ing images with sentences. In ACL, 2014.
[30] I. Sutskever, O. Vinyals, and Q. V . Le. Sequence to sequence
learning with neural networks. In NIPS , 2014.
[31] R. Vedantam, C. L. Zitnick, and D. Parikh. CIDEr:
Consensus-based image description evaluation. In
arXiv:1411.5726 , 2015.
[32] B. Z. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu. I2t:
Image parsing to text description. Proceedings of the IEEE ,
98(8), 2010.
[33] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From im-
age descriptions to visual denotations: New similarity met-
rics for semantic inference over event descriptions. In ACL,
2014.
[34] W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent neural
network regularization. In arXiv:1409.2329 , 2014.
  Credit Card Fraud Detection Using Autoencoder 
Neural Network  
 
Ping Jiang (M.Eng)  Jinliang Zhang (M.Eng)  Junyi Zou (M.Eng)  
Department of Electrical & Computer 
Engineer  Department of Electrical & Computer 
Engineer  Department of Electrical & Computer 
Engineer  
University of Western Ontario  University of Western Ontario  University of Western Ontario  
pjiang28@uwo.ca  jzhan964@uwo.ca  jzou44@uwo.ca  
250985261  250919668  250833154  
 
AbstractImbalanced data classification problem 
has always been a popular topic  in the field of machine 
learning research.  In order to balance the samples 
between majority and minority class.  Oversampling  
algorithm is used to synthesiz e new minority class 
samples , but it could bring in noise.  Pointing to the 
noise proble ms, this paper  proposed a denoising 
autoencoder  neural network (DAE) algorithm which 
can not only oversample minority class sample  through 
misclassification cost, but it can denoise and classify  the 
sampled dataset. Through  experiment s, compared with 
the denoising  autoencoder neural network ( DAE) with 
oversampling process  and traditional fully connected 
neural networks,  the results showed  the proposed 
algorithm improves the  classification accuracy of 
minority class of imbalanced  datasets.  
Keywords imbalanc ed data; oversampling;  denoising 
autoencoder neural network; classificatio n 
 
I. INTRODUCTION  
Credit card fraud is a growing threat  with far 
reaching consequences in the finance industry, 
corpor ation s and government. Fraud can be defined as 
criminal deception with intent  of acquiring financial 
gain. As credit card became the most popular method 
of payment for both online and offline transaction, the 
fraud rate also accelerates. The main reasons for fraud 
is due to the lack of security, which involves the use of 
stolen credit card to get cash from bank through 
legitimate access. T his results in high difficulty of 
preventing credit card fraud.  
So how to do fraud detection is very significant. A 
lot of researches have been proposed to the detection of 
such credit card fraud, which account for majority of 
credit card frauds. Detecting using traditional method is infeasible because of the big data. However, 
financial institutions have focused their attention to 
recent computational methodologies to handle c redit 
card fraud problem.  
Classification problem is one of the key research 
topics in the field of machine learning. Currently 
available classification methods can only achieve 
preferable performance on balanced datasets. 
However, there are a large number of imbalanced 
datasets in pract ical application. For the fraud problem, 
the minority class, which is the abnormal transaction, 
is more important [ 1]. For instance, when minority 
class accounts for less than 1 percent of the total 
dataset, the overall accur acy reaches more than 99% 
even though all the minority class has been 
misclassified.  
Minority class sampling is a common method to 
handle with the imbalanced data classification 
problem. The main purpose of oversampling is to 
increase the number of minori ty class samples so that 
the original classification information can get better 
retention. Therefore, in the fields where there is higher 
demand for the classification accuracy, oversampling 
algorithm is chosen in general.  
This paper seeks to implement cre dit card fraud 
detection using denoising autoencoder and 
oversampling. For imbalanced data, we decided use 
above method to achieve proper model.  
 
II. RELATED  WORKS  
 Data mining technique is one notable methods used 
in solving fraud detection problem. This is the process 
of identifying those transactions  that are belong to 
frauds or not, which is based on the behaviors and 
habits of cardholder, many techniques have been applied to this area, artificial neural network  [2], 
genetic algorithm, support vector ma chine, frequent 
item set mining, decision tree, migrating birds 
optimization algorithm, Naïve Bayes. A comparative 
analy sis of logistic regression and Naïve Bayes is 
carried out in [3 ]. The performance of  Bayesian and 
neural network [4 ] is evaluated on cre dit card fraud 
data. Decision tree, neural networks and logistic 
regression are tested for their applicability  in fraud 
detections [5 ].  
 In a seminar work,  [6] propose s two advanced data 
mining approaches, support vector machines and 
random forests, toget her with logistic regression, as 
part of an attempt to better detect credit card fraud 
while neural network and logistic regression is applied 
on credit card fraud d etection problem [7 ]. A number 
of challenges are associated with credit card detection, 
namely fraudulent behavior profile is dynamic, that is 
fraudulent transactions tend to look like legitimate 
ones; credit card transaction datasets are rarely 
available and highly imbalanced (or skewed); optimal 
feature (variables) selection for the models; su itable 
metric to evaluate performance of techniques on 
skewed credit card fraud data. Credit card fraud 
detection performance is greatly affected by type of 
sampling approach used, selection of variables and 
detection technique(s) used.  
 
III. BACKGROUND  
3.1 Autoencoder  
A. Traditional Autoencoder Neural Network (AE)   
 Autoencoder is an artificial neural network used for 
unsupervised learning. The aim of autoencoder is to 
learn representations to reconstructs features for a set 
of data, typically for the pur pose of dimensionality 
reduction. The simplest form of an autoencoder is a 
feedforward, non -recurrent neural network which is 
similar to the  multilayer perceptron  [8]. As the figure 1 
shown, i t has 2 parts: one is encoder and the other is 
decoder which are  consist of by an input layer, one or 
more hidden layers and an output layer. The significant 
difference between autoencoder and multiplayer 
perceptron is that the output layer of autoencoder has 
the same number of neurons as the input layer. The 
purpose i s to reconstruct  its own inputs instead of 
predicting the target value from the  given inputs.   
Fig. 1  architecture of autoencoder neural network  
 In autoencoder, the network structure has 
connections between layers, but has no connection 
inside each laye r, 𝑥𝑖 is input sample, 𝑥𝑖 is output 
feature.  
 The training of autoencoder neural network is to 
optimize reconstruction error using the given samples. 
The cost function of autoencoder neural network 
defined in the project is (1)  
𝐽𝐴,𝐸= 1
𝑚  (1
2  𝑥𝑖𝑥𝑖2)𝑚
𝑖=1               (1) 
where m represents number of input samples.  
B. Denoising Autoencoder Neural Network (DAE)  
 For human, when people  see an object, if there is a 
small part of the object is blocked, they can still 
recognize it.  But how the autoenc oder does for the 
contaminated data ? There is a variation of traditional 
autoencoder named denoising autoencoder which 
could m ake autoencoder neural network learn  how to 
remove the noise and  reconstruct undisturbed input  as 
much as possible  [9]. 
 As show n in figure  2, the original data is  x, and 𝑥 
is the data corrupted with  noise. Through the complete 
process of denoising autoencoder, the output is 𝑥. The 
loss function tr ies to minimize the difference between 
the output and the original data so that the  autoencoder 
has the ability of eliminating the influence of noise and 
extracting features from the corrupted data. Therefore, 
the features generated from  the learning of input 
corrupted with noise are more robust,  which improved 
the data generalization ab ility of  autoencoder neural 
network model to input data .  
 
Fig. 2  Denoising autoencoder neural network  
The commonly used noises are Gaussian noise, 
and Salt  and pepper noise. And the  cost function of 
denoising autoencoder  neural network is defined 
according to  (2) 
𝐽𝐷𝐴,𝐸= 1
𝑚  (1
2  𝑥𝑖𝑥𝑖2)𝑚
𝑖=1            (2) 
where 𝑥=  𝑓((𝑤𝑥+𝑏)), w represents weights 
and b represents bias.  
3.2 Oversampling  
Imbalanced data set is a common problem faced in 
machine learning, s ince most traditional machine 
learning classification model  cant handle imbalanced 
dataset . High misclassification cost often happened on 
minority class, because classification model will try to 
classify all the data sample to the majority class.  
Oversamplin g is a technique used to deal with 
imbalanced dataset, its subject to create specific class 
sample so the class distribution of the original dataset 
can be balanced.  The benefit of using o versampling is 
shown in figure 3 . 
 
Fig. 3  Benefit of using oversamp ling 
SMOTE (Synthetic Minority Oversampling 
Technique) is one of the most popular oversampling 
technique. In order to create a synthetic data point, first 
we need to find a k -nearest -neighbors cluster in the 
feature space, then randomly find a point within  this 
cluster, finally using weighted average to forge the 
new data point.  
3.3 Classification f ully connected  model  
Deep fully connected neural network is often used 
in classification problem, with SoftMax cross entropy 
as the loss function, deep learnin g classification model 
can achieve very high accuracy.  
The SoftMax function is often used in the final 
layer of a neural network -based classifier, it first 
calculate s the exponential value of each output, then 
normalize all the output and let the sum of th e output 
equal to 1. SoftMax function is often used for 
probability distribution transformation, since the 
output of SoftMax function is within  range 0 to 1 that 
add up to 1, shown in the formula 3 , 
      P(𝑦𝑖|𝑥𝑖;W)=𝑒𝑓𝑦𝑖
𝑒𝑓𝑗𝑗                      (3) Entropy is a measure for information contents and 
could be defined as the unpredictability of an event. So, 
the greater the probability is, the smaller the 
unpredictability is, which means the information 
contents is also very small. If an event occurs inevitably 
with the probability of 100%, then the unpredictability 
and information content are 0. cross -entropy loss 
function takes advantages of feature of entropy 
equation, cross -entropy loss function can measure the 
good ness of a classification mo del, w hich is shown in 
formula 4 , 
J(θ)=1
𝑚  1{𝑦𝑖=𝑗}𝑙𝑜𝑔𝑒𝜃𝑗𝑇𝑥𝑖
 𝑒𝜃𝑗𝑇𝑥𝑖 𝑘
𝑖=1𝑘
𝑗=1𝑚
𝑖=1    (4) 
Cross -entropy can be used in multi -classification 
problems with the combination of SoftMax (do not 
consider regularization). Compared with quadratic loss 
function, cross -entropy loss function gives better 
training performance on neural networks.  
3.4 Model evaluation  metric  
Accuracy is not sufficient to evaluate a 
classification model, especially for imbalanced dataset. 
For example, an imbalanced dataset with 99.9% of 
normal data and 0.1% of abnormal data, if the 
classification labels all the sample as normal class, the 
model can still achieve 99.9% accuracy. However, for 
anomaly detection, the detection rate of anomaly class  
is very important. Confusion m atrix is often used in this 
situation.  
Table 1.  Confusion matrix for two -class problem  
 
Recall (Detection rate) is the ratio between the 
number of correctly detected anomalies and the total 
number of anomalies, it evaluates how much of the 
anomalies can be detected in this classification model.  
 
IV. METHODOLOGY  
The credit card fraud transaction dataset we are 
using is downloaded from Kaggle, with totally 28315 
transaction detail and 0.5% of them are labeled as 
fraud , the dataset is shown in the fig 4 . The  subject is 
to build a classification model for anomaly detection. 
Dataset  contains only numerical input after doing PCA 
transformation. Features V1, V2, ... V28 are the 
principal components, the only features which have not 
been transformed with PCA are  Time and Amount. 
Feature 'Class' is the response variable and it takes 
value 1 in case of fraud and 0 otherwise.  
 
Fig. 4 Relationship between two classes  
The idea is very straight forward. First, use 
oversampling to transform imbalanced dataset to 
balanced dataset. Then use denoised autoencoder to get 
denoised dataset. Finally using deep fully connected 
neural network model for final classification.  
 
Fig. 5 Flowchart of the porcess  
4.1 Data Preprocessing  
For dataset preprocessing, drop TIME data, and 
normalized the AMOUNT part. Other features are 
obtained by PCA, do not need to do normalization. 
Then choose the test sample, which account for 20% of 
the total sample.  
4.2 Oversampling  
Our group only perform oversampling on the 
training dataset. Before oversampling, there are total 
22652 transaction records in training dataset, with 
22538 samples in normal class and 114 samples in 
abnormal class. After over sampling, the training 
dataset contains 22538 samples in normal class and 
22538 samples in abnormal class.  
4.3 Denoising  autoencoder  
Our group designed a 7 layers  autoencoder for 
dataset denoising  process . After we got balanced 
training dat aset from oversa mpling, we add G aussian 
noise to the training dataset, then feed the training 
dataset into this denoised autoencoder. After training 
this denoised autoencoder model, this autoencoder has 
the capability to denoise the testing dataset in the 
prediction proce ss. Table 2.  Model design for denoised autoencoder  
Dataset with noise (29)  
Fully -Connected -Layer (22)  
Fully -Connected -Layer ( 15) 
Fully -Connected -Layer ( 10) 
Fully -Connected -Layer ( 15) 
Fully -Connected -Layer ( 22) 
Fully -Connected -Layer ( 29) 
Square Loss  Function  
4.4 Classifier  
Our group designed a 6 layers autoencoder for 
dataset denoise process . After we got denoised training 
dataset from denoised autoencoder, we feed the 
training dataset into this deep fully connected neural 
network classifier. In the  end, we are using SoftMax 
with cross -entropy as the loss function for final 
classification.  
Table 3.  Model design for classifier  
Denoised Dataset (29)  
Fully -Connected -Layer (22)  
Fully -Connected -Layer (15)  
Fully -Connected -Layer (10)  
Fully -Connected -Layer (5)  
Fully -Connected -Layer (2)  
SoftMax Cross Entropy Loss Function  
 
V. EVALUATION  AND  RESULTS  
This section first discusses the implementation 
details, then presents evaluation results comparing the 
oversampling model with model with out 
oversampling.  
5.1 Implementation details  
Our group using built -in function from sklearn 
package for dataset normalization, and built -in function 
SMOTE from imblearn package for oversampling. 
In addition, we implement the denoised autoencoder 
mode l and deep fully connected neural network 
classifier with TensorFlow.  We choose 
TensorFlow because its capable of GPU 
acceleration. All models are trained on GTX 1060 
discrete GPU w/6GB GDDR5 graphics memory. It 
took 10 minutes for each model to conver ge. 
5.2 Results  
After the training process, we perform evaluation 
process using another separated evaluation dataset. the 
accuracy rate and recall rate are applied to evaluate the 
accuracy of each model. Th e results are shown in the 
fig 6 and fig 7. 
 
Fig. 6 Result for model 1  
 
Fig. 7 Result for model 2  
For model 1 without the usage of oversampling and 
autoencoder, the recall rate is very low, because the 
model classifies all the sample as normal, which means 
most fraud transaction is not detected. For model 2 with 
oversampling and autoencoder , the recall rate is 
acceptable, which means most fraud transaction can be 
detected. Some evaluation result  of model 2 is showed 
in Table 4.  
Table 4.  Model 2 Evaluation Result  
Threshold  Recall Rate  Accuracy  
0.2 90.66%  83.56%  
0.3 89.33%  90.93%  0.4 88% 94.58%  
0.5 86.66%  96.73%  
0.6 84% 97.93%  
 
VI. CONCLUSION  
In machine learning area, imbalance data 
classification receives increasing attention as big data 
become popular. On account of the drawbacks of 
traditional method, oversampling algorithm and 
autoencoder can be used.  This study combined stacked 
denoising autoencoder neural network  with 
oversampling to build the model, which can achieve 
minority class sampling on the basis of 
misclassification cost, and denoise and classify the 
sampled datasets. The proposed algorithm increases  
classification accuracy of minority class  compared to 
the former methods, we can achieve different accuracy 
by controlling the threshold. In this study, when 
threshold equal to 0.6, we can achieve the best 
performance, which is 97.93%. However, t he 
dimensionality reduction of high -dimensional data still 
need to be further researched.  
 
REFERENCES  
[1] Y. Sahin, S. Bulkan, and E. Duman, A cost -sensitive decision 
tree approach for fraud detection, Expert Systems with 
Applications,vol. 40, pp. 5916 -5923, 2013.  
[2] Ogwueleka, F. N., (2011). Data Mining Application in Credit  
Card Fraud Detection System, Journal of Engineering Science  and 
Technology, Vol. 6, No. 3, pp. 311  322 
[3] Ng, A. Y., and Jordan, M. I., (2002). On discriminative vs. 
generative c lassifiers: A comparison of logistic regression and naive 
bayes. Advances in neural information processing systems, 2, 841 -
848. 
[4] Maes, S., Tuyls, K., Vanschoenwinkel, B., & Manderick, B. 
(2002). Credit card fraud detection using Bayesian and neural 
netw orks. In Proceedings of the 1st international naiso congress on 
neuro fuzzy technologies (pp. 261 -270).  
[5] Shen, A., Tong, R., & Deng, Y. (2007). Application of  
classification models on credit card fraud detection. In Service 
Systems and Service Managemen t, 2007 International Conference 
on (pp. 1 -4). IEEE.  
[6] Bhattacharyya, S., Jha, S., Tharakunnel, K., & Westland, J. C. 
(2011). Data mining for credit card fraud: A comparative study. 
Decision Support Systems, 50(3), 602 -613. 
[7] Sahin, Y. and Duman, E., (2011). Detecting credit card fraud by 
ANN and logistic regression. In Innovations in Intelligent Systems 
and Applications (INISTA), 2011 International Symposium on (pp. 
315-319). IEEE.  
[8] Autoencoder for Words, Liou, C. -Y., Cheng, C. -W., Liou, J. -W., 
and Liou, D. -R., Neurocomputing, Volume 139, 84 96 (2014), 
doi:10.1016/j.neucom.2013.09.055  
[9] M. Koziarski and M. Wożniak, CCR: A combined cleaning and 
resampling algorithm for imbalanced data classification, 
International Journal of Applied Mathematics a nd Computer 
Science, vol. 27, no. 4, 2017.  
 

  A Neural Representation of Sketch Drawings
David Ha
Google Brain
hadavid@google.comDouglas Eck
Google Brain
deck@google.com
Abstract
We present sketch-rnn , a recurrent neural network (RNN) able to construct
stroke-based drawings of common objects. The model is trained on a dataset of
human-drawn images representing many different classes. We outline a framework
for conditional and unconditional sketch generation, and describe new robust
training methods for generating coherent sketch drawings in a vector format.
1 Introduction
Recently, there have been major advancements in generative modelling of images using neural
networks as a generative tool. Generative Adversarial Networks (GANs) [ 5], Variational Inference
(VI) [ 15], and Autoregressive (AR) [ 19] models have become popular tools in this fast growing area.
Most of the work thus far has been targeted towards modelling low resolution, pixel images. Humans,
however, do not understand the world as a grid of pixels, but rather develop abstract concepts to
represent what we see. From a young age, we develop the ability to communicate what we see
by drawing on paper with a pencil or crayon. In this way we learn to express a sequential, vector
representation of an image as a short sequence of strokes. In this paper we investigate an alternative
to traditional pixel image modelling approaches, and propose a generative model for vector images.
Figure 1: Latent space interpolation of various vector images produced by our model.
Our goal is to train machines to draw and generalize abstract concepts in a manner similar to humans.
As a ﬁrst step towards this goal, we train our model on a dataset of hand-drawn sketches, each
represented as a sequence of motor actions controlling a pen: which direction to move, when to lift
the pen up, and when to stop drawing. In doing so, we created a model that potentially has many
applications, from assisting the creative process of an artist, to helping teach students how to draw.
This paper makes the following contributions: We outline a framework for both unconditional and
conditional generation of vector images composed of a sequence of lines. Our recurrent neural
network-based generative model is capable of producing sketches of common objects in a vector
format. We develop a training procedure unique to vector images to make the training more robust. In
arXiv:1704.03477v4  [cs.NE]  19 May 2017the conditional generation model, we explore the latent space developed by the model to represent a
vector image. We also discuss potential creative applications of our methodology. We make available
a large dataset of hand drawn vector images to encourage further development of generative modelling
for vector images, and also release an implementation of our model as an open source project.1
2 Related Work
There is a long history of work related to algorithms that mimic painters. One such work is Portrait
Drawing by Paul the Robot [ 23,25], where an underlying algorithm controlling a mechanical robot
arm sketches lines on a canvas with a programmable artistic style to mimic a given digitized portrait
of a person. Reinforcement Learning based-approaches [ 25] have been developed to discover a set of
paint brush strokes that can best represent a given input photograph. These prior works generally
attempt to mimic digitized photographs, rather than develop generative models of vector images.
Neural Network-based approaches have been developed for generative models of images, although
the majority of neural network-related research on image generation deal with pixel images [ 5,10,
12,14,19,24]. There has been relatively little work done on vector image generation using neural
networks. An earlier work [ 22] makes use of Hidden Markov Models to synthesize lines and curves
of a human sketch. More recent work [ 6] on handwriting generation with Recurrent Neural Networks
laid the groundwork for utilizing Mixture Density Networks [ 1] to generate continuous data points.
Recent works of this approach attempted to generate vectorized Kanji characters unconditionally [ 7]
and conditionally [26] by modelling Chinese characters as a sequence of pen stroke actions.
In addition to unconditionally generating sketches, we also explore encoding existing sketches
into a latent space of embedding vectors. Previous work [ 2] outlined a methodology to combine
Sequence-to-Sequence models with a Variational Autoencoder to model natural English sentences in
latent vector space. A related work [16], utilizes probabilistic program induction, rather than neural
networks, to perform one-shot modelling of the Omniglot dataset containing images of symbols.
One of the factors limiting research development in the space of generative vector drawings is the
lack of publicly available datasets. Previously, the Sketch dataset [ 4], consisting of 20K vector
sketches, was used to explore feature extraction techniques. A subsequent work, the Sketchy dataset
[20], provided 70K vector sketches along with corresponding pixel images for various classes. This
allowed for a larger-scale exploration of human sketches. ShadowDraw [ 17] is an interactive system
that predicts what a ﬁnished drawing looks like based on a set of incomplete brush strokes from the
user while the sketch is being drawn. ShadowDraw used a dataset of 30K raster images combined
with extracted vectorized features. In this work, we use a much larger dataset of vector sketches that
is made publicly available.
3 Methodology
3.1 Dataset
We constructed QuickDraw , a dataset of vector drawings obtained from Quick, Draw! [11], an online
game where the players are asked to draw objects belonging to a particular object class in less than 20
seconds. QuickDraw consists of hundreds of classes of common objects. Each class of QuickDraw
is a dataset of 70K training samples, in addition to 2.5K validation and 2.5K test samples.
We use a data format that represents a sketch as a set of pen stroke actions. This representation is an
extension of the format used in [ 6]. Our format extends the binary pen stroke event into a multi-state
event. In this data format, the initial absolute coordinate of the drawing is located at the origin. A
sketch is a list of points, and each point is a vector consisting of 5 elements: (x,y,p1,p2,p3).
The ﬁrst two elements are the offset distance in the x and y directions of the pen from the previous
point. The last 3 elements represents a binary one-hot vector of 3 possible states. The ﬁrst pen state,
p1, indicates that the pen is currently touching the paper, and that a line will be drawn connecting the
next point with the current point. The second pen state, p2, indicates that the pen will be lifted from
the paper after the current point, and that no line will be drawn next. The ﬁnal pen state, p3, indicates
that the drawing has ended, and subsequent points, including the current point, will not be rendered.
1The code and dataset is available at https://magenta.tensorflow.org/sketch_rnn .
23.2 Sketch-RNN
Figure 2: Schematic diagram of sketch-rnn .
Our model is a Sequence-to-Sequence Variational Autoencoder (V AE), similar to the architecture
described in [ 2,15]. Our encoder is a bidirectional RNN [ 21] that takes in a sketch as an input, and
outputs a latent vector of size Nz. Speciﬁcally, we feed the sketch sequence, S, and also the same
sketch sequence in reverse order, Sreverse , into two encoding RNNs that make up the bidirectional
RNN, to obtain two ﬁnal hidden states:
h=encode(S), h=encode(Sreverse ), h= [h;h] (1)
We take this ﬁnal concatenated hidden state, h, and project it into two vectors µandˆσ, each of size
Nz, using a fully connected layer. We convert ˆσinto a non-negative standard deviation parameter
σusing an exponential operation. We use µandσ, along withN(0,I), a vector of IID Gaussian
variables of size Nz, to construct a random vector, zRNz, as in the approach for a V AE [15]:
µ=Wµh+bµ,ˆσ=Wσh+bσ, σ= exp/parenleftBigˆσ
2/parenrightBig
, z=µ+σN(0,I) (2)
Under this encoding scheme, the latent vector zis not a deterministic output for a given input sketch,
but a random vector conditioned on the input sketch.
Our decoder is an autoregressive RNN that samples output sketches conditional on a given latent
vectorz. The initial hidden states h0, and optional cell states c0(if applicable) of the decoder RNN is
the output of a single layer network: [h0;c0] = tanh(Wzz+bz)
At each step iof the decoder RNN, we feed the previous point, Si1and the latent vector zin as
a concatenated input xi, whereS0is deﬁned as (0,0,1,0,0). The output at each time step are the
parameters for a probability distribution of the next data point Si. In Equation 3, we model (x,y)
as a Gaussian mixture model (GMM) with Mnormal distributions as in [ 1,6], and (q1,q2,q3)as
a categorical distribution to model the ground truth data (p1,p2,p3), where (q1+q2+q3= 1) as
done in [ 7] and [ 26]. Unlike [ 6], our generated sequence is conditioned from a latent code zsampled
from our encoder, which is trained end-to-end alongside the decoder.
p(x,y) =M/summationdisplay
j=1ΠjN(x,y|µx,j,µy,j,σx,j,σy,j,ρxy,j),whereM/summationdisplay
j=1Πj= 1 (3)
N(x,y|µx,µy,σx,σy,ρxy)is the probability distribution function for a bivariate normal distribution.
Each of theMbivariate normal distributions consist of ﬁve parameters: (µx,µy,σx,σy,ρxy), where
µxandµyare the means, σxandσyare the standard deviations, and ρxyis the correlation parameter of
each bivariate normal distribution. An additional vector Πof lengthM, also a categorical distribution,
are the mixture weights of the Gaussian mixture model. Hence the size of the output vector yis
5M+M+ 3, which includes the 3 logits needed to generate (q1,q2,q3).
The next hidden state of the RNN, generated with its forward operation, projects into the output
vectoryiusing a fully-connected layer:
xi= [Si1;z],[hi;ci] =forward (xi,[hi1;ci1]), yi=Wyhi+by, yiR6M+3(4)
The vectoryiis broken down into the parameters of the probability distribution of the next data point:
[ (ˆΠ1µxµyˆσxˆσyˆρxy)1...(ˆΠ1µxµyˆσxˆσyˆρxy)M(ˆq1ˆq2ˆq3) ] =yi (5)
3As in [ 6], we apply expandtanh operations to ensure the standard deviation values are non-negative,
and that the correlation value is between -1 and 1:
σx= exp(ˆσx), σy= exp(ˆσy), ρxy= tanh(ˆρxy) (6)
The probabilities for the categorical distributions are calculated using the outputs as logit values:
qk=exp(ˆqk)/summationtext3
j=1exp(ˆqj),k{1,2,3},Πk=exp(ˆΠk)/summationtextM
j=1exp(ˆΠj),k{1, ..., M} (7)
A key challenge is to train our model to know when to stop drawing. Because the probabilities of
the three pen stroke events are highly unbalanced, the model becomes more difﬁcult to train. The
probability of a p1event is much higher than p2, and thep3event will only happen once per drawing.
The approach developed in [ 7] and later followed by [ 26] was to use different weightings for each
pen event when calculating the losses, such as a hand-tuned weighting of (1,10,100) . We ﬁnd this
approach to be inelegant and inadequate for our dataset of diverse image classes.
We develop a simpler, more robust approach that works well for a broad class of sketch drawing data.
In our approach, all sequences are generated to a length of NmaxwhereNmaxis the length of the
longest sketch in our training dataset. In principle Nmaxcan be considered a hyper parameter. As the
length ofSis usually shorter than Nmax, we setSito be (0,0,0,0,1)fori>Ns. We discuss the
training in detail in the next section.
After training, we can sample sketches from our model. During the sampling process, we generate
the parameters for both GMM and categorical distributions at each time step, and sample an outcome
S/prime
ifor that time step. Unlike the training process, we feed the sampled outcome S/prime
ias input for the
next time step. We continue to sample until p3= 1, or when we have reached i=Nmax. Like the
encoder, the sampled output is not deterministic, but a random sequence, conditioned on the input
latent vector z. We can control the level of randomness we would like our samples to have during the
sampling process by introducing a temperature parameter τ:
ˆqkˆqk
τ,ˆΠkˆΠk
τ, σ2
xσ2
xτ, σ2
yσ2
yτ (8)
We can scale the softmax parameters of the categorial distribution and also the σparameters of the
bivariate normal distribution by a temperature parameter τ, to control the level of randomness in
our samples. τis typically set between 0 and 1. In the limiting case as τ0, our model becomes
deterministic and samples will consist of the most likely point in the probability density function.
Figure 3 illustrates of effect of sampling sketches with various temperature parameters.
3.3 Unconditional Generation
Figure 3: Unconditional generation of ﬁretrucks, yoga poses, gardens and owls with varying τ.
As a special case, we can also train our model to generate sketches unconditionally, where we only
train the decoder RNN module, without any input or latent vectors. By removing the encoder, the
decoder RNN as a standalone model is an autoregressive model without latent variables. In this use
case, the initial hidden states and cell states of the decoder RNN are initialized to zero. The inputs xi
of the decoder RNN at each time step is only Si1orS/prime
i1, as we do not need to concatenate a latent
vectorz. In Figure 3, we sample various sketch images generated unconditionally by varying the
temperature parameter from τ= 0.2at the top in blue, to τ= 0.9at the bottom in red.
43.4 Training
Our training procedure follows the approach of the Variational Autoencoder [ 15], where the loss
function is the sum of two terms: the Reconstruction Loss, LR, and the Kullback-Leibler Divergence
Loss,LKL. We train our model to optimize this two-part loss function. The Reconstruction loss
term, described in Equation 9, maximizes the log-likehood of the generated probability distribution
to explain the training data S. We can calculate this reconstruction loss, LR, using the generated
parameters of the pdf and the training data S.LRis composed of the sum of the log loss of the offset
terms (x,y),Ls, and the log loss of the pen state terms (p1,p2,p3),Lp:
Ls=1
NmaxNs/summationdisplay
i=1log/parenleftBigM/summationdisplay
j=1Πj,iN(xi,yi|µx,j,i,µy,j,i,σx,j,i,σy,j,i,ρxy,j,i)/parenrightBig
Lp=1
NmaxNmax/summationdisplay
i=13/summationdisplay
k=1pk,ilog(qk,i), LR=Ls+Lp(9)
Note that we discard the pdf parameters modelling the (x,y)points beyond Nswhen calculating
Ls, whileLpis calculated using all of the pdf parameters modelling the (p1,p2,p3)points until
Nmax. Both terms are normalized by the total sequence length Nmax. We found this methodology of
loss calculation to be more robust and allows the model to easily learn when it should stop drawing,
unlike the earlier mentioned method of assigning importance weightings to p1,p2, andp3.
The Kullback-Leibler (KL) divergence loss term measures the difference between the distribution of
our latent vector z, to that of an IID Gaussian vector with zero mean and unit variance. Optimizing
for this loss term allows us to minimize this difference. We use the result in [ 15], and calculate the
KL loss term, LKL, normalized by number of dimensions Nzof the latent vector:
LKL=1
2Nz/parenleftBig
1 + ˆσµ2exp(ˆσ)/parenrightBig
(10)
The loss function in Equation 11 is a weighted sum of both the LRandLKLloss terms:
Loss =LR+wKLLKL (11)
There is a tradeoff between optimizing for one term over the other. As wKL0, our model
approaches a pure autoencoder, sacriﬁcing the ability to enforce a prior over our latent space while
obtaining better reconstruction loss metrics. Note that for unconditional generation, where our model
is the standalone decoder, there will be no LKLterm as we only optimize for LR.
Figure 4: Tradeoff between LRandLKL, for two models trained on single class datasets (left).
Validation Loss Graph for models trained on the Yoga dataset using various wKL. (right)
Figure 4 illustrates the tradeoff between different settings of wKLand the resulting LRandLKL
metrics on the test set, along with the LRmetric on a standalone decoder RNN for comparison.
As the unconditional model does not receive any prior information about the entire sketch it needs
to generate, the LRmetric for the standalone decoder model serves as an upper bound for various
conditional models using a latent vector.
4 Experiments
We conduct several experiments with sketch-rnn for both conditional and unconditional vector
image generation. We train sketch-rnn on various QuickDraw classes using various settings for
5wKLand record the breakdown of losses. To experiment with a diverse set of classes with varying
complexities, we select the cat, pig, face, ﬁretruck, garden, owl, mosquito and yoga class. We also
experiment on multi-class datasets by concatenating different classes together to form (cat, pig) and
(crab, face, pig, rabbit). The results for test set evaluation on various datasets are displayed in Table 1.
Thesketch-rnn model treats the RNN cell as an abstract component. In our experiments, we
use Long Short-Term Memory (LSTM) [ 9] as the encoder RNN. For the decoder RNN, we use
HyperLSTM, as this type of RNN cell excels at sequence generation tasks [ 8]. The ability for
HyperLSTM to spontaneously augment its own weights enables it to adapt to many different regimes
in a large diverse dataset. For model conﬁguration details, please see the Supplementary Material.
Dataset wKL= 1.00 wKL= 0.50 wKL= 0.25 Decoder Only
LR LKL LR LKL LR LKL LR
cat -0.98 0.29 -1.33 0.70 -1.46 1.01 -0.57
pig -1.14 0.22 -1.37 0.49 -1.52 0.80 -0.82
cat, pig -1.02 0.22 -1.24 0.49 -1.50 0.98 -0.75
crab, face, pig, rabbit -0.91 0.22 -1.04 0.40 -1.47 1.17 -0.67
face -1.13 0.27 -1.55 0.71 -1.90 1.44 -0.73
ﬁretruck -1.24 0.22 -1.26 0.24 -1.78 1.10 -0.90
garden -0.79 0.20 -0.81 0.25 -0.99 0.54 -0.62
owl -0.93 0.20 -1.03 0.34 -1.29 0.77 -0.66
mosquito -0.67 0.30 -1.02 0.66 -1.41 1.54 -0.34
yoga -0.80 0.24 -1.07 0.55 -1.51 1.33 -0.48
Table 1: Loss ﬁgures ( LRandLKL) for various wKLsettings.
The relative loss numbers are consistent with our expectations. We see that the reconstruction loss
termLRdecreases as we relax the wKLparameter controlling the weight for the KL loss term, and
meanwhile the KL loss term LRincreases as a result. The LRfor the conditional model is strictly
less than the unconditional, standalone decoder model. In Figure 4 (right), we plot validation-set loss
graphs for on the yoga class for models with various wKLsettings. As LRdecreases, the LKLterm
tends to increase due to the tradeoff between LRandLKL.
4.1 Conditional Reconstruction
We qualitatively assess the reconstructed sketch S/primegiven an input sketch S. In Figure 5 (left), we
sample several reconstructions at various levels of temperature τusing a model trained on the single
cat class, starting at 0.01 on the left and linearly increasing to 1.0 on the right. The reconstructed cat
sketches have similar properties as the input image, and occasionally add or remove details such as a
whisker, a mouth, a nose, or the orientation of the tail.
Figure 5: Conditional generation of cats (left) and pigs (right).
When presented with a non-standard image of a cat, such as a cats face with three eyes, the
reconstructed cat only has two eyes. If we input a sketch from another image class, such a toothbrush,
the model seemingly generate sketches with similar orientation and properties as the toothbrush
input image, but with some cat-like features such as cat ears, whiskers or feet. We perform a similar
experiment with a model trained on the pig class, as shown in Figure 5 (right).
64.2 Latent Space Interpolation
By interpolating between latent vectors, we can visualize how one image morphs into another image
by visualizing the reconstructions of the interpolations. As we enforce a Gaussian prior on the latent
space, we expect fewer gaps in the space between two encoded latent vectors. We expect a model
trained using a higher wKLsetting to produce images that are closer to the data manifold given a
spherically interpolated [24] latent vector z, compared to another model trained with a lower wKL.
Figure 6: Latent space interpolation between cat and pig using with various wKLsettings (left).
Sketch Drawing Analogies (right).
To demonstrate this, we train several models using various wKL, on a dataset consisting of both cat
and pigs, and we encode two distinct images from the test set - a cat face and a full pig. Figure 6
(left) shows the reconstructed images from the interpolated latent vectors between the two original
images. As expected, models trained with higher wKLproduce more coherent interpolated images.
4.3 Sketch Drawing Analogies
The interpolation example in Figure 6 (left) suggests that the latent vector zencode conceptual
features of a sketch. Can we use these features to augment other sketches without such features  for
example, adding a body to a cats head? Indeed, we ﬁnd that sketch drawing analogies are possible
for models trained with low LKLnumbers. Given the smoothness of the latent space, where any
interpolated vector between two latent vectors results in a coherent sketch, we can perform vector
arithmetic on the latent vectors encoded from different sketches and explore how the model organizes
the latent space to represent different concepts in the manifold of generated sketches.
For example, as shown in Figure 6 (right), we can subtract the latent vector of an encoded pig head
from the latent vector of a full pig, to arrive at a vector that represents a body. Adding this difference
to the latent vector of a cat head results in a full cat (i.e. cat head + body = full cat). We repeat the
experiment to remove the body of a full pig. These drawing analogies allow us to explore how the
model organizes its latent space to represent different concepts in the manifold of generated sketches.
4.4 Predicting Different Endings of Incomplete Sketches
Figure 7: sketch-rnn predicting possible endings of various incomplete sketches (the red lines).
We can use sketch-rnn to ﬁnish an incomplete sketch. By using the decoder RNN as a standalone
model, we can generate a sketch that is conditioned on the previous points. We use the decoder RNN
to ﬁrst encode an incomplete sketch into a hidden state h. Afterwards, we generate the remaining
points of the sketch using has the initial hidden state. We show results in Figure 7 using decoder-only
models trained on individual classes, and sample completions by setting τ= 0.8.
75 Applications and Future Work
We believe sketch-rnn will enable many creative applications. Even the decoder-only model trained
on various classes can assist the creative process of an artist by suggesting many possible ways of
ﬁnishing a sketch, helping artists expand their imagination. In the conditional model, exploring the
latent space between different objects can potentially enable artists to ﬁnd interesting intersections
and relationships between different drawings. Even in the simplest use, pattern designers can apply
sketch-rnn to generate a large number of similar, but unique designs for textile or wallpaper prints.
As we saw earlier in Section 4.1, a model trained to draw pigs can be made to draw pig-like trucks if
given an input sketch of a truck. We can extend this result to applications that might help creative
designers come up with abstract designs that can resonate more with their target audience. For
instance, in Figure 8 (right), we feed sketches of four different chairs into our cat-drawing model to
produce four chair-like cats. We can even interpolate between the four images to explore the latent
space of chair-like cats, and select from a large grid of generated designs.
Figure 8: Generating similar, but unique sketches based on a single human sketch in the box (left).
Latent space of generated cats conditioned on sketch drawings of chairs (right).
A model trained on higher quality sketches may ﬁnd its way into educational applications that can
help teach students how to draw. Even with the simple sketches in QuickDraw , the authors of this
work have become much more proﬁcient at drawing animals, insects, and various sea creatures after
conducting these experiments. A related application is to encode a crude, poorly sketched drawing
and generate more aesthetically looking reproductions by using a model trained with a high wKL
setting and sampling with a low temperature τto produce a more coherent version of the drawing. In
the future, we can also investigate augmenting the latent vector in the direction that maximizes the
aesthetics of the drawing by incorporating user-rating data into the training process.
Combining hybrid variations of sequence-generation models with unsupervised, cross-domain pixel
image generation models, such as Image-to-Image models [ 3,13,18], is another exciting direction
that we can explore. We can already combine this model with supervised, cross-domain models such
as Pix2Pix [ 10], to occasionally generate photo realistic cat images from generated sketches of cats.
The opposite direction of converting a photograph of a cat into an unrealistic, but similar looking
sketch of a cat composed of a minimal number of lines seems to be a more interesting problem.
6 Conclusion
In this work, we develop a methodology to model sketch drawings using recurrent neural networks.
sketch-rnn is able to generate possible ways to ﬁnish an existing, but unﬁnished sketch drawing.
Our model can also encode existing sketches into a latent vector, and generate similar looking sketches
conditioned on the latent space. We demonstrate what it means to interpolate between two different
sketches by interpolating between its latent space, and also show that we can manipulate attributes
of a sketch by augmenting the latent space. We demonstrate the importance of enforcing a prior
distribution on the latent vector for coherent vector image generation during interpolation. By making
available a large dataset of sketch drawings, we hope to encourage further research and development
in the area of generative vector image modelling.
8References
[1] C. M. Bishop. Mixture density networks. Technical Report , 1994.
[2]S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Józefowicz, and S. Bengio. Generating Sentences from
a Continuous Space. CoRR , abs/1511.06349, 2015.
[3]H. Dong, P. Neekhara, C. Wu, and Y . Guo. Unsupervised Image-to-Image Translation with Generative
Adversarial Networks. ArXiv e-prints , Jan. 2017.
[4]M. Eitz, J. Hays, and M. Alexa. How Do Humans Sketch Objects? ACM Trans. Graph. (Proc. SIGGRAPH) ,
31(4):44:144:10, 2012.
[5] I. Goodfellow. NIPS 2016 Tutorial: Generative Adversarial Networks. ArXiv e-prints , Dec. 2017.
[6] A. Graves. Generating sequences with recurrent neural networks. arXiv:1308.0850 , 2013.
[7] D. Ha. Recurrent Net Dreams Up Fake Chinese Characters in Vector Format with TensorFlow, 2015.
[8] D. Ha, A. M. Dai, and Q. V . Le. HyperNetworks. In ICLR , 2017.
[9] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation , 1997.
[10] P. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros. Image-to-Image Translation with Conditional Adversarial
Networks. ArXiv e-prints , Nov. 2016.
[11] J. Jongejan, H. Rowley, T. Kawashima, J. Kim, and N. Fox-Gieg. The Quick, Draw! - A.I. Experiment.
https://quickdraw.withgoogle.com/ , 2016.
[12] C. Kaae Sønderby, T. Raiko, L. Maaløe, S. Kaae Sønderby, and O. Winther. Ladder Variational Autoen-
coders. ArXiv e-prints , Feb. 2016.
[13] T. Kim, M. Cha, H. Kim, J. Lee, and J. Kim. Learning to Discover Cross-Domain Relations with Generative
Adversarial Networks. ArXiv e-prints , Mar. 2017.
[14] D. P. Kingma, T. Salimans, and M. Welling. Improving variational inference with inverse autoregressive
ﬂow. CoRR , abs/1606.04934, 2016.
[15] D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. ArXiv e-prints , Dec. 2013.
[16] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through probabilistic
program induction. Science , 350(6266):13321338, Dec. 2015.
[17] Y . J. Lee, C. L. Zitnick, and M. F. Cohen. Shadowdraw: Real-time user guidance for freehand drawing. In
ACM SIGGRAPH 2011 Papers , SIGGRAPH 11, pages 27:127:10, New York, NY , USA, 2011. ACM.
[18] M.-Y . Liu, T. Breuel, and J. Kautz. Unsupervised Image-to-Image Translation Networks. ArXiv e-prints ,
Mar. 2017.
[19] S. Reed, A. van den Oord, N. Kalchbrenner, S. Gómez Colmenarejo, Z. Wang, D. Belov, and N. de Freitas.
Parallel Multiscale Autoregressive Density Estimation. ArXiv e-prints , Mar. 2017.
[20] P. Sangkloy, N. Burnell, C. Ham, and J. Hays. The Sketchy Database: Learning to Retrieve Badly Drawn
Bunnies. ACM Trans. Graph. , 35(4):119:1119:12, July 2016.
[21] M. Schuster, K. K. Paliwal, and A. General. Bidirectional recurrent neural networks. IEEE Transactions
on Signal Processing , 1997.
[22] S. Simhon and G. Dudek. Sketch interpretation and reﬁnement using statistical models. In Proceedings of
the Fifteenth Eurographics Conference on Rendering Techniques , EGSR04, pages 2332, Aire-la-Ville,
Switzerland, Switzerland, 2004. Eurographics Association.
[23] P. Tresset and F. Fol Leymarie. Portrait drawing by paul the robot. Comput. Graph. , 37(5):348363, Aug.
2013.
[24] T. White. Sampling Generative Networks. ArXiv e-prints , Sept. 2016.
[25] N. Xie, H. Hachiya, and M. Sugiyama. Artist agent: A reinforcement learning approach to automatic
stroke generation in oriental ink painting. In ICML . icml.cc / Omnipress, 2012.
[26] X. Zhang, F. Yin, Y . Zhang, C. Liu, and Y . Bengio. Drawing and Recognizing Chinese Characters with
Recurrent Neural Network. CoRR , abs/1606.06539, 2016.
9Supplementary Material for A Neural Representation of Sketch Drawings
1 Dataset Details
Figure 1: Example sketch drawings from QuickDraw dataset.
The data from Quick, Draw! [4] expands daily, and every so often new classes are added to the game.
As such, the QuickDraw dataset now consists of hundreds of classes, from 75 classes initially, in
Table 1. Each class consists of 70K training samples and 2.5K validation and test samples. Stroke
simpliﬁcation using the RamerDouglasPeucker algorithm [ 3] with a parameter of /epsilon1= 2.0has been
applied to simplify the lines. The data was originally recorded in pixel-dimensions, so we normalized
the offsets (x,y)using a single scaling factor. This scaling factor was calculated to adjust the
offsets in the training set to have a standard deviation of 1. For simplicity, we do not normalize the
offsets (x,y)to have zero mean, since the means are already relatively small.
alarm clock ambulance angel ant barn basket bee
bicycle book bridge bulldozer bus butterﬂy cactus
castle cat chair couch crab cruise ship dolphin
duck elephant eye face fan ﬁre hydrant ﬁretruck
ﬂamingo ﬂower garden hand hedgehog helicopter kangaroo
key lighthouse lion map mermaid octopus owl
paintbrush palm tree parrot passport peas penguin pig
pineapple postcard power outlet rabbit radio rain rhinoceros
roller coaster sandwich scorpion sea turtle sheep skull snail
snowﬂake speedboat spider strawberry swan swing set tennis racquet
the mona lisa toothbrush truck whale windmill
Table 1: Initial 75 QuickDraw classes used for this work.
Figure 2 below shows a training example, before normalization of (x,y)columns.
Figure 2: A sample sketch, as a sequence of (x,y,p1,p2,p3)points and in rendered form.
In the rendered sketch, the line color corresponds to the sequential stroke ordering.
12 Training Details
As a recap from the main text, we deﬁned the Reconstruction loss term LR)as:
Ls=1
NmaxNs/summationdisplay
i=1log/parenleftBigM/summationdisplay
j=1Πj,iN(xi,yi|µx,j,i,µy,j,i,σx,j,i,σy,j,i,ρxy,j,i)/parenrightBig
Lp=1
NmaxNmax/summationdisplay
i=13/summationdisplay
k=1pk,ilog(qk,i)
LR=Ls+Lp(1)
We also deﬁned the KL loss term LKLas:
LKL=1
2Nz/parenleftBig
1 + ˆσµ2exp(ˆσ)/parenrightBig
(2)
The loss function in Equation 3 is a weighted sum of both the LRandLKLloss terms:
Loss =LR+wKLLKL (3)
While the loss function in Equation 3 can be used during training, we ﬁnd that annealing the KL term
in the loss function (Equation 4) produced better results. This modiﬁcation is only used for model
training, and the original loss function in Equation 3 is still used to evaluate validation and test sets,
and for early stopping.
ηstep= 1(1ηmin)Rstep
Loss train =LR+wKLηstepmax(LKL,KL min)(4)
We ﬁnd that annealing the KL loss term generally results in better losses. Annealing the LKL
term in the loss function directs the optimizer to ﬁrst focus more on the reconstruction term in
Equation 1, which is the more difﬁcult loss term of the model to optimize for, before having to deal
with optimizing for the KL loss term in Equation 2, a far simpler expression in comparison. This
approach has been used in [ 2,5,7]. Our annealing term ηstepstarts atηmin(typically 0 or 0.01) at
training step 0, and converges to 1 for large training steps. Ris a term close to, but less than 1.
If the distribution of zis close enough to N(0,I), we can sample sketches from the decoder using
randomly sampled zfromN(0,I)as the input. In practice, we ﬁnd that going from a larger LKL
value (LKL>1.0) to a smaller LKLvalue of0.3generally results in a substantial increase in the
quality of sampled images using randomly sampled zN(0,I). However, going from LKL= 0.3
toLKLvalues closer to zero does not lead to any further noticeable improvements. Hence we ﬁnd it
useful to put a ﬂoor on LKLin the loss function by enforcing max(LKL,KL min)in Equation 4.
TheKL minterm inside the max operator is typically set to a small value such as 0.10 to 0.50. This
term will encourage the optimizer to put less focus on optimizing for the KL loss term LKLonce it is
low enough, so we can obtain better metrics for the reconstruction loss term LR. This approach is
similar to the approach described in [ 7] asfree bits , where they apply the max operator separately
inside each dimension of the latent vector z.
3 Model Conﬁguration
Our encoder and decoder RNNs consist of 512 and 2048 nodes respectively. In our model, we use
M= 20 mixture components for the decoder RNN. The latent vector zhasNz= 128 dimensions.
We apply Layer Normalization [ 1] to our model, and during training apply recurrent dropout [ 9] with
a keep probability of 90%. We train the model with batch sizes of 100 samples, using Adam [ 6] with
a learning rate of 0.0001 and gradient clipping of 1.0. All models are trained with KL min= 0.20,
R= 0.99999 . During training, we perform simple data augmentation by multiplying the offset
columns (x,y)by two IID random factors chosen uniformly between 0.90 and 1.10. Unless
mentioned otherwise, all experiments are conducted with wKL= 1.00.
24 Model Limitations
Although sketch-rnn can model a large variety of sketch drawings, there are several limitations in
the current approach we wish to highlight. For most single-class datasets, sketch-rnn is capable
of modelling sketches up to around 300 data points. The model becomes increasingly difﬁcult to
train beyond this length. For our dataset, we applied the RamerDouglasPeucker algorithm [ 3] to
simplify the strokes of the sketch data to less than 200 data points while still keeping most of the
important visual information of each sketch.
Figure 3: Unconditional generated sketches of frogs, cats, and crabs at τ= 0.8.
For more complicated classes of images, such as mermaids or lobsters, the reconstruction loss metrics
are not as good compared to simpler classes such as ants, faces or ﬁretrucks. The models trained on
these more challenging image classes tend to draw smoother, more circular line segments that do not
resemble individual sketches, but rather resemble an averaging of many sketches in the training set.
We can see some of this artifact in the frog class, in Figure 3. This smoothness may be analogous
to the blurriness effect produced by a Variational Autoencoder [ 8] that is trained on pixel images.
Depending on the use case of the model, smooth circular lines can be viewed as aesthetically pleasing
and a desirable property.
Figure 4: Unconditional generations from model trained on 75 classes (left),
From model trained on crab, face, pig and rabbit classes (right).
While both conditional and unconditional models are capable of training on datasets consisting of
several classes, such as (cat, pig), and (crab, face, pig, rabbit), sketch-rnn is ineffective at modelling
a large number of classes simultaneously. In Figure 4, we sample sketches using an unconditional
model trained on 75 classes, and a model trained on 4 classes. The samples generated from the
75-class model are incoherent, with individual sketches displaying features from multiple classes.
The four-class unconditional model usually generates samples of a single class, but occasionally also
combines features from multiple classes. In the future, we will explore incorporating class information
outside of the latent space to handle the modelling of a large number of classes simultaneously.
35 Multi-Sketch Drawing Interpolation
Figure 5: Example of conditional generated sketches with single class models.
Latent space interpolation from left to right, and then top to bottom.
In addition to interpolating between two sketches, like in Figure 5, we can also visualize the
interpolation between four sketches in latent space to gain further insight from the model. In this
section we show more examples conditionally generated with sketch-rnn . We take four generated
images, place them on four corners of a grid, and populate the rest of the grid using the interpolation
of the latent vectors at the corners. Figure 6 shows two examples of this four-way interpolation, using
models trained on both (cat, pig) classes, and face class. All samples generated with τ= 0.1.
Figure 6: Example input sketches and sketch-rnn generated reproductions (Top),
Latent space interpolation between the four reproduced sketches (Bottom).
The left side of Figure 7 visualizes the interpolation between a full pig, a rabbits head, a crab, and a
face, using a model trained on these four classes. In certain parts of the space between a crab and a
face is a rabbits head, and we see that the ears of the rabbit becomes the crabs claws. Applying the
model on the yoga class, it is interesting to see how one yoga position slowly transitions to another
via a set of interpolated yoga positions generated by the model. For visual effect, we also interpolate
between four distinct colors, and color each sketch using a unique interpolated color.
4Figure 7: Interpolation of (pig, rabbit, crab and face), yoga poses, mosquitoes and mermaids.
We also interpolate between four distinct colors for visual effect.
We also construct latent space interpolation examples for the mosquito class and the mermaid class,
in the last two grids Figure 7. We see that the model can interpolate between concepts such as style
of wings, leg counts, and orientation. In Figure 8 below, we show more interpolation examples of
other classes from the dataset.
Figure 8: Latent space interpolation between four generated gardens, owls, cats, and ﬁretrucks.
6 Which Loss Controls Image Coherency?
We would like to question the relative importance of the reconstruction loss term LR, relative to the
KL loss term LKL, when our goal is to produce higher quality image reconstructions. While our
reconstruction loss term LRoptimizes for the log-likelihood of the set of strokes that make up a
sketch, this metric alone does not give us any guarantee that a model with a lower LRnumber will
produce higher quality reconstructions compared to a model with a higher LRnumber.
For example, imagine a simple sketch of an face, /smiley, where most of the data points of Sare be used to
represent the head, and only a minority of points represent facial features such as the eyes and mouth.
It is possible to reconstruct the face with incoherent facial features, and yet still score a lower LR
number compared to another reconstruction with a coherent and similar face, if the edges around the
incoherent face are generated more precisely.
In Figure 9, we compare the reconstructed images generated using models trained with various wKL
settings. In the ﬁrst three examples from the left, we train our model on a dataset consisting of four
image classes (crab, face, pig, rabbit). We deliberately sketch input drawings that contain features of
two classes, such as a rabbit with a pig mouth and pig tail, a person with animal ears, and a rabbit with
crab claws. We see that the model trained using higher wKLweights, tend to generate sketches with
features of a single class that look more coherent, despite having lower LKLnumbers. For instance,
the model with wKL= 1.00omit pig features, animal ears, and crab claws from its reconstructions.
In contrast, the model with wKL= 0.25, with higher LKL, but lowerLRnumbers tries to keep both
inconsistent features, while generating sketches that look less coherent.
In the last three examples in Figure 9, we repeat the experiment on models trained on single-class
images, and see similar results even when we deliberately choose input samples from the test set with
noisier lines.
If we look at the interpolations produced in the Latent Space Interpolation section in the main text,
models with better KL loss terms also generate more meaningful reconstructions from the interpolated
5Figure 9: Reconstructions of sketch images using models with various wKLsettings.
space between two latent vectors. This suggests the latent vector for models with lower LKLcontrol
more meaningful parts of the drawings, such as controlling whether the sketch is an animal head
only or a full animal with a body, or whether to draw a cat head or a pig head. Altering such latent
vectors can allow us to directly manipulate these animal features. Conversely, altering the latent
codes of models with higher LKLresults in scattered movement of individual line segments, rather
than alterations of meaningful conceptual features of the animal.
This result is consistent with incoherent reconstructions seen in Figure 9. With a lower LKL, the
model is likely to generate coherent images given any random z. Even with a non-standard, or noisy,
input image, the model will still encode a zthat produces coherent images. For models with lower
LKLnumbers, the encoded latent vectors contain conceptual features belonging to the input image,
while for models with higher LKLnumbers, the latent vectors merely encode information about
speciﬁc line segments. This observation suggests that when using sketch-rnn on a new dataset, we
should ﬁrst try different wKLsettings to evaluate the tradeoff between LRandLKL, and then choose
a setting for wKL(andKL min) that best suit our requirements.
7 Acknowledgements
We thank Ian Johnson, Jonas Jongejan, Martin Wattenberg, Mike Schuster, Thomas Deselaers,
Ben Poole, Kyle Kastner, Junyoung Chung and Kyle McDonald for their help with this project. This
work was done as part of the Google Brain Residency program ( g.co/brainresidency ).
References
[1] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. NIPS , 2016.
[2]S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Józefowicz, and S. Bengio. Generating Sentences from
a Continuous Space. CoRR , abs/1511.06349, 2015.
[3]D. H. Douglas and T. K. Peucker. Algorithms for the reduction of the number of points required to represent
a digitized line or its caricature. Cartographica: The International Journal for Geographic Information and
Geovisualization , 10(2):112122, Oct. 1973.
[4]J. Jongejan, H. Rowley, T. Kawashima, J. Kim, and N. Fox-Gieg. The Quick, Draw! - A.I. Experiment.
https://quickdraw.withgoogle.com/ , 2016.
[5]C. Kaae Sønderby, T. Raiko, L. Maaløe, S. Kaae Sønderby, and O. Winther. Ladder Variational Autoencoders.
ArXiv e-prints , Feb. 2016.
[6] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR , 2015.
[7]D. P. Kingma, T. Salimans, and M. Welling. Improving variational inference with inverse autoregressive
ﬂow. CoRR , abs/1606.04934, 2016.
[8] D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. ArXiv e-prints , Dec. 2013.
[9]S. Semeniuta, A. Severyn, and E. Barth. Recurrent dropout without memory loss. arXiv:1603.05118 , 2016.
6
  CS294A Lecture notes
Andrew Ng
Sparse autoencoder
1 Introduction
Supervised learning is one of the most powerful tools of AI, and has led to
automatic zip code recognition, speech recognition, self-driving cars, and a
continually improving understanding of the human genome. Despite its sig-
nicant successes, supervised learning today is still severely limited. Speci-
cally, most applications of it still require that we manually specify the input
featuresxgiven to the algorithm. Once a good feature representation is
given, a supervised learning algorithm can do well. But in such domains as
computer vision, audio processing, and natural language processing, there're
now hundreds or perhaps thousands of researchers who've spent years of their
lives slowly and laboriously hand-engineering vision, audio or text features.
While much of this feature-engineering work is extremely clever, one has to
wonder if we can do better. Certainly this labor-intensive hand-engineering
approach does not scale well to new problems; further, ideally we'd like to
have algorithms that can automatically learn even better feature representa-
tions than the hand-engineered ones.
These notes describe the sparse autoencoder learning algorithm, which
is one approach to automatically learn features from unlabeled data. In some
domains, such as computer vision, this approach is not by itself competitive
with the best hand-engineered features, but the features it can learn do turn
out to be useful for a range of problems (including ones in audio, text, etc).
Further, there're more sophisticated versions of the sparse autoencoder (not
described in these notes, but that you'll hear more about later in the class)
that do surprisingly well, and in many cases are competitive with or superior
to even the best hand-engineered representations.
1These notes are organized as follows. We will rst describe feedforward
neural networks and the backpropagation algorithm for supervised learning.
Then, we show how this is used to construct an autoencoder, which is an
unsupervised learning algorithm. Finally, we build on this to derive a sparse
autoencoder. Because these notes are fairly notation-heavy, the last page
also contains a summary of the symbols used.
2 Neural networks
Consider a supervised learning problem where we have access to labeled train-
ing examples ( x(i);y(i)). Neural networks give a way of dening a complex,
non-linear form of hypotheses hW;b(x), with parameters W;b that we can t
to our data.
To describe neural networks, we will begin by describing the simplest
possible neural network, one which comprises a single neuron. We will use
the following diagram to denote a single neuron:
This neuron is a computational unit that takes as input x1;x2;x3(and
a +1 intercept term), and outputs hW;b(x) =f(WTx) =f(P3
i=1Wixi+b),
wheref:R7!Ris called the activation function . In these notes, we will
choosef() to be the sigmoid function:
f(z) =1
1 + exp(z):
Thus, our single neuron corresponds exactly to the input-output mapping
dened by logistic regression.
Although these notes will use the sigmoid function, it is worth noting that
another common choice for fis the hyperbolic tangent, or tanh, function:
f(z) = tanh(z) =ezez
ez+ez; (1)
Here are plots of the sigmoid and tanh functions:
2The tanh(z) function is a rescaled version of the sigmoid, and its output
range is [1;1] instead of [0 ;1].
Note that unlike CS221 and (parts of) CS229, we are not using the con-
vention here of x0= 1. Instead, the intercept term is handled separately by
the parameter b.
Finally, one identity that'll be useful later: If f(z) = 1=(1 + exp(z)) is
the sigmoid function, then its derivative is given by f0(z) =f(z)(1f(z)).
(Iffis the tanh function, then its derivative is given by f0(z) = 1(f(z))2.)
You can derive this yourself using the denition of the sigmoid (or tanh)
function.
2.1 Neural network formulation
A neural network is put together by hooking together many of our simple
neurons, so that the output of a neuron can be the input of another. For
example, here is a small neural network:
3In this gure, we have used circles to also denote the inputs to the net-
work. The circles labeled +1 are called bias units , and correspond to the
intercept term. The leftmost layer of the network is called the input layer ,
and the rightmost layer the output layer (which, in this example, has only
one node). The middle layer of nodes is called the hidden layer , because
its values are not observed in the training set. We also say that our example
neural network has 3 input units (not counting the bias unit), 3 hidden
units , and 1 output unit .
We will let nldenote the number of layers in our network; thus nl= 3
in our example. We label layer lasLl, so layerL1is the input layer, and
layerLnlthe output layer. Our neural network has parameters ( W;b) =
(W(1);b(1);W(2);b(2)), where we write W(l)
ijto denote the parameter (or weight)
associated with the connection between unit jin layerl, and unitiin layer
l+1. (Note the order of the indices.) Also, b(l)
iis the bias associated with unit
iin layerl+1. Thus, in our example, we have W(1)2R33, andW(2)2R13.
Note that bias units don't have inputs or connections going into them, since
they always output the value +1. We also let sldenote the number of nodes
in layerl(not counting the bias unit).
We will write a(l)
ito denote the activation (meaning output value) of
unitiin layerl. Forl= 1, we also use a(1)
i=xito denote the i-th input.
Given a xed setting of the parameters W;b, our neural network denes a
hypothesishW;b(x) that outputs a real number. Specically, the computation
that this neural network represents is given by:
a(2)
1=f(W(1)
11x1+W(1)
12x2+W(1)
13x3+b(1)
1) (2)
a(2)
2=f(W(1)
21x1+W(1)
22x2+W(1)
23x3+b(1)
2) (3)
a(2)
3=f(W(1)
31x1+W(1)
32x2+W(1)
33x3+b(1)
3) (4)
hW;b(x) =a(3)
1=f(W(2)
11a(2)
1+W(2)
12a(2)
2+W(2)
13a(2)
3+b(2)
1) (5)
In the sequel, we also let z(l)
idenote the total weighted sum of inputs to unit
iin layerl, including the bias term (e.g., z(2)
i=Pn
j=1W(1)
ijxj+b(1)
i), so that
a(l)
i=f(z(l)
i).
Note that this easily lends itself to a more compact notation. Specically,
if we extend the activation function f() to apply to vectors in an element-
wise fashion (i.e., f([z1;z2;z3]) = [f(z1);f(z2);f(z3)]), then we can write
4Equations (2-5) more compactly as:
z(2)=W(1)x+b(1)
a(2)=f(z(2))
z(3)=W(2)a(2)+b(2)
hW;b(x) =a(3)=f(z(3))
More generally, recalling that we also use a(1)=xto also denote the values
from the input layer, then given layer l's activations a(l), we can compute
layerl+ 1's activations a(l+1)as:
z(l+1)=W(l)a(l)+b(l)(6)
a(l+1)=f(z(l+1)) (7)
By organizing our parameters in matrices and using matrix-vector operations,
we can take advantage of fast linear algebra routines to quickly perform
calculations in our network.
We have so far focused on one example neural network, but one can
also build neural networks with other architectures (meaning patterns of
connectivity between neurons), including ones with multiple hidden layers.
The most common choice is a nl-layered network where layer 1 is the input
layer, layer nlis the output layer, and each layer lis densely connected to
layerl+ 1. In this setting, to compute the output of the network, we can
successively compute all the activations in layer L2, then layer L3, and so on,
up to layerLnl, using Equations (6-7). This is one example of a feedforward
neural network, since the connectivity graph does not have any directed loops
or cycles.
Neural networks can also have multiple output units. For example, here
is a network with two hidden layers layers L2andL3and two output units
in layerL4:
5To train this network, we would need training examples ( x(i);y(i)) where
y(i)2R2. This sort of network is useful if there're multiple outputs that
you're interested in predicting. (For example, in a medical diagnosis applica-
tion, the vector xmight give the input features of a patient, and the dierent
outputsyi's might indicate presence or absence of dierent diseases.)
2.2 Backpropagation algorithm
Suppose we have a xed training set f(x(1);y(1));:::; (x(m);y(m))gofmtrain-
ing examples. We can train our neural network using batch gradient descent.
In detail, for a single training example ( x;y), we dene the cost function with
respect to that single example to be
J(W;b;x;y) =1
2khW;b(x)yk2:
This is a (one-half) squared-error cost function. Given a training set of m
examples, we then dene the overall cost function to be
J(W;b) =
1
mmX
i=1J(W;b;x(i);y(i))#
+
2nl1X
l=1slX
i=1sl+1X
j=1
W(l)
ji2
(8)
=
1
mmX
i=11
2

hW;b(x(i))y(i)

2#
+
2nl1X
l=1slX
i=1sl+1X
j=1
W(l)
ji2
The rst term in the denition of J(W;b) is an average sum-of-squares error
term. The second term is a regularization term (also called a weight de-
cayterm) that tends to decrease the magnitude of the weights, and helps
prevent overtting.1Theweight decay parameter controls the rela-
tive importance of the two terms. Note also the slightly overloaded notation:
J(W;b;x;y) is the squared error cost with respect to a single example; J(W;b)
is the overall cost function, which includes the weight decay term.
This cost function above is often used both for classication and for re-
gression problems. For classication, we let y= 0 or 1 represent the two class
labels (recall that the sigmoid activation function outputs values in [0 ;1]; if
1Usually weight decay is not applied to the bias terms b(l)
i, as re
ected in our denition
forJ(W; b). Applying weight decay to the bias units usually makes only a small dierent
to the nal network, however. If you took CS229, you may also recognize weight decay
this as essentially a variant of the Bayesian regularization method you saw there, where we
placed a Gaussian prior on the parameters and did MAP (instead of maximum likelihood)
estimation.
6we were using a tanh activation function, we would instead use -1 and +1
to denote the labels). For regression problems, we rst scale our outputs to
ensure that they lie in the [0 ;1] range (or if we were using a tanh activation
function, then the [ 1;1] range).
Our goal is to minimize J(W;b) as a function of Wandb. To train
our neural network, we will initialize each parameter W(l)
ijand eachb(l)
ito
a small random value near zero (say according to a N(0;2) distribution
for some small , say 0:01), and then apply an optimization algorithm such
as batch gradient descent. Since J(W;b) is a non-convex function, gradient
descent is susceptible to local optima; however, in practice gradient descent
usually works fairly well. Finally, note that it is important to initialize the
parameters randomly, rather than to all 0's. If all the parameters start o at
identical values, then all the hidden layer units will end up learning the same
function of the input (more formally, W(1)
ijwill be the same for all values of
i, so thata(2)
1=a(2)
2=a(2)
3=:::for any input x). The random initialization
serves the purpose of symmetry breaking .
One iteration of gradient descent updates the parameters W;b as follows:
W(l)
ij:=W(l)
ij@
@W(l)
ijJ(W;b)
b(l)
i:=b(l)
i@
@b(l)
iJ(W;b)
whereis the learning rate. The key step is computing the partial derivatives
above. We will now describe the backpropagation algorithm, which gives
an ecient way to compute these partial derivatives.
We will rst describe how backpropagation can be used to compute
@
@W(l)
ijJ(W;b;x;y) and@
@b(l)
iJ(W;b;x;y), the partial derivatives of the cost func-
tionJ(W;b;x;y) dened with respect to a single example ( x;y). Once we can
compute these, then by referring to Equation (8), we see that the derivative
of the overall cost function J(W;b) can be computed as
@
@W(l)
ijJ(W;b) =
1
mmX
i=1@
@W(l)
ijJ(W;b;x(i);y(i))#
+W(l)
ij;
@
@b(l)
iJ(W;b) =1
mmX
i=1@
@b(l)
iJ(W;b;x(i);y(i)):
The two lines above dier slightly because weight decay is applied to Wbut
notb.
7The intuition behind the backpropagation algorithm is as follows. Given
a training example ( x;y), we will rst run a forward pass to compute
all the activations throughout the network, including the output value of the
hypothesishW;b(x). Then, for each node iin layerl, we would like to compute
an error term (l)
ithat measures how much that node was responsible
for any errors in our output. For an output node, we can directly measure
the dierence between the network's activation and the true target value,
and use that to dene (nl)
i(where layer nlis the output layer). How about
hidden units? For those, we will compute (l)
ibased on a weighted average
of the error terms of the nodes that uses a(l)
ias an input. In detail, here is
the backpropagation algorithm:
1. Perform a feedforward pass, computing the activations for layers L2,
L3, and so on up to the output layer Lnl.
2. For each output unit iin layernl(the output layer), set
(nl)
i=@
@z(nl)
i1
2kyhW;b(x)k2=(yia(nl)
i)f0(z(nl)
i)
3. Forl=nl1;nl2;nl3;:::; 2
For each node iin layerl, set
(l)
i= sl+1X
j=1W(l)
ji(l+1)
j!
f0(z(l)
i)
4. Compute the desired partial derivatives, which are given as:
@
@W(l)
ijJ(W;b;x;y) =a(l)
j(l+1)
i
@
@b(l)
iJ(W;b;x;y) =(l+1)
i:
Finally, we can also re-write the algorithm using matrix-vectorial nota-
tion. We will use   to denote the element-wise product operator (denoted
.* in Matlab or Octave, and also called the Hadamard product), so that
ifa=bc, thenai=bici. Similar to how we extended the denition of
f() to apply element-wise to vectors, we also do the same for f0() (so that
f0([z1;z2;z3]) = [@
@z1f(z1);@
@z2f(z2);@
@z3f(z3)]). The algorithm can then be
written:
81. Perform a feedforward pass, computing the activations for layers L2,
L3, up to the output layer Lnl, using Equations (6-7).
2. For the output layer (layer nl), set
(nl)=(ya(nl))f0(z(n))
3. Forl=nl1;nl2;nl3;:::; 2
Set
(l)=
(W(l))T(l+1)
f0(z(l))
4. Compute the desired partial derivatives:
rW(l)J(W;b;x;y) =(l+1)(a(l))T;
rb(l)J(W;b;x;y) =(l+1):
Implementation note: In steps 2 and 3 above, we need to compute f0(z(l)
i)
for each value of i. Assuming f(z) is the sigmoid activation function, we
would already have a(l)
istored away from the forward pass through the net-
work. Thus, using the expression that we worked out earlier for f0(z), we
can compute this as f0(z(l)
i) =a(l)
i(1a(l)
i).
Finally, we are ready to describe the full gradient descent algorithm. In
the pseudo-code below,  W(l)is a matrix (of the same dimension as W(l)),
and b(l)is a vector (of the same dimension as b(l)). Note that in this
notation,  W(l) is a matrix, and in particular it isn't  times W(l). We
implement one iteration of batch gradient descent as follows:
1. Set W(l):= 0, b(l):= 0 (matrix/vector of zeros) for all l.
2. Fori= 1 tom,
2a. Use backpropagation to compute rW(l)J(W;b;x;y) and
rb(l)J(W;b;x;y).
2b. Set W(l):= W(l)+rW(l)J(W;b;x;y).
2c. Set b(l):= b(l)+rb(l)J(W;b;x;y).
93. Update the parameters:
W(l):=W(l)1
mW(l)
+W(l)
b(l):=b(l)1
mb(l)
To train our neural network, we can now repeatedly take steps of gradient
descent to reduce our cost function J(W;b).
2.3 Gradient checking and advanced optimization
Backpropagation is a notoriously dicult algorithm to debug and get right,
especially since many subtly buggy implementations of it|for example, one
that has an o-by-one error in the indices and that thus only trains some of
the layers of weights, or an implementation that omits the bias term|will
manage to learn something that can look surprisingly reasonable (while per-
forming less well than a correct implementation). Thus, even with a buggy
implementation, it may not at all be apparent that anything is amiss. In
this section, we describe a method for numerically checking the derivatives
computed by your code to make sure that your implementation is correct.
Carrying out the derivative checking procedure described here will signi-
cantly increase your condence in the correctness of your code.
Suppose we want to minimize J() as a function of . For this example,
supposeJ:R7!R, so that2R. In this 1-dimensional case, one iteration
of gradient descent is given by
:=d
dJ():
Suppose also that we have implemented some function g() that purportedly
computesd
dJ(), so that we implement gradient descent using the update
:=g(). How can we check if our implementation of gis correct?
Recall the mathematical denition of the derivative as
d
dJ() = lim
!0J(+)J()
2:
Thus, at any specic value of , we can numerically approximate the deriva-
tive as follows:
J(+EPSILON )J(EPSILON )
2EPSILON
10In practice, we set EPSILON to a small constant, say around 104. (There's
a large range of values of EPSILON that should work well, but we don't set
EPSILON to be extremely small, say 1020, as that would lead to numerical
roundo errors.)
Thus, given a function g() that is supposedly computingd
dJ(), we can
now numerically verify its correctness by checking that
g()J(+EPSILON )J(EPSILON )
2EPSILON:
The degree to which these two values should approximate each other will
depend on the details of J. But assuming EPSILON = 104, you'll usually
nd that the left- and right-hand sides of the above will agree to at least 4
signicant digits (and often many more).
Now, consider the case where 2Rnis a vector rather than a single
real number (so that we have nparameters that we want to learn), and
J:Rn7!R. In our neural network example we used  J(W;b), but one
can imagine unrolling the parameters W;b into a long vector . We now
generalize our derivative checking procedure to the case where may be a
vector.
Suppose we have a function gi() that purportedly computes@
@iJ();
we'd like to check if giis outputting correct derivative values. Let (i+)=
+EPSILON~ ei, where
~ ei=2
666666640
0
...
1
...
03
77777775
is thei-th basis vector (a vector of the same dimension as , with a 1
in thei-th position and 0s everywhere else). So, (i+)is the same as
, except its i-th element has been incremented by EPSILON . Similarly, let
(i)=EPSILON~ eibe the corresponding vector with the i-th element
decreased by EPSILON . We can now numerically verify gi()'s correctness by
checking, for each i, that:
gi()J((i+))J((i))
2EPSILON:
When implementing backpropagation to train a neural network, in a cor-
11rect implementation we will have that
rW(l)J(W;b) =1
mW(l)
+W(l)
rb(l)J(W;b) =1
mb(l):
This result shows that the nal block of psuedo-code in Section 2.2 is indeed
implementing gradient descent. To make sure your implementation of gradi-
ent descent is correct, it is usually very helpful to use the method described
above to numerically compute the derivatives of J(W;b), and thereby verify
that your computations of1
mW(l)
+Wand1
mb(l)are indeed giving the
derivatives you want.
Finally, so far our discussion has centered on using gradient descent to
minimizeJ(). If you have implemented a function that computes J() and
rJ(), it turns out there are more sophisticated algorithms than gradient
descent for trying to minimize J(). For example, one can envision an algo-
rithm that uses gradient descent, but automatically tunes the learning rate
so as to try to use a step-size that causes to approach a local optimum
as quickly as possible. There are other algorithms that are even more so-
phisticated than this; for example, there are algorithms that try to nd an
approximation to the Hessian matrix, so that it can take more rapid steps
towards a local optimum (similar to Newton's method). A full discussion of
these algorithms is beyond the scope of these notes, but one example is the
L-BFGS algorithm. (Another example is conjugate gradient .) You will
use one of these algorithms in the programming exercise. The main thing you
need to provide to these advanced optimization algorithms is that for any ,
you have to be able to compute J() andrJ(). These optimization algo-
rithms will then do their own internal tuning of the learning rate/step-size 
(and compute its own approximation to the Hessian, etc.) to automatically
search for a value of that minimizes J(). Algorithms such as L-BFGS and
conjugate gradient can often be much faster than gradient descent.
3 Autoencoders and sparsity
So far, we have described the application of neural networks to supervised
learning, in which we are have labeled training examples. Now suppose we
have only unlabeled training examples set fx(1);x(2);x(3);:::g, wherex(i)2
Rn. Anautoencoder neural network is an unsupervised learning algorithm
that applies backpropagation, setting the target values to be equal to the
inputs. I.e., it uses y(i)=x(i).
12Here is an autoencoder:
The autoencoder tries to learn a function hW;b(x)x. In other words, it
is trying to learn an approximation to the identity function, so as to output
^xthat is similar to x. The identity function seems a particularly trivial
function to be trying to learn; but by placing constraints on the network,
such as by limiting the number of hidden units, we can discover interesting
structure about the data. As a concrete example, suppose the inputs xare
the pixel intensity values from a 10 10 image (100 pixels) so n= 100,
and there are s2= 50 hidden units in layer L2. Note that we also have
y2R100. Since there are only 50 hidden units, the network is forced to
learn a compressed representation of the input. I.e., given only the vector of
hidden unit activations a(2)2R50, it must try to reconstruct the 100-pixel
inputx. If the input were completely random|say, each xicomes from an
IID Gaussian independent of the other features|then this compression task
would be very dicult. But if there is structure in the data, for example, if
some of the input features are correlated, then this algorithm will be able to
discover some of those correlations.2
2In fact, this simple autoencoder often ends up learning a low-dimensional representa-
tion very similar to PCA's.
13Our argument above relied on the number of hidden units s2being small.
But even when the number of hidden units is large (perhaps even greater
than the number of input pixels), we can still discover interesting structure,
by imposing other constraints on the network. In particular, if we impose
asparsity constraint on the hidden units, then the autoencoder will still
discover interesting structure in the data, even if the number of hidden units
is large.
Informally, we will think of a neuron as being active (or as ring)
if its output value is close to 1, or as being inactive if its output value is
close to 0. We would like to constrain the neurons to be inactive most of the
time.3
Recall that a(2)
jdenotes the activation of hidden unit jin the autoencoder.
However, this notation doesn't make explicit what was the input xthat led
to that activation. Thus, we will write a(2)
j(x) to denote the activation of this
hidden unit when the network is given a specic input x. Further, let
^j=1
mmX
i=1h
a(2)
j(x(i))i
be the average activation of hidden unit j(averaged over the training set).
We would like to (approximately) enforce the constraint
^j=;
whereis asparsity parameter , typically a small value close to zero (say
= 0:05). In other words, we would like the average activation of each
hidden neuron jto be close to 0.05 (say). To satisfy this constraint, the
hidden unit's activations must mostly be near 0.
To achieve this, we will add an extra penalty term to our optimization
objective that penalizes ^ jdeviating signicantly from . Many choices of
the penalty term will give reasonable results. We will choose the following:
s2X
j=1log
^j+ (1) log1
1^j:
Here,s2is the number of neurons in the hidden layer, and the index jis
summing over the hidden units in our network. If you are familiar with the
3This discussion assumes a sigmoid activation function. If you are using a tanh activa-
tion function, then we think of a neuron as being inactive when it outputs values close to
-1.
14concept of KL divergence, this penalty term is based on it, and can also be
writtens2X
j=1KL(jj^j);
where KL(jj^j) =log
^j+ (1) log1
1^jis the Kullback-Leibler (KL)
divergence between a Bernoulli random variable with mean and a Bernoulli
random variable with mean ^ j. KL-divergence is a standard function for
measuring how dierent two dierent distributions are. (If you've not seen
KL-divergence before, don't worry about it; everything you need to know
about it is contained in these notes.)
This penalty function has the property that KL( jj^j) = 0 if ^j=, and
otherwise it increases monotonically as ^ jdiverges from . For example, in
the gure below, we have set = 0:2, and plotted KL( jj^j) for a range of
values of ^j:
We see that the KL-divergence reaches its minimum of 0 at ^ j=, and blows
up (it actually approaches 1) as ^japproaches 0 or 1. Thus, minimizing
this penalty term has the eect of causing ^ jto be close to .
Our overall cost function is now
Jsparse(W;b) =J(W;b) +s2X
j=1KL(jj^j);
whereJ(W;b) is as dened previously, and controls the weight of the
sparsity penalty term. The term ^ j(implicitly) depends on W;balso, because
it is the average activation of hidden unit j, and the activation of a hidden
unit depends on the parameters W;b.
To incorporate the KL-divergence term into your derivative calculation,
there is a simple-to-implement trick involving only a small change to your
15code. Specically, where previously for the second layer ( l= 2), during
backpropagation you would have computed
(2)
i= s3X
j=1W(3)
ji(3)
j!
f0(z(2)
i);
now instead compute
(2)
i= s3X
j=1W(3)
ji(3)
j!
f0(z(2)
i) +

^i+1
1^i
:
One subtlety is that you'll need to know ^ ito compute this term. Thus,
you'll need to compute a forward pass on all the training examples rst
to compute the average activations on the training set, before computing
backpropagation on any example. If your training set is small enough to t
comfortably in computer memory (this will be the case for the programming
assignment), you can compute forward passes on all your examples and keep
the resulting activations in memory and compute the ^ is. Then you can
use your precomputed activations to perform backpropagation on all your
examples. If your data is too large to t in memory, you may have to scan
through your examples computing a forward pass on each to accumulate (sum
up) the activations and compute ^ i(discarding the result of each forward
pass after you have taken its activations a(2)
iinto account for computing ^ i).
Then after having computed ^ i, you'd have to redo the forward pass for each
example so that you can do backpropagation on that example. In this latter
case, you would end up computing a forward pass twice on each example in
your training set, making it computationally less ecient.
The full derivation showing that the algorithm above results in gradient
descent is beyond the scope of these notes. But if you implement the au-
toencoder using backpropagation modied this way, you will be performing
gradient descent exactly on the objective Jsparse(W;b). Using the derivative
checking method, you will be able to verify this for yourself as well.
4 Visualization
Having trained a (sparse) autoencoder, we would now like to visualize the
function learned by the algorithm, to try to understand what it has learned.
Consider the case of training an autoencoder on 10 10 images, so that
16n= 100. Each hidden unit icomputes a function of the input:
a(2)
i=f 100X
j=1W(1)
ijxj+b(1)
i!
:
We will visualize the function computed by hidden unit i|which depends on
the parameters W(1)
ij(ignoring the bias term for now) using a 2D image. In
particular, we think of a(1)
ias some non-linear feature of the input x. We ask:
What input image xwould cause a(1)
ito be maximally activated? For this
question to have a non-trivial answer, we must impose some constraints on
x. If we suppose that the input is norm constrained by jjxjj2=P100
i=1x2
i1,
then one can show (try doing this yourself) that the input which maximally
activates hidden unit iis given by setting pixel xj(for all 100 pixels, j=
1;:::; 100) to
xj=W(1)
ijqP100
j=1(W(1)
ij)2:
By displaying the image formed by these pixel intensity values, we can begin
to understand what feature hidden unit iis looking for.
If we have an autoencoder with 100 hidden units (say), then we our
visualization will have 100 such images|one per hidden unit. By examining
these 100 images, we can try to understand what the ensemble of hidden
units is learning.
When we do this for a sparse autoencoder (trained with 100 hidden units
on 10x10 pixel inputs4) we get the following result:
4The results below were obtained by training on whitened natural images. Whitening
is a preprocessing step which removes redundancy in the input, by causing adjacent pixels
to become less correlated.
17Each square in the gure above shows the (norm bounded) input image x
that maximally actives one of 100 hidden units. We see that the dierent hid-
den units have learned to detect edges at dierent positions and orientations
in the image.
These features are, not surprisingly, useful for such tasks as object recog-
nition and other vision tasks. When applied to other input domains (such
as audio), this algorithm also learns useful representations/features for those
domains too.
185 Summary of notation
x Input features for a training example, x2Rn.
y Output/target values. Here, ycan be vector valued. In the case
of an autoencoder, y=x.
(x(i);y(i))Thei-th training example
hW;b(x)Output of our hypothesis on input x, using parameters W;b.
This should be a vector of the same dimension as the target
valuey.
W(l)
ijThe parameter associated with the connection between unit j
in layerl, and unitiin layerl+ 1.
b(l)
iThe bias term associated with unit iin layerl+ 1. Can also
be thought of as the parameter associated with the connection
between the bias unit in layer land unitiin layerl+ 1.
 Our parameter vector. It is useful to think of this as the result
of taking the parameters W;b and unrolling them into a long
column vector.
a(l)
iActivation (output) of unit iin layerlof the network. In addi-
tion, since layer L1is the input layer, we also have a(1)
i=xi.
f() The activation function. Throughout these notes, we used
f(z) = tanh(z).
z(l)
iTotal weighted sum of inputs to unit iin layerl. Thus,a(l)
i=
f(z(l)
i).
 Learning rate parameter
sl Number of units in layer l(not counting the bias unit).
nl Number layers in the network. Layer L1is usually the input
layer, and layer Lnlthe output layer.
 Weight decay parameter.
^x For an autoencoder, its output; i.e., its reconstruction of the
inputx. Same meaning as hW;b(x).
 Sparsity parameter, which species our desired level of sparsity
^i The average activation of hidden unit i(in the sparse autoen-
coder).
 Weight of the sparsity penalty term (in the sparse autoencoder
objective).
19
  Robust Speech Recognition via Large-Scale Weak Supervision
Alec Radford* 1Jong Wook Kim* 1Tao Xu1Greg Brockman1Christine McLeavey1Ilya Sutskever1
Abstract
We study the capabilities of speech processing
systems trained simply to predict large amounts of
transcripts of audio on the internet. When scaled
to 680,000 hours of multilingual and multitask
supervision, the resulting models generalize well
to standard benchmarks and are often competitive
with prior fully supervised results but in a zero-
shot transfer setting without the need for any fine-
tuning. When compared to humans, the models
approach their accuracy and robustness. We are
releasing models and inference code to serve as
a foundation for further work on robust speech
processing.
1. Introduction
Progress in speech recognition has been energized by the
development of unsupervised pre-training techniques exem-
plified by Wav2Vec 2.0 (Baevski et al., 2020). Since these
methods learn directly from raw audio without the need for
human labels, they can productively use large datasets of un-
labeled speech and have been quickly scaled up to 1,000,000
hours of training data (Zhang et al., 2021), far more than the
1,000 or so hours typical of an academic supervised dataset.
When fine-tuned on standard benchmarks, this approach
has improved the state of the art, especially in a low-data
setting.
These pre-trained audio encoders learn high-quality repre-
sentations of speech, but because they are purely unsuper-
vised they lack an equivalently performant decoder mapping
those representations to usable outputs, necessitating a fine-
tuning stage in order to actually perform a task such as
speech recognition1. This unfortunately limits their use-
fulness and impact as fine-tuning can still be a complex
process requiring a skilled practitioner. There is an addi-
tional risk with requiring fine-tuning. Machine learning
*Equal contribution1OpenAI, San Francisco, CA 94110, USA.
Correspondence to: Alec Radford <alec@openai.com >, Jong
Wook Kim <jongwook@openai.com >.
1Baevski et al. (2021) is an exciting exception - having devel-
oped a fully unsupervised speech recognition systemmethods are exceedingly adept at finding patterns within a
training dataset which boost performance on held-out data
from the same dataset. However, some of these patterns are
brittle and spurious and dont generalize to other datasets
and distributions. In a particularly disturbing example, Rad-
ford et al. (2021) documented a 9.2% increase in object
classification accuracy when fine-tuning a computer vision
model on the ImageNet dataset (Russakovsky et al., 2015)
without observing any improvement in average accuracy
when classifying the same objects on seven other natural
image datasets. A model that achieves superhuman per-
formance when trained on a dataset can still make many
basic errors when evaluated on another, possibly precisely
because it is exploiting those dataset-specific quirks that
humans are oblivious to (Geirhos et al., 2020).
This suggests that while unsupervised pre-training has im-
proved the quality of audio encoders dramatically, the lack
of an equivalently high-quality pre-trained decoder, com-
bined with a recommended protocol of dataset-specific fine-
tuning, is a crucial weakness which limits their usefulness
and robustness. The goal of a speech recognition system
should be to work reliably out of the box in a broad range
of environments without requiring supervised fine-tuning of
a decoder for every deployment distribution.
As demonstrated by Narayanan et al. (2018), Likhomanenko
et al. (2020), and Chan et al. (2021) speech recognition sys-
tems that are pre-trained in a supervised fashion across many
datasets/domains exhibit higher robustness and generalize
much more effectively to held-out datasets than models
trained on a single source. These works achieve this by
combining as many existing high-quality speech recogni-
tion datasets as possible. However, there is still only a
moderate amount of this data easily available. SpeechStew
(Chan et al., 2021) mixes together 7 pre-existing datasets
totalling 5,140 hours of supervision. While not insignifi-
cant, this is still tiny compared to the previously mentioned
1,000,000 hours of unlabeled speech data utilized in Zhang
et al. (2021).
Recognizing the limiting size of existing high-quality super-
vised datasets, recent efforts have created larger datasets for
speech recognition. By relaxing the requirement of gold-
standard human-validated transcripts, Chen et al. (2021) and
Galvez et al. (2021) make use of sophisticated automatedRobust Speech Recognition via Large-Scale Weak Supervision 2
pipelines to scale weakly supervised speech recognition
to 10,000 and 30,000 hours of noisier training data. This
trade-off between quality and quantity is often the right
call. Although understudied so far for speech recognition,
recent work in computer vision has demonstrated that mov-
ing beyond gold-standard crowdsourced datasets such as
ImageNet (Russakovsky et al., 2015) to much larger but
weakly supervised datasets significantly improves the ro-
bustness and generalization of models (Mahajan et al., 2018;
Kolesnikov et al., 2020).
Yet these new datasets are only a few times larger than the
sum of existing high-quality datasets and still much smaller
than prior unsupervised work. In this work we close that
gap, scaling weakly supervised speech recognition the next
order of magnitude to 680,000 hours of labeled audio data.
We call our approach Whisper2. We demonstrate models
trained at this scale transfer well to existing datasets zero-
shot, removing the need for any dataset-specific fine-tuning
to achieve high-quality results.
In addition to scale, our work also focuses on broaden-
ing the scope of weakly supervised pre-training beyond
English-only speech recognition to be both multilingual and
multitask. Of those 680,000 hours of audio, 117,000 hours
cover 96 other languages. The dataset also includes 125,000
hours of Xentranslation data. We find that for sufficiently
large models there is no drawback and even benefits to joint
multilingual and multitask training.
Our work suggests that simple scaling of weakly supervised
pre-training has been underappreciated so far for speech
recognition. We achieve these results without the need for
the self-supervision or self-training techniques that have
been a mainstay of recent large-scale speech recognition
work. To serve as a foundation for further research on robust
speech recognition, we release inference code and models at
the following URL: https://github.com/openai/
whisper .
2. Approach
2.1. Data Processing
Following the trend of recent work leveraging web-scale
text from the internet for training machine learning systems,
we take a minimalist approach to data pre-processing. In
contrast to a lot of work on speech recognition, we train
Whisper models to predict the raw text of transcripts without
any significant standardization, relying on the expressive-
ness of sequence-to-sequence models to learn to map be-
tween utterances and their transcribed form. This simplifies
2If an acronym or basis for the name is desired, WSPSR stand-
ing for Web-scale Supervised Pretraining for Speech Recognition
can be used.the speech recognition pipeline since it removes the need
for a separate inverse text normalization step in order to
produce naturalistic transcriptions.
We construct the dataset from audio that is paired with tran-
scripts on the Internet. This results in a very diverse dataset
covering a broad distribution of audio from many different
environments, recording setups, speakers, and languages.
While diversity in audio quality can help train a model to be
robust, diversity in transcript quality is not similarly bene-
ficial. Initial inspection showed a large amount of subpar
transcripts in the raw dataset. To address this, we developed
several automated filtering methods to improve transcript
quality.
Many transcripts on the internet are not actually human-
generated but the output of existing ASR systems. Recent
research has shown that training on datasets of mixed human
and machine-generated data can significantly impair the per-
formance of translation systems (Ghorbani et al., 2021). In
order to avoid learning transcript-ese, we developed many
heuristics to detect and remove machine-generated tran-
scripts from the training dataset. Many existing ASR sys-
tems output only a limited subset of written language which
removes or normalizes away aspects that are difficult to pre-
dict from only audio signals such as complex punctuation
(exclamation points, commas, and question marks), format-
ting whitespace such as paragraphs, or stylistic aspects such
as capitalization. An all-uppercase or all-lowercase tran-
script is very unlikely to be human generated. While many
ASR systems include some level of inverse text normaliza-
tion, it is often simple or rule-based and still detectable from
other unhandled aspects such as never including commas.
We also use an audio language detector, which was created
by fine-tuning a prototype model trained on a prototype ver-
sion of the dataset on V oxLingua107 (Valk & Alum ae, 2021)
to ensure that the spoken language matches the language of
the transcript according to CLD2. If the two do not match,
we dont include the (audio, transcript) pair as a speech
recognition training example in the dataset. We make an
exception if the transcript language is English and add these
pairs to the dataset as Xenspeech translation training
examples instead. We use fuzzy de-duping of transcript
texts to reduce the amount of duplication and automatically
generated content in the training dataset.
We break audio files into 30-second segments paired with
the subset of the transcript that occurs within that time
segment. We train on all audio, including segments where
there is no speech (though with sub-sampled probability)
and use these segments as training data for voice activity
detection.
For an additional filtering pass, after training an initial model
we aggregated information about its error rate on trainingRobust Speech Recognition via Large-Scale Weak Supervision 3
data sources and performed manual inspection of these data
sources sorting by a combination of both high error rate and
data source size in order to identify and remove low-quality
ones efficiently. This inspection showed a large amount of
only partially transcribed or poorly aligned/misaligned tran-
scripts as well as remaining low-quality machine-generated
captions that filtering heuristics did not detect.
To avoid contamination, we perform de-duplication at a tran-
script level between the training dataset and the evaluation
datasets we thought were at higher risk of overlap, namely
TED-LIUM 3 (Hernandez et al., 2018).
2.2. Model
Since the focus of our work is on studying the capabilities
of large-scale supervised pre-training for speech recogni-
tion, we use an off-the-shelf architecture to avoid confound-
ing our findings with model improvements. We chose an
encoder-decoder Transformer (Vaswani et al., 2017) as this
architecture has been well validated to scale reliably. All
audio is re-sampled to 16,000 Hz, and an 80-channel log-
magnitude Mel spectrogram representation is computed on
25-millisecond windows with a stride of 10 milliseconds.
For feature normalization, we globally scale the input to
be between -1 and 1 with approximately zero mean across
the pre-training dataset. The encoder processes this input
representation with a small stem consisting of two convolu-
tion layers with a filter width of 3 and the GELU activation
function (Hendrycks & Gimpel, 2016) where the second
convolution layer has a stride of two. Sinusoidal position
embeddings are then added to the output of the stem after
which the encoder Transformer blocks are applied. The
transformer uses pre-activation residual blocks (Child et al.,
2019), and a final layer normalization is applied to the en-
coder output. The decoder uses learned position embeddings
and tied input-output token representations (Press & Wolf,
2017). The encoder and decoder have the same width and
number of transformer blocks. Figure 1 summarizes the
model architecture.
We use the same byte-level BPE text tokenizer used in GPT-
2 (Sennrich et al., 2015; Radford et al., 2019) for the English-
only models and refit the vocabulary (but keep the same size)
for the multilingual models to avoid excessive fragmenta-
tion on other languages since the GPT-2 BPE vocabulary is
English only.
2.3. Multitask Format
Although predicting which words were spoken in a given
audio snippet is a core part of the full speech recognition
problem and extensively studied in research, it is not the
only part. A fully featured speech recognition system can
involve many additional components such as voice activ-
ity detection, speaker diarization, and inverse text normal-ization. These components are often handled separately,
resulting in a relatively complex system around the core
speech recognition model. To reduce this complexity, we
would like to have a single model perform the entire speech
processing pipeline, not just the core recognition part. An
important consideration here is the interface for the model.
There are many different tasks that can be performed on
the same input audio signal: transcription, translation, voice
activity detection, alignment, and language identification
are some examples.
For this kind of one-to-many mapping to work with a single
model, some form of task specification is necessary. We use
a simple format to specify all tasks and conditioning infor-
mation as a sequence of input tokens to the decoder. Since
our decoder is an audio-conditional language model, we also
train it to condition on the history of text of the transcript in
the hope that it will learn to use longer-range text context
to resolve ambiguous audio. Specifically, with some proba-
bility we add the transcript text preceding the current audio
segment to the decoders context. We indicate the beginning
of prediction with a <|startoftranscript|> token.
First, we predict the language being spoken which is repre-
sented by a unique token for each language in our training
set (99 total). These language targets are sourced from the
aforementioned V oxLingua107 model. In the case where
there is no speech in an audio segment, the model is trained
to predict a <|nospeech|> token indicating this. The
next token specifies the task (either transcription or trans-
lation) with an <|transcribe|> or<|translate|>
token. After this, we specify whether to predict timestamps
or not by including a <|notimestamps|> token for that
case. At this point, the task and desired format is fully
specified, and the output begins. For timestamp predic-
tion, we predict time relative to the current audio segment,
quantizing all times to the nearest 20 milliseconds which
matches the native time resolution of Whisper models, and
add additional tokens to our vocabulary for each of these.
We interleave their prediction with the caption tokens: the
start time token is predicted before each captions text, and
the end time token is predicted after. When a final tran-
script segment is only partially included in the current 30-
second audio chunk, we predict only its start time token
for the segment when in timestamp mode, to indicate that
the subsequent decoding should be performed on an au-
dio window aligned with that time, otherwise we truncate
the audio to not include the segment. Lastly, we add a
<|endoftranscript|> token. We only mask out the
training loss over the previous context text, and train the
model to predict all other tokens. Please see Figure 1 for an
overview of our format and training setup.Robust Speech Recognition via Large-Scale Weak Supervision 4

2  Conv1D + GELUcross attention
Log-Mel Spectrogram~
SOT ENTRANS-  
CRIBE 0.0 The quick
Tokens in Multitask T raining FormatTransformer  
Encoder Blocks  Transformer  
Decoder Blocks  EN 0.0 The quick brown
 next-token  
prediction
Sinusoidal  
Positional  
Encoding
Learned  
Positional  
EncodingMultitask training data (680k hours)Sequence-to-sequence learning
Multitask training formatEnglish transcription
Any-to-English speech translation
Non-English transcription
No speech   Ask not what y our country can do for  
  Ask not what y our country can do for 
   El rápido z orro marrón salta sobre   
  The quick brown fo x jumps o ver 
   언덕 위에  올라  내려다보면  너무나  넓고  넓은    
  언덕 위에  올라  내려다보면  너무나  넓고  넓은  
 (background music pla ying) 
  
PREV
special
tokenstext 
tokenstimestamp
tokensSTART OF  
TRANSCRIPTLANGUAGE  
TAG
NO 
SPEECHEOTTRANSCRIBE
TRANSLA TEbegin  
time
NO 
TIMEST AMPSend 
timetext tokensbegin  
timeend 
timetext tokens
text tokens
Voice activity  
detection  
(VAD)Custom vocabulary /
promptingTime-aligned transcription
Text-only transcription  
(allows dataset-specific fine-tuning)X  English  
Translation previous  
text tokensX  X  
Transcription Language  
identificationMLP
self attention
MLP
self attention
MLP
self attentionMLP
cross attention
self attention
MLP
cross attention
self attention
MLP
cross attention
self attentionTRANS-  
CRIBE
Figure 1. Overview of our approach. A sequence-to-sequence Transformer model is trained on many different speech processing tasks,
including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. All of these
tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing for a single model to replace many different
stages of a traditional speech processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or
classification targets, as further explained in Section 2.3.
2.4. Training Details
We train a suite of models of various sizes in order to study
the scaling properties of Whisper. Please see Table 1 for an
overview. We train with data parallelism across accelerators
using FP16 with dynamic loss scaling and activation check-
pointing (Griewank & Walther, 2000; Chen et al., 2016).
Models were trained with AdamW (Loshchilov & Hutter,
2017) and gradient norm clipping (Pascanu et al., 2013)
with a linear learning rate decay to zero after a warmup over
the first 2048 updates. A batch size of 256 segments was
used, and the models are trained for 220updates which is
between two and three passes over the dataset. Due to only
training for a few epochs, over-fitting is not a large concern,
and we do not use any data augmentation or regularization
and instead rely on the diversity contained within such alarge dataset to encourage generalization and robustness.
Please see Appendix F for full training hyperparameters.3
During early development and evaluation we observed that
Whisper models had a tendency to transcribe plausible but
almost always incorrect guesses for the names of speakers.
This happens because many transcripts in the pre-training
dataset include the name of the person who is speaking,
encouraging the model to try to predict them, but this infor-
mation is only rarely inferable from only the most recent 30
3After the original release of Whisper, we trained an additional
Large model (denoted V2) for 2.5X more epochs while adding
SpecAugment (Park et al., 2019), Stochastic Depth (Huang et al.,
2016), and BPE Dropout (Provilkov et al., 2019) for regularization.
Reported results have been updated to this improved model unless
otherwise specified.Robust Speech Recognition via Large-Scale Weak Supervision 5
Model Layers Width Heads Parameters
Tiny 4 384 6 39M
Base 6 512 8 74M
Small 12 768 12 244M
Medium 24 1024 16 769M
Large 32 1280 20 1550M
Table 1. Architecture details of the Whisper model family.
seconds of audio context. To avoid this, we fine-tune Whis-
per models briefly on the subset of transcripts that do not
include speaker annotations which removes this behavior.
3. Experiments
3.1. Zero-shot Evaluation
The goal of Whisper is to develop a single robust speech
processing system that works reliably without the need for
dataset specific fine-tuning to achieve high-quality results
on specific distributions. To study this capability, we re-
use a wide set of existing speech processing datasets to
check whether Whisper is able to generalize well across
domains, tasks, and languages. Instead of using the standard
evaluation protocol for these datasets, which include both
a train and test split, we evaluate Whisper in a zero-shot
setting without using any of the training data for each of
these datasets so that we are measuring broad generalization.
3.2. Evaluation Metrics
Speech recognition research typically evaluates and com-
pares systems based on the word error rate (WER) metric.
However, WER, which is based on string edit distance, pe-
nalizes all differences between the models output and the
reference transcript including innocuous differences in tran-
script style. As a result, systems that output transcripts that
would be judged as correct by humans can still have a large
WER due to minor formatting differences. While this poses
a problem for all transcribers, it is particularly acute for
zero-shot models like Whisper, which do not observe any
examples of specific datasets transcript formats.
This is not a novel observation; the development of evalua-
tion metrics that better correlate with human judgement is an
active area of research, and while there are some promising
methods, none have seen widespread adoption for speech
recognition yet. We opt to address this problem with ex-
tensive standardization of text before the WER calculation
to minimize penalization of non-semantic differences. Our
text normalizer was developed through iterative manual in-
spection to identify common patterns where naive WER
penalized Whisper models for an innocuous difference. Ap-
pendix C includes full details. For several datasets, we
observe WER drops of up to 50 percent usually due to aquirk such as a datasets reference transcripts seperating
contractions from words with whitespace. We caution this
development procedure comes at a risk of overfitting to the
transcription style of Whisper models which we investigate
in Section 4.4. We are releasing the code for our text nor-
malizer to allow for easy comparison and to help others
study the performance of speech recognition systems in
out-of-distribution settings.
3.3. English Speech Recognition
In 2015, Deep Speech 2 (Amodei et al., 2015) reported
a speech recognition system matched human-level perfor-
mance when transcribing the LibriSpeech test-clean split.
As part of their analysis they concluded: Given this result,
we suspect that there is little room for a generic speech sys-
tem to further improve on clean read speech without further
domain adaptation.  Yet seven years later the SOTA WER
on LibriSpeech test-clean has dropped another 73% from
their 5.3% to 1.4% (Zhang et al., 2021), far below their re-
ported human-level error rate of 5.8%. Despite this massive
and unanticipated further improvement in performance on
held-out but in-distribution data, speech recognition mod-
els trained on LibriSpeech remain far above human error
rates when used in other settings. What explains this gap
between reportedly superhuman performance in-distribution
and subhuman performance out-of-distribution?
We suspect a large part of this gap between human and
machine behavior is due to conflating different capabilities
being measured by human and machine performance on
a test set. This claim may seem confusing at first; if both
humans and machines are taking the same test, how can it be
that different skills are being tested? The difference arises
not in the testing but in how they trained for it. Humans are
often asked to perform a task given little to no supervision
on the specific data distribution being studied. Thus human
performance is a measure of out-of-distribution generaliza-
tion. But machine learning models are usually evaluated
after training on a large amount of supervision from the
evaluation distribution, meaning that machine performance
is instead a measure of in-distribution generalization. While
both humans and machines are being evaluated on the same
testdata, two quite different abilities are being measured
due to a difference in train data.
Whisper models, which are trained on a broad and diverse
distribution of audio and evaluated in a zero-shot setting,
could potentially match human behavior much better than
existing systems. To study whether this is the case (or
whether the difference between machine and human per-
formance is due to yet-to-be-understood factors) we can
compare Whisper models with both human performance
and standard fine-tuned machine learning models and check
which they more closely match.Robust Speech Recognition via Large-Scale Weak Supervision 6
0 1 2 3 4 5 6 7 8
WER on LibriSpeech dev-clean (%)01020304050Average WER on [Common Voice, CHiME-6, TED-LIUM] (%)Supervised LibriSpeech models
Zero-shot Whisper models
Zero-shot Human (Alec)
Ideal robustness (y = x)
Figure 2. Zero-shot Whisper models close the gap to human
robustness. Despite matching or outperforming a human on Lib-
riSpeech dev-clean, supervised LibriSpeech models make roughly
twice as many errors as a human on other datasets demonstrating
their brittleness and lack of robustness. The estimated robustness
frontier of zero-shot Whisper models, however, includes the 95%
confidence interval for this particular human.
To quantify this difference, we examine both overall ro-
bustness, that is average performance across many distribu-
tions/datasets, and effective robustness, introduced by Taori
et al. (2020), which measures the difference in expected
performance between a reference dataset, which is usually
in-distribution, and one or more out-of-distribution datasets.
A model with high effective robustness does better than
expected on out-of-distribution datasets as a function of its
performance on the reference dataset and approaches the
ideal of equal performance on all datasets. For our analy-
sis, we use LibriSpeech as the reference dataset due to its
central role in modern speech recognition research and the
availability of many released models trained on it, which
allows for characterizing robustness behaviors. We use a
suite of 12 other academic speech recognition datasets to
study out-of-distribution behaviors. Full details about these
datasets can be found in Appendix A.
Our main findings are summarized in Figure 2 and Table 2.
Although the best zero-shot Whisper model has a relatively
unremarkable LibriSpeech clean-test WER of 2.5, which
is roughly the performance of modern supervised baseline
or the mid-2019 state of the art, zero-shot Whisper models
have very different robustness properties than supervised
LibriSpeech models and out-perform all benchmarked Lib-
riSpeech models by large amounts on other datasets. Evenwav2vec 2.0 Whisper RER
Dataset Large (no LM) Large V2 (%)
LibriSpeech Clean 2.7 2.7 0.0
Artie 24.5 6.2 74.7
Common V oice 29.9 9.0 69.9
Fleurs En 14.6 4.4 69.9
Tedlium 10.5 4.0 61.9
CHiME6 65.8 25.5 61.2
V oxPopuli En 17.9 7.3 59.2
CORAAL 35.6 16.2 54.5
AMI IHM 37.0 16.9 54.3
Switchboard 28.3 13.8 51.2
CallHome 34.8 17.6 49.4
WSJ 7.7 3.9 49.4
AMI SDM1 67.6 36.4 46.2
LibriSpeech Other 6.2 5.2 16.1
Average 29.3 12.8 55.2
Table 2. Detailed comparison of effective robustness across var-
ious datasets. Although both models perform within 0.1% of
each other on LibriSpeech, a zero-shot Whisper model performs
much better on other datasets than expected for its LibriSpeech
performance and makes 55.2% less errors on average. Results
reported in word error rate (WER) for both models after applying
our text normalizer.
the smallest zero-shot Whisper model, which has only 39
million parameters and a 6.7 WER on LibriSpeech test-clean
is roughly competitive with the best supervised LibriSpeech
model when evaluated on other datasets. When compared
to a human in Figure 2, the best zero-shot Whisper models
roughly match their accuracy and robustness. For a detailed
breakdown of this large improvement in robustness, Table
2 compares the performance of the best zero-shot Whisper
model with a supervised LibriSpeech model that has the
closest performance to it on LibriSpeech test-clean. Despite
their very close performance on the reference distribution,
the zero-shot Whisper model achieves an average relative
error reduction of 55.2% when evaluated on other speech
recognition datasets.
This finding suggests emphasizing zero-shot and out-of-
distribution evaluations of models, particularly when at-
tempting to compare to human performance, to avoid over-
stating the capabilities of machine learning systems due to
misleading comparisons.
3.4. Multi-lingual Speech Recognition
In order to compare to prior work on multilingual speech
recognition, we report results on two low-data benchmarks:
Multilingual LibriSpeech (MLS) (Pratap et al., 2020b) and
V oxPopuli (Wang et al., 2021) in Table 3.
Whisper performs well on Multilingual LibriSpeech, out-
performing XLS-R (Babu et al., 2021), mSLAM (BapnaRobust Speech Recognition via Large-Scale Weak Supervision 7
0.1 1 10 100 1K 10K 100K 1M
Hours of transcribed audio2.5510204080160Word Error Rate (WER)
r2 = 0.83
SW
PTJAFIML
FRROGL
KO
UKNELO
AZ
MKLT
NLMSGU
ISMY
CATE
TRCS
NBARAF
HRUZ
DEVILV
ID
PLSVTAFAHY
THBN
KM
ENHUUR
BSKA
ZHSL
SKCY
RUBGFIL
ELHIKNMT
BE
HE
ITMRPA
DA
ESKKTG
ETSR
Figure 3. Correlation of pre-training supervision amount with
downstream speech recognition performance. The amount of
pre-training speech recognition data for a given language is very
predictive of zero-shot performance on that language in Fleurs.
Model MLS V oxPopuli
VP-10K + FT - 15.3
XLS-R (1B) 10.9 10.6
mSLAM-CTC (2B) 9.7 9.1
Maestro - 8.1
Zero-Shot Whisper 7.3 13.6
Table 3. Multilingual speech recognition performance. Zero-
shot Whisper improves performance on Multilingual LibriSpeech
(MLS) but is still significantly behind both Maestro, XLS-R, and
mSLAM on V oxPopuli.
et al., 2022), and Maestro (Chen et al., 2022b) in a zero-shot
setting. We caution that we do use a simple text standardizer
for this result which prevents direct comparison or claims
of SOTA performance. On V oxPopuli, however, Whisper
significantly underperforms prior work and only beats the
VP-10K+FT baseline from the original paper. We suspect
the underperformance of Whisper models on V oxPopuli
could be due to other models including this distribution as
a major source for their unsupervised pre-training data and
the dataset having significantly more supervised data, which
benefits fine-tuning. While MLS has 10 hours of training
data per language, the average amount of training data per
language is roughly 10 higher for V oxPopuli.
These two benchmarks are somewhat narrow since they
only include 15 unique languages, almost all of which are in
1 10 100 1K 10K 100K
Hours of translated audio0510152025303540BLEU
r2 = 0.24
HR
AMNL
MYSWEL
NETH
KNPADA
AR
MIBG
ML
MRTESV
IT
FILGLRO
UK
FA
UZBE
KMTG
ASETOCCA
IS
KKHEFR AF
VI
HAMT
LOBNPT
HUFI
KO
SDID
UR
LNLV
AZ
YOLB
CYHYPL
LTDE
KARU
MKMSSR
ES
ZH
JANBBS
MNSNTR
PSSK
SOCS
SLHI
GU
TAFigure 4. Correlation of pre-training supervision amount with
downstream translation performance. The amount of pre-
training translation data for a given language is only moderately
predictive of Whispers zero-shot performance on that language in
Fleurs.
the Indo-European language family and many of which are
high-resource languages. These benchmarks only provide
limited coverage and room to study Whisper models multi-
lingual capabilities which include training data for speech
recognition in 75 languages. To study the performance of
Whisper more broadly we also report performance on the
Fleurs dataset (Conneau et al., 2022). In particular, we were
interested in studying the relationship between the amount
of training data we have for a given language and the result-
ing downstream zero-shot performance for that language.
We visualize this relation in Figure 3. We find a strong
squared correlation coefficient of 0.83 between the log of
the word error rate and the log of the amount of training
data per language. Checking the regression coefficient for a
linear fit to these log-log values results in an estimate that
WER halves for every 16 increase in training data. We
also observed that many of the largest outliers in terms of
worse than expected performance according to this trend are
languages that have unique scripts and are more distantly
related to the Indo-European languages making up the ma-
jority of the training dataset such as Hebrew ( HE), Telugu
(TE), Chinese ( ZH), and Korean ( KO). These differences
could be due to a lack of transfer due to linguistic distance,
our byte level BPE tokenizer being a poor match for these
languages, or variations in data quality.Robust Speech Recognition via Large-Scale Weak Supervision 8
XEnglish High Mid Low All
XMEF-X 34.2 20.2 5.9 14.7
XLS-R (2B) 36.1 27.7 15.1 22.1
mSLAM-CTC (2B) 37.8 29.6 18.5 24.8
Maestro 38.2 31.3 18.4 25.2
Zero-Shot Whisper 36.2 32.6 25.2 29.1
Table 4. XenSpeech translation performance. Zero-shot
Whisper outperforms existing models on CoV oST2 in the overall,
medium, and low resource settings but still moderately under-
performs on high-resource languages compared to prior directly
supervised work.
Language ID Fleurs
w2v-bert-51 (0.6B) 71.4
mSLAM-CTC (2B) 77.7
Zero-shot Whisper 64.5
Table 5. Language identification performance. Zero-shot Whis-
pers accuracy at language identification is not competitive with
prior supervised results on Fleurs. This is partially due to Whisper
being heavily penalized for having no training data for 20 of Fleurs
languages.
3.5. Translation
We study the translation capabilities of Whisper models
by measuring their performance on the Xensubset of
CoV oST2 (Wang et al., 2020b). We compare with Maestro,
mSLAM, and XLS-R, the highest-performing prior work.
We achieve a new state of the art of 29.1 BLEU zero-shot
without using any of the CoV oST2 training data. We at-
tribute this to the 68,000 hours of Xentranslation data
for these languages in our pre-training dataset which, al-
though noisy, is vastly larger than the 861 hours of training
data for Xentranslation in CoV oST2. Since Whisper eval-
uation is zero-shot, it does particularly well on the lowest
resource grouping of CoV oST2, improving over mSLAM
by 6.7 BLEU. Conversely, the best Whisper model does not
actually improve over Maestro and mSLAM on average for
the highest resource languages.
For an additional analysis on an even wider set of languages,
we also re-purpose Fleurs, which is a speech recognition
dataset, as a translation dataset. Since the same sentences
are transcribed for every language we use the English tran-
scripts as reference translations. In Figure 4 we visualize
the correlation between the amount of translation training
data per language and the resulting zero-shot BLEU score
on Fleurs. While there is a clear trend of improvement with
increasing training data, the squared correlation coefficient
is much lower than the 0.83 observed for speech recognition
40 30 20 10 0 -10
signal-to-noise ratio (dB)125102050100WER on LibriSpeech test-clean (%)
white noise
40 30 20 10 0 -10
signal-to-noise ratio (dB)
pub noise
unispeech-sat-base-100h-libri-ft
wav2vec2-base-100h
wav2vec2-base-960h
wav2vec2-large-960h
wav2vec2-large-robust-ft-libri-960h
wav2vec2-large-960h-lv60-self
asr-crdnn-rnnlm-librispeech
asr-transformer-transformerlm-librispeechhubert-large-ls960-ft
hubert-xlarge-ls960-ft
s2t-medium-librispeech-asr
s2t-large-librispeech-asr
stt_en_conformer_ctc_large
stt_en_conformer_transducer_xlarge
WhisperFigure 5. WER on LibriSpeech test-clean as a function of SNR
under additive white noise (left) and pub noise (right). The
accuracy of LibriSpeech-trained models degrade faster than the
best Whisper model ( ). NVIDIA STT models ( ) perform best
under low noise but are outperformed by Whisper under high noise
(SNR<10 dB). The second-best model under low noise ( ) is
fine-tuned on LibriSpeech only and degrades even more quickly.
and only 0.24. We suspect this is partly caused by the noisier
training data due to errors in audio language identification.
As an example, Welsh ( CY) is an outlier with much worse
than expected performance at only 13 BLEU despite sup-
posedly having 9,000 hours of translation data. This large
amount of Welsh translation data is surprising, ranking 4th
overall for translation data and ahead of some of the most
spoken languages in the world like French, Spanish, and
Russian. Inspection shows the majority of supposedly Welsh
translation data is actually English audio with English cap-
tions where the English audio was mis-classified as Welsh
by the language identification system, resulting in it being
included as translation training data rather transcription data
according to our dataset creation rules.
3.6. Language Identification
To evaluate language identification, we use the Fleurs
dataset (Conneau et al., 2022). The zero-shot performance
of Whisper is not competitive with prior supervised work
here and underperforms the supervised SOTA by 13.6%.
However, Whisper is heavily disadvantaged for language
identification on Fleurs, since the Whisper dataset contains
no training data for 20 of the 102 languages in Fleurs, upper-
bounding accuracy at 80.4%. On the 82 overlapping lan-
guages the best Whisper model achieves 80.3% accuracy.Robust Speech Recognition via Large-Scale Weak Supervision 9
3.7. Robustness to Additive Noise
We tested the noise robustness of Whisper models and 14
LibriSpeech-trained models by measuring the WER when
either white noise or pub noise from the Audio Degrada-
tion Toolbox (Mauch & Ewert, 2013) was added to the
audio. The pub noise represents a more natural noisy envi-
ronment with ambient noise and indistinct chatter typical
in a crowded restaurant or a pub. Among the 14 models,
twelve are pre-trained and/or fine-tuned on LibriSpeech, and
the other two are NVIDIA STT models trained on a mixture
dataset similar to prior work like SpeechStew that includes
LibriSpeech. The level of additive noise corresponding to
a given signal-to-noise ratio (SNR) is calculated based on
the signal power of individual examples. Figure 5 shows
how the ASR performance degrades as the additive noise
becomes more intensive. There are many models that out-
perform our zero-shot performance under low noise (40 dB
SNR), which is unsurprising given those models are trained
primarily on LibriSpeech, but all models quickly degrade as
the noise becomes more intensive, performing worse than
the Whisper model under additive pub noise of SNR below
10 dB. This showcases Whispers robustness to noise, es-
pecially under more natural distribution shifts like the pub
noise.
3.8. Long-form Transcription
Whisper models are trained on 30-second audio chunks and
cannot consume longer audio inputs at once. This is not aproblem with most academic datasets comprised of short
utterances but presents challenges in real-world applications
which often require transcribing minutes- or hours-long au-
dio. We developed a strategy to perform buffered transcrip-
tion of long audio by consecutively transcribing 30-second
segments of audio and shifting the window according to the
timestamps predicted by the model. We observed that it
is crucial to have beam search and temperature scheduling
based on the repetitiveness and the log probability of the
model predictions in order to reliably transcribe long audio.
The full procedure is described in Section 4.5.
We evaluate the long-form transcription performance on
seven datasets consisting of speech recordings of various
lengths and recording conditions, to cover as diverse a data
distribution as possible. These include a long-form adapta-
tion of TED-LIUM3 (Hernandez et al., 2018) concatenated
so that each example is a full-length TED talk, a collection
of jargon-laden segments taken from The Late Show with
Stephen Colbert (Meanwhile), sets of videos/podcasts that
has been used as ASR benchmarks in online blogs (Rev16
and Kincaid46), recordings of earnings calls (Del Rio et al.,
2021), and the full-length interviews from the Corpus of
Regional African American Language (CORAAL) (Gunter
et al., 2021). Full details about the long-form datasets can
be found in Appendix A.
We compare the performance with open-source models as
well as 4 commercial ASR services. The results are sum-
marized in Figure 6, showing the distribution of word error
rates from Whisper and the 4 commercial ASR services,
TED-LIUM3 Meanwhile Kincaid46 Rev16 Earnings-21 Earnings-22 CORAAL0510152025303540Word Error Rate (%)
Whisper Company A Company B Company C Company D NVIDIA STT (CTC large)
Figure 6. Whisper is competitive with state-of-the-art commercial and open-source ASR systems in long-form transcription. The
distribution of word error rates from six ASR systems on seven long-form datasets are compared, where the input lengths range from a
few minutes to a few hours. The boxes show the quartiles of per-example WERs, and the per-dataset aggregate WERs are annotated
on each box. Our model outperforms the best open source model (NVIDIA STT) on all datasets, and in most cases, commercial ASR
systems as well.Robust Speech Recognition via Large-Scale Weak Supervision 10
as well as the NVIDIA STT Conformer-CTC Large model
from the NeMo toolkit (Kuchaiev et al., 2019) which per-
formed the best among the open-source models. All com-
mercial ASR services are queried using their default English
transcription settings as of September 1st, 2022, and for
the NVIDIA STT model we used their buffered inference
implementation in the FrameBatchASR class to enable
long-form transcription. The results show that Whisper per-
forms better than the compared models on most datasets,
especially on the Meanwhile dataset which is heavy with
uncommon words. Additionally, we note the possibility that
some of the commercial ASR systems have been trained
on some of these publicly available datasets, and therefore
these results may not be accurately reflecting the relative
robustness of the systems.
3.9. Comparison with Human Performance
Because of ambiguous or indistinct speech as well as la-
beling errors, there are different levels of irreducible error
in each dataset, and with WER metrics from ASR systems
alone it is difficult to make sense of how much room for
improvement exists in each dataset. To quantify how close
Whispers performance is to the human performance, we se-
lected 25 recordings from the Kincaid46 dataset and used 5
services to obtain transcripts produced by professional tran-
scribers, among which one provides computer-assisted tran-
scription and the other four are entirely human-transcribed.
The audio selection covers various recording conditions
such as scripted and unscripted broadcast, telephone and
V oIP calls, and meetings. Figure 7 shows the distribution
of per-example WERs and aggregate WER across the 25
recordings, where the computer-assisted service has the
lowest aggregate WER that is 1.15% point better than Whis-
pers, and the pure-human performance is only a fraction
of a percentage point better than Whispers. These results
indicate that Whispers English ASR performance is not
perfect but very close to human-level accuracy.
4. Analysis and Ablations
4.1. Model Scaling
A large amount of the promise in weakly supervised train-
ing approaches is their potential to use datasets much larger
than those in traditional supervised learning. However, this
comes with the cost of using data that is possibly much
noisier and lower quality than gold-standard supervision.
A concern with this approach is that although it may look
promising to begin with, the performance of models trained
on this kind of data may saturate at the inherent quality level
of the dataset, which could be far below human level. A re-
lated concern is that as capacity and compute spent training
on the dataset increases, models may learn to exploit the
Whisper A B C D E F G H I
 ASR            human transcription  
          computer-assisted051015202530Word Error Rate (%)
Figure 7. Whispers performance is close to that of professional
human transcribers. This plot shows the WER distributions of
25 recordings from the Kincaid46 dataset transcribed by Whisper,
the same 4 commercial ASR systems from Figure 6 (A-D), one
computer-assisted human transcription service (E) and 4 human
transcription services (F-I). The box plot is superimposed with dots
indicating the WERs on individual recordings, and the aggregate
WER over the 25 recordings are annotated on each box.
idiosyncrasies of the dataset, and their ability to generalize
robustly to out-of-distribution data could even degrade.
To check whether this is the case, we study the zero-shot
generalization of Whisper models as a function of the model
size. Our analysis is summarized in Figure 8. With the
exception of English speech recognition, performance con-
tinues to increase with model size across multilingual speech
recognition, speech translation, and language identification.
The diminishing returns for English speech recognition
could be due to saturation effects from approaching human-
level performance as analysis in Section 3.9 suggests.
4.2. Dataset Scaling
At 680,000 hours of labeled audio, the Whisper dataset is
one of the largest ever created in supervised speech recog-
nition. Exactly how important is the raw dataset size to
Whispers performance? To study this, we trained a series
of medium-sized models on subsampled versions of the
dataset which are 0.5%, 1%, 2%, 4%, and 8% of the full
dataset size and compared their performance with the same
medium-sized model trained on the whole dataset. Early
stopping based on the validation loss was used to select
model checkpoints for each dataset size. Evaluation was
performed on an exponential moving average estimate of
the parameters (Polyak & Juditsky, 1992) using a smooth-
ing rate of 0.9999 to help reduce the effect of the learning
rate not fully decaying to zero for the models trained on the
subsampled datasets due to early stopping. Performance
on English and multilingual speech recognition and Xen
translation is reported in Table 6.Robust Speech Recognition via Large-Scale Weak Supervision 11
38M 73M 244M 768M 1549M1549M
Model parameters0.02.55.07.510.012.515.017.520.0WER on 12 datasets (%)
English Speech Recognition
Average
Large V2
38M 73M 244M 768M 1549M1549M
Model parameters020406080100WER on 67 languages (%)
Multilingual Speech Recognition (Fleurs)
Average
Large V2
38M 73M 244M 768M 1549M1549M
Model parameters01020304050BLEU on 21 languages
X->En Translation (CoVoST2)
Average
Large V2
38M 73M 244M 768M 1549M1549M
Model parameters304050607080Accuracy on 102 languages (%)
Language Identification (Fleurs)
Average
Large V2
Figure 8. Zero-shot Whisper performance scales reliably across tasks and languages with increasing model size. Lightly shaded
lines represent individual datasets or languages, showing that performance is more varied than the smooth trends in aggregate performance.
Large V2 distinguished with a dashed orange line since it includes several changes that are not present for the smaller models in this
analysis.
Dataset English Multilingual X En
size WER () WER ( ) BLEU ( )
3405 30.5 92.4 0.2
6811 19.6 72.7 1.7
13621 14.4 56.6 7.9
27243 12.3 45.0 13.9
54486 10.9 36.4 19.2
681070 9.9 29.2 24.8
Table 6. Performance improves with increasing dataset size.
English speech recognition performance refers to an average over
12 datasets while the Multilingual speech recognition reports per-
formance on the overlapping subset of languages in Fleurs and
Xentranslation reports average BLEU on CoV oST2. Dataset
size reported in hours.
All increases in the dataset size result in improved perfor-
mance on all tasks, although we see significant variability
in improvement rates across tasks and sizes. Performance
improves rapidly on English speech recognition from 3,000
to 13,000 hours and then slows down noticeably between
13,000 and 54,000 hours. Using the full dataset, which cor-
responds to another 12.5 increase in size results in only a
further 1 point drop in WER. This mirrors the diminishing
returns observed with model size scaling for English speech
recognition and could similarly be explained by saturation
effects when approaching human-level performance.
Improvements in WER follow a power-law trend for mul-
tilingual speech recognition till 54,000 hours and then de-
viate from this trend, improving only a further 7 points
when increasing to the full dataset size. For Xentransla-
tion, performance is practically zero when training on 7,000
hours of audio or less, and then follows a roughly log-linear
improvement trend till 54,000 hours before also showingdiminishing returns when further scaling to the full dataset
size.
The general trend across tasks of diminishing returns when
moving from 54,000 hours to our full dataset size of 680,000
hours could suggest that the current best Whisper models are
under-trained relative to dataset size and performance could
be further improved by a combination of longer training
and larger models. It could also suggest that we are nearing
the end of performance improvements from dataset size
scaling for speech recognition. Further analysis is needed to
characterize scaling laws for speech recognition in order
to decided between these explanations.
4.3. Multitask and Multilingual Transfer
A potential concern with jointly training a single model
on many tasks and languages is the possibility of negative
transfer where interference between the learning of several
tasks results in performance worse than would be achieved
by training on only a single task or language. To investigate
whether this is occurring, we compared the performance
of models trained on just English speech recognition with
our standard multitask and multilingual training setup and
measured their average performance across our suite of zero-
shot English speech recognition benchmarks. We adjust for
the amount of FLOPs spent training on the task of English
speech recognition as only 65% of compute is spent on this
task in a joint training setup; analysis would otherwise be
confounded by under-training on the task when compared
to a same-sized English-only model.
Our results visualized in Figure 9 show that for small models
trained with moderate amounts of compute, there is indeed
negative transfer between tasks and languages: joint mod-
els underperform English-only models trained for the same
amount of compute. However, multitask and multilingualRobust Speech Recognition via Large-Scale Weak Supervision 12
10e+19 10e+20 10e+21 10e+22
FLOPs training on english speech recognition8101214161820Average WER on 11 english speech recognition datasetsEnglish Only
Multilingual and Multitask
Figure 9. Multitask and multilingual transfer improves with
scale. For small models, performance on English speech recogni-
tion degrades when trained jointly in a multitask and multilingual
setup. However, multilingual and multitask models benefit more
from scale and eventually outperform models trained on English
data only. 95% bootstrap estimate confidence intervals are shown.
models scale better and for our largest experiments outper-
form their English-only counterparts demonstrating positive
transfer from other tasks. For our largest experiments, joint
models also slightly outperform English-only models even
when not adjusting for compute spent per task.
4.4. Text Normalization
Since we developed our text normalization jointly with
Whisper to discount innocuous word errors, there is a risk
that our normalizer is overfitted to fixing Whispers peculiar-
ities rather than addressing general variation in transcription.
To check this, we compared the performance of Whisper
using our normalizer versus an independently developed
one from the FairSpeech project (Koenecke et al., 2020). In
Figure 10, we visualize the differences. On most datasets
the two normalizers perform similarly, without significant
differences in WER reduction between Whisper and com-
pared open-source models, while on some datasets, namely
WSJ, CallHome, and Switchboard, our normalizer reduces
the WER of Whisper models significantly more. The differ-
ences in reduction can be traced down to different formats
used by the ground truth and how the two normalizers are pe-
nalizing them. For example, in CallHome and Switchboard,
our standardizer did not penalize differences in common
English contractions such as youre versus you are, and
in WSJ, our normalizer standardized the written and spo-
0 10 20 30 40 50
Relative WER reduction compared to FairSpeech's normalizer (%)CORAAL
CommonVoice9.en
AMI-SDM1
CommonVoice5.1
Fleurs.en_us
AMI-IHM
Artie
LibriSpeech
TED-LIUM3
VoxPopuli.en
WSJ
CallHome
SwitchboardOpen-source models
Whisper modelsFigure 10. On most datasets, our text normalizer has similar
effect on reducing WERs between Whisper models and other
open-source models, compared to FairSpeechs normalizer. For
each dataset, the boxplot shows the distribution of relative WER
reduction across different models in our eval suite, showing that
using our text normalizer generally results in lower WERs than
FairSpeechs. On a few datasets our normalizer reduces WER
significantly and more so for Whisper models, such as CallHome
and Switchboard which have many contractions in the ground truth
and WSJ which contains many numerical expressions.
ken forms of numerical and monetary expressions, such as
sixty-eight million dollars versus $68 million.
4.5. Strategies for Reliable Long-form Transcription
Transcribing long-form audio using Whisper relies on ac-
curate prediction of the timestamp tokens to determine the
amount to shift the models 30-second audio context win-
dow by, and inaccurate transcription in one window may
negatively impact transcription in the subsequent windows.
We have developed a set of heuristics that help avoid fail-
ure cases of long-form transcription, which is applied in
the results reported in sections 3.8 and 3.9. First, we use
beam search with 5 beams using the log probability as the
score function, to reduce repetition looping which happens
more frequently in greedy decoding. We start with tem-
perature 0, i.e. always selecting the tokens with the high-
est probability, and increase the temperature by 0.2 up to
1.0 when either the average log probability over the gen-
erated tokens is lower than 1or the generated text has a
gzip compression rate higher than 2.4. Providing the tran-
scribed text from the preceding window as previous-text
conditioning when the applied temperature is below 0.5
further improves the performance. We found that the proba-
bility of the <|nospeech|> token alone is not sufficientRobust Speech Recognition via Large-Scale Weak Supervision 13TED-LIUM3
Meanwhile
Kincaid46
Rev16
Earnings-21
Earnings-22
CORAAL
Average
Greedy decoding only 3.95 5.16 9.69 11.7 10.7 14.0 22.0 11.0
+ Beam search 4.16 5.71 9.42 11.5 10.2 13.4 20.0 10.6
+ Temperature fallback 4.16 5.71 9.42 11.5 10.2 13.4 20.0 10.6
+ V oice activity detection 3.56 4.61 9.45 11.4 10.1 13.2 19.4 10.2
+ Previous text conditioning 3.42 6.16 8.72 11.0 9.63 13.3 18.1 10.0
+ Initial timestamp constraint 3.51 5.26 8.41 11.5 9.73 12.6 19.1 10.0
Table 7. Long-form transcription performance improves incremen-
tally as additional decoding heuristics are employed. Details on
each intervention are described in Section 4.5.
to distinguish a segment with no speech, but combining
the no-speech probability threshold of 0.6 and the average
log-probability threshold of 1makes the voice activity
detection of Whisper more reliable. Finally, to avoid a fail-
ure mode where the model ignores the first few words in
the input, we constrained the initial timestamp token to be
between 0.0 and 1.0 second. Table 7 shows that adding each
of the interventions above incrementally reduces the WER
overall, but not evenly across the dataset. These heuristics
serve as a workaround for the noisy predictions of the model,
and more research would be needed to further improve the
reliability of long-form decoding.
5. Related Work
Scaling Speech Recognition A consistent theme across
speech recognition research has been documenting the bene-
fits of scaling compute, models, and datasets. Early work ap-
plying deep learning to speech recognition found improved
performance with model depth and size and leveraged GPU
acceleration to make training these larger models tractable
(Mohamed et al., 2009). Further research demonstrated that
the benefit of deep learning approaches to speech recogni-
tion increased with dataset size, improving from being only
competitive with prior GMM-HMM systems when using
just 3 hours of TIMIT training data for phone recognition
to achieving a 30% word error rate reduction when trained
on the 2,000 hour Switchboard dataset (Seide et al., 2011).
Liao et al. (2013) is an early example of leveraging weakly
supervised learning to increase the size of a deep learn-
ing based speech recognition dataset by over 1,000 hours.
These trends continued with Deep Speech 2 (Amodei et al.,
2015) being a notable system developing high-throughput
distributed training across 16 GPUs and scaling to 12,000
hours of training data while demonstrating continuing im-
provements at that scale. By leveraging semi-supervised
pre-training, Narayanan et al. (2018) were able to grow
dataset size much further and study training on 162,000
hours of labeled audio. More recent work has exploredbillion-parameter models (Zhang et al., 2020) and using up
to 1,000,000 hours of training data (Zhang et al., 2021).
Multitask Learning Multitask learning (Caruana, 1997)
has been studied for a long time. In speech recognition,
multi-lingual models have been explored for well over a
decade (Schultz & Kirchhoff, 2006). An inspirational and
foundational work in NLP exploring multi-task learning
with a single model is Collobert et al. (2011). Multitask
learning in the sequence-to-sequence framework (Sutskever
et al., 2014) using multiple encoders and decoders was in-
vestigated in Luong et al. (2015). The use of language codes
with a shared encoder/decoder architecture was first demon-
strated for machine translation by Johnson et al. (2017),
removing the need for separate encoders and decoders. This
approach was simplified further into the text-to-text frame-
work of McCann et al. (2018) and popularized by its success
with large transformer language models in the work of Rad-
ford et al. (2019) and Raffel et al. (2020). Toshniwal et al.
(2018) demonstrated jointly training a modern deep learn-
ing speech recognition system on several languages with a
single model, and Pratap et al. (2020a) scaled this line of
work significantly to 50 languages with a billion-parameter
model. MUTE (Wang et al., 2020c) and mSLAM (Bapna
et al., 2022) studied joint training over both text and speech
language tasks, demonstrating transfer between them.
Robustness The question of how effectively models trans-
fer and how robust they are to distribution shift and other
types of perturbations has long been studied and is actively
being researched across many fields of machine learning.
Torralba & Efros (2011) highlighted the lack of generaliza-
tion of machine learning models between datasets over a
decade ago. Many other works have shown and continu-
ally reiterated how despite high performance on IID test
sets, machine learning models can still make many mistakes
when evaluated in even slightly different settings (Lake et al.,
2017; Jia & Liang, 2017; Alcorn et al., 2019; Barbu et al.,
2019; Recht et al., 2019). More recently, Taori et al. (2020)
studied the robustness of image classification models, and
Miller et al. (2020) investigated this for question-answering
models. A key finding has been that multi-domain train-
ing increases robustness and generalization as discussed in
the Introduction. This finding has been replicated across
many fields in addition to speech recognition including NLP
(Hendrycks et al., 2020) and computer vision (Radford et al.,
2021).
6. Limitations and Future Work
From our experimental results, analyses, and ablations, we
have noted several limitations and areas for future work.Robust Speech Recognition via Large-Scale Weak Supervision 14
Improved decoding strategies. As we have scaled Whis-
per, we have observed that larger models have made steady
and reliable progress on reducing perception-related errors
such as confusing similar-sounding words. Many remaining
errors, particularly in long-form transcription seem more
stubborn in nature and decidedly non-human/perceptual.
They are a combination of failure modes of seq2seq mod-
els, language models, and text-audio alignment and include
problems such as getting stuck in repeat loops, not tran-
scribing the first or last few words of an audio segment, or
complete hallucination where the model will output a tran-
script entirely unrelated to the actual audio. Although the
decoding details discussed in Section 4.5 help significantly,
we suspect fine-tuning Whisper models on a high-quality
supervised dataset and/or using reinforcement learning to
more directly optimize for decoding performance could help
further reduce these errors.
Increase Training Data For Lower-Resource Languages
As Figure 3 shows, Whispers speech recognition perfor-
mance is still quite poor on many languages. The same
analysis suggests a clear route for improvement since perfor-
mance on a language is very well predicted by the amount
of training data for the language. Since our pre-training
dataset is currently very English-heavy due to biases of
our data collection pipeline, which sourced primarily from
English-centric parts of the internet, most languages have
less than 1000 hours of training data. A targeted effort at in-
creasing the amount of data for these rarer languages could
result in a large improvement to average speech recognition
performance even with only a small increase in our overall
training dataset size.
Studying fine-tuning In this work, we have focused on
the robustness properties of speech processing systems and
as a result only studied the zero-shot transfer performance
of Whisper. While this is a crucial setting to study due to it
being representative of general reliability, for many domains
where high-quality supervised speech data does exist, it is
likely that results can be improved further by fine-tuning.
An additional benefit of studying fine-tuning is that it allows
for direct comparisons with prior work since it is a much
more common evaluation setting.
Studying the impact of Language Models on Robustness
As argued in the introduction, we suspect that Whispers
robustness is partially due to its strong decoder, which is an
audio conditional language model. Its currently unclear to
what degree the benefits of Whisper stem from training its
encoder, decoder, or both. This could be studied by either
ablating various design components of Whisper, such as
training a decoder-less CTC model, or by studying how the
performance of existing speech recognition encoders suchas wav2vec 2.0 change when used together with a language
model.
Adding Auxiliary Training Objectives Whisper departs
noticeably from most recent state-of-the-art speech recog-
nition systems due to the lack of unsupervised pre-training
or self-teaching methods. While we have not found them
necessary to achieve good performance, it is possible that
the results could be further improved by incorporating this.
7. Conclusion
Whisper suggests that scaling weakly supervised pre-
training has been underappreciated so far in speech recogni-
tion research. We achieve our results without the need for
the self-supervision and self-training techniques that have
been a mainstay of recent large-scale speech recognition
work and demonstrate how simply training on a large and
diverse supervised dataset and focusing on zero-shot trans-
fer can significantly improve the robustness of a speech
recognition system.
ACKNOWLEDGMENTS
Wed like to thank the millions of people who were involved
in creating the data used by Whisper. Wed also like to
thank Nick Ryder, Will Zhuk, and Andrew Carr for the
conversation on the waterfall hike that inspired this project.
We are also grateful to the Acceleration and Supercomputing
teams at OpenAI for their critical work on software and
hardware infrastructure this project used. Wed also like to
thank Pamela Mishkin for advising the project from a policy
perspective. Finally, we are grateful to the developers of
the many software packages used throughout this project
including, but not limited, to Numpy (Harris et al., 2020),
SciPy (Virtanen et al., 2020), ftfy (Speer, 2019), PyTorch
(Paszke et al., 2019), pandas (pandas development team,
2020), and scikit-learn (Pedregosa et al., 2011).
References
Alcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.-
S., and Nguyen, A. Strike (with) a pose: Neural networks
are easily fooled by strange poses of familiar objects. In
Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 48454854, 2019.
Amodei, D., Anubhai, R., Battenberg, E., Case, C., Casper,
J., Catanzaro, B., Chen, J., Chrzanowski, M., Coates,
A., Diamos, G., et al. Deep speech 2: end-to-end speech
recognition in english and mandarin. arxiv. arXiv preprint
arXiv:1512.02595 , 2015.
Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler,
M., Meyer, J., Morais, R., Saunders, L., Tyers, F. M.,Robust Speech Recognition via Large-Scale Weak Supervision 15
and Weber, G. Common voice: A massively-multilingual
speech corpus. arXiv preprint arXiv:1912.06670 , 2019.
Babu, A., Wang, C., Tjandra, A., Lakhotia, K., Xu,
Q., Goyal, N., Singh, K., von Platen, P., Saraf, Y .,
Pino, J., et al. XLS-R: Self-supervised cross-lingual
speech representation learning at scale. arXiv preprint
arXiv:2111.09296 , 2021.
Baevski, A., Zhou, H., Mohamed, A., and Auli, M. wav2vec
2.0: A framework for self-supervised learning of speech
representations. arXiv preprint arXiv:2006.11477 , 2020.
Baevski, A., Hsu, W.-N., Conneau, A., and Auli, M. Unsu-
pervised speech recognition. Advances in Neural Infor-
mation Processing Systems , 34:2782627839, 2021.
Bapna, A., Cherry, C., Zhang, Y ., Jia, Y ., Johnson, M.,
Cheng, Y ., Khanuja, S., Riesa, J., and Conneau, A. mslam:
Massively multilingual joint pre-training for speech and
text. arXiv preprint arXiv:2202.01374 , 2022.
Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gut-
freund, D., Tenenbaum, J., and Katz, B. Objectnet: A
large-scale bias-controlled dataset for pushing the lim-
its of object recognition models. Advances in neural
information processing systems , 32, 2019.
Caruana, R. Multitask learning. Machine learning , 28(1):
4175, 1997.
Chan, W., Park, D., Lee, C., Zhang, Y ., Le, Q., and Norouzi,
M. SpeechStew: Simply mix all available speech recogni-
tion data to train one large neural network. arXiv preprint
arXiv:2104.02133 , 2021.
Chen, G., Chai, S., Wang, G., Du, J., Zhang, W.-Q.,
Weng, C., Su, D., Povey, D., Trmal, J., Zhang, J.,
et al. Gigaspeech: An evolving, multi-domain asr corpus
with 10,000 hours of transcribed audio. arXiv preprint
arXiv:2106.06909 , 2021.
Chen, S., Wu, Y ., Wang, C., Chen, Z., Chen, Z., Liu, S.,
Wu, J., Qian, Y ., Wei, F., Li, J., et al. Unispeech-sat: Uni-
versal speech representation learning with speaker aware
pre-training. In ICASSP 2022-2022 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP) , pp. 61526156. IEEE, 2022a.
Chen, T., Xu, B., Zhang, C., and Guestrin, C. Training
deep nets with sublinear memory cost. arXiv preprint
arXiv:1604.06174 , 2016.
Chen, Z., Zhang, Y ., Rosenberg, A., Ramabhadran, B.,
Moreno, P., Bapna, A., and Zen, H. Maestro: Matched
speech text representations through modality matching.
arXiv preprint arXiv:2204.03409 , 2022b.Child, R., Gray, S., Radford, A., and Sutskever, I. Gen-
erating long sequences with sparse transformers. arXiv
preprint arXiv:1904.10509 , 2019.
Collobert, R., Weston, J., Bottou, L., Karlen, M.,
Kavukcuoglu, K., and Kuksa, P. Natural language pro-
cessing (almost) from scratch. Journal of machine learn-
ing research , 12(ARTICLE):24932537, 2011.
Conneau, A., Ma, M., Khanuja, S., Zhang, Y ., Axelrod, V .,
Dalmia, S., Riesa, J., Rivera, C., and Bapna, A. Fleurs:
Few-shot learning evaluation of universal representations
of speech. arXiv preprint arXiv:2205.12446 , 2022.
Del Rio, M., Delworth, N., Westerman, R., Huang, M.,
Bhandari, N., Palakapilly, J., McNamara, Q., Dong, J.,
Zelasko, P., and Jett e, M. Earnings-21: a practical bench-
mark for asr in the wild. arXiv preprint arXiv:2104.11348 ,
2021.
Galvez, D., Diamos, G., Torres, J. M. C., Achorn, K., Gopi,
A., Kanter, D., Lam, M., Mazumder, M., and Reddi, V . J.
The peoples speech: A large-scale diverse english speech
recognition dataset for commercial usage. arXiv preprint
arXiv:2111.09344 , 2021.
Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Bren-
del, W., Bethge, M., and Wichmann, F. A. Shortcut learn-
ing in deep neural networks. Nature Machine Intelligence ,
2(11):665673, 2020.
Ghorbani, B., Firat, O., Freitag, M., Bapna, A., Krikun,
M., Garcia, X., Chelba, C., and Cherry, C. Scaling
laws for neural machine translation. arXiv preprint
arXiv:2109.07740 , 2021.
Griewank, A. and Walther, A. Algorithm 799: revolve: an
implementation of checkpointing for the reverse or ad-
joint mode of computational differentiation. ACM Trans-
actions on Mathematical Software (TOMS) , 26(1):1945,
2000.
Gunter, K., Vaughn, C., and Kendall, T. Contextualiz-
ing/s/retraction: Sibilant variation and change in wash-
ington dc african american language. Language Variation
and Change , 33(3):331357, 2021.
Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers,
R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J.,
Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van
Kerkwijk, M. H., Brett, M., Haldane, A., Fern andez del
Rıo, J., Wiebe, M., Peterson, P., G erard-Marchant, P.,
Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H.,
Gohlke, C., and Oliphant, T. E. Array programming
with NumPy. Nature , 585:357362, 2020. doi: 10.1038/
s41586-020-2649-2.Robust Speech Recognition via Large-Scale Weak Supervision 16
Hendrycks, D. and Gimpel, K. Gaussian error linear units
(gelus). arXiv preprint arXiv:1606.08415 , 2016.
Hendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan,
R., and Song, D. Pretrained transformers improve out-of-
distribution robustness. arXiv preprint arXiv:2004.06100 ,
2020.
Hernandez, F., Nguyen, V ., Ghannay, S., Tomashenko, N. A.,
and Est eve, Y . Ted-lium 3: twice as much data and corpus
repartition for experiments on speaker adaptation. In
SPECOM , 2018.
Hsu, W.-N., Bolte, B., Tsai, Y .-H. H., Lakhotia, K.,
Salakhutdinov, R., and Mohamed, A. Hubert: Self-
supervised speech representation learning by masked
prediction of hidden units. IEEE/ACM Transactions on
Audio, Speech, and Language Processing , 29:34513460,
2021a.
Hsu, W.-N., Sriram, A., Baevski, A., Likhomanenko, T.,
Xu, Q., Pratap, V ., Kahn, J., Lee, A., Collobert, R., Syn-
naeve, G., et al. Robust wav2vec 2.0: Analyzing do-
main shift in self-supervised pre-training. arXiv preprint
arXiv:2104.01027 , 2021b.
Huang, G., Sun, Y ., Liu, Z., Sedra, D., and Weinberger,
K. Q. Deep networks with stochastic depth. In European
conference on computer vision , pp. 646661. Springer,
2016.
Jia, R. and Liang, P. Adversarial examples for evalu-
ating reading comprehension systems. arXiv preprint
arXiv:1707.07328 , 2017.
Johnson, M., Schuster, M., Le, Q. V ., Krikun, M., Wu, Y .,
Chen, Z., Thorat, N., Vi egas, F., Wattenberg, M., Corrado,
G., et al. Googles multilingual neural machine translation
system: Enabling zero-shot translation. Transactions of
the Association for Computational Linguistics , 5:339
351, 2017.
Kendall, T. and Farrington, C. The corpus of regional
african american language. Version 2021.07. Eugene, OR:
The Online Resources for African American Language
Project. http://oraal.uoregon.edu/coraal ,
2021. Accessed: 2022-09-01.
Koenecke, A., Nam, A., Lake, E., Nudell, J., Quartey, M.,
Mengesha, Z., Toups, C., Rickford, J. R., Jurafsky, D.,
and Goel, S. Racial disparities in automated speech recog-
nition. Proceedings of the National Academy of Sciences ,
117(14):76847689, 2020.
Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung,
J., Gelly, S., and Houlsby, N. Big transfer (bit): General
visual representation learning. In European conference
on computer vision , pp. 491507. Springer, 2020.Kuchaiev, O., Li, J., Nguyen, H., Hrinchuk, O., Leary, R.,
Ginsburg, B., Kriman, S., Beliaev, S., Lavrukhin, V .,
Cook, J., et al. Nemo: a toolkit for building ai applications
using neural modules. arXiv preprint arXiv:1909.09577 ,
2019.
Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gersh-
man, S. J. Building machines that learn and think like
people. Behavioral and brain sciences , 40, 2017.
Liao, H., McDermott, E., and Senior, A. Large scale deep
neural network acoustic modeling with semi-supervised
training data for youtube video transcription. In 2013
IEEE Workshop on Automatic Speech Recognition and
Understanding , pp. 368373. IEEE, 2013.
Likhomanenko, T., Xu, Q., Pratap, V ., Tomasello, P., Kahn,
J., Avidov, G., Collobert, R., and Synnaeve, G. Rethink-
ing evaluation in asr: Are our models robust enough?
arXiv preprint arXiv:2010.11745 , 2020.
Loshchilov, I. and Hutter, F. Decoupled weight decay regu-
larization. arXiv preprint arXiv:1711.05101 , 2017.
Luong, M.-T., Le, Q. V ., Sutskever, I., Vinyals, O., and
Kaiser, L. Multi-task sequence to sequence learning.
arXiv preprint arXiv:1511.06114 , 2015.
Mahajan, D., Girshick, R., Ramanathan, V ., He, K., Paluri,
M., Li, Y ., Bharambe, A., and Van Der Maaten, L. Ex-
ploring the limits of weakly supervised pretraining. In
Proceedings of the European conference on computer
vision (ECCV) , pp. 181196, 2018.
Mauch, M. and Ewert, S. The audio degradation toolbox and
its application to robustness evaluation. In Proceedings of
the 14th International Society for Music Information Re-
trieval Conference (ISMIR 2013) , Curitiba, Brazil, 2013.
accepted.
McCann, B., Keskar, N. S., Xiong, C., and Socher, R. The
natural language decathlon: Multitask learning as ques-
tion answering. arXiv preprint arXiv:1806.08730 , 2018.
Meyer, J., Rauchenstein, L., Eisenberg, J. D., and Howell,
N. Artie bias corpus: An open dataset for detecting de-
mographic bias in speech applications. In Proceedings of
the 12th Language Resources and Evaluation Conference ,
pp. 64626468, Marseille, France, May 2020. European
Language Resources Association. ISBN 979-10-95546-
34-4. URL https://aclanthology.org/2020.
lrec-1.796 .
Miller, J., Krauth, K., Recht, B., and Schmidt, L. The effect
of natural distribution shift on question answering models.
InICML , 2020.Robust Speech Recognition via Large-Scale Weak Supervision 17
Mohamed, A.-r., Dahl, G., Hinton, G., et al. Deep belief net-
works for phone recognition. In Nips workshop on deep
learning for speech recognition and related applications ,
volume 1, pp. 39, 2009.
Narayanan, A., Misra, A., Sim, K. C., Pundak, G., Tripathi,
A., Elfeky, M., Haghani, P., Strohman, T., and Bacchi-
ani, M. Toward domain-invariant speech recognition via
large scale training. In 2018 IEEE Spoken Language
Technology Workshop (SLT) , pp. 441447. IEEE, 2018.
Panayotov, V ., Chen, G., Povey, D., and Khudanpur, S.
Librispeech: an asr corpus based on public domain au-
dio books. In 2015 IEEE international conference on
acoustics, speech and signal processing (ICASSP) , pp.
52065210. IEEE, 2015.
pandas development team, T. pandas-dev/pandas: Pan-
das, February 2020. URL https://doi.org/10.
5281/zenodo.3509134 .
Park, D. S., Chan, W., Zhang, Y ., Chiu, C.-C., Zoph, B.,
Cubuk, E. D., and Le, Q. V . SpecAugment: A simple data
augmentation method for automatic speech recognition.
arXiv preprint arXiv:1904.08779 , 2019.
Pascanu, R., Mikolov, T., and Bengio, Y . On the difficulty
of training recurrent neural networks. In International
conference on machine learning , pp. 13101318. PMLR,
2013.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,
M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
Bai, J., and Chintala, S. Pytorch: An imperative style,
high-performance deep learning library. In Advances
in Neural Information Processing Systems 32 , pp. 8024
8035, 2019.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V .,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V ., Vanderplas, J., Passos, A., Cour-
napeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
Scikit-learn: Machine learning in Python. Journal of
Machine Learning Research , 12:28252830, 2011.
Polyak, B. T. and Juditsky, A. B. Acceleration of stochastic
approximation by averaging. SIAM journal on control
and optimization , 30(4):838855, 1992.
Pratap, V ., Sriram, A., Tomasello, P., Hannun, A. Y .,
Liptchinsky, V ., Synnaeve, G., and Collobert, R. Mas-
sively multilingual asr: 50 languages, 1 model, 1 billion
parameters. ArXiv , abs/2007.03001, 2020a.
Pratap, V ., Xu, Q., Sriram, A., Synnaeve, G., and Collobert,
R. Mls: A large-scale multilingual dataset for speech
research. arXiv preprint arXiv:2012.03411 , 2020b.Press, O. and Wolf, L. Using the output embedding to
improve language models. In Proceedings of the 15th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics: Volume 2, Short
Papers , pp. 157163, Valencia, Spain, April 2017. As-
sociation for Computational Linguistics. URL https:
//aclanthology.org/E17-2025 .
Provilkov, I., Emelianenko, D., and V oita, E. Bpe-dropout:
Simple and effective subword regularization. arXiv
preprint arXiv:1910.13267 , 2019.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and
Sutskever, I. Language models are unsupervised multitask
learners. 2019.
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,
Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,
J., Krueger, G., and Sutskever, I. Learning transferable
visual models from natural language supervision. arXiv
preprint arXiv:2103.00020 , 2021.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y ., Li, W., Liu, P. J., et al. Exploring
the limits of transfer learning with a unified text-to-text
transformer. J. Mach. Learn. Res. , 21(140):167, 2020.
Ravanelli, M., Parcollet, T., Plantinga, P., Rouhe, A., Cor-
nell, S., Lugosch, L., Subakan, C., Dawalatabad, N.,
Heba, A., Zhong, J., Chou, J.-C., Yeh, S.-L., Fu, S.-W.,
Liao, C.-F., Rastorgueva, E., Grondin, F., Aris, W., Na,
H., Gao, Y ., Mori, R. D., and Bengio, Y . SpeechBrain: A
general-purpose speech toolkit, 2021. arXiv:2106.04624.
Recht, B., Roelofs, R., Schmidt, L., and Shankar, V .
Do ImageNet classifiers generalize to ImageNet? In
Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceed-
ings of the 36th International Conference on Machine
Learning , volume 97 of Proceedings of Machine Learn-
ing Research , pp. 53895400. PMLR, 0915 Jun 2019.
URLhttps://proceedings.mlr.press/v97/
recht19a.html .
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,
Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,
M., et al. Imagenet large scale visual recognition chal-
lenge. International journal of computer vision , 115(3):
211252, 2015.
Schultz, T. and Kirchhoff, K. Multilingual speech process-
ing. Elsevier, 2006.
Seide, F., Li, G., Chen, X., and Yu, D. Feature engineering
in context-dependent deep neural networks for conver-
sational speech transcription. In 2011 IEEE Workshop
on Automatic Speech Recognition & Understanding , pp.
2429. IEEE, 2011.Robust Speech Recognition via Large-Scale Weak Supervision 18
Sennrich, R., Haddow, B., and Birch, A. Neural machine
translation of rare words with subword units. arXiv
preprint arXiv:1508.07909 , 2015.
Speer, R. ftfy. Zenodo, 2019. URL https://doi.org/
10.5281/zenodo.2591652 . Version 5.5.
Sutskever, I., Vinyals, O., and Le, Q. V . Sequence to se-
quence learning with neural networks. Advances in neural
information processing systems , 27, 2014.
Taori, R., Dave, A., Shankar, V ., Carlini, N., Recht, B.,
and Schmidt, L. Measuring robustness to natural
distribution shifts in image classification. In Larochelle,
H., Ranzato, M., Hadsell, R., Balcan, M., and Lin,
H. (eds.), Advances in Neural Information Processing
Systems , volume 33, pp. 1858318599. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.
neurips.cc/paper/2020/file/
d8330f857a17c53d217014ee776bfd50-Paper.
pdf.
Torralba, A. and Efros, A. A. Unbiased look at dataset bias.
CVPR 2011 , pp. 15211528, 2011.
Toshniwal, S., Sainath, T. N., Weiss, R. J., Li, B., Moreno,
P. J., Weinstein, E., and Rao, K. Multilingual speech
recognition with a single end-to-end model. 2018 IEEE
International Conference on Acoustics, Speech and Sig-
nal Processing (ICASSP) , pp. 49044908, 2018.
Valk, J. and Alum ae, T. V oxlingua107: a dataset for spoken
language recognition. In 2021 IEEE Spoken Language
Technology Workshop (SLT) , pp. 652658. IEEE, 2021.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-
tion is all you need. In Advances in neural information
processing systems , pp. 59986008, 2017.
Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M.,
Reddy, T., Cournapeau, D., Burovski, E., Peterson, P.,
Weckesser, W., Bright, J., van der Walt, S. J., Brett, M.,
Wilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J.,
Jones, E., Kern, R., Larson, E., Carey, C. J., Polat, I.,
Feng, Y ., Moore, E. W., VanderPlas, J., Laxalde, D.,
Perktold, J., Cimrman, R., Henriksen, I., Quintero, E. A.,
Harris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa,
F., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy
1.0: Fundamental Algorithms for Scientific Computing
in Python. Nature Methods , 17:261272, 2020. doi:
10.1038/s41592-019-0686-2.
Wang, C., Tang, Y ., Ma, X., Wu, A., Okhonko, D., and Pino,
J. fairseq s2t: Fast speech-to-text modeling with fairseq.
arXiv preprint arXiv:2010.05171 , 2020a.Wang, C., Wu, A., and Pino, J. Covost 2 and massively
multilingual speech-to-text translation. arXiv preprint
arXiv:2007.10310 , 2020b.
Wang, C., Riviere, M., Lee, A., Wu, A., Talnikar, C., Haziza,
D., Williamson, M., Pino, J., and Dupoux, E. V oxpopuli:
A large-scale multilingual speech corpus for representa-
tion learning, semi-supervised learning and interpretation.
arXiv preprint arXiv:2101.00390 , 2021.
Wang, P., Sainath, T. N., and Weiss, R. J. Multitask training
with text data for end-to-end speech recognition. arXiv
preprint arXiv:2010.14318 , 2020c.
Watanabe, S., Mandel, M., Barker, J., Vincent, E., Arora,
A., Chang, X., Khudanpur, S., Manohar, V ., Povey, D.,
Raj, D., et al. Chime-6 challenge: Tackling multispeaker
speech recognition for unsegmented recordings. arXiv
preprint arXiv:2004.09249 , 2020.
Xu, Q., Baevski, A., Likhomanenko, T., Tomasello, P., Con-
neau, A., Collobert, R., Synnaeve, G., and Auli, M. Self-
training and pre-training are complementary for speech
recognition. In ICASSP 2021-2021 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP) , pp. 30303034. IEEE, 2021.
Zhang, Y ., Qin, J., Park, D. S., Han, W., Chiu, C.-C., Pang,
R., Le, Q. V ., and Wu, Y . Pushing the limits of semi-
supervised learning for automatic speech recognition.
arXiv preprint arXiv:2010.10504 , 2020.
Zhang, Y ., Park, D. S., Han, W., Qin, J., Gulati, A., Shor, J.,
Jansen, A., Xu, Y ., Huang, Y ., Wang, S., et al. BigSSL:
Exploring the frontier of large-scale semi-supervised
learning for automatic speech recognition. arXiv preprint
arXiv:2109.13226 , 2021.Robust Speech Recognition via Large-Scale Weak Supervision 19
A. Evaluation Datasets.
A.1. Short-form English-only datasets
LibriSpeech (Panayotov et al., 2015): We used the test-clean and test-other splits from the LibriSpeech ASR corpus.
TED-LIUM 3 (Hernandez et al., 2018): We used the test split of TED-LIUM Release 3, using the segmented manual
transcripts included in the release.
Common Voice 5.1 (Ardila et al., 2019): We downloaded the English subset of Common V oice Corpus 5.1 from the
official website.
Artie bias corpus (Meyer et al., 2020): We used the Artie bias corpus. This is a subset of the Common V oice dataset.
CallHome andSwitchboard : We used the two corpora from LDC2002S09 and LDC2002T43.
WSJ : We used LDC93S6B and LDC94S13B and followed the s5recipe to preprocess the dataset.
CORAAL : We used the 231 interviews from CORAAL (Kendall & Farrington, 2021) and used the preprocessing
script from the FairSpeech project.
CHiME-6 : For CHiME-6 (Watanabe et al., 2020), we downloaded the CHiME-5 dataset and followed the stage 0
of the s5track1 recipe to create the CHiME-6 dataset which fixes synchronization. We then used the binaural
recordings ( *P??.wav ) and the corresponding transcripts.
AMI-IHM andAMI-SDM1 : We preprocessed the AMI Corpus by following the stage 0 ad 2 of the s5b recipe.
A.2. Long-form English-only datasets
TED-LIUM 3 (Hernandez et al., 2018): We used the 11 full-length TED talks from the test split of TED-LIUM
Release 3, slicing the source audio files between the beginning of the first labeled segment and the end of the last
labeled segment of each talk, and we used the concatenated text as the label.
Meanwhile : This dataset consists of 64 segments from The Late Show with Stephen Colbert. The YouTube video ID
and the corresponding start and end timestamps are available as part of the code release. The labels are collected from
the closed-caption data for each video and corrected with manual inspection.
Rev16 : We use a subset of 16 files from the 30 podcast episodes in Rev.AIs Podcast Transcription Benchmark, after
finding that there are multiple cases where a significant portion of the audio and the labels did not match, mostly on the
parts introducing the sponsors. We selected 16 episodes that do not have this error, whose file numbers are:
3 4 9 10 11 14 17 18 20 21 23 24 26 27 29 32
Kincaid46 : This dataset consists of 46 audio files and the corresponding transcripts compiled in the blog article Which
automatic transcription service is the most accurate - 2018 by Jason Kincaid. We used the 46 audio files and reference
transcripts from the Airtable widget in the article. For the human transcription benchmark in the paper, we use a subset
of 25 examples from this data, whose Ref IDs are:
2 4 5 8 9 10 12 13 14 16 19 21 23 25 26 28 29 30 33 35 36 37 42 43 45
Earnings-21 (Del Rio et al., 2021) and Earnings-22 : We used the files available in the speech-datasets repository, as
of their 202206 version.
CORAAL : We used the 231 full-length interviews and transcripts from (Kendall & Farrington, 2021).Robust Speech Recognition via Large-Scale Weak Supervision 20
A.3. Multilingual datasets
Multilingual LibriSpeech (Pratap et al., 2020b): We used the test splits from each language in the Multilingual
LibriSpeech (MLS) corpus.
Fleurs (Conneau et al., 2022): We collected audio files and transcripts using the implementation available as Hug-
gingFace datasets. To use as a translation dataset, we matched the numerical utterance IDs to find the corresponding
transcript in English.
VoxPopuli (Wang et al., 2021): We used the getasrdata.py script from the official repository to collect the ASR
data in 16 languages, including English.
Common Voice 9 (Ardila et al., 2019): We downloaded the Common V oice Corpus 9 from the official website.
CoVOST 2 (Wang et al., 2020b): We collected the X into English data collected using the official repository.
B. Compared Models
For comparison, we use the following models from HuggingFace, downloaded as of September 2022 using version 4.21.0 of
thetransformers library:
facebook/wav2vec2-large-960h-lv60-self (Xu et al., 2021)
facebook/wav2vec2-large-robust-ft-libri-960h (Hsu et al., 2021b)
facebook/wav2vec2-base-100h (Baevski et al., 2020)
facebook/wav2vec2-base-960h (Baevski et al., 2020)
facebook/wav2vec2-large-960h (Baevski et al., 2020)
facebook/hubert-large-ls960-ft (Hsu et al., 2021a)
facebook/hubert-xlarge-ls960-ft (Hsu et al., 2021a)
facebook/s2t-medium-librispeech-asr (Wang et al., 2020a)
facebook/s2t-large-librispeech-asr (Wang et al., 2020a)
microsoft/unispeech-sat-base-100h-libri-ft (Chen et al., 2022a)
nvidia/stt enconformer ctclarge (Kuchaiev et al., 2019)
nvidia/stt enconformer transducer xlarge (Kuchaiev et al., 2019)
speechbrain/asr-crdnn-rnnlm-librispeech (Ravanelli et al., 2021)
speechbrain/asr-transformer-transformerlm-librispeech (Ravanelli et al., 2021)
We note that all of the models above are entirely or partly trained on LibriSpeech.Robust Speech Recognition via Large-Scale Weak Supervision 21
C. Text Standardization
Since Whisper may output any UTF-8 string rather than a restricted set of graphemes, the rules for text standardization need
to be more intricate and comprehensive than those defined on e.g. ASCII characters. We perform the following steps to
normalize English texts in different styles into a standardized form, which is a best-effort attempt to penalize only when a
word error is caused by actually mistranscribing a word, and not by formatting or punctuation differences.
1. Remove any phrases between matching brackets ( [,]).
2. Remove any phrases between matching parentheses ( (,)).
3. Remove any of the following words: hmm,mm,mhm,mmm,uh,um
4. Remove whitespace characters that comes before an apostrophe 
5. Convert standard or informal contracted forms of English into the original form.
6. Remove commas ( ,) between digits
7. Remove periods ( .) not followed by numbers
8.Remove symbols as well as diacritics from the text, where symbols are the characters with the Unicode category
starting with M,S, orP, except period, percent, and currency symbols that may be detected in the next step.
9.Detect any numeric expressions of numbers and currencies and replace with a form using Arabic numbers, e.g. Ten
thousand dollars $10000.
10. Convert British spellings into American spellings.
11. Remove remaining symbols that are not part of any numeric expressions.
12. Replace any successive whitespace characters with a space.
A different, language-specific set of transformations would be needed to equivalently normalize non-English text, but due to
our lack of linguistic knowledge to build such normalizers for all languages, we resort to the following basic standardization
for non-English text:
1. Remove any phrases between matching brackets ( [,]).
2. Remove any phrases between matching parentheses ( (,)).
3.Replace any markers, symbols, and punctuation characters with a space, i.e. when the Unicode category of each
character in the NFKC-normalized string starts with M,S, orP.
4. make the text lowercase.
5. replace any successive whitespace characters with a space.
Additionally, we put a space between every letter for the languages that do not use spaces to separate words, namely Chinese,
Japanese, Thai, Lao, and Burmese, effectively measuring the character error rate instead.
We note that the above is an imperfect solution, and it will sometimes produce unintended and unexpected outputs. We do
not claim that the text format resulting from the above is more correct in any measure. Rather, the procedures above are
designed to better distinguish between innocuous differences in wording and genuine mistranscriptions. Python code for
the standardization procedures above is available as part of our code and model release to facilitate future iterations and
improvements on text standardization.Robust Speech Recognition via Large-Scale Weak Supervision 22
D. Raw Performance Tables
D.1. English Transcription
D.1.1. G REEDY DECODING
ModelLibriSpeech.test-cleanLibriSpeech.test-otherTED-LIUM3WSJCallHomeSwitchboardCommonV oice5.1ArtieCORAALCHiME6AMI-IHMAMI-SDM1V oxPopuli.enFleurs.en us
Whisper tiny.en 5.6 14.6 6.0 5.0 24.1 17.8 26.3 20.0 23.9 41.3 23.7 50.3 11.7 11.6
Whisper tiny 7.6 16.9 7.0 6.7 30.0 22.8 29.6 23.9 31.0 49.6 27.6 58.1 12.7 13.7
Whisper base.en 4.2 10.2 4.9 4.6 20.9 15.2 19.0 13.4 22.6 36.4 20.5 46.7 10.0 7.6
Whisper base 5.0 12.4 5.5 5.1 23.0 16.8 21.6 16.9 26.0 40.2 22.0 49.9 10.0 10.1
Whisper small.en 3.1 7.4 4.0 3.3 18.2 15.7 13.1 9.7 20.2 27.6 17.5 38.0 8.1 6.0
Whisper small 3.4 7.6 4.3 4.0 17.5 14.5 13.5 10.3 18.1 29.3 19.0 39.6 8.3 6.6
Whisper medium.en 3.1 6.3 4.1 3.3 16.2 14.1 10.6 7.6 17.5 25.3 16.4 37.2 7.4 5.0
Whisper medium 2.9 5.9 3.8 2.9 16.4 14.0 10.3 7.2 16.6 26.4 16.6 36.0 7.4 5.4
Whisper large 2.7 5.6 4.0 3.1 15.8 13.1 9.5 6.7 19.4 25.6 16.4 36.9 7.3 4.6
Whisper large-v2 2.7 5.2 4.0 3.9 17.6 13.8 9.0 6.2 16.2 25.5 16.9 36.4 7.3 4.4
wav2vec2-base-100h 6.0 13.4 17.8 13.9 46.9 40.2 47.4 40.8 47.0 79.9 48.1 81.2 28.9 23.1
wav2vec2-base-960h 3.3 8.5 12.8 8.9 40.6 32.9 36.4 30.9 39.9 68.5 40.2 71.9 21.4 17.4
wav2vec2-large-960h-lv60-self 1.8 3.8 7.4 4.4 29.1 22.2 19.9 15.8 29.2 56.3 30.8 57.0 13.0 10.2
wav2vec2-large-960h 2.7 6.2 10.5 7.7 34.8 28.3 29.9 24.5 35.6 65.8 37.0 67.6 17.9 14.6
wav2vec2-large-robust-ft-libri-960h 2.6 5.3 9.2 6.1 23.4 19.8 20.3 16.2 29.4 58.1 31.7 61.6 15.1 11.8
asr-crdnn-rnnlm-librispeech 3.0 9.7 17.7 10.7 59.7 56.1 43.7 33.3 83.8 81.0 57.2 85.8 30.6 32.4
asr-transformer-transformerlm-librispeech 2.1 5.4 11.9 7.4 38.9 33.0 30.6 23.5 44.9 79.5 44.5 75.4 17.8 17.0
hubert-large-ls960-ft 2.0 4.1 8.4 5.4 29.6 22.8 20.8 16.0 32.0 60.0 33.7 59.1 14.4 10.9
hubert-xlarge-ls960-ft 1.9 3.5 8.3 5.4 29.3 22.2 19.8 14.8 31.5 58.5 33.3 58.9 14.2 10.5
s2t-large-librispeech-asr 3.3 8.1 14.9 9.4 54.5 40.3 38.1 30.7 50.2 79.2 53.4 79.5 21.6 18.0
s2t-medium-librispeech-asr 3.6 8.2 15.7 9.7 58.1 42.4 39.3 31.3 52.6 79.8 60.3 85.3 22.9 19.7
sttenconformer ctclarge 2.1 4.2 4.4 2.1 11.3 8.2 7.4 4.0 13.5 30.5 15.9 39.9 6.7 8.2
sttenconformer transducer xlarge 1.5 2.8 4.3 1.2 12.0 7.4 4.3 1.5 19.9 36.8 20.5 48.6 6.0 6.3
unispeech-sat-base-100h-libri-ft 5.7 13.8 17.7 13.6 46.5 40.0 45.3 38.6 44.7 74.8 47.8 77.7 29.8 22.4
Table 8. English transcription WER (%) with greedy decoding
D.1.2. B EAM SEARCH WITH TEMPERATURE FALLBACK
ModelLibriSpeech.test-cleanLibriSpeech.test-otherTED-LIUM3WSJCallHomeSwitchboardCommonV oice5.1ArtieCORAALCHiME6AMI-IHMAMI-SDM1V oxPopuli.enFleurs.en us
Whisper tiny.en 5.4 12.8 5.4 4.6 21.4 16.0 23.5 18.4 21.4 42.0 22.7 54.2 10.9 10.0
Whisper tiny 6.7 15.0 6.3 5.9 24.8 18.3 26.1 20.8 25.1 48.0 25.6 57.3 11.6 12.4
Whisper base.en 4.1 9.6 4.6 4.0 18.3 14.2 17.5 13.2 18.5 35.2 21.1 49.0 9.3 7.1
Whisper base 4.9 11.0 5.0 4.4 20.5 15.6 19.4 15.3 20.5 40.0 21.5 50.0 9.5 8.9
Whisper small.en 3.2 6.7 4.3 3.0 17.2 13.4 12.6 9.2 17.5 29.5 17.9 42.5 8.1 5.3
Whisper small 3.3 7.2 4.3 3.9 17.1 13.3 12.8 9.3 16.4 30.9 19.2 43.5 8.2 6.1
Whisper medium.en 3.0 5.7 4.3 2.8 14.7 12.4 10.3 7.4 15.3 27.0 17.1 39.4 7.8 4.5
Whisper medium 2.7 5.6 4.0 2.7 15.3 13.2 9.7 6.7 14.9 27.6 17.6 43.0 7.6 4.4
Whisper large 2.8 5.7 4.3 3.5 16.2 14.2 8.9 6.4 15.1 25.2 17.6 37.1 7.2 4.5
Whisper large-v2 2.5 4.9 3.7 2.6 16.4 13.6 8.2 5.7 14.2 24.9 17.4 39.9 7.0 4.2
Table 9. English transcription WER (%) with beam search and temperature fallbackRobust Speech Recognition via Large-Scale Weak Supervision 23
D.2. Multilingual Transcription
D.2.1. M ULTILINGUAL LIBRISPEECH
ModelDutchEnglishFrenchGermanItalianPolishPortugueseSpanish
Whisper tiny 39.4 15.7 36.8 24.9 41.7 34.2 31.3 19.2
Whisper base 28.4 11.7 26.6 17.7 31.1 22.8 21.9 12.8
Whisper small 17.2 8.3 16.2 10.5 21.4 11.2 13.0 7.8
Whisper medium 11.7 6.8 8.9 7.4 16.0 6.5 9.0 5.3
Whisper large 10.2 6.3 8.9 6.6 14.3 6.6 9.2 5.4
Whisper large-v2 9.3 6.2 7.3 5.5 13.8 5.0 6.8 4.2
Table 10. WER (%) on MLS
D.2.2. C OMMON VOICE 9
ModelArabicBulgarianBengaliCatalanCzechWelshDanishGermanGreekEnglishSpanishEstonianPersian
Whisper tiny 90.9 79.3 104.1 51.0 79.7 101.8 77.2 34.5 61.9 28.8 30.3 102.1 120.3
Whisper base 84.4 68.1 103.7 39.9 63.1 93.8 57.5 24.5 51.5 21.9 19.6 88.1 99.0
Whisper small 66.4 44.8 118.6 23.8 34.1 65.4 32.1 13.0 31.7 14.5 10.3 67.2 71.9
Whisper medium 60.3 26.7 124.7 16.4 18.8 43.6 19.3 8.5 20.0 11.2 6.9 45.6 49.9
Whisper large 56.0 24.1 106.0 15.3 17.1 40.3 18.3 7.7 18.3 10.1 6.4 41.4 44.8
Whisper large-v2 53.8 19.9 103.4 14.1 13.5 34.2 14.4 6.4 16.0 9.4 5.6 35.1 39.4
ModelFinnishFrenchHindiHungarianIndonesianItalianJapaneseLithuanianLatvianMalayalamMongolianDutchPolish
Whisper tiny 68.5 49.7 108.3 87.0 49.6 44.5 36.1 103.5 87.8 102.7 123.0 43.6 45.3
Whisper base 52.9 37.3 106.5 71.9 36.1 30.5 24.2 91.3 78.0 122.9 137.0 29.5 32.8
Whisper small 30.5 22.7 43.6 44.4 18.4 16.0 14.0 72.8 54.6 104.8 225.8 14.2 16.9
Whisper medium 18.8 16.0 31.5 26.9 11.6 9.4 10.5 49.4 37.2 137.8 113.4 8.0 10.1
Whisper large 17.0 14.7 25.0 23.5 10.6 8.1 9.4 43.9 34.8 107.1 117.4 7.1 9.0
Whisper large-v2 14.4 13.9 21.9 19.7 8.5 7.1 9.1 35.2 25.5 103.2 128.4 5.8 7.6
ModelPortugueseRomanianRussianSlovakSlovenianSerbianSwedishTamilThaiTurkishUrduVietnameseChinese
Whisper tiny 35.2 68.2 40.6 104.0 82.0 106.1 58.2 105.7 55.9 53.6 74.7 69.3 52.4
Whisper base 23.7 55.9 28.8 87.2 70.3 103.0 42.4 49.5 32.1 38.6 58.6 51.6 44.9
Whisper small 12.5 33.2 15.0 60.4 45.5 101.3 22.1 28.7 18.1 23.7 39.1 33.3 29.4
Whisper medium 8.1 21.5 9.3 42.0 29.8 85.6 13.7 19.6 10.5 17.7 29.9 24.4 23.2
Whisper large 7.1 19.8 8.2 37.9 25.1 87.4 12.4 17.6 8.8 16.6 28.1 19.9 29.1
Whisper large-v2 6.3 15.8 7.1 31.9 20.6 70.5 10.6 16.1 8.0 14.5 24.2 18.2 26.8
Table 11. WER (%) on CommonV oice9
D.2.3. V OXPOPULI
ModelCzechGermanEnglishenaccentedSpanishEstonianFinnishFrenchCroatianHungarianItalianLithuanianDutchPolishRomanianSlovakSlovenian
Whisper tiny 73.5 27.4 11.6 18.8 19.7 99.2 54.1 32.9 72.4 74.5 40.5 93.1 41.9 31.4 65.9 78.7 81.9
Whisper base 54.7 20.6 9.5 17.5 14.4 83.0 39.7 24.9 53.6 52.6 30.8 82.1 29.4 22.1 49.3 63.7 70.5
Whisper small 28.8 14.8 8.2 19.2 11.1 59.2 24.9 15.7 33.7 31.3 22.9 60.1 18.8 13.3 28.6 37.3 50.8
Whisper medium 18.4 12.4 7.6 19.1 9.6 38.2 16.6 12.2 23.9 19.3 19.7 39.3 14.9 10.1 18.4 23.0 36.3
Whisper large 15.9 11.9 7.2 20.8 8.8 33.3 15.5 11.0 19.0 16.8 18.4 35.0 14.0 9.0 17.0 19.1 31.3
Whisper large-v2 12.6 11.2 7.0 18.6 8.2 28.7 12.4 11.4 16.1 13.8 19.0 33.2 12.9 7.8 14.4 15.4 27.9
Table 12. WER (%) on V oxPopuliRobust Speech Recognition via Large-Scale Weak Supervision 24
D.2.4. F LEURS
ModelAfrikaansAmharicArabicAssameseAzerbaijaniBelarusianBulgarianBengaliBosnianCatalanChineseCzechWelshDanish
Whisper tiny 91.2 122.9 63.4 102.0 93.1 94.0 81.0 101.6 82.1 42.8 40.5 82.8 101.3 82.0
Whisper base 81.5 196.8 48.8 102.0 76.4 91.3 65.1 100.6 66.7 29.0 34.1 66.0 85.3 57.6
Whisper small 61.1 120.2 30.6 108.0 49.1 75.1 37.3 104.4 39.4 16.2 20.8 37.6 59.3 32.8
Whisper medium 44.9 229.3 20.4 102.3 33.1 60.4 21.4 100.6 23.9 9.6 12.1 21.3 40.8 19.5
Whisper large 42.6 129.3 18.1 105.6 28.7 56.6 18.4 104.9 20.7 8.0 19.6 17.4 36.6 16.8
Whisper large-v2 36.7 140.3 16.0 106.2 23.4 45.4 14.6 104.1 15.7 7.3 14.7 13.3 33.0 13.8
ModelGermanGreekEnglishSpanishEstonianPersianFinnishTagalogFrenchGalicianGujaratiHausaHebrewHindi
Whisper tiny 27.8 67.4 12.4 15.9 94.8 101.8 59.5 65.6 41.4 54.8 101.2 100.2 71.6 102.3
Whisper base 17.9 53.5 8.9 9.9 77.9 86.1 43.1 45.8 28.5 47.4 101.4 98.6 61.7 101.1
Whisper small 10.2 30.8 6.1 5.6 51.3 55.8 24.0 27.7 15.0 30.2 106.4 90.1 44.4 38.4
Whisper medium 6.5 19.0 4.4 3.6 29.8 41.0 13.9 19.1 8.7 21.2 104.8 106.6 33.1 26.8
Whisper large 5.5 18.7 4.5 3.5 25.5 36.1 12.2 15.8 7.7 19.0 103.9 87.0 30.2 26.9
Whisper large-v2 4.5 12.5 4.2 3.0 21.9 32.9 9.7 13.8 8.3 15.4 102.7 88.9 27.1 21.5
ModelCroatianHungarianArmenianIndonesianIcelandicItalianJapaneseJavaneseGeorgianKazakhKhmerKannadaKoreanLuxembourgish
Whisper tiny 79.0 83.8 118.6 51.7 113.3 29.8 37.0 107.3 123.0 165.2 100.6 100.7 36.1 99.1
Whisper base 59.1 65.0 126.3 33.1 95.5 17.9 22.8 89.5 114.7 109.2 101.6 107.2 27.8 100.7
Whisper small 33.4 38.9 86.6 16.3 72.6 9.8 12.0 88.6 118.3 70.3 104.4 100.4 19.6 100.1
Whisper medium 19.3 24.3 60.1 10.2 49.9 5.2 7.1 67.9 117.3 48.8 98.9 77.7 16.4 90.0
Whisper large 16.7 21.0 53.7 8.5 43.0 4.2 6.4 87.0 100.5 43.8 96.0 69.8 15.2 86.5
Whisper large-v2 13.4 17.0 44.6 7.1 38.2 4.0 5.3 nan 105.0 37.7 99.7 37.0 14.3 88.0
ModelLingalaLaoLithuanianLatvianMaoriMacedonianMalayalamMongolianMarathiMalayMalteseMyanmarNorwegianNepali
Whisper tiny 105.4 115.1 98.5 91.6 94.5 73.3 101.5 113.7 100.3 51.2 100.8 124.8 62.0 101.8
Whisper base 96.7 105.1 87.3 79.8 77.5 59.9 107.4 125.7 100.3 35.1 97.6 122.6 44.0 102.4
Whisper small 91.3 102.2 65.6 53.2 59.5 36.9 100.9 144.2 60.2 18.9 92.2 110.1 24.2 69.5
Whisper medium 83.2 101.4 41.1 32.0 77.8 22.0 101.1 103.7 63.2 12.2 83.2 123.0 12.9 54.4
Whisper large 76.8 101.6 35.2 28.3 45.7 20.6 101.4 106.2 43.7 10.2 80.5 124.5 11.4 52.2
Whisper large-v2 75.6 101.5 28.1 23.1 38.5 16.5 100.7 110.5 38.3 8.7 76.6 115.7 9.5 47.1
ModelDutchOccitanPunjabiPolishPashtoPortugueseRomanianRussianSindhiSlovakSlovenianShonaSomaliSerbian
Whisper tiny 49.0 95.9 102.6 45.6 105.6 20.1 74.7 31.1 105.8 77.2 87.2 128.1 105.6 83.7
Whisper base 33.0 82.9 101.5 30.8 99.0 13.0 56.0 20.5 103.9 60.6 74.6 126.0 109.6 64.3
Whisper small 16.4 87.3 103.6 14.7 92.9 7.3 29.8 11.4 131.7 33.3 49.3 140.0 105.3 42.2
Whisper medium 9.9 79.5 102.0 8.0 119.4 5.0 20.0 7.2 147.0 17.3 31.9 143.9 104.0 44.9
Whisper large 8.3 75.9 102.8 7.2 92.7 4.8 15.4 6.4 177.9 15.7 27.8 130.0 103.5 29.2
Whisper large-v2 6.7 75.3 102.4 5.4 93.7 4.3 14.4 5.6 156.5 11.7 23.1 121.0 102.9 33.9
ModelSwedishSwahiliTamilTeluguTajikThaiTurkishUkrainianUrduUzbekVietnameseYoruba
Whisper tiny 52.7 100.9 99.9 105.1 101.7 58.8 42.5 51.2 65.2 105.2 60.0 106.4
Whisper base 37.4 92.5 58.7 105.2 109.3 38.2 27.5 37.7 52.0 114.0 40.5 101.8
Whisper small 20.8 73.7 35.2 98.2 84.3 21.9 15.9 19.3 37.3 107.7 21.2 116.4
Whisper medium 11.2 52.8 23.1 82.8 74.0 15.4 10.4 11.6 28.2 109.6 12.7 105.1
Whisper large 10.5 47.9 20.6 100.6 74.5 13.2 9.4 10.3 25.0 93.3 10.7 111.7
Whisper large-v2 8.5 39.3 17.5 99.0 85.8 11.5 8.4 8.6 22.6 90.2 10.3 94.8
Table 13. WER (%) on FleursRobust Speech Recognition via Large-Scale Weak Supervision 25
D.3. Speech Translation
D.3.1. F LEURS
ModelAfrikaansAmharicArabicAssameseAzerbaijaniBelarusianBulgarianBengaliBosnianCatalanChineseCzechWelshDanish
Whisper tiny 1.6 0.1 0.1 0.4 0.1 0.8 0.4 0.4 0.4 5.2 0.6 0.6 0.6 0.7
Whisper base 4.4 0.3 1.0 0.4 0.8 3.3 2.7 0.7 4.1 13.1 1.9 2.7 0.7 5.0
Whisper small 18.1 0.2 10.6 1.2 5.8 7.1 14.8 2.7 16.8 25.1 9.3 14.2 1.3 18.1
Whisper medium 29.5 0.9 19.9 3.5 11.7 9.8 23.9 10.6 26.0 31.9 15.1 23.6 8.4 28.6
Whisper large 31.6 1.1 23.8 3.9 13.1 11.0 26.2 12.0 28.0 33.7 16.8 25.6 11.2 31.6
Whisper large-v2 34.1 1.9 25.5 5.4 13.7 11.7 28.5 13.2 29.7 34.2 18.4 27.8 13.0 32.7
ModelGermanGreekEnglishSpanishEstonianPersianFinnishTagalogFrenchGalicianGujaratiHausaHebrewHindi
Whisper tiny 5.2 0.1 68.6 7.7 0.1 0.1 0.2 0.8 4.7 4.0 0.7 0.1 0.2 1.0
Whisper base 13.7 0.7 73.3 12.4 0.3 0.2 0.5 2.1 13.1 10.5 1.5 0.0 0.6 3.4
Whisper small 25.9 11.6 77.3 18.2 3.6 5.8 7.3 12.0 23.5 17.5 3.9 0.3 5.4 11.1
Whisper medium 31.4 19.9 79.2 21.4 13.5 15.0 18.5 20.5 28.6 24.7 12.8 0.5 15.9 19.4
Whisper large 34.3 21.7 77.8 22.8 15.9 17.6 20.6 22.7 31.6 26.0 14.8 0.5 19.6 20.7
Whisper large-v2 34.6 23.7 80.2 23.3 18.7 19.6 22.1 24.4 32.2 27.9 16.2 0.4 21.8 22.0
ModelCroatianHungarianArmenianIndonesianIcelandicItalianJapaneseJavaneseGeorgianKazakhKhmerKannadaKoreanLuxembourgish
Whisper tiny 0.6 0.1 0.1 0.3 0.4 5.3 0.2 0.2 0.1 0.1 0.1 0.8 0.5 0.8
Whisper base 3.7 0.2 0.1 2.6 0.4 11.3 1.5 0.2 0.2 0.2 0.1 0.9 3.7 1.7
Whisper small 14.6 4.8 0.7 16.4 1.8 17.8 9.6 1.4 0.2 0.8 0.5 2.3 12.2 5.7
Whisper medium 23.0 15.5 10.4 24.1 6.8 21.6 14.9 5.0 1.3 4.3 3.3 8.5 19.2 13.6
Whisper large 25.4 18.3 13.2 27.2 6.6 23.5 17.0 5.1 2.7 6.3 5.2 9.9 20.0 15.4
Whisper large-v2 27.0 21.2 16.0 29.1 9.1 23.6 18.9 6.2 2.4 5.4 6.1 11.6 21.3 16.8
ModelLingalaLaoLithuanianLatvianMaoriMacedonianMalayalamMongolianMarathiMalayMalteseMyanmarNorwegianNepali
Whisper tiny 0.1 0.2 0.1 0.2 0.3 1.0 0.8 0.1 0.2 0.3 0.6 0.1 1.4 0.1
Whisper base 0.1 0.3 0.3 0.4 1.0 5.4 1.4 0.1 0.9 2.1 1.4 0.1 8.4 0.3
Whisper small 0.5 2.0 1.9 1.5 3.9 15.3 5.7 0.1 3.8 14.1 4.9 0.0 22.0 2.9
Whisper medium 0.9 8.1 9.6 10.0 8.5 23.5 13.8 0.5 10.9 23.2 11.2 0.2 29.1 12.7
Whisper large 1.2 9.3 12.0 12.5 9.4 26.4 16.5 1.0 13.1 25.5 12.8 0.5 30.5 12.9
Whisper large-v2 1.0 11.0 14.0 14.3 10.2 27.7 16.7 1.0 12.9 27.3 13.5 0.4 31.4 16.1
ModelDutchOccitanPunjabiPolishPashtoPortugueseRomanianRussianSindhiSlovakSlovenianShonaSomaliSerbian
Whisper tiny 2.7 1.7 0.3 0.8 0.3 12.1 1.0 3.1 0.5 0.7 0.3 0.1 0.0 0.6
Whisper base 7.5 4.2 1.1 5.1 0.4 22.4 4.9 12.1 0.7 4.6 1.3 0.3 0.1 5.4
Whisper small 15.9 9.5 4.4 14.0 0.8 31.2 18.3 19.7 2.0 14.4 6.9 0.6 0.1 19.3
Whisper medium 21.6 15.9 12.8 19.0 2.1 35.9 26.6 24.8 5.5 22.7 14.0 1.4 0.4 27.7
Whisper large 22.8 16.8 14.6 21.4 3.7 37.4 29.1 26.7 5.9 25.1 16.9 1.8 0.5 30.5
Whisper large-v2 24.0 20.2 15.7 22.3 3.4 38.1 31.5 27.8 5.7 26.1 17.0 1.8 0.7 32.5
ModelSwedishSwahiliTamilTeluguTajikThaiTurkishUkrainianUrduUzbekVietnameseYoruba
Whisper tiny 1.8 0.1 0.2 0.3 0.2 0.2 0.2 1.2 0.4 0.0 0.1 0.2
Whisper base 9.1 0.1 0.4 0.4 0.2 0.7 2.4 6.9 1.5 0.2 0.9 0.5
Whisper small 22.9 0.1 2.1 4.0 4.4 5.8 15.7 18.7 8.8 0.5 8.5 0.5
Whisper medium 32.1 3.1 7.0 10.8 11.4 12.8 22.9 25.8 14.9 3.8 16.6 0.9
Whisper large 33.1 5.3 8.5 10.9 13.0 15.2 25.7 28.0 16.3 5.8 19.5 1.2
Whisper large-v2 35.3 7.2 9.2 12.5 14.5 16.1 26.6 29.4 17.2 6.0 20.4 1.4
Table 14. BLEU scores on FleursRobust Speech Recognition via Large-Scale Weak Supervision 26
D.3.2. C OVOST 2
ModelArabicCatalanWelshGermanSpanishEstonianPersianFrenchIndonesianItalianJapaneseLatvianMongolian
Whisper tiny 0.2 4.9 0.4 4.0 10.5 0.2 0.1 6.1 0.3 5.1 0.3 0.1 0.1
Whisper base 1.2 11.0 0.5 11.7 21.3 0.3 0.1 15.4 4.9 13.0 4.9 0.5 0.1
Whisper small 17.7 22.3 1.0 25.3 33.0 2.4 4.9 27.3 27.6 24.0 17.3 1.4 0.2
Whisper medium 30.6 29.2 12.1 33.2 38.4 11.4 15.5 33.6 42.3 29.5 24.6 9.7 0.2
Whisper large 35.5 30.3 16.1 34.3 38.0 13.4 17.5 34.4 45.4 29.1 24.2 10.5 0.3
Whisper large-v2 39.7 31.8 21.5 36.3 40.1 15.0 19.3 36.4 48.1 30.9 26.1 13.9 0.1
ModelDutchPortugueseRussianSlovenianSwedishTamilTurkishChinese
Whisper tiny 4.3 9.5 5.7 0.4 2.0 0.1 0.2 0.4
Whisper base 12.4 23.2 16.1 1.4 10.5 0.4 2.8 1.4
Whisper small 28.1 40.6 30.9 9.2 29.9 1.7 16.8 6.8
Whisper medium 38.1 48.7 39.4 17.7 39.5 2.9 27.0 14.0
Whisper large 39.3 48.6 41.6 23.9 40.3 3.7 26.7 17.1
Whisper large-v2 41.2 51.6 43.3 21.6 42.9 4.2 28.3 18.0
Table 15. BLEU scores on CoV oST2
D.4. Long-form Transcription
ModelTED-LIUM3MeanwhileKincaid46Rev16Earnings-21Earnings-22CORAAL
Whisper tiny.en 5.5 12.8 13.8 15.1 17.0 22.0 30.3
Whisper tiny 6.8 15.5 16.7 17.0 18.7 24.4 33.1
Whisper base.en 4.6 9.4 11.2 13.2 12.5 16.6 25.2
Whisper base 4.8 12.2 12.2 14.5 13.5 18.4 26.9
Whisper small.en 4.6 6.0 9.4 12.0 10.8 14.0 21.9
Whisper small 4.2 6.9 10.1 12.1 11.1 14.3 22.3
Whisper medium.en 3.6 5.2 8.9 11.9 10.2 13.3 20.6
Whisper medium 3.8 5.4 8.6 11.4 10.3 13.2 20.3
Whisper large 3.8 5.3 8.8 11.0 10.3 13.4 20.4
Whisper large-v2 3.5 5.1 8.8 11.3 9.7 12.6 19.6
wav2vec2-base-100h 17.6 27.7 39.3 35.2 45.7 57.1 55.4
wav2vec2-base-960h 12.8 19.7 32.9 29.8 37.3 46.8 49.1
wav2vec2-large-960h-lv60-self 7.2 11.4 21.1 21.3 21.7 28.0 36.7
wav2vec2-large-960h 10.1 16.4 27.4 26.4 30.4 40.1 43.5
wav2vec2-large-robust-ft-libri-960h 8.8 15.2 22.9 23.4 23.0 31.0 36.8
hubert-large-ls960-ft 8.1 12.9 22.4 23.4 23.0 30.6 37.9
hubert-xlarge-ls960-ft 8.1 12.5 22.9 23.2 23.1 31.3 38.1
sttenconformer ctclarge 4.0 9.8 13.1 14.5 12.6 17.6 25.1
sttenconformer transducer xlarge 5.3 10.6 17.1 19.8 16.2 19.7 38.9
Table 16. Long-form English transcription WER (%)Robust Speech Recognition via Large-Scale Weak Supervision 27
E. Training Dataset Statistics
0.1 1 10 100 1K 10K
Hours of audioMultilingual Speech Recognition
Lao 0.1Sundanese0.1Burmese 0.1Malagasy 0.2T ajik 0.3Gujarati 0.3Uzbek 0.3Yiddish 0.4Malayalam 0.5Georgian 0.6Nepali 0.6Marathi 0.6Punjabi 0.8Haitian Creole 1.0Maltese 1.1Bengali 1.3Khmer 1.3Belarusian 2.4Kannada 3.8Afrikaans 4.1T elugu 4.3Swahili 5.4Sinhala 5.4Albanian 5.7Galician 8.9Bosnian 11Hindi 12Kazakh 12Armenian 13Macedonian 16Icelandic 16Basque 21Persian 24Serbian 28Slovenian 41Estonian 41Azerbaijani 47Latvian 65Lithuanian 67Welsh 73T agalog 75Bulgarian 86Slovak 90Croatian 91Urdu 104T amil 136Czech 192Thai 226Norwegian 266Romanian 356Hungarian 379Malay 382Danish 473Greek 529Hebrew 688Vietnamese 691Ukrainian 697Arabic 739Indonesian 1014Finnish 1066Catalan 1883Dutch 2077Swedish 2119Italian 2585Polish 4278Turkish 4333Japanese 7054Korean 7993Portuguese 8573French 9752Russian 9761Spanish 11100German 13344Chinese 23446
65% English Speech Recognition
(438,218 hours)18% Translation
(125,739 hours)17% Multilingual Speech Recognition
(117,113 hours)Dataset Components
1 10 100 1K 10K
Hours of audioTranslation
Turkmen 1Bashkir 1Malagasy 2Uzbek 4Sundanese 7Hausa 8Luxembourgish 10T atar 14T ajik 15Lingala 20Lao 20Somali 21Macedonian 30Kazakh 31Amharic 32Georgian 40Maltese 41Sindhi 46Faroese 46Occitan 49Burmese 59Pashto 63Latvian 68Albanian 72Haitian Creole 74Estonian 79Mongolian 79Icelandic 84Yiddish 85Azerbaijani 86Kannada 90Lithuanian 99Armenian 116Punjabi 117Belarusian 133Nepali 133Assamese 136Serbian 136Slovak 144Basque 168Tibetan 186Sanskrit 195Bulgarian 202Gujarati 208Sinhala 211Bosnian 219Catalan 236Croatian 239Breton 269Shona 279Swahili 282Marathi 288Norwegian 322Afrikaans 330Hawaiian 338Galician 368Danish 386Persian 392Slovenian 395Czech 401Hebrew 418Yoruba 432Ukrainian 509Hungarian 554Romanian 555Javanese 622Khmer 672Finnish 750Malayalam 892T agalog 894Greek 968T elugu 987Swedish 1055Indonesian 1174Maori 1381T amil 1484Latin 1614Thai 1635Malay 1691Vietnamese 1719Dutch 1767Norwegian Nynorsk 1889Bengali 1988Urdu 1990Italian 2145Polish 2200Turkish 2241Arabic 2286Portuguese 3620German 4309French 4481Hindi 5438Spanish 6693Russian 7687Welsh 8263Japanese 8860Chinese 11731Korean 19938
Figure 11. Training dataset statisticsRobust Speech Recognition via Large-Scale Weak Supervision 28
F. Hyperparameters
Hyperparameter Value
Updates 1048576
Batch Size 256
Warmup Updates 2048
Max grad norm 1.0
Optimizer AdamW
β1 0.9
β2 0.98
ϵ 106
Weight Decay 0.1
Weight Init Gaussian Fan-In
Learning Rate Schedule Linear Decay
Speechless audio subsample factor 10
Condition on prior text rate 50%
Table 17. Whisper training hyperparameters.
Hyperparameter Value
Updates 655360
Batch Size 1024
BPE Dropout 0.1
Stochastic Depth 0.1
SpecAugment Policy LibriSpeech Basic
Table 18. Hyperparameters changed for Whisper Large V2.
Model Max Learning Rate
Tiny 1.5103
Base 1103
Small 5104
Medium 2.5104
Large 1.75104
Large V2 2.0104
Table 19. Whisper model learning rates.
  2023-03-11
Resurrecting Recurrent Neural Networks for
Long Sequences
Antonio Orvieto1,+, Samuel L Smith2, Albert Gu2, Anushan Fernando2, Caglar Gulcehre2, Razvan Pascanu2
and Soham De2
1ETH Zurich,2DeepMind,+Work done at DeepMind.
Recurrent Neural Networks (RNNs) oﬀer fast inference on long sequences but are hard to optimize
and slow to train. Deep state-space models (SSMs) have recently been shown to perform remarkably
well on long sequence modeling tasks, and have the added beneﬁts of fast parallelizable training and
RNN-like fast inference. However, while SSMs are superﬁcially similar to RNNs, there are important
diﬀerences that make it unclear where their performance boost over RNNs comes from. In this paper,
we show that careful design of deep RNNs using standard signal propagation arguments can recover
the impressive performance of deep SSMs on long-range reasoning tasks, while also matching their
training speed. To achieve this, we analyze and ablate a series of changes to standard RNNs including
linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and
ensuring proper normalization of the forward pass. Our results provide new insights on the origins
of the impressive performance of deep SSMs, while also introducing an RNN block called the Linear
Recurrent Unit that matches both their performance on the Long Range Arena benchmark and their
computational eﬃciency.
1. Introduction
Recurrent neural networks (RNNs) have played a central role since the early days of deep learning, and are
a natural choice when modelling sequential data (Elman, 1990; Hopﬁeld, 1982; McCulloch and Pitts, 1943;
Rumelhart et al., 1985). However, while these networks have strong theoretical properties, such as Turing
completeness (Chung and Siegelmann, 2021; Kilian and Siegelmann, 1996), it is well-known that they can be
hard to train in practice. In particular, RNNs suﬀer from the vanishing and exploding gradient problem (Bengio
et al., 1994; Hochreiter, 1991; Pascanu et al., 2013), which makes it diﬃcult for these models to learn about
the long-range dependencies in the data. Several techniques were developed that attempt to mitigate this
issue, including orthogonal/unitary RNNs (Arjovsky et al., 2016; Helfrich et al., 2018), and gating mechanisms
such as long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent units
(GRUs)(Choetal.,2014a). Nonetheless,thesemodelsarestillslowtooptimizeduetotheinherentlysequential
nature of their computation (Kalchbrenner et al., 2016), and are therefore hard to scale.
Inrecentyears, Transformers(Vaswanietal.,2017)havegainedincreasingprominenceforsequencemodelling
tasks, achieving remarkable success in a wide range of applications (Brown et al., 2020; Dosovitskiy et al.,
2020; Jumper et al., 2021). Compared to RNNs, attention layers are easier to scale and parallelize during
training, and crucially they do not suﬀer from the vanishing gradient problem, since the interaction between
any two tokens in the sequence is modeled by direct edges in the network. A key issue with attention layers
however is that their computational and memory costs scale quadratically as 𝑂¹𝐿2ºwith the sequence length 𝐿.
Transformers can therefore be especially expensive to deploy on long sequences. RNNs, which scale linearly
with the sequence length, are therefore typically faster than transformers at inference time even for modest
sequence lengths (Liu et al., 2019).
Motivated by these problems, Gu et al. (2021a) recently introduced the S4 model, a carefully designed deep
state-space model (SSM) achieving remarkable performance on tasks from the Long Range Arena (LRA) (Tay
et al., 2020), a benchmark explicitly designed to require very long-ranged reasoning. S4 is theoretically
principled and inspired by continuous-time linear SSMs; well-established components of modern control
systems. More importantly, the S4 layer and its variants (DSS, S4D, S5, etc) (Gu et al., 2022a; Gupta et al.,
2022a; Smith et al., 2022) overcome the 𝑂¹𝐿2ºbottleneck of attention layers by modeling interactions between
Corresponding author(s): antonio.orvieto@inf.ethz.ch, sohamde@deepmind.com
2023 DeepMind. All rights reservedarXiv:2303.06349v1  [cs.LG]  11 Mar 2023Resurrecting Recurrent Neural Networks for Long Sequences
T anh-RNNLin-RNNDiagStableNo%m50%60%70%80%90%sCIFARListOpsPathFinde%PathX
7Dnh-5NNLLn-5NNDLDg6tDbleNRrm50%60%70%80%90%
 + Performance on Text/Retrieval always
aligned with S4/5
 densetanhlinear denselinear diag.+ stable + ring init.+ norm
/uni03B3 eﬃciency boostTest accuracy on LRA tasks
(LRU)Deep RNNs (the S4 way)
Lin. Encoder (same for all timestamps)Linear Recurrent Unit (LRU)
x number of layers
MLP/GLU(same for all timestamps)LRULinearRecurrentUnittime pool
# classesLinear Layer
Pre-LN/BNskip connection
xk= diag()xk1+Buk
<latexit sha1_base64=lq8dCfMSVIgBgFuyopdT2ZmlIOc=>AAACRXicbVBNaxsxENWmaZu6X2577EXEFFJKzW4JND0EQnLpMYU6CVhmmdVqHWFptUizwUboz+XSe2/9B73k0BJyTWTHgTTpgODx5s2b0SsaJR2m6a9k5cHqw0eP1550nj57/uJl99XrA2day8WAG2XsUQFOKFmLAUpU4qixAnShxGEx2Zv3D0+EddLU33HWiJGGcS0ryQEjlXfZNJ/QbcpQTNGXEsZhw7OFrY9KUSMEpqJdCeE9neZ+8jEL9AO90RSqFYEVlWdj0BpCoMyUBukubaNv3u2l/XRR9D7IlqBHlrWfd3+y0vBWx71cgXPDLG1w5MGi5EqEDmudaIBP4mXDCGvQwo384pRA30WmpJWx8dVIF+ztCQ/auZkuolIDHru7vTn5v96wxWpr5GXdtChqfr2oahVFQ+eR0lJawVHNIgBuZbyV8mOwwDEG34khZHe/fB8cfOpnm/0v3zZ7O7vLONbIW7JONkhGPpMd8pXskwHh5JT8Jn/I3+RHcpacJxfX0pVkOfOG/FPJ5RWDC7LW</latexit>
/uni03BBj=exp(exp(/uni03BDlogj)+iexp(/uni03B8logj))/uni03B3j/uni2190(1/uni007C/uni03BBj/uni007C2)1/2magnitudephase
NormalizationStable exponential parametrization/uni03BB/uni03B3Linear Recurrent Unit (LRU)
Recurrent Block Variantsj=e x p (exp(logj)+ij)
<latexit sha1_base64=rZa9gtg8yQCXmWFb6RTYwjAt8cM=>AAACFXicbVBNSwMxFMz6bf1a9eglWIQWteyKoB4E0YtHBWsL3bJk09c2NZtdkrdiKf4JL/4VLx4U8Sp489+Y1h60dSBhmJlH8iZKpTDoeV/OxOTU9Mzs3HxuYXFpecVdXbs2SaY5lHkiE12NmAEpFJRRoIRqqoHFkYRKdHPW9yu3oI1I1BV2U6jHrKVEU3CGVgrdnUDacIOFHXpMA7hLC7uDO5BJK1BZ2CluiwDbgDZRDN28V/IGoOPEH5I8GeIidD+DRsKzGBRyyYyp+V6K9R7TKLiE+1yQGUgZv2EtqFmqWAym3htsdU+3rNKgzUTbo5AO1N8TPRYb040jm4wZts2o1xf/82oZNg/rPaHSDEHxn4eamaSY0H5FtCE0cJRdSxjXwv6V8jbTjKMtMmdL8EdXHifXeyV/v3R0uZ8/OR3WMUc2yCYpEJ8ckBNyTi5ImXDyQJ7IC3l1Hp1n5815/4lOOMOZdfIHzsc3262eCA==</latexit>
Figure 1j(Left)Deep Linear Recurrent Unit (LRU) architecture introduced in this paper, inspired by S4 (Gu et al.,
2021a). The model is a stack of LRU blocks, with nonlinear projections in between, and also uses skip connections
andnormalizationmethodslikebatch/layernormalization. WeexpandonthedetailsinDandprovidepseudocode
in A. We also use the same architecture structure (Norm-Recurrence-GLU-Skip) for every variant of the recurrent
module in our study ( tanhdense, linear dense, etc..). (Right)Summary of eﬀects for the main steps outlined in the
introduction towards designing LRUs starting from tanhRNNs. Shown is the average performance (3 seeds) of the
recurrent module at each step on the Long Range Arena (LRA), compared to average performance of deep SSMs.
For all LRA tasks, we match the performance of deep SSMs like S4/S4D/S5 with LRUs. Detailed results in 3.
tokens using a hidden state (like RNNs) under proper discretization techniques. These models can be made
very eﬃcient at inference time by simply unrolling the layer like an RNN. Futhermore, since SSMs are linear in
the temporal dimension, they are easily parallelizable during training, in contrast to the slow sequential nature
of training a typical RNN. This makes them very computationally eﬃcient on long sequences.
While the S4 model is equivalent to an RNN during inference, it has a number of unique characteristics during
training. For example, S4 is parameterized as a discretization of a latent continuous-time system of diﬀerential
equations. S4 also uses speciﬁc initializations of the state matrices motivated from the theory of polynomial
projections (Gu et al., 2020). While these characteristics might seem to motivate the impressive performance of
these models, later works (Gu et al., 2022a; Gupta et al., 2022a,b; Smith et al., 2022) have suggested that the
speciﬁc initialization used by S4 is often not crucial for performance, and that the discretization rules which
achieve best performance may deviate from theory (Smith et al., 2022). It is therefore unclear what these
unique characteristics of the deep SSMs are doing mechanistically, and how they can be simpliﬁed.
Motivated by the striking similarities between RNNs and deep SSMs, and in an attempt to better understand
the underlying mechanism driving the performance of these models, we study the power and limitations of
RNNs when used as core components of deep architectures for long-range reasoning. Our main goal is to
answer the question:
Can we match the performance and eﬃciency of deep continuous-time SSMs using deep RNNs? 
We give a positive answer to this question. We show that the performance boost provided by deep SSMs like S4
can also be achieved via a series of small changes to a vanilla deep RNN. With these changes, we can recover
the performance and eﬃciency of these deep SSMs on the Long Range Arena (LRA) benchmark (Tay et al.,
2020). We call this new RNN model the Linear Recurrent Unit (or LRU for short).
Main Steps. We outline here the main steps needed towards crafting performant and eﬃcient RNN models.
Note while some of these observations have been made in prior works (see B), we provide novel perspectives
and careful ablations leading to new insights. Each step presented in this paper unveils a speciﬁc property of
2Resurrecting Recurrent Neural Networks for Long Sequences
recurrent networks, and showcases the challenges and best practices in training and initializing deep RNNs.
Linear Recurrences. When replacing SSM layers in a deep architecture with vanilla RNN layers using tanh
or ReLU activations, the performance on Long Range Arena (LRA) drops signiﬁcantly. Surprisingly, in 3.1
we ﬁnd that simply removing the nonlinearities in the recurrence of the RNN (i.e., using linear recurrences)
gives a substantial boost in test accuracy. We motivate this eﬀect in E.1 by showing that stacking linear
RNN layers and nonlinear MLP blocks (Fig.1) can indeed model complex nonlinear sequence-to-sequence
maps without the need for nonlinearities in the recurrence. While dropping the nonlinearity does not seem
to harm expressivity, it leads to several advantages, from the ability to directly control how quickly the
gradients might vanish or explode, to allowing us to parallelize training. Our ﬁndings also partially motivate
the success of deep SSMs, where the recurrence is also linear.
Complex Diagonal Recurrent Matrices. Dense linear RNN layers can be reparameterized to a complex
diagonal form without aﬀecting the expressivity of the network or the features at initialization (3.2).
Diagonal linear RNN layers additionally allow for a highly parallelizable unrolling of the recurrence using
parallel scans to substantially improve training speeds (Martin and Cundy, 2017). We validate that these
observations, which have been leveraged by prior SSMs (Gupta et al., 2022a; Smith et al., 2022), also
provide important eﬃciency improvements for linear RNN layers.
Stable Exponential Parameterization. In 3.3 we show that using an exponential parameterization for the
diagonal recurrent matrix has important beneﬁts. Crucially, this enables us to easily enforce stability during
training, which in turn allows us to modify the initialization distribution to facilitate long-range reasoning
and improve performance. Our results indicate that rather than the speciﬁc deterministic initializations
used by several recent SSMs, it is the eigenvalue distribution of the recurrent layer at initialization that
determines if the model can capture long-range reasoning.
Normalization. In 3.4 we show that normalizing the hidden activations on the forward pass is important
when learning tasks with very long-range dependencies. With this ﬁnal modiﬁcation, our RNNs can match
the performance of deep SSMs on all tasks in the LRA benchmark. Connecting back to state-space models,
we show in 4 how our normalization can be linked to the discretization structure in S4.
We summarize the deep Linear Recurrent Unit (LRU) architecture used in this paper, and the eﬀect of each of
the above steps on performance in Fig.1. We emphasize that the main purpose of our work is not to surpass
the performance of S4-based models, but rather to demonstrate that simple RNNs can also achieve strong
performance on long range reasoning tasks when properly initialized and parameterized. We believe the
insights derived in this paper can be useful to design future architectures, and to simplify existing ones.
2. Preliminaries
In this section, we compare the key architectural components (RNNs and SSMs) studied in this work, and also
describe our methodology and experimental setup. For a more thorough discussion or related architectures,
the reader can check our related work section B.
2.1. Recap of recurrent block structures
We give an overview of the main architectural components considered in this paper, focusing on the major
diﬀerence between Vanilla RNNs and recent S4-like deep SSMs (Gu et al., 2021a, 2022a; Gupta et al., 2022a;
Smith et al., 2022).
RNN Layer. Let¹𝑢1𝑢2𝑢𝐿ºbe a sequence of 𝐻in-dimensional inputs, which can be thought of as either
the result of intermediate layer computations (which keep the sequential structure) or as the initial input. An
RNN layer with 𝑁-dimensional hidden state computes a sequence of 𝐻out-dimensional outputs ¹𝑦1𝑦2𝑦𝐿º
through a recurrent computation1using learnable parameters 𝐴2ℝ𝑁𝑁𝐵2ℝ𝑁𝐻in𝐶2ℝ𝐻out𝑁𝐷2ℝ𝐻out𝐻in:
𝑥𝑘=𝜎¹𝐴𝑥𝑘1𝐵𝑢𝑘º 𝑦𝑘=𝐶𝑥𝑘𝐷𝑢𝑘 (1)
1WedonotusebiasparametersastheycanbeincorporatedintotheMLPblocksprecedingandfollowingtheRNNblock. ClassicalRNNs
also included a nonlinearity on the output 𝑦𝑘=𝜎out¹𝐶𝑥𝑘𝑏ºwith𝐷=0. Having𝐷0basically introduces a skip connection (standard in
modern architectures), and the 𝜎outcan be thought of as part of the MLP following the RNN.
3Resurrecting Recurrent Neural Networks for Long Sequences
starting from 𝑥0=02ℝ𝑁.𝜎here denotes a nonlinearity, often chosen to be a tanhor sigmoid activation. If 𝜎
is the identity function, then we say the RNN layer is linear.
S4-likerecurrentlayer. Wepresentasimpliﬁed2versionoftheS4recurrenceintroducedinGuetal.(2021a).
Theinput¹𝑢0𝑢1𝑢𝐿1ºisnowseenastheresultofsamplingalatentcontinuous-timesignal 𝑢ct:ℝ0!ℝ𝐻in
at multiples of a stepsize Δ0: i.e.𝑢ct¹Δ𝑘º:=𝑢𝑘for all𝑘20𝐿1. The output sequence ¹𝑦0𝑦1𝑦𝐿1º
isthensampled,againwithstepsize Δ,fromthesignal 𝑦ct:ℝ0!ℝ𝐻outcomputedbythefollowingcontinuous-
time state-space model, initialized at 𝑥ct¹0º=0:
𝑑
𝑑𝑡𝑥ct¹𝑡º=𝐴𝑥ct¹𝑡º𝐵𝑢ct¹𝑡º
𝑦ct¹𝑡º=<𝐶𝑥ct¹𝑡º
𝐷𝑢ct¹𝑡º (2)
where<¹𝑝ºdenotestherealpartofacomplex-valuedvector 𝑝,𝐴=diag¹𝑎ºwith 𝑎2ℂ𝑁,𝐵2ℂ𝑁𝐻in𝐶2ℂ𝐻out𝑁
and𝐷2ℝ𝐻out𝐻in. Ignoring the continuous-time nature of this model, the most striking diﬀerences compared
to Eq.(1)are that (a) the computation on the right-hand-side is linearin the hidden state and in the input,
and (b) most parameters are complexvalued, with 𝐴being diagonal. While 𝐵𝐶𝐷follow complex random
or uniform initialization, the transition matrix 𝐴isstructured , i.e., initialized deterministically through HiPPO
theory (Gu et al., 2020) in diagonal form. Common choices (Gu et al., 2022a) are 𝑎𝑛=1
2𝑖𝜋𝑛(S4D-Lin) and
𝑎𝑛=1
2𝑖𝑁
𝜋𝑁
𝑛11(S4D-Inv), for 𝑛=12𝑁.
For training and inference, the continuous-time system in Eq. (2)is discretized at stepsize Δthrough a high-
accuracy Zero-Order-Hold (ZOH) or Bilinear method. The ZOH method gives
𝑥𝑘=𝐴𝑥𝑘1𝐵𝑢𝑘 𝑦𝑘=𝐶𝑥𝑘𝐷𝑢𝑘 (3)
where𝑥1=0,𝐴=exp¹Δ𝐴º,𝐵=¹𝐴𝐼º𝐴1𝐵,𝐶=𝐶and𝐷=𝐷, and expdenotes the matrix exponential. Under
theassumptionthat 𝑢ctisconstantinbetweentimestamps(whichcanbethoughtofasamodelingassumption),
this numerical integration is exact(Jacquot, 2019). Moreover, note that all these discretization operations can
be quickly performed element-wise since 𝐴is diagonal.
Some key diﬀerences. It is worth pointing out a few structural and computational properties, to highlight
some crucial diﬀerences between RNNs and SSMs:
Since Eq. (3)is linear, it can be eﬃciently parallelized until 𝑘=𝐿1using parallel scans (Martin and Cundy,
2017; Smith et al., 2022), unlike a nonlinear RNN where the computation has to be performed sequentially.
WhileEq. (3)issimilartothelinearRNNcomputation,itiscrucialtonotethat(a) 𝐴and𝐵areparameterized
in a peculiar way, prescribed by discretization, and (b) these matrices share parameters; in particular Δ
aﬀects both 𝐴and𝐵. These diﬀerences are critical as in SSMs learning is performed on the continuous-time
parameters 𝐴𝐵𝐶𝐷Δ; hence parameterization choices directly aﬀect optimization.
Unlike vanilla RNNs, most SSMs use complex-valued diagonal recurrent matrices that are initialized deter-
ministically using HiPPO theory, and the literature attributes much of the success of SSMs to the speciﬁc
initialized used (Gu et al., 2021a, 2022b; Gupta et al., 2022a).
The points above motivate our investigation: in this paper we consider the same architecture as Gu et al.
(2021a, 2022a); Smith et al. (2022), but replace the SSM layer in the recurrent core by an RNN. We then study
which steps need to be taken to gradually retrieve S4-like performance on LRA (Tay et al., 2020) tasks. The
eﬀectiveness of each of our steps is supported by empirical evidence and theoretical considerations, and leads
to the architecture presented in Fig.1.
2.2. Experimental setup
In this paper, we consider the Long Range Arena benchmark (Tay et al., 2020), a set of tasks designed to test
the ability of models to do long-range sequence modelling (except we use coloured images instead of grayscale
imagesforthesequentialCIFAR-10classiﬁcationtask). Transformersfailtoperformwellonmostofthesetasks,
2This version is most similar to S5 (Smith et al., 2022), but is here presented for ease of reasoning for a single discretization parameter
Δ, shared across input dimensions. For more details, see B.
4Resurrecting Recurrent Neural Networks for Long Sequences
while deep SSMs have shown remarkable performance on these tasks (Dao et al., 2022a; Gu et al., 2021a).
This makes it an appropriate benchmark to explore the long-range modelling capabilities of deep RNNs.
For all our experiments, we use a network of 6 layers with residual connections and layer/batch normaliza-
tion (Ba et al., 2016; Ioﬀe and Szegedy, 2015) similar to Gu et al. (2021a) (Fig.1), and we replace the SSM
layers with RNN layers, building up to our LRU recurrence in a sequence of steps (see 3). All experiments are
repeated three times, and we report the mean and standard error. Networks are trained using the AdamW
optimizer (Loshchilov and Hutter, 2017). We use a smaller learning rate and no weight decay on the recurrent
parameters, as suggested by Gu et al. (2021a); Steil (2004). We tune hyperparameters such as learning rates
for all models on a logarithmic grid for best accuracy. See D for more details on our experimental setup.
3. Designing Performant Deep RNNs
Inthissection,wediscussthefundamentalstepsneededfordesigningRNNstoreachtheimpressiveperformance
of deep SSMs on the LRA benchmark. We present these steps, already outlined in the introduction, in logical
order, and support each claim with experimental evidence and theoretical considerations, expanded in E.
We consider the architecture of Fig.1, where the recurrent computation is gradually modiﬁed starting from a
vanilla RNN. We start by showcasing the advantage of using linear recurrences in 3.1; then, in 3.2, we show
how to speed-up training and inference without aﬀecting expressivity and initialization distribution. In 3.3,
we discuss how (and why) changing the parameterization and initialization distribution enables us to make the
RNN stable and improve long-range modeling. Finally, in 3.4, we ﬁnalize the LRU architecture by proposing a
normalization strategy for the hidden activations that results in a close match in performance with deep SSMs.
3.1. Linear RNN layers are performant
One of the main ﬁndings of our work is that linear RNN layers can be surprisingly expressive when coupled
with nonlinear MLP or GLU (Dauphin et al., 2017) blocks, outperforming tuned nonlinear RNN variants in
the same architecture. In Tb.1, we show that simply removing3the nonlinearity, and therefore computing the
next state as 𝑥𝑘=𝐴𝑥𝑘1𝐵𝑢𝑘, is able to improve test accuracy on most LRA tasks. While the boost provided by
vanilla linear RNN blocks leads to performance which is still far behind S4 on some tasks (sCIFAR, PathFinder
and PathX), this ﬁrst ﬁnding motivates us to drop nonlinearities in the recurrence for the rest of this paper. In
later sections, we leverage the linearity of the recurrence to signiﬁcantly speed up training as well as derive
principled initialization and normalization principles to learn long-range dependencies. We note that, on the
Text and Retrieval tasks, performance using vanilla RNNs already matches performance of deep SSMs (see
Tb.3 for the performance of S4D/S5 on these tasks).
R/e.sc/c.sc/u.sc/r.sc/r.sc/e.sc/n.sc/c.sc/e.sc /s.scCIFAR L/i.sc/s.sc/t.scO/p.sc/s.sc T/e.sc/x.sc/t.sc R/e.sc/t.sc/r.sc/i.sc/e.sc/v.sc/a.sc/l.sc
RNN-R/e.scLU 69.7 (0.2) 37.6 (8.0) 88.0 (0.1) 88.5 (0.1)
RNN-T/a.sc/n.sc/h.sc 69.9 (0.3) 43.9 (0.1) 87.2 (0.1) 88.9 (0.2)
RNN-L/i.sc/n.sc 72.2 (0.2) 50.4 (0.2) 89.1 (0.1) 89.1 (0.1)
Table 1jThe eﬀect of removing the nonlinearity from the recurrent unit on test accuracy (3.1). We show here
resultsonlyforthesCIFAR,ListOps, Textand RetrievaltasksinLRA asthesemodelsdidnotexceedrandomguessing
on PathFinder/PathX (further improvements in Tb.2 and 3). Performance of deep SSMs shown in Tb.3.
The empirical result in Tb.1 is surprising , since recurrent nonlinearities are believed to be a key component
for the success of RNNs  both in the theory and in practice (Erichson et al., 2021; Pascanu et al., 2013;
Siegelmann, 2012). Indeed, a strong property of single-layer sigmoidal and tanhRNNs is Turing completeness,
which cannot be achieved by the linear variant (Chung and Siegelmann, 2021). However, the architecture we
use (Fig.1) is deeper than a standard RNN and includes nonlinearies, placed position-wise aftereach RNN
block. In E.1, we investigate how the expressivity and trainability of deep models is aﬀected by recurrent
3All other settings in the recurrent block are kept the same as in the Vanilla RNN module of Haiku (Hennigan et al., 2020). That is,
all matrices have Glorot (Glorot and Bengio, 2010) initialization. The rest of the architecture is kept as in Fig.1, where the LRU block is
replaced by an RNN.
5Resurrecting Recurrent Neural Networks for Long Sequences
nonlinearities. Leveraging a spectral analysis and Koopman operator theory (Koopman and Neumann, 1932),
we discuss how interleaving linear RNN layers with nonlinear feedforward blocks is suﬃcient to approximate
highlynonlinearsystems. Akeyobservationinouranalysisisthatposition-wisenonlinearitieseﬀectivelytransfer
signal information to higher frequencies, enabling the system to go beyond linearity in the spectral domain and
increasing the layer capacity. To further strengthen our claim on the advantage of linear recurrences, in E.2
we show that, while linear and nonlinear RNNs share an important class of approximating functionals (linear
operators, see Wang et al. (2022)), nonlinear activations can potentially slow down training.
3.2. Using complex diagonal recurrent matrices is eﬃcient
We now show that we can signiﬁcantly speed up training and inference for deep linear RNNs without losing
performance by using complex-valued diagonal recurrent matrices. While the idea of diagonalizing linear
systems for computational eﬃciency is a dominating feature of all deep SSMs since the introduction of DSS
by Gupta et al. (2022a), in this section we construct our diagonalized version to exactly match the initialization
spectrum (see 3.2.1) of the Glorot-initialized deep linear RNN in Tb.1. Our main purpose with this approach
is todisentangle the eﬀects of initialization and diagonalization on performance (cf. Tb.2 and Tb.3).
Westartin3.2.1byrecallingsomeusefullinearalgebraelements,andthenproceedin3.2.2withadiscussion
on how to diagonalize the recurrence while preserving the eigenvalue spectrum at initialization.
3.2.1. Linear RNN eigendecomposition
The recurrence 𝑥𝑘=𝐴𝑥𝑘1𝐵𝑢𝑘can be unrolled easily using the assumption that 𝑥1=02ℝ𝑁:
𝑥0=𝐵𝑢0 𝑥 1=𝐴𝐵𝑢 0𝐵𝑢1 𝑥 2=𝐴2𝐵𝑢0𝐴𝐵𝑢 1𝐵𝑢2  =)𝑥𝑘=𝑘1
𝑗=0𝐴𝑗𝐵𝑢𝑘𝑗(4)
Exponentiations of the matrix 𝐴in the equation above are the source of the well-known vanishing/exploding
gradientissueinRNNs(Bengioetal.,1994;Pascanuetal.,2013). WhileinnonlinearRNNsthestate 𝑥𝑘isforced
to live on the compact image of the activation function, the hidden-state of our linear variant can potentially
explode or vanish exponentially as 𝑘increases. This phenomenon can be better understood by leveraging
an eigenvalue (a.k.a. spectral) analysis: up to an arbitrarily small perturbation of the entries, every matrix
𝐴2ℝ𝑁𝑁is diagonalizable4(Axler, 1997), i.e. one can write 𝐴=𝑃Λ𝑃1, where𝑃2ℂ𝑁𝑁is an invertible
matrix and Λ=diag¹𝜆1𝜆2𝜆𝑁º2ℂ𝑁𝑁. It is essential to note that, unlike the symmetric setting where
eigenvaluesandeigenvectorsarereal,inthenon-symmetriccase5onehastoallowfor complexentriestoachieve
full equivalence. Plugging the decomposition 𝐴=𝑃Λ𝑃1into Eq.(4)and multiplying both sides by 𝑃1, we get
𝑥𝑘=Í𝑘1
𝑗=0Λ𝑗𝐵𝑢𝑘𝑗, where 𝑥𝑘:=𝑃1𝑥𝑘,𝐵:=𝑃1𝐵. The output can then be computed as 𝑦𝑘=<𝐶𝑥𝑘¼𝐷𝑢𝑘2ℝ𝐻,
where 𝐶=𝐶𝑃1,andwetaketherealpartof 𝐶𝑥𝑘. Therefore,insteadoflearning ¹𝐴𝐵𝐶𝐷º,onecanequivalently
learn¹Λ𝐵𝐶𝐷º, where Λ𝐵𝐶are complex valued, and Λis a diagonal matrix.
Are complex numbers really necessary? We adopt complex numbers since they provide a convenient and
compact representation of non-symmetric matrices in diagonal form. However this is not the only option  one
could work (almost) as eﬃciently using real numbers. We discuss how this can be achieved in E.3.
Stability. Since 𝑥𝑘=Í𝑘1
𝑗=0Λ𝑗𝐵𝑢𝑘𝑗, the norm of component 𝑗of𝑥at timestamp 𝑘evolves such that j𝑥𝑘𝑗j=
𝑂¹j𝑥𝑘𝑗jº=𝑂¹j𝜆𝑗j𝑘º. Therefore, a suﬃcient condition to ensure stability (i.e. 𝑥𝑘does not explode) is therefore
j𝜆𝑗j1for all𝑗(Gu et al., 2021a).
3.2.2. Learning in the diagonalized space
Learning recurrent linear systems in diagonal form provides substantial computational speedups both for
training and inference. For example, in our implementation of sCIFAR, we found diagonal linear RNNs to be
8 times faster to train than a dense RNN with ReLUs, matching the speed of our implementations of S4D
and S5. The main reasons for this computational beneﬁt are that (a) taking powers of diagonal matrices is
4In other words, the set of non-diagonalizable matrices has measure zero, see e.g. Zhinan (2002) for a proof idea.
5Take e.g.𝐴=¹¹01º¹10ºº. The solution to the standard eigenvalue equation gives 𝜆=𝑖, where𝑖is the imaginary unit.
6Resurrecting Recurrent Neural Networks for Long Sequences
1.0
 0.5
 0.0 0.5 1.01.0
0.5
0.00.51.0
A is 2020
1.0
 0.5
 0.0 0.5 1.01.0
0.5
0.00.51.0
A is 100100
1.0
 0.5
 0.0 0.5 1.01.0
0.5
0.00.51.0
A is 500500
Figure2jEigenvaluesof 𝐴2ℝ𝑁𝑁followingGlorotinitialization: eachentry
of𝐴is sampled independently from a Gaussian with mean 0 and variance
1𝑁. The eigenvalues are complex ( 𝐴is not symmetric) and are represented
on the complex plane. The black circle is the unit disk fj𝑧j=1gℂ. The
limit behavior (uniform initialization) is predicted by Thm. 3.1.
1.0
 0.5
 0.0 0.5 1.01.0
0.5
0.00.51.0
rmin=0.4,rmax=0.9Figure 3jEigenvalues of a diago-
nalmatrix 𝐴withentriessampled
using Lemma 3.2. For 𝑟min=0,
𝑟max=1, the distribution coin-
cideswithGlorotinit. inthelimit.
trivial (speeding up both training and inference), while exponentiating dense matrices is computationally
expensive, and (b) while nonlinear recurrences must be computed sequentially, unrolling a linear recurrence
can be parallelized using associative scans resulting in faster training (Gupta et al., 2022a; Smith et al., 2022).
Equivalentinitialization. Todisentanglethebeneﬁtsofdiagonallinearsystemsfromtheroleofinitialization,
we seek an initialization for the diagonal system which keeps the eigenvalue spectrum of the recurrence
unchanged when comparing our diagonal system with the dense linear RNN in 3.1, where 𝐴followed Glorot
initialization. Fortunately, we can use a classical result from random matrix theory (Ginibre, 1965).
Theorem 3.1 (Strong circular law) .Let𝜇𝑁be the empirical spectral measure of 𝐴𝑁, where𝐴𝑁is a real𝑁𝑁
matrix with i.i.d. Gaussian entries, each with zero mean and variance 1𝑁. Then,𝜇𝑁converges weakly almost
surely as𝑁!1to the uniform probability measure on fj𝑧j1gℂ.
The theorem above, illustrated in Fig.2, shows that under Glorot initialization the spectrum of 𝐴isde-facto
sampled from the unit disk in ℂ. This result motivates the strong performance of linear RNNs in 3.1, since it
impliesGlorotinitializationprovidesanapproximatelystableinitialization(seedeﬁnitionin3.2.1).6Moreover,
from Theorem 3.1, an equivalent spectral initialization follows for the diagonal system, which holds exactly
for the large width limit: Λshould be diagonal with entries sampled uniformly on the unit disk. Using the
deﬁnitionofexponentialofacomplexnumber: exp¹𝜈𝑖𝜃º:=𝑒𝜈¹cos¹𝜃º𝑖sin¹𝜃ºº, weadoptasimplescheme
for sampling uniformly on a ring in between circles with radii 𝑟minand𝑟maxinℂ.
Lemma 3.2. Let𝑢1𝑢2be independent uniform random variables on the interval 01¼. Let 0𝑟min𝑟max1.
Compute𝜈=1
2log
𝑢1¹𝑟2
max𝑟2
minº𝑟2
min
and𝜃=2𝜋𝑢2. Then exp¹𝜈𝑖𝜃ºis uniformly distributed on the ring
inℂbetween circles of radii 𝑟minand𝑟max.
We recover the spectrum of Glorot-initialization (in the limit of inﬁnite width) by setting 𝑟𝑚𝑖𝑛=0and𝑟𝑚𝑎𝑥=1
(wewillexploretuningthesehyper-parametersin3.3). Tb.2(ﬁrsttworows)showstheresultsoflearningdeep
linear RNNs in complex diagonal form,7where each diagonal entry of Λis initialized uniformly on unit disk in
ℂusing Lemma 3.2 with 𝑟min𝑟max¼=01¼. In our experiments, 𝐵𝐶(which we rename for convenience back
to𝐵and𝐶) follow Glorot initialization for both real and imaginary parts (parameterized separately), with
halved variance in each component to preserve lengths on the input-output projections (Glorot and Bengio,
2010). Finally, after the SSM computation, the real part of the signal is kept and the imaginary discarded (as
in Gu et al. (2022a); Gupta et al. (2022a)).
Our results in Tb.2 show that diagonalizing the recurrence surprisingly improves accuracy on tasks like ListOps
and sCIFAR. More importantly, it drastically reduces training and inference time on all LRA tasks (see Tb.4 in
C.1 for training speed comparisons), and makes the RNN just as fast to train as deep SSMs like S4D and S5.
6Later in training, the system is less likely to become unstable if the learning rate is small enough.
7To avoid issues with backpropagation on complex variables, each complex parameter in the network is stored and learned as a pair of
ﬂoats encoding real and imaginary parts.
7Resurrecting Recurrent Neural Networks for Long Sequences
/s.scCIFAR L/i.sc/s.sc/t.scO/p.sc/s.sc P/a.sc/t.sc/h.sc/f.sc/i.sc/n.sc/d.sc/e.sc/r.sc
D/e.sc/n.sc/s.sc/e.sc𝐴 72.2 (0.2) 50.4 (0.2) %
ΛR/e.sc/a.sc/l.sc + I/m.sc 86.5 (0.1) 58.8 (0.3) %
ΛE/x.sc/p.sc 85.4 (0.7) 60.5 (0.3) 65.4 (9.0)
ΛS/t.sc/a.sc/b.sc/l.sc/e.sc E/x.sc/p.sc 87.2 (0.4) 59.4 (0.3) 93.5 (0.5)
+ R/i.sc/n.sc/g.sc I/n.sc/i.sc/t.sc 88.1 (0.0) 59.4 (0.3) 94.4 (0.3)
Table 2jTest accuracy of a linear diagonal complex RNNs under diﬀerent parametrizations of the transition
matrix (see 3.2). Performance directly improves the results in Tb.1, and showcases the advantage of exponen-
tial (polar) representation of Λ. In bold font is the best parametrization option for linear RNN blocks. Ring Init
denotes a changed initialization where 𝑟minand𝑟maxare tuned. Performance on the Text and Retrieval tasks is not
shown as linear RNNs already align with S4 results (c.f. Tb.1 with Tb.3). These models cannot solve PathX yet, and
requires normalizing the hidden activations and initializing the eigenvalues of Λwith small phase (see Tb.3).
3.3. Beneﬁts of stable exponential parameterization
In 3.2 we showed that moving to complex diagonal recurrences is computationally eﬃcient. However we
also observed that learning the diagonal model can be more unstable than learning the dense model in some
experiments. To learn long-range dependencies and avoid quickly vanishing gradients, eigenvalues in the
recurrenceneedtohavemagnitudecloseto1(Guetal.,2022b;Guptaetal.,2022a);however,theseeigenvalues
are also likely to make the system unstable during training. In this section, we show the beneﬁts of a stable
parameterization of the RNN, and of tuning 𝑟minand𝑟max(see Lemma 3.2).
Optimization under exponential parameterization. Lemma3.2suggestsanaturalparameterizationofthe
diagonalized RNN as Λ=diag¹exp¹𝜈𝑖𝜃ººwith𝜈2ℝ𝑁and𝜃2ℝ𝑁as the learnable parameters (instead
of the real and imaginary parts of Λ). As we explain in E.2 leveraging an easy-to-visualize 2-dimensional
example (see Fig.8), this choice decouples magnitude and oscillation frequencies, making optimization with
Adam easier. The positive eﬀects of this exponential parametrization, which resembles some features of ZOH
discretization (see 2 and 4) and notably takes the performance of PathFinder above random chance, can be
observed in the third row of Tb.2.
Enforcing stability. An important beneﬁt of the exponential parameterization is that it makes it simple to
enforce stability on the eigenvalues. To see this, note that at initialization, j𝜆𝑗j=jexp¹𝜈𝑗ºj1since𝜈𝑗0.
Therefore, to preserve stability during training, we can use an exponential or another positive nonlinearity:
𝜆𝑗:=exp¹exp¹𝜈log
𝑗º𝑖𝜃𝑗º, where𝜈log2ℝ𝑁is the parameter we optimize, and we set 𝜈log
𝑗:=log¹𝜈ºat
initialization. Note that a similar idea is used in deep SSMs (Gu et al., 2021a) in the context of discretization.
We choose an exponential non-linearity over a simple ReLU nonlinearity to increase granularity around j𝜆j=1,
achieved at 𝜈log=1(whilej𝜆j=0is achieved at 𝜈log=1). Stable parameterization helps on most LRA tasks.
In the fourth row of Tb.2, we show its eﬀects on sCIFAR, ListOps and Pathﬁnder. We observe the most drastic
improvement on Pathﬁnder, one of the harder long-range dependency tasks in LRA, where performance now
reaches above 93%.
The beneﬁts of the stable parameterization becomes more apparent when we explore the idea of initializing
the eigenvalues of Λon a ring closer to the unit disk (increasing 𝑟mincloser to 1in Lemma 3.2) to bias the
network towards longer-range interactions and avoid vanishing gradients. Indeed, as discussed in detail in Gu
et al. (2022b); Gupta et al. (2022a), for reasonings requiring consideration of interactions between distant
tokens, eigenvalues in the recurrence need to have magnitude close to 1. Otherwise, as clear from the diagonal
version of Eq. (4), when taking powers of eigenvalues close to the origin, the signal from past tokens quickly
dies out (see 3.2.1). As we show in the last row of Tb.5 in C, without enforcing stability, performance starts
to degrade as we increase 𝑟maxpast 0.9 in the sCIFAR task. With stability enforced, we can increase 𝑟maxup to
0.99 and improve performance. We see similar beneﬁts on the other tasks where we sweep diﬀerent values of
𝑟minand𝑟max(Tbs.7 & 8 have more details). Finally, note that while here we explore changing the magnitude
of the eigenvalues of Λ, in 3.4 we also show the beneﬁts of initializing the eigenvalues to have a small phase
to learn more global patterns, useful for particularly long-range reasoning tasks.
8Resurrecting Recurrent Neural Networks for Long Sequences
/s.scCIFAR L/i.sc/s.sc/t.scO/p.sc/s.sc T/e.sc/x.sc/t.sc R/e.sc/t.sc/r.sc/i.sc/e.sc/v.sc/a.sc/l.sc P/a.sc/t.sc/h.sc/f.sc/i.sc/n.sc/d.sc/e.sc/r.sc P/a.sc/t.sc/h.scX
LRU 89.0 (0.1) 60.2 (0.8) 89.4 (0.1) 89.9 (0.1) 95.1 (0.1) 94.2 (0.4)
S4D (/o.sc/u.sc/r.sc /r.sc/e.sc/p.sc/r.sc/o.sc/d.sc.) 91.5 (0.2) 60.2 (0.3) 86.4 (0.0) 89.5 (0.0) 94.2 (0.3) 97.5 (0.0)
S5 (/o.sc/u.sc/r.sc /r.sc/e.sc/p.sc/r.sc/o.sc/d.sc.) 88.8 (0.1) 58.5 (0.3) 86.2 (0.1) 88.9 (0.0) 95.7 (0.1) 96.0 (0.1)
S4 (/p.sc/a.sc/p.sc/e.sc/r.sc /r.sc/e.sc/s.sc/u.sc/l.sc/t.sc/s.sc) 91.1 59.6 86.8 90.9 94.2 96.4
S4D-L/e.sc/g.scS (/p.sc/a.sc/p.sc/e.sc/r.sc /r.sc/e.sc/s.sc/u.sc/l.sc/t.sc/s.sc) 89.9 60.5 86.2 89.5 93.1 91.9
S5 (/p.sc/a.sc/p.sc/e.sc/r.sc /r.sc/e.sc/s.sc/u.sc/l.sc/t.sc/s.sc) 90.1 62.2 89.3 91.4 95.3 98.6
Table 3jPerformance after adding the 𝛾normalization to the diagonal RNN with stable exponential parameter-
ization and initialization on the ring (see 3.4). For PathX, we additionally use a smaller eigenvalue phase at
initialization. We name this architecture LRU. We sweep 𝑟minand𝑟maxfor setting the initialization distribution
and the learning rate. We also report results from S4/S4D/S5 (along with reproductions in our own pipeline with
similar hyperparameter sweeps as our RNN models). LRU reaches similar performance as these deep SSMs on all
LRA tasks.
3.4. Additional considerations for long-range reasoning tasks
Up to this point, our model did not succeed in learning PathX  the hardest dataset in our benchmark, with a
sequence length of 16𝑘tokens. In this section, we discuss the additional modiﬁcations we need to make to
improve our models ability to learn very long-range dependencies and ﬁnalize our LRU model.
Normalization. In 3.3, we initialized the eigenvalues of Λclose to the unit disk for better performance on
long-rangetasks. However,weobservedthataswemoved 𝑟minand𝑟maxcloserto1,thetraininglossalsostarted
to blow up at initialization (see Fig.5). In this section, we ﬁrst present a result explaining this phenomenon,
before deriving a practical normalization scheme for the hidden activations to tackle this problem and further
improve performance.
Proposition 3.3 (Forward-pass blow-up) .LetΛbe diagonal with eigenvalues sampled uniformly on the ring in
ℂbetween circles of radii 𝑟min 𝑟max1. Then, under constant or white-noise input and Glorot input projection,
we have that the squared norm of the state 𝑥𝑘converges as 𝑘!1to the following quantity.
𝔼k𝑥1k2
2¼=1
𝑟2max𝑟2
minlog 
1𝑟2
min
1𝑟2max!
𝔼k𝐵𝑢k2
2¼
This result has the following intuitive form if 𝑟min=𝑟max=𝑟: if we initialize 𝜌-close to the unit disk, the
forward pass blows up by a factor 1𝜌(since the contributions from previous states take longer to decay): let
𝜖=𝑟2
max𝑟2
minand𝜌=1𝑟2
max, then:
lim
𝜖!0𝔼k𝑥1k2
2¼
𝔼k𝐵𝑢k2
2¼=lim
𝜖!01
𝜖log
1𝜖
𝜌
=lim
𝜖!01
𝜖𝜖
𝜌𝑂¹𝜖2º
=1
𝜌=1
1𝑟2 (5)
Towards the derivation of an eﬀective normalization scheme for the forward pass, we present a simpliﬁed
derivation of the 1𝜌gain formula for the one-dimensional setting under white-noise input8: letΛ=𝜆2ℂ,
and𝐵=1. Let𝑝denote the conjugate of 𝑝2ℂ, we have thatj𝑝j2=𝑝𝑝and in expectation over the input,
using Eq.(4) and the fact that 𝔼𝑢𝑘𝑖𝑢𝑘𝑗¼=0for𝑖𝑗:
𝔼j𝑥𝑘j2= 𝑘1
𝑖=0𝜆𝑖𝔼𝑢𝑘𝑖¼!

𝑘1
𝑗=0𝜆𝑗𝔼𝑢𝑘𝑗¼ª

=𝑘1
𝑖𝑗=0𝜆𝑖¹𝜆𝑗º𝔼𝑢𝑘𝑖𝑢𝑘𝑗¼=𝑘1
𝑖=0j𝜆j2𝑖1!1
1j𝜆j2 (6)
Since the formula above holds for every Euclidean direction in our recurrence ( Λis diagonal), we can add a
normalization parameter that is initialized element-wise. Additionally, note that as 𝜆approaches 1, 1j𝜆j2
8We use the random input assumption for our normalization scheme as we found it to work well in practice.
9Resurrecting Recurrent Neural Networks for Long Sequences
PathFinderPathX
Figure 4jEvolution of 𝑥2ℝ3under impulse input 𝑢=¹100 0º2ℝ16𝑘. Plotted in diﬀerent colors are the
3 components of 𝑥.Λhas parameters 𝜈𝑗=000005and𝜃𝑗sampled uniformly in 02𝜋¼or with small phase
0𝜋50¼. For small sequences, such as 𝐿=1024(PathFinder, sCIFAR), 02𝜋¼produces kernels with acceptable
overall number of oscillations: information about 𝑢0is recalled only a few times in the overall state history. Instead,
for high𝐿, the range of the imaginary part at initialization has to be smaller to obtain a similar eﬀect.
0 20 40 60 80 100
Training Iterations (x1000)22
21
2021Training LossNo Normalization
+  Normalization
+ Small Phase Init
0 20 40 60 80 100
Training Iterations (x1000)5060708090Training AccuracyNo Normalization
+  Normalization
+ Small Phase Init
0 20 40 60 80 100
Training Iterations (x1000)5060708090T est AccuracyNo Normalization
+  Normalization
+ Small Phase Init
Figure 5jEﬀect of normalization and using a small phase at initialization on the PathX task. For each setting,
we show mean and standard errors over three independent runs for 100k iterations. Without normalization, the
model presents higher loss values at initialization and quickly converges to a suboptimal value, where train and test
accuracy are both at random chance. Adding normalization helps: the train loss is lower at initialization, and the
optimizer is able to escape the suboptimal region and train accuracy also increases. Interestingly, this model still
fails to generalize at all. Finally, reducing initialization phase (i.e. tuning the range of 𝜃) dramatically improves
convergence on the training set, while also generalizing to the test set.
approaches 0, making further adaptations with SGD of this parameter hard. Therefore, we use normalization
parameter 𝛾log2ℝ𝑁, initialized element-wise as 𝛾log
𝑖 log¹
1j𝜆𝑖j2º,9and modify the recurrence as:
𝑥𝑘=Λ𝑥𝑘1exp¹𝛾logº¹𝐵𝑢𝑘º (7)
wheredenotes the element-wise product. The 𝛾parameter allows the RNN to adaptively scale the input fed
into the corresponding eigendirection. We found the 𝛾normalization to consistently improve performance on
tasks that beneﬁt from initializing close to the unit disk, such as sCIFAR and Pathﬁnder, as shown in Tb.3.
Reducing Eigenvalue Phase at Initialization. In the context of the diagonalized recurrence, we have Λ=
diag¹exp¹exp¹𝜈logº𝜃ºº,where𝜈log2ℝ𝑁isthevectoroflogeigenvaluemagnitudesand 𝜃2ℝ𝑁thevectorof
eigenvalue phases. While𝜈logencodes the distance to the origin, 𝜃is the angle from the vector 10𝑖.For long
sequences , initializing uniformly 𝜃02𝜋¼implies that most state entries will exhibit an overall large number
of oscillations at initialization, see upper panel in Fig.4. Equivalently, in this setting, most state dimensions
are the result of convolutions10capturing an average of local oscillation patterns . This behavior is independent
from the ability of capturing long-range dependencies (controlled by 𝜈log), but pertains to the nature of the
information stored by the RNN. Therefore, we claim that initializing Λwith uniform phase on long sequence
data inherently biases the network towards learning spurious features in the input sequence. The model cannot
recover from this suboptimal initialization: we indeed observe that, for our best to far model on PathX, the
9We also tried setting 𝛾𝑖to
1j𝜆𝑖j2in each training iteration, and found it to work similarly in practice to a trainable 𝛾.
10See (Gu et al., 2022a) for a discussion of kernel perspectives.
10Resurrecting Recurrent Neural Networks for Long Sequences
training loss after a few iterations converges to a highly suboptimal minimizer which leads to random chance
test performance (see Fig.5). To ﬁx this issue, we found it suﬃcient to restrict the range of 𝜃to a thin slice
around 0, biasing the model towards learning more global features. Since the optimal values of 𝜃are small, we
parameterize the phase logarithmically: 𝜃=exp¹𝜃logº, where𝜃logis optimized, to aid optimization.
Restricting the range of the phase at initialization to be 0𝜋10¼, our LRU achieved 942%on PathX, aligning
with state-of-the-art deep SSMs. We did not explore using a smaller phase at initialization for the other LRA
tasks, although we believe this might further improve performance on other tasks as well. Note that using both
𝛾normalization and restricting the eigenvalue phase at initialization were crucial to solving PathX. We were
unable to learn when using restricted phase at initialization without also introducing 𝛾normalization.
With all the components of 3 taken together, we name this new model the Linear Recurrent Unit (orLRUfor
short). It provides a ﬂexible, interpretable, and principled framework for initializing and learning deep RNNs
eﬃciently, and matches performance and eﬃciency of deep SSMs across all LRA tasks as shown in Tb.3.
4. Insights on S4 and Variants
We believe our ablations in 3 explain the underlying mechanisms driving the success of deep SSMs. Hence,
to conclude the paper, in this section, we inspect in detail the main similarities and diﬀerences between our
LRU model and diagonal SSMs, and elaborate a few insights. As in 2, to avoid technicalities, we provide
a simpliﬁed discussion capturing the main features of models stemming from the original S4 paper. For a
comparison of diﬀerent models, we defer the reader to B.
As detailed in 2, diagonal SSMs (DSS, S4D, S5) are instantiated and parameterized through discretization
of a latent continuous-time model 𝑥ct¹𝑡º=𝐴𝑥ct¹𝑡º𝐵𝑢ct¹𝑡º, where𝐴=diag¹𝑎ºis initialized with complex
entries, often prescribed or inspired by HiPPO theory (Gu et al., 2020). Zero-Order-Hold (ZOH) discretization
with stepsize Δleads to the recurrence 𝑥𝑘=exp¹Δ𝐴º𝑥𝑘1¹exp¹Δ𝐴º𝐼º𝐴1𝐵𝑢𝑘. This formula, while arguably
complex compared to our Eq.(7), relates to it as outlined in the next paragraphs.
Matrix exponentials make training easier. The exponential in the ZOH formula is due to exact integration
of𝑥ct¹𝑡º=𝐴𝑥ct¹𝑡º, which leads to 𝑥ct¹Δ𝑘º=exp¹Δ𝐴º𝑥ct¹Δ¹𝑘1ºº. In addition, to enforce stability, in models
inspired by S4 the real part of 𝐴is often fed into a positive nonlinearity, as we also do in 3.3. From our results
3.3 and our discussion on optimization advantages (see also E.2), we claim that the power of exponential
parameterization is not necessarily attributable to accurate integration (which is not present in our system),
but is more fundamentally rooted in a magnitude-phase decoupling on the recurrence (this makes training
with Adam easier, see Fig.8), as well as in the overall advantage of learning in diagonalized space (see Tb.2).
We also note that stabilizing the recurrence by adding a nonlinearity was beneﬁcial also in our experiments,
although this is not prescribed by the theory underlying S4.
Structured initialization is not necessary. While Gu et al. (2022a); Gupta et al. (2022b); Smith et al.
(2022) also discuss initializations for 𝐴deviating from the HiPPO structure (see 2 and B), to the best of
our knowledge we are the ﬁrst to show that simple uniform initialization on a slice of the unit disk, combined
with proper normalization, is able to also solve the hardest task in LRA: PathX.11We also show (Tb.2) that
uniform initialization on the disk, which is simply the diagonalized version of Glorot initialization (Thm. 3.1),
is suﬃcient to achieve performance close to more complex deep state-space models on the remaining LRA
tasks. Our results ultimately suggest that HiPPO theory, while fundamental for the development of this ﬁeld,
should not be thought of as the main source of S4 success.
Discretization changes initialization spectrum. For simplicity, let us restrict our attention to S4D-Lin, for
which𝐴=diag¹𝑎ºwith 𝑎𝑛=1
2𝑖𝜋𝑛, yielding a diagonal transition matrix with elements (i.e. eigenvalues)
initialized at exp¹Δ2𝑖𝜋Δ𝑛º. Under typical choices e.g. Δ=1𝑒3𝑁=128, the SSM eigenvalues have
magnitude exp¹Δ2º09995, and phase 𝜃=𝜋Δ𝑛20𝜋8¼ i.e. initialization is performed on a ring12
close to the unit circle in ℂ, with restricted phase connected to the eigenvalues magnitude. As is clear from
11Among the models in (Gu et al., 2022a), only S4D-inv and S4D-LegS (options heavily inspired by the HiPPO theory) perform beyond
random guessing on PathX. In S5, the skew-symmetric component of the HiPPO matrix is used for initialization.
12For all diagonal SSMs, Δis actually a vector initialized in the range ΔminΔmax¼. This interval can be directly mapped through the
exponential map to a ring in complex space (see Lemma 3.2).
11Resurrecting Recurrent Neural Networks for Long Sequences
the results in 3.3 and 3.4, linking the eigenvalues phase and magnitude is not necessary to achieve good
performance: indeed, as it can be seen in Tb.3, test accuracy on the Long Range Arena (except PathX) can be
recovered by using a more natural magnitude-independent initialization on the complete ring. As we discussed
in 3.4, changing the initialization phase to a small range around 0can be motivated by ﬁrst principles, yet is
only needed for extremely long sequences: this modiﬁcation is already hard-coded in S4, where choosing a
small Δalso shrinks the phase.13However, our results clearly show that connecting real and imaginary parts
during training through the Δparameter is not necessary to achieve good performance, even on PathX.
Discretizationperformsnormalization. ThemoststrikingvisualdiﬀerencebetweenoursandZOH-discretized
S4 recurrence is in the matrix multiplier for 𝑢𝑘:¹exp¹Δ𝐴º𝐼º𝐴1𝐵. After conducting experiments on S4D, we
found that simply replacing this multiplier with its ﬁrst-order expansion in Δ, i.e.Δ𝐵, yields a close match in
performance. For input dimension 𝐻=1and unit𝐵2ℝ𝑁1(to keep reasoning simple), the corresponding
recurrence is 𝑥𝑘=exp¹Δ𝑎ºΔ1𝑁𝑢𝑘. Elementwise unrolling of this recurrence  without the Δin front of 𝑢
yieldsj𝑥𝑘𝑖jÍ𝑘1
𝑗=0jexp¹Δ𝑎𝑖ºj𝑗𝑢𝑘𝑗𝑖, which in the limit 𝑘!1gives𝑂¹Δ1º. Therefore, the Δmultiplier in front
of𝐵eﬀectively scales the recurrence to avoid blow-ups  similar to our 𝛾normalization factor.
Parameter sharing is not necessary. As a result of discretization, the Δparameter multiplying both 𝐴and𝐵
couples the recurrence formula with the input projection during training. In our S4 ablations, we found that
decoupling these in two separate parameters  keeping the same initialization to guarantee no blow-ups (see
last paragraph)  does not decrease performance, suggesting that the ODE discretization viewpoint (which
induces parameter sharing) is not necessary to achieve S4 performance.
From this discussion, we conclude that the success of (diagonal) state-space models is attributable to the
use of linear recurrences and complex diagonal exponential matrices, combined with the normalization and
initialization induced by discretization. On the other hand, other artifacts of discretization such as parameter
sharing or the continuous-time interpretation do not necessarily contribute to its performance.
5. Conclusion
In this paper, we introduce a new RNN layer called the Linear Recurrent Unit or LRU and show how it can be
eﬀectively and eﬃciently used as core layers of deep sequence models. We provide theoretical insights and
extensive ablations on a series of step-by-step modiﬁcations of a vanilla RNNlinearization, diagonalization,
stable exponential parameterization and normalizationthat substantially improve performance, especially on
tasks requiring long range reasoning. While our recurrence shares similarities with modern deep SSMs, our
design does not rely on discretization of a latent continous-time system or on structured transition matrices.
Instead our improvements directly follow from initialization and forward pass analysis arguments standard in
thedeeplearningcommunity,startingfromaGlorot-initializedRNNs. Ourﬁnalmodelmatchestheperformance
of modern deep state-space models (e.g. S4 or S5) on all LRA tasks.
Acknowledgements
The authors would like to thank Michalis Titsias, Aleksandar Botev, James Martens and Yee Whye Teh for the
interesting discussions and perspectives on our work.
13This is a useful eﬀect of having a latent continuous-time model: choosing eigenvalues close to the unit circle (i.e. small Δ) changes
the oscillation frequencies in the discretized system.
12Resurrecting Recurrent Neural Networks for Long Sequences
References
M. Arjovsky, A. Shah, and Y. Bengio. Unitary evolution recurrent neural networks. In International conference
on machine learning . PMLR, 2016.
S. Axler. Linear algebra done right . Springer Science & Business Media, 1997.
J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016.
S. Bai, J. Z. Kolter, and V. Koltun. An empirical evaluation of generic convolutional and recurrent networks for
sequence modeling. arXiv preprint arXiv:1803.01271 , 2018.
Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is diﬃcult. IEEE
transactions on neural networks , 1994.
N. Bordin, C. Dallago, M. Heinzinger, S. Kim, M. Littmann, C. Rauer, M. Steinegger, B. Rost, and C. Orengo.
Novel machine learning approaches revolutionize protein knowledge. Trends in Biochemical Sciences , 2022.
J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas,
S. Wanderman-Milne, et al. JAX: composable transformations of python+ numpy programs, 2018.
T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, S. Shyam, G. Sastry,
A. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 , 2020.
K. Cho, B. Van Merriënboer, D. Bahdanau, and Y. Bengio. On the properties of neural machine translation:
Encoder-decoder approaches. arXiv preprint arXiv:1409.1259 , 2014a.
K.Cho,B.VanMerriënboer,C.Gulcehre,D.Bahdanau,F.Bougares,H.Schwenk,andY.Bengio. Learningphrase
representationsusingrnnencoder-decoderforstatisticalmachinetranslation. arXivpreprintarXiv:1406.1078 ,
2014b.
S. Chung and H. Siegelmann. Turing completeness of bounded-precision recurrent neural networks. Advances
in Neural Information Processing Systems , 2021.
T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Ré. Flashattention: Fast and memory-eﬃcient exact attention with
io-awareness. arXiv preprint arXiv:2205.14135 , 2022a.
T. Dao, D. Y. Fu, K. K. Saab, A. W. Thomas, A. Rudra, and C. Ré. Hungry hungry hippos: Towards language
modeling with state space models. arXiv preprint arXiv:2212.14052 , 2022b.
Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier. Language modeling with gated convolutional networks. In
International conference on machine learning . PMLR, 2017.
S. De and S. Smith. Batch normalization biases residual blocks towards the identity function in deep networks.
Advances in Neural Information Processing Systems , 2020.
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, N. Houlsby, S. Gelly, X. Zhang, and J. Uszkoreit. An
image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 ,
2020.
J. L. Elman. Finding structure in time. Cognitive science , 1990.
N. B. Erichson, O. Azencot, A. Queiruga, L. Hodgkinson, and M. W. Mahoney. Lipschitz recurrent neural
networks. In International Conference on Learning Representations , 2021.
J. Ginibre. Statistical ensembles of complex, quaternion, and real matrices. Journal of Mathematical Physics ,
1965.
X. Glorot and Y. Bengio. Understanding the diﬃculty of training deep feedforward neural networks. In
Proceedings of the thirteenth international conference on artiﬁcial intelligence and statistics . JMLR Workshop
and Conference Proceedings, 2010.
K. Goel, A. Gu, C. Donahue, and C. Ré. Its raw! audio generation with state-space models. arXiv preprint
arXiv:2202.09729 , 2022.
13Resurrecting Recurrent Neural Networks for Long Sequences
A. Gu, T. Dao, S. Ermon, A. Rudra, and C. Ré. Hippo: Recurrent memory with optimal polynomial projections.
Advances in Neural Information Processing Systems , 2020.
A. Gu, K. Goel, and C. Re. Eﬃciently modeling long sequences with structured state spaces. In International
Conference on Learning Representations , 2021a.
A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, and C. Ré. Combining recurrent, convolutional, and
continuous-time models with linear state space layers. Advances in neural information processing systems ,
2021b.
A. Gu, A. Gupta, K. Goel, and C. Ré. On the parameterization and initialization of diagonal state space models.
arXiv preprint arXiv:2206.11893 , 2022a.
A. Gu, I. Johnson, A. Timalsina, A. Rudra, and C. Ré. How to train your hippo: State space models with
generalized orthogonal basis projections. arXiv preprint arXiv:2206.12037 , 2022b.
A. Gupta, A. Gu, and J. Berant. Diagonal state spaces are as eﬀective as structured state spaces. In Advances in
Neural Information Processing Systems , 2022a.
A. Gupta, H. Mehta, and J. Berant. Simplifying and understanding state space models with diagonal linear
rnns.arXiv preprint arXiv:2212.00768 , 2022b.
R. Hasani, M. Lechner, A. Amini, D. Rus, and R. Grosu. Liquid time-constant networks. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence , 2021.
R. Hasani, M. Lechner, T.-H. Wang, M. Chahine, A. Amini, and D. Rus. Liquid structural state-space models.
arXiv preprint arXiv:2209.12951 , 2022.
K. Helfrich, D. Willmott, and Q. Ye. Orthogonal recurrent neural networks with scaled cayley transform. In
International Conference on Machine Learning . PMLR, 2018.
T. Hennigan, T. Cai, T. Norman, and I. Babuschkin. Haiku: Sonnet for JAX, 2020. URL http://github.com
/deepmind/dm-haiku .
S. Hochreiter. Untersuchungen zu dynamischen neuronales netzen. Diploma thesis, Institut fur Informatik,
Technische Universitat Munchen , 1991.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation , 1997.
J.J.Hopﬁeld.Neuralnetworksandphysicalsystemswithemergentcollectivecomputationalabilities. Proceedings
of the national academy of sciences , 1982.
S. L. Hyland and G. Rätsch. Learning unitary operators with help from u (n). In Thirty-First AAAI Conference
on Artiﬁcial Intelligence , 2017.
S.IoﬀeandC.Szegedy. Batchnormalization: Acceleratingdeepnetworktrainingbyreducinginternalcovariate
shift. InInternational Conference on Machine Learning , 2015.
M. M. Islam and G. Bertasius. Long movie clip classiﬁcation with state-space video models. In ECCV 2022 .
Springer, 2022.
R. G. Jacquot. Modern digital control systems . Routledge, 2019.
H. Jeﬀreys. The theory of probability . OUP Oxford, 1998.
L. Jing, Y. Shen, T. Dubcek, J. Peurifoy, S. Skirlo, Y. LeCun, M. Tegmark, and M. Soljačić. Tunable eﬃcient
unitaryneuralnetworks(eunn)andtheirapplicationtornns. In InternationalConferenceonMachineLearning .
PMLR, 2017.
J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Žídek,
A. Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 2021.
E. Kaiser, J. N. Kutz, and S. L. Brunton. Data-driven discovery of koopman eigenfunctions for control. Machine
Learning: Science and Technology , 2021.
14Resurrecting Recurrent Neural Networks for Long Sequences
N. Kalchbrenner, L. Espeholt, K. Simonyan, A. v. d. Oord, A. Graves, and K. Kavukcuoglu. Neural machine
translation in linear time. arXiv preprint arXiv:1610.10099 , 2016.
J. Kilian and H. T. Siegelmann. The dynamic universality of sigmoidal neural networks. Information and
computation , 1996.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , 2014.
B. O. Koopman and J. v. Neumann. Dynamical systems of continuous spectra. Proceedings of the National
Academy of Sciences , 1932.
M. Korda and I. Mezić. On convergence of extended dynamic mode decomposition to the koopman operator.
Journal of Nonlinear Science , 2018.
M. Korda and I. Mezić. Koopman model predictive control of nonlinear dynamical systems. In The Koopman
Operator in Systems and Control . Springer, 2020.
V.R.Kostic,P.Novelli,A.Maurer,C.Ciliberto,L.Rosasco,andmassimilianopontil. Learningdynamicalsystems
via koopman operator regression in reproducing kernel hilbert spaces. In Advances in Neural Information
Processing Systems , 2022.
J. N. Kutz, S. L. Brunton, B. W. Brunton, and J. L. Proctor. Dynamic mode decomposition: data-driven modeling of
complex systems . SIAM, 2016.
Q. V. Le, N. Jaitly, and G. E. Hinton. A simple way to initialize recurrent networks of rectiﬁed linear units. arXiv
preprint arXiv:1504.00941 , 2015.
J. Lee-Thorp, J. Ainslie, I. Eckstein, and S. Ontanon. Fnet: Mixing tokens with fourier transforms. arXiv preprint
arXiv:2105.03824 , 2021.
M. Lezcano-Casado and D. Martınez-Rubio. Cheap orthogonal constraints in neural networks: A simple
parametrizationoftheorthogonalandunitarygroup. In InternationalConferenceonMachineLearning .PMLR,
2019.
Y.Li,T.Cai,Y.Zhang,D.Chen,andD.Dey. Whatmakesconvolutionalmodelsgreatonlongsequencemodeling?
arXiv preprint arXiv:2210.09298 , 2022a.
Z.Li, J.Han, E.Weinan, andQ.Li. Approximationandoptimizationtheoryforlinearcontinuous-timerecurrent
neural networks. J. Mach. Learn. Res. , 2022b.
L. Liu, H. Wang, J. Lin, R. Socher, and C. Xiong. Mkd: a multi-task knowledge distillation approach for
pretrained language models. arXiv preprint arXiv:1911.03588 , 2019.
I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 , 2017.
X. Ma, C. Zhou, X. Kong, J. He, L. Gui, G. Neubig, J. May, and L. Zettlemoyer. Mega: moving average equipped
gated attention. arXiv preprint arXiv:2209.10655 , 2022.
E. Martin and C. Cundy. Parallelizing linear recurrent neural nets over sequence length. arXiv preprint
arXiv:1709.04057 , 2017.
A. Mauroy and I. Mezić. Global stability analysis using the eigenfunctions of the koopman operator. IEEE
Transactions on Automatic Control , 2016.
A. Mauroy, Y. Susuki, and I. Mezić. Koopman operator in systems and control . Springer, 2020.
W. S. McCulloch and W. Pitts. A logical calculus of the ideas immanent in nervous activity. The bulletin of
mathematical biophysics , 1943.
H. Mehta, A. Gupta, A. Cutkosky, and B. Neyshabur. Long range language modeling via gated state spaces.
arXiv preprint arXiv:2206.13947 , 2022.
Z. Mhammedi, A. Hellicar, A. Rahman, and J. Bailey. Eﬃcient orthogonal parametrisation of recurrent neural
networks using householder reﬂections. In International Conference on Machine Learning . PMLR, 2017.
15Resurrecting Recurrent Neural Networks for Long Sequences
T. Mikolov, M. Karaﬁát, L. Burget, J. Cernock y, and S. Khudanpur. Recurrent neural network based language
model. In Interspeech . Makuhari, 2010.
R. Nallapati, B. Zhou, C. Gulcehre, B. Xiang, et al. Abstractive text summarization using sequence-to-sequence
rnns and beyond. arXiv preprint arXiv:1602.06023 , 2016.
E. Nguyen, K. Goel, A. Gu, G. Downs, P. Shah, T. Dao, S. Baccus, and C. Ré. S4nd: Modeling images and videos
as multidimensional signals with state spaces. In Advances in Neural Information Processing Systems , 2022.
A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and
K. Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499 , 2016.
R. Pascanu, T. Mikolov, and Y. Bengio. On the diﬃculty of training recurrent neural networks. In International
conference on machine learning . PMLR, 2013.
J. L. Proctor, S. L. Brunton, and J. N. Kutz. Generalizing koopman theory to allow for inputs and control. SIAM
Journal on Applied Dynamical Systems , 2018.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations by error propagation.
Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985.
P. J. Schmid. Dynamic mode decomposition of numerical and experimental data. Journal of ﬂuid mechanics ,
2010.
H. T. Siegelmann. Neural networks and analog computation: beyond the Turing limit . Springer Science &
Business Media, 2012.
J. T. Smith, A. Warrington, and S. W. Linderman. Simpliﬁed state space layers for sequence modeling. arXiv
preprint arXiv:2208.04933 , 2022.
J. J. Steil. Backpropagation-decorrelation: online recurrent learning with o (n) complexity. In 2004 IEEE
international joint conference on neural networks . IEEE, 2004.
A. Surana. Koopman operator based observer synthesis for control-aﬃne nonlinear systems. In 2016 IEEE 55th
Conference on Decision and Control (CDC) . IEEE, 2016.
Y. Tay, M. Dehghani, S. Abnar, Y. Shen, D. Bahri, P. Pham, J. Rao, L. Yang, S. Ruder, and D. Metzler. Long range
arena: A benchmark for eﬃcient transformers. In International Conference on Learning Representations , 2020.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is
all you need. Advances in neural information processing systems , 2017.
A. Voelker, I. Kajić, and C. Eliasmith. Legendre memory units: Continuous-time representation in recurrent
neural networks. Advances in neural information processing systems , 2019.
C. R. Vogel. Computational methods for inverse problems . SIAM, 2002.
S. Wang, Z. Li, and Q. Li. The eﬀects of nonlinearity on approximation capacity of recurrent neural networks,
2022.
S. H. Weintraub. Jordan canonical form: theory and practice. Synthesis Lectures on Mathematics and Statistics ,
2009.
M. O. Williams, I. G. Kevrekidis, and C. W. Rowley. A datadriven approximation of the koopman operator:
Extending dynamic mode decomposition. Journal of Nonlinear Science , 2015.
S. Wisdom, T. Powers, J. Hershey, J. Le Roux, and L. Atlas. Full-capacity unitary recurrent neural networks.
Advances in neural information processing systems , 2016.
Z. Zhinan. The jordan canonical form of a rational random matrix. Science Direct Working Paper , 2002.
T. Zhou, Z. Ma, Q. Wen, L. Sun, T. Yao, R. Jin, et al. Film: Frequency improved legendre memory model for
long-term time series forecasting. arXiv preprint arXiv:2205.08897 , 2022.
16Resurrecting Recurrent Neural Networks for Long Sequences
Supplementary Materials
A. Simpliﬁed Implementation of the Linear Recurrent Unit
We present here a simpliﬁed JAX implementation (Bradbury et al., 2018) of the Linear Recurrent Unit (LRU).
The state of the LRU is driven by the input ¹𝑢𝑘º𝐿
𝑘=1of sequence length 𝐿according to the following formula (and
eﬃciently parallelized using an associative scan): 𝑥𝑘=Λ𝑥𝑘1exp¹𝛾logº¹𝐵𝑢𝑘º, and the output is computed
at each timestamp 𝑘as follows: 𝑦𝑘=𝐶𝑥𝑘𝐷𝑢𝑘. In our code, 𝐵𝐶follow Glorot initialization, with 𝐵scaled
additionallybyafactor2toaccountforhalvingthestatevariancebytakingtherealpartoftheoutputprojection.
𝐷is random 𝐻-dimensional and mutiplies elementwise each 𝑢𝑘, where𝑘is the timestamp. Λis initialized with
the help of Lemma 3.2, with phase potentially restricted to a thin slice (see 3.4).
Resurrecting Recurrent Neural Networks for Long Sequences
Supplementary Materials
A.Simpliﬁed Implementation of the Linear Recurrent Unit
We present here a simpliﬁed JAX implementation ( Bradbury et al. ,2018 ) of the Linear Recurrent Unit (LRU).
The state of the LRU is driven by the input (C9)!
9=1of sequence length !according to the following formula (and
eciently parallelized using an associative scan): F9=F91+exp(Wlog)( C9), and the output is computed
at each timestamp 9as follows: G9=F9+C9. In our code, , follow Glorot initialization, with scaled
additionally by a factor 2 to account for halving the state variance by taking the real part of the output projection.
is random -dimensional and mutiplies elementwise each C9, where 9is the timestamp. is initialized with
the help of Lemma 3.2, with phase potentially restricted to a thin slice (see  3.4).1import jax
2import jax.numpy asjnp
3import numpy asnp
4parallel_scan =jax.lax.associative_scan
5
6def forward (lru_parameters, input_sequence):
7 Forward pass of the LRU layer. Output y and input_sequence are of shape (L, H).
8
9 # All LRU parameters
10 nu_log, theta_log, B_re, B_im, C_re, C_im, D, gamma_log =lru_parameters
11
12 # Materializing the diagonal of Lambda and projections
13 Lambda =jnp.exp( -jnp.exp(nu_log) +1j*jnp.exp(theta_log))
14 B_norm =(B_re +1j*B_im) *jnp.expand_dims(jnp .exp(gamma_log), axis =-1)
15 C=C_re +1j*C_im
16
17 # Running the LRU + output projection
18 # For details on parallel scan, check discussion in Smith et al (2022).
19 Lambda_elements =jnp.repeat(Lambda[ None ,...], input_sequence .shape[ 0], axis =0)
20 Bu_elements =jax.vmap( lambda u: B_norm @u)(input_sequence)
21 elements =(Lambda_elements, Bu_elements)
22 _, inner_states =parallel_scan(binary_operator_diag, elements) # all x_k
23 y=jax.vmap( lambda x, u: (C @x).real +D*u)(inner_states, input_sequence)
24
25 return y
26
27def init_lru_parameters (N, H, r_min =0,r _ m a x =1,m a x _ p h a s e =6.28 ):
28 Initialize parameters of the LRU layer.
29
30 # N: state dimension, H: model dimension
31 # Initialization of Lambda is complex valued distributed uniformly on ring
32 # between r_min and r_max, with phase in [0, max_phase].
33 u1=np.random .uniform(size =(N,))
34 u2=np.random .uniform(size =(N,))
35 nu_log =np.log( -0.5*np.log(u1 *(r_max **2-r_min **2)+r_min **2))
36 theta_log =np.log(max_phase *u2)
37
38 # Glorot initialized Input/Output projection matrices
39 B_re =np.random .normal(size =(N,H)) /np.sqrt( 2*H)
40 B_im =np.random .normal(size =(N,H)) /np.sqrt( 2*H)
41 C_re =np.random .normal(size =(H,N)) /np.sqrt(N)
42 C_im =np.random .normal(size =(H,N)) /np.sqrt(N)
43 D=np.random .normal(size =(H,))
44
45 # Normalization factor
46 diag_lambda =np.exp( -np.exp(nu_log) +1j*np.exp(theta_log))
47 gamma_log =np.log(np .sqrt( 1-np.abs(diag_lambda) **2))
48
49 return nu_log, theta_log, B_re, B_im, C_re, C_im, D, gamma_log
50
51def binary_operator_diag (element_i, element_j):
52 # Binary operator for parallel scan of linear recurrence.
53 a_i, bu_i =element_i
54 a_j, bu_j =element_j
55 return a_j *a_i, a_j *bu_i +bu_j
17
17Resurrecting Recurrent Neural Networks for Long Sequences
B. Related works
We ﬁrst discuss standard RNN-based approaches for sequence-to-sequence modeling, and then provide a
historical overview on the progress of the literature stemming from the S4 paper (Gu et al., 2021a).
Recurrent neural networks (RNNs). Before the rise of transformers (Vaswani et al., 2017), RNNs were
widely used in various applications of natural language processing tasks such as language modeling (Mikolov
et al., 2010), machine translation (Cho et al., 2014b) and text summarization (Nallapati et al., 2016). The
modern RNN structure (see Eq.1) is mainly attributed to the works of Rumelhart et al. (1985). However,
it is possible to see the Hopﬁeld Networks as a particular form of RNN (Hopﬁeld, 1982). Modern RNN
formulations are also often related to the Elman Networks (Elman, 1990). The issue of vanishing or exploding
gradients, as described by Bengio et al. (1994); Pascanu et al. (2013), is one barrier to training Recurrent
Neural Networks (RNNs) with gradient descent. This problem limits the ability of RNNs to learn, especially on
tasks with long input sequences. One of the critical contributions to thesuccess of RNNswas the introduction of
gatingmechanismssuchastheLongShort-TermMemory(LSTM)proposedbytheHochreiterandSchmidhuber
(1997). LSTMs address the vanishing gradients problem by introducing input, output, and forget gates, which
enable the network to selectively remember or forget information from previous time steps. Another popular
variant of gated RNNs is the Gated Recurrent Unit (GRU) (Cho et al., 2014b) which simpliﬁes the LSTM
architecture by merging input and forget gates into a single update gate.
Mitigating the vanishing gradient problem with orthogonal and unitary RNNs. Recently, Arjovsky et al.
(2016)introducedunitaryevolutionRNNs(uRNN),whereeigenvaluesintheRNNtransitionmatrix(seeEq. (1))
are restricted to live on the unit circle. The induced map driving the hidden state evolution, therefore, mixes
state components taking into account new inputs  but the signal from past timestamps is not exponentially
vanishing/exploding as in the vanilla RNN case (see discussion on stability in 3.2.1). This idea is powerful but
introduces two problems: (1) choosing unitary transitions restricts the function approximation class, and (2)
training unitary matrices is expensive since a projection on the Stiefel manifold is required at each gradient
step. Toresolvethesecondissue,manyworksdevotedattentiontocarefullydesignedreparameterizationofthe
transitionmatrixase.g.,withtheproductofsimplermatrices(Arjovskyetal.,2016),Givensrotations(Jingetal.,
2017),Householderreﬂections(Mhammedietal.,2017),orasexponentialsofskew-symmetricmatrices(Hyland
and Rätsch, 2017; Lezcano-Casado and Martınez-Rubio, 2019). The approximation capacity of these models is
discussed and improved in (Wisdom et al., 2016). A further step in designing eﬃcient orthogonal RNNs is
provided by Helfrich et al. (2018), who parametrized skew-symmetric matrix using the Cayley transforms,
resulting in a fully real parameter space. Other works which proposed conceptually diﬀerent solutions to
mitigate the vanishing gradient problem include combinations with rectiﬁed linear units (Le et al., 2015),
Lipschitz RNNs (Erichson et al., 2021), and approaches based on dilated convolutions to increase context
size (Bai et al., 2018; Oord et al., 2016)
Deep state-space models (SSMs), a historical overview. Inspired by interesting approaches involving
continuous-time representation for recurrent neural networks (Voelker et al., 2019), Gu et al. (2020) recently
provided an alternative view on the vanishing gradient problem: one can design linearcontinuous-time state-
space models (SSMs), of the form 𝑥¹𝑡º=𝐴𝑥¹𝑡º𝐵𝑢¹𝑡ºwhere the state 𝑥¹𝑡º2ℝ𝑁is guaranteed to compress all
relevant (under a certain metric) information about previously observed (one-dimensional) inputs 𝑢¹0𝑡¼º.
For instance, by using speciﬁc pair of matrices ¹𝐴2ℝ𝑁𝑁𝐵2ℝ𝑁1º, one can discretize the continuous-time
SSM above using a stable, accurate integrator (e.g., bilinear or zero-order-hold) and retrieve the hidden state
𝑥¹𝑡º, which contains the coeﬃcients for the best 𝑁-th degree polynomial approximation to 𝑢¹0𝑡¼º. The idea
of Gu et al. (2020) was to then use the resulting discretized structured (i.e., using structured HiPPO matrices)
state-space model as a starting for the design and initialization of a novel gated RNN.
Later, Gu et al. (2021a) scaled up this idea into a deep architecture, where a collection (one for each input
dimension) of discretized continuous-time structured SSM was placed at each layer as a substitute14for the
attention block, in an attempt to mitigate the 𝑂¹𝐿2ºissue in transformers and provide a theoretically principled
component for sequence-to-sequence modeling. The model reached state-of-the-art on the Long Range Arena
benchmark(Tayetal.,2020),eﬀectivelyshowcasingthepowerofdiscretizedlinearrecurrencesusingstructured
14This idea is also leveraged in FNet (Lee-Thorp et al., 2021), where the attention mechanism is replaced with a simpler linear
token-mixing strategy.
18Resurrecting Recurrent Neural Networks for Long Sequences
transition matrices. Notably, the resulting model, named S4, uses a convenient and stable representation of
the HiPPO transition, which is initialized using a normal + low-rank matrix and then learned eﬃciently in
diagonal + low-rank form using fast Fourier transforms (FFTs) and Cauchy kernels.
In the months following the publication of S4, Gupta et al. (2022a) noticed that most of S4 performance can be
retrieved by only considering the diagonal component of the HiPPO matrix, and therefore showed the power
of discretized diagonal structured continuous-time state space models. This architecture is known as DSS.
As the interest of the community was rising, with ﬁrst applications of DSS and S4 in language (Mehta et al.,
2022), vision (Nguyen et al., 2022) and audio (Goel et al., 2022), Gu et al. (2022a) further simpliﬁed DSS
providing a diagonal form ( S4D) with theoretical guarantees in the inﬁnite width setting. Notably Gu et al.
(2022a) showed that, to retrieve most performance of S4, one can simply initialize the transition matrix 𝐴
in diagonal form, with entries 𝑎𝑛=1
2𝑖𝜋𝑛(S4D-Lin) or 𝑎𝑛=1
2𝑖𝑁
𝜋𝑁
𝑛11(S4D-Inv). Our interest in
S4-like models spiked at this point since the ﬁndings of Gu et al. (2022a) suggest that, given the eﬀectiveness
of such simpliﬁed versions of 𝐴, the root of S4 success might be attributable to more fundamental eﬀects are
orthogonal to the HiPPO theory.
Shortlyafter,Smithetal.(2022)foundthatonecanalsodepartfromtheformalone-dimensionaldiscretization
structure of S4, rooted in the HiPPO theory, and considered a simpliﬁed version where all input dimensions are
eﬃciently and simultaneously processed using parallel scans (Martin and Cundy, 2017)  not separately like
in S4, S4D, and DSS. This model (named S5) set a new state-of-the art on PathX, the hardest task in the Long
Range Arena, and provides further evidence for a conceptually simpler motivation for the performance of deep
state-space models. Indeed, as already mentioned, S5 is not precisely the discretization of a latent continuous-
time SSM, yet still includes parameters like discretization stepsizes that have an ambiguous interpretation in
this context15, suggesting further investigations are needed.
At the same time, a few interesting works developed novel variants of the S4 architecture. Liquid S4 used the
original (non-diagonal) S4 formulation combined with liquid time-constant networks (Hasani et al., 2021,
2022). SimilartoDSS,S4D,andS5, MegaalsosimpliﬁedS4toadiagonalSSM(Maetal.,2022)whileshowing
additionally that restricting the diagonal 𝐴to real numbers  giving it an exponential moving average (EMA)
interpretation  can still work well when combined with attention and a gated block design. Another intriguing
view was provided by the SGConvmodel (Li et al., 2022a), which leverages the convolutional interpretation of
SSMs (Gu et al., 2021b) to design a purely ﬁlter-based version of S4, with no latent continuous-time model or
need for discretization.
The discretization viewpoint also attracted the interest of Gupta et al. (2022b), concurrent to this work, who
pointed out that, after numerical integration, diagonal state-space models and linear RNNs share the same
function approximation class. Gupta et al. (2022b) then introduced DLR, most closely related to DSS and S4D
(each input is processed independently at each layer) but where the discretization stepsize Δis absorbed into
the continuous-time transition matrix 𝐴(see 2). Their focus was on a new set of synthetic long-range tasks
with strong supervision (e.g. segmentation), while ours is on the established Long Range Arena benchmark.
To conclude, we point the reader to interesting recent applications of models inspired by the S4 architecture.
In addition to earlier applications in NLP (Mehta et al., 2022), more sophisticated architectures based on S4
recently showed great promise in language modeling (Dao et al., 2022b; Ma et al., 2022). Speciﬁcally, Dao
et al. (2022b) designed a new generative language model, H3, that outperforms GPT-Neo-2.7B with SSMs,
augmented with two attention layers. Besides language, deep state-space models were also found successful
forlongvideo/audiounderstandingandgenerationtasks(Goeletal.,2022;IslamandBertasius,2022;Nguyen
et al., 2022), and have attracted interest in biology (Bordin et al., 2022) and time series forecasting (Zhou
et al., 2022).
15One can still view S5 as a discretized version of a continuous-time SSM. However, this requires adjusting the input projection matrix.
19Resurrecting Recurrent Neural Networks for Long Sequences
C. Additional experimental results
C.1. Training speedups
In Tb.4, we show training speed comparisons of the LRU with a regular RNN with tanh activations, as well as
with the S4D and S5 models. As we elaborate in 2.2, for the LRU, we closely followed the optimal model sizes
of the S5 model. Consequently, we also see similar training speeds as the S5 model on all tasks.
M/o.sc/d.sc/e.sc/l.sc /s.scCIFAR L/i.sc/s.sc/t.scO/p.sc/s.sc T/e.sc/x.sc/t.sc R/e.sc/t.sc/r.sc/i.sc/e.sc/v.sc/a.sc/l.sc P/a.sc/t.sc/h.sc/f.sc/i.sc/n.sc/d.sc/e.sc/r.sc P/a.sc/t.sc/h.scX
T/a.sc/n.sc/h.sc RNN 2.0 1.1 0.5 0.5 2.1 0.14
LRU 15.9 (8/x.sc) 2.1 (1.9/x.sc) 14.7 (29/x.sc) 5.7 (11.4/x.sc) 15.5 (7.4/x.sc) 2.4 (17/x.sc)
S4D (/o.sc/u.sc/r.sc /r.sc/e.sc/p.sc/r.sc/o.sc/d.sc/u.sc/c.sc/t.sc/i.sc/o.sc/n.sc) 13.5 2.2 10.6 3.0 24.5 2.6
S5 (/o.sc/u.sc/r.sc /r.sc/e.sc/p.sc/r.sc/o.sc/d.sc/u.sc/c.sc/t.sc/i.sc/o.sc/n.sc) 15.9 2.2 14.4 5.7 15.6 2.3
Table 4jSpeeds (steps/sec) during training on a A100 GPU. We also show the speedup of the LRU over the tanh
RNN for each task. The batch size used for each task is speciﬁed in Tb.9.
C.2. Eﬀect of stability and normalization
Inthissection,weexplorefurthertheeﬀectofintroducingstabilityduringtraining(3.3),aswellasintroducing
the𝛾normalization factor as shown in Eq. (7). To do this, we consider the sCIFAR experiment where we sweep
over diﬀerent settings of 𝑟maxand𝑟minto see the eﬀect when initializing closer to the unit disk. We keep
the learning rate ﬁxed at 0.004 for these experiments, which we found to be optimal when initializing with
𝑟max=10and𝑟min=00under a stable exponential parameterization.
We show our results in Tb.5. In the ﬁrst table Tb.5(A), we show results with our baseline where we use the
exponential parameterization described in 3.3. We see that under this setting, the optimal performance is
achieved when 𝑟max=𝑟min09, and performance degrades as 𝑟maxis increased beyond 0.9.
In Tb.5(B) we show results after enforcing stability. We now notice that for each 𝑟min, the optimal performance
is achieved by a higher 𝑟maxthan before, i.e., training is more when initializing closer to the unit disk. Our
optimal performance in this setting is achieved using 𝑟min=00and𝑟max=099. Note that even in this setting,
performance can sometimes degrade when moving to even higher 𝑟max.
Finally, in Tb.5(C) we also incorporate the 𝛾normalization factor, and we now notice no degradation in
performance even when 𝑟max=0999. We found training to be more stable in this setting, and our best result
of 89.0% performance is also obtained in this setting, with 𝑟min=09and𝑟max=0999.
These ablations further motivate the beneﬁts of enforcing stability and using the normalization parameter for
betterperformanceandmorestabletraining, particularlywhenrequiredtolearnverylong-rangedependencies.
C.3. Expanded tables
Below we show our full results on the Long Range Arena, expanding on Tables 1, 2, and 3 in the main paper.
The tables are presented in logical order: in Table 6, we show that vanilla (dense) RNNs proﬁt from dropping
recurrent nonlinearities when used in the context of the architecture in Fig. 1. Next, in Table 7 we diagonalize
our linear RNN model from 3.1 and show how diﬀerent parametrization for the diagonal elements aﬀect
performance. For all the rows in Table 7, initialization of the diagonal RNN was performed uniform on the disk,
to match the random Glorot initialization of our dense version (Thm. 3.1).
Further, the last row in Table 7 shows the positive eﬀects of changing initialization distribution to a thin ring
close to the circle boundary  eﬀectively enabling long-range reasoning through mitigation of vanishing
gradients. Our settings for the ring are reported on the ﬁrst row of Table 8. Finally, the second row of this table
shows the improvements that can be achieved by including model normalization (Eq. (7)), which closes the
accuracy gap with deep SSMs.
20Resurrecting Recurrent Neural Networks for Long Sequences
𝑟max𝑟min0 05 09
0.9 87.6 (0.4) 87.8 (0.1) 87.9 (0.2)
0.99 83.8 (0.9) 85.8 (1.2) 81.9 (3.8)
0.999 83.9 (0.2) 84.8 (0.4) 84.8 (0.8)
(/a.sc) N/o.sc /s.sc/t.sc/a.sc/b.sc/i.sc/l.sc/i.sc/t.sc/y.sc.
𝑟max𝑟min0 05 09
0.9 86.2 (0.2) 86.6 (0.3) 87.3 (0.1)
0.99 87.8 (0.2) 87.7 (0.1) 88.1 (0.0)
0.999 87.4 (0.2) 87.4 (0.1) 87.5 (0.4)
(/b.sc) W/i.sc/t.sc/h.sc /s.sc/t.sc/a.sc/b.sc/i.sc/l.sc/i.sc/t.sc/y.sc.
𝑟max𝑟min0 05 09
0.9 86.4 (0.1) 86.5 (0.1) 88.3 (0.1)
0.99 88.1 (0.1) 88.4 (0.1) 89.0 (0.2)
0.999 88.1 (0.1) 88.6 (0.0) 89.0 (0.1)
(/c.sc) W/i.sc/t.sc/h.sc 𝛾/n.sc/o.sc/r.sc/m.sc/a.sc/l.sc/i.sc/z.sc/a.sc/t.sc/i.sc/o.sc/n.sc.
Table 5jEﬀect of stability and normalization and diﬀerent 𝑟minand𝑟maxvalues on test accuracy for the sCIFAR10
task. Bothstabilityandnormalizationallowforinitializingeigenvaluesclosertotheunitdisk,resultinginimproved
performance.
R/e.sc/c.sc/u.sc/r.sc/r.sc/e.sc/n.sc/c.sc/e.sc /s.scCIFAR L/i.sc/s.sc/t.scO/p.sc/s.sc T/e.sc/x.sc/t.sc R/e.sc/t.sc/r.sc/i.sc/e.sc/v.sc/a.sc/l.sc P/a.sc/t.sc/h.sc/f.sc/i.sc/n.sc/d.sc/e.sc/r.sc P/a.sc/t.sc/h.scX
RNN-L/i.sc/n.sc 72.2 (0.2) 50.4 (0.2) 89.1 (0.1) 89.1 (0.1) % %
RNN-R/e.scLU 69.7 (0.2) 37.6 (8.0) 88.0 (0.1) 88.5 (0.1) % %
RNN-T/a.sc/n.sc/h.sc 69.9 (0.3) 43.9 (0.1) 87.2 (0.1) 88.9 (0.2) % %
S4D (/o.sc/u.sc/r.sc /r.sc/e.sc/p.sc/r.sc/o.sc/d.sc/u.sc/c.sc/t.sc/i.sc/o.sc/n.sc) 91.5 (0.2) 60.2 (0.3) 86.4 (0.0) 89.5 (0.0) 94.2 (0.3) 97.5 (0.0)
S5 (/o.sc/u.sc/r.sc /r.sc/e.sc/p.sc/r.sc/o.sc/d.sc/u.sc/c.sc/t.sc/i.sc/o.sc/n.sc) 88.8 (0.1) 58.5 (0.3) 86.2 (0.1) 88.9 (0.0) 95.7 (0.1) 96.0 (0.1)
S4 (/p.sc/a.sc/p.sc/e.sc/r.sc /r.sc/e.sc/s.sc/u.sc/l.sc/t.sc/s.sc) 91.1 59.6 86.8 90.9 94.2 96.4
S4D-L/e.sc/g.scS (/p.sc/a.sc/p.sc/e.sc/r.sc /r.sc/e.sc/s.sc/u.sc/l.sc/t.sc/s.sc) 89.9 60.5 86.2 89.5 93.1 91.9
S5 (/p.sc/a.sc/p.sc/e.sc/r.sc /r.sc/e.sc/s.sc/u.sc/l.sc/t.sc/s.sc) 90.1 62.2 89.3 91.4 95.3 98.6
Table 6jPlacing a Vanilla RNN as recurrent core in the architecture of Fig. 1. Shown is the eﬀect of removing the
RNN non-linearity on test accuracy (3.1).
D. Detailed experimental setup
In this section, we describe our experimental details.
D.1. Architecture
We consider the standard S4 architecture of Gu et al. (2021a) and replace the S4 layers with RNN layers or
with S5 (Smith et al., 2022) or S4D (Gu et al., 2022a) layers for our baselines. We give an overview of the
architecture used in Fig.1. The input is ﬁrst encoded into 𝐻features, followed by a stack of residual blocks.
For all our experiments, we use networks with a depth of 6 residual blocks. Each residual block consists of
identity skip connection, and the residual path containing a normalization layer (in our case, we always use
batch normalization in our experiments), followed by the RNN/SSM block. While using the post-norm option
of adding the normalization layer after the skip and residual branches typically improves performance, we stick
to this design due to this architecture being more scalable in general (De and Smith, 2020).
Each RNN/SSM block ﬁrst contains the recurrent layer as described in Eqs. (1)and(3)in 2. This is followed
by a mixing layer. For all experiments except PathX, we use the GLU activation function (Dauphin et al., 2017)
with dropout as the mixing layer, similar to Gu et al. (2021a). For PathX, we instead use a GLU activation
functionwithoutoneadditionallineartransform; thesameasusedbySmithetal.(2022)fortheirexperiments.
We use bidirectional models for our experiments on PathFinder and PathX, using a similar setup as Gu et al.
(2021a), and use unidirectional models for the rest of our experiments.
21Resurrecting Recurrent Neural Networks for Long Sequences
/s.scCIFAR L/i.sc/s.sc/t.scO/p.sc/s.sc T/e.sc/x.sc/t.sc R/e.sc/t.sc/r.sc/i.sc/e.sc/v.sc/a.sc/l.sc P/a.sc/t.sc/h.sc/f.sc/i.sc/n.sc/d.sc/e.sc/r.sc P/a.sc/t.sc/h.scX
D/e.sc/n.sc/s.sc/e.sc𝐴 72.2 (0.2) 50.4 (0.2) 89.1 (0.1) 89.1 (0.1) % %
ΛR/e.sc/a.sc/l.sc + I/m.sc 86.5 (0.1) 58.8 (0.3) 87.4 (0.3) 87.8 (0.5) % %
ΛE/x.sc/p.sc 85.4 (0.7) 60.5 (0.3) 86.5 (0.4) 89.4 (0.1) 65.4 (9.0) %
ΛS/t.sc/a.sc/b.sc/l.sc/e.sc E/x.sc/p.sc 87.2 (0.4) 59.4 (0.3) 87.6 (0.3) 89.1 (0.2) 93.5 (0.5) %
+ R/i.sc/n.sc/g.sc I/n.sc/i.sc/t.sc 88.1 (0.0) 59.4 (0.3) 89.4 (0.1) 90.1 (0.1) 94.4 (0.3) %
S4D (/o.sc/u.sc/r.sc /r.sc/e.sc/p.sc/r.sc/o.sc/d.sc/u.sc/c.sc/t.sc/i.sc/o.sc/n.sc) 91.5 (0.2) 60.2 (0.3) 86.4 (0.0) 89.5 (0.0) 94.2 (0.3) 97.5 (0.0)
S5 (/o.sc/u.sc/r.sc /r.sc/e.sc/p.sc/r.sc/o.sc/d.sc/u.sc/c.sc/t.sc/i.sc/o.sc/n.sc) 88.8 (0.1) 58.5 (0.3) 86.2 (0.1) 88.9 (0.0) 95.7 (0.1) 96.0 (0.1)
S4 (/p.sc/a.sc/p.sc/e.sc/r.sc /r.sc/e.sc/s.sc/u.sc/l.sc/t.sc/s.sc) 91.1 59.6 86.8 90.9 94.2 96.4
S4D-L/e.sc/g.scS (/p.sc/a.sc/p.sc/e.sc/r.sc /r.sc/e.sc/s.sc/u.sc/l.sc/t.sc/s.sc) 89.9 60.5 86.2 89.5 93.1 91.9
S5 (/p.sc/a.sc/p.sc/e.sc/r.sc /r.sc/e.sc/s.sc/u.sc/l.sc/t.sc/s.sc) 90.1 62.2 89.3 91.4 95.3 98.6
Table 7jTest accuracy of a linear diagonal complex RNNs under diﬀerent parameterizations of the transition
matrix (see 3.2). Performance directly improves the results in Tb. 1, and showcases the advantage of exponen-
tial (polar) representation of Λ. In bold font is the best parameterization option for linear RNN blocks. Ring Init
denotes a changed initialization where 𝑟minand𝑟maxare tuned. Performance and Text and Retrieval task already
aligns with S4 results in the dense setting (c.f. Tb.1 with Tb. 3). No model with able to solve PathX, which requires
normalization (see Tb.3).
/s.scCIFAR L/i.sc/s.sc/t.scO/p.sc/s.sc T/e.sc/x.sc/t.sc R/e.sc/t.sc/r.sc/i.sc/e.sc/v.sc/a.sc/l.sc P/a.sc/t.sc/h.sc/f.sc/i.sc/n.sc/d.sc/e.sc/r.sc P/a.sc/t.sc/h.scX
L/i.sc/n.sc/e.sc/a.sc/r.sc D/e.sc/n.sc/s.sc/e.sc RNN 72.2 (0.2) 50.4 (0.2) 89.1 (0.1) 89.1 (0.1) % %
D/i.sc/a.sc/g.sc/o.sc/n.sc/a.sc/l.sc C/o.sc/m.sc/p.sc/l.sc/e.sc/x.sc RNN 86.5 (0.1) 58.8 (0.3) 87.4 (0.3) 87.8 (0.5) % %
S/t.sc/a.sc/b.sc/l.sc/e.sc E/x.sc/p.sc P/a.sc/r.sc/a.sc/m.sc /w.sc/ R/i.sc/n.sc/g.sc I/n.sc/i.sc/t.sc 88.1 (0.0) 59.4 (0.3) 89.4 (0.1) 90.1 (0.1) 94.4 (0.3) %
𝑟min𝑟max¼ [0.9, 0.99] [0.0, 1.0] [0.0, 0.9] [0.5, 0.9] [0.9, 0.999]
𝛾N/o.sc/r.sc/m.sc/a.sc/l.sc/i.sc/z.sc/a.sc/t.sc/i.sc/o.sc/n.sc (LRU) 89.0 (0.1) 60.2 (0.8) 89.4 (0.1) 89.9 (0.1) 95.1 (0.1) 94.2 (0.4)
𝑟min𝑟max¼ [0.9, 0.999] [0.0, 0.99] [0.5, 0.9] [0.5, 0.9] [0.9, 0.999] [0.999, 0.9999]
S4D (/o.sc/u.sc/r.sc /r.sc/e.sc/p.sc/r.sc/o.sc/d.sc/u.sc/c.sc/t.sc/i.sc/o.sc/n.sc) 91.5 (0.2) 60.2 (0.3) 86.4 (0.0) 89.5 (0.0) 94.2 (0.3) 97.5 (0.0)
S5 (/o.sc/u.sc/r.sc /r.sc/e.sc/p.sc/r.sc/o.sc/d.sc/u.sc/c.sc/t.sc/i.sc/o.sc/n.sc) 88.8 (0.1) 58.5 (0.3) 86.2 (0.1) 88.9 (0.0) 95.7 (0.1) 96.0 (0.1)
S4 (/p.sc/a.sc/p.sc/e.sc/r.sc /r.sc/e.sc/s.sc/u.sc/l.sc/t.sc/s.sc) 91.1 59.6 86.8 90.9 94.2 96.4
S4D-L/e.sc/g.scS (/p.sc/a.sc/p.sc/e.sc/r.sc /r.sc/e.sc/s.sc/u.sc/l.sc/t.sc/s.sc) 89.9 60.5 86.2 89.5 93.1 91.9
S5 (/p.sc/a.sc/p.sc/e.sc/r.sc /r.sc/e.sc/s.sc/u.sc/l.sc/t.sc/s.sc) 90.1 62.2 89.3 91.4 95.3 98.6
Table 8jEﬀects of normalization on linear diagonal RNNs with stable exponential parameterization (see 3.4). In
bold is our best performing model, and we report the closely matching deep SSM results below. Tunings for our
rings are also reported. Results showcase the advantage of taking initialization close to the unit circle under proper
𝛾normalization. For PathX, we initialize eigenvalues to have a phase range of 0𝜋10¼, for all other tasks we use
a range of02𝜋¼(see 3.4).
D.2. General experimental details
We use AdamW as our optimizer (Loshchilov and Hutter, 2017). We use warmup for the learning rate, where
we start from a value of 107and increase the learning rate linearly up a speciﬁed value for the ﬁrst 10% of
training. This is followed by cosine annealing for the rest of training down to a value of 107.
We used a smaller learning rate for the RNN/SSM parameters 𝐴and𝐵. When using normalization in our RNNs,
we also used a smaller learning rate on the normalization parameter 𝛾. For our S5 and S4D baselines, we
used a smaller learning rate for the discretization step size Δ. This smaller learning rate was determined by
multiplying the base learning rate by a factor 1(See Tb.9 for the learning rate factor used for each task).
WeuseweightdecayforallparametersexcepttheRNN/SSMparameters 𝐴and𝐵(and𝛾andΔwhenapplicable).
All experiments were carried out on accelerated hardware A100 GPUs.
D.3. Hyperparameters
We closely followed the hyperparameter settings of the S5 model Smith et al. (2022) for all our experiments,
with minimal additional tuning. For our S5 baseline, we tuned the model dimension 𝐻and state dimension 𝑁,
22Resurrecting Recurrent Neural Networks for Long Sequences
T/a.sc/s.sc/k.sc D/e.sc/p.sc/t.sc/h.sc 𝐻𝑁 I/t.sc/e.sc/r.sc/a.sc/t.sc/i.sc/o.sc/n.sc/s.sc B/a.sc/t.sc/c.sc/h.sc /s.sc/i.sc/z.sc/e.sc LR /f.sc/a.sc/c.sc/t.sc/o.sc/r.sc W/e.sc/i.sc/g.sc/h.sc/t.sc D/e.sc/c.sc/a.sc/y.sc D/r.sc/o.sc/p.sc/o.sc/u.sc/t.sc
/s.scCIFAR 6 512 384 180/k.sc 50 0.25 0.05 0.1
L/i.sc/s.sc/t.scO/p.sc/s.sc 6 128 256 80/k.sc 32 0.5 0.05 0.0
T/e.sc/x.sc/t.sc 6 256 192 50/k.sc 32 0.1 0.05 0.1
R/e.sc/t.sc/r.sc/i.sc/e.sc/v.sc/a.sc/l.sc 6 128 256 100/k.sc 64 0.5 0.05 0.1
P/a.sc/t.sc/h.scF/i.sc/n.sc/d.sc/e.sc/r.sc 6 192 256 500/k.sc 64 0.25 0.05 0.0
P/a.sc/t.sc/h.scX 6 128 256 250/k.sc 32 0.25 0.05 0.0
Table 9jList of all the hyper-parameters used for each task for the LRU model.
and used the optimal values for the LRU model as well. For the S4D baseline, we also tuned 𝐻and𝑁. For all
our experiments, we tuned the base learning rate on a logarithmic grid of 2 to choose the optimal learning rate.
We present the hyperparameters we used for each LRU experiment in Tb.9.
D.4. Tasks
We use the 6 tasks in the Long Range Arena benchmark for our experiments (Tay et al., 2020), with the only
diﬀerence being we use colored sCIFAR images instead of the grayscale sCIFAR images used in LRA.
E. Theoretical insights
We provide here theoretical groundings for some observations made in 3. We start by showing in E.1 that,
when interleaved with MLP blocks, stacked linear RNNs can model highly nonlinear dynamical systems. We
provide two separate views that justify our ﬁndings: in E.1.1, we provide a spectral explanation, while in
E.1.2 we present a function-space prespective. Our results, combined with the observation that nonlinear
RNNs are diﬃcult to optimize (E.2), provide a justiﬁcation for the results in Tb. 1. Next, motivated by the
results in Tb. 3 we in discuss in the same section optimization of linear RNN blocks, and show that exponential
reparameterization can accelerate training.
E.1. Expressivity of linear RNN stacks
In our sequence-to-sequence setting, it is a natural to seek models which (at least in the width limit) are
able to map inputs 𝑢to outputs 𝑦(last layer) using a ﬂexible nonlinear transition map 𝑇learned from data.
Mathematically, a fully-expressive causalmodel should be able to approximate 𝑦𝑘=𝑇¹𝑢𝑘𝑢𝑘1𝑢 1º, where𝑇
is an arbitrary nonlinear map.
E.1.1. Spectral perspective
We show in this section how interleaving linear RNNs with MLPs in a deep architecture provides a ﬂexible and
modular recipe for the approximation of nonlinear transition maps.
SpectrallimitationsoflinearRNNs. Itisastandardresult(Lietal.,2022b)that linearRNNscanapproximate
any shift-invariant linearmap𝑇. In continuous-time, on the spectral domain, this property is easier to study:
let𝑌¹𝜔ºand𝑈¹𝜔ºbe the Fourier transforms for two continuous-time signals 𝑢𝑦:ℝ!ℝ. If there exists a
function𝐻:ℝ!ℝsuch that𝑌¹𝜔º=𝐻¹𝜔º𝑈¹𝜔º, then this can be approximated by a continuous-time linear
RNN𝑥=𝐴𝑥𝐵𝑢for some coeﬃcients 𝐴2ℝ𝑁𝑁𝐵2ℝ𝑁1, and the approximation can be made arbitrarily
accurate as 𝑁!1. However, one thing a linear RNN cannot do is store information under frequencies which
are not present in the input signal: if the input is a sine wave of a certain frequency, the output will be a scaled
and shifted sine wave of the same frequency .
Spectral eﬀects of interleaving with MLPs. In our architecture (Fig.1) an activation function, as well as a
linear position-wise layer, is placed right after each RNN output. As can be seen in Fig. 6, this operation causes
spectral leakage: information gets copied over diﬀerent frequency components.
The behavior shown in Fig. 6 can be characterized exactly:
23Resurrecting Recurrent Neural Networks for Long Sequences
0.00 0.01 0.02 0.03 0.04 0.050.010
0.005
0.0000.0050.010Signal
0.00 0.01 0.02 0.0305101520253035Abs. value FFT
original
after ReLU
Figure 6jReLU nonlinearity leaks information from the original signal to higher frequencies, as shown formally in
Prop. E.1.
Proposition E.1 (Spectral eﬀect of ReLU) .Let𝑢:ℝ!ℝbe a continuous-time signal. Let 𝑃𝑖be the𝑖-th region
activated by the ReLU applied to 𝑢, and let us write 𝑃𝑖=𝑝𝑖𝐿𝑖𝑝𝑖𝐿𝑖¼. Then
FReLU¹𝑢º=F𝑢¹𝜔º
𝑖2𝐿𝑖𝑒𝑖𝜔𝑝𝑖sinc¹𝜔𝐿𝑖º#
 (8)
whereFdenotes the Fourier transform, the convolution operation and sinc ¹𝑥º:=sin¹𝑥º𝑥.
This result is simple to parse: the Fourier transform of a ReLU activated signal is equal to the Fourier transform
before the ReLU, convolved with a kernel which transports information to higher frequencies  an operation
which is impossible for linear RNNs, even as the width increases. As such, introducing an MLP completes
the list of requirements for approximations of a nonlinear transition map: frequencies can be scaled
up and down arbitrarily by the RNN, and can then be translated in the space using the ReLU . As depth
increases, these operations can be combined in a modular fashion, leading to highly nonlinear dynamics using
easy-to-learn linear blocks, interleaved with simple activations.
To conclude, we provide a proof for the proposition above.
Proof.Recall that multiplications in the time domain are convolutions in the frequency domain.
𝑢1¹𝑡º𝑢2¹𝑡º=F1
𝑈1¹𝑡ºF1
𝑈2¹𝑡º (9)
=1
1𝑈1¹𝜈º𝑒𝑖𝜈𝑡𝑑𝜈
1
1𝑈2¹𝜉º𝑒𝑖𝜉𝑡𝑑𝜉
(10)
=1
1𝑈1¹𝜈º1
1𝑈2¹𝜉º𝑒𝑖¹𝜉𝜈º𝑡𝑑𝜉
𝑑𝜈 (11)
=1
1𝑈1¹𝜈º1
1𝑈2¹𝜔𝜈º𝑒𝑖𝜔𝑡𝑑𝜔
𝑑𝜈 (12)
=1
11
1𝑈1¹𝜈º𝑈2¹𝜔𝜈º𝑑𝜈
𝑒𝑖𝜔𝑡𝑑𝜔 (13)
=F1
𝑈1𝑈2¹𝑡º (14)
Let now𝑢1=𝑢and𝑢2=𝜒¹𝑢10º, then𝑢1𝑢2=ReLU¹𝑢º. Next, let 𝑃𝑖be the𝑖-th region activated by the ReLU,
and let us write 𝑃𝑖=𝑝𝑖𝐿𝑖𝑝𝑖𝐿𝑖¼. We can write 𝜒¹𝑢10º=Í
𝑖𝜒𝑝𝑖𝐿𝑖𝑝𝑖𝐿𝑖¼.
Recall now the following basic properties:
1.F𝑥¹𝑡𝑡0º¹𝜔º=𝑒𝑖𝜔𝑡0F𝑥¹𝑡º¹𝜔º.
2.The Fourier transform of a rectangular pulse between 𝜏and𝜏is2𝜏sinc¹𝜔𝜏º, wheresinc¹𝑥º=sin¹𝑥º𝑥.
Therefore, we have
F𝜒𝑝𝑖𝐿𝑖𝑝𝑖𝐿𝑖¼¹𝜔º=𝑒𝑖𝜔𝑝𝑖F𝜒𝐿𝑖𝐿𝑖¼¹𝜔º=2𝐿𝑖𝑒𝑖𝜔𝑝𝑖sinc¹𝜔𝐿𝑖º (15)
24Resurrecting Recurrent Neural Networks for Long Sequences
This concludes the proof:
FReLU¹𝑢º=𝑈
𝑖2𝐿𝑖𝑒𝑖𝜔𝑝𝑖sinc¹𝜔𝐿𝑖º#
 (16)

E.1.2. Insights from Koopman operator theory
We show how Koopman operator theory (Koopman and Neumann, 1932), combined with recent advances in
dynamic mode decomposition (Kutz et al., 2016; Schmid, 2010; Williams et al., 2015), can provide a solid
theoretical foundation for understanding the class of functions that can be approximated by linear RNNs,
interleaved with MLPs. Our notation and results are based on Korda and Mezić (2018); Mauroy et al. (2020).
Basic theory. Consider a discrete-time nonlinear dynamical system 𝑥𝑘1=𝑆¹𝑥𝑘º, where𝑆:ℝ𝑛!ℝ𝑛is a
suﬃciently regular map. The Koopman operator K𝑆for the dynamical system 𝑆prescribes the evolution of any
observable (measurement) 𝑓:ℝ𝑛!ℂ:
¹K𝑆𝑓º¹𝑥º:=𝑓¹𝑆¹𝑥ºº (17)
For instance, let us consider 𝑛=1and the observable 𝑓¹𝑥º=sin¹𝑥º: the Koopman operator is the map that
takes sin¹ºK𝑆!sin¹𝑆¹ºº, i.e.advances the measurement 𝑓one step forward in time.
The crucial property of the Koopman operator is that it is linearand bounded (Mauroy et al., 2020): let 𝑓1 𝑓2
be two observables, then
K𝑆¹𝛼𝑓1𝛽𝑓2º¹𝑥º=¹𝛼𝑓1𝛽𝑓2º¹𝑆¹𝑥ºº (18)
=𝛼𝑓1¹𝑆¹𝑥ºº𝛽𝑓2¹𝑆¹𝑥ºº (19)
=𝛼¹K𝑆𝑓1º¹𝑥º𝛽¹K𝑆𝑓2º¹𝑥º (20)
If𝑆isregularenough,i.e. iftheHilbertspaceofobservablescanbechosensuchthat Konlyhaspointspectrum,
then the spectral theory of bounded linear operators in Hilbert spaces implies that K𝑆is diagonalizable  i.e.
any observable 𝑓can be expanded in terms of eigenfunctions of K𝑆, where the Koopman acts linearly. We
recall the deﬁnition: 𝜙𝜆:ℂ𝑛!ℂis an eigenfunction of K𝑆with eigenvalue 𝜆2ℂifK𝑆𝜙𝜆=𝜆𝜙𝜆 i.e if the
system measured on 𝜙evolves linearly. Since the eigenfunctions of K𝑆form a basis for 𝐿2, for any observable
𝑓:ℂ𝑛!ℂ, there exist complex numbers 𝜈1𝜈2such that one can write (Mauroy and Mezić, 2016)
K𝑆𝑓¹𝑥º=K𝑆
1
𝑗=1𝜈𝑗𝜙𝑗ª
¹𝑥º=1
𝑗=1𝜆𝑘𝜈𝑗𝜙𝑗¹𝑥º (21)
Since also the identity measurement map 𝑥!𝑥can be decomposed into eigenfunctions of K𝑆coordinate-wise,
we have the following: assuming 𝑥𝑘1=𝑆¹𝑥𝑘º, with𝑥2ℝ𝑛, for any𝑘2ℕwe have
𝑥𝑘=𝑉Λ𝑘Φ¹𝑥0º (22)
where, with slight abuse of notation, Φ:ℝ𝑛!ℂ1is a vector of functions with the 𝑗coordinate deﬁned as
¹Φº𝑗:=𝑥!𝜙𝑗¹𝑥º, and𝑉2ℂ𝑛1(often named the Koopman modes matrix) is the inﬁnite dimensional matrix
such that, for the observable 𝑓𝑖:𝑥!𝑥𝑖, one has𝑓𝑖¹𝑥º=Í1
𝑗=1𝑉𝑖𝑗𝜙𝑗¹𝑥º.
Basic Theory Summary. In essence, Koopman operator theory, provides the following guarantee: any suf-
ﬁciently regular nonlinear autonomous dynamical system can be made linear under a high-dimensional
nonlinear blow-up of the state-space. Sounds familiar? This is exactly what a wide MLP + Linear RNN
can do. Moreover, to take the system back to the original coordinate system, one just needs a linear projection
withmatrix 𝑉. Inpractice,foridentiﬁcationanddiagnosisofnonlinearsystems(e.g. inmachanicalengineering),
this approach is used in a truncated version, where the ﬁnite class of dominant eigenfunctions is constructed
by using the dynamic mode decomposition (DMD) algorithm from Hermite Polynomials (Kaiser et al., 2021;
Schmid, 2010).
25Resurrecting Recurrent Neural Networks for Long Sequences
Extension to nonlinear systems with inputs. Several options exist for extending Koopman operator theory
to systems with inputs (Kaiser et al., 2021; Korda and Mezić, 2020; Proctor et al., 2018; Surana, 2016). Here,
webrieﬂyoutlinetheapproachof(KordaandMezić,2020). Let 𝑆:ℝ𝑛ℝ𝑚!ℝ𝑛beanonlinearfunctionwhich
evolves the state of the system as 𝑥𝑘1=𝑆¹𝑥𝑘𝑢𝑘º, where¹𝑢𝑘º1
𝑘=122¹ℝ𝑚ºis the input sequence. We wish to
take this nonlinear dynamical system with inputs to linear form in the inﬁnite-dimensional space of observables
𝑓of the form ℝ𝑛2¹ℝ𝑚º!ℂ. LetLdenote the left shift operator 𝑢=¹𝑢0𝑢1º!L¹ 𝑢º=¹𝑢1𝑢2º,
then one can deﬁne the Koopman operator for any observable 𝑓as follows:
K𝑆𝑓¹𝑥𝑢º=𝑓¹𝑆¹𝑥𝑢0ºL¹𝑢ºº (23)
This operator is again linear and bounded for regular enough 𝑆(Korda and Mezić, 2020)  hence the analysis
in the autonomous setting carries out also in this case. In particular, using the notation in the last paragraph:
𝑥𝑘=𝑉Λ𝑘
¹𝑥𝑢ºΦ¹𝑥0𝑢º (24)
where Λ¹𝑥𝑢ºis a diagonal complex inﬁnite-dimensional matrix which contains the eigenvalues corresponding
to the eigenfunctions of the extended state Φ¹𝑥0𝑢º.
Implication for deep RNNs. In essence, Koopman operator theory, provides the following guarantee: any
regular nonlinear dynamical system is representable by a linear RNN after proper nonlinear reparameterization of
the inputs  which can be performed by an MLP. While we believe this connection is conceptually solid and
gives substantial insights into our architecture, a quantitative discussion would require substantial technical
eﬀorts perhaps linked to recent contributions from the statistical learning community (Kostic et al., 2022).
E.2. Optimization of recurrent blocks
In this subsection we back-up some of our claims about optimization of linear RNNs with experimental ﬁndings
on toy examples. Our purpose is to conﬁrm validity of our intuition outside the deep learning setting, without
architecture-dependent confounders: i.e on vanilla RNNs with one layer.
Recurrent nonlinearities slow down gradient descent. In 3 and E.1 we showed how linear RNNs can be
used as elementary recurrent blocks for the purpose of modeling complex nonlinear dynamics when stacked in
deep architectures. Similarly, the results in (Li et al., 2022a) indicate that, to achieve S4 performance, one
can equivalently replace the recurrent core with a collection of convolutions parametrized by ﬁlters. While
a single-layer level, a (dense) RNNs (Eq.1) with tanhor sigmoid activation can express convolutions with
ﬁlters (Wang et al., 2022), the results in Tb. 1 (and Fig. 1(a) in Wang et al. (2022)) indicate an advantage on
test accuracy from dropping such nonlinearities in the recurrence  i.e. of making the RNN linear. Motivated
by this, in Fig. 7 we consider the problem of learning a single one-dimensional convolution kernel with a single
layer RNN, and compare performance of linear and tanhactivations. The sequence length in this problem was
100, and our data consists in 32 input-output one-dimensional trajectories, where the output is the result of a
convolution with the kernel of elements ℎ𝑘:=1
10exp¹0015𝑘ºcos¹004𝑘º2, which induces moderate-length
dependencies in the data (see bump in the kernel in Figure 7 at 𝑘=70). The 32 input sequences are generated
sampling random 𝑎𝑐parameters on a range and have form sin¹005𝑎𝑘ºcos¹005𝑐𝑘º2. Outputs are
generated by convolving each input by ℎ. Learning is performed using the Adam optimizer (Kingma and Ba,
2014) with standard momentum parameters.
Interestingly, already on this simple task, linear RNNs outperforms the tanhvariant even after careful tuning of
thestepsize. Whiletheinput-outputmapthesystemhadtoapproximateislinear(i.e. aconvolution),thisresult
still indicates that on deep architectures, where the MLPs interleaving RNNs can quickly perform position-wise
nonlinearities lifting the function approximation class (see E.1), linear RNNs are preferrable.
Beneﬁts of exponential parameterization. Our experimental results in 3.3 indicete that linear RNN cores
can be more eﬀectively learned under exponential parameterization of the eiganvalues: 𝜆=exp¹𝜈𝑖𝜃º.
To understand the reason behind this phenomenon, we go back at the classical (hard) problem of learning
powers (Bengio et al., 1994), crucially linked with linear RNN models (see Eq. (4)). For a speciﬁc planted
solution𝜆=𝜆
𝑟𝑖𝜆
𝑖=exp¹𝜈𝑖𝜃º, we consider the problem of minimizing the loss 𝐿¹ˆ𝜆º=1
2jˆ𝜆𝑘¹𝜆º𝑘j2,
where𝑘=100and ˆ𝜆is generated from two real parameters following standard ( real + imaginary) or
26Resurrecting Recurrent Neural Networks for Long Sequences
0 20 40 60 80 1000.000.020.040.060.080.10Convolution kernel to be learned
0 250 500 750 1000 1250 1500 1750 2000101
100101Training loss over iterations
tanh, lr = 0.0001
tanh, lr = 0.0003
tanh, lr = 0.001
tanh, lr = 0.005
lin, lr = 1e-05
lin, lr = 3e-05
lin, lr = 0.0001
Figure 7jLearning with Adam a one-dimensional convolution with a length- 100kernel using a single-layer RNNs
with linear or tanhrecurrent activations and 100-dimensional hidden state. Initialization is performed using
Glorot on all quantities for both options. For all learning rates in our grid, the linear variant is faster to converge.
0 100 200 300 400 5001027
1023
1019
1015
1011
107
103
Loss evolution (*=0.03)
Re + Im param
Exp. param
0 100 200 300 400 5001028
1024
1020
1016
1012
108
104
100Loss evolution (*=0.35)
Re + Im param
Exp. param
0.78 0.79 0.80 0.81 0.820.540.550.560.570.58Trajectory, Standard param. (*=0.35)
0.01
 0.00 0.01 0.02 0.03 0.040.590.600.610.620.63Trajectory, Exp param. (*=0.35)
Figure 8jExponential parametrization helps when learning a single complex eigenvalue 𝜆=exp¹𝜈𝑖𝜃º,
exponentiated 100times. As 𝜆gets close to the purely imaginary setting 𝜃=𝜋2, the geometry of the loss
landscape under standard real+imaginary parametrization becomes suboptimal for the Adam optimizer, which
works best in the axis-aligned setting (exponential parametrization). In the plot, the square denotes initialization ,
while the star denotes the solution after 500 iterations.
exponential parameterization. Note that in this paragraph 𝜆2ℂdenotes the solution, not the complex
conjugate of 𝜆. In Fig. 8, we show that as the target phase 𝜃approaches 𝜋2(i.e.𝜆gets close to the
imaginary axis), standard parameterization slows down learning, as the corresponding landscape gets non-axis-
aligned  a feature that does not match well the inner workings of the Adam optimizer16, which is a diagonal
preconditioner (Kingma and Ba, 2014). Instead, under exponential parameterization, the eﬀects of phase and
magnitude parameters on the powers of 𝜆are more eﬃciently decouped: for example, while the real part of
𝜆𝑘is simply exp¹𝑘𝜈ºusing exponential parameterization, if standard parameterization is used, Re
𝜆𝑘is a
function of both 𝜆𝑟and𝜆𝑖. We noticed that the performance diﬀerence gets most pronounced when the system
has to learn how to turn: i.e. the initialization magnitude is correct, but the position on the complex plane
is not (this is the precise setting for Figure 8): while for standard parameterization changing the phase 𝜃
requires a careful balance between real and imaginary components, for exponential parameterization gradients
are fully aligned with the phase parameter. This makes the learning more ﬂexible, a feature which we observed
necessary in our experiments on the Long Range Arena, see 3.3 and Tb.2.
E.3. On alternatives to using complex numbers
In this subsection, we show how to derive the canonical real form for a non-symmetric real-valued matrix 𝐴,
which we assume to be diagonalizable in the complex domain (always true up to arbitrary small perturbation
of the entries (Axler, 1997)). This derivation is classical and can be found in many textbooks under the context
ofreal Jordan form (more general), see e.g. Weintraub (2009). Here, we present a simpliﬁed discussion.
After diagonalizing 𝐴, we retrieve a set of purely real eigenvalues (each with multiplicity 1 up to vanishing
perturbations) with corresponding realeigenvectors, and pairs of complex conjugate eigenvalues, with corre-
sponding complex conjugate eigenvectors.
16Forthisproblem,vanillagradientdescentcannotbeeﬀectivelyusedasthelandscapeishighlynon-convex,withchallengingcurvature
vanishing asj𝜆japproaces 0.
27Resurrecting Recurrent Neural Networks for Long Sequences
We recall a proof for the facts above: letdenote the elementwise complex conjugate of any complex quantity.
This operation clearly commutes with multiplication. If 𝜆2ℂis an eigenvalue of 𝐴2ℝ𝑁𝑁with eigenvector
𝑣2ℂ𝑁, then since 𝐴is real-valued we have 𝐴𝑣=¹𝐴𝑣º=¹𝐴𝑣º=¹𝜆𝑣º=𝜆𝑣. Hence,𝜆is an eigenvalue
with eigenvector 𝑣. This also shows that there always does exist a real eigenvector corresponding to each real
eigenvalue: let 𝑣2ℂ𝑁be a complex eivengvector with real eigenvalue 𝜆, then𝑣𝑣2ℝ𝑁is an eigenvector
with eigenvalue 𝜆since, again using the fact that 𝐴is real,𝐴¹𝑣𝑣º=𝐴𝑣𝐴𝑣=𝐴𝑣¹𝐴𝑣º=𝜆¹𝑣𝑣º.
The action of 𝐴on its real eigenvectors (with real eigenvalues) is trivial and analogous to the symmetric case 
this corresponds to a diagonal entry in the diagonalized version of 𝐴. For the subspaces spanned by complex
eigenvalues, the discussion is more interesting: let 𝜆𝜆be a pair of conjugate eigenvalues with corresponding
eigenvectors 𝑣𝑣. Collect𝑣𝑣in a𝑁2matrix𝑉, then
𝐴𝑉=𝑉
𝜆0
0𝜆
=:𝑉Λ (25)
Letusnowchooseadiﬀerent realbasisforthecolumnsof 𝑉,therealandimaginarypartsof 𝑣:𝑉=Re¹𝑣ºIm¹𝑣º¼.
Note that this is a basis, since 𝑣𝑣are linearly independent and can be both written as (complex-weighted)
linear combination of real and imaginary parts of 𝑣. Now note that
𝐴Re¹𝑣º=1
2𝐴¹𝑣𝑣º
=1
2¹𝜆𝑣𝜆𝑣º
=Re¹𝜆𝑣º
=Re¹Re¹𝜆º𝑖Im¹𝜆ºº¹Re¹𝑣º𝑖Im¹𝑣ºº¼
=Re¹𝜆ºRe¹𝑣ºIm¹𝜆ºIm¹𝑣º
Similarly,
𝐴Im¹𝑣º=1
2𝐴¹𝑣𝑣º
=1
2¹𝜆𝑣𝜆𝑣º
=Im¹𝜆𝑣º
=Im¹Re¹𝜆º𝑖Im¹𝜆ºº¹Re¹𝑣º𝑖Im¹𝑣ºº¼
=Re¹𝜆ºIm¹𝑣ºIm¹𝜆ºRe¹𝑣º
This shows that the action of 𝐴on the new realbasis 𝑉is of simple form:
𝐴𝑉=𝑉Re¹𝜆º Im¹𝜆º
Im¹𝜆ºRe¹𝜆º
=:𝑉Λ (26)
This discussion shows that there exist a simple invertible change of basis (from 𝑉to𝑉for all pairs of conjugate
eigenvalues) which makes takes the system back to a simple decomposition in the real domain, both in terms of
eigenvalues and eigenvectors  one simply has to replace all diagonal blocks of form Λwith 22matrices Λ.
The careful reader might recognize that, in the resulting system, matrix multiplication for the 22blocks
is algebraically equivalent to multiplication of the corresponding complex numbers. Hence, while complex
numbers are not per-seneeded to ﬁnd a simple representation of non-symmetric matrices, they are convenient
to work with since the matrix in Eq. (26)is structured: has 4 entries but can be represented using just two 
real and imaginary parts, exactly what a complex number stores in memory.
28Resurrecting Recurrent Neural Networks for Long Sequences
F. Proofs
In this section we provide proofs for the propositions listed in the main paper.
F.1. Proof of Lemma 3.2
We provide here a proof for the following sampling lemma.
Lemma 3.2. Let𝑢1𝑢2be independent uniform random variables on the interval 01¼. Let 0𝑟min𝑟max1.
Compute𝜈=1
2log
𝑢1¹𝑟2
max𝑟2
minº𝑟2
min
and𝜃=2𝜋𝑢2. Then exp¹𝜈𝑖𝜃ºis uniformly distributed on the ring
inℂbetween circles of radii 𝑟minand𝑟max.
Proof.First, note that one can sample phase and magnitude independently by symmetry of the target distribu-
tion. Phase sampling can trivially performed through scaling a uniform distribution.
Next, we consider sampling the magnitude. The area of the ring in between 𝑟minand𝑟maxis𝜋¹𝑟2
max𝑟2
minº,
while the cumulative distribution function for the radius distribution is such that 𝐹𝑟¹𝑟minº=0,𝐹𝑟¹𝑟maxº=1and
for𝑟2𝑟min𝑟max¼we therefore have
𝐹¹𝑟º=𝑟2𝑟2
min
𝑟2max𝑟2
min (27)
Under parametrization of 𝑟using the exponential, 𝑟=𝑒𝜈, one gets
𝐹¹𝑟º=𝑒2𝜈𝑟2
min
𝑟2max𝑟2
min (28)
Finally, we use the inverse sampling theorem (see e.g. Vogel (2002)): one can sample 𝜈using the formula
𝜈=𝐹1¹𝑢º, where𝑢is uniform on01¼. By setting
𝑢=𝑒2𝜈𝑟2
min
𝑟2max𝑟2
min (29)
we get
𝑒2𝜈=¹𝑟2
max𝑟2
minº𝑢𝑟2
min (30)
from which follows that 𝜈=1
2log¹¹𝑟2
max𝑟2
minº𝑢𝑟2
minº. 
F.2. Proof of Proposition 3.3
Validity of this proposition is veriﬁed numerically in Figure 9.
Proposition 3.3 (Forward-pass blow-up) .LetΛbe diagonal with eigenvalues sampled uniformly on the ring in
ℂbetween circles of radii 𝑟min 𝑟max1. Then, under constant or white-noise input and Glorot input projection,
we have that the squared norm of the state 𝑥𝑘converges as 𝑘!1to the following quantity.
𝔼k𝑥1k2
2¼=1
𝑟2max𝑟2
minlog 
1𝑟2
min
1𝑟2max!
𝔼k𝐵𝑢k2
2¼
Proof.Assume ﬁrst (most diﬃcult case) that 𝑢𝑘is constant, i.e. such that 𝐵𝑢𝑘=: 𝑢for all𝑘. Then,
k𝑥1k2
2=1
𝑛=11
𝑚=1𝑢
𝑘𝑚¹Λ𝑚ºΛ𝑛𝑢𝑘𝑛 (31)
=𝑢1
𝑛=11
𝑚=1¹Λ𝑚ºΛ𝑛#
𝑢 (32)
29Resurrecting Recurrent Neural Networks for Long Sequences
0.8 0.9 0.95 0.99
Value of rmax2345678Constant input, rmin=0.75
predicted gain
0.8 0.9 0.95 0.99
Value of rmax345678White noise input, rmin=0.75
predicted gain
Figure 9jNumerical simulation for gain formula derived in Proposition 3.3. Here we chose 𝑁=500,𝐿=
10𝑘(sequence length) and plotted statistics for 10 runs with boxplot indicating median and (5,95) percentile.
Indicated in blue line is our prediction. The formula holds both for constant and random input, yet we notice that
it is more accurate in the random input setting.
Note that Λ=diag¹𝜆1𝜆𝑁ºis diagonal with equally distributed entries on the disk between radii 𝑟minand
𝑟max. One can then sample a generic entry 𝜆using the change of variables formula for probabilities (Jeﬀreys,
1998) as follows (see also Lemma 3.2):
𝜆=𝑟1
2𝑒𝑖2𝜋𝜃 𝑟U𝑟2
min𝑟2
max¼ 𝜃U 01¼ (33)
Where crucially 𝑟and𝜃are independent. Let 𝕋¹𝑟min𝑟maxº=f𝜆2ℂ:j𝜆j2𝑟min𝑟max¼g. We need to study the
following quantity:
𝔼𝜆𝕋¹𝑟min𝑟maxº1
𝑛=11
𝑚=1𝜆𝑛¹𝜆𝑚º#
=𝔼𝑟𝜃1
𝑛=11
𝑚=1𝑟1
2¹𝑛𝑚º𝑒𝑖2𝜋¹𝑛𝑚º𝜃#
(34)
=1
𝑛=11
𝑚=1𝔼𝑟h
𝑟1
2¹𝑛𝑚ºi
𝔼𝜃h
𝑒𝑖2𝜋¹𝑛𝑚º𝜃i
(35)
The expectation w.r.t 𝜃is non-zero only if 𝑛=𝑚, therefore
𝔼𝜆𝕋¹𝑟min𝑟maxº1
𝑛=11
𝑚=1𝜆𝑛¹𝜆𝑚º#
=1
𝑛=1𝔼𝑟𝑟𝑛¼ (36)
=𝔼𝑟1
𝑛=1𝑟𝑛#
(37)
=𝔼𝑟1
1𝑟
(38)
=1
𝑟2max𝑟2
min𝑟2
max
𝑟2
min1
1𝑟𝑑𝑟 (39)
=1
𝑟2max𝑟2
min¹log¹j1𝑟2
maxjºlog¹j1𝑟2
minjºº (40)
=1
𝑟2max𝑟2
minlog 
1𝑟2
min
1𝑟2max!
 (41)
Thewhite noise input case is simpler. Let us start from k𝑥1k2
2=Í1
𝑛=1Í1
𝑚=1𝑢
𝑘𝑚¹𝐴𝑚º𝐴𝑛𝑢𝑘𝑛. Now, we can
retrieve the single sum by the fact that 𝐴is diagonal and 𝔼𝑢
𝑘𝑚𝑢𝑘𝑛¼=0for𝑚𝑛. The rest of the proof is
identical, and presented in the main paper for the one-simensional setting. 
30
  Better speech synthesis through scaling
James Betker
Abstract
In recent years, the field of image generation has been revolutionized by the appli-
cation of autoregressive transformers and DDPMs. These approaches model the
process of image generation as a step-wise probabilistic processes and leverage
large amounts of compute and data to learn the image distribution.
This methodology of improving performance need not be confined to images. This
paper describes a way to apply advances in the image generative domain to speech
synthesis. The result is TorToise - an expressive, multi-voice text-to-speech sys-
tem.
All model code and trained weights have been open-sourced at
https://github.com/neonbjb/tortoise-tts.
1 Background
1.1 Text-to-speech
The field of text-to-speech (TTS) research has been largely constrained to the development of effi-
cient models trained on relatively small datasets. This choice has been driven by:
1. The desire to build efficient speech generation models that can be deployed at scale and
thus must have a high sampling rate.
2. The unavailability of very large, transcribed speech datasets.
3. Challenges scaling the encoder-decoder model architectures traditionally used in TTS.
1.1.1 Neural MEL Inverters
Most modern text-to-speech systems operate on speech data that is encoded as a MEL spectrogram.
There are many compelling reasons to operate in this encoding space, but for neural networks, the
most compelling reason is that it is highly spatially compressed. The MEL configuration used by
the Tacotron, for example, operates at 256x compression over raw audio waveform data sampled at
22kHz, but contains most of the information found in that data.
Because of this, an entire body of research has been dedicated to finding high-quality ways to decode
MEL spectrograms back into audio waveforms. A synthesizer that performs this task is generally
called a vocoder, but I more generally refer to it as a MEL inverter in this paper.
Modern MEL inverters built on neural networks are incredibly sophisticated. They produce wave-
forms that are nearly indistinguishable from recorded waveforms to human ears, and they are highly
generalizable outside of their training set. I capitalize on this work by using an implementation of
Univnet(Kim, 2021) as a final stage for my text-to-speech system.
1.2 Image generation
While TTS systems largely focus on latency, this has not been the case in other domains. For
example, with image generation, more focus has been applied to training models that generate high-arXiv:2305.07243v2  [cs.SD]  23 May 2023quality results, regardless of the sampling time. For the purposes of this paper, I dive into two bodies
of research:
1.2.1 DALL-E
DALL-E(Ramesh et al., 2021) showed how an autoregressive decoder can be applied to text-to-
image generation. This is particularly appealing because of the vast quantity of research that has
been poured into scaling decoder-only models in the NLP domain.
Two important problems persist with DALL-E: first, it relies on full-sequence self-attention, which
carries a cost of O(N2)compute and memory, where N is the sequence length. This is particularly
troublesome when dealing with modalities like images or audio, which have large sequence lengths
when dealt with naively.
Second, traditional autoregressive approaches require operating in the discrete domain. Images are
encoded into sequences of discrete tokens using a quantizing autoencoder. DALL-E then models
these sequences of tokens using an autoregressive prior model. This is a strength of DALL-E in
terms of expressiveness, but it comes at the cost of requiring a decoder which can convert these
image tokens back into the pixel values that actually comprise an image. It is my opinion that
learned VQV AE decoder used by DALL-E is principally responsible for the blurry incoherence
exhibited by most of its samples.
1.2.2 DDPMs
The generative model space has long been plagued by models that either exhibit mean-seeking
behavior (resulting in blurriness) or mode-collapse (resulting in a lack of diversity or generalization).
Denoising diffusion probabilistic models (DDPMs(Ho et al., 2020)) have recently arisen as the first
type of generative model capable of producing crisp, coherence and diverse images. These models
have been shown to be quite effective at using low-quality guidance signals to reconstruct the high-
dimensional space that those guidance signals were derived from. Put another way, they are great at
super-resolution.
There are two important caveats to DDPMS:
1. Traditional approaches to DDPMs rely on fixed output shapes that are known before sam-
pling begins. As a concrete example relevant to this paper, DDPMs cannot learn to convert
text into audio signals because they cannot solve the implicit alignment problem between
text and audio.
2. DDPMs must be sampled from over multiple iterations. This sampling process consumes
a great deal of compute, and means sampling from a DDPM will always incur a significant
latency cost.
1.2.3 Re-ranking
DALL-E introduced the process of re-ranking the outputs of autoregressive models. This process
samples randomly from the autoregressive model and picks the highest quality output of k outputs
for downstream use.
Such a procedure requires a strong discriminator: a model that can tell good text/image pairings
from bad. DALL-E used CLIP(Radford et al., 2021), a model trained with a contrastive text and
image pairing objective.
2 Methods
2.1 Joining Autoregressive Decoders and DDPMs
To review some of the conclusions drawn above:
1. Autoregressive models are strong at converting between unaligned domains like vision, text
and speech.
2Figure 1: TorToise-v2 architectural design diagram. Inputs of text and a reference audio clip (for speaker
cloning) flow through a series of decoding and filtering networks to produce high-quality speech.
2. DDPMs operate in the continuous domain which allows them to model expressive modali-
ties.
Both types of models have demonstrated the ability to scale performance with additional compute
and data.
It becomes evident that when posed with a problem like generating continuous data like speech
spectrograms or images, a marriage of these two approaches might have some distinct advantages.
Specifically, in inference, the autoregressive model will be used to convert a sequence of text tokens
to a sequence of tokens representing the output space (in our case, speech tokens). The DDPM will
then be used to decode these tokens into a high quality representation of speech.
2.2 Applying Autoregression+DDPMs to TTS
To build out the previously proposed system, we need to train the following neural networks:
1. An autoregressive decoder which predicts a probability distribution for speech tokens, con-
ditioned on text.
2. A contrastive model similar to CLIP which is used to rank outputs of the autoregressive
decoder.
3. A DDPM which can convert speech tokens back into speech spectrograms.
The architectures and training process for all of these networks largely follow the procedures found
in their respective literature. Details can be found in B
2.2.1 Conditioning Input
A unique design choice made with TorToise is an additional input which is provided to both the
autoregressive generator and the DDPM, which I term the speech conditioning input.
The speech conditioning input starts as one or more audio clips of the same speaker as the target.
These clips are converted to MEL spectrograms and fed through an encoder consisting of a stack
of self-attention layers. The autoregressive generator and the DDPM have their own conditioning
encoders, both of which are learned alongside their respective networks.
The output of these layers is averaged to produce a single vector. The vectors from all of the encoded
conditioning clips are then averaged again before being fed as an input into the autoregressive or
conditioning networks.
3The intuition behind the conditioning input is that it provides a way for the models to infer vocal
characteristics like tone and prosody such that the search space of possible speech outputs corre-
sponding to a given textual input is greatly reduced.
2.2.2 The TorToise Trick
For the majority of the training procedure, the DDPM is trained to convert discrete speech codes
into MEL spectrograms. After this process has converged, I fine-tune the DDPM on the autoregres-
sive latent space which is pulled from the AR model outputs instead of the speech codes. This is
described in detail in B.
The logic here is that the AR latent space is far more semantically rich than discrete tokens. By
fine-tuning on this latent space, we improve the efficiency of the downstream diffusion model. I
liken this to recent work showing that training decoder models conditioned on frozen text encoders
to produce large efficiency gains. This fine-tuning is one of the greatest contributors to model output
quality of any of the tweaks I made to the various model training processes.
2.3 CLVP
As mentioned earlier, a good strategy for gathering expressive outputs from generative models is
using a qualitative discriminator to re-rank several outputs, then choosing only the best. DALL-E
uses CLIP for this.
This same type of approach used for CLIP can be applied to speech: after all, most TTS datasets are
simply pairings of audio clips and text. By training a model on these pairs in a contrastive setting,
the model becomes a good discriminator for speech.
For TorToise, I train the Contrastive Language-V oice Pretrained Transformer, or CLVP. It has many
of the same properties of CLIP, but notably serves as a scoring model for use in re-ranking TTS
outputs from the AR model.
To make this work efficiently in inference, I trained CLVP to pair discretized speech tokens with
text tokens. This way, CLVP can rerank multiple AR outputs without the expensive diffusion model
being invoked.
3 Training
These models were trained on a small cluster of 8 NVIDIA RTX-3090s over the period of 1 year.
Specifics on how these models are trained can be found in B.
4 Inference Process
Once the four models of the framework are fully trained, the inference procedure is as follows:
1. Feed the conditioning inputs and the text into the autoregressive model and decode a large
number of output candidates.
2. Use CLVP to produce correlation scores between each speech candidate and text.
3. Choose the top k speech candidates, and for each candidate:
4. Decode to a MEL spectrogram using the DDPM.
5. Convert to a waveform using a conventional vocoder.
6. When decoding the autoregressive model, nucleus sampling is used with P=.8, repetition
penalty=2 and softmax temperature=.8.
Sampling from DDPMs is a highly studied and rapidly changing field. At the time TorToise was
designed, I found the sampling configuration with the best balance between quality and inference
speed to be as follows:
1. Algorithm: DDIM(Song et al., 2022)
42. Schedule: Linear
3. Sampling steps: 64
4. Conditioning-Free Guidance constant: 2
5 The Dataset
Since my goal was to train what is essentially a large language model, I needed a lot of data. I started
with the LibriTTS(Zen et al., 2019) and HiFiTTS(Bakhturina et al., 2021) datasets, which combined
contain 896 hours of transcribed speech. I built an additional, extended dataset of 49,000 hours of
speech audio from audiobooks and podcasts scraped from the internet. Details on how this dataset
was built are in appendix I. The official LibriTTS test split was used for validation purposes.
6 Experiments
Text to speech systems are challenging to experimentally compare because many state of the art sys-
tems are closed source with few samples to compare against. To this end, I built my own evaluation
suite which uses CLVP to produce a distance metric between real samples and generated samples,
similar to the FID score used by images. I also use an open source wav2vec model to characterize
the intelligibility of a speech segment. I have open sourced this work here.
Past this, comparisons between the samples generated from TorToise and those generated by other
papers can be found here.
7 Conclusion
TorToise is the latest in a line of recent state-of-the-art breakthroughs that use general model archi-
tectures. Almost no part of TorToise was designed specifically for audio processing, yet it outper-
forms all previous TTS models in realism. It does this by: Embracing generalist architectures like
stacks of transformer layers. Leveraging a large, high-quality dataset. Training at large-ish scale and
high batch size.
My main take-away from this project is how incredibly strong the results are from adhering to the
above 3 points. It seems likely to me that any digitized modality is subject to generative modeling
using this framework.
References
Bakhturina, E., Lavrukhin, V ., Ginsburg, B., and Zhang, Y . (2021). Hi-fi multi-speaker english tts
dataset.
Ho, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models.
Kim, J. (2021). Mindslab UnivNet implementation.
Nichol, A. and Dhariwal, P. (2021). Improved denoising diffusion probabilistic models.
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A.,
Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. (2021). Learning transferable visual models
from natural language supervision.
Ramesh, A., Pavlov, M., Goh, G., Gray, S., V oss, C., Radford, A., Chen, M., and Sutskever, I.
(2021). Zero-shot text-to-image generation.
Seonghyeon, K. (2019). VQV AE (rosinality).
Song, J., Meng, C., and Ermon, S. (2022). Denoising diffusion implicit models.
Wang, P. (2020). Vector Quantize (Lucidrains).
Wang, P. (2021). x-transformers (Lucidrains).
5Zen, H., Dang, V ., Clark, R., Zhang, Y ., Weiss, R. J., Jia, Y ., Chen, Z., and Wu, Y . (2019). Libritts:
A corpus derived from librispeech for text-to-speech.
A Extended Dataset Collection
I independently built an extended TTS dataset composed of audiobooks and podcasts scraped from
the web. This data was split on 500ms silences, and any audio clip between 5-20 seconds was kept.
I then fed the resulting clips through a pipeline of classifiers that I trained which remove any audio
with background noise, music, poor quality (such as phone calls), multiple voices speaking at once
and reverb. Due to disk space limitations, I was forced to limit the amount of scraping. The end
result was 49,000 hours of cleaned audio clips.
I transcribed this dataset using a wav2vec2-large model. I personally fine-tuned this model to predict
punctuation, as quotation marks, commas and exclamation marks are important for the purposes of
generating speech but are not generally included in the training of speech recognition models. Fine-
tuning was performed on LibriTTS and HiFiTTS and the pretrained model weights and transcription
scripts can be found here.
B Training and Architecture Details
B.1 VQVAE
The VQV AE used with TorToise is most similar to that of the original VQV AE by van der Oord
et al. It operates on MEL spectrograms. It consists of a small residual convolutional network that
compresses the spectrogram an additional 4x and produces a codebook consisting of 8192 tokens.
When training the VQV AE, I found that larger batch sizes decrease reconstruction losses, and thus
used a very large batch size for my infrastructure. Input samples were constricted to 40960 PCM
readings, or 2 seconds of audio. The primary bottleneck for training the VQV AE was the dataloader.
6Figure 2: Training curves for VQV AE. Y-axis is MSE loss in log-log scale. X-axis is number of training steps.
Model shape 1D Conv resnet, encoder + decoder
Top dim 512
Bottom dim 1024
Codebook dim 256
Quantizer token count 8192
Quantization algorithm Clustering a la original VQV AE, no restart
Batch size 8192
Total training 360M samples
Losses MSE reconstruction loss, commitment loss
LR 3e-4
B1, B2 .9 .9999
Weight decay .01
EMA weights replaces LR decay with rate .999
Table 1: VQV AE model details & hyperparameters
7B.2 Autoregressive Prior
The AR decoder uses a bog-standard GPT-2 architecture and generally follows the training instruc-
tions from the DALLE-1 paper. Unlike DALL-E, only dense self-attention is used. The prompt is
assembled as follows:
<SC> <BT> <T> <T> <T>..<T> <ET> <BM> <M> <M> <M>...<EM>
SC=Speech c o n d i t i o n i n g e n c o d i n g
BT=Begin t e x t t o k e n
T= Text t o k e n s
ET=End t e x t t o k e n
BM=Begin MEL t o k e n
M =MEL t o k e n s
EM=End MEL t o k e n
Speech conditioning encodings are learned by a separate encoder that takes in the MEL spectrogram
of a related clip (another clip of the same person speaking) and produces a single vector embedding
that is placed at the front of the attention context. Two encodings were produced for each training
sample, which are averaged together. The maximum input length to the conditioning encoder is
132,300 samples, or 6 seconds of audio.
Learned positional embeddings are used. The MEL tokens and the text tokens get their own posi-
tional parameters. Text inputs are unpadded, MEL tokens are right padded to conform the sequence
length of each batch. The maximum sequence length is 402 text tokens + 604 MEL tokens. For
efficiency reasons, in the first half of training, the model only saw 6 second audio clips. After this,
audio clips up to the full length ( 27 seconds) were seen.
8Figure 3: Early training curves in log-log scale. Y-axis is cross entropy loss for MEL tokens. X-axis is number
of training steps. Does not include a long tail of training and fine-tuning due to online changes that were made
adding non-reproducible noise to curves.
Model architecture Transformer stack with causal masking
Layers 30
Model dim 1024
Attention heads 16
Text tokenization Custom BPE, 256 tokens wide.
Batch size 1024
Total training 119M samples
Text, next token prediction, loss weight .01
MEL token, next token prediction weight 1
LR 1e-4
B1, B2 .9 .96
Weight decay .01
LR Warmup 500 steps
EMA decay rate .999
Table 2: AR prior details & hyperparameters
After training the autoregressive decoder to convergence, I fine-tuned it on the clean audio datasets
from LibriTTS and HIFITTS.
9B.3 CLVP
The original DALLE worked by decoding a large number of images for a given text prompt, which
were then fed through CLIP. The image that CLIP deemed closest to the input text was used as the
final output.
I continue following this lead for TorToise, for reasons that will become evident in the results section.
I built a simple model that is very similar to CLIP, which I call a Contrastive Language-V oice
Pretrained model, or CLVP. Like CLIP, this model produces distance metrics for text/speech pairs.
CLVP uses an architecture similar to the CLIP text encoder, except it uses two of them: one for text
tokens and the other for MEL tokens. Tokens from both encoders were dropped out at a rate of 15%.
Fixed positional embeddings were used. Maximum text input length was 350 tokens (in practice
never actually seen). Maximum MEL token input length was 293, or 13 seconds of audio.
Figure 4: Late training curves for CLVP in log-log scale. Y-axis is cross entropy loss. X-axis is number of
samples. Early training curves were lost.
Model architecture Dual transformer stacks
Depth 20
Model dim 768
Attention heads 12
Text tokenization Custom BPE, 256-token wide
Batch size 1024
Total training 80M samples.
Losses Contrastive
LR 3e-4
B1, B2 .9 .96
Weight decay .001
LR Warmup 500 steps
EMA decay rate .999
Table 3: CLVP training details & hyperparameters
10B.4 Diffusion Decoder
The diffusion model uses a bespoke architecture that combines residual convolutions with dense
self-attention. It most closely resembles the traditional U-Net model used for DDPMs but without
any upsampling or downsampling.
The diffusion model receives 3 sources of conditioning: The timestep signal, which modulates the
scale and shift of the group norms used by the network. A speech conditioning signal, which also
modulates the scale and shift of the group norms. The final activations of the autoregressive model.
In training the diffusion model, I iterated through several different architectures and conditioning
types before settling on this one. This includes: Architecture: A traditional U-net with attention
was tried. The full attention network performed significantly better in frechet distance evaluations.
Operating on PCM data rather than MELs. This necessitated very small context windows and still
took an inordinate amount of time to train. The results of decoding a MEL and using a vocoder
resulted in substantially better quality. In order to force compatibility with existing diffusion noise
schedules, I rescale input MELs to be on the interval [-1,1]. Decoding MEL tokens versus AR
activations. Training on AR activations is expensive because during each training step you must
forward prop through the AR network. However, training on AR activations constituted the single
greatest jump in output quality of any design decision made for the diffusion network. It is possible
that doing tricks like putting the text on the attention context may ablate this advantage somewhat.
As with image diffusion models, exploiting classifier-free guidance is extremely important for high
quality outputs. In the case of TorToise, I perform guidance on both the speech conditioning signal
and the activations of the AR model. During training, 15% of the time, both of these signals are
dropped out and replaced with a learned embedding.
When training the diffusion decoder, input audio was clipped randomly to 220,500 samples, or 10
seconds of audio. Conditioning inputs were clipped to 102,400 samples, or 5 seconds of audio.
While the rest of the TorToise stack operates at an audio sampling rate of 22kHz, the diffusion
decoder outputs MEL spectrograms which were computed from 24kHz audio. This discrepancy is
solely to ensure compatibility with the pretrained Univnet vocoder which the model stack uses, and
was not done for any performance reasons.
Figure 5: Diffusion model losses, log-log scale. Y-axis: MSE loss, X-axis: training samples.
11Model shape Alternating full attention + conv resblocks
Depth 10
Model dim 1024
Attention heads 16
Batch size 512
Total Training 65M samples
Losses MSE (weight 1) + VLB (weight n)
LR 1e-5
B1, B2 .9, .999
Weight decay .001
LR Warmup 1000 steps
EMA decay rate .999
Table 4: Diffusion decoder details & hyperparameters
C Future Work
TorToise is the product of playing way over my paygrade, so to speak. As an independent researcher,
I only had a small number of GPUs to perform my experiments with, and made many mistakes in
the process. Following are recommendations for architectural tweaks to be made in future work
building off of TorToise:
1. Constrict VQV AE codebook embedding dim. This has been experimentally shown to pro-
duce drastic performance improvements.
2. Relative positional encodings. The AR model uses fixed positional encodings, which limits
the total amount of speech it can produce. Using relative encodings would allow arbitrary
length sequences.
3. Train CLVP on larger batch sizes. Contrastive models benefit from extremely large batch
sizes.
4. Train CLVP on longer audio sequences. CLVP only ever saw 13 second clips, which is
likely why re-ranking on longer samples suffers.
5. Diffusion decoder architecture. The diffusion decoder is an attentional network that omits
Feedforward blocks. In retrosepct, this was a poor design decision and feed-forward blocks
should be included.
6. Train the entire model stack at 24kHz or re-train Univnet at 22kHz sampling rates.
7. Train on more data for longer. The training curves for TorToise indicate that we were far
from overfitting. Simply training longer likely would have improved results.
D Special Thanks
More than the prior work done by the research community, this project was a product of the open
source community. I wanted to thank a few extra contributors who have not already been mentioned
above whose work I found instrumental in building TorToiSe:
1. Phil Wang, who authored Wang (2021) and Wang (2020).
2. Kim Seonghyeon, who authored Seonghyeon (2019).
3. FAIR, who maintain most of the tooling I use and who open sourced much of the technol-
ogy underpinning TorToiSe.
4. Prafulla Dhariwal and Alex Nichol, without whose Nichol and Dhariwal (2021) I would
still be in GAN hell.
I also want to thank my wife, Kim Betker, who supported me through two years of high electricity
bills, a hot & noisy utility room, and the many late nights required to build this system.
12
  Published as a conference paper at ICLR 2021
ANIMAGE IS WORTH 16X16 W ORDS :
TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE
Alexey Dosovitskiy;y, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,
Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby;y
equal technical contribution,yequal advising
Google Research, Brain Team
fadosovitskiy, neilhoulsby g@google.com
ABSTRACT
While the Transformer architecture has become the de-facto standard for natural
language processing tasks, its applications to computer vision remain limited. In
vision, attention is either applied in conjunction with convolutional networks, or
used to replace certain components of convolutional networks while keeping their
overall structure in place. We show that this reliance on CNNs is not necessary
and a pure transformer applied directly to sequences of image patches can perform
very well on image classiﬁcation tasks. When pre-trained on large amounts of
data and transferred to multiple mid-sized or small image recognition benchmarks
(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent
results compared to state-of-the-art convolutional networks while requiring sub-
stantially fewer computational resources to train.1
1 I NTRODUCTION
Self-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become
the model of choice in natural language processing (NLP). The dominant approach is to pre-train on
a large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks
to Transformers computational efﬁciency and scalability, it has become possible to train models of
unprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the
models and datasets growing, there is still no sign of saturating performance.
In computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989;
Krizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining
CNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing
the convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while
theoretically efﬁcient, have not yet been scaled effectively on modern hardware accelerators due to
the use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet-
like architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al.,
2020).
Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard
Transformer directly to images, with the fewest possible modiﬁcations. To do so, we split an image
into patches and provide the sequence of linear embeddings of these patches as an input to a Trans-
former. Image patches are treated the same way as tokens (words) in an NLP application. We train
the model on image classiﬁcation in supervised fashion.
When trained on mid-sized datasets such as ImageNet without strong regularization, these mod-
els yield modest accuracies of a few percentage points below ResNets of comparable size. This
seemingly discouraging outcome may be expected: Transformers lack some of the inductive biases
1Fine-tuning code and pre-trained models are available at https://github.com/
google-research/vision_transformer
1arXiv:2010.11929v2  [cs.CV]  3 Jun 2021Published as a conference paper at ICLR 2021
inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well
when trained on insufﬁcient amounts of data.
However, the picture changes if the models are trained on larger datasets (14M-300M images). We
ﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent
results when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When
pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches
or beats state of the art on multiple image recognition benchmarks. In particular, the best model
reaches the accuracy of 88:55% on ImageNet, 90:72% on ImageNet-ReaL, 94:55% on CIFAR-100,
and77:63% on the VTAB suite of 19 tasks.
2 R ELATED WORK
Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-
come the state of the art method in many NLP tasks. Large Transformer-based models are often
pre-trained on large corpora and then ﬁne-tuned for the task at hand: BERT (Devlin et al., 2019)
uses a denoising self-supervised pre-training task, while the GPT line of work uses language mod-
eling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020).
Naive application of self-attention to images would require that each pixel attends to every other
pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,
to apply Transformers in the context of image processing, several approximations have been tried in
the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query
pixel instead of globally. Such local multi-head dot-product self attention blocks can completely
replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different
line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-
attention in order to be applicable to images. An alternative way to scale attention is to apply it in
blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho
et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate
promising results on computer vision tasks, but require complex engineering to be implemented
efﬁciently on hardware accelerators.
Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 22
from the input image and applies full self-attention on top. This model is very similar to ViT,
but our work goes further to demonstrate that large scale pre-training makes vanilla transformers
competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020)
use a small patch size of 22pixels, which makes the model applicable only to small-resolution
images, while we handle medium-resolution images as well.
There has also been a lot of interest in combining convolutional neural networks (CNNs) with forms
of self-attention, e.g. by augmenting feature maps for image classiﬁcation (Bello et al., 2019) or by
further processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018;
Carion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classiﬁcation (Wu
et al., 2020), unsupervised object discovery (Locatello et al., 2020), or uniﬁed text-vision tasks (Chen
et al., 2020c; Lu et al., 2019; Li et al., 2019).
Another recent related model is image GPT (iGPT) (Chen et al., 2020a), which applies Transformers
to image pixels after reducing image resolution and color space. The model is trained in an unsu-
pervised fashion as a generative model, and the resulting representation can then be ﬁne-tuned or
probed linearly for classiﬁcation performance, achieving a maximal accuracy of 72% on ImageNet.
Our work adds to the increasing collection of papers that explore image recognition at larger scales
than the standard ImageNet dataset. The use of additional data sources allows to achieve state-of-
the-art results on standard benchmarks (Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020).
Moreover, Sun et al. (2017) study how CNN performance scales with dataset size, and Kolesnikov
et al. (2020); Djolonga et al. (2020) perform an empirical exploration of CNN transfer learning from
large scale datasets such as ImageNet-21k and JFT-300M. We focus on these two latter datasets as
well, but train Transformers instead of ResNet-based models used in prior works.
2Published as a conference paper at ICLR 2021
Transformer 
Encoder
MLP 
Head
Vision 
Transformer 
(ViT)*
Linear 
Projection 
of 
Flattened 
Patches
*
 
Extra 
learnable
     
[class]
 
embedding
1
2
3
4
5
6
7
8
90Patch 
+ 
Position 
Embedding
Class
Bird
Ball
Car
...
Embedded 
Patches
Multi-Head 
Attention
Norm
MLP
Norm
+
L
 
x
+Transformer 
Encoder
Figure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,
add position embeddings, and feed the resulting sequence of vectors to a standard Transformer
encoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable
classiﬁcation token to the sequence. The illustration of the Transformer encoder was inspired by
Vaswani et al. (2017).
3 M ETHOD
In model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.
An advantage of this intentionally simple setup is that scalable NLP Transformer architectures  and
their efﬁcient implementations  can be used almost out of the box.
3.1 V ISION TRANSFORMER (VIT)
An overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D
sequence of token embeddings. To handle 2D images, we reshape the image x2RHWCinto a
sequence of ﬂattened 2D patches xp2RN(P2C), where (H;W )is the resolution of the original
image,Cis the number of channels, (P;P)is the resolution of each image patch, and N=HW=P2
is the resulting number of patches, which also serves as the effective input sequence length for the
Transformer. The Transformer uses constant latent vector size Dthrough all of its layers, so we
ﬂatten the patches and map to Ddimensions with a trainable linear projection (Eq. 1). We refer to
the output of this projection as the patch embeddings.
Similar to BERTs [class] token, we prepend a learnable embedding to the sequence of embed-
ded patches ( z0
0=xclass), whose state at the output of the Transformer encoder ( z0
L) serves as the
image representation y(Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at-
tached to z0
L. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training
time and by a single linear layer at ﬁne-tuning time.
Position embeddings are added to the patch embeddings to retain positional information. We use
standard learnable 1D position embeddings, since we have not observed signiﬁcant performance
gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting
sequence of embedding vectors serves as input to the encoder.
The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-
attention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before
every block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).
3Published as a conference paper at ICLR 2021
The MLP contains two layers with a GELU non-linearity.
z0= [xclass;x1
pE;x2
pE;;xN
pE] +Epos;E2R(P2C)D;Epos2R(N+1)D(1)
z0
= MSA(LN( z1)) +z1;  = 1:::L (2)
z= MLP(LN( z0
)) +z0
;  = 1:::L (3)
y= LN( z0
L) (4)
Inductive bias. We note that Vision Transformer has much less image-speciﬁc inductive bias than
CNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are
baked into each layer throughout the whole model. In ViT, only MLP layers are local and transla-
tionally equivariant, while the self-attention layers are global. The two-dimensional neighborhood
structure is used very sparingly: in the beginning of the model by cutting the image into patches and
at ﬁne-tuning time for adjusting the position embeddings for images of different resolution (as de-
scribed below). Other than that, the position embeddings at initialization time carry no information
about the 2D positions of the patches and all spatial relations between the patches have to be learned
from scratch.
Hybrid Architecture. As an alternative to raw image patches, the input sequence can be formed
from feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding
projection E(Eq. 1) is applied to patches extracted from a CNN feature map. As a special case,
the patches can have spatial size 1x1, which means that the input sequence is obtained by simply
ﬂattening the spatial dimensions of the feature map and projecting to the Transformer dimension.
The classiﬁcation input embedding and position embeddings are added as described above.
3.2 F INE-TUNING AND HIGHER RESOLUTION
Typically, we pre-train ViT on large datasets, and ﬁne-tune to (smaller) downstream tasks. For
this, we remove the pre-trained prediction head and attach a zero-initialized DKfeedforward
layer, where Kis the number of downstream classes. It is often beneﬁcial to ﬁne-tune at higher
resolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020). When feeding images
of higher resolution, we keep the patch size the same, which results in a larger effective sequence
length. The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints),
however, the pre-trained position embeddings may no longer be meaningful. We therefore perform
2D interpolation of the pre-trained position embeddings, according to their location in the original
image. Note that this resolution adjustment and patch extraction are the only points at which an
inductive bias about the 2D structure of the images is manually injected into the Vision Transformer.
4 E XPERIMENTS
We evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the
hybrid. To understand the data requirements of each model, we pre-train on datasets of varying size
and evaluate many benchmark tasks. When considering the computational cost of pre-training the
model, ViT performs very favourably, attaining state of the art on most recognition benchmarks at
a lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show
that self-supervised ViT holds promise for the future.
4.1 S ETUP
Datasets. To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes
and 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with
21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and
303M high-resolution images. We de-duplicate the pre-training datasets w.r.t. the test sets of the
downstream tasks following Kolesnikov et al. (2020). We transfer the models trained on these
dataset to several benchmark tasks: ImageNet on the original validation labels and the cleaned-up
ReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al.,
2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008). For these datasets, pre-processing
follows Kolesnikov et al. (2020).
4Published as a conference paper at ICLR 2021
Model Layers Hidden size D MLP size Heads Params
ViT-Base 12 768 3072 12 86M
ViT-Large 24 1024 4096 16 307M
ViT-Huge 32 1280 5120 16 632M
Table 1: Details of Vision Transformer model variants.
We also evaluate on the 19-task VTAB classiﬁcation suite (Zhai et al., 2019b). VTAB evaluates
low-data transfer to diverse tasks, using 1 000 training examples per task. The tasks are divided into
three groups: Natural  tasks like the above, Pets, CIFAR, etc. Specialized  medical and satellite
imagery, and Structured  tasks that require geometric understanding like localization.
Model Variants. We base ViT conﬁgurations on those used for BERT (Devlin et al., 2019), as
summarized in Table 1. The Base and Large models are directly adopted from BERT and we
add the larger Huge model. In what follows we use brief notation to indicate the model size and
the input patch size: for instance, ViT-L/16 means the Large variant with 1616input patch size.
Note that the Transformers sequence length is inversely proportional to the square of the patch size,
thus models with smaller patch size are computationally more expensive.
For the baseline CNNs, we use ResNet (He et al., 2016), but replace the Batch Normalization lay-
ers (Ioffe & Szegedy, 2015) with Group Normalization (Wu & He, 2018), and used standardized
convolutions (Qiao et al., 2019). These modiﬁcations improve transfer (Kolesnikov et al., 2020),
and we denote the modiﬁed model ResNet (BiT). For the hybrids, we feed the intermediate fea-
ture maps into ViT with patch size of one pixel. To experiment with different sequence lengths,
we either (i) take the output of stage 4 of a regular ResNet50 or (ii) remove stage 4, place the same
number of layers in stage 3 (keeping the total number of layers), and take the output of this extended
stage 3. Option (ii) results in a 4x longer sequence length, and a more expensive ViT model.
Training & Fine-tuning. We train all models, including ResNets, using Adam (Kingma & Ba,
2015) with1= 0:9,2= 0:999, a batch size of 4096 and apply a high weight decay of 0:1, which
we found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common
practices, Adam works slightly better than SGD for ResNets in our setting). We use a linear learning
rate warmup and decay, see Appendix B.1 for details. For ﬁne-tuning we use SGD with momentum,
batch size 512, for all models, see Appendix B.1.1. For ImageNet results in Table 2, we ﬁne-tuned at
higher resolution: 512for ViT-L/16 and 518for ViT-H/14, and also used Polyak & Juditsky (1992)
averaging with a factor of 0:9999 (Ramachandran et al., 2019; Wang et al., 2020b).
Metrics. We report results on downstream datasets either through few-shot or ﬁne-tuning accuracy.
Fine-tuning accuracies capture the performance of each model after ﬁne-tuning it on the respective
dataset. Few-shot accuracies are obtained by solving a regularized least-squares regression problem
that maps the (frozen) representation of a subset of training images to f1;1gKtarget vectors. This
formulation allows us to recover the exact solution in closed form. Though we mainly focus on
ﬁne-tuning performance, we sometimes use linear few-shot accuracies for fast on-the-ﬂy evaluation
where ﬁne-tuning would be too costly.
4.2 C OMPARISON TO STATE OF THE ART
We ﬁrst compare our largest models  ViT-H/14 and ViT-L/16  to state-of-the-art CNNs from
the literature. The ﬁrst comparison point is Big Transfer (BiT) (Kolesnikov et al., 2020), which
performs supervised transfer learning with large ResNets. The second is Noisy Student (Xie et al.,
2020), which is a large EfﬁcientNet trained using semi-supervised learning on ImageNet and JFT-
300M with the labels removed. Currently, Noisy Student is the state of the art on ImageNet and
BiT-L on the other datasets reported here. All models were trained on TPUv3 hardware, and we
report the number of TPUv3-core-days taken to pre-train each of them, that is, the number of TPU
v3 cores (2 per chip) used for training multiplied by the training time in days.
Table 2 shows the results. The smaller ViT-L/16 model pre-trained on JFT-300M outperforms BiT-L
(which is pre-trained on the same dataset) on all tasks, while requiring substantially less computa-
tional resources to train. The larger model, ViT-H/14, further improves the performance, especially
on the more challenging datasets  ImageNet, CIFAR-100, and the VTAB suite. Interestingly, this
5Published as a conference paper at ICLR 2021
Ours-JFT Ours-JFT Ours-I21k BiT-L Noisy Student
(ViT-H/14) (ViT-L/16) (ViT-L/16) (ResNet152x4) (EfﬁcientNet-L2)
ImageNet 88:550:04 87:760:03 85:300:02 87:540:02 88:4=88:5
ImageNet ReaL 90:720:05 90:540:03 88:620:05 90:54 90 :55
CIFAR-10 99:500:06 99:420:03 99:150:03 99:370:06
CIFAR-100 94:550:04 93:900:05 93:250:05 93:510:08
Oxford-IIIT Pets 97:560:03 97:320:11 94:670:15 96:620:23
Oxford Flowers-102 99:680:0299:740:0099:610:02 99:630:03
VTAB (19 tasks) 77:630:23 76:280:46 72:720:21 76:291:70
TPUv3-core-days 2:5k 0:68k 0:23k 9:9k 12:3k
Table 2: Comparison with state of the art on popular image classiﬁcation benchmarks. We re-
port mean and standard deviation of the accuracies, averaged over three ﬁne-tuning runs. Vision
Transformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on all
datasets, while taking substantially less computational resources to pre-train. ViT pre-trained on the
smaller public ImageNet-21k dataset performs well too.Slightly improved 88:5%result reported
in Touvron et al. (2020).
VTAB (19 tasks)65707580Accuracy [%]
Natural (7 tasks)708090
Specialized (4 tasks)8082858890
Structured (8 tasks)506070ViT-H/14 BiT-L (R152x4) VIVI-Ex-100% (R50x3) S4L (R50x1)
Figure 2: Breakdown of VTAB performance in Natural ,Specialized , and Structured task groups.
model still took substantially less compute to pre-train than prior state of the art. However, we note
that pre-training efﬁciency may be affected not only by the architecture choice, but also other pa-
rameters, such as training schedule, optimizer, weight decay, etc. We provide a controlled study of
performance vs. compute for different architectures in Section 4.4. Finally, the ViT-L/16 model
pre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking
fewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in ap-
proximately 30 days.
Figure 2 decomposes the VTAB tasks into their respective groups, and compares to previous SOTA
methods on this benchmark: BiT, VIVI  a ResNet co-trained on ImageNet and Youtube (Tschannen
et al., 2020), and S4L  supervised plus semi-supervised learning on ImageNet (Zhai et al., 2019a).
ViT-H/14 outperforms BiT-R152x4, and other methods, on the Natural andStructured tasks. On the
Specialized the performance of the top two models is similar.
4.3 P RE-TRAINING DATA REQUIREMENTS
The Vision Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer
inductive biases for vision than ResNets, how crucial is the dataset size? We perform two series of
experiments.
First, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-
300M. To boost the performance on the smaller datasets, we optimize three basic regularization
parameters  weight decay, dropout, and label smoothing. Figure 3 shows the results after ﬁne-
tuning to ImageNet (results on other datasets are shown in Table 5)2. When pre-trained on the
smallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite
(moderate) regularization. With ImageNet-21k pre-training, their performances are similar. Only
with JFT-300M, do we see the full beneﬁt of larger models. Figure 3 also shows the performance
2Note that the ImageNet pre-trained models are also ﬁne-tuned, but again on ImageNet. This is because the
resolution increase during ﬁne-tuning improves the performance.
6Published as a conference paper at ICLR 2021
ImageNet ImageNet-21k JFT-300M
Pre-training dataset7075808590ImageNet Top1 Accuracy [%]
BiT
ViT-B/32
ViT-B/16ViT-L/32
ViT-L/16
ViT-H/14
Figure 3: Transfer to ImageNet. While
large ViT models perform worse than BiT
ResNets (shaded area) when pre-trained on
small datasets, they shine when pre-trained on
larger datasets. Similarly, larger ViT variants
overtake smaller ones as the dataset grows.
10 M 30 M 100 M 300 M
Number of JFT pre-training samples3040506070Linear 5-shot ImageNet Top1 [%]
ViT-L/16
ViT-L/32ViT-B/32
ViT-b/32ResNet50x1 (BiT)
ResNet152x2 (BiT)Figure 4: Linear few-shot evaluation on Ima-
geNet versus pre-training size. ResNets per-
form better with smaller pre-training datasets
but plateau sooner than ViT, which performs
better with larger pre-training. ViT-b is ViT-B
with all hidden dimensions halved.
1021039095Transfer accuracy [%]Average-5
Transformer (ViT)
ResNet (BiT)
Hybrid
10210375808590ImageNet
Transformer (ViT)
ResNet (BiT)
Hybrid
Total pre-training compute [exaFLOPs]
Figure 5: Performance versus pre-training compute for different architectures: Vision Transformers,
ResNets, and hybrids. Vision Transformers generally outperform ResNets with the same compu-
tational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap
vanishes for larger models.
region spanned by BiT models of different sizes. The BiT CNNs outperform ViT on ImageNet, but
with the larger datasets, ViT overtakes.
Second, we train our models on random subsets of 9M, 30M, and 90M as well as the full JFT-
300M dataset. We do not perform additional regularization on the smaller subsets and use the same
hyper-parameters for all settings. This way, we assess the intrinsic model properties, and not the
effect of regularization. We do, however, use early-stopping, and report the best validation accuracy
achieved during training. To save compute, we report few-shot linear accuracy instead of full ﬁne-
tuning accuracy. Figure 4 contains the results. Vision Transformers overﬁt more than ResNets with
comparable computational cost on smaller datasets. For example, ViT-B/32 is slightly faster than
ResNet50; it performs much worse on the 9M subset, but better on 90M+ subsets. The same is true
for ResNet152x2 and ViT-L/16. This result reinforces the intuition that the convolutional inductive
bias is useful for smaller datasets, but for larger ones, learning the relevant patterns directly from
data is sufﬁcient, even beneﬁcial.
Overall, the few-shot results on ImageNet (Figure 4), as well as the low-data results on VTAB
(Table 2) seem promising for very low-data transfer. Further analysis of few-shot properties of ViT
is an exciting direction of future work.
7Published as a conference paper at ICLR 2021
4.4 S CALING STUDY
We perform a controlled scaling study of different models by evaluating transfer performance from
JFT-300M. In this setting data size does not bottleneck the models performances, and we assess
performance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,
R50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained
for 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus
L/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre-
trained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the
end of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet
backbone).
Figure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5
for details on computational costs). Detailed results per model are provided in Table 6 in the Ap-
pendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the
performance/compute trade-off. ViT uses approximately 24less compute to attain the same
performance (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu-
tational budgets, but the difference vanishes for larger models. This result is somewhat surprising,
since one might expect convolutional local feature processing to assist ViT at any size. Third, Vision
Transformers appear not to saturate within the range tried, motivating future scaling efforts.
4.5 I NSPECTING VISION TRANSFORMER
Input
 Attention
Figure 6: Representative ex-
amples of attention from the
output token to the input
space. See Appendix D.7 for
details.To begin to understand how the Vision Transformer processes im-
age data, we analyze its internal representations. The ﬁrst layer of
the Vision Transformer linearly projects the ﬂattened patches into a
lower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin-
cipal components of the the learned embedding ﬁlters. The com-
ponents resemble plausible basis functions for a low-dimensional
representation of the ﬁne structure within each patch.
After the projection, a learned position embedding is added to the
patch representations. Figure 7 (center) shows that the model learns
to encode distance within the image in the similarity of position em-
beddings, i.e. closer patches tend to have more similar position em-
beddings. Further, the row-column structure appears; patches in the
same row/column have similar embeddings. Finally, a sinusoidal
structure is sometimes apparent for larger grids (Appendix D). That
the position embeddings learn to represent 2D image topology ex-
plains why hand-crafted 2D-aware embedding variants do not yield
improvements (Appendix D.4).
Self-attention allows ViT to integrate information across the entire
image even in the lowest layers. We investigate to what degree
the network makes use of this capability. Speciﬁcally, we compute
the average distance in image space across which information is
integrated, based on the attention weights (Figure 7, right). This
attention distance is analogous to receptive ﬁeld size in CNNs.
We ﬁnd that some heads attend to most of the image already in the lowest layers, showing that
the ability to integrate information globally is indeed used by the model. Other attention heads
have consistently small attention distances in the low layers. This highly localized attention is
less pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),
suggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the
attention distance increases with network depth. Globally, we ﬁnd that the model attends to image
regions that are semantically relevant for classiﬁcation (Figure 6).
4.6 S ELF-SUPERVISION
Transformers show impressive performance on NLP tasks. However, much of their success stems
not only from their excellent scalability but also from large scale self-supervised pre-training (Devlin
8Published as a conference paper at ICLR 2021
RGB embedding filters
(first 28 principal components)
1 2 3 4 5 6 7
Input patch column1
2
3
4
5
6
7Input patch rowPosition embedding similarity
1
1
Cosine similarity
0 5 10 15 20
Network depth (layer)020406080100120Mean attention distance (pixels)
ViT-L/16
Head 1
Head 2
Head 3
...
Figure 7: Left: Filters of the initial linear embedding of RGB values of ViT-L/32. Center: Sim-
ilarity of position embeddings of ViT-L/32. Tiles show the cosine similarity between the position
embedding of the patch with the indicated row and column and the position embeddings of all other
patches. Right: Size of attended area by head and network depth. Each dot shows the mean attention
distance across images for one of 16 heads at one layer. See Appendix D.7 for details.
et al., 2019; Radford et al., 2018). We also perform a preliminary exploration on masked patch
prediction for self-supervision, mimicking the masked language modeling task used in BERT. With
self-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a
signiﬁcant improvement of 2% to training from scratch, but still 4% behind supervised pre-training.
Appendix B.1.2 contains further details. We leave exploration of contrastive pre-training (Chen
et al., 2020b; He et al., 2020; Bachman et al., 2019; H enaff et al., 2020) to future work.
5 C ONCLUSION
We have explored the direct application of Transformers to image recognition. Unlike prior works
using self-attention in computer vision, we do not introduce image-speciﬁc inductive biases into
the architecture apart from the initial patch extraction step. Instead, we interpret an image as a
sequence of patches and process it by a standard Transformer encoder as used in NLP. This simple,
yet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.
Thus, Vision Transformer matches or exceeds the state of the art on many image classiﬁcation
datasets, whilst being relatively cheap to pre-train.
While these initial results are encouraging, many challenges remain. One is to apply ViT to other
computer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion
et al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-
supervised pre-training methods. Our initial experiments show improvement from self-supervised
pre-training, but there is still large gap between self-supervised and large-scale supervised pre-
training. Finally, further scaling of ViT would likely lead to improved performance.
ACKNOWLEDGEMENTS
The work was performed in Berlin, Z urich, and Amsterdam. We thank many colleagues at Google
for their help, in particular Andreas Steiner for crucial help with the infrastructure and the open-
source release of the code; Joan Puigcerver and Maxim Neumann for help with the large-scale
training infrastructure; Dmitry Lepikhin, Aravindh Mahendran, Daniel Keysers, Mario Lu ˇcic, Noam
Shazeer, Ashish Vaswani, and Colin Raffel for useful discussions.
REFERENCES
Samira Abnar and Willem Zuidema. Quantifying attention ﬂow in transformers. In ACL, 2020.
Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing
mutual information across views. In NeurIPS , 2019.
9Published as a conference paper at ICLR 2021
Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In
ICLR , 2019.
I. Bello, B. Zoph, Q. Le, A. Vaswani, and J. Shlens. Attention augmented convolutional networks.
InICCV , 2019.
Lucas Beyer, Olivier J. H enaff, Alexander Kolesnikov, Xiaohua Zhai, and A aron van den Oord. Are
we done with imagenet? arXiv , 2020.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv , 2020.
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and
Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV , 2020.
Mark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun. Generative pretraining from
pixels. In ICML , 2020a.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework
for contrastive learning of visual representations. In ICML , 2020b.
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and
Jingjing Liu. UNITER: UNiversal Image-TExt Representation Learning. In ECCV , 2020c.
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv , 2019.
Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-
attention and convolutional layers. In ICLR , 2020.
J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In CVPR , 2009.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In NAACL , 2019.
Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander
Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander DAmour, Dan Moldovan, Sylvan
Gelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On robustness and transferability of convo-
lutional neural networks. arXiv , 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR , 2016.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In CVPR , 2020.
Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-
mensional transformers. arXiv , 2019.
Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object
detection. In CVPR , 2018.
Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition.
InICCV , 2019.
Zilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, and
Thomas S. Huang. Ccnet: Criss-cross attention for semantic segmentation. In ICCV , 2020.
Olivier J. H enaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami,
and Aaron van den Oord. Data-efﬁcient image recognition with contrastive predictive coding. In
ICML , 2020.
10Published as a conference paper at ICLR 2021
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. 2015.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,
and Neil Houlsby. Big transfer (BiT): General visual representation learning. In ECCV , 2020.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convo-
lutional neural networks. In NIPS , 2012.
Y . LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropa-
gation applied to handwritten zip code recognition. Neural Computation , 1:541551, 1989.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional
computation and automatic sharding. arXiv , 2020.
Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBERT: A
Simple and Performant Baseline for Vision and Language. In Arxiv , 2019.
Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,
Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot atten-
tion. arXiv , 2020.
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visi-
olinguistic Representations for Vision-and-Language Tasks. In NeurIPS . 2019.
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,
Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised
pretraining. In ECCV , 2018.
M. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In
ICVGIP , 2008.
Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V . Jawahar. Cats and dogs. In CVPR ,
2012.
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and
Dustin Tran. Image transformer. In ICML , 2018.
B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM
Journal on Control and Optimization , 30(4):838855, 1992. doi: 10.1137/0330046. URL
https://doi.org/10.1137/0330046 .
Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Weight standardization. arXiv
preprint arXiv:1903.10520 , 2019.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing with unsupervised learning. Technical Report , 2018.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. Technical Report , 2019.
Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.
Stand-alone self-attention in vision models. In NeurIPS , 2019.
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable ef-
fectiveness of data in deep learning era. In ICCV , 2017.
Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint
model for video and language representation learning. In ICCV , 2019.
11Published as a conference paper at ICLR 2021
Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution
discrepancy. In NeurIPS . 2019.
Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution
discrepancy: Fixefﬁcientnet. arXiv preprint arXiv:2003.08237 , 2020.
Michael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran, Neil Houlsby, Sylvain
Gelly, and Mario Lucic. Self-supervised learning of video-induced visual invariances. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June
2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS , 2017.
Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen.
Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV , 2020a.
Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh
Chen. Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. arXiv preprint
arXiv:2003.07853 , 2020b.
Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao.
Learning deep transformer models for machine translation. In ACL, 2019.
Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In
CVPR , 2018.
Dirk Weissenborn, Oscar T ackstr om, and Jakob Uszkoreit. Scaling autoregressive video models. In
ICLR , 2019.
Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt
Keutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing
for computer vision. arxiv , 2020.
Yuxin Wu and Kaiming He. Group normalization. In ECCV , 2018.
Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V . Le. Self-training with noisy student
improves imagenet classiﬁcation. In CVPR , 2020.
Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4L: Self-Supervised Semi-
Supervised Learning. In ICCV , 2019a.
Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario
Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A
large-scale study of representation learning with the visual task adaptation benchmark. arXiv
preprint arXiv:1910.04867 , 2019b.
Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In
CVPR , 2020.
12Published as a conference paper at ICLR 2021
Models Dataset Epochs Base LR LR decay Weight decay Dropout
ViT-B/f16,32g JFT-300M 7 8104linear 0.1 0.0
ViT-L/32 JFT-300M 7 6104linear 0.1 0.0
ViT-L/16 JFT-300M 7/14 4104linear 0.1 0.0
ViT-H/14 JFT-300M 14 3104linear 0.1 0.0
R50xf1,2g JFT-300M 7 103linear 0.1 0.0
R101x1 JFT-300M 7 8104linear 0.1 0.0
R152xf1,2g JFT-300M 7 6104linear 0.1 0.0
R50+ViT-B/f16,32gJFT-300M 7 8104linear 0.1 0.0
R50+ViT-L/32 JFT-300M 7 2104linear 0.1 0.0
R50+ViT-L/16 JFT-300M 7/14 4104linear 0.1 0.0
ViT-B/f16,32g ImageNet-21k 90 103linear 0.03 0.1
ViT-L/f16,32g ImageNet-21k 30/90 103linear 0.03 0.1
ViT- ImageNet 300 3103cosine 0.3 0.1
Table 3: Hyperparameters for training. All models are trained with a batch size of 4096 and learn-
ing rate warmup of 10k steps. For ImageNet we found it beneﬁcial to additionally apply gradient
clipping at global norm 1. Training resolution is 224.
APPENDIX
A M ULTIHEAD SELF-ATTENTION
Standard qkv self-attention (SA, Vaswani et al. (2017)) is a popular building block for neural archi-
tectures. For each element in an input sequence z2RND, we compute a weighted sum over all
values vin the sequence. The attention weights Aijare based on the pairwise similarity between
two elements of the sequence and their respective query qiand key kjrepresentations.
[q;k;v] =zUqkv Uqkv2RD3Dh; (5)
A= softmax
qk>=p
Dh
A2RNN; (6)
SA(z) =Av: (7)
Multihead self-attention (MSA) is an extension of SA in which we run kself-attention operations,
called heads, in parallel, and project their concatenated outputs. To keep compute and number of
parameters constant when changing k,Dh(Eq. 5) is typically set to D=k .
MSA( z) = [SA 1(z); SA 2(z);; SAk(z)]Umsa Umsa2RkDhD(8)
B E XPERIMENT DETAILS
B.1 T RAINING
Table 3 summarizes our training setups for our different models. We found strong regularization
to be key when training models from scratch on ImageNet. Dropout, when used, is applied after
every dense layer except for the the qkv-projections and directly after adding positional- to patch
embeddings. Hybrid models are trained with the exact setup as their ViT counterparts. Finally, all
training is done on resolution 224.
B.1.1 F INE-TUNING
We ﬁne-tune all ViT models using SGD with a momentum of 0.9. We run a small grid search over
learning rates, see learning rate ranges in Table 4. To do so, we use small sub-splits from the training
set (10% for Pets and Flowers, 2% for CIFAR, 1% ImageNet) as development set and train on the
remaining data. For ﬁnal results we train on the entire training set and evaluate on the respective
test data. For ﬁne-tuning ResNets and hybrid models we use the exact same setup, with the only
exception of ImageNet where we add another value 0:06to the learning rate sweep. Additionally,
13Published as a conference paper at ICLR 2021
Dataset Steps Base LR
ImageNet 20 000 f0.003, 0.01, 0.03, 0.06 g
CIFAR100 10 000 f0.001, 0.003, 0.01, 0.03 g
CIFAR10 10 000 f0.001, 0.003, 0.01, 0.03 g
Oxford-IIIT Pets 500 f0.001, 0.003, 0.01, 0.03 g
Oxford Flowers-102 500 f0.001, 0.003, 0.01, 0.03 g
VTAB (19 tasks) 2 500 0.01
Table 4: Hyperparameters for ﬁne-tuning. All models are ﬁne-tuned with cosine learning rate decay,
a batch size of 512, no weight decay, and grad clipping at global norm 1. If not mentioned otherwise,
ﬁne-tuning resolution is 384.
for ResNets we also run the setup of Kolesnikov et al. (2020) and select the best results across
this run and our sweep. Finally, if not mentioned otherwise, all ﬁne-tuning experiments run at 384
resolution (running ﬁne-tuning at different resolution than training is common practice (Kolesnikov
et al., 2020)).
When transferring ViT models to another dataset, we remove the whole head (two linear layers) and
replace it by a single, zero-initialized linear layer outputting the number of classes required by the
target dataset. We found this to be a little more robust than simply re-initializing the very last layer.
For VTAB we follow the protocol in Kolesnikov et al. (2020), and use the same hyperparameter
setting for all tasks. We use a learning rate of 0:01and train for 2500 steps (Tab. 4). We chose this
setting by running a small sweep over two learning rates and two schedules, and selecting the setting
with the highest VTAB score on the 200-example validation sets. We follow the pre-processing used
in Kolesnikov et al. (2020), except that we do not use task-speciﬁc input resolutions. Instead we ﬁnd
that Vision Transformer beneﬁts most from a high resolution ( 384384) for all tasks.
B.1.2 S ELF-SUPERVISION
We employ the masked patch prediction objective for preliminary self-supervision experiments. To
do so we corrupt 50% of patch embeddings by either replacing their embeddings with a learnable
[mask] embedding (80%), a random other patch embedding (10%) or just keeping them as is
(10%). This setup is very similar to the one used for language by Devlin et al. (2019). Finally, we
predict the 3-bit, mean color (i.e., 512 colors in total) of every corrupted patch using their respective
patch representations.
We trained our self-supervised model for 1M steps (ca. 14 epochs) with batch size 4096 on JFT. We
use Adam, with a base learning rate of 2104, warmup of 10k steps and cosine learning rate decay.
As prediction targets for pretraining we tried the following settings: 1) predicting only the mean,
3bit color (i.e., 1 prediction of 512 colors), 2) predicting a 44downsized version of the 1616
patch with 3bit colors in parallel (i.e., 16 predictions of 512 colors), 3) regression on the full patch
using L2 (i.e., 256 regressions on the 3 RGB channels). Surprisingly, we found that all worked quite
well, though L2 was slightly worse. We report ﬁnal results only for option 1) because it has shown
best few-shot performance. We also experimented with 15% corruption rate as used by Devlin et al.
(2019) but results were also slightly worse on our few-shot metrics.
Lastly, we would like to remark that our instantiation of masked patch prediction doesnt require
such an enormous amount of pretraining nor a large dataset such as JFT in order to lead to sim-
ilar performance gains on ImageNet classiﬁcation. That is, we observed diminishing returns on
downstream performance after 100k pretraining steps, and see similar gains when pretraining on
ImageNet.
C A DDITIONAL RESULTS
We report detailed results corresponding to the ﬁgures presented in the paper. Table 5 corresponds
to Figure 3 from the paper and shows transfer performance of different ViT models pre-trained
on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-300M. Table 6 corresponds to
14Published as a conference paper at ICLR 2021
ViT-B/16 ViT-B/32 ViT-L/16 ViT-L/32 ViT-H/14
ImageNet CIFAR-10 98.13 97.77 97.86 97.94 -
CIFAR-100 87.13 86.31 86.35 87.07 -
ImageNet 77.91 73.38 76.53 71.16 -
ImageNet ReaL 83.57 79.56 82.19 77.83 -
Oxford Flowers-102 89.49 85.43 89.66 86.36 -
Oxford-IIIT-Pets 93.81 92.04 93.64 91.35 -
ImageNet-21k CIFAR-10 98.95 98.79 99.16 99.13 99.27
CIFAR-100 91.67 91.97 93.44 93.04 93.82
ImageNet 83.97 81.28 85.15 80.99 85.13
ImageNet ReaL 88.35 86.63 88.40 85.65 88.70
Oxford Flowers-102 99.38 99.11 99.61 99.19 99.51
Oxford-IIIT-Pets 94.43 93.02 94.73 93.09 94.82
JFT-300M CIFAR-10 99.00 98.61 99.38 99.19 99.50
CIFAR-100 91.87 90.49 94.04 92.52 94.55
ImageNet 84.15 80.73 87.12 84.37 88.04
ImageNet ReaL 88.85 86.27 89.99 88.28 90.33
Oxford Flowers-102 99.56 99.27 99.56 99.45 99.68
Oxford-IIIT-Pets 95.80 93.40 97.11 95.83 97.56
Table 5: Top1 accuracy (in %) of Vision Transformer on various datasets when pre-trained on Im-
ageNet, ImageNet-21k or JFT300M. These values correspond to Figure 3 in the main text. Models
are ﬁne-tuned at 384 resolution. Note that the ImageNet results are computed without additional
techniques (Polyak averaging and 512 resolution images) used to achieve results in Table 2.
Epochs ImageNet ImageNet ReaL CIFAR-10 CIFAR-100 Pets Flowers exaFLOPs
name
ViT-B/32 7 80.73 86.27 98.61 90.49 93.40 99.27 55
ViT-B/16 7 84.15 88.85 99.00 91.87 95.80 99.56 224
ViT-L/32 7 84.37 88.28 99.19 92.52 95.83 99.45 196
ViT-L/16 7 86.30 89.43 99.38 93.46 96.81 99.66 783
ViT-L/16 14 87.12 89.99 99.38 94.04 97.11 99.56 1567
ViT-H/14 14 88.08 90.36 99.50 94.71 97.11 99.71 4262
ResNet50x1 7 77.54 84.56 97.67 86.07 91.11 94.26 50
ResNet50x2 7 82.12 87.94 98.29 89.20 93.43 97.02 199
ResNet101x1 7 80.67 87.07 98.48 89.17 94.08 95.95 96
ResNet152x1 7 81.88 87.96 98.82 90.22 94.17 96.94 141
ResNet152x2 7 84.97 89.69 99.06 92.05 95.37 98.62 563
ResNet152x2 14 85.56 89.89 99.24 91.92 95.75 98.75 1126
ResNet200x3 14 87.22 90.15 99.34 93.53 96.32 99.04 3306
R50x1+ViT-B/32 7 84.90 89.15 99.01 92.24 95.75 99.46 106
R50x1+ViT-B/16 7 85.58 89.65 99.14 92.63 96.65 99.40 274
R50x1+ViT-L/32 7 85.68 89.04 99.24 92.93 96.97 99.43 246
R50x1+ViT-L/16 7 86.60 89.72 99.18 93.64 97.03 99.40 859
R50x1+ViT-L/16 14 87.12 89.76 99.31 93.89 97.36 99.11 1668
Table 6: Detailed results of model scaling experiments. These correspond to Figure 5 in the main
paper. We show transfer accuracy on several datasets, as well as the pre-training compute (in ex-
aFLOPs).
Figure 5 from the paper and shows the transfer performance of ViT, ResNet, and hybrid models of
varying size, as well as the estimated computational cost of their pre-training.
D A DDITIONAL ANALYSES
D.1 SGD VS. ADAM FOR RESNETS
ResNets are typically trained with SGD and our use of Adam as optimizer is quite unconventional.
Here we show the experiments that motivated this choice. Namely, we compare the ﬁne-tuning
15Published as a conference paper at ICLR 2021
ResNet50 ResNet152x2
Dataset Adam SGD Adam SGD
ImageNet 77:54 78 :24 84 :97 84 :37
CIFAR10 97:67 97 :46 99 :06 99 :07
CIFAR100 86:07 85 :17 92 :05 91 :06
Oxford-IIIT Pets 91:11 91 :00 95 :37 94 :79
Oxford Flowers-102 94:26 92 :06 98 :62 99 :32
Average 89:33 88 :79 94 :01 93 :72
Table 7: Fine-tuning ResNet models pre-trained with Adam and SGD.
100101
Relative Compute0.20.30.40.50.6ImageNet 5shot
Models
All
Depth
Patch size
Width MLP
Width
100101
Relative Compute0.40.50.60.70.8Average 5shot
Models
All
Depth
Patch size
Width MLP
Width
Figure 8: Scaling different model dimensions of the Vision Transformer.
performance of two ResNets  50x1 and 152x2  pre-trained on JFT with SGD and Adam. For
SGD, we use the hyperparameters recommended by Kolesnikov et al. (2020). Results are presented
in Table 7. Adam pre-training outperforms SGD pre-training on most datasets and on average.
This justiﬁes the choice of Adam as the optimizer used to pre-train ResNets on JFT. Note that the
absolute numbers are lower than those reported by Kolesnikov et al. (2020), since we pre-train only
for7epochs, not 30.
D.2 T RANSFORMER SHAPE
We ran ablations on scaling different dimensions of the Transformer architecture to ﬁnd out which
are best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet
for different conﬁgurations. All conﬁgurations are based on a ViT model with 8layers,D= 1024 ,
DMLP = 2048 and a patch size of 32, the intersection of all lines. We can see that scaling the
depth results in the biggest improvements which are clearly visible up until 64 layers. However,
diminishing returns are already visible after 16 layers. Interestingly, scaling the width of the net-
work seems to result in the smallest changes. Decreasing the patch size and thus increasing the
effective sequence length shows surprisingly robust improvements without introducing parameters.
These ﬁndings suggest that compute might be a better predictor of performance than the number of
parameters, and that scaling should emphasize depth over width if any. Overall, we ﬁnd that scaling
all dimensions proportionally results in robust improvements.
D.3 H EAD TYPE AND C L A S S TOKEN
In order to stay as close as possible to the original Transformer model, we made use of an additional
[class] token, which is taken as image representation. The output of this token is then trans-
formed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity
in the single hidden layer.
This design is inherited from the Transformer model for text, and we use it throughout the main
paper. An initial attempt at using only image-patch embeddings, globally average-pooling (GAP)
them, followed by a linear classiﬁerjust like ResNets ﬁnal feature mapperformed very poorly.
However, we found that this is neither due to the extra token, nor to the GAP operation. Instead,
16Published as a conference paper at ICLR 2021
0 1 2 3 4 5 6 7
Epochs of training2530354045505560ImageNet linear 5-shot accuracy [%]CLS-Token, lr=8e-4
GAP, lr=8e-4
GAP, lr=3e-4
Figure 9: Comparison of class-token and global average pooling classiﬁers. Both work similarly
well, but require different learning-rates.
Pos. Emb. Default/Stem Every Layer Every Layer-Shared
No Pos. Emb. 0.61382 N/A N/A
1-D Pos. Emb. 0.64206 0.63964 0.64292
2-D Pos. Emb. 0.64001 0.64046 0.64022
Rel. Pos. Emb. 0.64032 N/A N/A
Table 8: Results of the ablation study on positional embeddings with ViT-B/16 model evaluated on
ImageNet 5-shot linear.
the difference in performance is fully explained by the requirement for a different learning-rate, see
Figure 9.
D.4 P OSITIONAL EMBEDDING
We ran ablations on different ways of encoding spatial information using positional embedding. We
tried the following cases:
 Providing no positional information: Considering the inputs as a bag of patches .
 1-dimensional positional embedding: Considering the inputs as a sequence of patches in
the raster order (default across all other experiments in this paper).
 2-dimensional positional embedding: Considering the inputs as a grid of patches in two
dimensions. In this case, two sets of embeddings are learned, each for one of the axes,
X-embedding, and Y-embedding, each with size D=2. Then, based on the coordinate on
the path in the input, we concatenate the XandYembedding to get the ﬁnal positional
embedding for that patch.
 Relative positional embeddings: Considering the relative distance between patches to en-
code the spatial information as instead of their absolute position. To do so, we use 1-
dimensional Relative Attention, in which we deﬁne the relative distance all possible pairs
of patches. Thus, for every given pair (one as query, and the other as key/value in the at-
tention mechanism), we have an offset pqpk, where each offset is associated with an
embedding. Then, we simply run extra attention, where we use the original query (the
content of query), but use relative positional embeddings as keys. We then use the log-
its from the relative attention as a bias term and add it to the logits of the main attention
(content-based attention) before applying the softmax.
In addition to different ways of encoding spatial information, we also tried different ways of in-
corporating this information in our model. For the 1-dimensional and 2-dimensional positional
embeddings, we tried three different cases: (1) add positional embeddings to the inputs right after
17Published as a conference paper at ICLR 2021
1234567891011121314
Input patch column1
2
3
4
5
6
7
8
9
10
11
12
13
14Input patch rowViT-L16
7 epochs, LR=0.0002, WD=0.01
1
1
Cosine similarity
1234567891011121314
Input patch column1
2
3
4
5
6
7
8
9
10
11
12
13
14Input patch rowViT-L16
7 epochs, LR=0.0004, WD=0.1
1
1
Cosine similarity
1234567891011121314
Input patch column1
2
3
4
5
6
7
8
9
10
11
12
13
14Input patch rowViT-L16
14 epochs, LR=0.0004, WD=0.1
1
1
Cosine similarity
Figure 10: Position embeddings of models trained with different hyperparameters.
the stem of them model and before feeding the inputs to the Transformer encoder (default across
all other experiments in this paper); (2) learn and add positional embeddings to the inputs at the
beginning of each layer; (3) add a learned positional embeddings to the inputs at the beginning of
each layer (shared between layers).
Table 8 summarizes the results from this ablation study on a ViT-B/16 model. As we can see, while
there is a large gap between the performances of the model with no positional embedding and mod-
els with positional embedding, there is little to no difference between different ways of encoding
positional information. We speculate that since our Transformer encoder operates on patch-level
inputs, as opposed to pixel-level, the differences in how to encode spatial information is less impor-
tant. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original
pixel-level inputs, e.g., 1414as opposed to 224224, and learning to represent the spatial re-
lations in this resolution is equally easy for these different positional encoding strategies. Even so,
the speciﬁc pattern of position embedding similarity learned by the network depends on the training
hyperparameters (Figure 10).
0 5 10 15 20
Network depth (layer)020406080100120Mean attention distance (pixels)
ViT-L/16
Head 1
Head 2
Head 3
...
0 5 10 15 20
Network depth (layer)020406080100120
R50x1 + ViT-L/16
Head 1
Head 2
Head 3
...
Figure 11: Size of attended area by head and network depth. Attention distance was computed for
128 example images by averaging the distance between the query pixel and all other pixels, weighted
by the attention weight. Each dot shows the mean attention distance across images for one of 16
heads at one layer. Image width is 224 pixels.
D.5 E MPIRICAL COMPUTATIONAL COSTS
We are also interested in real-world speed of the architectures on our hardware, which is not always
well predicted by theoretical FLOPs due to details like lane widths and cache sizes. For this purpose,
18Published as a conference paper at ICLR 2021
we perform timing of inference speed for the main models of interest, on a TPUv3 accelerator; the
difference between inference and backprop speed is a constant model-independent factor.
Figure 12 (left) shows how many images one core can handle per second, across various input sizes.
Every single point refers to the peak performance measured across a wide range of batch-sizes. As
can be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening
for the largest models at the largest resolutions.
Another quantity of interest is the largest batch-size each model can ﬁt onto a core, larger being
better for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models.
This shows that large ViT models have a clear advantage in terms of memory-efﬁciency over ResNet
models.
64 128 224 384 512
Input size [px]102103104Peak inference speed [img/sec/core]64 128 224 384 512
Input size [px]102103Largest per-core batch-sizeR50x1
R50x2ViT-B/32
ViT-L/32ViT-B/16
ViT-L/16ViT-H/14
R152x4
Figure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models
have speed comparable to similar ResNets. Right : Largest per-core batch-size ﬁtting on device with
various architectures across input sizes. ViT models are clearly more memory-efﬁcient.
D.6 A XIAL ATTENTION
Axial Attention (Huang et al., 2020; Ho et al., 2019) is a simple, yet effective technique to run self-
attention on large inputs that are organized as multidimensional tensors. The general idea of axial
attention is to perform multiple attention operations, each along a single axis of the input tensor,
instead of applying 1-dimensional attention to the ﬂattened version of the input. In axial attention,
each attention mixes information along a particular axis, while keeping information along the other
axes independent. Along this line, Wang et al. (2020b) proposed the AxialResNet model in which
all the convolutions with kernel size 33in a ResNet50 are replaced by axial self-attention, i.e.
a row and column attention, augmented by relative positional encoding. We have implemented
AxialResNet as a baseline model.3.
Moreover, we have modiﬁed ViT to process inputs in the 2-dimensional shape, instead of a 1-
dimensional sequence of patches, and incorporate Axial Transformer blocks, in which instead of
a self-attention followed by an MLP, we have a a row-self-attention plus an MLP followed by a
column-self-attention plus an MLP.
Figure 13, present the performance of Axial ResNet, Axial-ViT-B/32 and Axial-ViT-B/16 on Ima-
geNet 5shot linear, when pretrained on JFT dataset, verses the pretraining compute, both in terms of
number of FLOPs and inference time (example per seconds). As we can see, both Axial-ViT-B/32
and Axial-ViT-B/16 do better than their ViT-B counterpart in terms of performance, but it comes at
3Our implementation is based on the open-sourced PyTorch implementation in https://github.com/
csrhddlam/axial-deeplab . In our experiments, we reproduced the scores reported in (Wang et al.,
2020b) in terms of accuracy, however, our implementation, similar to the open-source implementation, is very
slow on TPUs. Therefore, we were not able to use it for extensive large-scale experiments. These may be
unlocked by a carefully optimized implementation.
19Published as a conference paper at ICLR 2021
102
Total compute [exaFLOPs]0.5000.5250.5500.5750.6000.6250.650ImageNet 5-shot linear top-1 accuracyAxialViT-B/16
AxialViT-B/32ViT-B/16
ViT-B/32
ResNet50AxialResNet50
102103
Peak inference speed [img/sec/core]0.5000.5250.5500.5750.6000.6250.650ImageNet 5-shot linear top-1 accuracyAxialViT-B/16
AxialViT-B/32ViT-B/16
ViT-B/32
ResNet50AxialResNet50
Figure 13: Performance of Axial-Attention based models, in terms of top-1 accuracy on ImageNet
5-shot linear, versus their speed in terms of number of FLOPs ( left) and inference time ( left).
the cost of more compute. This is because in Axial-ViT models, each Transformer block with global
self-attention is replaced by two Axial Transformer blocks, one with row and one with column self-
attention and although the sequence length that self-attention operates on is smaller in axial case,
there is a extra MLP per Axial-ViT block. For the AxialResNet, although it looks reasonable in
terms of accuracy/compute trade-off (Figure 13, left), the naive implementation is extremely slow
on TPUs (Figure 13, right).
D.7 A TTENTION DISTANCE
To understand how ViT uses self-attention to integrate information across the image, we analyzed
the average distance spanned by attention weights at different layers (Figure 11). This attention
distance is analogous to receptive ﬁeld size in CNNs. Average attention distance is highly variable
across heads in lower layers, with some heads attending to much of the image, while others attend
to small regions at or near the query location. As depth increases, attention distance increases for all
heads. In the second half of the network, most heads attend widely across tokens.
D.8 A TTENTION MAPS
To compute maps of the attention from the output token to the input space (Figures 6 and 14), we
used Attention Rollout (Abnar & Zuidema, 2020). Brieﬂy, we averaged attention weights of ViT-
L/16 across all heads and then recursively multiplied the weight matrices of all layers. This accounts
for the mixing of attention across tokens through all layers.
D.9 O BJECT NETRESULTS
We also evaluate our ﬂagship ViT-H/14 model on the ObjectNet benchmark following the evaluation
setup in Kolesnikov et al. (2020), resulting in 82.1% top-5 accuracy and 61.7% top-1 accuracy.
D.10 VTAB B REAKDOWN
Table 9 shows the scores attained on each of the VTAB-1k tasks.
20Published as a conference paper at ICLR 2021
1
 2
 3
 4
 5
 6
 7
 8
9
 10
 11
 12
 13
 14
 15
 16
17
 18
 19
 20
 21
 22
 23
 24
25
 26
 27
 28
 29
 30
 31
 32
33
 34
 35
 36
 37
 38
 39
 40
41
 42
 43
 44
 45
 46
 47
 48
49
 50
 51
 52
 53
 54
 55
 56
57
 58
 59
 60
 61
 62
 63
 64
65
 66
 67
 68
 69
 70
 71
 72
73
 74
 75
 76
 77
 78
 79
 80
81
 82
 83
 84
 85
 86
 87
 88
89
 90
 91
 92
 93
 94
 95
 96
97
 98
 99
 100
 101
 102
 103
 104
105
 106
 107
 108
 109
 110
 111
 112
113
 114
 115
 116
 117
 118
 119
 120
121
 122
 123
 124
 125
 126
 127
 128
Figure 14: Further example attention maps as in Figure 6 (random selection).
21Published as a conference paper at ICLR 2021
Table 9: Breakdown of VTAB-1k performance across tasks.Caltech101
CIFAR-100
DTD
Flowers102
Pets
Sun397
SVHN
Camelyon
EuroSAT
Resisc45
Retinopathy
Clevr-Count
Clevr-Dist
DMLab
dSpr-Loc
dSpr-Ori
KITTI-Dist
sNORB-Azim
sNORB-Elev
Mean
ViT-H/14 (JFT) 95.3 85.5 75.2 99.7 97.2 65.0 88.9 83.3 96.7 91.4 76.6 91.7 63.8 53.1 79.4 63.3 84.5 33.2 51.2 77.6
ViT-L/16 (JFT) 95.4 81.9 74.3 99.7 96.7 63.5 87.4 83.6 96.5 89.7 77.1 86.4 63.1 49.7 74.5 60.5 82.2 36.2 51.1 76.3
ViT-L/16 (I21k) 90.8 84.1 74.1 99.3 92.7 61.0 80.9 82.5 95.6 85.2 75.3 70.3 56.1 41.9 74.7 64.9 79.9 30.5 41.7 72.7
22
  Neural Redshift: Random Networks are not Random Functions
Damien Teney
Idiap Research Institute
damien.teney@idiap.chArmand Mihai Nicolicioiu
ETH Zurich
armandmihai.nicolicioiu@inf.ethz.ch
Valentin Hartmann
EPFL
valentin.hartmann@epfl.chEhsan Abbasnejad
University of Adelaide
ehsan.abbasnejad@adelaide.edu
Abstract
Our understanding of the generalization capabilities of
neural networks (NNs) is still incomplete. Prevailing ex-
planations are based on implicit biases of gradient de-
scent (GD) but they cannot account for the capabilities of
models from gradient-free methods [9] nor the simplicity
bias recently observed in untrained networks [29]. This pa-
per seeks other sources of generalization in NNs.
Findings. To understand the inductive biases provided
by architectures independently from GD, we examine un-
trained, random-weight networks. Even simple MLPs show
strong inductive biases: uniform sampling in weight space
yields a very biased distribution of functions in terms of
complexity. But unlike common wisdom, NNs do not have
an inherent simplicity bias. This property depends on
components such as ReLUs, residual connections, and layer
normalizations. Alternative architectures can be built with
a bias for any level of complexity. Transformers also inherit
all these properties from their building blocks.
Implications. We provide a fresh explanation for the suc-
cess of deep learning independent from gradient-based
training. It points at promising avenues for controlling the
solutions implemented by trained models.
1. Introduction
Among various models in machine learning, neural net-
works (NNs) are the most successful on a variety of tasks.
While we are pushing their capabilities with ever-larger
models [72], much remains to be understood at the level of
their building blocks. This work seeks to understand what
provides NNs with their unique generalization capabilities.
The need for inductive biases. The success of ML de-
pends on using suitable inductive biases1[45]. They specify
1Inductive biases are assumptions about the target function encoded in
the learning algorithm as the hypothesis class ( e.g. architecture), optimiza-
tion method ( e.g. SGD), objective ( e.g. cross-entropy risk), regularizer, etc.ReLU GELU TanH Gaussian Weights magnitude
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
Depth
Lower frequencies
 Higher frequencies
Figure 1. We examine the complexity of the functions imple-
mented by various MLP architectures. We find that much of their
generalization capabilities can be understood independently from
the optimization, training objective, scaling, or even data distribu-
tion. For example, ReLU and GELU networks (left) overwhelm-
ingly represent low-frequency functions for any network depth or
weight magnitude . Other activations lack this property.
how to generalize from a finite set of training examples to
novel test cases. For example, linear models allow learning
from few examples but generalize correctly only when the
target function is itself linear. Large NNs are surprising in
having large representational capacity [36] yet generalizing
well across many tasks. In other words, among all learn-
able functions that fit the training data, those implemented
by trained networks are often similar to the target function.
Explaining the success of neural networks. Some suc-
cessful architectures are tailored to specific domains e.g.
CNNs for image recognition. But even the simplest MLP
architectures (multi-layer perceptrons) often display re-
markable generalization. Two explanations for this success
prevail, although they are increasingly challenged.
Implicit biases of gradient-based optimization . A large
amount of work studies the preference of (stochastic) gra-
dient descent or (S)GD for particular solutions [20, 51].
But conflicting results have also appeared. First, full-
batch GD can be as effective as SGD [24, 38, 46, 69].
Second, Chiang et al. [9] showed that zeroth-order op-
1arXiv:2403.02241v2  [cs.LG]  5 Mar 2024timization can yield models with good generalization as
frequently as GD. And third, Goldblum et al. [29] showed
that language models with random weights already dis-
play a preference for low-complexity sequences. This
simplicity bias was previously thought to emerge from
training [65]. In short, gradient descent may help with
generalization but it does not seem necessary.
Good generalization as a fundamental property of all
nonlinear architectures [33]. This vague conjecture
does not account for the selection bias in the architectures
and algorithms that researchers have converged on. For
example, implicit neural representations ( i.e. a network
trained to represent a specific image or 3D shape) show
that the success of NNs is not automatic and requires, in
that case, activations very different from ReLUs.
The success of deep learning is thus not a product primarily
of GD, nor is it universal to all architectures. This paper
propose an explanation compatible with all above observa-
tions. It builds on the growing evidence that NNs benefit
from their parametrization and the structure of their weight
space [9, 29, 37, 64, 77, 79].
Contributions. We present experiments supporting this
three-part proposition (stated formally in Appendix C).
(1)NNs are biased to implement functions of a particu-
lar level of complexity (not necessarily low) determined
by the architecture. (2)This preferred complexity is ob-
servable in networks with random weights from an un-
informed prior. (3)Generalization is enabled by popular
components like ReLUs setting this bias to a low com-
plexity that often aligns with the target function.
We name it the Neural Redshift (NRS) by analogy to phys-
ical effects2that bias the observations of a signal towards
low frequencies. Here, the parameter space of NNs is biased
towards functions of low frequency, one of the measures of
complexity used in this work (see Figure 1).
The NRS differs from prior work on the spectral bias [57,
84] and simplicity bias [3, 65] which confound the effects of
architectures and gradient descent. The spectral bias refers
to the earlier learning of low-frequencies during training
(see discussion in Section 6). The NRS only involves the
parametrization3of NNs and thus shows interesting proper-
ties independent from optimization [80], scaling [5], learn-
ing objectives [9], or even data distributions [52].
Concretely, we examine various architectures with ran-
dom weights. We use three measures of complexity: (1) de-
compositions in Fourier series and (2) in bases of orthog-
2https://en.wikipedia.org/wiki/Redshift
3Parametrization refers to the mapping between a networks weights
and the function it represents. An analogy in geometry is the parametriza-
tion of 2D points with Euclidean or polar coordinates. Sampling uniformly
from one or the other gives different distributions of points.onal polynomials (equating simplicity with low frequen-
cies/order) and (3) compressibility as an approximation of
the Kolmogorov complexity [15]. We study how they vary
across architectures and how these properties at initializa-
tion correlate with the performance of trained networks.
Summary of findings.
 We verify the NRS with three notions of complexity that
rely on frequencies in Fourier decompositions, order in
polynomial decompositions, and compressibility of the
input-output mapping (Section 3).
 We visualize the input-output mappings of 2D networks
(Figure 3). They show intuitively the diversity of induc-
tive biases across architectures that a scalar complexity
cannot fully describe. Therefore, matching the complex-
ity of an architecture with the target function is beneficial
for generalization but hardly sufficient (Section 4.1).
 We show that the simplicity bias is not universal but de-
pends on common components (ReLUs, residual connec-
tions, layer normalizations). ReLU networks also have
the unique property of maintaining their simplicity bias
for any depth and weight magnitudes. It suggests that
the historical importance of ReLUs in the development of
deep learning goes beyond the common narrative about
vanishing gradients [42].
 We construct architectures where the NRS can be modu-
lated (with alternative activations and weight magnitudes)
or entirely avoided (parametrization in Fourier space,
Section 3). It further demonstrates that the simplicity
bias is not universal and can be controlled to learn com-
plex functions ( e.g. modulo arithmetic) or mitigate short-
cut learning (Section 4.1).
 We show that the NRS is relevant to transformer sequence
models. Random-weight transformers produce sequences
of low complexity and this can also be modulated with the
architecture. This suggests that transformers inherit in-
ductive biases from their building blocks via mechanisms
similar to those of simple models. (Section 5).
2. How to Measure Inductive Biases?
Our goal is to understand why NNs generalize when other
models of similar capacity would often overfit. The im-
mediate answer is simple: NNs have an inductive bias
for functions with properties matching real-world data .
Hence two subquestions.
1.What are these properties?
We will show that three metrics are relevant: low fre-
quency, low order, and compressibility. Hereafter, they
are collectively referred to as simplicity.
2.What gives neural networks these properties?
We will show that an overwhelming fraction of their pa-
rameter space corresponds to functions with such sim-
2Chosen architecture
E.g. 2-layer MLP, TanH,
width 256, layer norm.Random weights
Evaluation
gridMeasures of
complexityFrequency
Fourier decomp osition
Order
Polynomial decomp osition
Compressibility
LZW compression
Figure 2. Our methodology to characterize the inductive biases of an architecture. We evaluate a network with random weights/biases on
a grid of points. This yields a representation of the function implemented by the network, shown here as a grayscale image for a 2D input.
We then characterize this function using three measures of complexity.
plicity. While there exist solutions of arbitrary complex-
ity, simple ones are found by default when navigating
this space (especially with gradient-based methods).
Analyzing random networks. Given an architecture to
characterize, we propose to sample random weights and bi-
ases, then evaluate the network on a regular grid in its input
space (see Figure 2). Let fθ(x)represent the function im-
plemented by a network of parameters θ(weights and bi-
ases), evaluated on the input xRd. The frepresents an
architecture with a scalar output and no output activation,
which could serve for any regression or classification task.
We sample weights and biases from an uninformed prior
chosen as a uniform distribution. Biases are sampled from
U(1,1), and weights from the range proposed by Glo-
rot and Bengio [28] commonly used for initialization i.e.
U(s, s)withs=αp
6/(fanin+fanout)where αis an
extra factor ( 1by default) to manipulate the weights magni-
tude in our experiments. These choices are not critical. Ap-
pendix E shows that other distributions (Gaussian, uniform-
ball, long-tailed) lead to similar findings.
We then evaluate fθ()on points Xgrid={x1, . . . ,xN}
sampled regularly in the input space. We restrict Xgridto
the hypercube [1,+1]dsince the data used with NNs is
commonly normalized. The evaluation of fθ()onXgrid
yields a d-dimensional grid of scalars. In experiments of
Section 3 where d=2, this is conveniently visualized as a 2D
grayscale image to provide visual intuition about the func-
tion represented by the network. Next, we describe three
quantifiable properties to extract from such representations.
Measures of complexity. Applying the above procedure
to various architectures with 2D inputs yields clearly di-
verse visual representations (see Figure 3). For quantita-
tive comparisons, we propose three functions of the form
C(Xgrid, f)that estimate proxies of the complexity of f.
Fourier frequency. A first natural choice is to use
Fourier analysis as in classical signal and image process-
ing. The image to analyze is the d-dimensional evalua-
tion of fonXgridmentioned above. We first compute
a discrete Fourier transform that approximates fwith
a weighted sum of sines of various frequencies. For-mally, we have f(x) := (2 π)d/2Rf(k)eikxdkwhere
f(k) :=R
f(x)eikxdxis the Fourier transform. The
discrete transform means that the frequency numbers k
are regularly spaced, {0,1,2, . . . , K }with the maximum
Kset according to the NyquistShannon limit of Xgrid.
We use the intuition that complex functions are those with
large high-frequency coefficients [57]. Therefore, we de-
fine our measure of complexity as the average of the co-
efficients weighted by their corresponding frequency i.e.
CFourier(f) = ΣK
k=1f(k)k /ΣK
k=1f(k).
For example, a smoothly varying function is likely to in-
volve mostly low-frequency components, and therefore
give a low value to CFourier .
Polynomial order. A minor variation of Fourier anal-
ysis uses decompositions in bases of polynomials. The
procedure is nearly identical, except for the sine waves
of increasing frequencies being replaced with fixed poly-
nomials of increasing order (details in Appendix D). We
obtain an approximation of fas a weighted sum of
such polynomials, and define our complexity measure
CChebyshev exactly as above, i.e. the average of the coef-
ficients weighted by their corresponding order. For ex-
ample, the first two basis elements are a constant and a
first-order polynomial, hence the decomposition of a lin-
earfwill use large coefficients on these low-order ele-
ments and give a low complexity. We implemented this
method with several canonical bases of orthogonal poly-
nomials (Hermite, Legendre, and Chebyshev polynomi-
als) and found the latter to be the most numerically stable.
Compressibility has been proposed as an approximation
of the Kolmogorov complexity [15, 29, 79]. We apply the
classical Lempel-Ziv (LZ) compression on the sequence
Y={f(xi) :xiX}. We then use the size of the dictio-
nary built by the algorithm as our measure of complexity
CLZ(f). A sequence with repeating patterns will require
a small dictionary and give a low complexity.
3. Inductive Biases in Random Networks
We are now equipped to compare architectures. We will
show that various components shift the inductive bias to-
3Table 1. Components that bias NNs towards low/high complexity.
Lower complexity No impact Higher complexity
ReLU-like activations
Small weights / activations
Layer normalization
Residual connectionsWidth
Bias magnitudesOther activations
Large weights / activations
Depth
Multiplicative interactions
wards low or high complexity (see Table 1). In particular,
ReLU activations will prove critical for a simplicity bias in-
sensitive to depth and weight / activation magnitude.
ReLU MLPs. We start with a 1-hidden layer multi-layer
perceptron (MLP) with ReLU activations. We will then ex-
amine variations of this architecture. Formally, each hidden
layer applies a transformation on the input: xϕ(Wx+b)
with weights W, biases b, and activation function ϕ().
MLPs are so simple that they are often thought as providing
little or no inductive bias [5]. On the contrary, we observe
in Figures 4 & 6 that MLPs have a very strong bias towards
low-frequency, low-order, compressible functions. And this
simplicity bias is remarkably unaffected by the magnitude
of the weights, nor increased depth.
The variance in complexity across the random networks
is essentially zero: virtually allof them have low complex-
ity. This does not mean that they cannot represent com-
plex functions, which would violate their universal approx-
imation property [36]. Complex functions simply require
precisely-adjusted weights and biases that are unlikely to
be found by random sampling. These can still be found by
gradient descent though, as we will see in Section 4.
ReLU-like activations (GELU, Swish, SELU [16]) are
also biased towards low complexity. Unlike ReLUs, close
examination in Appendix F shows that increasing depth or
weight magnitudes slightly increases the complexity.
Others activations (TanH, Gaussian, sine) show com-
pletely different behaviour. Depth and weight magnitude
cause a dramatic increase in complexity. Unsurprisingly,
these activations are only used in special applications [58]
with careful initializations [68]. Networks with these acti-
vations have no fixed preferred complexity independent of
the weights or activations magnitudes.4Mechanistically,
the dependency on weight magnitudes is trivial to explain.
Unlike with a ReLU, the output e.g. of a GELU is not equiv-
ariant to a rescaling of the weights and biases.
Figure 6 shows close correlations between complexity
measures, though they measure different proxies. Figure 3
shows that different activations make distinctive patterns
not captured by the complexity measures. More work will
be needed to characterize such fine-grained differences.
4The weight magnitudes examined in Figure 4 are larger than typically
used for initialization, but the same effects would result from large activa-
tionmagnitudes that occur in trained models.ReLU TanH
Weights 
U(s, s)
Weights 
U(6s,6s)
Figure 3. Comparison of functions implemented by random MLPs
(2D input, 3 hidden layers). ReLU and TanH architectures are bi-
ased towards different functions despite their universal approxima-
tion capabilities. ReLU architectures have the unique property of
maintaining their simplicity bias regardless of weight magnitude.
Width has no impact on complexity, perhaps surprisingly.
Additional neurons change the capacity of a model (what
can be represented after training) but they do not affect its
inductive biases. Indeed, the contribution of all neurons in
a layer averages out to something invariant to their number.
Layer normalization is a popular component in modern
architectures, including transformers [55]. It shifts and
rescales the internal representation to zero mean and unit
variance [4]. We place layer normalizations before each ac-
tivation such that each hidden layer now applies the trans-
formation: x(Wx+b);xϕ((xx)/std(x))where
xandstd(x)denote the mean and standard deviation across
channels. Layer normalization has the significant effect of
removing variations in complexity with the weights magni-
tude for all activations (Figure 5). The weights can now vary
(e.g. during training) without directly affecting the preferred
complexity of the architecture. Layer normalizations also
usually apply a learnable offset ( 0by default) and scaling
(1by default) post-normalization. Given the above observa-
tions, when paired with an activation with some slight sen-
sitivity to weight magnitude ( e.g. GELUs, see Appendix F),
this scaling can now be interpreted as a learnable shift in
complexity, modulated by a single scalar (rather than the
whole weight matrix without the normalization).
Residual connections [31]. We add these such that each
non-linearity is now described with: x(x+ϕ(x)). This
has the dramatic effect of forcing the preferred complexity
to some of the lowest levels for all activations regardless of
depth. This can be explained by the fact that residual con-
nections essentially bypass the stacking of non-linearities
that causes the increased complexity with increased depth.
Multiplicative interactions refer to multiplications of in-
ternal representations with one another [39] as in attention
layers, highway networks, dynamic convolutions, etc. We
place them in our MLPs as gating operations, such that each
hidden layer corresponds to: x
ϕ(Wx+b)σ(Wx+
b)
where σ()is the logistic function. This creates a clear
increase in complexity dependent on depth and weight mag-
4ReLU GELU Swish SELU Tanh Gaussian Sin Unbiased
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
Figure 4. Heatmaps of the average Fourier complexity of functions implemented by random-weight networks. Each heatmap corresponds
to an activation function and each cell (within a heatmap) corresponds to a depth (heatmap columns) and weight magnitude (heatmap
rows). We also show grayscale images of functions implemented by networks of an architecture corresponding to every other heatmap cell.
ReLU GELU Swish SELU TanH Gaussian Sin UnbiasedComplexity (Fourier)
1 1001
1 1001
1 1001
1 1001
1 5001
1 3001
1 2001
1 1001
    MLP
 + Gating
 + Residual
 + Layer norm.
Figure 5. The complexity of random models (Y axis) generally increases with weights / activations magnitudes (X axis). The sensitivity
is however very different across activation functions. This sensitivity also increases with multiplicative interactions ( i.e. gating), decreases
with residual connections, and is essentially absent with layer normalization.
nitude, even for ReLU activations. This agrees with prior
work showing that multiplicative interactions in polynomial
networks [10] facilitate learning high frequencies.
Unbiased model. As a counter-example to models show-
ing some preferred complexity, we construct an architec-
ture with no bias by design in the complexity measured
with Fourier frequencies. This special architecture imple-
ments an inverse Fourier transform, parametrized directly
with the coefficients and phase shifts of the Fourier com-
ponents (details in Appendix D). The inverse Fourier trans-
form is a weighted sum of sine waves, so this architecture
can be implemented as a one-layer MLP with sine activa-
tions and fixed input weights representing each one Fourier
component. These fixed weights prior to sine activations
thus enforce a uniform prior over frequencies .
This architecture behaves very differently from standard
MLPs (Figure 4). With random weights, its Fourier spec-
trum is uniform, which gives a high complexity for any
weight magnitude (depth is fixed). Functions implemented
by this architecture look like white noise. Even though
this architecture can be trained by gradient descent like any
MLP, we show in Appendix E that it is practically useless
because of its lack of any complexity bias.
0 LZ 101Fourier
0 LZ 101Legendre
0 LZ 101ChebyshevFigure 6. Our various complexity measures are closely correlated
despite measuring each a different proxy i.e. frequency (Fourier),
polynomial order (Legendre, Chebyshev), or compressibility (LZ).
Importance of ReLU activations
The fact that a strong simplicity bias depends on ReLU
activations suggests that their historical importance in
the development of deep learning goes beyond the com-
mon narrative about vanishing gradients [42]. The same
may apply to residual connections and layer normaliza-
tion since they alsox contribute strongly to the simplic-
ity bias. This contrasts with the current literature that
mostly invokes their numerical properties [6, 82, 83].
54. Inductive Biases in Trained Models
We now examine how the inductive biases of an architec-
ture impact a model trained by standard gradient descent.
We will show that there is a strong correlation between the
complexity at initialization ( i.e. with random weights as ex-
amined in the previous section) and in the trained model.
We will also see that unusual architectures with a bias to-
wards high complexity can improve generalization on tasks
where the standard simplicity bias is suboptimal.
4.1. Learning Complex Functions
The NRS proposes that good generalization requires a good
match between the complexity preferred by the architecture
and the target function. We verify this claim by demon-
strating improved generalization on complex tasks with ar-
chitectures biased towards higher complexity. This is also
a proof of concept of the potential utility of controlling in-
ductive biases.
Experimental setup. We consider a simple binary classi-
fication task involving modulo arithmetic. Such tasks like
the parity function [66] are known to be challenging for
standard architectures because they contain high-frequency
patterns. The input to our task is a vector of integers x
[0, N1]d. The correct labels are 1(Σxi(M/2) mod M).
We consider three versions with N=16 andM={10,7,4}
that correspond to increasingly higher frequencies in the tar-
get function (see Figure 7 and Appendix D for details).
Results. We see in Figure 7 that a ReLU MLP only solves
the low-frequency version of the task. Even though this
model can be trained to perfect training accuracy on the
higher-frequency versions, it then fails to generalize be-
cause of its simplicity bias. We then train MLPs with other
activations (TanH, Gaussian, sine) whose preferred com-
plexity is sensitive to the activations magnitude. We also
introduce a constant multiplicative prefactor before each
activation function to modulate this bias without changing
the weights magnitude, which could introduce optimiza-
tion side effects. Some of these models succeed in learn-
ing all versions of the task when the prefactor is correctly
tuned. For higher-frequency versions, the prefactor needs
to be larger to shift the bias towards higher complexity. In
Figure 7, we fit a quadratic approximation to the accuracy
of Gaussian-activated models. The peak then clearly shifts
to the right on the complex tasks. This agrees with the
NRS proposition that complexity at initialization relates
to properties of the trained model .
Let us also note that not all activations succeed, even with
a tuned prefactor. This shows that matching the complexity
of the architecture and of the target function is beneficial but
not sufficient for good generalization. The inductive biases
of an architecture are clearly not fully captured by any of
our measures of complexity.Target function (3 versions of modulo addition)
Low freq.
 Med. freq.
 High freq.Test accuracy
0 10.51
0 10.51
0 10.51
Complexity (LZ) at initialization
(modulated by choices of activation and prefactor value)
mlpRelu mlpGelu mlpSwish mlpTanh mlpGaussian mlpSin
Figure 7. Results training networks on three tasks of increasing
complexity. Each point represents a different architecture. ReLU-
like activations are biased towards low complexity and fail to
generalize on complex tasks. With other activations , the com-
plexity bias depends on the activation magnitudes, which we can
control with a multiplicative prefactor. This enables generaliza-
tion on complex tasks by shifting the bias to higher complexity.
Indeed, the optimum prefactor (peak of the quadratic fit) shifts to
the right on each task of increasing complexity.
Low frequency Medium frequency High frequencyTest accuracy
01
0 2 4 6 8
Prefactor0.51
01
0 2 4 6 8
Prefactor0.51
01
0 2 4 6 8
Prefactor0.51
Figure 8. Detail of Figure 7 for Gaussian activations. The peak
accuracy shifts to the right on tasks of increasing complexity. This
corresponds to a larger prefactor that shifts the bias towards higher
complexity. Each point represents one random seed.
Reinterpretation of existing work
Loss landscapes Are All You Need [9]
Chiang et al. showed that networks with random
weights, as long as they fit the training data with low
training loss, are good solutions that generalize to the
test data. We find  loss landscapes  slightly misleading
because the key is in the parametrization of the network
(and by extension of this landscape) and not in the loss
function. Their results can be replicated by replacing the
cross-entropy loss with an MSE loss, but not by replac-
ing their MLP with our unbiased learner architecture.
The sampled solutions are good, not only because of
their low training loss, but because they are found by
uniformly sampling the weight space. Bad low-loss so-
lutions also exist, but they are unlikely to be found by
random sampling. Because of the NRS, all random-
weight networks implement simple functions, which
generalize as long as they fit the training data. An al-
ternative title could be  Uniformly Sampling the Weight
Space Is All You Need .
64.2. Impact on Shortcut Learning
Shortcut learning refers to situations where the simplicity
bias causes a model to rely on simple spurious features
rather than learning the more-complex target function [65].
Experimental setup. We consider a regression task sim-
ilar to Colored-MNIST. Inputs are images of handwritten
digits juxtaposed with a uniform band of colored pixels that
simulate spurious features. The labels in the training data
are values in [0,1]proportional to the digit value as well
as to the color intensity. Therefore, a model can attain high
training accuracy by relying either on the simple linear rela-
tion with the color, or the more complex recognition of the
digits (the target task). To measure the reliance of a model
on color or digit, we use two test sets where either the color
or digit is correlated with the label while the other is ran-
domized. See Appendix D for details.
Results. We train 2-layer MLPs with different activa-
tion functions. We also use a multiplicative prefactor, i.e.
a constant αR+placed before each activation func-
tion such that each non-linear layer performs the following:
xϕ
α(Wx+b)
. The prefactor mimics a rescaling of
the weights and biases with no optimization side effects.
We see in Figure 9 that the LZ complexity at initialization
increases with prefactor values for TanH, Gaussian, and sine
activations. Most interestingly, the accuracy on the digit and
color also varies with the prefactor. The color is learned
more easily with small prefactors (corresponding to a low
complexity at initialization) while the digit is learned more
easily at an intermediate value (corresponding to medium
complexity at initialization). The best performance on the
digit is reached at a sweet spot that we explain as the hy-
pothesized best match between the complexity of the tar-
get function, and that preferred by the architecture. With
larger prefactors, i.e. beyond this sweet spot, the accuracy
on the digit decreases, and even more so with sine activa-
tions for which the complexity also increases more rapidly,
further supporting the proposed explanation.
TanH Gaussian SineTest accuracy Complexity at init. (LZ)
01
0 1 2 3 4
Prefactor0.51
Digit
Color
01
0 1 2 3 4
Prefactor0.51
Digit
Color
01
0 1 2 3 4
Prefactor0.51
Digit
Color
Figure 9. Experiments on Colored-MNIST show a clear corre-
lation between complexity at initialization (top) and test accuracy
(bottom). Models with a bias for low complexity rely on the color
i.e. the simpler feature. The accuracy on the digit peaks at a sweet
spot where the models preferred complexity matches the digits.Reinterpretation of existing work
How You Start Matters for Generalization [59]
Ramasinghe et al. examine implicit neural represen-
tations ( i.e. a network trained to represent one image).
They observe that models showing high frequencies at
initialization also learn high frequencies better. They
conclude that complexity at initialization causally in-
fluences the solution. But our results suggest instead
that these are two effects of a common cause: the ar-
chitecture is biased towards a certain complexity, which
influences both the untrained model and the solutions
found by gradient descent. There exist configurations of
weights that correspond to complex functions, but they
are unlikely to be found in either case. Appendix E.3
shows that initializing GD from such a solution with
an architecture biased toward simplicity does not yield
complex solutions, thus disproving the causal relation.
5. Transformers are Biased Towards
Compressible Sequences
We now show that the inductive biases observed with MLPs
are relevant to transformer sequence models. The experi-
ments below confirm the bias of a transformer for generat-
ing simple, compressible sequences [29] which could then
explain the tendency of language models to repeat them-
selves [21, 34]. The experiments also suggest that trans-
formers inherit this inductive bias from the same compo-
nents as those explaining the simplicity bias in MLPs.
Experimental setup. We sample sequences from an un-
trained GPT-2 [55]. For each sequence, we sample random
weights from their default initial distribution, then prompt
the model with one random token (all of them being equiva-
lent since the model is untrained), then generate a sequence
of 100 tokens by greedy maximum-likelihood decoding.
We evaluate the complexity of each sequence with the LZ
measure (Section 2) and report the average over 1,000 se-
quences. We evaluate variations of the architecture: replac-
ing activation functions in MLP blocks (GELUs by default),
varying the depth ( 12transformer blocks by default), and
varying the activations magnitude by modifying the scal-
ing factor in layer normalizations ( 1by default).
Results. We first observe in Figure 10 that the default
architecture is biased towards relatively simple sequences.
This observation, already reported by Goldblum et al. [29],
is non-trivial since a random model could as well gener-
ate completely random sequences. Changing the activation
function from the default GELUs has a large effect. The
complexity increases with SELU, TanH, sine, and decreases
with ReLU. It is initially low with Gaussian activations, but
climbs higher than most others with larger activation magni-
tudes. This is consistent with observations made on MLPs,
7where ReLU induced the strongest bias for simplicity, and
TanH, Gaussian, sine for complexity. Variations of acti-
vations magnitude (via scaling in layer normalizations)
has the same monotonic effect on complexity as observed
in MLPs. However, we lack an explanation for the shoul-
ders in the curves of SELU, Tanh, and sine. It may relate to
them being the activations that output negative values most.
Varying depth also has the expected effect of magnifying
the differences across activations and scales.
Complexity (LZ)
1 3 6 9 1201
0 1 2 3 4 5 6 701
Relu
GeLU
Swish
SeLU
TanH
Gaussian
Sine
Depth (num. of transformer Scaling in layer
blocks, scaling=1) normalization (depth=12)
Figure 10. Average complexity (LZ) of sequences generated by
an untrained GPT-2. Variations of the architecture correspond to
variations in complexity comparable to MLPs. This suggests that
transformers inherit a bias for simple sequences from their build-
ing blocks via mechanisms similar to those in simple models.
Take-away. These results suggest that the bias for sim-
ple sequences of transformers originates from their build-
ing blocks via similar mechanisms to those causing the
simplicity bias in other predictive models. The building
blocks of transformers also seem to balance a shift to-
wards higher complexity (attention, multiplicative interac-
tions) and lower complexity (GELUs, layer normalizations,
residual connections).
6. Related Work
Much effort has gone into explaining the success of deep
learning through the inductive biases of SGD [48] and struc-
tured architectures [11, 90]. This work rather focuses on
implicit inductive biases from unstructured architectures.
The simplicity bias is the tendency of NNs to fit data
with simple functions [3, 25, 53, 75]. The spectral bias
suggests that NNs prioritize learning low-frequency compo-
nents of the target function [57, 84]. These studies confound
architectures and optimization. And most explanations in-
voke implicit regularization of gradient descent [70, 85] and
are specific to ReLU networks [35, 38, 88]. In contrast, we
show that some form of spectral bias exists in common ar-
chitectures independently of gradient descent.
A related line of study showed that Boolean MLPs are bi-
ased towards low-entropy functions [12, 44]. Work closer to
ours [12, 44, 79] examines the simplicity bias of networks
with random weights . These works are limited to MLPs
with binary inputs or outputs [12, 44], ReLU activations,
and simplicity measured as compressibility. In contrast, our
work examines multiple measures of simplicity and a widerset of architectures. In work concurrent to ours, Abbe et al.
[1] used Walsh decompositions (analogous to Fourier se-
ries for binary functions) to characterize the simplicity of
learned binary classification networks. Their discussion is
specific to classification and highly complementary to ours.
Our work also provides a new lens to explain why
choices of activation functions are critical [16, 60, 67]. See
Appendix A for an extended literature review.
7. Conclusions
We examined inductive biases that NNs possess indepen-
dently of their optimization. We found that the parame-
ter space of popular architectures corresponds overwhelm-
ingly to functions with three quantifiable properties: low
frequency, low order, and compressibility. They correspond
to the simplicity bias previously observed in trained models
which we now explain without involving (S)GD. We also
showed that the simplicity bias is not universal to all ar-
chitectures. It results from ReLUs, residual connections,
layer normalization, etc. The popularity of these compo-
nents likely reflects the collective search for architectures
that perform well on real-world data. In short, the effective-
ness of NNs is not an intrinsic property but the result of the
adequacy between key choices ( e.g. ReLUs) and properties
of real-world data (prevalence of low-complexity patterns).
Limitations and open questions.
 Our analysis used mostly small models and data to en-
able visualizations (2D function maps) and computations
(Fourier decompositions). We showed the relevance of
our findings to large transformers, but the study could be
extended to other large architectures and tasks.
 Our analysis relies on empirical simulations . It could be
carried out analytically to provide theoretical insights.
 Our results do not invalidate prior work on implicit biases
of (S)GD. Future work should clarify the interplay of dif-
ferent sources of inductive biases . Even if most of the
parameter space corresponds to simple functions, GD can
navigate to complex ones. Are they isolated points in pa-
rameter space, islands, or connected regions? This relates
to mode connectivity, lottery tickets [19], and the hypoth-
esis that good flat minima occupy a large volume [37].
 We proposed three quantifiable facets of inductive bi-
ases. Much is missed about the shape of functions pre-
ferred by different activations (Figure 3). An extension
could discover other reasons for the success of NNs and
fundamental properties shared across real-world datasets.
 An application of our findings is in the control of
inductive biases to nudge the behaviour of trained
networks [87]. For example by manipulating the
parametrization of NNs on which (S)GD is performed.
8Acknowledgements
This work was partially supported by the Australian Re-
search Council (DP240103278).
References
[1] Emmanuel Abbe, Samy Bengio, Aryo Lotfi, and Kevin Rizk.
Generalization on the unseen, logic reasoning and degree
curriculum. arXiv preprint arXiv:2301.13105 , 2023. 8, 1
[2] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Im-
plicit regularization in deep matrix factorization. NeurIPS ,
32, 2019. 1
[3] Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David
Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan
Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio,
et al. A closer look at memorization in deep networks. In
ICML , pages 233242. PMLR, 2017. 2, 8, 1
[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. Layer normalization. arXiv preprint arXiv:1607.06450 ,
2016. 4
[5] Gregor Bachmann, Sotiris Anagnostidis, and Thomas Hof-
mann. Scaling MLPs: A tale of inductive bias. arXiv preprint
arXiv:2306.13575 , 2023. 2, 4
[6] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis,
Kurt Wan-Duo Ma, and Brian McWilliams. The shattered
gradients problem: If resnets are the answer, then what is the
question? In International Conference on Machine Learn-
ing, pages 342350. PMLR, 2017. 5
[7] Satwik Bhattamishra, Arkil Patel, Varun Kanade, and Phil
Blunsom. Simplicity bias in transformers and their abil-
ity to learn sparse boolean functions. arXiv preprint
arXiv:2211.12316 , 2022. 2
[8] Akhilan Boopathy, Kevin Liu, Jaedong Hwang, Shu Ge,
Asaad Mohammedsaleh, and Ila R Fiete. Model-agnostic
measure of generalization difficulty. In ICML , pages 2857
2884. PMLR, 2023. 1
[9] Ping-yeh Chiang, Renkun Ni, David Yu Miller, Arpit Bansal,
Jonas Geiping, Micah Goldblum, and Tom Goldstein. Loss
landscapes are all you need: Neural network generalization
can be explained without the implicit bias of gradient de-
scent. In ICLR , 2022. 1, 2, 6
[10] Moulik Choraria, Leello Tadesse Dadi, Grigorios Chrysos,
Julien Mairal, and V olkan Cevher. The spectral bias of poly-
nomial neural networks. arXiv preprint arXiv:2202.13473 ,
2022. 5, 2
[11] Nadav Cohen and Amnon Shashua. Inductive bias of deep
convolutional networks through pooling geometry. arXiv
preprint arXiv:1605.06743 , 2016. 8, 1
[12] Giacomo De Palma, Bobak Kiani, and Seth Lloyd. Random
deep neural networks are biased towards simple functions.
NeurIPS , 32, 2019. 8, 1
[13] Gr egoire Del etang, Anian Ruoss, Jordi Grau-Moya, Tim
Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy,
Marcus Hutter, Shane Legg, Joel Veness, et al. Neu-
ral networks and the chomsky hierarchy. arXiv preprint
arXiv:2207.02098 , 2022. 1[14] Benoit Dherin, Michael Munn, Mihaela Rosca, and David
Barrett. Why neural networks find simple solutions: The
many regularizers of geometric complexity. NeurIPS , 35:
23332349, 2022. 2
[15] Kamaludin Dingle, Chico Q Camargo, and Ard A Louis.
Inputoutput maps are strongly biased towards simple out-
puts. Nature communications , 9(1):761, 2018. 2, 3, 1
[16] Shiv Ram Dubey, Satish Kumar Singh, and Bidyut Baran
Chaudhuri. Activation functions in deep learning: A com-
prehensive survey and benchmark. Neurocomputing , 2022.
4, 8, 2, 3
[17] Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam
Neyshabur. The role of permutation invariance in lin-
ear mode connectivity of neural networks. arXiv preprint
arXiv:2110.06296 , 2021. 1
[18] Emanuele Francazi, Aurelien Lucchi, and Marco Baity-Jesi.
Initial guessing bias: How untrained networks favor some
classes. arXiv preprint arXiv:2306.00809 , 2023. 2
[19] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy,
and Michael Carbin. Linear mode connectivity and the lot-
tery ticket hypothesis. In International Conference on Ma-
chine Learning , pages 32593269. PMLR, 2020. 8
[20] Spencer Frei, Gal Vardi, Peter L Bartlett, Nathan Srebro, and
Wei Hu. Implicit bias in leaky relu networks trained on high-
dimensional data. arXiv preprint arXiv:2210.07082 , 2022. 1
[21] Zihao Fu, Wai Lam, Anthony Man-Cho So, and Bei Shi. A
theoretical analysis of the repetition problem in text genera-
tion. In AAAI , pages 1284812856, 2021. 7, 4
[22] Gallant. There exists a neural network that does not make
avoidable mistakes. In IEEE International Conference on
Neural Networks , pages 657664. IEEE, 1988. 2
[23] Adri a Garriga-Alonso, Carl Edward Rasmussen, and Lau-
rence Aitchison. Deep convolutional networks as shallow
gaussian processes. arXiv preprint arXiv:1808.05587 , 2018.
1
[24] Jonas Geiping, Micah Goldblum, Phillip E Pope, Michael
Moeller, and Tom Goldstein. Stochastic training is not nec-
essary for generalization. arXiv preprint arXiv:2109.14119 ,
2021. 1
[25] Robert Geirhos, J orn-Henrik Jacobsen, Claudio Michaelis,
Richard Zemel, Wieland Brendel, Matthias Bethge, and Fe-
lix A Wichmann. Shortcut learning in deep neural networks.
Nature Machine Intelligence , 2(11):665673, 2020. 8, 1
[26] Alan Martin Gilkes. Photograph enhancement by adaptive
digital unsharp masking. PhD thesis, Massachusetts Institute
of Technology, 1974. 5
[27] Raja Giryes, Guillermo Sapiro, and Alex M Bronstein. Deep
neural networks with random gaussian weights: A universal
classification strategy? IEEE Transactions on Signal Pro-
cessing , 64(13):34443457, 2016. 1
[28] Xavier Glorot and Yoshua Bengio. Understanding the diffi-
culty of training deep feedforward neural networks. In Inter-
national Conference on Artificial Intelligence and Statistics ,
pages 249256. JMLR, 2010. 3
[29] Micah Goldblum, Marc Finzi, Keefer Rowan, and An-
drew Gordon Wilson. The no free lunch theorem, kol-
mogorov complexity, and the role of inductive biases in ma-
9chine learning. arXiv preprint arXiv:2304.05366 , 2023. 1,
2, 3, 7
[30] Michael Hahn, Dan Jurafsky, and Richard Futrell. Sensitivity
as a complexity measure for sequence classification tasks.
Transactions of the ACL , 9:891908, 2021. 2
[31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
pages 770778, 2016. 4
[32] Katherine L Hermann and Andrew K Lampinen. What
shapes feature representations? exploring datasets, architec-
tures, and training. arXiv preprint arXiv:2006.12433 , 2020.
1
[33] Katherine L Hermann, Hossein Mobahi, Thomas Fel, and
Michael C Mozer. On the foundations of shortcut learning.
arXiv preprint arXiv:2310.16228 , 2023. 2
[34] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin
Choi. The curious case of neural text degeneration. arXiv
preprint arXiv:1904.09751 , 2019. 7, 4
[35] Qingguo Hong, Jonathan W Siegel, Qinyang Tan, and
Jinchao Xu. On the activation function dependence of
the spectral bias of neural networks. arXiv preprint
arXiv:2208.04924 , 2022. 8, 1
[36] Kurt Hornik, Maxwell Stinchcombe, and Halbert White.
Multilayer feedforward networks are universal approxima-
tors. Neural networks , 2(5):359366, 1989. 1, 4
[37] W. Ronny Huang, Zeyad Emam, Micah Goldblum, Liam
Fowl, Justin K. Terry, Furong Huang, and Tom Goldstein.
Understanding generalization through visualizations. In I
Cant Believe Its Not Better! NeurIPS Workshop , pages
8797. PMLR, 2020. 2, 8
[38] Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian
Cheung, Pulkit Agrawal, and Phillip Isola. The low-
rank simplicity bias in deep networks. arXiv preprint
arXiv:2103.10427 , 2021. 1, 8
[39] Siddhant M Jayakumar, Wojciech M Czarnecki, Jacob
Menick, Jonathan Schwarz, Jack Rae, Simon Osindero,
Yee Whye Teh, Tim Harley, and Razvan Pascanu. Multi-
plicative interactions and where to find them. In ICLR , 2020.
4
[40] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S
Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein.
Deep neural networks as gaussian processes. arXiv preprint
arXiv:1711.00165 , 2017. 1
[41] Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora.
Gradient descent on two-layer nets: Margin maximization
and simplicity bias. NeurIPS , 34:1297812991, 2021. 1
[42] Andrew L Maas, Awni Y Hannun, Andrew Y Ng, et al. Rec-
tifier nonlinearities improve neural network acoustic models.
InICML , page 3. Atlanta, GA, 2013. 2, 5
[43] Alexander G de G Matthews, Mark Rowland, Jiri Hron,
Richard E Turner, and Zoubin Ghahramani. Gaussian pro-
cess behaviour in wide deep neural networks. In ICLR , 2018.
1
[44] Chris Mingard, Joar Skalse, Guillermo Valle-P erez, David
Mart ınez-Rubio, Vladimir Mikulik, and Ard A Louis. Neural
networks are a priori biased towards boolean functions with
low entropy. arXiv preprint arXiv:1909.11522 , 2019. 8, 1, 2[45] Tom M Mitchell. The need for biases in learning general-
izations. Rutgers University CS tech report CBM-TR-117 ,
1980. 1
[46] Amirkeivan Mohtashami, Martin Jaggi, and Sebastian U
Stich. Special properties of gradient descent with large learn-
ing rates. arXiv preprint arXiv:2205.15142 , 2023. 1
[47] Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and
Yoshua Bengio. On the number of linear regions of deep
neural networks. NeurIPS , 27, 2014. 5
[48] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In
search of the real inductive bias: On the role of implicit regu-
larization in deep learning. arXiv preprint arXiv:1412.6614 ,
2014. 8, 1
[49] Elisa Oostwal, Michiel Straat, and Michael Biehl. Hidden
unit specialization in layered neural networks: Relu vs. sig-
moidal activation. Physica A: Statistical Mechanics and its
Applications , 564:125517, 2021. 2
[50] Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli.
The emergence of spectral universality in deep networks.
InInternational Conference on Artificial Intelligence and
Statistics , pages 19241932. PMLR, 2018. 1
[51] Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammar-
ion. Implicit bias of sgd for diagonal linear networks: a
provable benefit of stochasticity. NeurIPS , 34:2921829230,
2021. 1
[52] Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio,
Aaron C Courville, Doina Precup, and Guillaume Lajoie.
Gradient starvation: A learning proclivity in neural net-
works. NeurIPS , 34:12561272, 2021. 2, 1
[53] Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Mi-
randa, Lorenzo Rosasco, Xavier Boix, Jack Hidary, and
Hrushikesh Mhaskar. Theory of deep learning III: the non-
overfitting puzzle. CBMM Memo , 73:138, 2018. 8, 1
[54] Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-
Dickstein, and Surya Ganguli. Exponential expressivity in
deep neural networks through transient chaos. NeurIPS , 29,
2016. 1
[55] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, Ilya Sutskever, et al. Language models are unsu-
pervised multitask learners. OpenAI blog , 1(8):9, 2019. 4,
7
[56] Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli,
and Jascha Sohl-Dickstein. On the expressive power of deep
neural networks. In ICML , pages 28472854. PMLR, 2017.
1
[57] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix
Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and
Aaron Courville. On the spectral bias of neural networks.
InICML , pages 53015310. PMLR, 2019. 2, 3, 8, 1
[58] Sameera Ramasinghe and Simon Lucey. Beyond periodicity:
Towards a unifying framework for activations in coordinate-
MLPs. In ECCV , pages 142158. Springer, 2022. 4, 2
[59] Sameera Ramasinghe, Lachlan MacDonald, Moshiur Farazi,
Hemanth Saratchandran, and Simon Lucey. How you start
matters for generalization. arXiv preprint arXiv:2206.08558 ,
2022. 7, 9
10[60] Sameera Ramasinghe, Lachlan E MacDonald, and Simon
Lucey. On the frequency-bias of coordinate-MLPs. NeurIPS ,
35:796809, 2022. 8, 2
[61] Vishwanath Saragadam, Daniel LeJeune, Jasper Tan, Guha
Balakrishnan, Ashok Veeraraghavan, and Richard G Bara-
niuk. Wire: Wavelet implicit neural representations. In
CVPR , pages 1850718516, 2023. 2
[62] J urgen Schmidhuber. Discovering neural nets with low
kolmogorov complexity and high generalization capability.
Neural Networks , 10(5):857873, 1997. 1
[63] Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and
Jascha Sohl-Dickstein. Deep information propagation. arXiv
preprint arXiv:1611.01232 , 2016. 1, 2
[64] Luca Scimeca, Seong Joon Oh, Sanghyuk Chun, Michael
Poli, and Sangdoo Yun. Which shortcut cues will dnns
choose? a study from the parameter-space perspective. arXiv
preprint arXiv:2110.03095 , 2021. 2
[65] Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek
Jain, and Praneeth Netrapalli. The pitfalls of simplicity bias
in neural networks. NeurIPS , 33:95739585, 2020. 2, 7
[66] Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah.
Failures of gradient-based deep learning. In ICML , pages
30673075. PMLR, 2017. 6
[67] James Benjamin Simon, Sajant Anand, and Mike Deweese.
Reverse engineering the neural tangent kernel. In ICML ,
pages 2021520231. PMLR, 2022. 8, 2
[68] Vincent Sitzmann, Julien Martel, Alexander Bergman, David
Lindell, and Gordon Wetzstein. Implicit neural representa-
tions with periodic activation functions. NeurIPS , 33:7462
7473, 2020. 4, 8
[69] Samuel L Smith, Benoit Dherin, David GT Barrett, and So-
ham De. On the origin of implicit regularization in stochastic
gradient descent. arXiv preprint arXiv:2101.12176 , 2021. 1
[70] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya
Gunasekar, and Nathan Srebro. The implicit bias of gradient
descent on separable data. The Journal of Machine Learning
Research , 19(1):28222878, 2018. 8, 1
[71] Joshua Stock, Jens Wettlaufer, Daniel Demmler, and
Hannes Federrath. Property unlearning: A defense strat-
egy against property inference attacks. arXiv preprint
arXiv:2205.08821 , 2022. 1
[72] Richard Sutton. The bitter lesson. Incomplete Ideas (blog) ,
13(1), 2019. 1
[73] Remi Tachet, Mohammad Pezeshki, Samira Shabanian,
Aaron Courville, and Yoshua Bengio. On the learn-
ing dynamics of deep neural networks. arXiv preprint
arXiv:1809.06848 , 2018. 1
[74] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara
Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-
mamoorthi, Jonathan Barron, and Ren Ng. Fourier features
let networks learn high frequency functions in low dimen-
sional domains. NeurIPS , 33:75377547, 2020. 2
[75] Damien Teney, Ehsan Abbasnejad, Simon Lucey, and Anton
van den Hengel. Evading the simplicity bias: Training a
diverse set of models discovers solutions with superior ood
generalization. arXiv preprint arXiv:2105.05612 , 2021. 8,
1, 2[76] Damien Teney, Maxime Peyrard, and Ehsan Abbasnejad.
Predicting is not understanding: Recognizing and addressing
underspecification in machine learning. In European Confer-
ence on Computer Vision , pages 458476. Springer, 2022. 2
[77] Ryan Theisen, Jason Klusowski, and Michael Mahoney.
Good classifiers are abundant in the interpolating regime. In
AISTATS , pages 33763384. PMLR, 2021. 2
[78] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
Deep image prior. In CVPR , pages 94469454, 2018. 1
[79] Guillermo Valle-Perez, Chico Q Camargo, and Ard A Louis.
Deep learning generalizes because the parameter-function
map is biased towards simple functions. arXiv preprint
arXiv:1805.08522 , 2018. 2, 3, 8, 1
[80] Gal Vardi. On the implicit bias in deep-learning algorithms.
Communications of the ACM , 66(6):8693, 2023. 2
[81] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany,
Shiqin Yan, Numair Khan, Federico Tombari, James Tomp-
kin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in
visual computing and beyond. In Computer Graphics Forum ,
pages 641676. Wiley Online Library, 2022. 2, 5
[82] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin
Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei
Wang, and Tieyan Liu. On layer normalization in the trans-
former architecture. In International Conference on Machine
Learning , pages 1052410533. PMLR, 2020. 5
[83] Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and
Junyang Lin. Understanding and improving layer normaliza-
tion. Advances in Neural Information Processing Systems ,
32, 2019. 5
[84] Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao,
and Zheng Ma. Frequency principle: Fourier analy-
sis sheds light on deep neural networks. arXiv preprint
arXiv:1901.06523 , 2019. 2, 8, 1, 3
[85] Zhi-Qin John Xu, Yaoyu Zhang, and Yanyang Xiao. Training
behavior of deep neural network in frequency domain. In
ICONIP , pages 264274. Springer, 2019. 8, 1
[86] Greg Yang and Hadi Salman. A fine-grained spec-
tral perspective on neural networks. arXiv preprint
arXiv:1907.10599 , 2019. 1
[87] Enyan Zhang, Michael A Lepori, and Ellie Pavlick. In-
stilling inductive biases with subnetworks. arXiv preprint
arXiv:2310.10899 , 2023. 8
[88] Shijun Zhang, Hongkai Zhao, Yimin Zhong, and Haomin
Zhou. Why shallow networks struggle with approximat-
ing and learning high frequency: A numerical study. arXiv
preprint arXiv:2306.17301 , 2023. 8, 1
[89] Allan Zhou, Kaien Yang, Kaylee Burns, Yiding Jiang,
Samuel Sokota, J Zico Kolter, and Chelsea Finn. Per-
mutation equivariant neural functionals. arXiv preprint
arXiv:2302.14040 , 2023. 1
[90] Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin,
Omid Saremi, Josh Susskind, Samy Bengio, and Preetum
Nakkiran. What algorithms can transformers learn? a study
in length generalization. arXiv preprint arXiv:2310.16028 ,
2023. 8, 1, 2
11Neural Redshift: Random Networks are not Random Functions
Supplementary Material
A. Additional Related Work
A vast literature examines the success of deep learn-
ing using inductive biases of optimization methods
(e.g. SGD [48]) and architectures ( e.g. CNNs [11], trans-
formers [90]). This paper instead examines implicit induc-
tive biases in unstructured architectures.
Parametrization of NNs. It is challenging to under-
stand the structure and effective dimensionality of the
weight space of NNs because multiple weight configura-
tions and their permutations correspond to the same func-
tion [17, 71, 89]. A recent study quantified the information
needed to identify a model with good generalization [8].
However, the estimated values are astronomical (meaning
that no dataset would ever be large enough to learn the tar-
get function). Our work reconciles these results with the
reality (the fact that deep learning does work in practice)
by showing that the overlap of the set of good generalizing
functions with uniform samples in weight space [8, Fig. 1]
is much denser than its overlap with truly random functions.
In other words, random sampling in weight space generally
yields functions likely to generalize. Much less information
is needed to pick one solution among those than estimated
in [8].
Some think that  stronger inductive biases come at the
cost of decreasing the universality of a model  [13]. This is
a misunderstanding of the role of inductive biases: they are
fundamentally necessary for machine learning and they do
not imply a restriction on the set of learnable functions. We
show in particular that MLPs have strong inductive biases
yet remain universal.
The simplicity bias refers to the observed tendency of NNs
to fit their training data with simple functions. It is desirable
when it prevents overparametrized networks from overfit-
ting the training data [3, 53]. But it is a curse when it causes
shortcut learning [25, 75]. Most papers on this topic are
about trained networks, hence they confound the inductive
biases of the architectures and of the optimization. Most ex-
planations of the simplicity bias involve loss functions [52]
and gradient descent [2, 32, 41, 73].
Work closer to ours [12, 44, 79] examines the simplic-
ity bias of networks with random weights . These studies
are limited to MLPs with binary inputs/outputs, ReLU ac-
tivations, and/or simplicity measured as compressibility. In
contrast, we examine more architectures and other measures
of complexity. Earlier works with random-weight networks
include [23, 27, 40, 43, 50, 54, 56, 63].
Goldblum et al. [29] proposed that NNs are effective be-cause they combine a simplicity bias with a flexible hypoth-
esis space. Thus they can represent complex functions and
benefit from large datasets. Our results also support this
argument.
The spectral bias [57] or frequency principle [84] is a par-
ticular form of the simplicity bias. It refers to the observa-
tion that NNs learn low-frequency components of the target
function earlier during training.5Works on this topic are
specific to gradient descent [70, 85]. and often to ReLU
networks [35, 38, 88]. Our work is about properties of ar-
chitectures independent of the training.
Work closer to ours [86] has noted that the spectral bias
exists with ReLUs but not with sigmoidal activations, and
that it depends on weight magnitudes and depth (all of
which we also observe in our experiments). Their analy-
sis uses the neural tangent kernel (NTK) whereas we use
a Fourier decomposition of the learned function, which is
arguably more direct and intuitive. We also examine other
notions of complexity, and other architectures.
In work concurrent to ours, Abbe et al. [1] used Walsh
decompositions (a variant of Fourier analysis suited to bi-
nary functions) to characterize learned binary classification
networks. They also propose that typical NNs preferably
fit low-degree basis functions to the training data and this
explains their generalization capabilities. Their discussion,
which focuses on classification tasks, is highly complemen-
tary to ours.
The deep image prior [78] is an image processing
method that exploit the inductive biases of an untrained net-
work. However it specifically relies on convolutional (U-
Net) architectures, whose inductive biases have little to do
with those studied in this paper.
Measures of complexity. Quantifying complexity is an
open problem in the fundamental sciences. Algorithmic
information theory (AIT) and Kolmogorov complexity are
one formalization of this problem. Kolmogorov complex-
ity has been proposed as an explicit regularizer to train NNs
by Schmidhuber [62]. Dingle et al. [15] used AIT to explain
the prevalence of simplicity in the real-world with examples
in biology and finance. Building on this work, Valle-Perez
et al. [79] showed that binary ReLU networks with random
weights have a similar bias for simplicity. Our work extends
5Frequencies of the target function , used throughout this paper,
should not be confused with frequencies of the input data. For example,
high frequencies in images correspond to sharp edges. High frequencies
in the target function correspond to frequent changes of label for similar
images. A low-frequency target function means that similar inputs usually
have similar labels.
1this line of inquiry to continuous data, to other architectures,
and to other notions of complexity.
Other measures of complexity for to machine learn-
ing models include four related notions: sensitivity, Lips-
chitz constant, norms of input gradients, and Dirichlet en-
ergy [14]. Hahn et al. [30] adapted sensitivity to the dis-
crete nature of language data to measure the complexity of
language classification tasks and of models.
Simplicity bias in transformers. Zhou et al. [90] ex-
plain generalization of transformer models on toy reason-
ing tasks using a transformer-specific measure of complex-
ity. They propose that the function learned by a transformer
corresponds to the shortest program (in a custom program-
ming language) that could generate the training data. Bhat-
tamishra et al. [7] showed that transformers are more biased
for simplicity than LSTMs.
Controlling inductive biases. Recent work has inves-
tigated how to explicitly tweak the inductive biases of
NNs through learning objectives [75, 76] and architec-
tures [10, 74]. Our results confirms that the choice of acti-
vation function is critical [16]. Most studies on activation
functions focus on individual neurons [63] or compare the
generalization properties of entire networks [49]. Francazi
et al. [18] showed that some activations cause a model at
initialization to have non-uniform preference over classes.
Simon et al. [67] showed that the behaviour of a deep MLP
can be mimicked by a single-layer MLP with a specifically-
crafted activation function.
Implicit neural representations (INRs) are an application
of NNs with a need to control their spectral bias. An INR
is a regression network trained to represent e.g. one spe-
cific image by mapping image coordinates to pixel inten-
sities (they are also known as neural fields orcoordinate
MLPs ). To represent sharp image details, a network must
represent a high-frequency function, which is at odds with
the low-frequency bias of typical architectures. It has been
found that replacing ReLUs with periodic functions [81,
Sect. 5], Gaussians [58], or wavelets [61] can shift the spec-
tral bias towards higher frequencies [60]. Interestingly, such
architectures (Fourier Neural Networks) were investigated
as early as 1988 [22]. Our work shows that several findings
about INRs are also relevant to general learning tasks.
B. Why study networks with random weights?
A motivation can be found in prior work that argued for
interpreting the inductive biases of an architecture as a prior
over functions that plays in the training of the model by
gradient descent.
Valle-Perez et al. [79] and Mingard et al. [44] argued that
the probability of sampling certain functions upon random
sampling in parameter space could be treated as a prior over
functions for Bayesian inference. They then presented pre-liminary empirical evidence that training with SGD does
approximate Bayesian inference, such that the probability
of landing on particular solutions is proportional to their
prior probability when sampling random parameters.
C. Formal Statement of the NRS
We denote with
F: the target function we want to learn;
fθ: a chosen neural architecture with parameters θ;
f:=fθ: atrained network withθoptimized s.t.
fapproximates F;
f:=fθ,θpprior(θ): anuntrained random-weight net-
work with parameters drawn from an uninformed prior,
such as the uniform distribution used to initialize the net-
work prior to gradient descent.
C(f): a scalar estimate the of complexity of the func-
tionfas proposed in Section 2;
perf( f): a scalar measure of generalization perfor-
mance i.e. how well fapproximates F, for example the
accuracy on a held-out test set.
The Neural Redshift (NRS) makes three propositions.
1.NNs are biased to implement functions of a particular
level of complexity determined by the architecture.
2.This preferred complexity is observable in networks
with random weights from an uninformed prior.
Formally, architecture f, distribution pprior(θ),
preferred complexity cRs.t.
C(f) =c with very high probability, and
C(f) =g(c)with g:RRa monotonic function.
This means that the choice of architecture shifts the com-
plexity of the learned function up or down similarly as it
does an untrained models. The precise shift is usually
not predictable because g()is unknown.
3.Generalization occurs when the preferred complexity
of the architecture matches the target functions.
Formally, given two architectures f1,f2with preferred
complexities c1,c2, the one with a complexity closer to
the target functions achieves better generalization:
|C(F)g(c1)|<|C(F)g(c2)|
=perf( f
1)>perf( f
2).
For example, ReLUs are popular because their low-
complexity bias often aligns with the target function.
2D. Technical Details
Activation functions. See Figure 11 for a summary of the
activations used in our experiments and [16] for a survey.
Discrete network evaluation. For a given network that
implements the function f(x)of input xRd, we ob-
tain obtain a discrete representation as follows. We de-
fine a sequence of points Xgrid={xi}md
i=1corresponding
to a regular grid on the d-dimensional hypercube [1,1]d,
withmvalues in each dimension ( m=64in our exper-
iments) hence mdpoints in total. We evaluate the net-
work on every point. This gives the sequence of scalars
Yf={f(xi):xiXgrid}.
Visualizations as grayscale images. For a network fwith
2D inputs ( d=2) we produce a visualization as a grayscale
image as follows. The values in Yfare simply scaled and
shifted to fill the range from black ( 0) to white ( 1) as:
Y= (Ymin(Y))/(max( Y)min(Y)).
We then reshape Yinto an mmsquare image.
Measures of complexity. We use our measures of com-
plexity based on Fourier and polynomial decompositions
only with d=2because of the computational expense. These
methods first require an evaluation of the network on a dis-
crete grid as described above ( Yf) whose size grows expo-
nentially in the number of dimensions d.
Xu et al. [84] proposed two approximations for Fourier
analysis in higher dimensions. They were not used in our
experiments but could be valuable for extensions of our
work to higher-dimensional settings.
Fourier decomposition. To compute the measure of com-
plexity CFourier(f), we first precompute values of fon a dis-
crete grid Xgrid, yielding Yfas describe above. We then
perform a standard discrete Fourier decomposition with
these precomputed values. We get:
f(k) = Σ xXgridωxkf(x)
where ω=e2πi/mandkZdare discrete frequency
numbers. Per the Nyquist-Shannon theorem, with an eval-
uation of fon a grid of mvalues in each dimension, we
can reliably measure the energy for frequency numbers up
tom/2in each dimension i.e. forkK=[0,..., m/2]d.
The value f(k)is a complex number that captures both
the magnitude and phase of the kth Fourier component. We
do not care about the phase, hence our measure of com-
plexity only uses the real magnitude |f(k)|of each Fourier
component k. We then seek to summarize the distribution
of these magnitudes across frequencies into a single value.
We define the measure of complexity:
CFourier(f) = Σ kK|f(k)|.||k||2/ΣkK|f(k)|.
This is the average of magnitudes, weighted each by the
corresponding frequency, disregarding orientation ( e.g. hor-
izontal and vertical patterns in a 2D visualization of thefunction are treated similarly), and normalized such that
magnitudes sum to 1.
See [57] for a technical discussion justifying Fourier
analysis on non-periodic bounded functions.
Limitations of a scalar measure of complexity. The
above definition is necessarily imperfect at summarizing the
distributions of magnitudes across frequencies. For exam-
ple, an fcontaining both low and high-frequencies could
receive the same value as one containing only medium fre-
quencies. In practice however, we use this complexity mea-
sure on random networks, and we verified empirically that
the distributions of magnitudes are always unimodal. This
summary statistic is therefore a reasonable choice to com-
pare distributions.
Polynomial decomposition. As an alternative to Fourier
analysis, we use decomposition in polynomial series.6It
uses a predefined set of polynomials Pn(x),n= [0,..., N]
to approximate a function f(x)on the interval x[1,1]
asf(x)ΣN
c=0cnPn(x). The coefficients are calculated
ascn=0.5 (2n+ 1)R+1
1f(x)Pn(x)dx. These definitions
readily extends to higher dimensions.
In a Fourier decomposition, the coefficients indicate the
amount the various frequency components in f. Here, each
coefficient cnindicates the amount of a component of a cer-
tain order. In 2 dimensions ( d=2), we have N2coefficients
c00, c01,..., cNN. We define our measure of complexity:
CChebyshev (f) =ΣN
n1,n2=0|cn1n2|.||[n1,n2]||2
ΣN
n1,n2=0|cn1,n2|.
This definition is nearly identical to the Fourier one.
In practice, we experimented with Hermite, Legendre,
and Chebyshev bases of polynomials. We found the latter
to be more numerically stable. To compute the coefficients,
we use trapezoidal numerical integration and the same sam-
pling of fonXgridas described above, and a maximum
order N=100. To make the evaluation of the integrals more
numerically stable (especially with Legendre polynomials),
we omit a border near the edges of the domain [1,1]d.
With a 6464grid, we omit 3 values on every side.
LZ Complexity. We use the compression-based measure
of complexity described in [15, 79] as an approximation of
the Kolmogorov complexity. We first evaluate fon a grid to
getYfas described above. The values in Yfare reals and
generally unique, so we discretize them on a coarse scale of
10values regularly spaced in the range of Yf(the granular-
ity of 10is arbitrary can be set much higher with virtually no
effect if Yfis large enough). We then apply the classical
LempelZiv compression algorithm on the resulting num-
ber sequence. The measure of complexity CLZ(f)is then
defined as the size of the dictionary built by the compres-
sion algorithm. The LZ algorithm is sensitive to the order
6Seee.g.https://www.thermopedia.com/content/918/ .
3-202-202
Relu
-202-202
Gelu
-202-202
Swish
-202-202
Selu
-202-202
Tanh
-202-202
Gaussian
-202-202
Sinmax(0 , x) x.PGaussian (Xx) x.σ(x) λ(
α(ex1), x0
x, x> 0tanh( x) e(0.5x2)sin(x)
Figure 11. Activation functions used in our experiments.
of the sequence to compress, but we find very little differ-
ence across different orderings of Yf(snake, zig-zag, spiral
patterns). Thus we use a simple column-wise vectorization
of the 2D grid.
In higher dimensions (Colored-MNIST experiments), it
would be computationally too expensive to define Xgridas
a dense sampling of the full hypercube [1,1]d(since d
is large). Instead, we randomly pick mcorners of the hy-
percube and sample mpoints xiregularly between each
successive pair of them. This gives a total of m2points cor-
responding to random linear traversals of the input space.
Instead of feeding Yfdirectly to the LZ algorithm, we
also feed it with successive differences between succes-
sive values, which we found to improve the stability of
the estimated complexity (for example, the pixel values
10,12,15,18are turned into 2,3,3).
LZ Complexity with transformers. These experiments
useCLZ(f)on sequences of tokens. Each token is repre-
sented by its index in the vocabulary, and the LZ algorithm
is directly applied on these sequences of integers.
Absolute complexity values. The different measures of
complexity have different absolute scales and no compara-
ble units. Therefore, for each measure, we rescale the val-
ues such that observed values fill the range [0,1].
Unbiased model. We construct an architecture that dis-
plays no bias for any frequency in a Fourier decomposition
of the functions it implements. This architecture fθ()im-
plements an inverse discrete Fourier transform with learn-
able parameters θ={θmag,θphase}that correspond to the
magnitude and phase of each Fourier component. It can
be implement as a one-hidden-layer MLP with sine acti-
vation, fixed input weights (each channel defining the fre-
quency of one Fourier component), learnable input biases
(the phase shifts), and learnable output weights (the Fourier
coefficients).
Experiments with modulo addition. These experiments
use a 4-layer MLP of width 128. We train them with full-
batch Adam, a learning rate 0.001, for 3k iterations with
no early stopping. Each experiment is run with 5 random
seeds. The Figure 7 shows the average over seeds for clar-
ity (each point corresponds to a different architecture). Fig-
ure 8 shows all seeds (each point corresponds to a different
seed).
Experiments on Colored-MNIST. The dataset is builtfrom the MNIST digits, keeping the original separation be-
tween training and test images. To define a regression task,
we turn the original classification labels {0,1,...,9}into
values in [0,1]. To introduce a spurious feature, each im-
age is concatenated with a column of pixels of uniform
grayscale intensity (the color of the image). This color
is directly correlated with the label with some added noise
to simulate a realistic spurious feature: in 3% of the training
data, the color is replaced with a random one.
The models are 2-layer MLPs of width 64. They are
trained with an MSE loss with full-batch Adam, learning
rate 0.002, 10k iterations with no early stopping. The accu-
racy in our plots is actually: 1MAE (mean average error).
Since this is a regression task with test labels distributed
uniformly in [0,1], this metric is indeed interpretable as a
binary accuracy, with 0.5equivalent to random chance.
Experiments with transformers. In all experiments de-
scribed above, we directly examine the input  output map-
pings implemented by neural networks. In the experiments
with transformer sequence models, we examine sequences
generated by the models. These models are autoregressive,
which means that the function they implement is the map-
ping context  next token. We expect a simple function
(e.g. low-frequency) to produce lots of repetitions in se-
quences sampled auto-regressively. (language models are
indeed known to often repeat themselves [21, 34]). Such
sequences are highly compressible. They should therefore
give a low values of CLZ.
4E. Additional Experiments with
Trained Models
This section presents experiments with models trained with
standard gradient descent. We will show that there is a cor-
relation between the complexity of a model at initialization
(i.e. with random weights) and that of a trained model of the
same architecture.
Setup with coordinate-MLPs. The experiments in this
section use models trained as implicit neural representations
of images (INRs), also known as coordinate-MLPs [81].
Such a model is trained to represent a specific grayscale
image, It takes as input 2D coordinates in the image plane
x[1,1]2. It produces as output the scalar grayscale
value of the image at this location. The ground truth data
is a chosen image (Figure 12). For training, we use a subset
of pixels. For testing, we evaluate the network on a 6464
grid, which directly gives a 6464pixel representation of
the function learned.
Why use coordinate-MLPs? This setup produces in-
terpretable visualizations and allows comparing visually
the ground truth (original image) with the learned func-
tion. Because the ground truth is defined on a regular
grid (unlike most real data) it also facilitates the compu-
tation of 2D Fourier transforms. We use Fourier trans-
forms to quantitatively compare the ground truth with
the learned function and verify the third part of the NRS
(generalization is enabled by matching of the architec-
tures preferred complexity with the target functions).
Data. We use a 6464pixel version of the well-known
cameraman image (Figure 12, left) [26]. For training, we
use a random 40% of the pixels. This image contains
both uniform areas (low frequencies) and fine details with
sharp transitions (high frequencies). We also use a synthetic
waves image (Figure 12, right). It is the sum of two orthog-
onal sine waves, one twice the frequency of the other. For
training, we only use pixels on local extrema of the image.
They form a very sparse set of points. This makes the task
severely underconstrained. A model can fit this data with a
variety functions. This will reveal whether a model prefers
fitting low- or high-frequency patterns.
E.1. Visualizing Inductive Biases
We first perform experiments to get a visual intuition of
the inductive biases provided by different activation func-
tions. We train 3-layer MLPs of width 64 with full-batch
Adam and a learning rate of 0.02 on the cameraman and
waves data. Figure 13 (next page) shows very different
functions across architectures. The cameraman image con-
tains fine details with sharp edges. Their presence in the
reconstruction indicate whether the model learned high-
frequency components.Cameraman Waves
Full
data
Training
points
Figure 12. Data used in the coordinate-MLP experiments.
Differences across architectures. TheReLU-like activa-
tions are biased for simplicity, hence the learned functions
tend to smooth out image details, favor large uniform re-
gions and smooth variations. Yet, they can also represent
sharp transitions, when these are necessary to fit the training
data. The decision boundary with ReLUs, which is a poly-
tope [47] is faintly discernible as criss-crossing lines in the
image. Surprisingly, we observe differences across different
initial weight magnitudes with ReLU, even though our ex-
periments on random networks did not show any such effect
(Section 3). We believe that this is a sign of optimization
difficulties when the initial weights are large ( i.e. difficulty
of reaching a complex solution).
With other activations (TanH, Gaussian, sine) the bias
for low or high frequencies is much more clearly modulated
by the initial weight magnitude. With large magnitudes, the
images contain high-frequency patterns. Similar observa-
tions are made with the waves data (Figure 14).
Theunbiased model is useless, as expected. It reaches
perfect accuracy on the training data, but the predictions on
other pixels look essentially random.
With random weights. We also examine in Figure 13 the
function represented by each model at initialization (with
random weights). As expected, we observe a strong cor-
relation between the amount of high frequencies at initial-
ization and in the trained model. We also examine mod-
els at the end of training, after shuffling the trained weights
(within each layer). This is another random-weight model,
but its distribution of weight magnitudes matches exactly
the trained model. Indeed, the shuffling preserves the
weights within each layer but destroys the precise connec-
tions across layers. This enables a very interesting obser-
vation. With non-ReLU-like architectures, there is a clear
increase in complexity between the functions at initializa-
tion and with shuffled weights. This means that the learned
increase in complexity in non-ReLU networks is partly
encoded by changes in the distribution of weight magni-
tudes (the only thing preserved through shuffling). In con-At init.
ReLU Trained
Shuffled
At init.
GELU Trained
Shuffled
At init.
Swish Trained
Shuffled
At init.
TanH Trained
Shuffled
At init.
Gaussian Trained
Shuffled
At init.
Sin Trained
Shuffled
At init.
Unbiased Trained
Shuffled
(Default) Magnitude of initial weights  (Larger)
Figure 13. Coordinate-MLPs trained to represent the cameraman with various activations and initial weight magnitudes. The model is
trained on 40% pf pixels and evaluated on a 6464grid. The images provide intuitions about the inductive biases of each architecture. The
differences across models with random weights (at init.) and with shuffled trained weights (shuffled) show that the increase in complexity in
non-ReLU models is realized by changes in weight magnitudes (which are maintained through the shuffling). In contrast, ReLU networks
revert to a low complexity after shuffling, suggesting that complexity is encoded in the precise weight values, not their magnitudes.Full data
 Sparse training points
ReLU
GELU
Swish
TanH
Gaussian
Sine
0.4 0.8 1.0 7.0 13.0 19.0 22.0
Magnitude of initial weights (fraction of standard magnitude)
Figure 14. Coordinate-MLPs trained on sparse points of the waves data. Variations across learned functions show how architectures are
biased towards low (ReLU) or high frequencies (Sine). ReLU activations give the most consistent behaviour across weight magnitudes.
trast, ReLU networks revert to a low complexity after shuf-
fling. This suggests that complexity in ReLU networks is
encoded in the weights precise values and connections
across layers, not in their magnitude .
E.2. Training Trajectories
We will now show that NNs can represent any function, but
complex ones require precise weight values and connec-
tions across layers that are unlikely through random sam-
pling but that can be found through gradient-based training.
Unlike prior work [59] that claimed that the complexity
at initialization causally influences the solution, our results
indicate instead they are two effects of a common cause (the
preferred complexity of the architecture). The architec-
ture is biased towards a certain complexity, and this influ-
ences both the randomly-initialized model and those found
by gradient descent. There exist weight values for other
functions (of complexity much lower or higher than the pre-
ferred one) but they are less likely to occur in either case.
For example, ReLU networks are biased towards sim-
plicity but can represent complex functions. Yet, contrary
to [59], initializing gradient descent with such a complex
function does not yield a complex solutions after training
on simple data. In other words, the architectures bias pre-
vails over the exact starting point of the training.
Experimental setup. We train models with different acti-
vations and initial magnitudes on the cameraman data, using
1/9pixels for training. We plot in Figure 16 the training tra-
jectory of each model. Each point of a trajectory represents
the average weight magnitude vs. the Fourier complexity of
the function represented by the model.Changes in weight magnitudes during training. The first
observation is that the average weight magnitude changes
surprisingly little. However, further examination (Fig-
ure 15) shows that the distribution shifts from uniform to
long-tailed. The trained models contain more and more
large-magnitude weights.
Changes in complexity during training. In Figure 16,
we observe that models with ReLU-like activations at ini-
tialization have low complexity regardless of the initializa-
tion magnitude. As training progresses, the complexity in-
creases to fit the training data. This increased complex-
ity is encoded in the weights precise values and connec-
tions across layers, since at the end of training, shuffling
the weights reverts models back to the initial low complex-
ity. With other activations, the initial weight magnitudes
impact the complexity at initialization and of the trained
model. Some of the additional complexity in the trained
model seems to be partly encoded by increases in weight
magnitudes, since shuffling the trained weights does seem
to retain some of this additional complexity.
Summary. The choice of activation function and initial
weight magnitude affect the preferred complexity of a
model. This complexity is visible both at initialization (with
random weights) and after training with gradient descent.
The complexity of the learned function can depart from the
preferred level just enough to fit the training points. Out-
side the interpolated training points, the shape of the learned
function is very much affected by the preferred complexity.
With ReLU networks, this effect usually drives the com-
plexity downwards (the simplicity bias ). With other archi-
7Weights Biases Activations
Figure 15. Distributions of the magnitudes of the weights, biases, and activations during training of a 3-layer MLP (the 4th row is
the output layer) on the cameraman data. Weights and biases are initialized from a uniform distribution and zero, respectively. The
distributions become very long-tailed as training progresses. The occurrence of large values is the reason why the dependence of the
preferred complexity of certain architecture on weight magnitudes is important (it would not matter if the distribution of magnitudes
remained constant throughout training).
tectures and large weight magnitudes, this often drives the
complexity upwards. Both can be useful: Section 4 showed
that sine activations can enable learning the parity function
from sparse training points, and reduce shortcut learning by
shifting the preferred complexity upwards.
Our observations also explain why the coordinate-MLPs
with sine activations proposed in [68] (SIREN) require a
careful initialization. This adjusts the preferred complexity
to the typical frequencies found in natural images.
E.3. Pretraining and Fine-tuning
We outline preliminary results from additional experiments.
Why study fine-tuning? We have seen that the preferred
complexity of an architecture can be observed with random
weights. The model can then be trained by gradient descent
to represent data with a different level of complexity. For
example, a ReLU network, initially biased for simplicity,
can represent a complex function after training on complex
data. Gradient descent finely adjusts the weights to repre-
sent a complex function. We will now see how pretrainingthen fine-tuning on data with different levels of complexity
blends the two in the final fine-tuned model.
Experimental setup. We pretrain an MLP with ReLU or
TanH activations on high-frequency data (high-frequency
2D sine waves). We then fine-tune it on lower-frequency
2D sine waves of a different random orientation.
Observations. During early fine-tuning iterations, TanH
models retain a high-frequency bias much more than ReLU
models. This agrees with the proposition in E.1 that the for-
mer encode high frequencies in weight magnitudes, while
ReLU models encode them in precise weight values, which
are quickly lost during fine-tuning.
We further verify this explanation by shuffling the pre-
trained weights (within each layer) before fine-tuning. The
ReLU models then show no high-frequency bias at all
(since the precise arrangement of weights is completely lost
through the shuffling). TanH models, however, do still show
high-frequency components in the fine-tuned solution. This
confirms that TanH models encode high frequencies partly
in weight magnitudes since this is the only property pre-
8ReLU GELU SwishComplexity (Fourier) 
0.2 0.3 0.4 0.5 0.6
0.2 0.3 0.4 0.5 0.6
0.2 0.3 0.4 0.5 0.6
TanH Gaussian Sine
0.5 1 1.5
0.2 0.4 0.6 0.8 1 1.2
0.2 0.3 0.4 0.5 0.6
Average magnitude of weights during training
At initialization At last epoch With shuffled trained weights
Lower frequencies
 Higher frequencies
Figure 16. Training trajectories of MLP models trained on the cameraman data. Each line corresponds to one training run (with a
different seed or initial weight magnitude). With ReLU-like activations, the models at initialization have low complexity regardless of the
initialization magnitude. As training progresses, the complexity increases to fit the training data. This increased complexity is encoded in
the weights precise values and connections across layers, since at the end of training, shuffling the weights reverts models to the initial low
complexity. With other activations, the initial weight magnitude impacts the complexity at initialization and of the trained model. Some
of the additional complexity in the trained model seems to be partly encoded by increases in the weight magnitudes, since shuffling the
trained weights does seem to retain some of this additional complexity.
served by the shuffling.
Finally, we do not find evidence for the prior
claim [59] that complexity at initialization persists indef-
initely throughout fine-tuning. Instead, with enough it-
erations of fine-tuning, any pretraining effect on the pre-
ferred complexity eventually vanishes. For example, a
ReLU model pretrained on high frequencies initially con-
tains high-frequency components in the fine-tuned model.
But with enough iterations, they eventually disappear i.e.
the simplicity bias of ReLUs eventually takes over. We be-
lieve that the experiments in [59] were simply not run for
long enough. This observation also disproves the causal
link proposed in [59] between the complexity at initializa-
tion and in the trained model.
9F. Full Results with Random Networks
On the next pages (Figures 1921), we present heatmaps showing the average complexity of functions implemented by
neural networks of various architectures with random weights and biases. Each heatmap corresponds to one architecture with
varying number of layers (heatmap columns) and weight magnitudes (heatmap rows). For every other cell of a heatmap, we
visualize, as a 2D grayscale image, a function implemented by one such a network with 2D input and scalar output.
ReLU GELU Swish SELU TanH Gaussian Sin UnbiasedFourier
1 1001
1 1001
1 1001
1 1001
1 5001
1 3001
1 2001
1 1001
    MLP
 + Gating
 + Residual
 + Layer norm. Chebyshev
1 1001
1 1001
1 1001
1 1001
1 5001
1 3001
1 2001
1 1001 Legendre
1 1001
1 1001
1 1001
1 1001
1 5001
1 3001
1 2001
1 1001 LZ
1 1001
1 1001
1 1001
1 1001
1 5001
1 3001
1 2001
1 1001
Figure 17. All measures of complexity (Y axes) of random networks generally increase with weight/activation magnitudes (X axis) . The
sensitivity is however very different across activation functions (columns). This sensitivity also increases with multiplicative interactions
(i.e. gating), decreases with residual connections, and is essentially absent with layer normalization. These effects are also visible on the
heatmaps (see next pages), but faint hence visualized here as line plots.
0 LZ 101Fourier
0 LZ 101Legendre
0 LZ 101Chebyshev
Figure 18. Correlations between our measures of complexity on random networks. They are based on frequency (Fourier), polynomial
order (Legendre, Chebyshev), or compressibility (LZ). They capture different notions, yet they are closely correlated.
1011 ReLU GELU Swish SELU TanH Gaussian Sin Unbiased
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
With residual connections:
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
With layer normalization:
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
With gating:
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
Figure 19. Heatmaps of the average complexity ( Fourier ) of various architectures with random weights, and example functions.12 ReLU GELU Swish SELU TanH Gaussian Sin Unbiased
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
With residual connections:
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
With layer normalization:
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
With gating:
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
Figure 20. Same as Figure 19 with the LZ measure instead of the Fourier-based one. Results are nearly identical.13 ReLU GELU Swish SELU TanH Gaussian Sin Unbiased
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
With residual connections:
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
With layer normalization:
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
With gating:
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
2
3
4
5
6
7
8
9
10
1 2 3 4 5 61
6
11
17
22
28
33
39
44
50
1 2 3 4 5 61
4
7
10
13
17
20
23
26
30
1 2 3 4 5 61
3
5
7
9
11
13
15
17
20
1 1 1 1 1 11
2
3
4
5
6
7
8
9
10
Figure 21. Same as Figure 19 with the LZ measure instead of the Fourier-based one. Results are nearly identical but noisier.
  EFFICIENTLY TRAINABLE TEXT-TO-SPEECH SYSTEM BASED ON
DEEP CONVOLUTIONAL NETWORKS WITH GUIDED ATTENTION
Hideyuki Tachibana, Katsuya Uenoyama
PKSHA Technology, Inc.
Bunkyo, Tokyo, Japan
h_tachibana@pkshatech.comShunsuke Aihara
Independent Researcher
Shinjuku, Tokyo, Japan
aihara@argmax.jpc
2018 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works.
ABSTRACT
This paper describes a novel text-to-speech (TTS) technique based
on deep convolutional neural networks (CNN), without use of any
recurrent units. Recurrent neural networks (RNN) have become a
standard technique to model sequential data recently, and this tech-
nique has been used in some cutting-edge neural TTS techniques.
However, training RNN components often requires a very power-
ful computer, or a very long time, typically several days or weeks.
Recent other studies, on the other hand, have shown that CNN-
based sequence synthesis can be much faster than RNN-based tech-
niques, because of high parallelizability. The objective of this paper
is to show that an alternative neural TTS based only on CNN alle-
viate these economic costs of training. In our experiment, the pro-
posed Deep Convolutional TTS was sufﬁciently trained overnight
(15 hours), using an ordinary gaming PC equipped with two GPUs,
while the quality of the synthesized speech was almost acceptable.
Index Terms Text-to-speech, deep learning, convolutional
neural network, attention, sequence-to-sequence learning.
1. INTRODUCTION
Text-to-speech (TTS) is getting more and more common recently,
and is getting to be a basic user interface for many systems. To
further promote the use of TTS in various systems, it is signiﬁcant to
develop a manageable, maintainable, and extensible TTS component
that is accessible to speech non-specialists, enterprising individuals
and small teams.
Traditional TTS systems, however, are not necessarily friendly
for them, since they are typically composed of many domain-speciﬁc
modules. For example, a typical parametric TTS system is an elab-
orate integration of many modules e.g. a text analyzer, an F0gen-
erator, a spectrum generator, a pause estimator, and a vocoder that
synthesize a waveform from these data, etc.
Deep learning [1] may integrate these internal building blocks
into a single model, and connects the input and the output directly.
This type of technique is sometimes called end-to-end learning.
Although such a technique is sometimes criticized as a black box,
an end-to-end TTS system named Tacotron [2], which directly esti-
mates a spectrogram from an input text, has achieved promising per-
formance recently, without using hand-engineered parametric mod-
els based on domain-speciﬁc knowledge. Tacotron, however, has a
drawback that it uses many recurrent units which are quite costly to
train. It is almost infeasible for ordinary labs who do not have lux-
urious machines to study and extend it further. In fact, some people
tried to implement open clones of Tacotron [3, 4, 5, 6], but they arestruggling to reproduce the speech of satisfactory quality as clear as
the original results.
The purpose of this paper is to show Deep Convolutional TTS
(DCTTS), a novel fully convolutional neural TTS. The architecture
is largely similar to Tacotron [2], but is based on a fully convolu-
tional sequence-to-sequence learning model similar to the literature
[7]. We show that this fully convolutional TTS actually works in a
reasonable setting. The contribution of this article is twofold: (1)
Propose a fully CNN-based TTS system which can be trained much
faster than an RNN-based state-of-the-art neural TTS system, while
the sound quality is still acceptable. (2) An idea to rapidly train the
attention module, which we call guided attention, is also shown.
1.1. Related Work
1.1.1. Deep Learning and TTS
Recently, deep learning-based TTS systems have been intensively
studied, and surprisingly high quality results are obtained in some
of recent studies. The TTS systems based on deep neural networks
include Zens work [8], the studies based on RNN e.g. [9, 10, 11,
12], and recently proposed techniques e.g. WaveNet [13, sec. 3.2],
Char2Wav [14], DeepV oice1&2 [15, 16], and Tacotron [2].
Some of them tried to reduce the dependency on hand-engineered
internal modules. The most extreme technique in this trend would
be Tacotron [2], which depends only on mel and linear spectro-
grams, and not on any other speech features e.g. F0. Our method is
close to Tacotron in a sense that it depends only on these spectral
representations of audio signals.
Most of the existing methods above use RNN, a natural tech-
nique of time series prediction. An exception is WaveNet, which
is fully convolutional. Our method is also based only on CNN but
our usage of CNN would be different from WaveNet, as WaveNet is
a kind of a vocoder, or a back-end, which synthesizes a waveform
from some conditioning information that is given by front-end com-
ponents. On the other hand, ours is rather a front-end (and most of
back-end processing). We use CNN to synthesize a spectrogram,
from which a simple vocoder can synthesize a waveform.
1.1.2. Sequence to Sequence (seq2seq )Learning
Recently, recurrent neural networks (RNN) have become a standard
technique for mapping a sequence to another sequence, especially
in the ﬁeld of natural language processing, e.g. machine transla-
tion [17, 18], dialogue system [19, 20], etc. See also [1, sec. 10.4].
RNN-based seq2seq, however, has some disadvantages. Firstly,
a vanilla encode-decoder model cannot encode too long sequence
into a ﬁxed-length vector effectively. This problem has been re-
solved by a mechanism called attention [21], and the attention
Published as a conference paper at ICASSP 2018, Calgary, Canada c
IEEE 2018.arXiv:1710.08969v2  [cs.SD]  30 Sep 2020mechanism now has become a standard idea of seq2seq learning
techniques; see also [1, sec. 12.4.5.1].
Another problem is that RNN typically requires much time to
train, since it is less suited for parallel computation using GPUs. To
overcome this problem, several researchers proposed the use of CNN
instead of RNN, e.g. [22, 23, 24, 25, 26]. Some studies have shown
that CNN-based alternative networks can be trained much faster, and
sometimes can even outperform the RNN-based techniques.
Gehring et al. [7] recently united these two improvements of
seq2seq learning. They proposed an idea on how to use attention
mechanism in a CNN-based seq2seq learning model, and showed
that the method is quite effective for machine translation. The
method we proposed is based on the similar idea to the literature [7].
2. PRELIMINARY
2.1. Basic Knowledge of the Audio Spectrograms
An audio waveform and a complex spectrogram Z=fZftg 2
CF0T0are mutually transformed by linear maps called Short-term
Fourier Transform (STFT) and inverse STFT, where F0andT0de-
note the number of frequency bins and temporal bins, respectively.
It is common to consider only the magnitude jZj=fjZftjg 2
RF0T0, since it still has useful information for many purposes,
and thatjZjandZare almost the same in the sense that there ex-
ist many phase estimation (i.e., jZjtoZestimation, and therefore,
waveform synthesis) techniques, e.g. the famous Grifﬁn&Lim algo-
rithm (GLA) [27]; see also e.g. [28]. We always use RTISI-LA [29],
an online GLA, to synthesize a waveform. In this paper, we al-
ways normalize STFT spectrograms as jZj (jZj=max(jZj))
,
and convert backjZj jZj=
when we ﬁnally need to synthesize
the waveform, where 
;are pre- and post-emphasis factors.
It is also common to consider a mel spectrogram S2RFT0,
(FF0), obtained by applying a mel ﬁlter-bank to jZj. This is
a standard dimension reduction technique for speech processing. In
this paper, we also reduce the temporal dimensionality from T0to
dT0=4e=:Tby picking up a time frame every four time frames, to
accelerate the training of Text2Mel shown below. We also normalize
mel spectrograms as S (S=max(S))
.
2.2. Notation: Convolution and Highway Activation
In this paper, we denote the 1D convolution layer [30] by a space sav-
ing notation Co i
k?(X), whereiis the size of input channel, ois the
size of output channel, kis the size of kernel, is the dilation factor,
and an argument Xis a tensor having three dimensions ( batch, chan-
nel, temporal ). The stride of convolution is always 1. Convolution
layers are preceded by appropriately-sized zero padding, whose size
is suitably determined by a simple arithmetic so that the length of
the sequence is kept constant. Let us similarly denote the 1D decon-
volution layer as Do i
k?(X). The stride of deconvolution is always 2
in this paper. Let us write a layer composition operator as /, and
let us write networks like F/ReLU/G(X) := F(ReLU (G(X)));
and(F/G)2(X) :=F/G/F/G(X), etc. ReLU is an element-wise
activation function deﬁned by ReLU (x) = max(x;0).
Convolution layers are sometimes followed by a Highway Net-
work [31]-like gated activation, which is advantageous in very deep
networks: Highway (X;L) =(H1)ReLU (H2)+(1(H1))
X;whereH1;H2are tensors of the same shape as X, and are output
as[H1;H2] =L(X)by a layer L. The operatoris the element-
wise product, and is the element-wise sigmoid function. Hereafter
let us denote HCd d
k?(X) :=Highway (X;C2d d
k? ).
Fig. 1 . Network architecture.
TextEnc (L) := ( HC2d 2d
1?1)2/(HC2d 2d
3?1)2/(HC2d 2d
3?27/HC2d 2d
3?9/
HC2d 2d
3?3/HC2d 2d
3?1)2/C2d 2d
1?1/ReLU/C2d e
1?1/CharEmbede-dim(L):
AudioEnc (S) := ( HCd d
3?3)2/(HCd d
3?27/HCd d
3?9/HCd d
3?3/HCd d
3?1)2/
Cd d
1?1/ReLU/Cd d
1?1/ReLU/Cd F
1?1(S):
AudioDec (R0) :=/CF d
1?1/(ReLU/Cd d
1?1)3/(HCd d
3?1)2/(HCd d
3?27/
HCd d
3?9/HCd d
3?3/HCd d
3?1)/Cd 2d
1?1(R0):
SSRN (Y) :=/CF0 F0
1?1/(ReLU/CF0 F0
1?1)2/CF0 2c
1?1/(HC2c 2c
3?1)2/
C2c c
1?1/(HCc c
3?3/HCc c
3?1/Dc c
2?1)2/(HCc c
3?3/HCc c
3?1)/Cc F
1?1(Y):
Fig. 2 . Details of each component. For notation, see section 2.2.
3. PROPOSED NETWORK
Since some literature [2, 32] suggest that the staged synthesis from
low- to high-resolution has advantages over the direct synthesis of
high-resolution data, we synthesize the spectrograms using the fol-
lowing two networks. (1) Text2Mel, which synthesizes a mel spec-
trogram from an input text, and (2) Spectrogram Super-resolution
Network (SSRN), which synthesizes a full STFT spectrogram from
a coarse mel spectrogram. Fig. 1 shows the overall architecture.
3.1. Text2Mel: Text to Mel Spectrogram Network
We ﬁrst consider to synthesize a coarse mel spectrogram from a text.
This is the main part of the proposed method. This module consists
of four submodules: Text Encoder, Audio Encoder, Attention, and
Audio Decoder. The network TextEnc ﬁrst encodes the input sen-
tenceL= [l1;:::;lN]2CharNconsisting of Ncharacters, into the
two matrices K(key),V(value)2RdN, wheredthe dimension of
encoded characters. On the other hand, the network AudioEnc en-
codes the coarse mel spectrogram S=S1:F;1:T2RFT, of speech
of lengthT, into a matrix Q(query)2RdT.
(K;V ) =TextEnc (L); Q=AudioEnc (S1:F;1:T): (1)
An attention matrix A2RNT, deﬁned as follows, evaluates how
strongly the n-th character lnand thet-th mel spectrum S1:F;tare
related.
A=softmaxn-axis(KTQ=p
d): (2)
Ant1implies that the module is focusing on n-th character ln
at timet, and it will focus on lnorln+1(or others nearby), at the
subsequent time t+ 1. Whatever, let us expect those are encoded in
n-th column of V. Thus a matrix R2RdT, which is the seed of
the subsequent mel spectra S1:F;2:T+1, is obtained as
R=Att(Q;K;V ) :=VA: (Note: matrix product.) (3)
2The resultant Ris concatenated with the encoded audio Q, asR0=
[R;Q], because we found it beneﬁcial in our pilot study. Then, the
Audio Decoder module estimates a mel spectrogram from the seed
matrixR02R2dTas follows,
Y1:F;2:T+1=AudioDec (R0): (4)
The resultant Y1:F;2:T+1needs to approximate the temporally-
shifted ground truth S1:F;2:T+1. The error is evaluated by a loss
functionLspec(Y1:F;2:T+1jS1:F;2:T+1), and is back-propagated to
the network parameters. The loss function was the sum of L1 loss
and a functionDbinwhich we call binary divergence,
Dbin(YjS) :=Eft
SftlogYft
Sft(1Sft) log1Yft
1Sft
=Eft[Sft^Yft+ log(1 + exp ^Yft)] + const:;(5)
where ^Yft= logit(Yft). Since the gradient is non-vanishing, i.e.,
@Dbin(YjS)=@^Yft/YftSft, it is advantageous for gradient-
based training. It is easily veriﬁed that the spectrogram error is non-
negative,Lspec(YjS) =Dbin(YjS) +E[jYftSftj]0, and the
equality holds iff Y=S.
3.1.1. Details of TextEnc, AudioEnc, and AudioDec
The networks are fully convolutional, and are not dependent on any
recurrent units. In order to take into account the long contextual in-
formation, we used dilated convolution [33, 13, 24] instead of RNN.
The top equation of Fig. 2 is the content of TextEnc . It con-
sists of a character embedding and several 1D non-causal convolu-
tion layers. In the literature [2] an RNN-based component named
CBHG was used, but we found this convolutional network also
works well. AudioEnc andAudioDec , shown in Fig. 2, are com-
posed of 1D causal convolution layers. These convolution should be
causal because the output of AudioDec is feedbacked to the input of
AudioEnc at the synthesis stage.
3.2. Spectrogram Super-resolution Network (SSRN)
We ﬁnally synthesize a full spectrogram jZj2RF04T, from the
obtained coarse mel spectrogram Y2RFT, using the spectrogram
super-resolution network (SSRN). Upsampling frequency from Fto
F0is fairly simple. We can achieve that by increasing the convo-
lution channels of a 1D convolutional network. Upsampling of the
temporal axis is not done the same way. Instead, we quadruple the
length of the sequence from Tto4T=T0, by applying deconvo-
lution layers of stride size 2 twice.
The bottom equation of Fig. 2 shows SSRN . In this paper, all
convolutions of SSRN are non-causal, since we do not consider on-
line processing. The loss function is the same as Text2Mel: the sum
of L1 distance and Dbin, deﬁned by (5), between the synthesized
spectrogram SSRN (S)and the ground truth jZj.
4. GUIDED ATTENTION
4.1. Guided Attention Loss: Motivation, Method and Effects
In general, an attention module is quite costly to train. Therefore,
if there is some prior knowledge, incorporating them into the model
may be a help to alleviate the heavy training. We show that the
following simple method helps to train the attention module.
In TTS, the possible attention matrix Alies in the very small
subspace of RNT. This is because of the rough correspondence of
the order of the characters and the audio segments. That is, when one
reads a text, it is natural to assume that the text position nprogresses
Fig. 3 . Comparison of the attention matrix A, trained with
and without the guided attention loss Latt(A). (Left) Without, and
(Right) with the guided attention. The test text is icassp stands
for the international conference on acoustics, speech, and signal
processing.  We did not use the heuristics described in section 4.2.
nearly linearly to the time t, i.e.,nat, whereaN=T . This
is a noticeable difference of TTS from other seq2seq learning tech-
niques such as machine translation, where an attention module needs
to resolve the word alignment between two languages that have very
different syntax, e.g. English and Japanese.
Based on this idea, we introduce guided attention loss, which
prompts the attention matrix Ato be nearly diagonal, Latt(A) =
Ent[AntWnt];whereWnt= 1expf(n=Nt=T)2=2g2g:In
this paper, we set g= 0:2. IfAis far from diagonal (e.g., reading the
characters in the random order), it is strongly penalized by the loss
function. This subsidiary loss function is simultaneously optimized
with the main lossLspecwith the equal weight.
Although this measure is based on quite a rough assumption,
it improved the training efﬁciency. In our experiment, if we added
the guided attention loss to the objective, the term began decreas-
ing only after100 iterations. After 5K iterations, the attention
became roughly correct, not only for training data, but also for new
input texts. On the other hand, without the guided attention loss,
it required much more iterations. It began learning after 10K it-
erations, and it required 50K iterations to look at roughly correct
positions, but the attention matrix was still vague. Fig. 3 compares
the attention matrix, trained with and without guided attention loss.
4.2. Forcibly Incremental Attention at the Synthesis Stage
At the synthesis stage, the attention matrix Asometimes fails to
focus on the correct characters. Typical errors we observed were
(1) skipping several letters, and (2) repeating a same word twice or
more. In order to make the system more robust, we heuristically
modiﬁed the matrix Ato be nearly diagonal, by a simple rule as
follows. We observed this method sometimes alleviated such fail-
ures.
Letntbe the position of the character which is read at time t,
i.e.,nt= argmaxnAnt. Comparing the current position ntand the
previous position nt1, if the difference ntnt1is outside of the
range1ntnt13, the current attention is forcibly set
asAnt=n;n t1+1(Kronecker delta ), to increment the attention
target asnt=nt1+ 1.
5. EXPERIMENT
5.1. Experimental Setup
We used LJ Speech Dataset [34] to train the networks. This is
a public domain speech dataset consisting of 13K pairs of text
and speech, without phoneme-level alignment, 24 hours in total.
These speech data have a little reverberation. We preprocessed the
3Table 1 . Parameter Settings.
Sampling rate of audio signals 22050 Hz
STFT window function Hanning
STFT window length and shift 1024 (46.4 [ms]), 256 ( 11.6[ms])
STFT spectrogram size F04T 5134T(Tdepends on audio clip)
Mel spectrogram size FT 80T(Tdepends on audio clip)
Dimensione,dandc 128, 256, 512
ADAM parameters (; 1;2;) (2104;0:5;0:9;106)
Minibatch size 16
Emphasis factors (
;) (0.6, 1.3)
RTISI-LA window and iteration 100, 10
Character set, Char a-z,.- andSpace andNULL
Table 2 . Comparison of MOS (95% conﬁdence interval), training
time, and iterations (Text2Mel/SSRN), of an open Tacotron [5] and
the proposed method (DCTTS). The digits with * were excerpted
from the repository [5].
Method Iteration Time MOS (95% CI)
Open Tacotron [5] 877K12 days2:070:62
DCTTS 20K/ 40K 2 hours 1:740:52
DCTTS 90K/150K 7 hours 2:630:75
DCTTS 200K/340K 15 hours 2:710:66
DCTTS 540K/900K 40 hours 2:610:62
texts by spelling out some of abbreviations and numeric expressions,
decapitalizing the capitals, and removing less frequent characters
not shown in Table 1, where NULL is a dummy character for zero-
padding.
We implemented our neural networks using Chainer 2.0 [35].
We trained the models using a household gaming PC equipped with
two GPUs. The main memory of the machine was 62GB, which
is much larger than the audio dataset. Both GPUs were NVIDIA
GeForce GTX 980 Ti, with 6 GB memories.
For simplicity, we trained Text2Mel and SSRN independently
and asynchronously using different GPUs. All network parameters
were initialized using Hes Gaussian initializer [36]. Both networks
were trained by the ADAM optimizer [37]. When training SSRN,
we randomly extracted short sequences of length T= 64 for each
iteration to save memory usage. To reduce the disk access, we re-
duced the frequency of creating the snapshot of parameters to only
once per 5K iterations. Other parameters are shown in Table 1.
As it is not easy for us to reproduce the original results of
Tacotron, we instead used a ready-to-use model [5] for comparison,
which seemed to produce the most reasonable sounds in the open
implementations. It is reported that this model was trained using
LJ Dataset for 12 days (877K iterations) on a GTX 1080 Ti, newer
GPU than ours. Note, this iteration is still much less than the original
Tacotron, which was trained for more than 2M iterations.
We evaluated mean opinion scores (MOS) of both methods
by crowdsourcing on Amazon Mechanical Turk using crowdMOS
toolkit [38]. We used 20 sentences from Harvard Sentences List
1&2. The audio data were synthesized using ﬁve methods shown
in Table 2. The crowdworkers rated these 100 clips from 1 (Bad)
to 5 (Excellent). Each worker is supposed to rate at least 10 clips.
To obtain more responses of higher quality, we set a few incentives
shown in the literature. The results were statistically processed using
the method shown in the literature [38].
5.2. Result and Discussion
In our setting, the training throughput was 3.8 minibatch/s (Text2Mel)
and6.4 minibatch/s (SSRN). This implies that we can iterate the
updating formulae of Text2Mel 200K times in 15 hours. Fig. 4
Fig. 4 . (Top) Attention, (middle) mel, and (bottom) linear STFT
spectrogram, synthesized by the proposed method, after 15 hours
training. The input text is icassp stands for the international con-
ference on acoustics, speech and signal processing.  (90 chars)
shows an example of attention, synthesized mel and full spectro-
grams, after 15 hours training. It shows that the method can almost
correctly focus on the correct characters, and synthesize quite clear
spectrograms. More samples are available at the authors web page.1
In our crowdsourcing experiment, 31 subjects evaluated our
data. After the automatic screening by the toolkit [38], 560 scores
rated by 6 subjects were selected for ﬁnal statistics calculation. Ta-
ble 2 compares the performance of our proposed method (DCTTS)
and an open Tacotron. Our MOS (95% conﬁdence interval) was
2:710:66(15 hours training) while the Tacotrons was 2:070:62.
Although it is not a strict comparison since the frameworks and
the machines were different, it would be still concluded that our
proposed method is quite rapidly trained to the satisfactory level
compared to Tacotron.
Note that the MOS were below the level reported in [2]. The
reasons may be threefold: (1) the limited number of iterations, (2)
SSRN needs to synthesize the spectrograms from less information
than [2], and (3) the reverberation of the training data.
6. SUMMARY AND FUTURE WORK
This paper described a novel TTS technique based on deep convo-
lutional neural networks, and a technique to train the attention mod-
ule rapidly. In our experiment, the proposed Deep Convolutional
TTS was trained overnight ( 15 hours), using an ordinary gaming
PC equipped with two GPUs, while the quality of the synthesized
speech was almost acceptable.
Although the audio quality is far from perfect yet, it may be
improved by tuning some hyper-parameters thoroughly, and by ap-
plying some techniques developed in the deep learning community.
We believe this method will encourage further development of
the applications based on speech synthesis. We can expect that this
simple neural TTS may be extended to many other purposes e.g.
emotional/non-linguistic/personalized speech synthesis, etc., by fur-
ther studies. In addition, since a neural TTS has become lighter, the
studies on more integrated speech systems e.g. some multimodal
systems, may have become more feasible. These issues should be
worked out in the future.
7. ACKNOWLEDGEMENT
The authors would like to thank the OSS contributors and the data
creators (LibriV ox contributors and @keithito).
1https://github.com/tachi-hi/tts_samples
48. REFERENCES
[1] I. Goodfellow et al., Deep Learning , MIT Press, 2016, http:
//www.deeplearningbook.org .
[2] Y . Wang et al., Tacotron: Towards end-to-end speech synthe-
sis, in Proc. Interspeech , 2017, arXiv:1703.10135.
[3] A. Barron, Implementation of Googles Tacotron in Ten-
sorFlow, 2017, Available at GitHub, https://github.
com/barronalex/Tacotron (visited Oct. 2017).
[4] K. Park, A TensorFlow implementation of Tacotron: A
fully end-to-end text-to-speech synthesis model, 2017, Avail-
able at GitHub, https://github.com/Kyubyong/
tacotron (visited Oct. 2017).
[5] K. Ito, Tacotron speech synthesis implemented in Tensor-
Flow, with samples and a pre-trained model, 2017, Avail-
able at GitHub, https://github.com/keithito/
tacotron (visited Oct. 2017).
[6] R. Yamamoto, PyTorch implementation of Tacotron speech
synthesis model, 2017, Available at GitHub, https://
github.com/r9y9/tacotron_pytorch (visited Oct.
2017).
[7] J. Gehring et al., Convolutional sequence to sequence learn-
ing, in Proc. ICML , 2017, pp. 12431252, arXiv:1705.03122.
[8] H. Zen et al., Statistical parametric speech synthesis using
deep neural networks, in Proc. ICASSP , 2013, pp. 79627966.
[9] Y . Fan et al., TTS synthesis with bidirectional LSTM based
recurrent neural networks, in Proc. Interspeech , 2014, pp.
19641968.
[10] H. Zen and H. Sak, Unidirectional long short-term memory
recurrent neural network with recurrent output layer for low-
latency speech synthesis, in Proc. ICASSP , 2015, pp. 4470
4474.
[11] S. Achanta et al., An investigation of recurrent neural network
architectures for statistical parametric speech synthesis., in
Proc. Interspeech , 2015, pp. 859863.
[12] Z. Wu and S. King, Investigating gated recurrent networks for
speech synthesis, in Proc. ICASSP , 2016, pp. 51405144.
[13] A. van den Oord et al., WaveNet: A generative model for raw
audio, arXiv:1609.03499 , 2016.
[14] J. Sotelo et al., Char2wav: End-to-end speech synthesis, in
Proc. ICLR , 2017.
[15] S. Arik et al., Deep voice: Real-time neural text-to-speech,
inProc. ICML , 2017, pp. 195204, arXiv:1702.07825.
[16] S. Arik et al., Deep voice 2: Multi-speaker neural text-to-
speech, in Proc. NIPS , 2017, arXiv:1705.08947.
[17] K. Cho et al., Learning phrase representations using RNN
encoder-decoder for statistical machine translation, in Proc.
EMNLP , 2014, pp. 17241734.
[18] I. Sutskever et al., Sequence to sequence learning with neural
networks, in Proc. NIPS , 2014, pp. 31043112.
[19] O. Vinyals and Q. Le, A neural conversational model, in
Proc. Deep Learning Workshop, ICML , 2015.
[20] I. V . Serban et al., Building end-to-end dialogue systems us-
ing generative hierarchical neural network models., in Proc.
AAAI , 2016, pp. 37763784.[21] D Bahdanau et al., Neural machine translation by jointly
learning to align and translate, in Proc. ICLR 2015,
arXiv:1409.0473 , 2014.
[22] Y . Kim, Convolutional neural networks for sentence
classiﬁcation, in Proc. EMNLP , 2014, pp. 17461752,
arXiv:1408.5882.
[23] X. Zhang et al., Character-level convolutional networks for
text classiﬁcation, in Proc. NIPS , 2015, arXiv:1509.01626.
[24] N. Kalchbrenner et al., Neural machine translation in linear
time, arXiv:1610.10099 , 2016.
[25] Y . N. Dauphin et al., Language modeling with gated convolu-
tional networks, arXiv:1612.08083 , 2016.
[26] J. Bradbury et al., Quasi-recurrent neural networks, in Proc.
ICLR 2017 , 2016.
[27] D. Grifﬁn and J. Lim, Signal estimation from modiﬁed short-
time fourier transform, IEEE Trans. ASSP , vol. 32, no. 2, pp.
236243, 1984.
[28] P. Mowlee et al., Single Channel Phase-Aware Signal Process-
ing in Speech Communication: Theory and Practice , Wiley,
2016.
[29] X. Zhu et al., Real-time signal estimation from modiﬁed
short-time fourier transform magnitude spectra, IEEE Trans.
ASLP , vol. 15, no. 5, 2007.
[30] Y . LeCun and Y . Bengio, The handbook of brain theory and
neural networks, chapter Convolutional Networks for Images,
Speech, and Time Series, pp. 255258. MIT Press, 1998.
[31] R. K. Srivastava et al., Training very deep networks, in Proc.
NIPS , 2015, pp. 23772385.
[32] H. Zhang et al., StackGAN: Text to photo-realistic image syn-
thesis with stacked generative adversarial networks, in Proc.
ICCV , 2017, pp. 59075915, arXiv:1612.03242.
[33] F. Yu and V . Koltun, Multi-scale context aggregation by di-
lated convolutions, in Proc. ICLR , 2016.
[34] K. Ito, The LJ speech dataset, 2017, Available at https://
keithito.com/LJ-Speech-Dataset/ (visited Sep.
2017).
[35] S. Tokui et al., Chainer: A next-generation open source
framework for deep learning, in Proc. Workshop LearningSys,
NIPS , 2015.
[36] K. He et al., Delving deep into rectiﬁers: Surpassing human-
level performance on ImageNet classiﬁcation, in Proc. ICCV ,
2015, pp. 10261034, arXiv:1502.01852.
[37] D. P. Kingma and J. Ba, Adam: A method for stochastic opti-
mization, in Proc. ICLR 2015 , 2014, arXiv:1412.6980.
[38] F. Ribeiro et al., CrowdMOS: An approach for crowdsourcing
mean opinion score studies, in Proc ICASSP , 2011, pp. 2416
2419.
5
  Generating Sequences With
Recurrent Neural Networks
Alex Graves
Department of Computer Science
University of Toronto
graves@cs.toronto.edu
Abstract
This paper shows how Long Short-term Memory recurrent neural net-
works can be used to generate complex sequences with long-range struc-
ture, simply by predicting one data point at a time. The approach is
demonstrated for text (where the data are discrete) and online handwrit-
ing (where the data are real-valued). It is then extended to handwriting
synthesis by allowing the network to condition its predictions on a text
sequence. The resulting system is able to generate highly realistic cursive
handwriting in a wide variety of styles.
1 Introduction
Recurrent neural networks (RNNs) are a rich class of dynamic models that have
been used to generate sequences in domains as diverse as music [6, 4], text [30]
and motion capture data [29]. RNNs can be trained for sequence generation by
processing real data sequences one step at a time and predicting what comes
next. Assuming the predictions are probabilistic, novel sequences can be gener-
ated from a trained network by iteratively sampling from the network's output
distribution, then feeding in the sample as input at the next step. In other
words by making the network treat its inventions as if they were real, much like
a person dreaming. Although the network itself is deterministic, the stochas-
ticity injected by picking samples induces a distribution over sequences. This
distribution is conditional, since the internal state of the network, and hence its
predictive distribution, depends on the previous inputs.
RNNs are fuzzy' in the sense that they do not use exact templates from
the training data to make predictions, but rather|like other neural networks|
use their internal representation to perform a high-dimensional interpolation
between training examples. This distinguishes them from n-gram models and
compression algorithms such as Prediction by Partial Matching [5], whose pre-
dictive distributions are determined by counting exact matches between the
recent history and the training set. The result|which is immediately appar-
1arXiv:1308.0850v5  [cs.NE]  5 Jun 2014ent from the samples in this paper|is that RNNs (unlike template-based al-
gorithms) synthesise and reconstitute the training data in a complex way, and
rarely generate the same thing twice. Furthermore, fuzzy predictions do not suf-
fer from the curse of dimensionality, and are therefore much better at modelling
real-valued or multivariate data than exact matches.
In principle a large enough RNN should be sucient to generate sequences
of arbitrary complexity. In practice however, standard RNNs are unable to
store information about past inputs for very long [15]. As well as diminishing
their ability to model long-range structure, this amnesia' makes them prone to
instability when generating sequences. The problem (common to all conditional
generative models) is that if the network's predictions are only based on the last
few inputs, and these inputs were themselves predicted by the network, it has
little opportunity to recover from past mistakes. Having a longer memory has
a stabilising eect, because even if the network cannot make sense of its recent
history, it can look further back in the past to formulate its predictions. The
problem of instability is especially acute with real-valued data, where it is easy
for the predictions to stray from the manifold on which the training data lies.
One remedy that has been proposed for conditional models is to inject noise into
the predictions before feeding them back into the model [31], thereby increasing
the model's robustness to surprising inputs. However we believe that a better
memory is a more profound and eective solution.
Long Short-term Memory (LSTM) [16] is an RNN architecture designed to
be better at storing and accessing information than standard RNNs. LSTM has
recently given state-of-the-art results in a variety of sequence processing tasks,
including speech and handwriting recognition [10, 12]. The main goal of this
paper is to demonstrate that LSTM can use its memory to generate complex,
realistic sequences containing long-range structure.
Section 2 denes a deep' RNN composed of stacked LSTM layers, and ex-
plains how it can be trained for next-step prediction and hence sequence gener-
ation. Section 3 applies the prediction network to text from the Penn Treebank
and Hutter Prize Wikipedia datasets. The network's performance is compet-
itive with state-of-the-art language models, and it works almost as well when
predicting one character at a time as when predicting one word at a time. The
highlight of the section is a generated sample of Wikipedia text, which showcases
the network's ability to model long-range dependencies. Section 4 demonstrates
how the prediction network can be applied to real-valued data through the use
of a mixture density output layer, and provides experimental results on the IAM
Online Handwriting Database. It also presents generated handwriting samples
proving the network's ability to learn letters and short words direct from pen
traces, and to model global features of handwriting style. Section 5 introduces
an extension to the prediction network that allows it to condition its outputs on
a short annotation sequence whose alignment with the predictions is unknown.
This makes it suitable for handwriting synthesis, where a human user inputs
a text and the algorithm generates a handwritten version of it. The synthesis
network is trained on the IAM database, then used to generate cursive hand-
writing samples, some of which cannot be distinguished from real data by the
2Figure 1: Deep recurrent neural network prediction architecture. The
circles represent network layers, the solid lines represent weighted connections
and the dashed lines represent predictions.
naked eye. A method for biasing the samples towards higher probability (and
greater legibility) is described, along with a technique for priming' the sam-
ples on real data and thereby mimicking a particular writer's style. Finally,
concluding remarks and directions for future work are given in Section 6.
2 Prediction Network
Fig. 1 illustrates the basic recurrent neural network prediction architecture used
in this paper. An input vector sequence x= (x1;:::;x T) is passed through
weighted connections to a stack of Nrecurrently connected hidden layers to
compute rst the hidden vector sequences hn= (hn
1;:::;hn
T) and then the
output vector sequence y= (y1;:::;y T). Each output vector ytis used to
parameterise a predictive distribution Pr( xt+1jyt) over the possible next inputs
xt+1. The rst element x1of every input sequence is always a null vector whose
entries are all zero; the network therefore emits a prediction for x2, the rst
real input, with no prior information. The network is deep' in both space
and time, in the sense that every piece of information passing either vertically
or horizontally through the computation graph will be acted on by multiple
successive weight matrices and nonlinearities.
Note the skip connections' from the inputs to all hidden layers, and from
all hidden layers to the outputs. These make it easier to train deep networks,
3by reducing the number of processing steps between the bottom of the network
and the top, and thereby mitigating the vanishing gradient' problem [1]. In
the special case that N= 1 the architecture reduces to an ordinary, single layer
next step prediction RNN.
The hidden layer activations are computed by iterating the following equa-
tions fromt= 1 toTand fromn= 2 toN:
h1
t=H
Wih1xt+Wh1h1h1
t1+b1
h
(1)
hn
t=H
Wihnxt+Whn1hnhn1
t+Whnhnhn
t1+bn
h
(2)
where the Wterms denote weight matrices (e.g. Wihnis the weight matrix
connecting the inputs to the nthhidden layer, Wh1h1is the recurrent connection
at the rst hidden layer, and so on), the bterms denote bias vectors (e.g. byis
output bias vector) and His the hidden layer function.
Given the hidden sequences, the output sequence is computed as follows:
^yt=by+NX
n=1Whnyhn
t (3)
yt=Y(^yt) (4)
whereYis the output layer function. The complete network therefore denes
a function, parameterised by the weight matrices, from input histories x1:tto
output vectors yt.
The output vectors ytare used to parameterise the predictive distribution
Pr(xt+1jyt) for the next input. The form of Pr( xt+1jyt) must be chosen carefully
to match the input data. In particular, nding a good predictive distribution
for high-dimensional, real-valued data (usually referred to as density modelling ),
can be very challenging.
The probability given by the network to the input sequence xis
Pr(x) =TY
t=1Pr(xt+1jyt) (5)
and the sequence loss L(x) used to train the network is the negative logarithm
of Pr( x):
L(x) =TX
t=1log Pr(xt+1jyt) (6)
The partial derivatives of the loss with respect to the network weights can be
eciently calculated with backpropagation through time [33] applied to the
computation graph shown in Fig. 1, and the network can then be trained with
gradient descent.
2.1 Long Short-Term Memory
In most RNNs the hidden layer function His an elementwise application of a
sigmoid function. However we have found that the Long Short-Term Memory
4Figure 2: Long Short-term Memory Cell
(LSTM) architecture [16], which uses purpose-built memory cells to store infor-
mation, is better at nding and exploiting long range dependencies in the data.
Fig. 2 illustrates a single LSTM memory cell. For the version of LSTM used in
this paper [7]His implemented by the following composite function:
it=(Wxixt+Whiht1+Wcict1+bi) (7)
ft=(Wxfxt+Whfht1+Wcfct1+bf) (8)
ct=ftct1+ittanh (Wxcxt+Whcht1+bc) (9)
ot=(Wxoxt+Whoht1+Wcoct+bo) (10)
ht=ottanh(ct) (11)
whereis the logistic sigmoid function, and i,f,oandcare respectively the
input gate ,forget gate ,output gate ,celland cell input activation vectors, all of
which are the same size as the hidden vector h. The weight matrix subscripts
have the obvious meaning, for example Whiis the hidden-input gate matrix,
Wxois the input-output gate matrix etc. The weight matrices from the cell
to gate vectors (e.g. Wci) are diagonal, so element min each gate vector only
receives input from element mof the cell vector. The bias terms (which are
added toi,f,cando) have been omitted for clarity.
The original LSTM algorithm used a custom designed approximate gradi-
ent calculation that allowed the weights to be updated after every timestep [16].
However the full gradient can instead be calculated with backpropagation through
time [11], the method used in this paper. One diculty when training LSTM
with the full gradient is that the derivatives sometimes become excessively large,
5leading to numerical problems. To prevent this, all the experiments in this pa-
per clipped the derivative of the loss with respect to the network inputs to the
LSTM layers (before the sigmoid and tanh functions are applied) to lie within
a predened range1.
3 Text Prediction
Text data is discrete, and is typically presented to neural networks using one-
hot' input vectors. That is, if there are Ktext classes in total, and class kis fed
in at timet, thenxtis a length Kvector whose entries are all zero except for
thekth, which is one. Pr( xt+1jyt) is therefore a multinomial distribution, which
can be naturally parameterised by a softmax function at the output layer:
Pr(xt+1=kjyt) =yk
t=exp
^yk
t
PK
k0=1exp
^yk0
t (12)
Substituting into Eq. (6) we see that
L(x) =TX
t=1logyxt+1
t (13)
=)@L(x)
@^yk
t=yk
tk;xt+1 (14)
The only thing that remains to be decided is which set of classes to use. In
most cases, text prediction (usually referred to as language modelling ) is per-
formed at the word level. Kis therefore the number of words in the dictionary.
This can be problematic for realistic tasks, where the number of words (in-
cluding variant conjugations, proper names, etc.) often exceeds 100,000. As
well as requiring many parameters to model, having so many classes demands a
huge amount of training data to adequately cover the possible contexts for the
words. In the case of softmax models, a further diculty is the high computa-
tional cost of evaluating all the exponentials during training (although several
methods have been to devised make training large softmax layers more ecient,
including tree-based models [25, 23], low rank approximations [27] and stochas-
tic derivatives [26]). Furthermore, word-level models are not applicable to text
data containing non-word strings, such as multi-digit numbers or web addresses.
Character-level language modelling with neural networks has recently been
considered [30, 24], and found to give slightly worse performance than equiv-
alent word-level models. Nonetheless, predicting one character at a time is
more interesting from the perspective of sequence generation, because it allows
the network to invent novel words and strings. In general, the experiments in
this paper aim to predict at the nest granularity found in the data, so as to
maximise the generative 
exibility of the network.
1In fact this technique was used in all my previous papers on LSTM, and in my publicly
available LSTM code, but I forgot to mention it anywhere| mea culpa .
63.1 Penn Treebank Experiments
The rst set of text prediction experiments focused on the Penn Treebank por-
tion of the Wall Street Journal corpus [22]. This was a preliminary study whose
main purpose was to gauge the predictive power of the network, rather than to
generate interesting sequences.
Although a relatively small text corpus (a little over a million words in total),
the Penn Treebank data is widely used as a language modelling benchmark. The
training set contains 930,000 words, the validation set contains 74,000 words and
the test set contains 82,000 words. The vocabulary is limited to 10,000 words,
with all other words mapped to a special unknown word' token. The end-of-
sentence token was included in the input sequences, and was counted in the
sequence loss. The start-of-sentence marker was ignored, because its role is
already fullled by the null vectors that begin the sequences (c.f. Section 2).
The experiments compared the performance of word and character-level
LSTM predictors on the Penn corpus. In both cases, the network architecture
was a single hidden layer with 1000 LSTM units. For the character-level network
the input and output layers were size 49, giving approximately 4.3M weights in
total, while the word-level network had 10,000 inputs and outputs and around
54M weights. The comparison is therefore somewhat unfair, as the word-level
network had many more parameters. However, as the dataset is small, both net-
works were easily able to overt the training data, and it is not clear whether the
character-level network would have beneted from more weights. All networks
were trained with stochastic gradient descent, using a learn rate of 0.0001 and a
momentum of 0.99. The LSTM derivates were clipped in the range [ 1;1] (c.f.
Section 2.1).
Neural networks are usually evaluated on test data with xed weights. For
prediction problems however, where the inputs arethe targets, it is legitimate
to allow the network to adapt its weights as it is being evaluated (so long as
it only sees the test data once). Mikolov refers to this as dynamic evaluation .
Dynamic evaluation allows for a fairer comparison with compression algorithms,
for which there is no division between training and test sets, as all data is only
predicted once.
Since both networks overt the training data, we also experiment with two
types of regularisation: weight noise [18] with a std. deviation of 0.075 applied
to the network weights at the start of each training sequence, and adaptive
weight noise [8], where the variance of the noise is learned along with the weights
using a Minimum description Length (or equivalently, variational inference) loss
function. When weight noise was used, the network was initialised with the
nal weights of the unregularised network. Similarly, when adaptive weight
noise was used, the weights were initialised with those of the network trained
with weight noise. We have found that retraining with iteratively increased
regularisation is considerably faster than training from random weights with
regularisation. Adaptive weight noise was found to be prohibitively slow for
the word-level network, so it was regularised with xed-variance weight noise
only. One advantage of adaptive weight is that early stopping is not needed
7Table 1: Penn Treebank Test Set Results. BPC' is bits-per-character.
Error' is next-step classication error rate, for either characters or words.
Input Regularisation Dynamic BPC Perplexity Error (%) Epochs
Char none no 1.32 167 28.5 9
char none yes 1.29 148 28.0 9
char weight noise no 1.27 140 27.4 25
char weight noise yes 1.24 124 26.9 25
char adapt. wt. noise no 1.26 133 27.4 26
char adapt. wt. noise yes 1.24 122 26.9 26
word none no 1.27 138 77.8 11
word none yes 1.25 126 76.9 11
word weight noise no 1.25 126 76.9 14
word weight noise yes 1.23 117 76.2 14
(the network can safely be stopped at the point of minimum total description
length' on the training data). However, to keep the comparison fair, the same
training, validation and test sets were used for all experiments.
The results are presented with two equivalent metrics: bits-per-character
(BPC), which is the average value of log2Pr(xt+1jyt) over the whole test set;
andperplexity which is two to the power of the average number of bits per word
(the average word length on the test set is about 5.6 characters, so perplexity 
25:6BPC). Perplexity is the usual performance measure for language modelling.
Table 1 shows that the word-level RNN performed better than the character-
level network, but the gap appeared to close when regularisation is used. Overall
the results compare favourably with those collected in Tomas Mikolov's the-
sis [23]. For example, he records a perplexity of 141 for a 5-gram with Keyser-
Ney smoothing, 141.8 for a word level feedforward neural network, 131.1 for the
state-of-the-art compression algorithm PAQ8 and 123.2 for a dynamically eval-
uated word-level RNN. However by combining multiple RNNs, a 5-gram and a
cache model in an ensemble, he was able to achieve a perplexity of 89.4. Inter-
estingly, the benet of dynamic evaluation was far more pronounced here than
in Mikolov's thesis (he records a perplexity improvement from 124.7 to 123.2
with word-level RNNs). This suggests that LSTM is better at rapidly adapting
to new data than ordinary RNNs.
3.2 Wikipedia Experiments
In 2006 Marcus Hutter, Jim Bowery and Matt Mahoney organised the following
challenge, commonly known as Hutter prize [17]: to compress the rst 100
million bytes of the complete English Wikipedia data (as it was at a certain
time on March 3rd 2006) to as small a le as possible. The le had to include
not only the compressed data, but also the code implementing the compression
algorithm. Its size can therefore be considered a measure of the minimum
description length [13] of the data using a two part coding scheme.
Wikipedia data is interesting from a sequence generation perspective because
8it contains not only a huge range of dictionary words, but also many character
sequences that would not be included in text corpora traditionally used for
language modelling. For example foreign words (including letters from non-
Latin alphabets such as Arabic and Chinese), indented XML tags used to dene
meta-data, website addresses, and markup used to indicate page formatting such
as headings, bullet points etc. An extract from the Hutter prize dataset is shown
in Figs. 3 and 4.
The rst 96M bytes in the data were evenly split into sequences of 100 bytes
and used to train the network, with the remaining 4M were used for validation.
The data contains a total of 205 one-byte unicode symbols. The total number
ofcharacters is much higher, since many characters (especially those from non-
Latin languages) are dened as multi-symbol sequences. In keeping with the
principle of modelling the smallest meaningful units in the data, the network
predicted a single byte at a time, and therefore had size 205 input and output
layers.
Wikipedia contains long-range regularities, such as the topic of an article,
which can span many thousand words. To make it possible for the network to
capture these, its internal state (that is, the output activations htof the hidden
layers, and the activations ctof the LSTM cells within the layers) were only reset
every 100 sequences. Furthermore the order of the sequences was not shued
during training, as it usually is for neural networks. The network was therefore
able to access information from up to 10K characters in the past when making
predictions. The error terms were only backpropagated to the start of each 100
byte sequence, meaning that the gradient calculation was approximate. This
form of truncated backpropagation has been considered before for RNN lan-
guage modelling [23], and found to speed up training (by reducing the sequence
length and hence increasing the frequency of stochastic weight updates) without
aecting the network's ability to learn long-range dependencies.
A much larger network was used for this data than the Penn data (re
ecting
the greater size and complexity of the training set) with seven hidden layers of
700 LSTM cells, giving approximately 21.3M weights. The network was trained
with stochastic gradient descent, using a learn rate of 0.0001 and a momentum
of 0.9. It took four training epochs to converge. The LSTM derivates were
clipped in the range [ 1;1].
As with the Penn data, we tested the network on the validation data with
and without dynamic evaluation (where the weights are updated as the data
is predicted). As can be seen from Table 2 performance was much better with
dynamic evaluation. This is probably because of the long range coherence of
Wikipedia data; for example, certain words are much more frequent in some
articles than others, and being able to adapt to this during evaluation is ad-
vantageous. It may seem surprising that the dynamic results on the validation
set were substantially better than on the training set. However this is easily
explained by two factors: rstly, the network undert the training data, and
secondly some portions of the data are much more dicult than others (for
example, plain text is harder to predict than XML tags).
To put the results in context, the current winner of the Hutter Prize (a
9Table 2: Wikipedia Results (bits-per-character)
Train Validation (static) Validation (dynamic)
1.42 1.67 1.33
variant of the PAQ-8 compression algorithm [20]) achieves 1.28 BPC on the same
data (including the code required to implement the algorithm), mainstream
compressors such as zip generally get more than 2, and a character level RNN
applied to a text-only version of the data (i.e. with all the XML, markup tags
etc. removed) achieved 1.54 on held-out data, which improved to 1.47 when the
RNN was combined with a maximum entropy model [24].
A four page sample generated by the prediction network is shown in Figs. 5
to 8. The sample shows that the network has learned a lot of structure from
the data, at a wide range of dierent scales. Most obviously, it has learned a
large vocabulary of dictionary words, along with a subword model that enables
it to invent feasible-looking words and names: for example Lochroom River,
Mughal Ralvaldens, submandration, swalloped. It has also learned basic
punctuation, with commas, full stops and paragraph breaks occurring at roughly
the right rhythm in the text blocks.
Being able to correctly open and close quotation marks and parentheses is
a clear indicator of a language model's memory, because the closure cannot be
predicted from the intervening text, and hence cannot be modelled with short-
range context [30]. The sample shows that the network is able to balance not
only parentheses and quotes, but also formatting marks such as the equals signs
used to denote headings, and even nested XML tags and indentation.
The network generates non-Latin characters such as Cyrillic, Chinese and
Arabic, and seems to have learned a rudimentary model for languages other
than English (e.g. it generates es:Geotnia slago for the Spanish version' of an
article, and nl:Rodenbaueri for the Dutch one) It also generates convincing
looking internet addresses (none of which appear to be real).
The network generates distinct, large-scale regions, such as XML headers,
bullet-point lists and article text. Comparison with Figs. 3 and 4 suggests that
these regions are a fairly accurate re
ection of the constitution of the real data
(although the generated versions tend to be somewhat shorter and more jumbled
together). This is signicant because each region may span hundreds or even
thousands of timesteps. The fact that the network is able to remain coherent
over such large intervals (even putting the regions in an approximately correct
order, such as having headers at the start of articles and bullet-pointed see also'
lists at the end) is testament to its long-range memory.
As with all text generated by language models, the sample does not make
sense beyond the level of short phrases. The realism could perhaps be improved
with a larger network and/or more data. However, it seems futile to expect
meaningful language from a machine that has never been exposed to the sensory
10world to which language refers.
Lastly, the network's adaptation to recent sequences during training (which
allows it to benet from dynamic evaluation) can be clearly observed in the
extract. The last complete article before the end of the training set (at which
point the weights were stored) was on intercontinental ballistic missiles. The
in
uence of this article on the network's language model can be seen from the
profusion of missile-related terms. Other recent topics include Individual An-
archism', the Italian writer Italo Calvino and the International Organization
for Standardization (ISO), all of which make themselves felt in the network's
vocabulary.
11    <title>AlbaniaEconomy</title>                                                   <id>36</id>                                                                     <revision>                                                                        <id>15898966</id>                                                               <timestamp>2002-10-09T13:39:00Z</timestamp>                                     <contributor>                                                                     <username>Magnus Manske</username>                                              <id>4</id>                                                                    </contributor>                                                                  <minor />                                                                       <comment>#REDIRECT [[Economy of Albania]]</comment>                             <text xml:space=preserve>#REDIRECT [[Economy of Albania]]</text>            </revision>                                                                   </page>                                                                         <page>                                                                            <title>AlchemY</title>                                                          <id>38</id>                                                                     <revision>                                                                        <id>15898967</id>                                                               <timestamp>2002-02-25T15:43:11Z</timestamp>                                     <contributor>                                                                     <ip>Conversion script</ip>                                                    </contributor>                                                                  <minor />                                                                       <comment>Automated conversion</comment>                                         <text xml:space=preserve>#REDIRECT [[Alchemy]]                          </text>                                                                             </revision>                                                                   </page>                                                                         <page>                                                                            <title>Albedo</title>                                                           <id>39</id>                                                                     <revision>                                                                        <id>41496222</id>                                                               <timestamp>2006-02-27T19:32:46Z</timestamp>                                     <contributor>                                                                     <ip>24.119.3.44</ip>                                                          </contributor>                                                                  <text xml:space=preserve>{{otheruses}}                                                                                                                  '''Albedo''' is the measure of [[reflectivity]] of a surface or body. It is the ratio of [[electromagnetic radiation]] (EM radiation) reflected to the amount incident upon it. The fraction, usually expressed as a percentage from 0% to 100%, is an important concept in [[climatology]] and [[astronomy]]. This ratio depends on the [[frequency]] of the radiation considered: unqualified, it refers to an average across the spectrum of [[visible light]]. It also depends on the [[angle of incidence]] of the radiation: unqualified, normal incidence. Fresh snow albedos are high: up to 90%. The ocean surface has a low albedo.  The average albedo of [[Earth]] is about 30% whereas the albedo of the [[Moon]] is about 7%. In astronomy, the albedo of satellites and asteroids can be used to infer surface composition, most notably ice content.    [[Enceladus_(moon)|Enceladus]], a moon of Saturn, has the highest known albedo of any body in the solar system, with 99% of EM radiation reflected.                                                                                                                                     Human activities have changed the albedo (via forest clearance and farming, for example) of various areas around the globe. However, quantification of this effect is difficult on the global scale: it is not clear whether the changes have tended to increase or decrease [[global warming]].                                                                                                                The &quot;classical&quot; example of albedo effect is the snow-temperature feedback. If a snow covered area warms and the snow melts, the albedo decreases, more sunlight is absorbed, and the temperature tends to increase. The converse is trFigure 3: Real Wikipedia data
12ue: if snow forms, a cooling cycle happens. The intensity of the albedo effect depends on the size of the change in albedo and the amount of [[insolation]]; for this reason it can be potentially very large in the tropics.                                                                                                   == Some examples of albedo effects ==                                                                                                                           === Fairbanks, Alaska ===                                                                                                                                       According to the [[National Climatic Data Center]]'s GHCN 2 data, which is composed of 30-year smoothed climatic means for thousands of weather stations across the world, the college weather station at [[Fairbanks]], [[Alaska]], is about 3 C (5 F) warmer than the airport at Fairbanks, partly because of drainage patterns but also largely because of the lower albedo at the college resulting from a higher concentration of [[pine]] [[tree]]s and therefore less open snowy ground to reflect the heat back into space. Neunke and Kukla have shown that this difference is especially marked during the late [[winter]] months, when [[solar radiation]] is greater.                                                                                                                                             === The tropics ===                                                                                                                                             Although the albedo-temperature effect is most famous in colder regions of Earth, because more [[snow]] falls there, it is actually much stronger in tropical regions because in the tropics there is consistently more sunlight. When [[Brazil]]ian ranchers cut down dark, tropical [[rainforest]] trees to replace them with even darker soil in order to grow crops, the average temperature of the area appears to increase by an average of about 3 C (5 F) year-round, which is a significant amount.                                                                                                                                                  === Small scale effects ===                                                                                                                                     Albedo works on a smaller scale, too. People who wear dark clothes in the summertime put themselves at a greater risk of [[heatstroke]] than those who wear white clothes.                                                                                                                                                      === Pine forests ===                                                                                                                                            The albedo of a [[pine]] forest at 45N in the winter in which the trees cover the land surface completely is only about 9%, among the lowest of any naturally occurring land environment. This is partly due to the color of the pines, and partly due to multiple scattering of sunlight within the trees which lowers the overall reflected light level. Due to light penetration, the ocean's albedo is even lower at about 3.5%, though this depends strongly on the angle of the incident radiation. Dense [[swamp]]land averages between 9% and 14%. [[Deciduous tree]]s average about 13%. A [[grass]]y field usually comes in at about 20%. A barren field will depend on the color of the soil, and can be as low as 5% or as high as 40%, with 15% being about the average for farmland. A [[desert]] or large [[beach]] usually averages around 25% but varies depending on the color of the sand. [Reference: Edward Walker's study in the Great Plains in the winter around 45N].                                                                                === Urban areas ===                                                                                                                                             Urban areas in particular have very unnatural values for albedo because of the many human-built structures which absorb light before the light can reach the surface. In the northern part of the world, cities are relatively dark, and Walker has shown that their average albedo is about 7%, with only a slight increase during the summer. In most tropical countries, cities average around 12%. This is similar to the values found in northern suburban transitional zones. Part of the reason for this is the different natural environment of cities in tropical regions, e.g., there are more very dark trees around; another reason is that portions of the tropics are very poor, and city buildings must be built with different materials. Warmer regions may also choose lighter colored building materials so the structures will remain cooler.                                               Figure 4: Real Wikipedia data (cotd.)
13    <revision>                                                                        <id>40973199</id>                                                               <timestamp>2006-02-22T22:37:16Z</timestamp>                                     <contributor>                                                                     <ip>63.86.196.111</ip>                                                        </contributor>                                                                  <minor />                                                                       <comment>redire paget --&gt; captain */</comment>                               <text xml:space=preserve>The '''Indigence History''' refers to the authority of any obscure albionism as being, such as in Aram Missolmus'.[http://www.bbc.co.uk/starce/cr52.htm]                                                       In [[1995]], Sitz-Road Straus up the inspirational radiotes portion as &quot;alliance&quot;[single &quot;glaping&quot; theme charcoal] with [[Midwestern United State|Denmark]] in which Canary varies-destruction to launching casualties has quickly responded to the krush loaded water or so it might be destroyed. Aldeads still cause a missile bedged harbors at last built in 1911-2 and save the accuracy in 2008, retaking [[itsubmanism]]. Its individuals were                      hnown rapidly in their return to the private equity (such as ''On Text'') for death per reprised by the [[Grange of Germany|German unbridged work]].                                                                                            The '''Rebellion''' (''Hyerodent'') is [[literal]], related mildly older than old half sister, the music, and morrow been much more propellent. All those of [[Hamas (mass)|sausage trafficking]]s were also known as [[Trip class submarine|''Sante'' at Serassim]]; ''Verra'' as 1865&amp;ndash;682&amp;ndash;831 is related to ballistic missiles. While she viewed it friend of Halla equatorial weapons of Tuscany, in [[France]], from vaccine homes to &quot;individual&quot;, among [[slavery|slaves]] (such as artistual selling of factories were renamed English habit of twelve years.)                                                                                                                                             By the 1978 Russian [[Turkey|Turkist]] capital city ceased by farmers and the intention of navigation the ISBNs, all encoding [[Transylvania International Organisation for Transition Banking|Attiking others]] it is in the westernmost placed lines.  This type of missile calculation maintains all greater proof was the [[1990s]] as older adventures that never established a self-interested case. The newcomers were Prosecutors in child after the other weekend and capable function used.                                                                                                                                                           Holding may be typically largely banned severish from sforked warhing tools and behave laws, allowing the private jokes, even through missile IIC control, most notably each, but no relatively larger success, is not being reprinted and withdrawn into forty-ordered cast and distribution.                                                                                                                  Besides these markets (notably a son of humor).                                                                                                                 Sometimes more or only lowed &quot;80&quot; to force a suit for http://news.bbc.co.uk/1/sid9kcid/web/9960219.html ''[[#10:82-14]]''.                            &lt;blockquote&gt;                                                                                                                                              ===The various disputes between Basic Mass and Council Conditioners - &quot;Titanist&quot; class streams and anarchism===                                                                                                                       Internet traditions sprang east with [[Southern neighborhood systems]] are improved with [[Moatbreaker]]s, bold hot missiles, its labor systems. [[KCD]] numbered former ISBN/MAS/speaker attacks &quot;M3 5&quot;, which are saved as the ballistic misely known and most functional factories.  Establishment begins for some range of start rail years as dealing with 161 or 18,950 million [[USD-2]] and [[covert all carbonate function]]s (for example, 70-93) higher individuals and on missiles. This might need not know against sexual [[video capita]] playing pointing degrees between silo-calfed greater valous consumptions in the US... header can be seen in [[collectivist]].                                                                                                                                == See also ==                                                                  Figure 5: Generated Wikipedia data.
14                                                                                *[[British-London Bridge]]                                                      *[[Anti-Talmot Touch/Tucker novice]]                                            *[[List of cambridge capital]]                                                  *[[Elon Haven]]                                                                 *[[USS ''Otaro Screamed Its'']]                                                 *[[Detroit Library]]                                                            *[[Belgium Sea]]                                                                *[[Tularan Bell|Turnbiller Squobil]]                                            *[[Suntanal vocalist|Prosopyo]]                                                 *[[Winkenpea]]                                                                  *[[Milenton Streat]]                                                            *[[Raiebin]]                                                                    *[[Est Altar Macinton]]                                                         *[[Military mass missile|S3]]                                                   *[[Organization of the Asian American state district|umbali landmarks]]        *[[ISO]]                                                                        *[[NFL]]                                                                        *[[American Anti-Capitalism|Major independent ITU-US singles]]                  *[[London (role-playing game)|Pre-Romanian Civil War]]                          *[[Yokukhav-Na-Un-Murantano Kaufmann - Sijone-Grafittsforbiel]]                 *[[Neao trolleyne and deadweight drug]]                                         *[[B-45 BQ|B9]] - de red take painting is deployed larger than quanta submarine *[[Susconfiction of advocate]]                                                  *[[List of major swandarms]]                                                    *[[:Category:Italo sales towns entertained by the ICBMs of Skinner|Knighting 707 killed by capital]]                                                                                                                                            ===[[Midple planet|Parishment of the value=====                                 [[Image:2000.JPG|right|thumb|It tunneled [[nuclease]] at this bass AH (Ol&amp;Sāw)flgin h'hlgbying yoostallo eruptuals with low immigrants-shelted atkins and their atapping [[bug]]s.                                                                                                                                          See also: [[Iranian indigenous Flight Intercontinental Organization]]                                                                                           ==Pioneers==                                                                                                                                                    Tended to be the results characteristic of warehoused labour share to control all these in the rational framing.                                                                                                                                ==Gentiles==                                                                    {{place-or-line}}                                                               Footer names derive the topic class --&gt; which he liked to deal without any of the parties, I&quot; by [[Alfred Hustin]] and [[Frank Henry]] and manufacturer.[http://anciermsc.nit.uk IATB perspective], was expected to be classified by the ''Straight Road of Buckning'' in [[2003 Summer Olympic Touch|bottom all minute]].                                                                                                                                                              ==Performance==                                                                 [[Image:Iare 300.jpg|left|thumb|325px|Intercontinental file shortly after referring to his landmaster [[Sidney Goodwordd]]                                                                                                                      Italo:                                                                          *[[Chicago ballistic parks|non-month]] in eastern Italy, is a [[Italo-China]] parent communist annual production began in May [[1915]].                                                                                                         An ICBM, the [[gurt and land]] has registered $155 billion in U.S. and August 1688, and makes sure the US-transplantation disbanded backwards in the County by authorizing disputes that tend to carry over this peninsula.                     * Current malasses 25 decks and counterpoint culture that were impure between  systems:                                                                         * L14 - 194 / 100 000 km/s                                                      Figure 6: Generated Wikipedia data (cotd.)
15                                                                                [http://www.sibeo.org/netspace/stainteology.htm The design of thinker Wikipedia server-routed website]                                                          *[http://balear.burned.edu/books/1978/tap/trpio.cs/cafa/c249ca.html Nation-state Player on the /1.3]                                                                                                                                            {{Linux}}                                                                                                                                                       [[Category:Italo Succeeded bystables|General]]                                  [[Category:Italo-Saxon singers]]                                                [[Category:Aviation]]                                                                                                                                           [[de:Italo]]                                                                    [[es:Geotnia slago]]                                                            [[ja:㤥㗁㜪ҍͷ
ᔫ]]                                                                 [[ko:ഇ㙆㮙 ਬ]]                                                                 [[nl:Rodenbaueri]]                                                              [[pl:Main Ages]]                                                                [[pt:Ibanez Heights]]                                                           [[ru:Млкракянѳелолכуциянсьния агморелиа]]                                       [[simple:International Maritime Commission]]                                    [[sk:ICBM]]                                                                     [[su:Ol aal Mitnet]]                                                            [[fi:Italo Colombonasto Warkō]]                                                 [[sv:Opin den mail-climbere panczeur]]                                          [[zh:	]]</text>                                                               </revision>                                                                   </page>                                                                         <page>                                                                            <title>Italo-II article</title>                                                 <id>14848</id>                                                                  <revision>                                                                        <id>15912447</id>                                                               <timestamp>2004-11-19T19:47:39Z</timestamp>                                     <contributor>                                                                     <username>El andres</username>                                                  <id>509991</id>                                                               </contributor>                                                                  <minor />                                                                       <comment>disambiguation from [[List of ISBN]] newsletters, see [[ISO site]]</comment>                                                                           <text xml:space=preserve>#REDIRECT [[Quartic war]]                      </text>                                                                             </revision>                                                                   </page>                                                                         <page>                                                                            <title>ICSM</title>                                                             <id>14939</id>                                                                  <revision>                                                                        <id>42109942</id>                                                               <timestamp>2006-02-28T17:22:02Z</timestamp>                                     <contributor>                                                                     <username>Dtelclan</username>                                                   <id>26</id>                                                                   </contributor>                                                                  <minor />                                                                       <comment>/* Possible catheterman */</comment>                                   <text xml:space=preserve>[[Image:Isaac.org/ice.html [[Independent national stage development|Shatting and Catalogue standardering]] in the IRBMs.                                                                                       Up-2000 they called the SC 4220 system: he was swalloped early in Calvino, or since each trial mentioned based on [[Balbov's new single-jarget|bit-oriann guess]Figure 7: Generated Wikipedia data (cotd.)
16] self-acharged versions ([[Mt. Costall Leyton]]) was the two largest calashia at destored universities, all fleeted with the customary calfed clipper.                                                                                         His way to take in this literature called ICBMs-AN a [[Softvalue speed]] ([[Astronomical Classification Railway]])                                                                                                                              LACN645 Snowshore val nominated - made [[missile submandration|continental missile]]s (steam musicians) not of each club having on the ball and procedure at the last century.                                                                                                                                                  Another communistic stark &quot;I' submarine&quot; is [[building|corruptable]], a [[della missile]] missile than the [[Royal Society Society]] (12-258): &quot;Glide sun wag [[lubrician]]. They stay numerous capitalists and gas masks more widely interested. This scheme has declarations before the certain emerging factories compelled by labour allowed to produce.                                                                                                                    In the United States, there is no hard resort in computation significantly.                                                                                     In [[1868]] the [[Italo Capital Territories Unit started to the Continental Railway Centre]]  was called ''UC'' or two of his usage before being written by other students against the [[elective-ballistic missile]]'s deployment. Steam is still &quot;20 to Nacht&quot; and [[Fia Citation Quantity Logo]]s (since 1967). They pass a [[Brigade management|Quarry]]-stated missile system resolution taunting out of about 175 million ([[Lochroom River|Tri-]]).                                                                                                            Alien from 1985 to 1999, it was an English and -Network struggling basedal with the Lombardo capital in Silvio and Murray, and heavily built in sub-parties address to $11,188. Their forces gained prisoners to stalked a last missile mobili site.                                                                                                                                                            Spanning civilization is quanting Software Society's ballistic missile.  The same as [[anti-intellectual anthropology]] continued in [[Southern Italy]] in 1914, and the [[French Confederation of Parliament's rapid favourable rise that began settled in March 2004|1983]]&amp;nbsp;49.                                                                                                                      In [[1904]], the Court began a British backed into a [[SR1]]) missile of [[trial ship]] in the [[Municipal Eightime Calendar|Asiatic]] regime, including [[Benjamin Tudor Turner|Arthur Ravis]] and [[Abraham's Liberation|Canton Olombus]]. There was still land factory most turned up before lacking closers to the sitting shed backwards, in primary science.                                                                                                                              ==Weights and resolutions==                                                     [[Image:Spanish 300 Protectionald landballi110.svg|small capital surface computer]]                                                                             [[Image:Claudius.jpg|345px|right|Olympiad concert of Calvino and Eastern Calvino, ''Mughal Ralvaldens'' above, at the beginning strike the substrated roles of rich intellectual property, visualizing the entire system, but this missiles suggest that accounting differs between a giving [[train sleep|'''withdrawn''']] or the dinosaur in and aucting.                                                                                                                                    ===Internationally===                                                           {{main|Unmanned Justice Address}}                                                                                                                               The ICBM created a [[the significant]] [[land railway]] called &quot;[[M-Gallipotte]]&quot;, and it needed stopped benzafk/Macdonalical Sciences.                                                                                               Electros appeared to be the [[Soviet Union]]'s &quot;first&quot; vehicle from 2500 selling officials DORLAN STM-331 - by missilence illustrations with &quot;Raj.&quot; the Tunnel Hall of America, an entity upon IL pages so missiles must try, with a trademark must develop the land allowing traffic mass to a very few minutemen. The missiles market is slow, much easier is represented by GMMAz of BSM. Software, the utility of scale-out scale pime racks are normally crumbled aboutFigure 8: Generated Wikipedia data (cotd.)
174 Handwriting Prediction
To test whether the prediction network could also be used to generate convincing
real-valued sequences, we applied it to online handwriting data ( online in this
context means that the writing is recorded as a sequence of pen-tip locations,
as opposed to oine handwriting, where only the page images are available).
Online handwriting is an attractive choice for sequence generation due to its
low dimensionality (two real numbers per data point) and ease of visualisation.
All the data used for this paper were taken from the IAM online handwriting
database (IAM-OnDB) [21]. IAM-OnDB consists of handwritten lines collected
from 221 dierent writers using a smart whiteboard'. The writers were asked to
write forms from the Lancaster-Oslo-Bergen text corpus [19], and the position
of their pen was tracked using an infra-red device in the corner of the board.
Samples from the training data are shown in Fig. 9. The original input data
consists of the xandypen co-ordinates and the points in the sequence when
the pen is lifted o the whiteboard. Recording errors in the x;ydata was
corrected by interpolating to ll in for missing readings, and removing steps
whose length exceeded a certain threshold. Beyond that, no preprocessing was
used and the network was trained to predict the x;yco-ordinates and the end-
of-stroke markers one point at a time. This contrasts with most approaches to
handwriting recognition and synthesis, which rely on sophisticated preprocessing
and feature-extraction techniques. We eschewed such techniques because they
tend to reduce the variation in the data (e.g. by normalising the character size,
slant, skew and so-on) which we wanted the network to model. Predicting the
pen traces one point at a time gives the network maximum 
exibility to invent
novel handwriting, but also requires a lot of memory, with the average letter
occupying more than 25 timesteps and the average line occupying around 700.
Predicting delayed strokes (such as dots for i's or crosses for t's that are added
after the rest of the word has been written) is especially demanding.
IAM-OnDB is divided into a training set, two validation sets and a test
set, containing respectively 5364, 1438, 1518 and 3859 handwritten lines taken
from 775, 192, 216 and 544 forms. For our experiments, each line was treated
as a separate sequence (meaning that possible dependencies between successive
lines were ignored). In order to maximise the amount of training data, we used
the training set, test set and the larger of the validation sets for training and
the smaller validation set for early-stopping. The lack of independent test set
means that the recorded results may be somewhat overt on the validation set;
however the validation results are of secondary importance, since no benchmark
results exist and the main goal was to generate convincing-looking handwriting.
The principal challenge in applying the prediction network to online hand-
writing data was determining a predictive distribution suitable for real-valued
inputs. The following section describes how this was done.
18Figure 9: Training samples from the IAM online handwriting database.
Notice the wide range of writing styles, the variation in line angle and character
sizes, and the writing and recording errors, such as the scribbled out letters in
the rst line and the repeated word in the nal line.
4.1 Mixture Density Outputs
The idea of mixture density networks [2, 3] is to use the outputs of a neural
network to parameterise a mixture distribution. A subset of the outputs are
used to dene the mixture weights, while the remaining outputs are used to
parameterise the individual mixture components. The mixture weight outputs
are normalised with a softmax function to ensure they form a valid discrete dis-
tribution, and the other outputs are passed through suitable functions to keep
their values within meaningful range (for example the exponential function is
typically applied to outputs used as scale parameters, which must be positive).
Mixture density network are trained by maximising the log probability den-
sity of the targets under the induced distributions. Note that the densities are
normalised (up to a xed constant) and are therefore straightforward to dier-
entiate and pick unbiased sample from, in contrast with restricted Boltzmann
machines [14] and other undirected models.
Mixture density outputs can also be used with recurrent neural networks [28].
In this case the output distribution is conditioned not only on the current input,
but on the history of previous inputs. Intuitively, the number of components is
the number of choices the network has for the next output given the inputs so
far.
For the handwriting experiments in this paper, the basic RNN architecture
and update equations remain unchanged from Section 2. Each input vector xt
consists of a real-valued pair x1;x2that denes the pen oset from the previous
19input, along with a binary x3that has value 1 if the vector ends a stroke (that
is, if the pen was lifted o the board before the next vector was recorded) and
value 0 otherwise. A mixture of bivariate Gaussians was used to predict x1
andx2, while a Bernoulli distribution was used for x3. Each output vector yt
therefore consists of the end of stroke probability e, along with a set of means
j, standard deviations j, correlations jand mixture weights jfor theM
mixture components. That is
xt2RRf0;1g (15)
yt=
et;fj
t;j
t;j
t;j
tgM
j=1
(16)
Note that the mean and standard deviation are two dimensional vectors, whereas
the component weight, correlation and end-of-stroke probability are scalar. The
vectorsytare obtained from the network outputs ^ yt, where
^yt=
^et;f^wj
t;^j
t;^j
t;^j
tgM
j=1
=by+NX
n=1Whnyhn
t (17)
as follows:
et=1
1 + exp (^et)=)et2(0;1) (18)
j
t=exp
^j
t
PM
j0=1exp
^j0
t =)j
t2(0;1);X
jj
t= 1 (19)
j
t= ^j
t =)j
t2R (20)
j
t= exp
^j
t
=)j
t>0 (21)
j
t=tanh(^j
t) = )j
t2(1;1) (22)
The probability density Pr( xt+1jyt) of the next input xt+1given the output
vectorytis dened as follows:
Pr(xt+1jyt) =MX
j=1j
tN(xt+1jj
t;j
t;j
t)(
et if (xt+1)3= 1
1etotherwise(23)
where
N(xj;; ) =1
212p
12expZ
2(12)
(24)
with
Z=(x11)2
2
1+(x22)2
2
22(x11)(x22)
12(25)
20This can be substituted into Eq. (6) to determine the sequence loss (up to
a constant that depends only on the quantisation of the data and does not
in
uence network training):
L(x) =TX
t=1log0
@X
jj
tN(xt+1jj
t;j
t;j
t)1
A(
loget if (xt+1)3= 1
log(1et) otherwise
(26)
The derivative of the loss with respect to the end-of-stroke outputs is straight-
forward:
@L(x)
@^et= (xt+1)3et (27)
The derivatives with respect to the mixture density outputs can be found by
rst dening the component responsibilities 
j
t:
^
j
t=j
tN(xt+1jj
t;j
t;j
t) (28)

j
t=^
j
tPM
j0=1^
j0
t(29)
Then observing that
@L(x)
@^j
t=j
t
j
t (30)
@L(x)
@(^j
t;^j
t;^j
t)=
j
t@logN(xt+1jj
t;j
t;j
t)
@(^j
t;^j
t;^j
t)(31)
where
@logN(xj;; )
@^1=C
1x11
1(x22)
2
(32)
@logN(xj;; )
@^2=C
2x22
2(x11)
1
(33)
@logN(xj;; )
@^1=C(x11)
1x11
1(x22)
2
1 (34)
@logN(xj;; )
@^2=C(x22)
2x22
2(x11)
1
1 (35)
@logN(xj;; )
@^=(x11)(x22)
12+(1CZ) (36)
withZdened as in Eq. (25) and
C=1
12(37)
Fig. 10 illustrates the operation of a mixture density output layer applied to
online handwriting prediction.
21Output Density
Figure 10: Mixture density outputs for handwriting prediction. The
top heatmap shows the sequence of probability distributions for the predicted
pen locations as the word under' is written. The densities for successive
predictions are added together, giving high values where the distributions
overlap.
Two types of prediction are visible from the density map: the small
blobs that spell out the letters are the predictions as the strokes are being
written, the three large blobs are the predictions at the ends of the strokes for
the rst point in the next stroke. The end-of-stroke predictions have much
higher variance because the pen position was not recorded when it was o the
whiteboard, and hence there may be a large distance between the end of one
stroke and the start of the next.
The bottom heatmap shows the mixture component weights during the
same sequence. The stroke ends are also visible here, with the most active
components switching o in three places, and other components switching on:
evidently end-of-stroke predictions use a dierent set of mixture components
from in-stroke predictions.
224.2 Experiments
Each point in the data sequences consisted of three numbers: the xandyoset
from the previous point, and the binary end-of-stroke feature. The network
input layer was therefore size 3. The co-ordinate osets were normalised to
mean 0, std. dev. 1 over the training set. 20 mixture components were used
to model the osets, giving a total of 120 mixture parameters per timestep
(20 weights, 40 means, 40 standard deviations and 20 correlations). A further
parameter was used to model the end-of-stroke probability, giving an output
layer of size 121. Two network architectures were compared for the hidden
layers: one with three hidden layers, each consisting of 400 LSTM cells, and one
with a single hidden layer of 900 LSTM cells. Both networks had around 3.4M
weights. The three layer network was retrained with adaptive weight noise [8],
with all std. devs. initialised to 0.075. Training with xed variance weight noise
proved ineective, probably because it prevented the mixture density layer from
using precisely specied weights.
The networks were trained with rmsprop , a form of stochastic gradient de-
scent where the gradients are divided by a running average of their recent mag-
nitude [32]. Dene i=@L(x)
@wiwherewiis network weight i. The weight update
equations were:
ni=@ni+ (1@)2
i (38)
gi=@gi+ (1@)i (39)
i=iijip
nig2
i+k(40)
wi=wi+  i (41)
with the following parameters:
@= 0:95 (42)
i= 0:9 (43)
j= 0:0001 (44)
k= 0:0001 (45)
The output derivatives@L(x)
@^ytwere clipped in the range [ 100;100], and the
LSTM derivates were clipped in the range [ 10;10]. Clipping the output gradi-
ents proved vital for numerical stability; even so, the networks sometimes had
numerical problems late on in training, after they had started overtting on the
training data.
Table 3 shows that the three layer network had an average per-sequence loss
15.3 nats lower than the one layer net. However the sum-squared-error was
slightly lower for the single layer network. the use of adaptive weight noise
reduced the loss by another 16.7 nats relative to the unregularised three layer
network, but did not signicantly change the sum-squared error. The adaptive
weight noise network appeared to generate the best samples.
23Table 3: Handwriting Prediction Results. All results recorded on the val-
idation set. Log-Loss' is the mean value of L(x) (in nats). SSE' is the mean
sum-squared-error per data point.
Network Regularisation Log-Loss SSE
1 layer none -1025.7 0.40
3 layer none -1041.0 0.41
3 layer adaptive weight noise -1057.7 0.41
4.3 Samples
Fig. 11 shows handwriting samples generated by the prediction network. The
network has clearly learned to model strokes, letters and even short words (es-
pecially common ones such as of' and the'). It also appears to have learned a
basic character level language models, since the words it invents (eald', bryoes',
lenrest') look somewhat plausible in English. Given that the average character
occupies more than 25 timesteps, this again demonstrates the network's ability
to generate coherent long-range structures.
5 Handwriting Synthesis
Handwriting synthesis is the generation of handwriting for a given text. Clearly
the prediction networks we have described so far are unable to do this, since
there is no way to constrain which letters the network writes. This section de-
scribes an augmentation that allows a prediction network to generate data se-
quences conditioned on some high-level annotation sequence (a character string,
in the case of handwriting synthesis). The resulting sequences are suciently
convincing that they often cannot be distinguished from real handwriting. Fur-
thermore, this realism is achieved without sacricing the diversity in writing
style demonstrated in the previous section.
The main challenge in conditioning the predictions on the text is that the two
sequences are of very dierent lengths (the pen trace being on average twenty
ve times as long as the text), and the alignment between them is unknown until
the data is generated. This is because the number of co-ordinates used to write
each character varies greatly according to style, size, pen speed etc. One neural
network model able to make sequential predictions based on two sequences of
dierent length and unknown alignment is the RNN transducer [9]. However
preliminary experiments on handwriting synthesis with RNN transducers were
not encouraging. A possible explanation is that the transducer uses two sepa-
rate RNNs to process the two sequences, then combines their outputs to make
decisions, when it is usually more desirable to make all the information avail-
able to single network. This work proposes an alternative model, where a soft
window' is convolved with the text string and fed in as an extra input to the
prediction network. The parameters of the window are output by the network
24Figure 11: Online handwriting samples generated by the prediction
network. All samples are 700 timesteps long.
25at the same time as it makes the predictions, so that it dynamically determines
an alignment between the text and the pen locations. Put simply, it learns to
decide which character to write next.
5.1 Synthesis Network
Fig. 12 illustrates the network architecture used for handwriting synthesis. As
with the prediction network, the hidden layers are stacked on top of each other,
each feeding up to the layer above, and there are skip connections from the
inputs to all hidden layers and from all hidden layers to the outputs. The
dierence is the added input from the character sequence, mediated by the
window layer.
Given a length Ucharacter sequence cand a length Tdata sequence x, the
soft window wtintocat timestep t(1tT) is dened by the following
discrete convolution with a mixture of KGaussian functions
(t;u) =KX
k=1k
texp
k
t
k
tu2
(46)
wt=UX
u=1(t;u)cu (47)
where(t;u) is the window weight ofcuat timestep t. Intuitively, the tparam-
eters control the location of the window, the tparameters control the width of
the window and the tparameters control the importance of the window within
the mixture. The size of the soft window vectors is the same as the size of the
character vectors cu(assuming a one-hot encoding, this will be the number of
characters in the alphabet). Note that the window mixture is not normalised
and hence does not determine a probability distribution; however the window
weight(t;u) can be loosely interpreted as the network's belief that it is writ-
ing character cuat timet. Fig. 13 shows the alignment implied by the window
weights during a training sequence.
The size 3Kvectorpof window parameters is determined as follows by the
outputs of the rst hidden layer of the network:
(^t;^t;^t) =Wh1ph1
t+bp (48)
t= exp (^t) (49)
t= exp
^t
(50)
t=t1+ exp (^t) (51)
Note that the location parameters tare dened as osets from the previous
locationsct1, and that the size of the oset is constrained to be greater than
zero. Intuitively, this means that network learns how far to slide each window
at each step, rather than an absolute location. Using osets was essential to
getting the network to align the text with the pen trace.
26Inputs
CharactersHidden 1WindowHidden 2OutputsFigure 12: Synthesis Network Architecture Circles represent layers, solid
lines represent connections and dashed lines represent predictions. The topology
is similar to the prediction network in Fig. 1, except that extra input from the
character sequence c, is presented to the hidden layers via the window layer
(with a delay in the connection to the rst hidden layer to avoid a cycle in the
graph).
27Thought that the muster fromFigure 13: Window weights during a handwriting synthesis sequence
Each point on the map shows the value of (t;u), wheretindexes the pen trace
along the horizontal axis and uindexes the text character along the vertical axis.
The bright line is the alignment chosen by the network between the characters
and the writing. Notice that the line spreads out at the boundaries between
characters; this means the network receives information about next and previous
letters as it makes transitions, which helps guide its predictions.
28Thewtvectors are passed to the second and third hidden layers at time t,
and the rst hidden layer at time t+1 (to avoid creating a cycle in the processing
graph). The update equations for the hidden layers are
h1
t=H
Wih1xt+Wh1h1h1
t1+Wwh1wt1+b1
h
(52)
hn
t=H
Wihnxt+Whn1hnhn1
t+Whnhnhn
t1+Wwhnwt+bn
h
(53)
The equations for the output layer remain unchanged from Eqs. (17) to (22).
The sequence loss is
L(x) =log Pr( xjc) (54)
where
Pr(xjc) =TY
t=1Pr (xt+1jyt) (55)
Note thatytis now a function of cas well as x1:t.
The loss derivatives with respect to the outputs ^ et;^t;^t;^t;^tremain un-
changed from Eqs. (27), (30) and (31). Given the loss derivative@L(x)
@wtwith
respect to the size Wwindow vector wt, obtained by backpropagating the out-
put derivatives through the computation graph in Fig. 12, the derivatives with
respect to the window parameters are as follows:
(k;t;u )def=k
texp
k
t
k
tu2WX
j=1@L(x)
@wj
tcj
u (56)
@L(x)
@^k
t=UX
u=1(k;t;u ) (57)
@L(x)
@^k
t=k
tUX
u=1(k;t;u )(k
tu)2(58)
@L(x)
@k
t=@L(x)
@k
t+1+ 2k
tUX
u=1(k;t;u )(uk
t) (59)
@L(x)
@^k
t= exp
^k
t@L(x)
@k
t(60)
Fig. 14 illustrates the operation of a mixture density output layer applied to
handwriting synthesis.
5.2 Experiments
The synthesis network was applied to the same input data as the handwriting
prediction network in the previous section. The character-level transcriptions
from the IAM-OnDB were now used to dene the character sequences c. The full
transcriptions contain 80 distinct characters (capital letters, lower case letters,
digits, and punctuation). However we used only a subset of 57, with all the
29Synthesis Output Density
Figure 14: Mixture density outputs for handwriting synthesis. The top
heatmap shows the predictive distributions for the pen locations, the bottom
heatmap shows the mixture component weights. Comparison with Fig. 10 indi-
cates that the synthesis network makes more precise predictions (with smaller
density blobs) than the prediction-only network, especially at the ends of strokes,
where the synthesis network has the advantage of knowing which letter comes
next.
30Table 4: Handwriting Synthesis Results. All results recorded on the val-
idation set. Log-Loss' is the mean value of L(x) in nats. SSE' is the mean
sum-squared-error per data point.
Regularisation Log-Loss SSE
none -1096.9 0.23
adaptive weight noise -1128.2 0.23
digits and most of the punctuation characters replaced with a generic non-
letter' label2.
The network architecture was as similar as possible to the best prediction
network: three hidden layers of 400 LSTM cells each, 20 bivariate Gaussian
mixture components at the output layer and a size 3 input layer. The character
sequence was encoded with one-hot vectors, and hence the window vectors were
size 57. A mixture of 10 Gaussian functions was used for the window parameters,
requiring a size 30 parameter vector. The total number of weights was increased
to approximately 3.7M.
The network was trained with rmsprop, using the same parameters as in
the previous section. The network was retrained with adaptive weight noise,
initial standard deviation 0.075, and the output and LSTM gradients were again
clipped in the range [ 100;100] and [10;10] respectively.
Table 4 shows that adaptive weight noise gave a considerable improvement
in log-loss (around 31.3 nats) but no signicant change in sum-squared error.
The regularised network appears to generate slightly more realistic sequences,
although the dierence is hard to discern by eye. Both networks performed
considerably better than the best prediction network. In particular the sum-
squared-error was reduced by 44%. This is likely due in large part to the im-
proved predictions at the ends of strokes, where the error is largest.
5.3 Unbiased Sampling
Given c, an unbiased sample can be picked from Pr( xjc) by iteratively drawing
xt+1from Pr (xt+1jyt), just as for the prediction network. The only dierence is
that we must also decide when the synthesis network has nished writing the text
and should stop making any future decisions. To do this, we use the following
heuristic: as soon as (t;U+ 1)>(t;u)81uUthe current input xtis
dened as the end of the sequence and sampling ends. Examples of unbiased
synthesis samples are shown in Fig. 15. These and all subsequent gures were
generated using the synthesis network retrained with adaptive weight noise.
Notice how stylistic traits, such as character size, slant, cursiveness etc. vary
2This was an oversight; however it led to the interesting result that when the text contains
a non-letter, the network must select a digits or punctuation mark to generate. Sometimes
the character can be be inferred from the context (e.g. the apostrophe in can't); otherwise
it is chosen at random.
31widely between the samples, but remain more-or-less consistent within them.
This suggests that the network identies the traits early on in the sequence,
then remembers them until the end. By looking through enough samples for a
given text, it appears to be possible to nd virtually any combination of stylistic
traits, which suggests that the network models them independently both from
each other and from the text.
Blind taste tests' carried out by the author during presentations suggest
that at least some unbiased samples cannot be distinguished from real hand-
writing by the human eye. Nonetheless the network does make mistakes we
would not expect a human writer to make, often involving missing, confused
or garbled letters3; this suggests that the network sometimes has trouble de-
termining the alignment between the characters and the trace. The number of
mistakes increases markedly when less common words or phrases are included
in the character sequence. Presumably this is because the network learns an
implicit character-level language model from the training set that gets confused
when rare or unknown transitions occur.
5.4 Biased Sampling
One problem with unbiased samples is that they tend to be dicult to read
(partly because real handwriting is dicult to read, and partly because the
network is an imperfect model). Intuitively, we would expect the network to
give higher probability to good handwriting because it tends to be smoother
and more predictable than bad handwriting. If this is true, we should aim to
output more probable elements of Pr( xjc) if we want the samples to be easier to
read. A principled search for high probability samples could lead to a dicult
inference problem, as the probability of every output depends on all previous
outputs. However a simple heuristic, where the sampler is biased towards more
probable predictions at each step independently, generally gives good results.
Dene the probability bias bas a real number greater than or equal to zero.
Before drawing a sample from Pr( xt+1jyt), each standard deviation j
tin the
Gaussian mixture is recalculated from Eq. (21) to
j
t= exp
^j
tb
(61)
and each mixture weight is recalculated from Eq. (19) to
j
t=exp
^j
t(1 +b)
PM
j0=1exp
^j0
t(1 +b) (62)
This articially reduces the variance in both the choice of component from the
mixture, and in the distribution of the component itself. When b= 0 unbiased
sampling is recovered, and as b!1 the variance in the sampling disappears
3We expect humans to make mistakes like misspelling temperament' as temperement', as
the second writer in Fig. 15 seems to have done.
32Figure 15: Real and generated handwriting . The top line in each block is
real, the rest are unbiased samples from the synthesis network. The two texts
are from the validation set and were not seen during training.
33and the network always outputs the mode of the most probable component in
the mixture (which is not necessarily the mode of the mixture, but at least a
reasonable approximation). Fig. 16 shows the eect of progressively increasing
the bias, and Fig. 17 shows samples generated with a low bias for the same texts
as Fig. 15.
5.5 Primed Sampling
Another reason to constrain the sampling would be to generate handwriting
in the style of a particular writer (rather than in a randomly selected style).
The easiest way to do this would be to retrain it on that writer only. But
even without retraining, it is possible to mimic a particular style by priming'
the network with a real sequence, then generating an extension with the real
sequence still in the network's memory. This can be achieved for a real x,cand
a synthesis character string sby setting the character sequence to c0=c+s
and clamping the data inputs to xfor the rst Ttimesteps, then sampling
as usual until the sequence ends. Examples of primed samples are shown in
Figs. 18 and 19. The fact that priming works proves that the network is able to
remember stylistic features identied earlier on in the sequence. This technique
appears to work better for sequences in the training data than those the network
has never seen.
Primed sampling and reduced variance sampling can also be combined. As
shown in Figs. 20 and 21 this tends to produce samples in a cleaned up' version
of the priming style, with overall stylistic traits such as slant and cursiveness
retained, but the strokes appearing smoother and more regular. A possible
application would be the articial enhancement of poor handwriting.
6 Conclusions and Future Work
This paper has demonstrated the ability of Long Short-Term Memory recur-
rent neural networks to generate both discrete and real-valued sequences with
complex, long-range structure using next-step prediction. It has also introduced
a novel convolutional mechanism that allows a recurrent network to condition
its predictions on an auxiliary annotation sequence, and used this approach to
synthesise diverse and realistic samples of online handwriting. Furthermore, it
has shown how these samples can be biased towards greater legibility, and how
they can be modelled on the style of a particular writer.
Several directions for future work suggest themselves. One is the applica-
tion of the network to speech synthesis, which is likely to be more challenging
than handwriting synthesis due to the greater dimensionality of the data points.
Another is to gain a better insight into the internal representation of the data,
and to use this to manipulate the sample distribution directly. It would also
be interesting to develop a mechanism to automatically extract high-level an-
notations from sequence data. In the case of handwriting, this could allow for
34Figure 16: Samples biased towards higher probability. The probability
biasesbare shown at the left. As the bias increases the diversity decreases and
the samples tend towards a kind of average handwriting' which is extremely
regular and easy to read (easier, in fact, than most of the real handwriting in the
training set). Note that even when the variance disappears, the same letter is
not written the same way at dierent points in a sequence (for examples the e's
in exactly the same, the l's in until they all look), because the predictions
are still in
uenced by the previous outputs. If you look closely you can see that
the last three lines are not quite exactly the same.
35Figure 17: A slight bias. The top line in each block is real. The rest are
samples from the synthesis network with a probability bias of 0.15, which seems
to give a good balance between diversity and legibility.
36Figure 18: Samples primed with real sequences. The priming sequences
(drawn from the training set) are shown at the top of each block. None of the
lines in the sampled text exist in the training set. The samples were selected
for legibility.
37Figure 19: Samples primed with real sequences (cotd).
38Figure 20: Samples primed with real sequences and biased towards
higher probability. The priming sequences are at the top of the blocks. The
probability bias was 1. None of the lines in the sampled text exist in the training
set.
39Figure 21: Samples primed with real sequences and biased towards
higher probability (cotd)
40more nuanced annotations than just text, for example stylistic features, dierent
forms of the same letter, information about stroke order and so on.
Acknowledgements
Thanks to Yichuan Tang, Ilya Sutskever, Navdeep Jaitly, Georey Hinton and
other colleagues at the University of Toronto for numerous useful comments
and suggestions. This work was supported by a Global Scholarship from the
Canadian Institute for Advanced Research.
References
[1] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies
with gradient descent is dicult. IEEE Transactions on Neural Networks ,
5(2):157{166, March 1994.
[2] C. Bishop. Mixture density networks. Technical report, 1994.
[3] C. Bishop. Neural Networks for Pattern Recognition . Oxford University
Press, Inc., 1995.
[4] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Modeling tempo-
ral dependencies in high-dimensional sequences: Application to polyphonic
music generation and transcription. In Proceedings of the Twenty-nine In-
ternational Conference on Machine Learning (ICML'12) , 2012.
[5] J. G. Cleary, Ian, and I. H. Witten. Data compression using adaptive cod-
ing and partial string matching. IEEE Transactions on Communications ,
32:396{402, 1984.
[6] D. Eck and J. Schmidhuber. A rst look at music composition using lstm
recurrent neural networks. Technical report, IDSIA USI-SUPSI Instituto
Dalle Molle.
[7] F. Gers, N. Schraudolph, and J. Schmidhuber. Learning precise timing
with LSTM recurrent networks. Journal of Machine Learning Research ,
3:115{143, 2002.
[8] A. Graves. Practical variational inference for neural networks. In Advances
in Neural Information Processing Systems , volume 24, pages 2348{2356.
2011.
[9] A. Graves. Sequence transduction with recurrent neural networks. In ICML
Representation Learning Worksop , 2012.
[10] A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep
recurrent neural networks. In Proc. ICASSP , 2013.
41[11] A. Graves and J. Schmidhuber. Framewise phoneme classication with bidi-
rectional LSTM and other neural network architectures. Neural Networks ,
18:602{610, 2005.
[12] A. Graves and J. Schmidhuber. Oine handwriting recognition with multi-
dimensional recurrent neural networks. In Advances in Neural Information
Processing Systems , volume 21, 2008.
[13] P. D. Gr unwald. The Minimum Description Length Principle (Adaptive
Computation and Machine Learning) . The MIT Press, 2007.
[14] G. Hinton. A Practical Guide to Training Restricted Boltzmann Machines.
Technical report, 2010.
[15] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient Flow
in Recurrent Nets: the Diculty of Learning Long-term Dependencies.
In S. C. Kremer and J. F. Kolen, editors, A Field Guide to Dynamical
Recurrent Neural Networks . 2001.
[16] S. Hochreiter and J. Schmidhuber. Long Short-Term Memory. Neural
Computation , 9(8):1735{1780, 1997.
[17] M. Hutter. The Human Knowledge Compression Contest, 2012.
[18] K.-C. Jim, C. Giles, and B. Horne. An analysis of noise in recurrent neural
networks: convergence and generalization. Neural Networks, IEEE Trans-
actions on , 7(6):1424 {1438, 1996.
[19] S. Johansson, R. Atwell, R. Garside, and G. Leech. The tagged LOB corpus
user's manual; Norwegian Computing Centre for the Humanities, 1986.
[20] B. Knoll and N. de Freitas. A machine learning perspective on predictive
coding with paq. CoRR , abs/1108.3298, 2011.
[21] M. Liwicki and H. Bunke. IAM-OnDB - an on-line English sentence
database acquired from handwritten text on a whiteboard. In Proc. 8th
Int. Conf. on Document Analysis and Recognition , volume 2, pages 956{
961, 2005.
[22] M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. Building a large
annotated corpus of english: The penn treebank. COMPUTATIONAL
LINGUISTICS , 19(2):313{330, 1993.
[23] T. Mikolov. Statistical Language Models based on Neural Networks . PhD
thesis, Brno University of Technology, 2012.
[24] T. Mikolov, I. Sutskever, A. Deoras, H. Le, S. Kombrink, and J. Cernocky.
Subword language modeling with neural networks. Technical report, Un-
published Manuscript, 2012.
42[25] A. Mnih and G. Hinton. A Scalable Hierarchical Distributed Language
Model. In Advances in Neural Information Processing Systems , volume 21,
2008.
[26] A. Mnih and Y. W. Teh. A fast and simple algorithm for training neural
probabilistic language models. In Proceedings of the 29th International
Conference on Machine Learning , pages 1751{1758, 2012.
[27] T. N. Sainath, A. Mohamed, B. Kingsbury, and B. Ramabhadran. Low-
rank matrix factorization for deep neural network training with high-
dimensional output targets. In Proc. ICASSP , 2013.
[28] M. Schuster. Better generative models for sequential data problems: Bidi-
rectional recurrent mixture density networks. pages 589{595. The MIT
Press, 1999.
[29] I. Sutskever, G. E. Hinton, and G. W. Taylor. The recurrent temporal
restricted boltzmann machine. pages 1601{1608, 2008.
[30] I. Sutskever, J. Martens, and G. Hinton. Generating text with recurrent
neural networks. In ICML , 2011.
[31] G. W. Taylor and G. E. Hinton. Factored conditional restricted boltzmann
machines for modeling motion style. In Proc. 26th Annual International
Conference on Machine Learning , pages 1025{1032, 2009.
[32] T. Tieleman and G. Hinton. Lecture 6.5 - rmsprop: Divide the gradient by
a running average of its recent magnitude, 2012.
[33] R. Williams and D. Zipser. Gradient-based learning algorithms for recur-
rent networks and their computational complexity. In Back-propagation:
Theory, Architectures and Applications , pages 433{486. 1995.
43
  FlashAttention : Fast and Memory-Eﬃcient Exact Attention
with IO-Awareness
Tri Daoy, Daniel Y. Fuy, Stefano Ermony, Atri Rudraz, and Christopher Réy
yDepartment of Computer Science, Stanford University
zDepartment of Computer Science and Engineering, University at Buﬀalo, SUNY
{trid,danfu}@cs.stanford.edu ,ermon@stanford.edu ,atri@buffalo.edu ,
chrismre@cs.stanford.edu
June 24, 2022
Abstract
Transformers are slow and memory-hungry on long sequences, since the time and memory complexity
of self-attention are quadratic in sequence length. Approximate attention methods have attempted
to address this problem by trading oﬀ model quality to reduce the compute complexity, but often do
not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-
awareaccounting for reads and writes between levels of GPU memory. We propose FlashAttention ,
an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes
between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity
ofFlashAttention , showing that it requires fewer HBM accesses than standard attention, and is
optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding
an approximate attention algorithm that is faster than any existing approximate attention method.
FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup
on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3 speedup on
GPT-2 (seq. length 1K), and 2.4 speedup on long-range arena (seq. length 1K-4K). FlashAttention
and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models
(0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classiﬁcation) and entirely new
capabilities: the ﬁrst Transformers to achieve better-than-chance performance on the Path-X challenge
(seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).
1 Introduction
Transformer models [ 82] have emerged as the most widely used architecture in applications such as natural
language processing and image classiﬁcation. Transformers have grown larger [ 5] and deeper [ 83], but
equipping them with longer context remains diﬃcult [ 80], since the self-attention module at their heart
has time and memory complexity quadratic in sequence length. An important question is whether making
attention faster and more memory-eﬃcient can help Transformer models address their runtime and memory
challenges for long sequences.
Many approximate attention methods have aimed to reduce the compute and memory requirements of
attention. These methods range from sparse-approximation [ 51,74] to low-rank approximation [ 12,50,84],
and their combinations [ 3,9,92]. Although these methods reduce the compute requirements to linear or
near-linear in sequence length, many of them do not display wall-clock speedup against standard attention
and have not gained wide adoption. One main reason is that they focus on FLOP reduction (which may not
correlate with wall-clock speed) and tend to ignore overheads from memory access (IO).
In this paper, we argue that a missing principle is making attention algorithms IO-aware [1]that is,
carefully accounting for reads and writes to diﬀerent levels of fast and slow memory (e.g., between fast GPU
on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM [ 45], Figure 1 left). On modern
1arXiv:2205.14135v2  [cs.LG]  23 Jun 2022FlashAttentionMemory Hierarchy with
Bandwidth & Memory SizeAttention on GPT-2
FlashAttention PyTorchTime (ms)
MatmulMaskSoftmaxDropoutMatmul
Fused
KernelQ: N x d V: N X dKT: d x N
QKT: N x N
sm(Q KT)V: N x dOuter Loop
Copy Block to SRAM
CopyOuter Loop
CopyInner LoopCompute Block
on SRAM
Output to HBM
Inner LoopInner LoopOuter Loop
GPU
SRAM
GPU
HBM
Main Memory
(CPU DRAM)SRAM : 19 TB/s (20 MB)
HBM: 1.5 TB/s (40 GB)
DRAM : 12.8 GB/s
                (>1 TB)
051015Figure 1: Left: FlashAttention uses tiling to prevent materialization of the large 𝑁𝑁attention matrix
(dotted box) on (relatively) slow GPU HBM. In the outer loop (red arrows), FlashAttention loops through
blocks of the KandVmatrices and loads them to fast on-chip SRAM. In each block, FlashAttention
loops over blocks of Qmatrix (blue arrows), loading them to SRAM, and writing the output of the attention
computation back to HBM. Right:Speedup over the PyTorch implementation of attention on GPT-2.
FlashAttention does not read and write the large 𝑁𝑁attention matrix to HBM, resulting in an 7.6 
speedup on the attention computation.
GPUs, compute speed has out-paced memory speed [ 61,62,63], and most operations in Transformers are
bottlenecked by memory accesses [ 43]. IO-aware algorithms have been critical for similar memory-bound
operations, when reading and writing data can account for a large portion of the runtimesuch as database
joins [71], image processing [ 70], numerical linear algebra [ 4], and more [ 40,85]. However, common Python
interfaces to deep learning such as PyTorch and Tensorﬂow do not allow ﬁne-grained control of memory
access.
We propose FlashAttention , a new attention algorithm that computes exact attention with far fewer
memory accesses. Our main goal is to avoid reading and writing the attention matrix to and from HBM.
This requires (i) computing the softmax reduction without access to the whole input (ii) not storing the large
intermediate attention matrix for the backward pass. We apply two well-established techniques to address
these challenges. (i) We restructure the attention computation to split the input into blocks and make several
passes over input blocks, thus incrementally performing the softmax reduction (also known as tiling). (ii) We
store the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the
backward pass, which is faster than the standard approach of reading the intermediate attention matrix from
HBM. We implement FlashAttention in CUDA to achieve ﬁne-grained control over memory access and
fuse all the attention operations into one GPU kernel. Even with the increased FLOPs due to recomputation,
our algorithm both runs faster (up to 7.6x on GPT-2 [ 67], Figure 1 right) and uses less memory linear
in sequence lengththan standard attention, thanks to the massively reduced amount of HBM access.
We analyze the IO complexity [ 1] ofFlashAttention , proving that it requires 𝑂¹𝑁2𝑑2𝑀1ºHBM
accesses where 𝑑is the head dimension and 𝑀is the size of SRAM, as compared to Ω¹𝑁𝑑𝑁2ºof standard
attention. For typical values of 𝑑and𝑀,FlashAttention requires many times fewer HBM accesses
compared to standard attention (up to 9 fewer, as shown in Fig. 2). Moreover, we provide a lower bound,
showing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over
all SRAM sizes.
We also show that FlashAttention can serve as a useful primitive for realizing the potential of
approximate attention algorithms by overcoming their issues with memory access overhead. As a proof of
concept, we implement block-sparse FlashAttention , a sparse attention algorithm that is 2-4 faster than
evenFlashAttention , scaling up to sequence length of 64k. We prove that block-sparse FlashAttention
has better IO complexity than FlashAttention by a factor proportional to the sparsity ratio. We discuss
further extensions to other operations (attention on multi-GPU, kernel regression, block-sparse matrix
2multiply) in Section 5. We open-source FlashAttention to make it easier to build on this primitive.1
We empirically validate that FlashAttention speeds up model training and improves model quality by
modeling longer context. We also benchmark the runtime and memory footprint of FlashAttention and
block-sparse FlashAttention compared to prior attention implementations.
Faster Model Training. FlashAttention trains Transformer models faster in wall-clock time. We
train BERT-large (seq. length 512) 15% faster than the training speed record in MLPerf 1.1 [ 58], GPT2
(seq. length 1K) 3 faster than baseline implementations from HuggingFace [ 87] and Megatron-LM [ 77],
and long-range arena (seq. length 1K-4K) 2.4 faster than baselines.
Higher Quality Models. FlashAttention scales Transformers to longer sequences, which improves
their quality and enables new capabilities. We observe a 0.7 improvement in perplexity on GPT-2 and
6.4 points of lift from modeling longer sequences on long-document classiﬁcation [13]. FlashAttention
enables the ﬁrst Transformer that can achieve better-than-chance performance on the Path-X [ 80] challenge,
solely from using a longer sequence length (16K). Block-sparse FlashAttention enables a Transformer
to scale to even longer sequences (64K), resulting in the ﬁrst model that can achieve better-than-chance
performance on Path-256.
Benchmarking Attention. FlashAttention is up to 3faster than the standard attention implemen-
tation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512,
FlashAttention is both faster and more memory-eﬃcient than any existing attention method, whereas
for sequence length beyond 1K, some approximate attention methods (e.g., Linformer) start to become
faster. On the other hand, block-sparse FlashAttention is faster than all existing approximate attention
methods that we know of.
2 Background
We provide some background on the performance characteristics of common deep learning operations on
modern hardware (GPUs). We also describe the standard implementation of attention.
2.1 Hardware Performance
We focus here on GPUs. Performance on other hardware accelerators are similar [46, 48].
GPU Memory Hierarchy. The GPU memory hierarchy (Fig. 1 left) comprises multiple forms of
memory of diﬀerent sizes and speeds, with smaller memory being faster. As an example, the A100 GPU
has 40-80GB of high bandwidth memory (HBM) with bandwidth 1.5-2.0TB/s and 192KB of on-chip SRAM
per each of 108 streaming multiprocessors with bandwidth estimated around 19TB/s [ 44,45]. The on-chip
SRAM is an order of magnitude faster than HBM but many orders of magnitude smaller in size. As compute
has gotten faster relative to memory speed [ 61,62,63], operations are increasingly bottlenecked by memory
(HBM) accesses. Thus exploiting fast SRAM becomes more important.
Execution Model. GPUs have a massive number of threads to execute an operation (called a kernel).
Each kernel loads inputs from HBM to registers and SRAM, computes, then writes outputs to HBM.
Performance characteristics. Depending on the balance of computation and memory accesses, op-
erations can be classiﬁed as either compute-bound or memory-bound. This is commonly measured by the
arithmetic intensity [85], which is the number of arithmetic operations per byte of memory access.
1.Compute-bound: the time taken by the operation is determined by how many arithmetic operations there
are, while time accessing HBM is much smaller. Typical examples are matrix multiply with large inner
dimension, and convolution with large number of channels.
2.Memory-bound: the time taken by the operation is determined by the number of memory accesses, while
time spent in computation is much smaller. Examples include most other operations: elementwise (e.g.,
activation, dropout), and reduction (e.g., sum, softmax, batch norm, layer norm).
Kernel fusion. The most common approach to accelerate memory-bound operations is kernel fusion: if
there are multiple operations applied to the same input, the input can be loaded once from HBM, instead of
multiple times for each operation. Compilers can automatically fuse many elementwise operations [ 53,65,75].
1FlashAttention code is available at https://github.com/HazyResearch/flash-attention
3However, in the context of model training, the intermediate values still need to be written to HBM to save
for the backward pass, reducing the eﬀectiveness of naive kernel fusion.
2.2 Standard Attention Implementation
Given input sequences QKV2R𝑁𝑑where𝑁is the sequence length and 𝑑is the head dimension, we want
to compute the attention output O2R𝑁𝑑:
S=QK>2R𝑁𝑁P=softmax¹Sº2R𝑁𝑁O=PV2R𝑁𝑑
where softmax is applied row-wise.
Standard attention implementations materialize the matrices SandPto HBM, which takes 𝑂¹𝑁2ºmemory.
Often𝑁𝑑(e.g., for GPT2, 𝑁=1024and𝑑=64). We describe the standard attention implementation
in Algorithm 0. As some or most of the operations are memory-bound (e.g., softmax), the large number of
memory accesses translates to slow wall-clock time.
This problem is exacerbated by other elementwise operations applied to the attention matrix, such as
masking applied to Sor dropout applied to P. As a result, there have been many attempts to fuse several
elementwise operations, such as fusing masking with softmax [77].
In Section 3.2, we will show that the standard attention implementation performs HBM accesses quadratic
in the sequence length 𝑁. We also compare the number of FLOPs and number of HBM accesses of standard
attention and of our method ( FlashAttention ).
Algorithm 0 Standard Attention Implementation
Require: Matrices QKV2R𝑁𝑑in HBM.
1:Load QKby blocks from HBM, compute S=QK>, write Sto HBM.
2:Read Sfrom HBM, compute P=softmax¹Sº, write Pto HBM.
3:Load PandVby blocks from HBM, compute O=PV, write Oto HBM.
4:Return O.
3FlashAttention : Algorithm, Analysis, and Extensions
We show how to compute exact attention with fewer HBM reads/writes and without storing large intermediate
matrices for the backward pass. This yields an attention algorithm that is both memory eﬃcient and faster in
wall-clock time. We analyze its IO complexity, showing that our method requires much fewer HBM accesses
compared to standard attention. We further show that FlashAttention can serve as a useful primitive by
extending it to handle block-sparse attention.
We focus here on the forward pass for ease of exposition; Appendix B contains details for the backward.
3.1 An Eﬃcient Attention Algorithm With Tiling and Recomputation
Given the inputs QKV2R𝑁𝑑in HBM, we aim to compute the attention output O2R𝑁𝑑and write it to
HBM. Our goal is to reduce the amount of HBM accesses (to sub-quadratic in 𝑁).
We apply two established techniques (tiling, recomputation) to overcome the technical challenge of
computing exact attention in sub-quadratic HBM accesses. We describe this in Algorithm 1. The main idea
is that we split the inputs QKVinto blocks, load them from slow HBM to fast SRAM, then compute the
attention output with respect to those blocks. By scaling the output of each block by the right normalization
factor before adding them up, we get the correct result at the end.
Tiling. We compute attention by blocks. Softmax couples columns of K, so we decompose the large
softmax with scaling [51, 60, 66]. For numerical stability, the softmax of vector 𝑥2R𝐵is computed as:
𝑚¹𝑥º:=max
𝑖𝑥𝑖 𝑓¹𝑥º:=
𝑒𝑥1𝑚¹𝑥º 𝑒𝑥𝐵𝑚¹𝑥º
 ℓ¹𝑥º:=
𝑖𝑓¹𝑥º𝑖softmax¹𝑥º:=𝑓¹𝑥º
ℓ¹𝑥º
4For vectors 𝑥¹1º𝑥¹2º2R𝐵, we can decompose the softmax of the concatenated 𝑥=
𝑥¹1º𝑥¹2º
2R2𝐵as:
𝑚¹𝑥º=𝑚¹
𝑥¹1º𝑥¹2º
º=max¹𝑚¹𝑥¹1ºº𝑚¹𝑥¹2ººº 𝑓¹𝑥º=h
𝑒𝑚¹𝑥¹1ºº𝑚¹𝑥º𝑓¹𝑥¹1ºº𝑒𝑚¹𝑥¹2ºº𝑚¹𝑥º𝑓¹𝑥¹2ººi

ℓ¹𝑥º=ℓ¹
𝑥¹1º𝑥¹2º
º=𝑒𝑚¹𝑥¹1ºº𝑚¹𝑥ºℓ¹𝑥¹1ºº𝑒𝑚¹𝑥¹2ºº𝑚¹𝑥ºℓ¹𝑥¹2ººsoftmax¹𝑥º=𝑓¹𝑥º
ℓ¹𝑥º
Therefore if we keep track of some extra statistics ( 𝑚¹𝑥ºℓ¹𝑥º), we can compute softmax one block at a time.2
We thus split the inputs QKVinto blocks (Algorithm 1 line 3), compute the softmax values along with
extra statistics (Algorithm 1 line 10), and combine the results (Algorithm 1 line 12).
Recomputation. One of our goals is to not store 𝑂¹𝑁2ºintermediate values for the backward pass. The
backward pass typically requires the matrices SP2R𝑁𝑁to compute the gradients with respect to QKV.
However, by storing the output Oand the softmax normalization statistics ¹𝑚ℓº, we can recompute the
attention matrix SandPeasily in the backward pass from blocks of QKVin SRAM. This can be seen as a
form of selective gradient checkpointing [ 10,34]. While gradient checkpointing has been suggested to reduce
the maximum amount of memory required [ 66], all implementations (that we know oﬀ) have to trade speed
for memory. In contrast, even with more FLOPs, our recomputation speeds up the backward pass due to
reduced HBM accesses (Fig. 2). The full backward pass description is in Appendix B.
Implementation details: Kernel fusion. Tiling enables us to implement our algorithm in one
CUDA kernel, loading input from HBM, performing all the computation steps (matrix multiply, softmax,
optionally masking and dropout, matrix multiply), then write the result back to HBM (masking and dropout
in Appendix B). This avoids repeatedly reading and writing of inputs and outputs from and to HBM.
Algorithm 1 FlashAttention
Require: Matrices QKV2R𝑁𝑑in HBM, on-chip SRAM of size 𝑀.
1:Set block sizes 𝐵𝑐=𝑀
4𝑑
𝐵𝑟=min𝑀
4𝑑
𝑑.
2:Initialize O=¹0º𝑁𝑑2R𝑁𝑑ℓ=¹0º𝑁2R𝑁𝑚=¹1º𝑁2R𝑁in HBM.
3:Divide Qinto𝑇𝑟=l
𝑁
𝐵𝑟m
blocks Q1Q𝑇𝑟of size𝐵𝑟𝑑each, and divide KVin to𝑇𝑐=l
𝑁
𝐵𝑐m
blocks
K1K𝑇𝑐andV1V𝑇𝑐, of size𝐵𝑐𝑑each.
4:Divide Ointo𝑇𝑟blocks O𝑖O𝑇𝑟of size𝐵𝑟𝑑each, divide ℓinto𝑇𝑟blocksℓ𝑖ℓ𝑇𝑟of size𝐵𝑟each,
divide𝑚into𝑇𝑟blocks𝑚1𝑚𝑇𝑟of size𝐵𝑟each.
5:for1𝑗𝑇𝑐do
6:Load K𝑗V𝑗from HBM to on-chip SRAM.
7:for1𝑖𝑇𝑟do
8:Load Q𝑖O𝑖ℓ𝑖𝑚𝑖from HBM to on-chip SRAM.
9:On chip, compute S𝑖𝑗=Q𝑖K𝑇
𝑗2R𝐵𝑟𝐵𝑐.
10:On chip, compute ~𝑚𝑖𝑗=rowmax¹S𝑖𝑗º 2R𝐵𝑟,~P𝑖𝑗=exp¹S𝑖𝑗~𝑚𝑖𝑗º 2R𝐵𝑟𝐵𝑐(pointwise), ~ℓ𝑖𝑗=
rowsum¹~P𝑖𝑗º2R𝐵𝑟.
11:On chip, compute 𝑚new
𝑖=max¹𝑚𝑖~𝑚𝑖𝑗º2R𝐵𝑟,ℓnew
𝑖=𝑒𝑚𝑖𝑚new
𝑖ℓ𝑖𝑒~𝑚𝑖𝑗𝑚new
𝑖~ℓ𝑖𝑗2R𝐵𝑟.
12:Write O𝑖 diag¹ℓnew
𝑖º1¹diag¹ℓ𝑖º𝑒𝑚𝑖𝑚new
𝑖O𝑖𝑒~𝑚𝑖𝑗𝑚new
𝑖~P𝑖𝑗V𝑗ºto HBM.
13:Writeℓ𝑖 ℓnew
𝑖,𝑚𝑖 𝑚new
𝑖to HBM.
14:end for
15:end for
16:Return O.
We show FlashAttention s correctness, runtime, and memory requirement (proof in Appendix C).
Theorem 1. Algorithm 1 returns O=softmax¹QK>ºVwith𝑂¹𝑁2𝑑ºFLOPs and requires 𝑂¹𝑁ºadditional
memory beyond inputs and output.
3.2 Analysis: IO Complexity of FlashAttention
We analyze the IO complexity of FlashAttention , showing signiﬁcant reduction in HBM accesses compared
to standard attention. We also provide a lower bound, proving that no exact attention algorithm can
2This style of aggregation is called algebraic aggregation [33].
5Attention Standard FlashAttention
GFLOPs 66.6 75.2
HBM R/W (GB) 40.3 4.4
Runtime (ms) 41.7 7.3
Sparsity Speedup
% Non-Zero Blocks20 6050100150Fwd + Bwd (ms)Eﬀect of Block Size
Block Size64128 256 512Fwd Runtime (ms)
6
2HBM Accesses (GB)Dense
FlashAttention
Block-Sparse
FlashAttention246
RuntimeHBMAccesses
Figure 2: Left: Forward + backward runtime of standard attention and FlashAttention for GPT-2 medium
(seq. length 1024, head dim. 64, 16 heads, batch size 64) on A100 GPU. HBM access is the primary factor aﬀecting
runtime. Middle: Forward runtime of FlashAttention (seq. length 1024, head dim. 64, 16 heads, batch size 64) on
A100 GPU. Fewer HBM accesses result in faster runtime, up to a point. Right: The runtime (for seq. length 4K) of
block-sparse FlashAttention is faster than FlashAttention by a factor proportional to the sparsity.
asymptotically improve on HBM accesses over all SRAM sizes. Proofs are in Appendix C.
Theorem2. Let𝑁be the sequence length, 𝑑be the head dimension, and 𝑀be size of SRAM with 𝑑𝑀𝑁𝑑.
Standard attention (Algorithm 0) requires Θ¹𝑁𝑑𝑁2ºHBM accesses, while FlashAttention (Algorithm 1)
requiresΘ¹𝑁2𝑑2𝑀1ºHBM accesses.
For typical values of 𝑑(64-128) and 𝑀(around 100KB), 𝑑2is many times smaller than 𝑀, and thus
FlashAttention requires many times fewer HBM accesses than standard implementation. This leads to
both faster execution and lower memory footprint, which we validate in Section 4.3.
The main idea of the proof is that given the SRAM size of 𝑀, we can load blocks of KVof sizeΘ¹𝑀ºeach
(Algorithm 1 line 6). For each block of KandV, we iterate over all blocks of Q(Algorithm 1 line 8) to compute
the intermediate values, resulting in Θ¹𝑁𝑑𝑀1ºpasses over Q. Each pass loads Θ¹𝑁𝑑ºelements, which
amounts to Θ¹𝑁2𝑑2𝑀1ºHBM accesses. We similarly prove that the backward pass of standard attention
requiresΘ¹𝑁𝑑𝑁2ºHBM accesses while the backward pass of FlashAttention requiresΘ¹𝑁2𝑑2𝑀1º
HBM accesses (Appendix B).
We prove a lower-bound: one cannot asymptotically improve on the number of HBM accesses for all
values of𝑀(the SRAM size) when computing exact attention.
Proposition 3. Let𝑁be the sequence length, 𝑑be the head dimension, and 𝑀be size of SRAM with
𝑑𝑀𝑁𝑑. There does not exist an algorithm to compute exact attention with 𝑜¹𝑁2𝑑2𝑀1ºHBM accesses
for all𝑀in the range𝑑𝑁𝑑¼.
The proof relies on the fact that for 𝑀= Θ¹𝑁𝑑ºany algorithm must perform Ω¹𝑁2𝑑2𝑀1º= Ω¹𝑁𝑑º
HBM accesses. This type of lower bound over a subrange of 𝑀is common in the streaming algorithms
literature [ 88]. We leave proving parameterized complexity [ 27] lower bounds in terms of 𝑀as exciting future
work.
We validate that the number of HBM accesses is the main determining factor of attention run-time.
In Fig. 2 (left), we see that even though FlashAttention has higher FLOP count compared to standard
attention (due to recomputation in the backward pass), it has much fewer HBM accesses, resulting in much
faster runtime. In Fig. 2 (middle), we vary the block size 𝐵𝑐ofFlashAttention , which results in diﬀerent
amounts of HBM accesses, and measure the runtime of the forward pass. As block size increases, the number
of HBM accesses decreases (as we make fewer passes over the input), and runtime decreases. For large enough
block size (beyond 256), the runtime is then bottlenecked by other factors (e.g., arithmetic operations).
Moreover, larger block size will not ﬁt into the small SRAM size.
3.3 Extension: Block-Sparse FlashAttention
We extend FlashAttention to approximate attention: we propose block-sparse FlashAttention , whose
IO complexity is smaller than FlashAttention by a factor proportional to the sparsity.
Given inputs QKV2R𝑁𝑑and a mask matrix ~M2f01g𝑁𝑁, we want to compute:
S=QK>2R𝑁𝑁P=softmax¹S𝟙~Mº2R𝑁𝑁O=PV2R𝑁𝑑
where¹S𝟙~Mº𝑘𝑙=S𝑘𝑙if~M𝑘𝑙=1and1ifM𝑘𝑙=0. We require ~Mto have block form: for some block sizes
𝐵𝑟𝐵𝑐, for all𝑘𝑙,~M𝑘𝑙=M𝑖𝑗with𝑖=b𝑘𝐵𝑟c𝑗=b𝑙𝐵𝑐cfor some M2f01g𝑁𝐵𝑟𝑁𝐵𝑐.
6Given a predeﬁned block sparsity mask M2f01g𝑁𝐵𝑟𝑁𝐵𝑐we can easily adapt Algorithm 1 to only
compute the nonzero blocks of the attention matrix. The algorithm is identical to Algorithm 1, except we
skip zero blocks. We reproduce the algorithm description in Algorithm 5 in Appendix B.
We also analyze the IO complexity of block-sparse FlashAttention .
Proposition 4. Let𝑁be the sequence length, 𝑑be the head dimension, and 𝑀be size of SRAM with
𝑑𝑀𝑁𝑑. Block-sparse FlashAttention (Algorithm 5) requires Θ¹𝑁𝑑𝑁2𝑑2𝑀1𝑠ºHBM accesses
where𝑠is the fraction of nonzero blocks in the block-sparsity mask.
We see that applying block-sparsity yields a direct improvement by the sparsity to the larger term in the
IO complexity. For large sequence lengths 𝑁,𝑠is often set to 𝑁12[11] or𝑁1log𝑁[3,17,92], resulting
inΘ¹𝑁p
𝑁ºorΘ¹𝑁log𝑁ºIO complexity. For downstream experiments, we use the ﬁxed butterﬂy sparsity
pattern [17], which has been shown to be able to approximate arbitrary sparsity [16].
In Fig. 2 (right), we validate that as the sparsity increases, the runtime of block-sparse FlashAttention
improves proportionally. On the LRA benchmark, block-sparse FlashAttention achieves 2.8speedup,
while performing on par with standard attention (Section 4).
4 Experiments
We evaluate the impact of using FlashAttention to train Transformer models. We validate two claims
about training time and model accuracy, and report attention runtime and memory benchmarks.
Training Speed. FlashAttention outperforms the MLPerf 1.1 [ 58] speed record for BERT by 15%, and
speeds up GPT-2 up to 3 over HuggingFace [ 87] and 18over Megatron [ 77] over standard Transformers.
FlashAttention speeds up the long-range arena (LRA) benchmark 2.4 .
Quality. FlashAttention scales Transformers to longer sequences, yielding higher quality. FlashAt-
tention trains GPT-2 with context length 4K faster than Megatron trains GPT-2 with context length
1K, while achieving 0.7 better perplexity. Modeling longer sequences yields 6.4 points of lift on two long-
document classiﬁcation tasks. Finally, FlashAttention yields the ﬁrst Transformer that can achieve
better-than-random performance on the challenging Path-X task (sequence length 16K), and block-sparse
FlashAttention yields the ﬁrst sequence model that we know of that can achieve better-than-random
performance on Path-256 (sequence length 64K).
Benchmarking Attention. We measure the runtime and memory performance of FlashAttention
and block-sparse FlashAttention based on sequence length. We conﬁrm that the memory footprint
ofFlashAttention scales linearly with seq. length and is up to 3 faster than standard attention for
common seq. lengths (up to 2K). We conﬁrm that runtime of block-sparse FlashAttention scales linearly
in seq. length and is faster than all existing approximate attention baselines.
Additional experiment details are in Appendix E.
4.1 Faster Models with FlashAttention
BERT. FlashAttention yields the fastest single-node BERT training speed that we know of. We train a
BERT-large [ 22] model with FlashAttention on Wikipedia. Table 1 compares our training time to the
implementation from Nvidia that set the training speed record for MLPerf 1.1 [ 58]. Our implementation is
15% faster.
Table 1: Training time of BERT-large, starting from the same initialization provided by the MLPerf benchmark, to
reach the target accuracy of 72.0% on masked language modeling. Averaged over 10 runs on 8 A100 GPUs.
BERT Implementation Training time (minutes)
Nvidia MLPerf 1.1 [58] 20.01.5
FlashAttention (ours) 17.41.4
GPT-2. FlashAttention yieldsfastertrainingtimesforGPT-2[ 67]onthelargeOpenWebtextdataset[ 32]
than the widely used HuggingFace [ 87] and Megatron-LM [ 77] implementations. Table 2 shows up to 3 end-
to-end speedup compared to Huggingface and 1.7 speedup compared to Megatron-LM. FlashAttention
7achieves the same perplexity as the other two implementations, as we do not change the model deﬁnition.
Appendix E includes plots of the validation perplexity throughout training, conﬁrming that FlashAttention
is as numerically stable as the baselines and produces the same training / validation curves.
Table 2: GPT-2 small and medium using FlashAttention achieve up to 3speed up compared to Huggingface
implementation and up to 1.7 compared to Megatron-LM. Training time reported on 8 A100s GPUs.
Model implementations OpenWebText (ppl) Training time (speedup)
GPT-2 small - Huggingface [87] 18.2 9.5 days (1.0 )
GPT-2 small - Megatron-LM [77] 18.2 4.7 days (2.0 )
GPT-2 small - FlashAttention 18.2 2.7 days (3.5)
GPT-2 medium - Huggingface [87] 14.2 21.0 days (1.0 )
GPT-2 medium - Megatron-LM [77] 14.3 11.5 days (1.8 )
GPT-2 medium - FlashAttention 14.3 6.9 days (3.0)
Long-range Arena. We compare vanilla Transformer (with either standard implementation or FlashAt-
tention ) on the long-range arena (LRA [ 80]) benchmark. We measure accuracy, throughput, and training
time of all models. Each task has a diﬀerent sequence length varying between 1024 and 4096. We follow the
implementation and experimental setting in Tay et al. [80]and Xiong et al. [90].3Table 3 shows that FlashAt-
tention achieves up 2.4speed-up compared to standard attention. Block-sparse FlashAttention is
faster than all of the approximate attention methods that we have tested.
Table 3: The performance of standard attention, FlashAttention , block-sparse FlashAttention , and approximate
attention baselines on the Long-Range-Arena benchmarks.
Models ListOps Text Retrieval Image Pathﬁnder AvgSpeedup
Transformer 36.0 63.6 81.6 42.3 72.7 59.3 -
FlashAttention 37.6 63.9 81.4 43.5 72.7 59.8 2.4
Block-sparse FlashAttention 37.0 63.0 81.3 43.6 73.3 59.6 2.8
Linformer [84] 35.6 55.9 77.7 37.8 67.6 54.9 2.5
Linear Attention [50] 38.8 63.2 80.7 42.6 72.5 59.6 2.3
Performer [12] 36.8 63.6 82.2 42.1 69.9 58.9 1.8
Local Attention [80] 36.1 60.2 76.7 40.6 66.6 56.0 1.7
Reformer [51] 36.5 63.8 78.5 39.6 69.4 57.6 1.3
Smyrf [19] 36.1 64.1 79.0 39.6 70.5 57.9 1.7
4.2 Better Models with Longer Sequences
Language Modeling with Long Context. The runtime and memory-eﬃciency of FlashAttention
allow us to increase the context length of GPT-2 by 4 while still running faster than the optimized
implementation from Megatron-LM. Table 4 shows that that GPT-2 with FlashAttention and context
length 4K is still 30% faster than GPT-2 from Megatron with context length 1K, while achieving 0.7 better
perplexity.
Table 4: GPT-2 small with FlashAttention , with 4larger context length compared to Megatron-LM, is still 30%
faster while achieving 0.7 better perplexity. Training time on 8 A100 GPUs is reported.
Model implementations Context length OpenWebText (ppl) Training time (speedup)
GPT-2 small - Megatron-LM 1k 18.2 4.7 days (1.0 )
GPT-2 small - FlashAttention 1k 18.2 2.7 days (1.7)
GPT-2 small - FlashAttention 2k 17.6 3.0 days (1.6 )
GPT-2 small - FlashAttention 4k 17.5 3.6 days (1.3)
Long Document Classiﬁcation. Training Transformers with longer sequences with FlashAttention
improves performance on the MIMIC-III [ 47] and ECtHR [ 6,7] datasets. MIMIC-III contains intensive care
unit patient discharge summaries, each annotated with multiple labels. ECtHR contains legal cases from the
3LRA accuracy results are known to be highly dependent on the tuning procedure [ 90]. Our reproduced baselines perform
better than as reported in the original comparison [80].
8Attention Memory Usage
Sequence LengthAttention Runtime (Fwd Pass + Bwd Pass)
Sequence LengthRuntime (ms)
Memory Footprint (GB)256 8K 16K 32K 64K 128 256 512 1024 2048 4096101102
1020
FlashAttention
Block-Sparse FlashAttentionPyTorch Attention
Megatron AttentionLinformer Attention
OpenAI Sparse Attention8192100Crossover Points
20x2xFigure 3: Left:runtime of forward pass + backward pass. Right:attention memory usage.
European Court of Human Rights, each of which is mapped to articles of the Convention of Human Rights
that were allegedly violaged. Both of these datasets contain very long text documents; the average number of
tokens in MIMIC is 2,395 tokens, and the longest document contains 14,562 tokens, while the average and
longest numbers in ECtHR are 2,197 and 49,392, respectively. We evaluate lift from increasing the sequence
length of a pretrained RoBERTa model [56] (we repeat the positional embeddings, as in Beltagy et al. [3]).
Table 5 shows that sequence length 16K outperforms length 512 by 4.3 points on MIMIC, and that length
8K outperforms length 512 by 8.5 points on ECtHR. The discrepancies may be due to subtle distribution
shifts: MIMIC-III contains specialized medical text and thus may be more susceptible to a distribution shift
in the document length, whereas ECtHR contains general language.
Table 5: Long Document performance (mi-
cro𝐹1) at diﬀerent sequence lengths using
FlashAttention .
512 1024 2048 4096 8192 16384
MIMIC-III [47] 52.8 50.7 51.7 54.6 56.4 57.1
ECtHR [6] 72.2 74.3 77.1 78.6 80.779.2Table 6: We report the ﬁrst Transformer
model that can achieve non-random perfor-
mance on Path-X and Path-256.
Model Path-X Path-256
Transformer 7 7
Linformer [84] 7 7
Linear Attention [50] 7 7
Performer [12] 7 7
Local Attention [80] 7 7
Reformer [51] 7 7
SMYRF [19] 7 7
FlashAttention 61.4 7
Block-sparse FlashAttention 56.0 63.1
Path-X and Path-256. The Path-X and Path-256 benchmarks are challenging tasks from the long-range
arena benchmark designed to test long context. The task is to classify whether two points in a black and
white 128128 (or 256256) image have a path connecting them, and the images are fed to the transformer
one pixel at a time. In prior work, all transformer models have either run out of memory, or only achieved
random performance [ 80]. There has been a search for alternative architectures that can model such long
context [ 37]. We present here the ﬁrst result of Transformer models being able to solve Path-X and Path-256
(Table 6). We pretrain a transformer on Path-64, and then transfer to Path-X by spatially interpolating
the positional embeddings. FlashAttention achieves 61.4 accuracy on Path-X. Additionally, block-sparse
FlashAttention enables the Transformers to scale to sequence length 64K, achieving 63.1 accuracy4on
Path-256.
4.3 Benchmarking Attention
We vary sequence length and measure runtime and memory usage of FlashAttention and block-sparse
FlashAttention against various attention baselines on one A100 GPU with 40 GB HBM, with dropout and
a padding mask. We compare against reference implementations for exact attention, approximate attention,
and sparse attention. We report a subset of baselines in the main body; Appendix E contains more baselines
and full details.
4Path-256 requires longer sequences but has relatively shorter paths than Path-X, so it is easier to obtain a higher accuracy.
9Runtime. Figure 3 (left) reports the runtime in milliseconds of the forward + backward pass of FlashAt-
tention and block-sparse FlashAttention compared to the baselines in exact, approximate, and sparse
attention (exact numbers in Appendix E). Runtime grows quadratically with sequence length, but FlashAt-
tention runs signiﬁcantly faster than exact attention baselines, up to 3 faster than the PyTorch
implementation. The runtimes of many approximate/sparse attention mechanisms grow linearly with se-
quence length, but FlashAttention still runs faster than approximate and sparse attention for short
sequences due to fewer memory accesses. The approximate attention runtimes begin to cross over with
FlashAttention at sequences between 512 and 1024. On the other hand, block-sparse FlashAttention
is faster than all implementations of exact, sparse, and approximate attention that we know of, across all
sequence lengths.
Memory Footprint. Figure 3 (right) shows the memory footprint of FlashAttention and block-sparse
FlashAttention compared to various exact, approximate, and sparse attention baselines. FlashAttention
and block-sparse FlashAttention have the same memory footprint, which grows linearly with sequence
length. FlashAttention is up to 20more memory eﬃcient than exact attention baselines, and is more
memory-eﬃcient than the approximate attention baselines. All other algorithms except for Linformer run
out of memory on an A100 GPU before 64K, and FlashAttention is still 2more eﬃcient than Linformer.
5 Limitations and Future Directions
We discuss limitations of our approach and future directions. Related work is given in Appendix A.
Compiling to CUDA. Our current approach to building IO-aware implementations of attention requires
writing a new CUDA kernel for each new attention implementation. This requires writing the attention
algorithm in a considerably lower-level language than PyTorch, and requires signiﬁcant engineering eﬀort.
Implementations may also not be transferrable across GPU architectures. These limitations suggest the
need for a method that supports writing attention algorithms in a high-level language (e.g., PyTorch), and
compiling to IO-aware implementations in CUDAsimilar to eﬀorts such as Halide in image processing [ 70].
IO-Aware Deep Learning. We believe that the IO-aware approach can extend beyond attention.
Attention is the most memory-intensive computation in Transformers, but every layer in a deep network
touches GPU HBM. We hope our work inspires IO-aware implementations of additional modules. We discuss
these potential extensions in Appendix D.
Multi-GPU IO-Aware Methods. Our IO-aware implementation of attention is optimal within con-
stants for computing attention on a single GPU. However, the attention computation may be parallelizable
across multiple GPUs [ 72]. Using multiple GPUs adds an additional layer to IO analysisaccounting for
data transfer between GPUs. We hope our work inspires future work in this direction.
Acknowledgments
Our implementation uses Apexs FMHA code ( https://github.com/NVIDIA/apex/tree/master/apex/
contrib/csrc/fmha ) as a starting point. We thank Young-Jun Ko for the in-depth explanation of his FMHA
implementation and for his thoughtful answers to our questions about CUDA. We thank Sabri Eyuboglu,
Megan Leszczynski, Laurel Orr, Yuhuai Wu, Beidi Chen, and Xun Huang for their constructive feedback and
suggestions on early drafts of the paper. We thank Markus Rabe and Charles Staats for helpful discussion of
their attention algorithm.
We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos.
CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ARL under
No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266 (Unifying Weak
Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine
Learning; N000142012275 (NEPTUNE); NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba,
TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce,
Total, the HAI-GCP & HAI-Azure Cloud Credits for Research program, the Stanford Data Science Initiative
(SDSI), Department of Defense (DoD) through the National Defense Science and Engineering Graduate
Fellowship (NDSEG) Program, and members of the Stanford DAWN project: Facebook, Google, and
VMWare. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes
10notwithstanding any copyright notation thereon. Any opinions, ﬁndings, and conclusions or recommendations
expressed in this material are those of the authors and do not necessarily reﬂect the views, policies, or
endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government. Atri Rudras research is
supported by NSF grant CCF-1763481.
References
[1]Alok Aggarwal and S Vitter, Jeﬀrey. The input/output complexity of sorting and related problems.
Communications of the ACM , 31(9):11161127, 1988.
[2]Irwan Bello. LambdaNetworks: Modeling long-range interactions without attention. arXiv preprint
arXiv:2102.08602 , 2021.
[3]Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv
preprint arXiv:2004.05150 , 2020.
[4]L Susan Blackford, Antoine Petitet, Roldan Pozo, Karin Remington, R Clint Whaley, James Demmel,
Jack Dongarra, Iain Duﬀ, Sven Hammarling, Greg Henry, et al. An updated set of basic linear algebra
subprograms (blas). ACM Transactions on Mathematical Software , 28(2):135151, 2002.
[5]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:18771901, 2020.
[6]Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras. Neural legal judgment prediction in English.
InProceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages
43174323, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1424.
URL https://www.aclweb.org/anthology/P19-1424 .
[7]Ilias Chalkidis, Manos Fergadiotis, Dimitrios Tsarapatsanis, Nikolaos Aletras, Ion Androutsopoulos, and
Prodromos Malakasiotis. Paragraph-level rationale extraction through regularization: A case study on
european court of human rights cases. In Proceedings of the Annual Conference of the North American
Chapter of the Association for Computational Linguistics , Mexico City, Mexico, 2021. Association for
Computational Linguistics.
[8]Benjamin Charlier, Jean Feydy, Joan Alexis Glaunès, François-David Collin, and Ghislain Durif. Kernel
operations on the gpu, with autodiﬀ, without memory overﬂows. Journal of Machine Learning Research ,
22(74):16, 2021. URL http://jmlr.org/papers/v22/20-275.html .
[9]Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Ré. Scatterbrain: Unifying
sparse and low-rank attention. In Advances in Neural Information Processing Systems (NeurIPS) , 2021.
[10]Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory
cost.arXiv preprint arXiv:1604.06174 , 2016.
[11]Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509 , 2019.
[12]Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,
Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking
attention with performers. In International Conference on Learning Representations (ICLR) , 2020.
[13]Xiang Dai, Ilias Chalkidis, Sune Darkner, and Desmond Elliott. Revisiting transformer-based models for
long document classiﬁcation. arXiv preprint arXiv:2204.06683 , 2022.
[14]Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov.
Transformer-XL: Attentive language models beyond a ﬁxed-length context. In Proceedings of the 57th
Annual Meeting of the Association for Computational Linguistics , pages 29782988, 2019.
11[15]Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher Ré. Learning fast algorithms
for linear transforms using butterﬂy factorizations. In International Conference on Machine Learning
(ICML), 2019.
[16]Tri Dao, Nimit Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra,
and Christopher Ré. Kaleidoscope: An eﬃcient, learnable representation for all structured linear maps.
InInternational Conference on Learning Representations (ICLR) , 2020.
[17]Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Ré.
Pixelated butterﬂy: Simple and eﬃcient sparse training for neural network models. In International
Conference on Learning Representations (ICLR) , 2022.
[18]Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh
Rao, Atri Rudra, and Christopher Ré. Monarch: Expressive structured matrices for eﬃcient and accurate
training. In International Conference on Machine Learning (ICML) , 2022.
[19]Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros G Dimakis. Smyrf-eﬃcient attention
using asymmetric clustering. Advances in Neural Information Processing Systems , 33:64766489, 2020.
[20]Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher Ré, and Atri Rudra. A two-pronged
progress in structured dense matrix vector multiplication. In Proceedings of the Twenty-Ninth Annual
ACM-SIAM Symposium on Discrete Algorithms , pages 10601079. SIAM, 2018.
[21]Peter J Denning. The working set model for program behavior. Communications of the ACM , 11(5):
323333, 1968.
[22]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. 2019.
[23]Xin Dong, Shangyu Chen, and Sinno Jialin Pan. Learning to prune deep neural networks via layer-wise
optimal brain surgeon. arXiv preprint arXiv:1705.07565 , 2017.
[24]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is
worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning
Representations , 2020.
[25]Y Eidelman and I Gohberg. On a new class of structured matrices. Integral Equations and Operator
Theory, 34(3):293324, 1999.
[26]Jean Feydy, Joan Glaunès, Benjamin Charlier, and Michael Bronstein. Fast geometric learning with
symbolic matrices. Advances in Neural Information Processing Systems , 33, 2020.
[27] Jörg Flum and Martin Grohe. Parameterized Complexity Theory . Springer, 2006.
[28]Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In International Conference on Learning Representations , 2018.
[29]Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Stabilizing the
lottery ticket hypothesis. arXiv preprint arXiv:1903.01611 , 2019.
[30]Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity
and the lottery ticket hypothesis. In International Conference on Machine Learning , pages 32593269.
PMLR, 2020.
[31]Karan Goel, Albert Gu, Chris Donahue, and Christopher Ré. Its raw! audio generation with state-space
models. In International Conference on Machine Learning (ICML) , 2022.
[32] Aaron Gokaslan, Vanya Cohen, Pavlick Ellie, and Stefanie Tellex. Openwebtext corpus, 2019.
12[33]Jim Gray, Surajit Chaudhuri, Adam Bosworth, Andrew Layman, Don Reichart, Murali Venkatrao,
Frank Pellow, and Hamid Pirahesh. Data cube: A relational aggregation operator generalizing group-by,
cross-tab, and sub-totals. Data mining and knowledge discovery , 1(1):2953, 1997.
[34]Andreas Griewank and Andrea Walther. Evaluating derivatives: principles and techniques of algorithmic
diﬀerentiation . SIAM, 2008.
[35]Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. Hippo: Recurrent memory with
optimal polynomial projections. In Advances in neural information processing systems (NeurIPS) , 2020.
[36]Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré. Combining
recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural
Information Processing Systems , 34, 2021.
[37]Albert Gu, Karan Goel, and Christopher Ré. Eﬃciently modeling long sequences with structured state
spaces. In The International Conference on Learning Representations (ICLR) , 2022.
[38]Song Han, Jeﬀ Pool, John Tran, and William J Dally. Learning both weights and connections for eﬃcient
neural networks. arXiv preprint arXiv:1506.02626 , 2015.
[39]Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huﬀman coding. In International Conference on Learning
Representations , 2016.
[40]John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative
Approach , pages 390525, 2003.
[41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489 , 2020.
[42]Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv
preprint arXiv:2202.10447 , 2022.
[43]Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoeﬂer. Data movement is all
you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems , 3:
711732, 2021.
[44]Zhe Jia and Peter Van Sandt. Dissecting the Ampere GPU architecture via microbenchmarking. GPU
Technology Conference, 2021.
[45]Zhe Jia, Marco Maggioni, Benjamin Staiger, and Daniele P Scarpazza. Dissecting the nvidia Volta GPU
architecture via microbenchmarking. arXiv preprint arXiv:1804.06826 , 2018.
[46]Zhe Jia, Blake Tillman, Marco Maggioni, and Daniele Paolo Scarpazza. Dissecting the graphcore IPU
architecture via microbenchmarking. arXiv preprint arXiv:1912.03413 , 2019.
[47]Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi,
Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible
critical care database. Scientiﬁc data , 3(1):19, 2016.
[48]Norman P Jouppi, Cliﬀ Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah
Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of a tensor
processing unit. In Proceedings of the 44th annual international symposium on computer architecture ,
pages 112, 2017.
[49]Thomas Kailath, Sun-Yuan Kung, and Martin Morf. Displacement ranks of matrices and linear equations.
Journal of Mathematical Analysis and Applications , 68(2):395407, 1979.
[50]Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are RNNs:
Fast autoregressive transformers with linear attention. In International Conference on Machine Learning ,
pages 51565165. PMLR, 2020.
13[51]Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The eﬃcient transformer. In The
International Conference on Machine Learning (ICML) , 2020.
[52]Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.
Albert: A lite BEDRT for self-supervised learning of language representations. In The International
Conference on Learning Representations (ICLR) , 2020.
[53]Mingzhen Li, Yi Liu, Xiaoyan Liu, Qingxiao Sun, Xin You, Hailong Yang, Zhongzhi Luan, Lin Gan,
Guangwen Yang, and Depei Qian. The deep learning compiler: A comprehensive survey. IEEE
Transactions on Parallel and Distributed Systems , 32(3):708727, 2020.
[54]Valerii Likhosherstov, Krzysztof Choromanski, Jared Davis, Xingyou Song, and Adrian Weller. Sub-linear
memory: How to make performers slim. arXiv preprint arXiv:2012.11346 , 2020.
[55]Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural
Information Processing Systems , volume 30. Curran Associates, Inc., 2017.
[56]Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach.
arXiv preprint arXiv:1907.11692 , 2019.
[57]Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer.
Luna: Linear uniﬁed nested attention. Advances in Neural Information Processing Systems , 34, 2021.
[58]Peter Mattson, Christine Cheng, Gregory Diamos, Cody Coleman, Paulius Micikevicius, David Patterson,
Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor Bittorf, et al. Mlperf training benchmark. Proceedings
of Machine Learning and Systems , 2:336349, 2020.
[59]Frank McSherry, Michael Isard, and Derek G Murray. Scalability! but at what fCOSTg? In15th
Workshop on Hot Topics in Operating Systems (HotOS XV) , 2015.
[60]Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint
arXiv:1805.02867 , 2018.
[61] NVIDIA. Nvidia Tesla V100 GPU architecture, 2017.
[62] NVIDIA. Nvidia A100 tensor core GPU architecture, 2020.
[63] NVIDIA. Nvidia H100 tensor core GPU architecture, 2022.
[64]D Stott Parker. Random butterﬂy transformations with applications in computational linear algebra.
1995.
[65]Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. Advances in neural information processing systems , 32, 2019.
[66]Markus N Rabe and Charles Staats. Self-attention does not need 𝑂¹𝑛2ºmemory. arXiv preprint
arXiv:2112.05682 , 2021.
[67]Alec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
[68]Jack Rae and Ali Razavi. Do transformers need deep long-range memory? In Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics , Online, July 2020. Association for
Computational Linguistics. URL https://www.aclweb.org/anthology/2020.acl-main.672 .
[69]Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive trans-
formers for long-range sequence modelling. In The International Conference on Learning Representations
(ICLR), 2020.
14[70]Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo Durand, and Saman
Amarasinghe. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in
image processing pipelines. Acm Sigplan Notices , 48(6):519530, 2013.
[71]Raghu Ramakrishnan, Johannes Gehrke, and Johannes Gehrke. Database management systems , volume 3.
McGraw-Hill New York, 2003.
[72]Benjamin Recht and Christopher Ré. Parallel stochastic gradient algorithms for large-scale matrix
completion. Mathematical Programming Computation , 5(2):201226, 2013.
[73]Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai.
Combiner: Full attention transformer with sparse computation cost. Advances in Neural Information
Processing Systems , 34, 2021.
[74]Aurko Roy, Mohammad Saﬀar, Ashish Vaswani, and David Grangier. Eﬃcient content-based sparse
attention with routing transformers. Transactions of the Association for Computational Linguistics , 9:
5368, 2021.
[75] Amit Sabne. XLA: Compiling machine learning for peak performance. 2020.
[76]Victor Sanh, Thomas Wolf, and Alexander M Rush. Movement pruning: Adaptive sparsity by ﬁne-tuning.
arXiv preprint arXiv:2005.07683 , 2020.
[77]Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.
Megatron-LM: Training multi-billion parameter language models using model parallelism. arXiv preprint
arXiv:1909.08053 , 2019.
[78]Vikas Sindhwani, Tara Sainath, and Sanjiv Kumar. Structured transforms for small-footprint deep
learning. In Advances in Neural Information Processing Systems , pages 30883096, 2015.
[79]Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span
in transformers. In Proceedings of the Annual Meeting of the Association for Computational Linguistics ,
2019.
[80]Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu
Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for eﬃcient transformers.
InInternational Conference on Learning Representations , 2020.
[81]Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Eﬃcient transformers: A survey. arXiv
preprint arXiv:2009.06732 , 2020.
[82]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing
systems, 30, 2017.
[83]Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet:
Scaling transformers to 1,000 layers. arXiv preprint arXiv:2203.00555 , 2022.
[84]Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768 , 2020.
[85]Samuel Williams, Andrew Waterman, and David Patterson. Rooﬂine: an insightful visual performance
model for multicore architectures. Communications of the ACM , 52(4):6576, 2009.
[86]Michael E Wolf and Monica S Lam. A data locality optimizing algorithm. In Proceedings of the ACM
SIGPLAN 1991 conference on Programming language design and implementation , pages 3044, 1991.
15[87]Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric
Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing.
InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System
Demonstrations , pages 3845, Online, October 2020. Association for Computational Linguistics. URL
https://www.aclweb.org/anthology/2020.emnlp-demos.6 .
[88]David P Woodruﬀ. Optimal space lower bounds for all frequency moments. In SODA, volume 4, pages
167175. Citeseer, 2004.
[89]Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention with
lightweight and dynamic convolutions. In The International Conference on Learning Representations
(ICLR), 2019.
[90]Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas
Singh. Nyströmformer: A nystöm-based algorithm for approximating self-attention. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence. AAAI Conference on Artiﬁcial Intelligence , volume 35,
page 14138, 2021.
[91]Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng,
and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In
Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 558567, 2021.
[92]Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago
Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer
sequences. Advances in Neural Information Processing Systems , 33, 2020.
[93]Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh
Susskind. An attention free transformer. arXiv preprint arXiv:2105.14103 , 2021.
[94]Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and
Bryan Catanzaro. Long-short transformer: Eﬃcient transformers for language and vision. Advances in
Neural Information Processing Systems , 34, 2021.
16A Related Work
IO-Aware Runtime Optimization. The broad concept of optimizing for reading and writing to fast/slow
memory has a long history in computer science and has been known by many names. We draw the most
direct connection to the literature of analyzing I/O complexity in this work [ 1], but concepts of memory
hierarchies are fundamental and has appeared in many forms, from the working set model [ 21], to data
locality [ 86], to the Rooﬂine model of arithmetic intensity [ 85], to analyses of scalability [ 59], to standard
textbook treatments of computer architecture [ 40]. We hope that this work encourages the community to
adopt these ideas in more parts of the deep learning stack.
Eﬃcient ML Models with Structured Matrices. Matrix multiply is the core computational bottle-
neck of most machine learning models. To reduce the computational complexity, there have been numerous
approaches to learn over a more eﬃcient set of matrices. These matrices are called structured matrices , which
have subquadratic ( 𝑜¹𝑛2ºfor dimension 𝑛𝑛) number of parameters and runtime. Most common examples
of structured matrices are sparse and low-rank matrices, along with fast transforms commonly encountered
in signal processing (Fourier, Chebyshev, sine/cosine, orthogonal polynomials). There have been several
more general classes of structured matrices proposed in machine learning: Toeplitz-like [ 78], low-displacement
rank [49], quasi-separable [ 25]). The butterﬂy pattern we use for our block-sparse attention is motivated
by the fact that butterﬂy matrices [ 15,64] and their products have been shown to be able to express any
structured matrices with almost optimal runtime and number of parameters [ 16,20]. However, even though
structured matrices are eﬃcient in theory, they have not seen wide adoption since it is hard to translate their
eﬃciency to wall-clock speedup since dense unconstrained matrix multiply has very optimize implementation,
a phenomenon known as the hardware lottery [ 41]. Extensions of butterﬂy matrices [ 17,18] aimed to make
butterﬂy matrices more hardware-friendly.
Sparse Training. Our block-sparse FlashAttention can be seen as a step towards making sparse model
training more eﬃcient. Sparse models have seen success in compressing models for inference (pruning) by
sparsifyingtheweightmatrices[ 23,38,39,55,76]. Formodeltraining, thelotteryticketshypothesis[ 28,29,30]
suggests that there are a set of small sub-networks derived from a larger dense network that performs as
well as the original dense network. Out block-sparse FlashAttention can also be seen as a ﬁxed lottery
ticket in the context of attention: we ﬁx the sparsity pattern to be the butterﬂy pattern through training,
and observe that it performs almost as well as the (dense) FlashAttention on the Long-range Arena tasks.
Eﬃcient Transformer. Transformer-based models have become the most widely-used architecture in
natural language processing [ 22] and computer vision [ 24,91]. However, one of their computational bottlenecks
is that their time and memory scales quadratic in the sequence length. There are numerous approaches to
overcome this bottleneck, including approximation with hashing (i.e., sparse) such as Reformer [ 51] and
Smyrf [19] and with low-rank approximation such as Performer [ 12,54]. One can even combine sparse and
low-rank approximation for better accuracy (e.g., Longformer [ 3], BigBird [ 92], Scatterbrain [ 9], Long-short
transformer [ 94], Combiner [ 73]). Other approaches include compressing along the sequence dimension to
attend to multiple tokens at once [ 52,57,79,89]. One can also attend over the states from previous sequences
to help lengthen the context (e.g., Transformer-XL [ 14] and Compressive Transformer [ 69]). We recommend
the survey [81] for more details.
There are several lines of work on developing other modules instead of attention to model longer context.
HiPPO [ 35] and its extensions, most notably S4 [ 31,36,37] projects the history on a polynomial basis,
allowing accurate reconstruction of the history through state-space models. They combine the strengths of
CNNs (eﬃcient training), RNNs (eﬃcient inference), and continuous models (robust to change in sampling
rates). LambdaNetworks [ 2], AFT [ 93] and FLASH [ 42] are other attempts at replacing attention in the
context of image classiﬁcation and language modeling.
B Algorithm Details
We ﬁrst derive the forward and backward passes of attention and show that they can be computed in a
memory-eﬃcient manner (requiring extra memory linear instead of quadratic in the sequence length). Though
they reduce the amount of extra memory required, naively they still incur quadratic HBM accesses, resulting
in slower execution speed. We describe the FlashAttention algorithm to implement both the forward
17and the backward passes on GPUs that reduces HBM accesses, leading to both faster runtime and smaller
memory footprint.
B.1 Memory-eﬃcient forward pass
The main challenge in making attention memory-eﬃcient is the softmax that couples the columns of K(and
columns of V). Our approach is to compute the softmax normalization constant separately to decouple the
columns. This technique [ 60] has been used in the literature [ 51,66] to show that attention computation
does not need quadratic extramemory (though the number of HBM accesses is still quadratic, resulting in
slow run-time).
For simplicity, we omit here the max-shifting step during softmax. The full algorithm in Appendix B.3
contains all the steps.
Recall that given input sequences QKV2R𝑁𝑑, we want to compute the attention output O2R𝑁𝑑:
S=QK>2R𝑁𝑁P=softmax¹Sº2R𝑁𝑁O=PV2R𝑁𝑑
We have that 𝑆𝑖𝑗=𝑞𝑇
𝑖𝑘𝑗where𝑞𝑖and𝑘𝑗are the𝑖-th and𝑗-th columns of QandKrespectively. Deﬁne
the normalization constants of softmax:
𝐿𝑖=
𝑗𝑒𝑞𝑇
𝑖𝑘𝑗 (1)
Let𝑣𝑗be the𝑗-th column of V, then the𝑖-th columns of the output is
𝑜𝑖=𝑃𝑖:V=
𝑗𝑃𝑖𝑗𝑣𝑗=
𝑗𝑒𝑞𝑇
𝑖𝑘𝑗
𝐿𝑖𝑣𝑗 (2)
We see that once 𝐿𝑖is computed, we can compute 𝑜𝑖without extra memory by repeatedly summing
𝑒𝑞𝑇
𝑖𝑘𝑗
𝐿𝑖𝑣𝑗. Therefore the forward pass can be computed with 𝑂¹𝑛ºextra memory:
1. Compute 𝐿𝑖for all𝑖according to Eq. (1), which takes 𝑂¹𝑛ºextra memory.
2. Compute 𝑜𝑖for all𝑖according to Eq. (2), which takes 𝑂¹𝑑ºextra memory.
B.2 Memory-eﬃcient backward pass
We derive the backward pass of attention and show that it can also be computed with linear memory. Rabe
and Staats [66]suggests that the backward pass can be done without quadratic extra memory by applying
gradient checkpointing to the memory-eﬃcient forward pass. We instead derive the backward pass explicitly
and show how it can be computed in a memory-eﬃcient manner.
Suppose that there is a scalar loss function 𝜙, and let the output gradient be dO2R𝑛𝑑(where dOdenotes
𝜙
O). We want to compute the input gradients dQdKdV2R𝑛𝑑(where dQdKdVdenote𝜙
Q𝜙
K𝜙
V
respectively).
The gradient dVis easy to see. Applying reverse-mode autodiﬀ by hand (aka the chain rule), we obtain
(in matrix notation) dV=P𝑇dO. Thus:
𝑑𝑣𝑗=
𝑖𝑃𝑖𝑗𝑑𝑜𝑖=
𝑖𝑒𝑞𝑇
𝑖𝑘𝑗
𝐿𝑖𝑑𝑜𝑖 (3)
Since we already computed 𝐿𝑖,𝑑𝑣𝑗can be computed without extra memory by repeated summing.
The gradients dQanddKare a little more complicated. We go through the gradients dPanddSﬁrst.
From Eq. (2), we have that dP=dOV𝑇, and so:
𝑑𝑃𝑖𝑗=𝑑𝑜𝑇
𝑖𝑣𝑗
Recall that 𝑃𝑖:=softmax¹𝑆𝑖:º. Using the fact that the Jacobian of 𝑦=softmax¹𝑥ºisdiag¹𝑦º𝑦𝑦𝑇, we
have that
𝑑𝑆𝑖:=¹diag¹𝑃𝑖:º𝑃𝑖:𝑃𝑇
𝑖:º𝑑𝑃𝑖:=𝑃𝑖:𝑑𝑃𝑖:¹𝑃𝑇
𝑖:𝑑𝑃𝑖:º𝑃𝑖:
18wheredenotes pointwise multiplication.
Deﬁne
𝐷𝑖=𝑃𝑇
𝑖:𝑑𝑃𝑖:=
𝑗𝑒𝑞𝑇
𝑖𝑘𝑗
𝐿𝑖𝑑𝑜𝑇
𝑖𝑣𝑗=𝑑𝑜𝑇
𝑖
𝑗𝑒𝑞>
𝑖𝑘𝑗
𝐿𝑖𝑣𝑗=𝑑𝑜𝑇
𝑖𝑜𝑖 (4)
then
𝑑𝑆𝑖:=𝑃𝑖:𝑑𝑃𝑖:𝐷𝑖𝑃𝑖:
Hence
𝑑𝑆𝑖𝑗=𝑃𝑖𝑗𝑑𝑃𝑖𝑗𝐷𝑖𝑃𝑖𝑗=𝑃𝑖𝑗¹𝑑𝑃𝑖𝑗𝐷𝑖º
Now we can get the gradients dQanddK. Recall that 𝑆𝑖𝑗=𝑞𝑇
𝑖𝑘𝑗, so
𝑑𝑞𝑖=
𝑗𝑑𝑆𝑖𝑗𝑘𝑗=
𝑗𝑃𝑖𝑗¹𝑑𝑃𝑖𝑗𝐷𝑖º𝑘𝑗=
𝑗𝑒𝑞𝑇
𝑖𝑘𝑗
𝐿𝑖¹𝑑𝑜𝑇
𝑖𝑣𝑗𝐷𝑖º𝑘𝑗 (5)
Similarly,
𝑑𝑘𝑗=
𝑖𝑑𝑆𝑖𝑗𝑞𝑖=
𝑖𝑃𝑖𝑗¹𝑑𝑃𝑖𝑗𝐷𝑖º𝑞𝑖=
𝑖𝑒𝑞𝑇
𝑖𝑘𝑗
𝐿𝑖¹𝑑𝑜𝑇
𝑖𝑣𝑗𝐷𝑖º𝑞𝑖 (6)
Therefore the backward pass can also be computed with 𝑂¹𝑛ºextra memory:
1. Compute 𝑑𝑣𝑗for all𝑗according to Eq. (3), which takes 𝑂¹𝑑ºextra memory.
2. Compute 𝐷𝑖for all𝑖according to Eq. (4), which takes 𝑂¹𝑛ºextra memory.
3. Compute 𝑑𝑞𝑖for all𝑖according to Eq. (5), which takes 𝑂¹𝑑ºextra memory.
4. Compute 𝑑𝑘𝑗for all𝑗according to Eq. (6), which takes 𝑂¹𝑑ºextra memory.
B.3 FlashAttention : Forward Pass
We describe the full details of FlashAttention forward pass. Given input sequences QKV2R𝑁𝑑, we
want to compute the attention output O2R𝑁𝑑:
S=𝜏QK>2R𝑁𝑁Smasked=mask¹𝑆º2R𝑁𝑁P=softmax¹Smaskedº2R𝑁𝑁
Pdropped=dropout¹P𝑝dropºO=PdroppedV2R𝑁𝑑
where𝜏2Ris some softmax scaling (typically1p
𝑑),maskis some masking function that sets some entries of
the input to1and keep other entries the same (e.g., key padding mask when sequences in the batch dont
have the same lengths and are padded), and dropout¹𝑥𝑝ºapplies dropout to 𝑥elementwise (i.e., output𝑥
1𝑝
with probability 1𝑝and output 0 with probability 𝑝for each element 𝑥).
The full algorithm is in Algorithm 2. We save the output O, the softmax statistics ℓand𝑚, and the
pseudo-random number generator state Rfor the backward pass.
19Algorithm 2 FlashAttention Forward Pass
Require: Matrices QKV2R𝑁𝑑in HBM, on-chip SRAM of size 𝑀, softmax scaling constant 𝜏2R,
masking function mask, dropout probability 𝑝drop.
1:Initialize the pseudo-random number generator state Rand save to HBM.
2:Set block sizes 𝐵𝑐=𝑀
4𝑑
𝐵𝑟=min𝑀
4𝑑
𝑑.
3:Initialize O=¹0º𝑁𝑑2R𝑁𝑑ℓ=¹0º𝑁2R𝑁𝑚=¹1º𝑁2R𝑁in HBM.
4:Divide Qinto𝑇𝑟=l
𝑁
𝐵𝑟m
blocks Q1Q𝑇𝑟of size𝐵𝑟𝑑each, and divide KVin to𝑇𝑐=l
𝑁
𝐵𝑐m
blocks
K1K𝑇𝑐andV1V𝑇𝑐, of size𝐵𝑐𝑑each.
5:Divide Ointo𝑇𝑟blocks O𝑖O𝑇𝑟of size𝐵𝑟𝑑each, divide ℓinto𝑇𝑟blocksℓ𝑖ℓ𝑇𝑟of size𝐵𝑟each,
divide𝑚into𝑇𝑟blocks𝑚1𝑚𝑇𝑟of size𝐵𝑟each.
6:for1𝑗𝑇𝑐do
7:Load K𝑗V𝑗from HBM to on-chip SRAM.
8:for1𝑖𝑇𝑟do
9:Load Q𝑖O𝑖ℓ𝑖𝑚𝑖from HBM to on-chip SRAM.
10:On chip, compute S𝑖𝑗=𝜏Q𝑖K𝑇
𝑗2R𝐵𝑟𝐵𝑐.
11:On chip, compute Smasked
𝑖𝑗=mask¹S𝑖𝑗º.
12:On chip, compute ~𝑚𝑖𝑗=rowmax¹Smasked
𝑖𝑗º2R𝐵𝑟,~P𝑖𝑗=exp¹Smasked
𝑖𝑗~𝑚𝑖𝑗º2R𝐵𝑟𝐵𝑐(pointwise),
~ℓ𝑖𝑗=rowsum¹~P𝑖𝑗º2R𝐵𝑟.
13:On chip, compute 𝑚new
𝑖=max¹𝑚𝑖~𝑚𝑖𝑗º2R𝐵𝑟,ℓnew
𝑖=𝑒𝑚𝑖𝑚new
𝑖ℓ𝑖𝑒~𝑚𝑖𝑗𝑚new
𝑖~ℓ𝑖𝑗2R𝐵𝑟.
14:On chip, compute ~Pdropped
𝑖𝑗=dropout¹~P𝑖𝑗𝑝dropº.
15:Write O𝑖 diag¹ℓnew
𝑖º1¹diag¹ℓ𝑖º𝑒𝑚𝑖𝑚new
𝑖O𝑖𝑒~𝑚𝑖𝑗𝑚new
𝑖~Pdropped
𝑖𝑗V𝑗ºto HBM.
16:Writeℓ𝑖 ℓnew
𝑖,𝑚𝑖 𝑚new
𝑖to HBM.
17:end for
18:end for
19:Return Oℓ𝑚R.
B.4 FlashAttention : Backward Pass
We describe the full details of FlashAttention backward pass. Given input sequences QKV2R𝑁𝑑, the
output O2R𝑁𝑑, and the output gradient dO, we want to compute the input gradients dQdKdV2R𝑁𝑑.
We ﬁrst describe the standard attention backward pass in Algorithm 3 for completeness.
Algorithm 3 Standard Attention Backward Pass
Require: Matrices QKVdO2R𝑁𝑑,P2R𝑁𝑁in HBM.
1:Load PdOby blocks from HBM, compute dV=P>dO2R𝑁𝑑, write dVto HBM.
2:Load dOVby blocks from HBM, compute dP=dOV>2R𝑁𝑁, write dPto HBM.
3:Read PdPfrom HBM, compute dS2R𝑁𝑁where𝑑𝑆𝑖𝑗=𝑃𝑖𝑗¹𝑑𝑃𝑖𝑗Í
𝑙𝑃𝑖𝑙𝑑𝑃𝑖𝑙º, write dSto HBM.
4:Load dSandKby blocks from HBM, compute dQ=dSK, write dQto HBM.
5:Load dSandQby blocks from HBM, compute dK=dS>Q, write dKto HBM.
6:Return dQdKdV.
We now make two observations about FlashAttention backward pass:
1.We do not need to store the dropout mask of size 𝑂¹𝑁2ºfrom the forward pass. Instead, we can save
the pseudo-random number generator states from the forward pass and re-generate the dropout mask
in the backward pass. This allows us to only use 𝑂¹𝑁ºextra memory.
2.When computing the softmax gradient, we use Eq. (4) to compute 𝐷𝑖=𝑃>
𝑖:𝑑𝑃𝑖:without reducing over
𝑃𝑖:and𝑑𝑃𝑖:of size𝑁(they might not ﬁt into SRAM). Instead we can rewrite 𝐷𝑖=𝑑𝑜>
𝑖𝑜𝑖and compute
the dot product between vectors of size 𝑑.
20The full FlashAttention backward pass algorithm is in Algorithm 4. Conceptually it is just a block
version of the derivation in Appendix B.2.
Algorithm 4 FlashAttention Backward Pass
Require: Matrices QKVOdO2R𝑁𝑑in HBM, vectors ℓ𝑚2R𝑁in HBM, on-chip SRAM of size 𝑀,
softmax scaling constant 𝜏2R, masking function mask, dropout probability 𝑝drop, pseudo-random
number generator state Rfrom the forward pass.
1:Set the pseudo-random number generator state to R.
2:Set block sizes 𝐵𝑐=𝑀
4𝑑
𝐵𝑟=min𝑀
4𝑑
𝑑.
3:Divide Qinto𝑇𝑟=l
𝑁
𝐵𝑟m
blocks Q1Q𝑇𝑟of size𝐵𝑟𝑑each, and divide KVin to𝑇𝑐=l
𝑁
𝐵𝑐m
blocks
K1K𝑇𝑐andV1V𝑇𝑐, of size𝐵𝑐𝑑each.
4:Divide Ointo𝑇𝑟blocks O𝑖O𝑇𝑟of size𝐵𝑟𝑑each, divide dOinto𝑇𝑟blocks dO𝑖dO𝑇𝑟of size
𝐵𝑟𝑑each, divide ℓinto𝑇𝑟blocksℓ𝑖ℓ𝑇𝑟of size𝐵𝑟each, divide 𝑚into𝑇𝑟blocks𝑚1𝑚𝑇𝑟of size
𝐵𝑟each.
5:Initialize dQ=¹0º𝑁𝑑in HBM and divide it into 𝑇𝑟blocks dQ1dQ𝑇𝑟of size𝐵𝑟𝑑each. Initialize
dK=¹0º𝑁𝑑dV=¹0º𝑁𝑑in HBM and divide dKdVin to𝑇𝑐blocks dK1dK𝑇𝑐anddV1dV𝑇𝑐,
of size𝐵𝑐𝑑each.
6:for1𝑗𝑇𝑐do
7:Load K𝑗V𝑗from HBM to on-chip SRAM.
8:Initialize ~dK𝑗=¹0º𝐵𝑐𝑑~dV𝑗=¹0º𝐵𝑐𝑑on SRAM.
9:for1𝑖𝑇𝑟do
10:Load Q𝑖O𝑖dO𝑖dQ𝑖ℓ𝑖𝑚𝑖from HBM to on-chip SRAM.
11:On chip, compute S𝑖𝑗=𝜏Q𝑖K𝑇
𝑗2R𝐵𝑟𝐵𝑐.
12:On chip, compute Smasked
𝑖𝑗=mask¹S𝑖𝑗º.
13:On chip, compute P𝑖𝑗=diag¹𝑙𝑖º1exp¹Smasked
𝑖𝑗𝑚𝑖º2R𝐵𝑟𝐵𝑐.
14:On chip, compute dropout mask Z𝑖𝑗2R𝐵𝑟𝐵𝑐where each entry has value1
1𝑝dropwith probability
1𝑝dropand value 0 with probability 𝑝drop.
15:On chip, compute Pdropped
𝑖𝑗=P𝑖𝑗Z𝑖𝑗(pointwise multiply).
16:On chip, compute ~dV𝑗 ~dV𝑗¹Pdropped
𝑖𝑗º>dO𝑖2R𝐵𝑐𝑑.
17:On chip, compute dPdropped
𝑖𝑗=dO𝑖V>
𝑗2R𝐵𝑟𝐵𝑐.
18:On chip, compute dP𝑖𝑗=dPdropped
𝑖𝑗Z𝑖𝑗(pointwise multiply).
19:On chip, compute 𝐷𝑖=rowsum¹dO𝑖O𝑖º2R𝐵𝑟.
20:On chip, compute dS𝑖𝑗=P𝑖𝑗¹dP𝑖𝑗𝐷𝑖º2R𝐵𝑟𝐵𝑐.
21:Write dQ𝑖 dQ𝑖𝜏dS𝑖𝑗K𝑗2R𝐵𝑟𝑑to HBM.
22:On chip, compute ~dK𝑗 ~dK𝑗𝜏dS>
𝑖𝑗Q𝑖2R𝐵𝑐𝑑.
23:end for
24:Write dK𝑗 ~dK𝑗dV𝑗 ~dV𝑗to HBM.
25:end for
26:Return dQdKdV.
We see that similar to the forward pass, the backward pass performs 𝑂¹𝑁2ºFLOPs and only requires
𝑂¹𝑁ºextra memory beyond inputs, output, output gradient, and input gradients.
We analyze the IO-complexity of the backward pass, similar to the forward pass (Theorem 2).
Theorem5. Let𝑁be the sequence length, 𝑑be the head dimension, and 𝑀be size of SRAM with 𝑑𝑀𝑁𝑑.
Standard attention (Algorithm 0) backward pass requires Θ¹𝑁𝑑𝑁2ºHBM accesses, while FlashAttention
backward pass (Algorithm 4) requires Θ¹𝑁2𝑑2𝑀1ºHBM accesses.
The proof is in Appendix C.
21B.5 Comparison with Rabe and Staats [66]
We describe here some similarities and diﬀerences between our FlashAttention algorithm and the algorithm
of Rabe and Staats [66].
Conceptually, both FlashAttention and Rabe and Staats [66]operate on blocks of the attention matrix
using the well-established technique of tiling (or softmax scaling) [ 51,60]. To reduce the memory footprint,
both methods avoid storing the large attention matrix in the forward pass and recompute it in the backward
pass.
The ﬁrst major diﬀerence is that Rabe and Staats [66]focuses on the reducing the total memory footprint
(maximum amount of GPU memory required) while FlashAttention focuses on reducing memory accesses
(the number of memory reads/writes). As mentioned in Section 2, the amount of memory access is the
primary determining factor of runtime. Reducing memory accesses also necessarily reduces the total amount
of memory required (e.g., if an operation incurs 𝐴memory accesses, then its total memory requirement is at
most𝐴). As a result, FlashAttention is faster than standard attention (2-4 ) while Rabe and Staats [66]
is around the same speed or slightly slower than standard attention. In terms of total memory required, both
methods oﬀer substantial memory saving.
The second diﬀerence between the two methods is the way information is summarized from each block
to pass to the next block. Rabe and Staats [66]summarizes each block with its temporary output along
with the softmax normalization statistics. At the end of the forward pass, the temporary outputs of all the
blocks are combined using the statistics to produce the ﬁnal output. FlashAttention instead incrementally
updates the output (Algorithm 1 line 12) after processing each block, so only one copy of the output is needed
(instead of𝐾copies for𝐾blocks). This means that FlashAttention has smaller total memory requirement
compared to Rabe and Staats [66].
The ﬁnal major diﬀerence is the way the backward pass is computed. Rabe and Staats [66]uses gradient
checkpointing to recompute the attention matrix and the temporary output of each block. FlashAttention
instead simpliﬁes the backward pass analytically (Appendices B.2 and B.4). It only recomputes the attention
matrix and does not recompute the temporary output of each block. This reduces the memory requirement
for the backward pass and yields speedup.
C Proofs
Proof of Theorem 1. We ﬁrst count the number of FLOPs and extra memory required.
The dominating FLOPs are from matrix multiplication. In the inner loop, (Algorithm 1 line 9), we
compute Q𝑖K>
𝑗2R𝐵𝑟𝐵𝑐forQ𝑖2R𝐵𝑟𝑑andK𝑗2R𝐵𝑐𝑑, which takes 𝑂¹𝐵𝑟𝐵𝑐𝑑ºFLOPs. We also compute
(Algorithm 1 line 12) ~P𝑖𝑗V𝑗2R𝐵𝑟𝑑for~P𝑖𝑗2R𝐵𝑟𝐵𝑐andV𝑗2R𝐵𝑐𝑑, which takes 𝑂¹𝐵𝑟𝐵𝑐𝑑ºFLOPs. We
execute the inner loops 𝑇𝑐𝑇𝑟=l
𝑁
𝐵𝑐ml
𝑁
𝐵𝑟m
times. Therefore the total number of FLOPs is
𝑂𝑁2
𝐵𝑐𝐵𝑟𝐵𝑟𝐵𝑐𝑑
=𝑂¹𝑁2𝑑º
In terms of extra memory required, we see that we need 𝑂¹𝑁ºmemory to store the statistics ¹ℓ𝑚º.
We now prove the algorithms correctness by induction on 𝑗for0𝑗𝑇𝑐. Let K:𝑗2R𝑗𝐵𝑐𝑑be the
ﬁrst𝑗𝐵𝑐rows of K, and similarly V:𝑗2R𝑗𝐵𝑐𝑑the the ﬁrst 𝑗𝐵𝑐rows of V. Let S::𝑗=QK>
:𝑗2R𝑁𝑗𝐵𝑐, and
P::𝑗=softmax¹S::𝑗º2R𝑁𝑗𝐵𝑐(softmax applied row-wise). Let 𝑚𝑗ℓ¹𝑗ºO¹𝑗ºbe the values of 𝑚ℓOin HBM
after the𝑗-th iteration of the outer loop (Algorithm 1 line 5). (Note that these values of 𝑚ℓOare updated
after each iteration of the outer loop.) We want to show that after the 𝑗-th iteration of the outer loop, we
have computed in HBM:
𝑚¹𝑗º=rowmax¹S::𝑗º2R𝑁 ℓ¹𝑗º=rowsum¹exp¹S::𝑗𝑚¹𝑗ººº2R𝑁O¹𝑗º=P::𝑗V:𝑗2R𝑁𝑑
Based on our initialization (Algorithm 1 line 2), this claim is true for 𝑗=0(i.e., before the any iteration
of the outer loop is executed). Suppose that the claim holds for some 𝑗=0𝑇𝑐1. We want to show that
the claim also holds for 𝑗1. Indeed, when we update the statistics in the inner loop (Algorithm 1 line 10)
22on the¹𝑗1º-th iteration of the outer loop, we update 𝑚¹𝑗1º=max¹𝑚¹𝑗º~𝑚ºwhere ~𝑚2R𝑁is the row-max
ofS:𝑗:𝑗1, the slice of Sfrom column 𝑗𝐵𝑐to column¹𝑗1º𝐵𝑐1. This implies that
𝑚¹𝑗1º=rowmax¹S::𝑗1º2R𝑁
Similarly, we update
ℓ¹𝑗1º=𝑒𝑚¹𝑗º𝑚¹𝑗1ºℓ¹𝑗º𝑒~𝑚𝑚¹𝑗1º~ℓ
where ~ℓ=rowsum¹exp¹S:𝑗:𝑗1~𝑚ºº2R𝑁. By the same algebraic manipulation in Section 3.1, we obtain:
ℓ¹𝑗1º=rowsum¹exp¹S::𝑗1𝑚¹𝑗1ººº2R𝑁
LetV𝑗:𝑗1be the slice of Vfrom column 𝑗𝐵𝑐to column¹𝑗1º𝐵𝑐1, we also update:
O¹𝑗1º=diag¹ℓ¹𝑗1ºº1¹diag¹ℓ¹𝑗ºº𝑒𝑚¹𝑗º𝑚¹𝑗1ºO¹𝑗º𝑒~𝑚𝑚¹𝑗1ºexp¹S𝑗:𝑗1~𝑚ºV𝑗:𝑗1º
=diag¹ℓ¹𝑗1ºº1¹diag¹ℓ¹𝑗ºº𝑒𝑚¹𝑗º𝑚¹𝑗1ºP::𝑗V:𝑗𝑒𝑚¹𝑗1ºexp¹S𝑗:𝑗1ºV𝑗:𝑗1º
=diag¹ℓ¹𝑗1ºº1¹diag¹ℓ¹𝑗ºº𝑒𝑚¹𝑗º𝑚¹𝑗1ºdiag¹ℓ¹𝑗ººexp¹S::𝑗𝑚¹𝑗ººV:𝑗𝑒𝑚¹𝑗1ºexp¹S𝑗:𝑗1ºV𝑗:𝑗1º
=diag¹ℓ¹𝑗1ºº1¹𝑒𝑚¹𝑗1ºexp¹S::𝑗ºV:𝑗𝑒𝑚¹𝑗1ºexp¹S𝑗:𝑗1ºV𝑗:𝑗1º
=diag¹ℓ¹𝑗1ºº1¹exp¹S::𝑗𝑚¹𝑗1ººV:𝑗exp¹S𝑗:𝑗1𝑚¹𝑗1ººV𝑗:𝑗1º
=diag¹ℓ¹𝑗1ºº1
expS::𝑗S𝑗:𝑗1
𝑚¹𝑗1ºV:𝑗
V𝑗:𝑗1
=softmax¹S:𝑗1ºV:𝑗1
We then see that the claim is also true for 𝑗1. By induction, the claim is true for all 𝑗=0𝑇𝑐.
When𝑗=𝑇𝑐, we conclude that the ﬁnal value of Oin HBM is softmax¹SºV=softmax¹QK>ºV.

Proof of Theorem 2. We ﬁrst analyze the IO complexity of standard attention implementation. The inputs
QKV2R𝑁𝑑reside in HBM, and the at the end of the algorithm the output O2R𝑁𝑑is written to HBM.
In the ﬁrst step of computing the matrix multiply S=QK>, the inputs QKare read from HBM and the
output S2R𝑁𝑁is written to HBM (Algorithm 0 line 1). This incurs Θ¹𝑁𝑑𝑁2ºHBM accesses.
In the second step of computing P=softmax¹Sº, the input Sis read from HBM and the output Pis
written to HBM (Algorithm 0 line 2). This incurs Θ¹𝑁2ºHBM accesses.
In the last step of computing O=PV, the inputs PVare read from global memory and the output Ois
written to HBM (Algorithm 0 line 3). This incurs Θ¹𝑁𝑑𝑁2ºHBM accesses.
Overall, standard attention implementation requires Θ¹𝑁𝑑𝑁2ºglobal memory accesses.
We now analyze the IO complexity of streaming attention.
Following Algorithm 1, we see that each element of KandVis loaded from HBM once (Algorithm 1
line 6). We make 𝑇𝑐passes over QandO, each pass loading all of Qand all of Oto HBM (Algorithm 1
line 8). Therefore the number of HBM accesses is Θ¹𝑁𝑑𝑁𝑑𝑇𝑐º=Θ¹𝑁𝑑𝑇𝑐º.
We derive the conditions on the block sizes 𝐵𝑐and𝐵𝑟. We need the blocks K𝑗andV𝑗of size𝐵𝑐𝑑to ﬁt
into on-chip memory, which translates to:
𝐵𝑐𝑑=𝑂¹𝑀º,𝐵𝑐=𝑂𝑀
𝑑

Similarly, we need the blocks Q𝑖O𝑖of size𝐵𝑟𝑑to ﬁt into on-chip memory, which translates to:
𝐵𝑟𝑑=𝑂¹𝑀º,𝐵𝑟=𝑂𝑀
𝑑

Finally, we need the block S𝑖𝑗of size𝐵𝑟𝐵𝑐to ﬁt into on-chip memory, which translates to:
𝐵𝑟𝐵𝑐=𝑂¹𝑀º
23We therefore set:
𝐵𝑐=Θ𝑀
𝑑
 𝐵𝑟=Θ
min𝑀
𝑑𝑀
𝐵𝑐
=Θ
min𝑀
𝑑𝑑

We then have:
𝑇𝑐=𝑁
𝐵𝑐=Θ𝑁𝑑
𝑀

As a result, the number of HBM accesses is:
Θ¹𝑁𝑑𝑇𝑐º=Θ𝑁2𝑑2
𝑀


Proof of Proposition 3. For contradiction, suppose that there exists an algorithm that computes exact
attention where the number for HBM access for all 𝑀2𝑑𝑁𝑑¼is
𝑜𝑁2𝑑2
𝑀

In the regime of 𝑀=Θ¹𝑁𝑑º, this results in the number of HBM accesses:
𝑜𝑁2𝑑2
𝑁𝑑
=𝑜¹𝑁𝑑º
However, the input to attention (matrices QKV) and the output Ohave size𝑁𝑑and they start out being
in HBM, so if the algorithm computes exact attention it must incur at least Ω¹𝑁𝑑ºHBM accesses. This is a
contradiction. 
Proof of Theorem 5. The IO complexity of the attention backward is very similar to the IO complexity of
the attention forward (Theorem 2). Here we provide a sketch of the proof.
We ﬁrst analyze the IO complexity of standard attention backward pass. The inputs QKVdO2R𝑁𝑑
reside in HBM, and the at the end of the algorithm the outputs dQdKdV2R𝑁𝑑are written to HBM.
At each step of the standard attention backward pass, one needs to load inputs of size 𝑁𝑑or𝑁2from
HBM, and needs to write the outputs of size 𝑁2or𝑁𝑑to HBM. This incurs Θ¹𝑁𝑑𝑁2ºHBM accesses.
We now analyze the IO complexity of FlashAttention backward pass.
Similar to Theorem 2, we see that each element of KandVis loaded from HBM once. Each element of
dKanddVis only written to HBM once. We make 𝑇𝑐passes over QOdO, each pass loading all of QOdO
to HBM. We also make 𝑇𝑐passes over dQ, each pass reading/writing all of dQfrom/to HBM. Therefore the
number of HBM accesses is Θ¹𝑁𝑑𝑁𝑑𝑇𝑐º=Θ¹𝑁𝑑𝑇𝑐º.
As in the proof of Theorem 2, the constraints on the block sizes are that:
𝐵𝑐=Θ𝑀
𝑑
 𝐵𝑟=Θ
min𝑀
𝑑𝑑

We then have:
𝑇𝑐=𝑁
𝐵𝑐=Θ𝑁𝑑
𝑀

As a result, the number of HBM accesses is:
Θ¹𝑁𝑑𝑇𝑐º=Θ𝑁2𝑑2
𝑀


24Algorithm 5 Block-Sparse FlashAttention Forward Pass
Require: Matrices QKV2R𝑁𝑑in HBM, on-chip SRAM of size 𝑀, softmax scaling constant 𝜏2R,
masking function mask, dropout probability 𝑝drop, block sizes 𝐵𝑐=𝑀
4𝑑
𝐵𝑟=min𝑀
4𝑑
𝑑, block
sparsity mask 𝑀2f01g𝑁𝐵𝑟𝑁𝐵𝑐..
1:Initialize the pseudo-random number generator state Rand save to HBM.
2:Initialize O=¹0º𝑁𝑑2R𝑁𝑑ℓ=¹0º𝑁2R𝑁𝑚=¹1º𝑁2R𝑁in HBM.
3:Divide Qinto𝑇𝑟=l
𝑁
𝐵𝑟m
blocks Q1Q𝑇𝑟of size𝐵𝑟𝑑each, and divide KVin to𝑇𝑐=l
𝑁
𝐵𝑐m
blocks
K1K𝑇𝑐andV1V𝑇𝑐, of size𝐵𝑐𝑑each.
4:Divide Ointo𝑇𝑟blocks O𝑖O𝑇𝑟of size𝐵𝑟𝑑each, divide ℓinto𝑇𝑟blocksℓ𝑖ℓ𝑇𝑟of size𝐵𝑟each,
divide𝑚into𝑇𝑟blocks𝑚1𝑚𝑇𝑟of size𝐵𝑟each.
5:for1𝑗𝑇𝑐do
6:Load K𝑗V𝑗from HBM to on-chip SRAM.
7:for1𝑖𝑇𝑟do
8:if𝑀𝑖𝑗0then
9: Load Q𝑖O𝑖ℓ𝑖𝑚𝑖from HBM to on-chip SRAM.
10: On chip, compute S𝑖𝑗=𝜏Q𝑖K𝑇
𝑗2R𝐵𝑟𝐵𝑐.
11: On chip, compute Smasked
𝑖𝑗=mask¹S𝑖𝑗º.
12: On chip, compute ~𝑚𝑖𝑗=rowmax¹Smasked
𝑖𝑗º2R𝐵𝑟,~P𝑖𝑗=exp¹Smasked
𝑖𝑗~𝑚𝑖𝑗º2R𝐵𝑟𝐵𝑐(pointwise),
~ℓ𝑖𝑗=rowsum¹~P𝑖𝑗º2R𝐵𝑟.
13: On chip, compute 𝑚new
𝑖=max¹𝑚𝑖~𝑚𝑖𝑗º2R𝐵𝑟,ℓnew
𝑖=𝑒𝑚𝑖𝑚new
𝑖ℓ𝑖𝑒~𝑚𝑖𝑗𝑚new
𝑖~ℓ𝑖𝑗2R𝐵𝑟.
14: On chip, compute ~Pdropped
𝑖𝑗=dropout¹~P𝑖𝑗𝑝dropº.
15: Write O𝑖 diag¹ℓnew
𝑖º1¹diag¹ℓ𝑖º𝑒𝑚𝑖𝑚new
𝑖O𝑖𝑒~𝑚𝑖𝑗𝑚new
𝑖~Pdropped
𝑖𝑗V𝑗ºto HBM.
16: Writeℓ𝑖 ℓnew
𝑖,𝑚𝑖 𝑚new
𝑖to HBM.
17:end if
18:end for
19:end for
20:Return Oℓ𝑚R.
D Extension Details
D.1 Block-sparse FlashAttention
We describe the full block-sparse FlashAttention algorithm in Algorithm 5. The algorithm is identical
to Algorithm 2, except that we skip zero blocks.
We prove the IO-complexity of block-sparse FlashAttention .
Proof of Proposition 4. The proof is very similar to the proof of Theorem 2. For the block-sparse case, notice
that we only need to load blocks corresponding to nonzero blocks. As a result, the number of HBM accesses
are scaled by 𝑠, the fraction of nonzero blocks in the block-sparsity mask. However, for small values of 𝑠, we
would still need to write the result O2R𝑁𝑑. Therefore the number of HBM accesses is
Θ
𝑁𝑑𝑁2𝑑2
𝑀𝑠


D.2 Potential Extensions
We discuss here a few potential extensions of the IO-aware approach to speed up deep learning training.
Multi-GPU Attention. Large language models are trained on hundreds or thousands of GPUs, and
one typically splits the attention computation between 4-8 GPUs on the same node [ 77]. This introduces
another level of memory hierarchy: beside GPU SRAM and GPU HBM, we also have the HBM of other
25GPUs. For very long sequences, the diﬀerent GPUs on the same node can cooperate to compute attention by
taking into account the asymmetry of diﬀerent levels of memory hierarchy.
Sparse MLP layers. Typical dense MLP layers are compute-bound and not memory-bound. To improve
their eﬃciency, MLP layers with sparse weight matrices can be used [ 17]. However, many sparse MLP layers
are instead memory-bound, and their speedup is often not proportional to the sparsity. We believe that an
IO-aware implementation can alleviate this issue and realize the beneﬁts of sparsity. We are excited about
future work in this direction, to reduce the computational requirement of large models and improve their
wall-block runtime.
Kernel machine learning. Our approach in FlashAttention relies on the fact that the 𝑁𝑁
attention matrix is a function of a low-rank matrix QK>(of rank𝑑𝑁). As a result, we can repeatedly
load the inputs QKand recompute the block of the attention matrix that we need, signiﬁcantly reducing
HBM access. As similar scenario happens in kernel machine learning: each element 𝐾𝑖𝑗of the𝑁𝑁kernel
matrix Kis a function of two vectors of size 𝑑𝑁, as it measures the similarity between two datapoints 𝑥𝑖
and𝑥𝑗. The KeOps library [ 8,26] is a successful example of how reducing memory reads/writes can speed up
kernel operations. We hope that this will motivate kernel methods that focus more on reducing IOs instead
of just FLOPs.
E Full Experimental Results
E.1 BERT
We train BERT-large following the training procedure and hyperparameters of the reference MLPerf 1.1
implementation. In particular, we use the LAMB optimizer with learning rate 3.75e-3, with batch size 448,
trained for at most 7100 steps. The training is stopped once the validation accuracy (for masked language
modeling) reaches the target 72.0%, and the wall-clock run-time is measured. We train with FP16 precision
using Apex AMP (with O2 optimization level).
We compare our results with the reported training speed from Nvidia that was submitted to MLPerf 1.1
(Table 1).
We use the same train / validation data split provided by MLPerf 1.1 reference implementation. In
particular, we evaluate on the same 10000 validation examples as the baseline from Nvidia.
We train the model on 8 A100-80GB GPUs. Each training run takes between 16 and 19 minutes, and we
average the results of 10 runs.
E.2 GPT-2
We use the standard implementations of GPT-2 [ 67] from Huggingface transformers library and from
Nvidias Megatron-LM repo. We follow the training recipe of the Megatron-LM repo.
We use an eﬀective batch size of 512, and use gradient accumulation to ﬁt into available GPU memory.
We use the AdamW optimizer, with learning rate 6e-4 for GPT-2 small and 1.5e-4 for GPT-2 medium, and
weight decay of 0.1. All models are trained with the same hyperparameters for 400K steps. We run all
implementations with mixed-precision training (PyTorch AMP).
We use the Openwebtext dataset, with the GPT-2 BPE tokenizer. We randomly select 0.5% of the dataset
as the validation set, with the rest being used as training set. This random selection of validation set is done
once, and all models are evaluated on the same validation set.
We train the model on 8 A100-40GB GPUs, and we measure the wall-clock training time. Training
GPT-2 small takes between 2.7-9.5 days, and training GPT-2 medium takes between 6.9-21.0 days (Table 2).
In Fig. 4, we plot of the validation perplexity throughout training of GPT-2 small/medium, using either
HuggingFace implementation or our FlashAttention implementation. We see that FlashAttention be-
haves the same as the baseline implementation and the validation perplexity curves of the two implementations
almost lie on top of each other.
Long Document Classiﬁcation. For MIMIC-III and ECtHR, we follow the hyperparameters of Dai et al.
[13].
26100k 200k 300k
Training steps1015202530Val perplexityGPT-2-small HuggingFace
GPT-2-small FlashAttention
GPT-2-medium HuggingFace
GPT-2-medium FlashAttentionFigure 4: Validation perplexity of GPT-2 small/medium using two implementations. We conﬁrm that
FlashAttention yields the same validation curves as the baseline implementation from HuggingFace.
E.3 LRA details
We follow the hyperparameters from the Long-range arena paper [ 80], the Long-range arena repo ( https:
//github.com/google-research/long-range-arena ), and the Nyströmformer reproduction [ 90]. To be
generous to the baseline methods, if we are unable to reproduce the performance of any baseline for any of
the ﬁve tasks, we report the better performance from Tay et al. [80]or Xiong et al. [90]for that baseline on
that task.
After hyperparameter tuning, almost all of the attention methods achieve similar accuracy on all of the
ﬁve LRA tasks.
We run all methods with mixed-precision training, except for Performer (not stable with mixed precision)
and Local Attention (implementation does not support FP16).
To calculate the overall wallclock-time speedup, we take the geometric mean of the wallclock-time speedup
of each of the ﬁve tasks.
Path-X For Path-X and Path-256, we follow the hyperparameters from the PathFinder-32 experiments
from the long-range arena paper[ 80]. For both, we ﬁrst pretrain a model on Path-64. We take the checkpoint
after 200 epochs, upsample its positional embedding (we duplicate the positional embeddings gridwise in
space), and ﬁne-tune it on the downstream task for 200 epochs with one epoch of linear warmup, and cosine
decay of the learning rate. For Path-X, we take the best performing checkpoint (according to val accuracy),
and additionally ﬁne-tune it for 200 epochs with the same warmup and learning rate (this adds roughly 4
points of accuracy to FlashAttention for Path-X, but the model starts overﬁtting afterwards).
E.4 Comparison with Apex FMHA
We compare our method/implementation with Apex FMHA ( https://github.com/NVIDIA/apex/tree/
master/apex/contrib/csrc/fmha ).
When we started this project, Apex FMHA was the fastest implementation of attention (that we knew
of), tailored for short sequences of length at most 512. In fact, almost all MLPerf submissions for BERT
training benchmark running on Nvidia GPUs use FMHA for their model code, as of MLPerf 1.1 [ 58]. Since
27Table 7: Runtime (ms) of FlashAttention compared to FMHA by sequence length, with masking and dropout,
measured on an A100-SXM4-40GB GPU. Batch size 64, 16 heads, head dimension 64 (i.e., BERT-large size).
Attention Method 128 256 512
Apex FMHA forward 0.10 0.29 1.14
FlashAttention forward 0.08 0.22 0.81
Apex FMHA backward 0.17 0.52 1.81
FlashAttention backward 0.20 0.53 2.00
Apex FMHA forward + backward 0.270.81 2.95
FlashAttention forward + backward 0.28 0.75 2.81
FMHA targets BERT models, it only supports head dimension 64, and only runs on A100 GPUs. FMHA
fuses the attention computation dropout¹softmax¹mask¹QK>ºººVinto one CUDA kernel. In the forward
pass, it stores the attention matrix softmax¹mask¹QK𝑇ººto HBM to be used in gradient computation. As a
result, it does not oﬀer substantial memory saving (though for shorter sequences memory footprint is often
not a primary concern).
We use FMHA code as a starting point, and apply two well-established techniques (tiling and recomputa-
tion) to deal with long sequences and to save memory as mentioned in Section 3. As a result, we can support
much longer sequences (e.g., up to length 64K). We also support more head dimensions (16, 32, 64, 128) and
broader GPU types (all Turing and Ampere GPUs at the time of writing).
In Table 7, we compare the performance of FlashAttention and Apex FMHA for short sequences (as
FMHA only supports sequence length at most 512). Generally FlashAttention is slightly faster than
FMHA in the forward pass and slightly slower than FMHA in the backward pass. This is because we do not
store the attention matrix in the forward pass and recompute it in the backward pass. Compared to FMHA,
the overall runtime of FlashAttention is about 4% slower for sequence length 128, 8% faster for sequence
length 256, and 5% faster for sequence length 512.
E.5 Speedup On Diﬀerent Hardware and Conﬁgurations
Speedup varies between diﬀerent types of GPU types and generations depending on HBM bandwidth and
SRAM size. In this section, we proﬁle FlashAttention speedup on diﬀerent GPUs and conﬁgurations.
Figure 5: Speedup over standard PyTorch attention at diﬀerent sequence lengths, on A100.
A100Figure 5 shows speedup on an A100 GPU with batch size 8, head dimension 64, and 12 attention
heads, across diﬀerent sequence lengths. We generally see 2-4 speedup, and we see more speedup when
using dropout and masking due to kernel fusion.
28Figure 6: Speedup over standard PyTorch attention at diﬀerent sequence lengths, on A100, with head
dimension 128.
A100, Head Dimension 128 Speedup also changes when we increase the head dimension. Each block
requires more memory, so we need to use smaller block sizes to ﬁt into SRAM. Figure 6 shows speedup with
head dimension 128 on an A100 (batch size 16, 12 heads). We see less speedup overallbut we can still see
signiﬁcant speedup (up to 3 ) with a causal mask, where half the blocks are masked out.
Figure 7: Speedup over standard PyTorch attention at diﬀerent sequence lengths, on RTX 3090.
RTX 3090 Figure 7 shows speedup on an RTX 3090 GPU. Here, we use batch size 12 with 12 attention
heads. We observe slightly higher speedups on the RTX 3090 (between 2.5-4.5 ), since the memory bandwidth
on an RTX 3090 is lower than on an A100 (roughly 900 GB/s vs. 1.5 TB/s).
T4Figure 8 shows speedup on a T4 GPU. T4 SRAM is smaller than A100, so we need to make the block
sizes smaller in FlashAttention . As a result, we observe less speedup on T4, which matches the IO
complexity analysis in Section 3.2. T4 GPUs are commonly used for inference, so we also report speedup on
the forward pass only.
29Figure 8: Speedup over standard PyTorch attention at diﬀerent sequence lengths, on T4. Top:Combined
forward pass + backward pass. Bottom: Forward pass only.
E.6 Full Benchmarking Results
We report the full benchmarking results and experimental details on A100.
Baselines We compare against reference implementations for exact attention from PyTorch/HuggingFace
and Megatron, approximate attention, and sparse attention. For approximate attention, we compare against
reference implementations of Reformer [ 51], Local Attention [ 68], Linformer Attention [ 84], Smyrf [ 19], and
LongShortFormer (LSFormer) [ 94]. For sparse attention, we compare against reference implementations of
Block-Sparse Attention form OpenAI [ 11], Longformer[ 3], and BigBird Attention [ 92]. For the approximate
and sparse attention, we use a compression ratio of 1/8, or a compressed sequence length of 256, whichever is
smaller.
Setup We measure runtime and memory usage of the attention computation with 8 heads of dimension 64,
and batch size 16 on a machine with one A100 GPU with 40 GB of GPU HBM. We vary sequence length
in our experiments. We compute attention on random vectors for Q,K, and V(we do not measure the
projection from the hidden layer). For dropout, we use dropout 0.1; for masking, we use a padding mask
with uniformly-random mask lengths between the total sequence length and the total sequence length minus
20. To measure runtime, we take the average of 100 measurements of the attention call. We only measure
memory footprint once, since it does not vary between runs.
30Table 8: Pointers to results tables.
Dropout Masking Pass Table
Yes Yes Forward Table 9
Yes Yes Backward Table 10
Yes Yes Combined Table 11
No Yes Forward Table 12
No Yes Backward Table 13
No Yes Combined Table 14
Yes No Forward Table 15
Yes No Backward Table 16
Yes No Combined Table 17
No No Forward Table 18
No No Backward Table 19
No No Combined Table 20
No No Memory Usage (Combined) Table 21
Table 9: Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length,
with dropout and masking . Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.36 0.34 0.78 2.54 9.33 36.33 - - - -
Megatron 0.40 0.40 1.10 3.65 16.19 - - - - -
Reformer 2.03 3.15 5.67 11.02 22.59 46.14 97.38 212.13 - -
Local Attention 0.83 0.86 1.01 2.20 7.13 14.32 28.60 57.79 117.67 -
Linformer 0.67 0.52 0.69 0.71 1.65 3.18 6.15 12.16 24.17 52.39
Smyrf 2.27 2.34 3.91 7.44 14.71 29.22 58.27 116.41 - -
LSformer 1.18 1.27 1.34 3.38 11.40 22.55 44.95 89.76 179.66 -
Block Sparse 1.12 1.11 2.13 2.77 6.95 20.91 - - - -
Longformer 1.22 1.14 1.08 1.95 5.72 12.98 - - - -
BigBird 1.13 1.12 1.12 1.77 6.03 13.68 - - - -
FlashAttention 0.04 0.06 0.21 0.82 2.85 10.41 41.74 167.19 670.76 2682.35
Block-Sparse FlashAttention 0.06 0.06 0.06 0.12 0.44 0.86 1.70 3.29 6.55 13.34
We report timing results on the forward pass, backward pass, and combined forward + backward pass.
We measure each method with and without dropout, masking, or bothexcept for Block Sparse, Longformer,
and BigBird. These methods did not successfully run the backward pass with masking due to a bug in
external libraries, so we measured them without masking to be generous. We use FP16 for all measurements,
except for Local Attention, whose implementation only supports FP32.
For each baseline, we increase sequence length until it runs out of memory on the GPU, except for the
following exceptions: The Megatron implementation does not support sequence lengths longer than 2048.
Block-Sparse (OpenAI) does not support sequence lengths longer than 4096. Longformer and BigBird do not
support sequence lengths longer than 8092.
We measure memory usage on the combined forward + backward pass, without dropout or masking.
Results Table 8 summarizes all the experimental conﬁgurations and contains pointers to the results tables.
31Table 10: Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length,
with dropout and masking . Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.37 0.49 1.66 5.81 22.32 87.67 - - - -
Megatron 0.35 0.32 0.77 2.42 8.43 - - - - -
Reformer 2.37 4.59 8.91 17.68 35.13 70.05 140.01 - - -
Local Attention 0.55 0.62 1.49 4.03 13.78 27.61 55.20 110.27 221.40 -
Linformer 0.89 0.80 0.81 0.93 2.48 4.75 9.29 18.27 36.53 -
Smyrf 1.41 2.83 5.43 10.72 21.25 42.31 84.48 168.95 - -
LSformer 1.75 1.76 3.01 7.50 20.07 39.08 76.39 150.82 - -
Block Sparse 1.29 1.28 2.18 3.04 7.27 21.16 - - - -
Longformer 1.27 1.31 1.29 2.04 5.24 10.74 25.95 - - -
BigBird 1.33 1.28 1.32 1.81 5.55 11.44 27.45 - - -
FlashAttention 0.30 0.26 0.68 2.02 6.84 26.89 105.70 418.96 1666.89 6660.44
Block-Sparse FlashAttention 0.30 0.27 0.29 0.59 1.50 2.94 5.82 11.85 23.98 47.61
Table 11: Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by
sequence length, with dropout and masking . Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.84 0.86 2.35 8.29 31.75 124.19 - - - -
Megatron 0.87 0.89 1.33 4.21 16.50 - - - - -
Reformer 4.30 7.76 14.60 28.74 57.79 116.34 237.57 - - -
Local Attention 1.40 1.60 2.06 6.06 20.94 42.01 84.08 168.48 339.45 -
Linformer 1.57 1.49 1.55 1.60 4.19 8.04 15.71 30.92 61.47 -
Smyrf 3.41 5.08 9.35 18.18 36.03 71.68 143.04 285.87 - -
LSformer 3.08 3.10 4.26 10.90 31.59 61.72 121.51 241.18 - -
Block Sparse 2.54 2.52 3.71 5.44 13.29 39.19 - - - -
Longformer 2.47 2.49 2.51 3.10 10.39 22.49 60.44 - - -
BigBird 2.51 2.49 2.52 3.40 10.97 23.89 63.28 - - -
FlashAttention 0.43 0.41 0.95 2.55 9.56 37.49 147.75 586.61 2339.11 9341.30
Block-Sparse FlashAttention 0.44 0.44 0.45 0.89 1.95 4.12 7.64 16.60 32.73 64.11
Table 12: Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length,
with masking . Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.30 0.30 0.63 1.93 7.08 27.45 112.90 - - -
Megatron 0.45 0.41 0.43 1.52 5.80 - - - - -
Reformer 1.87 3.00 5.37 10.43 21.40 43.83 92.80 203.24 - -
Local Attention 0.70 0.81 1.02 2.09 6.64 13.34 26.77 54.02 110.11 -
Linformer 0.63 0.50 0.67 0.65 1.36 2.60 5.04 9.92 19.69 43.47
Smyrf 2.38 2.32 3.76 7.16 14.14 28.09 55.98 111.73 - -
LSformer 1.22 1.29 1.44 3.28 10.99 21.72 43.29 86.32 172.76 -
Block Sparse 0.96 1.04 1.66 2.16 5.41 16.15 - - - -
Longformer 0.99 0.98 0.99 1.56 4.79 11.07 32.98 - - -
BigBird 0.96 1.02 1.02 1.48 5.05 11.59 34.16 - - -
FlashAttention 0.03 0.04 0.17 0.68 2.28 8.40 33.55 134.14 537.50 2150.88
Block-Sparse FlashAttention 0.05 0.04 0.05 0.11 0.35 0.68 1.33 2.54 5.34 10.73
Table 13: Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length,
with masking . Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.44 0.46 1.53 5.33 20.34 79.87 - - - -
Megatron 0.29 0.31 0.65 1.95 6.49 - - - - -
Reformer 2.31 4.47 8.68 17.20 34.14 68.09 136.02 - - -
Local Attention 0.51 0.62 1.30 3.81 13.33 26.72 53.41 106.82 214.15 -
Linformer 0.76 0.81 0.94 0.87 2.24 4.25 8.35 16.38 32.67 72.11
Smyrf 1.34 2.77 5.30 10.46 20.73 41.27 82.41 164.86 - -
LSformer 1.66 1.61 3.09 7.42 19.68 38.35 74.92 147.86 - -
Block Sparse 1.24 1.25 2.04 2.91 6.78 19.67 - - - -
Longformer 1.27 1.23 1.24 1.85 4.99 10.21 24.89 - - -
BigBird 1.43 1.50 1.44 1.69 5.25 10.86 26.26 - - -
FlashAttention 0.21 0.22 0.62 1.84 5.77 22.25 86.21 338.91 1343.91 5361.09
Block-Sparse FlashAttention 0.22 0.22 0.26 0.57 1.55 3.13 5.98 12.21 23.49 47.85
32Table 14: Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by
sequence length, with masking . Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.80 0.81 2.08 7.23 27.51 107.58 - - - -
Megatron 0.81 0.83 1.09 3.36 12.39 - - - - -
Reformer 4.16 7.46 14.06 27.68 55.66 112.15 229.37 - - -
Local Attention 1.39 1.68 2.08 5.83 20.04 40.16 80.44 161.35 325.11 -
Linformer 1.51 1.42 1.56 1.67 3.67 6.99 13.63 26.77 53.36 117.56
Smyrf 3.38 4.93 9.07 17.66 34.94 69.55 138.72 277.41 - -
LSformer 3.08 3.10 4.26 10.90 31.59 61.72 121.51 241.18 - -
Block Sparse 2.39 2.40 3.31 5.02 12.25 35.94 - - - -
Longformer 2.36 2.34 2.38 2.94 9.83 21.35 58.12 - - -
BigBird 2.35 2.35 2.37 3.25 10.36 22.57 60.63 - - -
FlashAttention 0.32 0.30 0.83 2.37 7.95 30.77 119.98 473.65 1883.43 7513.01
Block-Sparse FlashAttention 0.34 0.34 0.36 0.69 1.85 3.89 7.16 14.85 30.46 60.03
Table 15: Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length,
with dropout . Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.26 0.24 0.57 1.80 6.56 25.34 - - - -
Megatron 0.27 0.27 0.56 1.88 6.56 - - - - -
Reformer 1.83 2.96 5.31 10.33 21.19 43.42 91.96 201.34 - -
Local Attention 0.51 0.60 0.78 2.01 6.23 12.52 25.07 50.50 102.18 -
Linformer 0.47 0.37 0.49 0.52 1.37 2.65 5.12 10.13 20.25 44.16
Smyrf 2.12 2.01 3.15 5.97 11.83 23.36 46.48 92.72 - -
LSformer 1.28 1.33 1.51 3.39 11.40 22.54 44.96 89.85 179.73 -
Block Sparse 1.03 1.00 1.72 2.39 5.96 17.88 - - - -
Longformer 1.02 1.03 1.03 1.73 5.10 11.63 34.22 - - -
BigBird 0.99 1.03 1.01 1.58 5.36 12.27 35.56 - - -
FlashAttention 0.10 0.10 0.22 0.83 2.81 10.38 41.63 167.01 668.74 2678.11
Block-Sparse FlashAttention 0.54 0.51 0.68 0.61 0.67 1.10 1.89 3.71 7.18 14.41
Table 16: Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length,
with dropout . Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.44 0.35 0.90 2.94 10.77 41.67 - - - -
Megatron 0.28 0.33 0.92 2.94 10.80 - - - - -
Reformer 2.24 4.34 8.39 16.62 33.02 65.77 131.52 - - -
Local Attention 0.51 0.58 1.41 3.71 12.96 25.98 51.94 103.72 207.78 -
Linformer 0.84 0.74 0.79 0.85 2.28 4.37 8.66 17.02 33.78 -
Smyrf 1.27 2.56 4.90 9.66 19.16 38.13 76.17 152.39 - -
LSformer 1.67 1.77 3.03 7.52 20.10 39.13 76.35 150.83 - -
Block Sparse 1.27 1.36 2.15 3.04 7.27 21.18 - - - -
Longformer 1.28 1.34 1.38 1.98 5.24 10.74 25.95 - - -
BigBird 1.48 1.47 1.50 1.81 5.57 11.38 27.43 - - -
FlashAttention 0.15 0.18 0.58 1.86 6.50 26.21 104.27 416.10 1661.92 6643.01
Block-Sparse FlashAttention 0.17 0.17 0.17 0.40 1.10 2.04 4.43 9.33 18.28 37.31
Table 17: Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by
sequence length, with dropout . Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.66 0.67 1.43 4.82 17.47 67.29 - - - -
Megatron 0.88 0.90 1.49 4.73 17.41 - - - - -
Reformer 4.06 7.28 13.68 26.98 54.27 109.39 223.80 - - -
Local Attention 1.09 1.40 1.99 5.61 19.23 38.62 77.30 154.63 311.12 -
Linformer 1.31 1.21 1.30 1.39 3.73 7.15 14.05 27.69 55.00 -
Smyrf 3.00 4.37 8.05 15.66 31.04 61.64 123.04 245.65 - -
LSformer 3.07 3.17 4.31 10.89 31.54 61.78 121.56 240.94 - -
Block Sparse 2.54 2.52 3.71 5.44 13.29 39.19 - - - -
Longformer 2.47 2.49 2.51 3.10 10.39 22.49 60.44 - - -
BigBird 2.51 2.49 2.52 3.40 10.97 23.89 63.28 - - -
FlashAttention 0.35 0.36 0.80 2.52 9.16 36.70 146.13 583.45 2332.01 9323.63
Block-Sparse FlashAttention 0.91 0.83 0.94 0.92 1.83 3.50 7.02 13.56 26.71 53.92
33Table 18: Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length.
Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.21 0.22 0.43 1.27 4.32 16.47 67.77 - - -
Megatron 0.24 0.26 0.42 1.33 4.28 - - - - -
Reformer 1.77 2.82 5.01 9.74 20.03 41.11 87.39 192.40 - -
Local Attention 0.48 0.57 0.80 1.90 5.76 11.56 23.13 46.65 94.74 -
Linformer 0.46 0.36 0.45 0.50 1.09 2.09 4.01 7.90 15.70 35.40
Smyrf 1.94 1.96 3.01 5.69 11.26 22.23 44.21 88.22 - -
LSformer 1.21 1.34 1.34 3.31 11.01 21.71 43.27 86.32 172.85 -
Block Sparse 0.96 1.04 1.66 2.16 5.41 16.15 - - - -
Longformer 0.99 0.98 0.99 1.56 4.79 11.07 32.98 - - -
BigBird 0.96 1.02 1.02 1.48 5.05 11.59 34.16 - - -
FlashAttention 0.08 0.09 0.18 0.68 2.40 8.42 33.54 134.03 535.95 2147.05
Block-Sparse FlashAttention 0.56 0.52 0.63 0.65 0.61 0.96 1.69 3.02 5.69 11.77
Table 19: Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length.
Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.26 0.29 0.78 2.44 8.82 33.87 - - - -
Megatron 0.29 0.30 0.80 2.59 8.86 - - - - -
Reformer 2.18 4.21 8.14 16.12 32.02 63.84 127.60 - - -
Local Attention 0.51 0.64 1.28 3.60 12.52 25.08 50.22 100.23 200.66 -
Linformer 0.69 0.76 0.69 0.80 2.04 3.88 7.67 15.04 30.11 63.15
Smyrf 1.24 2.49 4.77 9.42 18.65 37.12 74.15 148.35 - -
LSformer 1.68 1.61 3.02 7.40 19.72 38.27 74.89 147.99 - -
Block Sparse 1.24 1.25 2.04 2.91 6.78 19.67 - - - -
Longformer 1.27 1.23 1.24 1.85 4.99 10.21 24.89 - - -
BigBird 1.43 1.50 1.44 1.69 5.25 10.86 26.26 - - -
FlashAttention 0.11 0.16 0.52 1.62 5.45 21.57 84.75 336.00 1338.56 5343.19
Block-Sparse FlashAttention 0.11 0.12 0.16 0.38 1.20 2.34 4.69 9.10 18.74 37.04
Table 20: Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by
sequence length. Best in bold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 0.67 0.70 1.18 3.67 13.22 50.44 - - - -
Megatron 0.74 0.65 1.23 3.80 13.21 - - - - -
Reformer 3.93 7.01 13.15 25.89 52.09 105.00 215.13 - - -
Local Attention 1.09 1.27 1.99 5.38 18.32 36.77 73.67 147.29 296.35 -
Linformer 1.31 1.25 1.30 1.29 3.20 6.10 11.93 23.39 46.72 100.52
Smyrf 2.98 4.23 7.78 15.12 29.96 59.45 118.60 237.02 - -
LSformer 3.03 3.05 4.26 10.70 30.77 60.15 118.33 234.94 - -
Block Sparse 2.39 2.40 3.31 5.02 12.25 35.94 - - - -
Longformer 2.36 2.34 2.38 2.94 9.83 21.35 58.12 - - -
BigBird 2.35 2.35 2.37 3.25 10.36 22.57 60.63 - - -
FlashAttention 0.31 0.31 0.73 2.29 7.64 30.09 118.50 470.51 1876.08 7492.85
Block-Sparse FlashAttention 0.74 0.77 0.82 0.88 1.71 3.21 6.56 12.60 24.93 50.39
Table 21: Memory usage (MB) of various exact/approximate/sparse attention mechanisms by sequence length. Best
inbold, second best underlined .
Attention Method 128 256 512 1024 2048 4096 8192 16384 32768 65536
PyTorch Attention 36 104 336 1184 4416 17024 - - - -
Megatron 36 104 336 1184 4416 - - - - -
Reformer 377 754 1508 3016 6033 12067 24134 - - -
Local Attention 53 110 232 592 1696 3392 6784 13568 27136 -
Linformer 25 52 114 287 832 1652 3292 6572 13132 26252
Smyrf 217 434 868 1737 3474 6947 13894 27788 - -
LSformer 72 152 333 796 2540 5068 10125 20240 - -
Block Sparse 33 82 228 408 910 2401 - - - -
Longformer 30 61 124 277 681 1370 2748 - - -
BigBird 33 66 131 294 708 1431 2872 - - -
FlashAttention 22 44 104 209 418 836 1672 3344 6688 13376
Block-Sparse FlashAttention 22 44 104 209 418 836 1672 3344 6690 13384
34
  Generating Long Sequences with Sparse Transformers
Rewon Child1Scott Gray1Alec Radford1Ilya Sutskever1
Abstract
Transformers are powerful sequence models, but
require time and memory that grows quadrati-
cally with the sequence length. In this paper we
introduce sparse factorizations of the attention
matrix which reduce this to O(npn). We also
introduce a) a variation on architecture and initial-
ization to train deeper networks, b) the recompu-
tation of attention matrices to save memory, and
c) fast attention kernels for training. We call net-
works with these changes Sparse Transformers,
and show they can model sequences tens of thou-
sands of timesteps long using hundreds of layers.
We use the same architecture to model images,
audio, and text from raw bytes, setting a new state
of the art for density modeling of Enwik8, CIFAR-
10, and ImageNet-64. We generate unconditional
samples that demonstrate global coherence and
great diversity, and show it is possible in principle
to use self-attention to model sequences of length
one million or more.
1. Introduction
Estimating complex, high-dimensional data distributions is
a central problem in unsupervised learning, as many down-
stream applications of interest involve generation of text,
images, audio, and other data. Additionally, it is believed to
be a key component of unsupervised representation learning.
Recently, neural autoregressive models have achieved im-
pressive results in this domain, achieving state-of-the-art in
modeling natural language (Jozefowicz et al., 2016) (Rad-
ford et al., 2018) (Dai et al., 2018), raw audio (Van Den Oord
et al., 2016) (Mehri et al., 2016), and images (Oord et al.,
2016) (Menick & Kalchbrenner, 2018) (Salimans et al.,
2017) (Reed et al., 2017) (Chen et al., 2017).
These methods decompose a joint probability distribution
into a product of conditional ones. Modeling these condi-
tional distributions is extremely challenging, however, as
they contain many complex, long-range dependencies and
require a suitably expressive model architecture to learn
them.
Architectures based off CNNs (Oord et al., 2016) have made
Figure 1. Unconditional samples from our neural autoregressive
model on ImageNet 64 and a classical music dataset. We used the
same self-attention based architecture for audio, images, and text.
The samples above were generated with softmax temperature 1.0,
and had lengths 12,288 and 65,536. Audio samples be listened to at
https://openai.com/blog/sparse-transformer
great progress in this direction, but require signiﬁcant depth
to expand their receptive ﬁeld. To address this, WaveNet
(Van Den Oord et al., 2016) introduced dilated convolutions,
which allowed the network to model long-range dependen-
cies in a logarithmic number of layers.
Separately, the Transformer (Vaswani et al., 2017) has been
shown to excel on many natural language tasks, which may
be in part due to its ability to model arbitrary dependencies
in a constant number of layers. As each self-attention layer
has a global receptive ﬁeld, the network can allocate rep-
resentational capacity to the input regions for which it isarXiv:1904.10509v1  [cs.LG]  23 Apr 2019Generating Long Sequences with Sparse Transformers
most useful. Thus the architecture may be more ﬂexible
at generating diverse data types than networks with ﬁxed
connectivity patterns.
However, the memory and computational requirements of
such networks grows quadratically with sequence length,
which excludes their use on long sequences.
The main contribution of this work is to introduce several
sparse factorizations of the attention matrix, which scale
asO(nppn)with the sequence length without sacriﬁcing
performance. These work by separating the full attention
computation into several faster attention operations which,
when combined, can approximate the dense attention oper-
ation. We use this to apply self-attention to sequences of
unprecedented length.
Additionally, we introduce several other changes to the
Transformer, including:
A restructured residual block and weight initialization
to improve training of very deep networks
A set of sparse attention kernels which efﬁciently com-
pute subsets of the attention matrix
Recomputation of attention weights during the back-
wards pass to reduce memory usage
We empirically validate that models augmented in this man-
ner can achieve state-of-the-art compression and generation
of natural language, raw audio, and natural images. The
simplicity of the architecture leads us to believe it may be
useful for many problems of interest.
2. Related Work
The most related work involves other techniques for scaling
up autoregressive generative models. For images, (Reed
et al., 2017) models conditional independence between the
pixels in order to generate many locations in parallel, and
(Menick & Kalchbrenner, 2018) imposes an ordering and
multi-scale upsampling procedure to generate high ﬁdelity
samples. (Parmar et al., 2018) uses blocks of local attention
to apply Transformers to images. For text, (Dai et al., 2018)
introduces a state reuse memory for modeling long-term
dependencies. And for audio, in addition to (Van Den Oord
et al., 2016), (Mehri et al., 2016) used a hierarchical struc-
ture and RNNs of varying clock-rates to use long contexts
during inference, similar to (Koutnik et al., 2014). (Huang
et al., 2018) apply Transformers to MIDI generation with
an efﬁcient relative attention.
Our work is simpler than many of the techniques above and
can be applied equally across images, text, and audio. Many
of the above techniques are orthogonal to ours, moreover,
and could be used in conjunction with ours.Outside of generative modeling, there are several works
relevant to improving the efﬁciency of attention based off
chunking (Chiu & Raffel, 2017) or using ﬁxed length repre-
sentations (Britz et al., 2017). Other works have investigated
attention with multiple hops, such as (Sukhbaatar et al.,
2015) and (Gehring et al., 2017).
It is worth noting that the Gated Pixel CNN (Oord et al.,
2016) and WaveNet (Van Den Oord et al., 2016) use multi-
plicative interactions in their networks, which are related to
self-attention.
3. Background
We consider the task of autoregressive sequence gener-
ation, where the joint probability of a sequence x=
fx1;x2;:::;x ngis modeled as the product of conditional
probability distributions and parameterized by a network .
p(x) =nY
i=1p(xijx1;:::;x i1;) (1)
We treat images, text, and audio as a sequence of discrete
tokens, typically raw bytes. The network takes in the se-
quence of tokens and outputs a categorical distribution over
thevpossible values of the next token using the softmax
function, where vis the size of the vocabulary . The training
objective is to maximize the log-probability of the data with
respect to.
A simple and powerful choice for model is a Transformer
(Vaswani et al., 2017) in decoder-only mode, as demon-
strated by (Radford et al., 2018) and (Liu et al., 2018). These
models transform the input sequence with blocks of mul-
tihead self-attention over the entire sequence, followed by
dense transformations over each sequence element. The self-
attention portion of the network must compute nweightings
for each ofnelements, however, which can quickly become
intractable as the sequence length grows.
In the following sections, we describe our modiﬁcations to
the Transformer architecture which make it more suitable
for modeling long sequences.
4. Factorized Self-Attention
Sparse Transformers separate the full self-attention opera-
tion across several steps of attention, as visualized in Figure
3(b) and 3(c). To motivate our approach, we ﬁrst perform
a qualitative assessment of attention patterns learned by a
standard Transformer on an image dataset.Generating Long Sequences with Sparse Transformers
Figure 2. Learned attention patterns from a 128-layer network on CIFAR-10 trained with full attention. White highlights denote attention
weights for a head while generating a given pixel, and black denotes the autoregressive mask. Layers are able to learn a variety of
specialized sparse structures, which may explain their ability to adapt to different domains. a) Many early layers in the network learn
locally connected patterns, which resemble convolution. b) In layers 19 and 20, the network learned to split the attention across a
row attention and column attention, effectively factorizing the global attention calculation. c) Several attention layers showed global,
data-dependent access patterns. d) Typical layers in layers 64-128 exhibited high sparsity, with positions activating rarely and only for
speciﬁc input patterns.
(a) Transformer
 (b) Sparse Transformer (strided)
 (c) Sparse Transformer (ﬁxed)
Figure 3. Two 2d factorized attention schemes we evaluated in comparison to the full attention of a standard Transformer (a). The top
row indicates, for an example 6x6 image, which positions two attention heads receive as input when computing a given output. The
bottom row shows the connectivity matrix (not to scale) between all such outputs (rows) and inputs (columns). Sparsity in the connectivity
matrix can lead to signiﬁcantly faster computation. In (b) and (c), full connectivity between elements is preserved when the two heads are
computed sequentially. We tested whether such factorizations could match in performance the rich connectivity patterns of Figure 2.Generating Long Sequences with Sparse Transformers
4.1. Qualitative assessment of learned attention
patterns
We visualized the attention patterns learned by a 128-layer
self-attention network on CIFAR-10, and present several
examples in Figure 2. Visual inspection showed that most
layers had sparse attention patterns across most data points,
suggesting that some form of sparsity could be introduced
without signiﬁcantly affecting performance. Several layers
(Figure 2c) clearly exhibited global patterns, however, and
others exhibited data-dependent sparsity (Figure 2d), both
of which would be impacted by introducing a predetermined
sparsity pattern into all of the attention matrices.
In this paper, we restricted our investigation to a class of
sparse attention patterns that have connectivity between all
positions over several steps of attention. These methods can
be more efﬁcient than full attention while still providing
global context to any given position. We aimed to empiri-
cally validate the performance of these factorized patterns
on a range of tasks, given that they are unable to learn the
exact same mappings as those in Figure 2. We present the
formulation of factorized attention below.
4.2. Factorized self-attention
A self-attention layer maps a matrix of input embeddings
Xto an output matrix and is parameterized by a connectiv-
ity patternS=fS1;:::;S ng, whereSidenotes the set of
indices of the input vectors to which the ith output vector
attends. The output vector is a weighted sum of transforma-
tions of the input vectors:
Attend(X;S) =
a(xi;Si)
i2f1;:::;ng(2)
a(xi;Si) = softmax 
(Wqxi)KT
Sip
d!
VSi (3)
KSi=
Wkxj
j2SiVSi=
Wvxj
j2Si(4)
HereWq,Wk, andWvrepresent the weight matrices which
transform a given xiinto a query ,key, orvalue , anddis
the inner dimension of the queries and keys. The output at
each position is a sum of the values weighted by the scaled
dot-product similarity of the keys and queries.
Full self-attention for autoregressive models deﬁnes Si=
fj:jig, allowing every element to attend to all previous
positions and its own position.
Factorized self-attention instead has pseparate attention
heads, where the mth head deﬁnes a subset of the indices
A(m)
i fj:jigand letsSi=A(m)
i. We are
chieﬂy interested in efﬁcient choices for the subset A, where
jA(m)
ij/ppn.Additionally, for the time being we consider valid choices
ofA, where all input positions are connected to all future
output positions across the psteps of attention.
For everyjipair, we set every Asuch thatican attend
tojthrough a path of locations with maximum length p+ 1.
Speciﬁcally, if (j;a;b;c;:::;i )is the path of indices, then
j2A(1)
a,a2A(2)
b,b2A(3)
c, and so forth.
These two criteria allow us keep the ability of Transformers
to propagate signals from arbitrary input positions to arbi-
trary output positions in a constant number of steps, while
reducing the total effective computation to O(nppn). We
also note that softening the validity criterion (for instance,
having a series of only locally connected layers) may be a
useful inductive bias for certain domains.
In this work, we explore two factorizations for p= 2, which
we describe in the following section, though we note that
the same techniques can be easily extended to higher dimen-
sions.
4.3. Two-dimensional factorized attention
A natural approach to deﬁning a factorized attention pattern
in two dimensions is to have one head attend to the previous
llocations, and the other head attend to every lth location,
wherelis the stride and chosen to be close topn, a method
we call strided attention.
Formally,A(1)
i=ft;t+ 1;:::;igfort= max(0;il)
andA(2)
i=fj: (ij) modl= 0g. This pattern can be
visualized in Figure 3(b).
This formulation is convenient if the data naturally has a
structure that aligns with the stride, like images or some
types of music. For data without a periodic structure, like
text, however, we ﬁnd that the network can fail to properly
route information with the strided pattern, as spatial coor-
dinates for an element do not necessarily correlate with the
positions where the element may be most relevant in the
future.
In those cases, we instead use a ﬁxed attention pattern (Fig-
ure 3(c)), where speciﬁc cells summarize previous locations
and propagate that information to all future cells.
Formally,A(1)
i=fj: (bj=lc=bi=lc)g, where the brackets
denote the ﬂoor operation, and A(2)
i=fj:jmodl2
ft;t+ 1;:::;lg, wheret=lcandcis a hyperparameter.
Concretely, if the stride is 128 and c= 8, then all future
positions greater than 128 can attend to positions 120-128,
all positions greater than 256 can attend to 248-256, and so
forth.
A ﬁxed-attention pattern with c= 1limits the expressivity
of the network signiﬁcantly, as many representations inGenerating Long Sequences with Sparse Transformers
the network are only used for one block whereas a small
number of locations are used by all blocks. We instead
found choosing c2f8;16;32gfor typical values of l2
f128;256gto perform well, although it should be noted that
this increases the computational cost of this method by cin
comparison to the strided attention.
Additionally, we found that when using multiple heads,
having them attend to distinct subblocks of length cwithin
the block of size lwas preferable to having them attend to
the same subblock.
In the subsequent section, we describe how to incorporate
factorized attention into the Sparse Transformer architec-
ture.
5. Sparse Transformer
Here we fully describe the Sparse Transformer architecture,
which is a modiﬁed version of the Transformer (Vaswani
et al., 2017).
5.1. Factorized attention heads
Standard dense attention simply performs a linear transfor-
mation of the attend function deﬁned in Equation 2:
attention(X) =Wpattend(X;S) (5)
whereWpdenotes the post-attention weight matrix. The
simplest technique for integrating factorized self-attention
is to use one attention type per residual block, and interleave
them sequentially or at a ratio determined as a hyperparam-
eter:
attention(X) =Wpattend(X;A(rmodp))(6)
Hereris the index of the current residual block and pis the
number of factorized attention heads.
A second approach is to have a single head attend to the
locations of the pixels that both factorized heads would
attend to, which we call a merged head:
attention(X) =Wpattend(X;p[
m=1A(m)) (7)
This is slightly more computationally intensive, but only
by a constant factor. A third approach is to use multi-head
attention (Vaswani et al., 2017), where nhattention products
are computed in parallel, then concatenated along the feature
dimension:
attention(X) =Wp
attend(X;A)(i)
i2f1;:::;n hg(8)
embed
linearsoftmaxnorm
norm
normdropout
dropoutattention
feed-forward. . . Figure 4. Diagram depicting one residual block of the Sparse Trans-
former. The shaded background indicates tensors which are check-
pointed (Chen et al., 2016) and stored in GPU memory. The other
tensors, including the attention weights and feedforward network
activations, are recomputed during the calculation of gradients,
reducing memory usage substantially.
Here, theAcan be the separate attention patterns, the
merged patterns, or interleaved as in Eq. 2. Also, the di-
mensions of the weight matrices inside the attend function
are reduced by a factor of 1=nh, such that the number of
parameters are invariant across values of nh.
We typically ﬁnd multiple heads to work well, though for
extremely long sequences where the attention dominates the
computation time, it is more worthwhile to perform them
one at a time and sequentially.
5.2. Scaling to hundreds of layers
We found that Transformers were difﬁcult to train with
many layers, as noted by (Al-Rfou et al., 2018). Instead
of incorporating auxillary losses, we adopted the followingGenerating Long Sequences with Sparse Transformers
architectural changes.
First, we use the pre-activation residual block of (He et al.,
2016), deﬁning a network of Nlayers in the following way:
H0= embed(X;W e) (9)
Hk=Hk1+ resblock( Hk1) (10)
y= softmax(norm( HN)Wout) (11)
where embed is a function we describe in the next section,
Woutis a weight matrix, and resblock(h)normalizes the
input to the attention block and a positionwise feedforward
network in the following way:
a(H) = dropout(attention(norm( H))) (12)
b(H) = dropout((norm( H+a(H)))) (13)
resblock(H) =a(H) +b(H) (14)
Thenorm function denotes Layer Normalization (Ba et al.,
2016), and (x) =W2f(W1x+b1) +b2. Our choice of
fis the Gaussian Error Linear Unit (Hendrycks & Gimpel,
2016),f(X) =Xsigmoid(1:702X), as used in (Rad-
ford et al., 2018). The output dimension of W1is 4.0 times
the input dimension, unless otherwise noted.
Observe that HNis the sum of Napplications of functions
aandb, and thus each function block receives a gradient
directly from the output layer . We scale the initialization
ofW2andWpin Eq. 5 by1p
2Nto keep the ratio of input
embedding scale to residual block scale invariant across
values ofN.
5.3. Modeling diverse data types
In addition to the embedding of input symbols, positional
embeddings are typically used in Transformers and other
location-agnostic architectures to encode the spatial relation-
ships of data (Gehring et al., 2017), (Parmar et al., 2018).
We found using learned embeddings which either encoded
the structure of the data or the factorized attention patterns
were important for performance of our models.
We added either nemb=ddata ornemb=dattn embed-
dings to each input location, where ddata refers to the num-
ber of dimensions of the data, and dattnis the number of
dimensions of the factorized attention. If xiis the one-hot
encodedith element in the sequence, and o(j)
irepresents
the one-hot encoded position of xiin thejth dimension
(1jnemb), then:
embed(X;W e) =0
@xiWe+nembX
j=1o(j)
iWj1
A
xi2X(15)For images, we used data embeddings, where ddata= 3
for the row, column, and channel location of each input
byte. For text and audio, we used two-dimensional attention
embeddings, where dattn= 2and the index corresponds to
each positions row and column index in a matrix of width
equal to the stride.
5.4. Saving memory by recomputing attention weights
Gradient checkpointing has been shown to be effective in
reducing the memory requirements of training deep neural
networks (Chen et al., 2016), (Gruslys et al., 2016). It is
worth noting, however, that this technique is particularly
effective for self-attention layers when long sequences are
processed, as memory usage is high for these layers relative
to the cost of computing them.
Using recomputation alone, we are able to train dense atten-
tion networks with hundreds of layers on sequence lengths
of 16,384, which would be infeasible on modern hardware
otherwise.
In our experiments, we recompute the attention and feed-
forward blocks during the backwards pass. To simplify
our implementation, we do not apply dropout within the
attention blocks, as in (Vaswani et al., 2017), and instead
only apply it at the end of each residual addition, as seen in
Figure 4.
5.5. Efﬁcient block-sparse attention kernels
The sparse attention masks in 3(b) and 3(c) can be efﬁciently
computed by slicing out sub-blocks from the query, key, and
value matrices and computing the product in blocks. Atten-
tion over a local window can be computed as-is, whereas
attention with a stride of kcan be computed by transposing
the matrix and computing a local window. Fixed attention
positions can be aggregated and computed in blocks.
In order to ease experimentation, we implemented a set of
GPU kernels which efﬁciently perform these operations.
The softmax operation is fused into a single kernel and
also uses registers to eliminate loading the input data more
than once, allowing it to run at the same speed as a simple
nonlinearity. The upper triangle of the attention matrix
is never computed, moreover, removing the need for the
negative bias term of (Vaswani et al., 2017) and halving the
number of operations to be performed.
5.6. Mixed-precision training
We store network weights in single-precision ﬂoating-point,
but otherwise compute network activations and gradients in
half-precision, as in (Micikevicius et al., 2017). This acceler-
ates our training due to the usage of Tensor Core operations
on the V100 GPU. During the gradient calculation, we useGenerating Long Sequences with Sparse Transformers
Figure 5. Unconditional samples from ImageNet 64x64, generated with an unmodiﬁed softmax temperature of 1.0. We are able to learn
long-range dependencies directly from pixels without using a multi-scale architecture.
dynamic loss scaling to reduce numerical underﬂow, and
we communicate half-precision gradients when averaging
across multiple GPUs. When sampling, we cast the queries
and keys to single-precision, as the query-key product can
sometimes overﬂow the max value of half-precision.
6. Training
We use the Adam optimizer with a linear warmup of 5000
iterations and a gradient clipping of 1.0, both of which we
found important for model stability. We use a weight decay
penalty of 0.01. We annealed the learning rate according to
a cosine decay as in (Radford et al., 2018). We train on 8
V100 GPUs unless otherwise noted.
All embeddings are of a constant dimension d, usually one
off256;512;1024g. By default, all linear transforms are to
the same dimension, with the exception of the feed-forward
network, which projects the input to 4d, unless we use
half-size transformations, where it is 2d. Additionally,
sometimes we halve the size of the query and key transfor-
mations.
We initialize the token embedding WefromN(0;0:125p
d)and
the position embeddings from N(0;0:125pdnemb). Within the
attention and feedforward components, all biases are initial-ized to 0 and all weights are initialized from N(0;0:125pdin)
wheredinis the fan-in dimension. The weight matrix for
the output logits was initialized to 0.
7. Experiments
We empirically test our architecture on density modeling
tasks including natural images, text, and raw audio. A
summary of the results is available in Table 1. We found
that, in addition to running signiﬁcantly faster than full
attention, sparse patterns also converged to lower error, as
shown in Table 2. This may point to a useful inductive bias
from the sparsity patterns we introduced, or an underlying
optimization issue with full attention.
7.1. CIFAR-10
We train strided Sparse Transformers on CIFAR-10 images
represented as sequences of 3072 bytes. Models have 2
heads, 128 layers, d= 256, half-size feedforward network
and query-key projections, and are trained for 120 epochs
with a learning rate of 0.00035 and a dropout rate of 0.25
until validation error stops decreasing.
We use 48000 examples for training and 2000 examples for
validation, evaluating the performance of our best models onGenerating Long Sequences with Sparse Transformers
Table 1. Summary of our ﬁndings for density modeling tasks. Re-
sults are reported in bits per byte, which is equivalent to bits per
dim for image tasks. M refers to millions of parameters.
Model Bits per byte
CIFAR-10
PixelCNN (Oord et al., 2016) 3.03
PixelCNN++ (Salimans et al., 2017) 2.92
Image Transformer (Parmar et al., 2018) 2.90
PixelSNAIL (Chen et al., 2017) 2.85
Sparse Transformer 59M (strided) 2.80
Enwik8
Deeper Self-Attention (Al-Rfou et al., 2018) 1.06
Transformer-XL 88M (Dai et al., 2018) 1.03
Transformer-XL 277M (Dai et al., 2018) 0.99
Sparse Transformer 95M (ﬁxed) 0.99
ImageNet 64x64
PixelCNN (Oord et al., 2016) 3.57
Parallel Multiscale (Reed et al., 2017) 3.7
Glow (Kingma & Dhariwal, 2018) 3.81
SPN 150M (Menick & Kalchbrenner, 2018) 3.52
Sparse Transformer 152M (strided) 3.44
Classical music, 5 seconds at 12 kHz
Sparse Transformer 152M (strided) 1.97
the test set. The model achieves 2.80 bits per dim ( 2:798
0:004over seeds 1, 2, 3) versus the previous 2:85state of
the art (Chen et al., 2017). We also compare performance of
different attention patterns in Table 2. The strided attention
reaches the lowest error in the shortest amount of time,
surpassing the error of dense attention at 2.82 bits per dim.
7.2. Text
In order to assess Sparse Transformers on datasets without
a strong two-dimensional structure, we trained models on
the EnWik8 dataset, which represents the ﬁrst 108bytes
of Wikipedia and contains a great degree of variability in
periodic structure. We trained with a context length of
12,288, which is longer than previous approaches.
We trained on the ﬁrst 90 million tokens and reserved the last
10 million for validation and test. We used 30-layer ﬁxed
Sparse Transformers with 8 heads, d= 512, and a dropout
rate of 0:40. We trained for 80 epochs until validation loss
stopped decreasing. We used a stride of 128, c= 32 , and
merged the factorized attention heads.
Our best model reached 0.99 bits per dim ( 0:9920:001
over seeds 1, 2, 3), surpassing the 1.03 state-of-the-art for
a similarly-sized Transformer-XL (Dai et al., 2018) and
matching the 0.99 of a model trained with more than doubleTable 2. Sparse patterns showed increased speed and also better
loss on the datasets where we could compare both, which may
point to a useful inductive bias in the patterns we learned or an
underlying optimization issue with full attention.
Model Bits per byte Time/Iter
Enwik8 (12,288 context)
Dense Attention 1.00 1.31
Sparse Transformer (Fixed) 0.99 0.55
Sparse Transformer (Strided) 1.13 0.35
CIFAR-10 (3,072 context)
Dense Attention 2.82 0.54
Sparse Transformer (Fixed) 2.85 0.47
Sparse Transformer (Strided) 2.80 0.38
Table 3. We observe increased compression of Enwik8 with longer
contexts, suggesting the Sparse Transformer can effectively incor-
porate long-term dependencies.
Minimum context length during evaluation Bits per byte
6,144 tokens 0.9952
9,216 tokens 0.9936
10,752 tokens 0.9932
11,904 tokens 0.9930
12,096 tokens 0.9922
12,160 tokens 0.9908
the number of parameters. Strided attention failed to do well
on this dataset, whereas ﬁxed patterns were able to recover
and surpass the performance of dense attention, as listed in
Table 2.
Additionally, during evaluation of the test set, we modiﬁed
the minimum context length the network could use by evalu-
ating fewer tokens in parallel. We saw monotonic increases
in performance with more tokens used, up to 12,160 out
of the 12,288 tokens used for training (see Table 3), which
suggests the network is effectively incorporating long-term
dependencies.
7.3. ImageNet 64x64
In order to test the ability of the model to learn long range
dependencies and scale to a large dataset, we train on the
version of downsampled ImageNet released by (Oord et al.,
2016) and evaluate on the validation set. We used a 48 layer
strided Sparse Transformer with 16 attention heads and d
= 512, totaling 152 million parameters. We used a stride
of 128, a dropout of 0.01, and trained for 70 epochs, which
took 7 days on 64 V100 GPUs.
Our model achieves a loss of 3.44 bits per dim (3.437 across
1 run), in comparison to the previous 3.52 (Menick & Kalch-
brenner, 2018).Generating Long Sequences with Sparse Transformers
Additionally, we generate unconditional samples (Figure
5) at an unmodiﬁed softmax temperature of 1.0, from the
model and from one trained with twice the layers (300M
parameters total). We include here samples from the 300M
parameter model. On visual assessment we ﬁnd no artifacts
from the sparsity patterns and see evidence of long-term
structure in most images.
7.4. Classical music from raw audio
To test the extent to which Sparse Transformers are able
to scale to very long contexts, we trained models on the
classical music dataset released by (Dieleman et al., 2018).
As details of the dataset processing are unavailable, we omit
any direct comparison to other work and instead study what
size of Sparse Transformer we can train with increasing
context size. For each sequence length, we attempted to
train the largest model which could entirely ﬁt into 16GB
V100 accelerators without model parallelism.
Overall, we found that increasing the sequence length by a
factor of 4 requires a reduction in model capacity of approx-
imately 4p
4 = 8 . Thus we found we could use factorized
self-attention on sequences over 1 million timesteps long,
albeit with extremely few parameters (3 million).
Samples are available for sequences of length 65,536, which
correspond to around 5 seconds of generated audio at 12kHz.
The samples clearly demonstrate global coherence over the
sampled period, and exhibit a variety of play styles and
tones, swapping from rhythmic playing to forceful. To
listen to samples, visit https://openai.com/blog/
sparse-transformer . Sample quality quickly de-
grades for greater sequence lengths due to reduced model
capacity.
Table 4. Performance of a strided Sparse Transformer on a classical
audio dataset ( -law encoded at 12 kHz) as a function of sequence
length and model size.
Sequence length Parameters Bits per byte
65,536 152M 1.97
262,144 25M 2.17
1,048,576 3M 2.99
8. Conclusion
We introduced Sparse Transformers and showed they attain
equivalent or better performance on density modeling of
long sequences than standard Transformers while requiring
signiﬁcantly fewer operations. This performance is state-
of-the-art in images and text and is easily adaptable to raw
audio. The model demonstrates usage of long-term context
and generates globally coherent samples.9. Acknowledgements
We would like to thank Ashish Vaswani for insightful dis-
cussions during the genesis of the project. We also thank
Joshua Meier and Mark Chen for helpful discussions, and
Johannes Otterbach, Prafulla Dhariwal, and David Luan for
feedback on drafts of this paper.
References
Al-Rfou, R., Choe, D., Constant, N., Guo, M., and Jones,
L. Character-level language modeling with deeper self-
attention. arXiv preprint arXiv:1808.04444 , 2018.
Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.
arXiv preprint arXiv:1607.06450 , 2016.
Britz, D., Guan, M. Y ., and Luong, M.-T. Efﬁcient attention
using a ﬁxed-size memory representation. arXiv preprint
arXiv:1707.00110 , 2017.
Chen, T., Xu, B., Zhang, C., and Guestrin, C. Training
deep nets with sublinear memory cost. arXiv preprint
arXiv:1604.06174 , 2016.
Chen, X., Mishra, N., Rohaninejad, M., and Abbeel, P.
Pixelsnail: An improved autoregressive generative model.
arXiv preprint arXiv:1712.09763 , 2017.
Chiu, C.-C. and Raffel, C. Monotonic chunkwise attention.
arXiv preprint arXiv:1712.05382 , 2017.
Dai, Z., Yang, Z., Yang, Y ., Cohen, W. W., Carbonell, J., Le,
Q. V ., and Salakhutdinov, R. Transformer-xl: Language
modeling with longer-term dependency. 2018.
Dieleman, S., van den Oord, A., and Simonyan, K. The chal-
lenge of realistic music generation: modelling raw audio
at scale. In Advances in Neural Information Processing
Systems , pp. 80008010, 2018.
Gehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin,
Y . N. Convolutional sequence to sequence learning. arXiv
preprint arXiv:1705.03122 , 2017.
Gruslys, A., Munos, R., Danihelka, I., Lanctot, M., and
Graves, A. Memory-efﬁcient backpropagation through
time. In Advances in Neural Information Processing
Systems , pp. 41254133, 2016.
He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in
deep residual networks. arXiv preprint arXiv:1603.05027 ,
2016.
Hendrycks, D. and Gimpel, K. Bridging nonlinearities and
stochastic regularizers with gaussian error linear units.
arXiv preprint arXiv:1606.08415 , 2016.Generating Long Sequences with Sparse Transformers
Huang, C.-Z. A., Vaswani, A., Uszkoreit, J., Shazeer, N.,
Hawthorne, C., Dai, A. M., Hoffman, M. D., and Eck,
D. An improved relative self-attention mechanism for
transformer with application to music generation. arXiv
preprint arXiv:1809.04281 , 2018.
Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., and
Wu, Y . Exploring the limits of language modeling. arXiv
preprint arXiv:1602.02410 , 2016.
Kingma, D. P. and Dhariwal, P. Glow: Generative ﬂow
with invertible 1x1 convolutions. In Advances in Neural
Information Processing Systems , pp. 1023610245, 2018.
Koutnik, J., Greff, K., Gomez, F., and Schmidhuber, J. A
clockwork rnn. arXiv preprint arXiv:1402.3511 , 2014.
Liu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepa-
ssi, R., Kaiser, L., and Shazeer, N. Generating
wikipedia by summarizing long sequences. arXiv preprint
arXiv:1801.10198 , 2018.
Mehri, S., Kumar, K., Gulrajani, I., Kumar, R., Jain, S.,
Sotelo, J., Courville, A., and Bengio, Y . Samplernn: An
unconditional end-to-end neural audio generation model.
arXiv preprint arXiv:1612.07837 , 2016.
Menick, J. and Kalchbrenner, N. Generating high ﬁdelity im-
ages with subscale pixel networks and multidimensional
upscaling. arXiv preprint arXiv:1812.01608 , 2018.
Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen,
E., Garcia, D., Ginsburg, B., Houston, M., Kuchaev, O.,
Venkatesh, G., et al. Mixed precision training. arXiv
preprint arXiv:1710.03740 , 2017.
Oord, A. v. d., Kalchbrenner, N., and Kavukcuoglu,
K. Pixel recurrent neural networks. arXiv preprint
arXiv:1601.06759 , 2016.
Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, Ł., Shazeer,
N., and Ku, A. Image transformer. arXiv preprint
arXiv:1802.05751 , 2018.
Radford, A., Narasimhan, K., Salimans, T., and Sutskever,
I. Improving language understanding by genera-
tive pre-training. URL https://s3-us-west-2. ama-
zonaws. com/openai-assets/research-covers/language-
unsupervised/language understanding paper. pdf , 2018.
Reed, S., Oord, A. v. d., Kalchbrenner, N., Colmenarejo,
S. G., Wang, Z., Belov, D., and de Freitas, N. Paral-
lel multiscale autoregressive density estimation. arXiv
preprint arXiv:1703.03664 , 2017.
Salimans, T., Karpathy, A., Chen, X., and Kingma, D. P.
Pixelcnn++: Improving the pixelcnn with discretized lo-
gistic mixture likelihood and other modiﬁcations. arXiv
preprint arXiv:1701.05517 , 2017.Sukhbaatar, S., Weston, J., Fergus, R., et al. End-to-end
memory networks. In Advances in neural information
processing systems , pp. 24402448, 2015.
Van Den Oord, A., Dieleman, S., Zen, H., Simonyan, K.,
Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and
Kavukcuoglu, K. Wavenet: A generative model for raw
audio. CoRR abs/1609.03499 , 2016.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-
tion is all you need. In Advances in Neural Information
Processing Systems , pp. 59986008, 2017.
  xLSTM: Extended Long Short-Term Memory
Maximilian Beck1,2Korbinian Pöppel1,2Markus Spanring1
Andreas Auer1,2Oleksandra Prudnikova1Michael Kopp
Günter Klambauer1,2Johannes Brandstetter1,2,3Sepp Hochreiter1,2,3
Equal contribution
1ELLIS Unit, LIT AI Lab, Institute for Machine Learning, JKU Linz, Austria
2NXAI Lab, Linz, Austria,3NXAI GmbH, Linz, Austria
Abstract
In the 1990s, the constant error carousel and gating were introduced as the central
ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood
the test of time and contributed to numerous deep learning success stories, in
particular they constituted the first Large Language Models (LLMs). However,
the advent of the Transformer technology with parallelizable self-attention at its
core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a
simple question: How far do we get in language modeling when scaling LSTMs to
billions of parameters, leveraging the latest techniques from modern LLMs, but
mitigating known limitations of LSTMs? Firstly, we introduce exponential gating
with appropriate normalization and stabilization techniques. Secondly, we modify
the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar
update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a
matrix memory and a covariance update rule. Integrating these LSTM extensions
into residual block backbones yields xLSTM blocks that are then residually stacked
into xLSTM architectures. Exponential gating and modified memory structures
boost xLSTM capabilities to perform favorably when compared to state-of-the-art
Transformers and State Space Models, both in performance and scaling.
Memory Cells
 Constant Error Carousel
 Sigmoid Gating
 Recurrent Inference
 Recurrent TrainingsLSTM
+ New Memory MixingMemory Cells xLSTM Blocks xLSTM
mLSTM
+ Exponential Gating
+ Parallel Training
+ Covariance Update Rule+ Matrix MemoryLSTM
+ Exponential Gating
Figure 1: The extended LSTM (xLSTM) family. From left to right: 1. The original LSTM memory
cell with constant error carousel and gating. 2. New sLSTM and mLSTM memory cells that introduce
exponential gating. sLSTM offers a new memory mixing technique. mLSTM is fully parallelizable
with a novel matrix memory cell state and new covariance update rule. 3. mLSTM and sLSTM in
residual blocks yield xLSTM blocks. 4. Stacked xLSTM blocks give an xLSTM architecture.arXiv:2405.04517v1  [cs.LG]  7 May 20241 Introduction
The Long Short-Term Memory (LSTM) ideas (Hochreiter, 1991; Hochreiter & Schmidhuber,
1997b,a), i.e., the constant error carousel and gating, were introduced to overcome the vanishing
gradient problem of recurrent neural networks (Hochreiter, 1991; Hochreiter et al., 2000):
ct=ftct1+itzt, h t=otψ(ct). (1)
The constant error carousel is the additive update of the cell state ct1(green) by cell inputs ztand
moderated by sigmoid gates (blue). The input gate itand the forget gate ftcontrol this update, while
the output gate otcontrols the output of the memory cell, i.e. the hidden state ht. The cell state is
normalized or squashed by ψand then output gating gives the hidden state.
LSTMs have been successfully applied to various domains (Hochreiter et al., 2001, 2007; Schmid-
huber, 2015), and prevailed over text generation until the dawn of Transformers in 2017 (Vaswani
et al., 2017). The effectiveness of LSTMs has been demonstrated at numerous sequence-related tasks
such as generating text (Graves, 2013; Karpathy, 2015), generating handwritings (Graves, 2013),
sequence-to-sequence translation (Sutskever et al., 2014), evaluating computer programs (Zaremba
& Sutskever, 2014), generating image captions (Karpathy & Fei-Fei, 2015; Hossain et al., 2019),
generating source code (Karpathy, 2015), rainfall-runoff modeling (Kratzert et al., 2018, 2019),
or hydrological models for flooding warnings (Nearing et al., 2024). In reinforcement learning,
LSTMs are the best performing sequence models, e.g., the AlphaStar model for StarCraft II (Vinyals
et al., 2017), the OpenAI Five model for Dota 2 (Karpathy, 2019), and models of the magnetic
controller for nuclear fusion (Degrave et al., 2022). LSTMs excel at learning abstractions, i.e., adeptly
extracting semantic information and storing it in their memory cells (Karpathy, 2015), which for
example became evident by number and syntax neurons (Lakretz et al., 2019), linguistic neurons (Bau
et al., 2019), and sentiment neurons (Radford et al., 2017). LSTMs are still used in highly relevant
applications (Degrave et al., 2022; Nearing et al., 2024) and have stood the test of time.
Figure 2: LSTM limitations. Left: Nearest Neighbor Search
problem in terms of mean squared error (MSE). Given a
reference vector, a sequence is scanned sequentially for the
most similar vector with the objective to return its attached
value at sequence end. LSTM struggles to revise a stored
value when a more similar vector is found. Our new xLSTM
overcomes this limitation by exponential gating. Right : Rare
Token Prediction. The perplexity (PPL) of token predic-
tion on Wikitext-103, in buckets of token frequency. LSTM
performs worse on predicting rare tokens because of its lim-
ited storage capacities, whereas our new xLSTM solves this
problem via a matrix memory.Despite their tremendous successes,
LSTMs have three main limitations:
(i) Inability to revise storage deci-
sions. We exemplify this limitation
via the Nearest Neighbor Search prob-
lem (see also Appendix B): With a ref-
erence vector given, a sequence must
be scanned sequentially for the most
similar vector in order to provide its
attached value at sequence end. The
left panel of Figure 2 shows the mean
squared error at this task. LSTM strug-
gles to revise a stored value when a
more similar vector is found, while
our new xLSTM remediates this limi-
tation by exponential gating. (ii) Lim-
ited storage capacities, i.e., informa-
tion must be compressed into scalar
cell states. We exemplify this limita-
tion via Rare Token Prediction . In the
right panel of Figure 2, the perplex-
ity of token prediction on Wikitext-
103 (Merity et al., 2017) is given for
buckets of different token frequency.
LSTM performs worse on rare tokens because of its limited storage capacities. Our new xLSTM
solves this problem by a matrix memory. (iii) Lack of parallelizability due to memory mixing, i.e.,
the hidden-hidden connections between hidden states from one time step to the next, which enforce
sequential processing.
These limitations of LSTM have paved the way for the emergence of Transformers (Vaswani et al.,
2017) in language modeling. What performances can we achieve in language modeling when
overcoming these limitations and scaling LSTMs to the size of current Large Language Models?
22 Extended Long Short-Term Memory
To overcome the LSTM limitations, Extended Long Short-Term Memory (xLSTM) introduces two
main modifications to the LSTM idea of Equation (1). Those modifications  exponential gating
and novel memory structures  enrich the LSTM family by two members: (i) the new sLSTM (see
Section 2.2) with a scalar memory, a scalar update, and memory mixing, and (ii) the new mLSTM
(see Section 2.3) with a matrix memory and a covariance (outer product) update rule, which is fully
parallelizable. Both sLSTM and mLSTM enhance the LSTM through exponential gating. To enable
parallelization, the mLSTM abandons memory mixing, i.e., the hidden-hidden recurrent connections.
Both mLSTM and sLSTM can be extended to multiple memory cells, where sLSTM features memory
mixing across cells. Further, the sLSTM can have multiple heads without memory mixing across the
heads, but only memory mixing across cells within each head. This introduction of heads for sLSTM
together with exponential gating establishes a new way of memory mixing. For mLSTM multiple
heads and multiple cells are equivalent.
Integrating these new LSTM variants into residual block modules results in xLSTM blocks (see
Section 2.4). Residually stacking those xLSTM blocks in architectures provides xLSTM architectures
(see Section 2.4). See Figure 1 for the xLSTM architecture with its components.
2.1 Review of the Long Short-Term Memory
The original LSTM idea (Hochreiter, 1991; Hochreiter & Schmidhuber, 1997b,a) introduced the
scalar memory cell as a central processing and storage unit that avoids vanishing gradients (Hochreiter,
1991; Hochreiter et al., 2000) through the constant error carousel (the cell state update). The memory
cell contains three gates: input, output, and forget gate. The forget gate has been introduced by Gers
et al. (2000). The LSTM memory cell update rules at time step tare:
ct=ftct1+itzt cell state (2)
ht=otht, ht=ψ
ct
hidden state (3)
zt=φ(zt), zt=w
zxt+rzht1+bz cell input (4)
it=σit
, it=w
ixt+riht1+bi input gate (5)
ft=σ
ft
, ft=w
fxt+rfht1+bf forget gate (6)
ot=σ( ot),  ot=w
oxt+roht1+bo output gate (7)
The weight vectors wz,wi,wf, andwocorrespond to the input weight vectors between inputs xt
and cell input, input gate, forget gate, and output gate, respectively. The weights rz,ri,rf, and ro
correspond to the recurrent weights between hidden state ht1and cell input, input gate, forget gate,
and output gate, respectively. bz,bi,bf, and boare the corresponding bias terms. φandψare the
cell input and hidden state activation functions (typically tanh ).ψis used to normalize or squash
the cell state, which would be unbounded otherwise. All gate activation functions are sigmoid, i.e.,
σ(x) = 1 /(1 + exp( x)). In later formulations, multiple memory cells were combined in a vector,
which allows the usage of recurrent weight matrices to mix the cell outputs of memory cells (Greff
et al., 2015), for more details see Appendix A.1. Ablation studies showed that all components of the
memory cell are crucial (Greff et al., 2015).
2.2 sLSTM
To empower LSTMs with the ability to revise storage decisions, we introduce exponential gates
(red) together with normalization and stabilization. In particular, input and forget gates can have
exponential activation functions. For normalization, we introduce a normalizer state that sums up the
product of input gate times all future forget gates.
3The sLSTM forward pass is:
ct=ftct1+itzt cell state (8)
nt=ftnt1+it normalizer state (9)
ht=otht, ht=ct/nt hidden state (10)
zt=φ(zt), zt=w
zxt+rzht1+bz cell input (11)
it=expit
, it=w
ixt+riht1+bi input gate (12)
ft=σ
ft
ORexp
ft
,ft=w
fxt+rfht1+bf forget gate (13)
ot=σ( ot),  ot=w
oxt+roht1+bo output gate (14)
We broadcast the original LSTM gating techniques, i.e., input- and/or hidden-dependent gating plus
bias term, to the new architectures. Exponential activation functions can lead to large values that cause
overflows. Therefore, we stabilize gates with an additional state mt(Milakov & Gimelshein, 2018) :
mt= max
log(ft) +mt1,log(it)
stabilizer state (15)
i
t=exp
log (i t)mt
=exp
itmt
stabil. input gate (16)
f
t=exp
log (f t) +mt1mt
stabil. forget gate (17)
We show in Appendix A.2, that replacing ftbyf
tanditbyi
tin the forward pass does neither change
the output of the whole network nor the derivatives of the loss with respect to the parameters.
New Memory Mixing. sLSTM can have multiple memory cells like the original LSTM (see
Appendix A.2). Multiple memory cells enable memory mixing via recurrent connections Rz,Ri,
Rf,Rofrom hidden state vector hto memory cell input zand the gates i,f,o, respectively. A new
aspect in memory mixing is the effect of exponential gating. The new sLSTM can have multiple
heads with memory mixing within each head but not across heads. The introduction of heads for
sLSTM together with exponential gating establishes a new way of memory mixing.
2.3 mLSTM
To enhance storage capacities of LSTMs, we increase the LSTM memory cell from a scalar cRto
a matrix CRdd. Hence, retrieval is performed via a matrix multiplication. At time t, we want to
store a pair of vectors, the key ktRdand the value vtRd(we use the Transformer terminology).
Later at time t+τ, the value vtshould be retrieved by a query vector qt+τRd. This is the setting
of Bidirectional Associative Memories (BAMs) (Kohonen, 1972; Anderson, 1972; Nakano, 1972;
Anderson et al., 1977). The covariance update rule (Sejnowski, 1977; Dayan & Willshaw, 1991) for
storing a key-value pair is
Ct=Ct1+vtk
t. (18)
We assume a layer-norm before projecting inputs to keys and values, therefore they have zero mean.
The covariance update rule is optimal (Dayan & Willshaw, 1991) for a maximal separability of
retrieved binary vectors, which is equivalent to a maximal signal/noise ratio. Higher separability is
possible when limiting retrieval to pairwise interactions and conceding quadratic complexity like
attention (Krotov & Hopfield, 2016, 2017; Ramsauer et al., 2021). The covariance update rule is
equivalent to Fast Weight Programmers (Schmidhuber, 1992; Schlag et al., 2021), which have later
been equipped with a constant decay rate multiplied to Ct1and a constant learning rate multiplied
tovtk
t(Ba et al., 2016a). In this spirit, we integrate the covariance update rule into the LSTM
framework, where the forget gate corresponds to decay rate and the input gate to the learning rate,
while the output gate scales the retrieved vector.
For this matrix memory, the normalizer state is the weighted sum of key vectors, where each key
vector is weighted by the input gate and all future forget gates. Again, the normalizer state keeps
4record of the strength of the gates. Since the dot product between query and normalizer state can
be close to zero, we use the absolute value of this dot product and lower bound it by a threshold
(typically 1.0) as done previously (Sun et al., 2023). The mLSTM forward pass is:
Ct=ftCt1+itvtk
t cell state (19)
nt=ftnt1+itkt normalizer state (20)
ht=otht, ht=Ctqt/maxnn
tqt,1o
hidden state (21)
qt=Wqxt+bq query input (22)
kt=1
dWkxt+bk key input (23)
vt=Wvxt+bv value input (24)
it=expit
, it=w
ixt+bi input gate (25)
ft=σ
ft
ORexp
ft
,ft=w
fxt+bf forget gate (26)
ot=σ(ot), ot=Woxt+bo output gate (27)
mLSTM can have multiple memory cells like the original LSTM. For mLSTM, multiple heads and
multiple cells are equivalent as there is no memory mixing. In order to stabilize the exponential gates
of mLSTM, we use the same stabilization techniques as for sLSTM, see Equation (15). Since the
mLSTM has no memory mixing, this recurrence can be reformulated in a parallel version. For more
details we refer to Appendix A.3.
2.4 xLSTM Architecture
Figure 3: xLSTM blocks. Left: A residual sLSTM block
with post up-projection (like Transformers): The input is fed
into an sLSTM  with an optional convolution  followed
by a gated MLP. Right : A residual mLSTM block with
pre up-projection (like State Space models): mLSTM is
wrapped inside two MLPs, via a convolution, a learnable
skip connection, and an output gate that acts component-
wise. See Figure 9 and Figure 10 in the appendix for details.xLSTM Blocks. An xLSTM block
should non-linearly summarize the
past in a high-dimensional space to
better separate different histories or
contexts. Separating histories is the
prerequisite to correctly predict the
next sequence element such as the
next token. We resort to Covers
Theorem (Cover, 1965), which states
that in a higher dimensional space
non-linearly embedded patterns can
more likely be linearly separated
than in the original space. We con-
sider two residual block architectures:
(i) A residual block with post up-
projection (like Transformers), which
non-linearly summarizes the past in
the original space, then linearly maps
into a high-dimensional space, applies
a non-linear activation function, and
linearly maps back to the original
space; see left panel of Figure 3 and third column in Figure 1. A more detailed version is de-
picted in Figure 9 in the appendix. (ii) A residual block with pre up-projection (like State Space
Models), which linearly maps to a high-dimensional space, non-linearly summarizes the past in the
high-dimensional space and then linearly maps back to the original space. For an xLSTM block
containing an sLSTM, we mostly use the post up-projection block. For an xLSTM block containing
an mLSTM, we use the pre up-projection block since the memory capacity becomes larger in the
high-dimensional space. See left panel of Figure 3 and third column in Figure 1, or Figure 9 in the
appendix for more details.
5xLSTM Architecture. An xLSTM architecture is constructed by residually stacking build-
ing blocks (Srivastava et al., 2015; He et al., 2016). We rely on the most commonly used pre-
LayerNorm (Ba et al., 2016b) residual backbones as used in contemporary Large Language Models.
See last column in Figure 1.
2.5 Memory and Speed Considerations
Contrary to Transformers, xLSTM networks have a linear computation and a constant memory
complexity with respect to the sequence length. Since the xLSTM memory is compressive, it is well
suited for industrial applications and implementations on the edge.
The memory of mLSTM does not require parameters but is computationally expensive through its dd
matrix memory and ddupdate. We trade off memory capacity against computational complexity.
Nevertheless, the computations can be done in parallel on GPUs, therefore these computations have
only a minor effect on the wall clock time.
While mLSTM is parallelizable analog to FlashAttention (Dao et al., 2022; Dao, 2024) or GLA (Yang
et al., 2023), sLSTM is not parallelizable due to the memory mixing (hidden-hidden connections).
However, we developed a fast CUDA implementation with GPU memory optimizations to the register
level which is typically less than two times slower than mLSTM.
3 Related Work
Linear Attention. Several methods have been suggested to overcome the quadratic complexity
in terms of context length of the Transformer and make attention linear in the context length. The
Synthesizer learns synthetic attention weights without token-token interactions (Tay et al., 2020).
Linformer realizes self-attention by a low-rank matrix and even linearly approximates it (Wang
et al., 2020). Linear Transformer linearizes the attention mechanism (Katharopoulos et al., 2020).
Performer linearly approximates the attention softmax by positive orthogonal random features
approach (Choromanski et al., 2021). Attention has been replaced by fast long convolutions in the
Structured Global Convolution (SGConv) (Li et al., 2022) and the Hyena Hierarchy (Poli et al., 2023).
State Space Models. Recently, State Space Models (SSMs) became very popular since they are
linear in the context length and show promising performance compared to Transformers. One of the
first proposed models was Structured State Space sequence model (S4) (Gu et al., 2021), followed by
Diagonal State Space (DSS) model (Gupta et al., 2022), Gated State Space (GSS) models (Mehta
et al., 2022), S5 model (Smith et al., 2022), Bidirectional Gated SSM (BiGS) (Wang et al., 2022), H3
model (Fu et al., 2023), and Mamba (Gu & Dao, 2023).
Recurrent Neural Networks. Recurrent Neural Networks (RNNs) have been suggested to replace
Transformer and attention due to their linearity in the context length. RNNs with Deep Linear
Recurrent Units (LRUs) showed promising results for language modeling (Orvieto et al., 2023; De
et al., 2024), as did Hierarchically Gated Linear RNN (HGRN) (Qin et al., 2023) and HGRN2 (Qin
et al., 2024). A well-known RNN approach to large language modeling is RWKV (Peng et al., 2023,
2024), showcasing competitive performance to Transformers.
Gating. One of the key ideas of LSTM is gating, which was rediscovered and reinterpreted in many
recent approaches. Gating was used in HGRN (Qin et al., 2023), HGRN2 (Qin et al., 2024), Gated
Linear Attention (GLA) (Yang et al., 2023), Gated State Space (GSS) models (Mehta et al., 2022),
Bidirectional Gated SSM (BiGS) (Wang et al., 2022), Moving Average Equipped Gated Attention
(MEGA) (Ma et al., 2022), RWKV (Peng et al., 2023), and Mamba (Gu & Dao, 2023).
Covariance Update Rule. To enhance storage capacities, we equipped the mLSTM cell with
a matrix memory with a covariance update rule. Other methods which build on such an update
mechanism are Fast Weight Programmers (Schmidhuber, 1992; Schlag et al., 2021), RWKV-5 and
RWKV-6 (Peng et al., 2024), Retention (Sun et al., 2023), Linear Transformer (Katharopoulos et al.,
2020), and HGRN2 (Qin et al., 2024).
6Most Related. Conceptually the closest models to xLSTM are Retention (Sun et al., 2023),
RWKV (Peng et al., 2023, 2024), and HGRN2 (Qin et al., 2024). These models share the con-
cepts matrix memory and/or gating. However, in contrast to the new sLSTM, these approaches do
not allow memory mixing. Memory mixing enables to solve state tracking problems, and therefore
LSTMs are more expressive than State Space Models (SSMs) and Transformers (Merrill et al., 2024;
Delétang et al., 2023). State tracking is required to evaluate code or to track entities in a long narrative.
Residually Stacking Architectures. Like almost all contemporary large deep learning models,
xLSTM architectures are constructed by residually stacking building blocks (Srivastava et al., 2015;
He et al., 2016). This construction enabled deep convolutional networks (He et al., 2016) and
Transformers (Vaswani et al., 2017). Transformers are the ultimate force behind Large Language
Models (LLMs) like GPT-3 (Brown et al., 2020), ChatGPT (Schulman et al., 2022), GPT-4 (Achiam
et al., 2023), Megatron-LM (Shoeybi et al., 2019), Gopher (Rae et al., 2021), ERNIE 3.0 Titan (Wang
et al., 2021), GLaM (Du et al., 2021), Chinese M6 (Lin et al., 2021), mutilingual AlexaTM
20B (Soltan et al., 2022), OPT (Zhang et al., 2022), Chinchilla (Hoffmann et al., 2022), BLOOM (Scao
et al., 2022), GLM-130B (Zeng et al., 2022), LaMDA (Thoppilan et al., 2022), PaLM (Chowdhery
et al., 2022), Llama (Touvron et al., 2023), Gemini (Google, 2023; Reid et al., 2024).
4 Experiments
In this section, we experimentally evaluate xLSTM and compare it to existing methods with a focus on
language modeling. We investigate xLSTMs specific capabilities on synthetic tasks in Section 4.1. In
Section 4.2, we compare the validation set perplexity of various current language modeling methods
that were trained on 15B tokens from SlimPajama (Soboleva et al., 2023). On the same dataset, we
perform ablation studies for xLSTM. Then, we assess the scaling behavior of the different methods
analogous to Kaplan et al. (2020) and Brown et al. (2020). In Section 4.3, we conduct a more
thorough language modeling experiment. We compare xLSTM and the best performing methods from
Section 4.2 after being trained on 300B tokens from SlimPajama (Soboleva et al., 2023). First, we
assess how well the methods perform in extrapolating to longer contexts, secondly we test the methods
via validation perplexity and performance on downstream tasks (Sutawika et al., 2024), thirdly we
evaluate the methods on 571 text domains of the PALOMA language benchmark dataset (Magnusson
et al., 2023), fourthly we again assess the scaling behavior of the different methods, but now with 20
times more training data.
For all experiments, we use the notation xLSTM[ a:b] for the ratio a/bof mLSTM-based versus
sLSTM-based xLSTM blocks. For example, xLSTM[7:1] means that out of eight blocks, seven are
mLSTM-based blocks and one is an sLSTM-based block. For a common total block number of 48,
this translates to 6 sLSTM-based blocks and 42 mLSTM-based blocks. Further, for all experiments,
we use pre and post up-projection blocks for mLSTM and sLSTM, respectively.
4.1 Synthetic Tasks and Long Range Arena
First, we test the effectiveness of xLSTMs new exponential gating with memory mixing on formal
languages (Delétang et al., 2023). Then, we assess the effectiveness of xLSTMs new matrix memory
on the Multi-Query Associative Recall task (Arora et al., 2023). Finally, xLSTMs performance at
processing long sequences in the Long Range Arena is evaluated (Tay et al., 2021).
Test of xLSTMs Exponential Gating with Memory Mixing. We test xLSTMs new exponential
gating with memory mixing, which should enable it to solve state tracking problems (Merrill
et al., 2024; Merrill & Sabharwal, 2023). We implement and extend the formal language tasks
from Delétang et al. (2023) to enable multi-length training for length extrapolation. For a detailed
description of all tasks and extended results see Appendix B.1.1. We compare xLSTM to other
methods including Transformers, State Space Models, and Recurrent Neural Networks. The accuracy
of the tested methods is evaluated on those tokens relevant to the task. The accuracy is scaled between
0 (random) and 1 (perfect). We compare 2-block architectures of the following methods on these
tasks: xLSTM[0:1] (i.e., only sLSTM), xLSTM[1:0] (i.e., only mLSTM), xLSTM[1:1], Llama,
Mamba, RWKV , Retention, Hyena, LSTM, and LSTM in Transformer blocks (LSTM (Block)). The
results of this experiment are shown in Figure 4. Models such as Transformers or State Space Models
without memory mixing (no state tracking) cannot solve e.g. regular grammars like the parity task.
7Bucket SortMissing
DuplicateMod
Arithmetic
(w Brackets)Solve
Equation Cycle Nav Even PairsMod
Arithmetic
(w/o Brackets) Parity MajorityMajority
Count
Llama
Mamba
Retention
Hyena
RWKV-4
RWKV-5
RWKV-6
LSTM
(Block)
LSTM
xLSTM[0:1]
xLSTM[1:0]
xLSTM[1:1]0.92
0.020.08
0.00.02
0.00.02
0.00.04
0.011.0
0.00.03
0.00.03
0.010.37
0.010.13
0.0
0.69
0.00.15
0.00.04
0.010.05
0.020.86
0.041.0
0.00.05
0.020.13
0.020.69
0.010.45
0.03
0.13
0.010.03
0.00.03
0.00.03
0.00.05
0.010.51
0.070.04
0.00.05
0.010.36
0.00.12
0.01
0.3
0.020.06
0.020.05
0.00.02
0.00.06
0.010.93
0.070.04
0.00.04
0.00.36
0.010.18
0.02
0.54
0.00.21
0.010.06
0.00.07
0.00.13
0.01.0
0.00.07
0.00.06
0.00.63
0.00.13
0.0
0.49
0.040.15
0.010.08
0.00.08
0.00.26
0.051.0
0.00.15
0.020.06
0.030.73
0.010.34
0.03
0.96
0.00.23
0.060.09
0.010.09
0.020.31
0.141.0
0.00.16
0.00.22
0.120.76
0.010.24
0.01
0.99
0.00.15
0.00.76
0.00.5
0.050.97
0.031.0
0.00.91
0.091.0
0.00.58
0.020.27
0.0
0.94
0.010.2
0.00.72
0.040.38
0.050.93
0.071.0
0.01.0
0.01.0
0.00.82
0.020.33
0.0
0.84
0.080.23
0.010.57
0.090.55
0.091.0
0.01.0
0.01.0
0.01.0
0.00.75
0.020.22
0.0
0.97
0.00.33
0.220.03
0.00.03
0.010.86
0.011.0
0.00.04
0.00.04
0.010.74
0.010.46
0.0
0.7
0.210.2
0.010.15
0.060.24
0.040.8
0.031.0
0.00.6
0.41.0
0.00.64
0.040.5
0.0Context SentsitiveDeterministic
Context Free RegularFigure 4: Test of xLSTMs exponential gating with memory mixing. Results are given by the scaled
accuracy of different models at solving formal language tasks, of which some require state tracking.
The different tasks are grouped by the Chomsky hierarchy.
This result is in agreement with findings that Transformers and State Space models are fundamentally
less powerful than RNNs (Merrill et al., 2024; Merrill & Sabharwal, 2023; Delétang et al., 2023).
Test of xLSTMs Memory Capacities on Associative Recall Tasks. In this experiment, we test
xLSTMs new matrix memory in terms of the memory capacity on the Multi-Query Associative
Recall task (Arora et al., 2023): For each sequence, key-value pairs are randomly chosen from a
large vocabulary, which must be memorized for later retrieval. To enhance the difficulty of the
original task, we increase the number of key-value pairs up to 256 and extend the context length up
to 2048. Thus, we have broader tests for the memory capacities of different models. We compare
2-block architectures of Llama, Mamba, RWKV-5, RWKV-6, xLSTM[1:1] and xLSTM[1:0]. The
models are evaluated by the accuracy at recalling the pairs. Since Transformers (e.g. Llama) have
a memory that is exponential in the coding dimension (Ramsauer et al., 2021), they constitute the
gold standard at this task. Results are shown in Figure 5. xLSTM[1:1] performs best among all
non-Transformer models, also for small models. Interestingly, the sLSTM block does not diminish
the memory capacity but rather leverages it, which becomes evident at the most difficult task with 256
key-value pairs. Additional results are presented in Appendix B.1.2, where extrapolation analyses
indicate that xLSTMs enhanced memory capacities also pertain when extrapolating to contexts that
are longer than those seen during training.
32 64 128 256 512
Model Dim0.000.250.500.751.00Accuracy
KV Pairs = 48
32 64 128 256 512
Model Dim
KV Pairs = 96
32 64 128 256 512
Model Dim
KV Pairs = 256Llama Mamba RWKV-5 RWKV-6 xLSTM[1:0] xLSTM[1:1]
Figure 5: Test of memory capacities of different models at the Multi-Query Associative Recall task
with context length 2048. Each panel is dedicated to a different number of key-value pairs. The
x-axis displays the model size and the y-axis the validation accuracy.
8Test of xLSTMs Long Context Capabilities on Long Range Arena. To assess xLSTMs per-
formance on long sequences and large contexts, we compare different methods on the Long Range
Arena (Tay et al., 2021). xLSTM demonstrates consistent strong performance on all of the tasks,
suggesting that the xLSTM architecture is remarkably efficient in handling different aspects of long
context problems. For more details, see Appendix B.1.3.
4.2 Method Comparison and Ablation Study
The main question of this paper is, what can we achieve in language modeling when scaling up the
new LSTM variants. Therefore, we train xLSTMs, Transformers, State Space Models, and other
methods on 15B tokens from SlimPajama in an auto-regressive language modeling setting. We
compare the trained models on the validation set. Finally, we perform ablation studies for xLSTM.
Model#Params
MSlimPajama
(15B) ppl 
GPT-3 356 14.26
Llama 407 14.25
H3 420 18.23
Mamba 423 13.70
Hyena 435 17.59
RWKV-4 430 15.62
RWKV-5 456 16.53
RWKV-6 442 17.40
RetNet 431 16.23
HGRN 411 21.83
GLA 412 19.56
HGRN2 411 16.77
xLSTM[1:0] 409 13.43
xLSTM[7:1] 408 13.48
Table 1: Method comparison on next
token prediction when trained on 15B
tokens from SlimPajama. Best valida-
tion perplexities within model classes, i.e.,
Transformers, LSTMs, SSMs, RNNs, and
linear Transformers are underlined and
overall best is in bold. For each model
class, the best performing methods are
later used in Section 4.3 for LLM training.
xLSTMs with new memory (xLSTM[1:0]
and xLSTM[7:1]) perform best.Comparing xLSTM to Other Methods. For com-
parison, we train models on 15B tokens from SlimPa-
jama (Soboleva et al., 2023). The trained models
are evaluated by their perplexity on the validation
set. We compare the following methods: xLSTM
(our new method), GPT-3 (Transformer) (Brown et al.,
2020), Llama (Transformer) (Touvron et al., 2023), H3
(SSM) (Fu et al., 2023), Mamba (SSM) (Gu & Dao,
2023), RWKV-4 (RNN) (Peng et al., 2023), RWKV-5
(RNN) (Peng et al., 2024), RWKV-6 (RNN) (Peng et al.,
2024), GLA (linear Transformer) (Yang et al., 2023),
HGRN (RNN) (Qin et al., 2023), HGRN2 (RNN) (Qin
et al., 2024). RetNet (linear Transformer) (Sun et al.,
2023), Hyena (linear Transformer) (Poli et al., 2023),
xLSTM[1:0], and xLSTM[7:1] (see Section 4). The
models were trained with mixed precision, except
RWKV-5, RWKV-6, GLA, HGRN, HGRN2, where
mixed-precision training was not supported by the ref-
erence implementation. We categorize the methods into
(a) Transformers, (b) State Space Models (SSMs), and
(c) Recurrent Neural Networks (RNNs) together with lin-
ear Transformers. Linear Transformers are linear meth-
ods that substitute the Transformer attention mechanism.
The models match a GPT-3 model with 350M param-
eters in size, i.e. embedding dim 1024 and 24 residual
blocks. Only GPT-3 uses shared weights for token and
output embeddings, therefore has fewer parameters. The
results in Table 1 show that xLSTM outperforms all ex-
isting methods in validation perplexity. For details see
Appendix B.2. Figure 6 shows the scaling behaviour
for this experiment, indicating that xLSTM will also
perform favorably for larger models.
Ablation Studies. Table 1 and Figure 6 demonstrate that xLSTM achieves excellent results at
language modeling when being trained on 15B tokens from SlimPajama. Thus, it is only natural
to ask which of the elements of xLSTM is responsible for the improvements over vanilla LSTM
performances, evoking an ablation study of the individual new xLSTM components. For doing
so, we morph a vanilla LSTM architecture step-by-step into an xLSTM architecture. First, we
integrate LSTM layers into pre-LayerNorm residual backbones, second we extend this to a post
up-projection block, then we add exponential gating, and finally the matrix memory. The results are
shown in Table 2 (top). The ablation studies attribute the strong performance improvement to both
the exponential gating and the matrix memory. Additionally, since gating is an ever-occuring topic
in RNNs and State Space Models, we ablate different gating mechanisms. In Table 2 (bottom), we
conclude that having each gate learnable and influenced by the input has an incremental positive
effect. Additional studies on the individual backbone components are discussed in Appendix B.2.
90.2 0.4 1.0 1.4 2.7
Number of Parameters 10910111213141516171819Validation Perplexity
Llama
Mamba
RWKV-4
xLSTM[7:1]
xLSTM[1:0]
15B T okensFigure 6 : Method comparison
on next token prediction when
trained on 15B tokens from
SlimPajama. Performance mea-
sure in validation perplexity for
the best methods of each model
class (see Table 1) are reported.
The performance degradation of
xLSTM[7:1] at 2.7B is due to
initially slower training conver-
gence that leads to an especially
undertrained model. xLSTM is
the best method at all sizes.
Ablation studies on the new xLSTM components.
Model ModificationExponential
GatingMatrix
Memory#Params
MSlimPajama
(15B) ppl 
LSTMVanilla Multi-Layer LSTM   607.8 2417.86
Adding Resnet Backbone   506.1 35.46
Adding Up-Projection Backbone   505.9 26.01
xLSTM[0:1] Adding Exponential Gating   427.3 17.70
xLSTM[7:1] Adding Matrix Memory   408.4 13.48
Ablation studies on different gating techniques.
Learnable GatesForget Gate Input GateSlimPajama
(15B) ppl  Input
DependentLearnable
BiasBias
InitInput
DependentLearnable
BiasBias
Init
No Gates   +   0 NaN
No Gates   [3,6]   0 13.95
Forget Gate   [3,6]   0 13.58
Input Gate   [3,6]   N(0,0.1) 13.69
Forget Gate Bias   [3,6]   0 13.76
Forget + Input Gate Bias   [3,6]   N(0,0.1) 13.73
Forget Gate + Input Gate Bias   [3,6]   N(0,0.1) 13.55
Forget Gate + Input Gate   [3,6]   N(0,0.1) 13.43
Table 2: Ablation studies. Top: Ablation studies on the new xLSTM components, contributing
the strong performance improvement of xLSTM over vanilla LSTM to both the exponential gating
and the matrix memory. Bottom: Ablation studies on different gating techniques. We consider an
xLSTM[1:0] with sigmoid forget gate and exponential input gate. Bias initialization means that the
forget gate is set to one, [3,6]indicates that values are taken equidistant in the respective interval, and
N(0,0.1)that values are randomly chosen from a Gaussian with mean 0and std 0.1. PPL denotes
validation perplexity. The first two lines correspond to models similar to linearized attention, line
four to Retention, line five to RWKV-5, and line six to RWKV-6. Dependencies of the gates on the
input lead to better performance.
104.3 xLSTM as Large Language Model
We culminate this study in large-scale language modeling experiments, testing the potential of
xLSTM as an LLM. We therefore increase the amount of training data and train on 300B tokens from
SlimPajama. The same number of tokens is used in e.g., Mamba (Gu & Dao, 2023) and Griffin (De
et al., 2024). We compare xLSTM, RWKV-4, Llama, and Mamba, which were selected as the
best-performing methods in their respective method classes in the model comparison in Section 4.2.
We train different model sizes (125M, 350M, 760M, 1.3B), test all models for length extrapolation
capabilities and evaluate their performance on the validation set. We assess their performance on
downstream tasks, test their performance in language modeling on 471 text domains of the PALOMA
benchmark, and, finally, investigate their scaling law behavior.
Sequence Length Extrapolation. First, we test the sequence length extrapolation for 1.3B-sized,
large models of xLSTM, RWKV-4, Llama, and Mamba. All models are trained on context length
2048, and then tested for context lengths up to 16384. See Figure 7 for the results. In contrast to
other methods, xLSTM models maintain low perplexities for longer contexts.
ModelSlimPajama
(300B) ppl 
at 16k
Llama 337.83
Mamba 14.00
RWKV-4 13.75
xLSTM[7:1] 8.92
xLSTM[1:0] 9.01
Figure 7: Sequence extrapolation in language modeling. This is a comparison of 1.3B-sized, large
models of xLSTM, RWKV-4, Llama, and Mamba at next token prediction on the SlimPajama
validation set after training on 300B tokens from SlimPajama. Models are trained with context length
2048 and then tested for context lengths up to 16384. Left: Token perplexities evaluated at different
context lengths. In contrast to other methods, xLSTM models remain at low perplexities for longer
contexts. Right: Prediction quality when extrapolating to long context sizes in terms of validation
perplexity (PPL). xLSTM yields the best PPL values (best in bold, second best underlined).
Validation Perplexity and Downstream Tasks. Secondly, for all model sizes, we evaluate the
performance of xLSTM, RWKV-4, Llama, and Mamba models on the SlimPajama validation set for
next token prediction and on downstream tasks that measure common sense reasoning. The third
column of Table 3 lists the validation set perplexities of different methods. Both xLSTM[1:0] and
xLSTM[7:1] are the best models for all model sizes with respect to the validation set perplexity. The
other columns of Table 3 provide the performance on downstream tasks. In the vast majority of tasks
and across all model sizes xLSTM is the best method  only on the ARC task Mamba is in some
cases the best method. For details see Appendix B.3.
Performance on PALOMA Language Tasks. Thirdly, for all model sizes, we test the next token
prediction performance of xLSTM, RWKV-4, Llama, and Mamba models on PALOMA language
tasks (Magnusson et al., 2023). We measure the performance by the perplexity for next token
prediction on 571 text domains, which range from nytimes.com to r/depression on Reddit. Table 4
shows token prediction perplexity grouped into language modeling (first seven columns) and fine-
grained domain benchmarks (last 5 columns). xLSTM[1:0] performs better than xLSTM[7:1] on
these language tasks. xLSTM[1:0] has in 568 out of 571 (99.5%) text domains a lower perplexity
than Mamba, in 486 out of 571 (85.1%) a lower perplexity than Llama, in 570 out of 571 (99.8%) a
lower perplexity than RWKV-4. For details see Appendix B.3.
11Model#Params
MSlimPajama
(300B) ppl LAMBADA
pplLAMBADA
accHellaSwag
accPIQA
accARC-E
accARC-C
accWinoGrande
accAverage
acc125MRWKV-4 169.4 16.66 54.72 23.77 34.03 66.00 47.94 24.06 50.91 41.12
Llama 162.2 15.89 39.21 31.54 34.09 65.45 45.33 23.63 50.67 41.78
Mamba 167.8 15.08 27.76 34.14 36.47 66.76 48.86 24.40 51.14 43.63
xLSTM[1:0] 163.8 14.63 25.98 36.52 36.74 65.61 47.81 24.83 51.85 43.89
xLSTM[7:1] 163.7 14.60 26.59 36.08 36.75 66.87 48.32 25.26 51.70 44.16350MRWKV-4 430.5 12.62 21.57 36.62 42.47 69.42 54.46 25.43 51.22 46.60
Llama 406.6 12.19 15.73 44.19 44.45 69.15 52.23 26.28 53.59 48.32
Mamba 423.1 11.64 12.83 46.24 47.55 69.70 55.47 27.56 54.30 50.14
xLSTM[1:0] 409.3 11.31 11.49 49.33 48.06 69.59 55.72 26.62 54.38 50.62
xLSTM[7:1] 408.4 11.37 12.11 47.74 47.89 71.16 56.61 27.82 53.28 50.75760MRWKV-4 891.0 10.55 10.98 47.43 52.29 72.69 58.84 28.84 55.41 52.58
Llama 834.1 10.60 9.90 51.41 52.16 70.95 56.48 28.75 56.67 52.74
Mamba 870.5 10.24 9.24 50.84 53.97 71.16 60.44 29.78 56.99 53.86
xLSTM[1:0] 840.4 9.86 8.09 54.78 55.72 72.69 62.75 32.59 58.17 56.12
xLSTM[7:1] 839.7 9.91 8.07 55.27 56.12 72.74 61.36 29.61 56.43 55.261.3BRWKV-4 1515.2 9.83 9.84 49.78 56.20 74.70 61.83 30.63 55.56 54.78
Llama 1420.4 9.44 7.23 57.44 57.81 73.12 62.79 31.74 59.04 56.99
Mamba 1475.3 9.14 7.41 55.64 60.45 74.43 66.12 33.70 60.14 58.41
xLSTM[1:0] 1422.6 8.89 6.86 57.83 60.91 74.59 64.31 32.59 60.62 58.48
xLSTM[7:1] 1420.1 9.00 7.04 56.69 60.26 74.92 65.11 32.34 59.27 58.10
Table 3: Validation set perplexity and downstream tasks. Comparison of xLSTM, RWKV-4, Llama,
and Mamba on the validation set at next token prediction and on downstream tasks after training on
300B tokens from SlimPajama. Model sizes are 125M, 250M, 760M, and 1.3B. The first column
shows the methods and the second the actual number of parameters. The third column lists the
validation set perplexities, while the remaining columns show the performance on downstream tasks.
Best model per model size is depicted bold and the second best is underlined. In the vast majority of
tasks and across all model sizes xLSTM is the best method  only on the ARC task Mamba is in
some cases the best method. xLSTM[1:0] and xLSTM[7:1] are the two best models with respect to
validation set perplexity.
Model#Params
MC4MC4
ENWikitext
103Penn
TreebankRed
PajamaRefined
WebDolmaM2D2
S2ORCM2D2
WikipediaC4
DomainsDolma
SubredditsDolma
CodingAverage125MRWKV-4 169.4 26.25 22.33 29.18 38.45 8.99 32.47 17.04 23.86 21.42 22.68 37.08 5.12 23.74
Llama 162.2 24.64 17.23 23.16 31.56 8.26 29.15 15.10 19.71 20.41 21.45 36.73 3.61 20.92
Mamba 167.8 23.12 17.04 22.49 30.63 7.96 27.73 14.60 19.38 19.36 20.14 34.32 3.77 20.05
xLSTM[1:0] 163.8 22.54 16.32 21.98 30.47 7.80 27.21 14.35 19.02 19.04 19.65 34.15 3.64 19.68
xLSTM[7:1] 163.7 22.39 16.13 21.47 30.01 7.75 26.91 14.13 18.6 18.84 19.52 33.9 3.59 19.44350MRWKV-4 430.5 19.55 15.82 19.64 27.58 6.97 24.28 12.94 17.59 15.96 16.98 29.40 3.90 17.55
Llama 406.6 18.38 13.28 16.41 21.82 6.56 22.09 11.76 15.05 15.25 15.99 28.30 3.12 15.67
Mamba 423.1 17.33 13.05 16.11 22.24 6.34 21.04 11.42 14.83 14.53 15.16 27.02 3.20 15.19
xLSTM[1:0] 409.3 17.01 12.55 15.17 22.51 6.20 20.66 11.16 14.44 14.27 14.85 26.70 3.08 14.88
xLSTM[7:1] 408.4 16.98 12.68 15.43 21.86 6.23 20.70 11.22 14.62 14.30 14.85 26.61 3.11 14.88760MRWKV-4 891.0 15.51 12.76 14.84 21.39 5.91 19.28 10.70 14.27 13.04 13.68 24.22 3.32 14.08
Llama 834.1 15.75 11.59 13.47 18.33 5.82 19.04 10.33 13.00 13.05 13.76 24.80 2.90 13.49
Mamba 870.5 15.08 11.54 13.47 19.34 5.69 18.43 10.15 13.05 12.62 13.25 23.94 2.99 13.30
xLSTM[1:0] 840.4 14.60 11.03 12.61 17.74 5.52 17.87 9.85 12.50 12.20 12.81 23.46 2.87 12.76
xLSTM[7:1] 839.7 14.72 11.11 12.68 17.61 5.55 18.01 9.87 12.59 12.25 12.89 23.43 2.88 12.801.3BRWKV-4 1515.2 14.51 12.04 13.73 19.37 5.62 18.25 10.11 13.46 12.10 12.87 22.85 3.25 13.18
Llama 1420.4 13.93 10.44 11.74 15.92 5.29 17.03 9.35 11.61 11.53 12.24 22.63 2.74 12.04
Mamba 1475.3 13.35 10.40 11.76 16.65 5.21 16.50 9.17 11.73 11.18 11.83 21.43 2.83 11.84
xLSTM[1:0] 1422.6 13.13 10.09 11.41 15.92 5.10 16.25 9.01 11.43 10.95 11.60 21.29 2.73 11.58
xLSTM[7:1] 1420.1 13.31 10.21 11.32 16.00 5.16 16.48 9.11 11.61 11.10 11.76 21.50 2.75 11.69
Table 4: Performance on PALOMA Language Modeling Tasks. Comparison of xLSTM, RWKV-4,
Llama, and Mamba by the perplexity of next token prediction on the PALOMA language benchmark
after training on 300B tokens from SlimPajama. Model sizes are 125M, 250M, 760M, and 1.3B.
The second column shows the actual number of parameters. The 571 text domains are grouped into
language modeling (next seven columns) and fine-grained domain benchmarks (further 5 columns).
The last column shows the average perplexity across all of these tasks. Best model per model size is
given in bold and the second best is underlined. xLSTM yields the best performance.
12Scaling Laws. Fourthly, we assess the power-law scaling behavior, which allows to extrapolate the
performance to larger model sizes (Kaplan et al., 2020; Brown et al., 2020). Figure 8 presents the
scaling behavior. All models share a similar scaling behavior but with different offsets. RWKV-4
performs worst, followed by Llama and Mamba. xLSTM is better than Mamba with a similar margin
to Mamba as Mamba has to Llama. The scaling behavior indicates that for larger models xLSTM
will continue to perform favourable compared to Transformers and State-Space models.
0.2 0.4 1.0 1.4
Number of Parameters 10991011121314151617Validation Perplexity
Llama
Mamba
RWKV-4
xLSTM[7:1]
xLSTM[1:0]
300B T okens
Figure 8: Scaling laws. Next token prediction perplexity of xLSTM, RWKV-4, Llama, and Mamba
on the SlimPajama validation set when trained on 300B tokens from SlimPajama. Model sizes are
125M, 350M, 760M, and 1.3B. Best models for each model class, see Table 1, were selected. The
scaling laws indicate that for larger models xLSTM will perform well, too.
5 Limitations
(i) In contrast to mLSTM, memory mixing of the sLSTM prohibits parallelizable operations, and
therefore does not allow a fast parallel implementation. Nevertheless, we developed a fast CUDA ker-
nel for sLSTM, which is currently around 1.5 times slower than our parallel mLSTM implementation.
(ii) The CUDA kernels for mLSTM are not optimized, and therefore the current implementation is
about 4 times slower than FlashAttention or the scan used in Mamba. Faster CUDA kernels could be
obtained in the vein of FlashAttention. (iii) The matrix memory of mLSTM has high computation
complexity since ddmatrices must be processed. Still, the memory update and retrieval does not
use parameters and can be parallelized using standard matrix operations, therefore the wall clock
time overhead due to the complex memory is minor. (iv) The initialization of the forget gates must be
chosen carefully. (v) Since the matrix memory is independent of the sequence length, increasing the
sequence length might overload the memory for longer context sizes. Still, this does not appear to be
a limitation for contexts up to 16k, see Section 4.3. (vi) Due to the expensive computational load for
large language experiments, we did neither fully optimize the architecture nor the hyperparameters,
especially for larger xLSTM architectures. We anticipate that an extensive optimization process is
needed for xLSTM to reach its full potential.
136 Conclusion
We have partly answered our simple question: How far do we get in language modeling when scaling
LSTM to billions of parameters? So far, we can answer: At least as far as current technologies
like Transformers or State Space Models. We have enhanced LSTM to xLSTM by exponential
gating with memory mixing and a new memory structure. xLSTM models perform favorably on
language modeling when compared to state-of-the-art methods like Transformers and State Space
Models. The scaling laws indicate that larger xLSTM models will be serious competitors to current
Large Language Models that are built with the Transformer technology. xLSTM has the potential to
considerably impact other deep learning fields like Reinforcement Learning, Time Series Prediction,
or the modeling of physical systems.
Acknowledgements
We thank Sebastian Lehner, Daniel Klotz, Thomas Adler, Matthias Dellago, Gerald Gutenbrunner,
Fabian Paischer, Vihang Patil, Niklas Schmidinger, Benedikt Alkin, Kajetan Schweighofer, Anna
Zimmel, Lukas Aichberger, Lukas Hauzenberger, Bernhard Schäfl, Johannes Lehner for helpful
discussions and feedback.
References
J. Achiam, S. Adler, S. Agarwal, et al. GPT-4 technical report. ArXiv , 2303.08774, 2023.
J. Anderson, J. Silverstein, S. Ritz, and R. Jones. Distinctive features, categorical perception, and
probability learning: Some applications of a neural model. Psychological Review , 84:413451,
1977. doi: 10.1037/0033-295X.84.5.413.
J. A. Anderson. A simple neural network generating an interactive memory. Mathematical Bio-
sciences , 14, 1972. doi: 10.1016/0025-5564(72)90075-2.
S. Arora, S. Eyuboglu, A. Timalsina, I. Johnson, M. Poli, J. Zou, A. Rudra, and C. Ré. Zoology:
Measuring and improving recall in efficient language models. ArXiv , 2312.04927, 2023.
J. Ba, G. E. Hinton, V . Mnih, J. Z. Leibo, and C. Ionescu. Using fast weights to attend to the recent
past. In D. D. Lee, M. Sugiyama, U. V . Luxburg, I. Guyon, and R. Garnett (eds.), Advances in
Neural Information Processing Systems 29 , pp. 43314339. Curran Associates, Inc., 2016a.
J. Ba, J. R. Kiros, and G. Hinton. Layer normalization. ArXiv , 1607.06450, 2016b.
A. Bau, Y . Belinkov, H. Sajjad, N. Durrani, F. Dalvi, and J. Glass. Identifying and controlling
important neurons in neural machine translation. In International Conference on Learning Repre-
sentations (ICLR) , 2019. URL https://openreview.net/forum?id=H1z-PsR5KX .
Y . Bisk, R. Zellers, R. LeBras, J. Gao, and Y . Choi. Piqa: Reasoning about physical commonsense in
natural language. In AAAI Conference on Artificial Intelligence , volume 34, pp. 74327439, 2020.
S. L. Blodgett, L. Green, and B. OConnor. Demographic dialectal variation in social media: A case
study of African-American English. In Conference on Empirical Methods in Natural Language
Processing , pp. 11191130, 2016. doi: 10.18653/v1/D16-1120.
T. Brown, B. Mann, N. Ryder, et al. Language models are few-shot learners. In H. Larochelle,
M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing
Systems , volume 33, pp. 18771901. Curran Associates, Inc., 2020.
K. M. Choromanski, V . Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlós, P. Hawkins, J. Q.
Davis, A. Mohiuddin, L. Kaiser, D. B. Belanger, L. J. Colwell, and A. Weller. Rethinking
attention with performers. In 9th International Conference on Learning Representations (ICLR) .
OpenReview.net, 2021. URL https://openreview.net/forum?id=Ua6zuk0WRH .
A. Chowdhery, S. Narang, J. Devlin, et al. PaLM: scaling language modeling with pathways. ArXiv ,
2204.02311, 2022.
14A. Chronopoulou, M. Peters, and J. Dodge. Efficient hierarchical domain adaptation for pretrained lan-
guage models. In Conference of the North American Chapter of the Association for Computational
Linguistics , pp. 13361351, 2022. doi: 10.18653/v1/2022.naacl-main.96.
P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have
solved question answering? Try ARC, the AI2 reasoning challenge. ArXiv , 1803.05457, 2018.
T. M. Cover. Geometrical and statistical properties of systems of linear inequalities with applications
in pattern recognition. Electronic Computers, IEEE Transactions on , EC-14(3):326334, 1965.
T. Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In In-
ternational Conference on Learning Representations (ICLR) , volume 12, 2024. URL https:
//openreview.net/forum?id=mZn2Xyh9Ec .
T. Dao, D. Y . Fu, S. Ermon, A. Rudra, and C. Ré. Flashattention: Fast and memory-efficient exact
attention with IO-awareness. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho (eds.), Advances
in Neural Information Processing Systems (NeurIPS) , 2022. URL https://openreview.net/
forum?id=H4DqfPSibmx .
P. Dayan and D. J. Willshaw. Optimising synaptic learning rules in linear associative memories.
Biological Cybernetics , 65, 1991. doi: 10.1007/bf00206223.
S. De, S. L. Smith, A. Fernando, A. Botev, G. Cristian-Muraru, A. Gu, R. Haroun, L. Berrada,
Y . Chen, S. Srinivasan, G. Desjardins, A. Doucet, D. Budden, Y . W. Teh, R. Pascanu, N. DeFreitas,
and C. Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language
models. ArXiv , 2402.19427, 2024.
J. Degrave, F. Felici, J. Buchli, et al. Magnetic control of tokamak plasmas through deep reinforcement
learning. Nature , 602:414419, 2022. doi: 10.1038/s41586-021-04301-9.
G. Delétang, A. Ruoss, J. Grau-Moya, T. Genewein, L. K. Wenliang, E. Catt, C. Cundy, M. Hutter,
S. Legg, J. Veness, and P. A. Ortega. Neural networks and the Chomsky hierarchy. In International
Conference on Learning Representations (ICLR) , volume 11, 2023. URL https://openreview.
net/forum?id=WbxHAzkeQcn .
N. Du, Y . Huang, A. M. Dai, et al. GLaM: efficient scaling of language models with mixture-of-
experts. ArXiv , 2112.06905, 2021.
D. Y . Fu, T. Dao, K. K. Saab, A. W. Thomas, A. Rudra, and C. Re. Hungry hungry hippos: Towards
language modeling with state space models. In The Eleventh International Conference on Learning
Representations , 2023. URL https://openreview.net/forum?id=COZDy0WYGg .
L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite,
N. Nabeshima, S. Presser, and C. Leahy. The Pile: An 800gb dataset of diverse text for lan-
guage modeling. ArXiv , 2101.00027, 2021.
F. A. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: Continual prediction with LSTM.
Neural Compututation , 12(10):24512471, 2000.
Gemini Team Google. Gemini: A family of highly capable multimodal models. ArXiv , 2312.11805,
2023.
A. Graves. Generating sequences with recurrent neural networks. ArXiv , 1308.0850, 2013.
S. Greenbaum and G. Nelson. The international corpus of English (ICE) project. World Englishes ,
15(1):315, 1996.
K. Greff, R. K. Srivastava, J. Koutník, B. R. Steunebrink, and J. Schmidhuber. LSTM: A search
space odyssey. ArXiv , 1503.04069, 2015.
A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. ArXiv ,
2312.00752, 2023.
A. Gu, K. Goel, and C. Ré. Efficiently modeling long sequences with structured state spaces. ArXiv ,
2111.00396, 2021.
15A. Gupta, A. Gu, and J. Berant. Diagonal state spaces are as effective as structured state spaces.
ArXiv , 2203.14343, 2022.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 770778, 2016.
S. Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. Masters thesis, Technische
Universität München, 1991.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation , 9(8):17351780,
1997a.
S. Hochreiter and J. Schmidhuber. LSTM can solve hard long time lag problems. In M. C. Mozer,
M. I. Jordan, and T. Petsche (eds.), Advances in Neural Information Processing Systems (NeurIPS) ,
volume 9, pp. 473479. MIT Press, Cambridge MA, 1997b.
S. Hochreiter, Y . Bengio, P. Frasconi, and J. Schmidhuber. Gradient flow in recurrent nets: the
difficulty of learning long-term dependencies. In J. Kolen and S. Kremer (eds.), A Field Guide to
Dynamical Recurrent Networks . IEEE, 2000.
S. Hochreiter, A. Steven Younger, and Peter R. Conwell. Learning to learn using gradient descent.
In G. Dorffner, H. Bischof, and K. Hornik (eds.), Proc. Int. Conf. on Artificial Neural Networks
(ICANN 2001) , pp. 8794. Springer, 2001.
S. Hochreiter, M. Heusel, and K. Obermayer. Fast model-based protein homology detection without
alignment. Bioinformatics , 23(14):17281736, 2007.
J. Hoffmann, S. Borgeaud, A. Mensch, et al. Training compute-optimal large language models. ArXiv ,
2203.15556, 2022.
M. D. Hossain, F. Sohel, M. F. Shiratuddin, and H. Laga. A comprehensive survey of deep learning
for image captioning. ACM Computing Surveys (CSUR) , 51(6):118, 2019.
J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu,
and D. Amodei. Scaling laws for neural language models. ArXiv , 2001.08361, 2020.
A. Karpathy. The unreasonable effectiveness of recurrent neural networks.
http://karpathy.github.io/2015/05/21/rnn-effectiveness/, 2015.
A. Karpathy. OpenAI Five defeats Dota 2 world champions. https://openai.com/research/openai-five-
defeats-dota-2-world-champions, 2019.
A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp.
31283137, 2015.
A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are RNNs: Fast autoregressive
transformers with linear attention. In E. H. Daumé III and A. Singh (eds.), International Conference
on Machine Learning (ICML) , volume 119 of Proceedings of Machine Learning Research , pp.
51565165. PMLR, 2020.
T. Katsch. GateLoop: Fully data-controlled linear recurrence for sequence modeling. ArXiv ,
2311.01927, 2023.
D. Kocetkov, R. Li, L. BenAllal, J. Li, C. Mou, C. Mu nozFerrandis, Y . Jernite, M. Mitchell,
S. Hughes, T. Wolf, D. Bahdanau, L. vonWerra, and H. deVries. The Stack: 3 TB of permissively
licensed source code. ArXiv , 2211.15533, 2022.
T. Kohonen. Correlation matrix memories. IEEE Transactions on Computers , C-21(4), 1972. doi:
10.1109/tc.1972.5008975.
F. Kratzert, D. Klotz, C. Brenner, K. Schulz, and M. Herrnegger. Rainfall-runoff modelling using long
short-term memory (LSTM) networks. Hydrology and Earth System Sciences , 22(11):60056022,
2018.
16F. Kratzert, D. Klotz, G. Shalev, G. Klambauer, S. Hochreiter, and G. Nearing. Benchmarking a
catchment-aware long short-term memory network (LSTM) for large-scale hydrological modeling.
ArXiv , 1907.08456, 2019.
A. Krizhevsky. Learning multiple layers of features from tiny images. Masters thesis, Deptartment
of Computer Science, University of Toronto, 2009.
D. Krotov and J. J. Hopfield. Dense associative memory for pattern recognition. In D. D. Lee,
M. Sugiyama, U. V . Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information
Processing Systems , pp. 11721180. Curran Associates, Inc., 2016.
D. Krotov and J. J. Hopfield. Dense associative memory is robust to adversarial inputs. ArXiv ,
1701.00939, 2017.
Y . Lakretz, G. Kruszewski, T. Desbordes, D. Hupkes, S. Dehaene, and M. Baroni. The emergence of
number and syntax units in LSTM language models. In J. Burstein, C. Doran, and T. Solorio (eds.),
Conference of the North American Chapter of the Association for Computational Linguistics , pp.
1120. Association for Computational Linguistics, 2019. doi: 10.18653/v1/N19-1002.
Y . Li, T. Cai, Y . Zhang, D. Chen, and D. Dey. What makes convolutional models great on long
sequence modeling? ArXiv , 2210.09298, 2022.
P. Liang, R. Bommasani, T. Lee, et al. Holistic evaluation of language models. Annals of the New
York Academy of Sciences , 1525:140146, 2023.
J. Lin, R. Men, A. Yang, C. Zhou, M. Ding, Y . Zhang, P. Wang, A. Wang, L. Jiang, X. Jia, J. Zhang,
J. Zhang, X. Zou, Z. Li, X. Deng, J. Liu, J. Xue, H. Zhou, J. Ma, j. Yu, Y . Li, W. Lin, J. Zhou,
J. Tang, and H. Yang. M6: A Chinese multimodal pretrainer. ArXiv , 2103.00823, 2021.
D. Linsley, J. Kim, V . Veerabadran, C. Windolf, and T. Serre. Learning long-range spatial dependen-
cies with horizontal gated recurrent units. Advances in Neural Information Processing Systems
(NeurIPS) , 31, 2018.
I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Confer-
ence on Learning Representations (ICLR) , 2019. URL https://openreview.net/forum?id=
Bkg6RiCqY7 .
X. Ma, C. Zhou, X. Kong, J. He, L. Gui, G. Neubig, J. May, and L. Zettlemoyer. Mega: Moving
average equipped gated attention. ArXiv , 2209.10655, 2022.
A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y . Ng, and C. Potts. Learning word vectors
for sentiment analysis. In Annual Meeting of the Association for Computational Linguistics ,
volume 49, pp. 142150, 2011.
I. Magnusson, A. Bhagia, V . Hofmann, et al. Paloma: A benchmark for evaluating language model
fit.ArXiv , 2312.10523, 2023.
H. Mehta, A. Gupta, A. Cutkosky, and B. Neyshabur. Long range language modeling via gated state
spaces. ArXiv , 2206.13947, 2022.
S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. In Interna-
tional Conference on Learning Representations (ICRL) , 2017. URL https://openreview.net/
forum?id=Byj72udxe .
W. Merrill and A. Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers.
Transactions of the Association for Computational Linguistics , 11:531545, 2023. doi: 10.1162/
tacl_a_00562.
W. Merrill, J. Petty, and A. Sabharwal. The illusion of state in state-space models. ArXiv , 2404.08819,
2024.
M. Milakov and N. Gimelshein. Online normalizer calculation for softmax. ArXiv , 1805.02867,
2018.
17K. Nakano. Associatron  a model of associative memory. IEEE Transactions on Systems, Man, and
Cybernetics , SMC-2(3):380388, 1972. doi: 10.1109/TSMC.1972.4309133.
G. Nearing, D. Cohen, V . Dube, M. Gauch, O. Gilon, S. Harrigan, A. Hassidim, D. Klotz, F. Kratzert,
A. Metzger, S. Nevo, F. Pappenberger, C. Prudhomme, G. Shalev, S. Shenzis, T. Y . Tekalign,
D. Weitzner, and Y . M. B. Kosko. Global prediction of extreme floods in ungauged watersheds.
Nature , 627:559563, 2024. doi: 10.1038/s41586-024-07145-1.
C. Olsson, N. Elhage, N. Nanda, et al. In-context learning and induction heads. ArXiv , 2209.11895,
2022.
A. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gulcehre, R. Pascanu, and S. De. Resurrecting
recurrent neural networks for long sequences. In Proceedings of the 40th International Conference
on Machine Learning (ICML) . JMLR.org, 2023. doi: 10.5555/3618408.3619518.
A. Papasavva, S. Zannettou, E. DeCristofaro, G. Stringhini, and J. Blackburn. Raiders of the lost
KeK: 3.5 years of augmented 4chan posts from the politically incorrect board. In International
AAAI Conference on Web and Social Media (ICWSM) , volume 14, pp. 885894, 2020.
D. Paperno, G. Kruszewski, A. Lazaridou, N.-Q. Pham, R. Bernardi, S. Pezzelle, M. Baroni, Gemma
G. Boleda, and R. Fernández. The LAMBADA dataset: Word prediction requiring a broad
discourse context. In Annual Meeting of the Association for Computational Linguistics , volume 1,
pp. 15251534, 2016.
G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli, B. Pannier, E. Al-
mazrouei, and J. Launay. The RefinedWeb dataset for Falcon LLM: Outperforming curated corpora
with web data, and web data only. ArXiv , 2306.01116, 2023.
B. Peng, E. Alcaide, Q. Anthony, et al. RWKV: Reinventing RNNs for the transformer era. ArXiv ,
2305.13048, 2023.
B. Peng, D. Goldstein, Q. Anthony, et al. Eagle and Finch: RWKV with matrix-valued states and
dynamic recurrence. ArXiv , 2404.05892, 2024.
M. Poli, S. Massaroli, E. Nguyen, D. Y . Fu, T. Dao, S. Baccus, Y . Bengio, S. Ermon, and C. Ré. Hyena
hierarchy: Towards larger convolutional language models. In Proceedings of the 40th International
Conference on Machine Learning (ICML) . JMLR.org, 2023. doi: 10.5555/3618408.3619572.
M. Poli, A. W. Thomas, E. Nguyen, P. Ponnusamy, B. Deiseroth, K. Kersting, T. Suzuki, B. Hie, S. Er-
mon, C. Ré, C. Zhang, and S. Massaroli. Mechanistic design and scaling of hybrid architectures.
ArXiv , 2403.17844, 2024.
Z. Qin, S. Yang, and Y . Zhong. Hierarchically gated recurrent neural network for sequence modeling.
InAdvances in Neural Information Processing Systems (NeurIPS) , volume 37, 2023. URL
https://openreview.net/forum?id=P1TCHxJwLB .
Z. Qin, S. Yang, W. Sun, X. Shen, D. Li, W. Sun, and Y . Zhong. HGRN2: Gated linear RNNs with
state expansion. ArXiv , 2404.07904, 2024.
D. R. Radev, P. Muthukrishnan, and V . Qazvinian. The ACL anthology network corpus. In Workshop
on Text and Citation Analysis for Scholarly Digital Libraries (NLPIR4DL) , pp. 5461. Association
for Computational Linguistics, 2009.
A. Radford, R. Jozefowicz, and I. Sutskever. Learning to generate reviews and discovering sentiment.
ArXiv , 1704.01444, 2017.
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised
multitask learners. https://openai.com/index/better-language-models , 2019.
J. W. Rae, S. Borgeaud, T. Cai, et al. Scaling language models: Methods, analysis & insights from
training Gopher. ArXiv , 2112.11446, 2021.
C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu.
Exploring the limits of transfer learning with a unified text-to-text transformer. ArXiv , 1910.10683,
2019.
18H. Ramsauer, B. Schäfl, J. Lehner, P. Seidl, M. Widrich, L. Gruber, M. Holzleitner, M. Pavlovi c,
G. K. Sandve, V . Greiff, D. Kreil, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter.
Hopfield networks is all you need. In International Conference on Learning Representations
(ICLR) . OpenReview, 2021.
M. Reid, V . Zhong, S. Gururangan, and L. Zettlemoyer. M2D2: A massively multi-domain language
modeling dataset. In Conference on Empirical Methods in Natural Language Processing , pp.
964975, 2022.
M. Reid, N. Savinov, D. Teplyashin, et al. Gemini 1.5: Unlocking multimodal understanding across
millions of tokens of context. ArXiv , 2403.05530, 2024.
M. H. Ribeiro, J. Blackburn, B. Bradlyn, E. DeCristofaro, G. Stringhini, S. Long, S. Greenberg, and
S. Zannettou. The evolution of the manosphere across the web. In Proceedings of the international
AAAI conference on web and social media , volume 15, pp. 196207, 2021.
K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y . Choi. Winogrande: An adversarial winograd
schema challenge at scale. Communications of the ACM , 64(9):99106, 2021.
T. L. Scao, A. Fan, C. Akiki, et al. BLOOM: A 176B-parameter open-access multilingual language
model. ArXiv , 2211.05100, 2022.
I. Schlag, K. Irie, and J. Schmidhuber. Linear transformers are secretly fast weight programmers.
In M. Meila and T. Zhang (eds.), Proceedings of the 38th International Conference on Machine
Learning (ICML) , volume 139 of Proceedings of Machine Learning Research , pp. 93559366.
PMLR, 2021.
J. Schmidhuber. Learning to control fast-weight memories: An alternative to recurrent nets. Neural
Computation , 4(1):131139, 1992.
J. Schmidhuber. Deep learning in neural networks: An overview. Neural Networks , 61:85117, 2015.
doi: 10.1016/j.neunet.2014.09.003.
J. Schulman, B. Zoph, C. Kim, J. Hilton, et al. ChatGPT: Optimizing language models for dialogue.
https://openai.com/blog/chatgpt/, 2022. OpenAI Research.
T. J. Sejnowski. Storing covariance with nonlinearly interacting neurons. Journal of Mathematical
Biology , 4, 1977. doi: 10.1007/BF00275079.
M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-LM: Training
multi-billion parameter language models using model parallelism. ArXiv , 1909.08053, 2019.
J. T. H. Smith, A. Warrington, and S. W. Linderman. Simplified state space layers for sequence
modeling. ArXiv , 2208.04933, 2022.
D. Soboleva, F. Al-Khateeb, R. Myers, J. R. Steeves, J. Hestness, and N. Dey. SlimPajama: A 627B
token cleaned and deduplicated version of RedPajama. https://www.cerebras.net/blog/
slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama ,
2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B .
L. Soldaini, R. Kinney, A. Bhagia, et al. Dolma: an open corpus of three trillion tokens for language
model pretraining research. ArXiv , 2306.01116, 2023.
S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza, H. Khan, C. Peris, S. Rawls,
A. Rosenbaum, A. Rumshisky, C. S. Prakash, M. Sridhar, F. Triefenbach, A. Verma, G. Tur, and
P. Natarajan. AlexaTM 20B: Few-shot learning using a large-scale multilingual Seq2Seq model.
ArXiv , 2208.01448, 2022.
R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. In C. Cortes,
N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information
Processing Systems (NeurIPS) , volume 28. Curran Associates, Inc., 2015.
Y . Sun, L. Dong, S. Huang, S. Ma, Y . Xia, J. Xue, J. Wang, and F. Wei. Retentive network: A
successor to transformer for large language models. ArXiv , 2307.08621, 2023.
19L. Sutawika, L. Gao, H. Schoelkopf, et al. EleutherAI/lm-evaluation-harness: Major refactor, 2023.
L. Sutawika, H. Schoelkopf, L. Gao, B. Abbasi, S. Biderman, J. Tow, B. fattori, C. Lovering,
farzanehnakhaee70, J. Phang, A. Thite, Fazz, T. Wang, N. Muennighoff, Aflah, sdtblck, nopperl,
gakada, tttyuntian, researcher2, Chris, J. Etxaniz, H. A. Lee, Z. Kasner, Khalid, J. Hsu, A. Kanekar,
P. S. Ammanamanchi, V . Boykis, and AndyZwei. EleutherAI/lm-evaluation-harness, 2024.
I. Sutskever, O. Vinyals, and Q. V . V . Le. Sequence to sequence learning with neural networks. In
Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in
Neural Information Processing Systems 27 (NIPS13) , pp. 31043112. Curran Associates, Inc.,
2014.
Y . Tay, D. Bahri, D. Metzler, D.-C. Juan, Z. Zhao, and C. Zheng. Synthesizer: Rethinking self-
attention in transformer models. ArXiv , 2005.00743, 2020.
Y . Tay, M. Dehghani, S. Abnar, Y . Shen, D. Bahri, P. Pham, J. Rao, L. Yang, S. Ruder, and D. Metzler.
Long range arena: A benchmark for efficient transformers. In International Conference on Learning
Representations (ICRL) , 2021. URL https://openreview.net/forum?id=qVyeW-grC2k .
R. Thoppilan, D. deFreitas, J. Hall, et al. LaMDA: Language models for dialog applications. ArXiv ,
2201.08239, 2022.
TogetherComputer. Redpajama: an open dataset for training large language models, 2023. URL
https://github.com/togethercomputer/RedPajama-Data .
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,
E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient
foundation language models. ArXiv , 2302.1397, 2023.
D. Vadas and J. R. Curran. Parsing noun phrases in the Penn Treebank. Computational Linguistics ,
37(4):753809, 2011.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin.
Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS) ,
volume 30, pp. 59986008. Curran Associates, Inc., 2017.
O. Vinyals, T. Ewalds, S. Bartunov, et al. Starcraft II: A new challenge for reinforcement learning.
ArXiv , 1708.04782, 2017.
J. Wang, J. N. Yan, A. Gu, and A. M. Rush. Pretraining without attention. ArXiv , 2212.10544, 2022.
S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma. Linformer: Self-attention with linear complexity.
ArXiv , 2006.04768, 2020.
S. Wang, Y . Sun, Y . Xiang, et al. ERNIE 3.0 Titan: Exploring larger-scale knowledge enhanced
pre-training for language understanding and generation. ArXiv , 2112.12731, 2021.
Y . Wu and K. He. Group normalization. In Proceedings of the European conference on computer
vision (ECCV) , pp. 319, 2018.
L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel.
mT5: A massively multilingual pre-trained text-to-text transformer. In Conference of the North
American Chapter of the Association for Computational Linguistics , pp. 483498, 2021. doi:
10.18653/v1/2021.naacl-main.41.
S. Yang and Y . Zhang. FLA: A Triton-based library for hardware-efficient implementa-
tions of linear attention mechanism, 2024. URL https://github.com/sustcsonglin/
flash-linear-attention .
S. Yang, B. Wang, Y . Shen, R. Panda, and Y . Kim. Gated linear attention transformers with hardware-
efficient training. ArXiv , 2312.06635, 2023.
S. Zannettou, B. Bradlyn, E. DeCristofaro, H. Kwak, M. Sirivianos, G. Stringini, and J. Blackburn.
What is Gab: A bastion of free speech or an alt-right echo chamber. In The Web Conference , pp.
10071014, 2018. doi: 10.1145/3184558.3191531.
20W. Zaremba and I. Sutskever. Learning to execute. ArXiv , 1410.4615, 2014.
R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, and Y . Choi. HellaSwag: Can a machine really
finish your sentence? In Annual Meeting of the Association for Computational Linguistics , pp.
47914800, 2019.
A. Zeng, X. Liu, Z. Du, et al. GLM-130B: An open bilingual pre-trained model. ArXiv , 2210.02414,
2022.
S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V . Lin,
T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and
L. Zettlemoyer. OPT: Open pre-trained transformer language models. ArXiv , 2205.01068, 2022.
21Contents
A Extended Long Short-Term Memory 23
A.1 Vanilla Long Short-Term Memory Formulation: Vector Notation . . . . . . . . . . 23
A.2 sLSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
A.3 mLSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
A.4 Detailed Block Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
B Experiments 31
B.1 Synthetic Tasks and Long Range Arena . . . . . . . . . . . . . . . . . . . . . . . . 31
B.1.1 Test of xLSTMs Exponential Gating with Memory Mixing. . . . . . . . . . 31
B.1.2 Test of xLSTMs Memory Capacities on Associative Recall Tasks. . . . . . 34
B.1.3 Test of xLSTMs Long Range Capabilities on the Long Range Arena. . . . 36
B.2 Method Comparison and Ablation Study on SlimPajama (15B) . . . . . . . . . . . 40
B.3 xLSTM Large Language Models  SlimPajama300B . . . . . . . . . . . . . . . . 42
C Detailed Results on PALOMA Language Model Evaluation 44
22A Extended Long Short-Term Memory
A.1 Vanilla Long Short-Term Memory Formulation: Vector Notation
The vanilla LSTM memory cell update rules (Greff et al., 2015) at time step textend the scalar cell
state formulation to a vector of cell states:
ct=ftct1+itzt cell state (28)
ht=otht, ht=ψ
ct
hidden state (29)
zt=φ(zt), zt=Wzxt+Rzht1+bz cell input (30)
it=σ
it
, it=Wixt+Riht1+bi input gate (31)
ft=σ
ft
, ft=Wfxt+Rfht1+bf forget gate (32)
ot=σ(ot), ot=Woxt+Roht1+bo output gate (33)
The matrices Wz,Wi,Wf, andWocorrespond to the input weights between inputs xtand cell
input, input gate, forget gate, and output gate, respectively. The matrices Rz,Ri,Rf, andRo
correspond to the recurrent weights between hidden state ht1and cell input, input gate, forget gate,
and output gate, respectively. bz,bi,bf, andboare the corresponding bias vectors. φandψare the
cell input and hidden state activation functions (typically tanh ).ψis used to normalize or squash the
cell state, which would be unbounded otherwise.
A.2 sLSTM
Similar to the LSTM in Section A.1, also the sLSTM can be vectorized to multiple cells:
ct=ftct1+itzt cell state (34)
nt=ftnt1+it normalizer state (35)
ht=otht, ht=ctn1
t hidden state (36)
zt=φ(zt), zt=Wzxt+Rzht1+bz cell input (37)
it=exp
it
, it=Wixt+Riht1+bi input gate (38)
ft=exp
ft
ORσ
ft
, ft=Wfxt+Rfht1+bf forget gate (39)
ot=σ(ot), ot=Woxt+Roht1+bo output gate (40)
Here, the cell input activation function φistanh , the hidden state activation function is the identity.
φhelps stabilizing the recurrence.
Considering external gradient contribution δext
htfrom subsequent layers and recurrent gradient contri-
bution δR
htfrom gradients from future states flowing over the cell interaction matrix R, we obtain the
recursive backward pass of sLSTM, where δaindicates gradients with respect to parameter / internal
variable a:
23δht=δext
ht+δR
ht(41)
δct1=ftδct+ot1nt11δht1 (42)
δnt1=ftδntotct1n2
t1δht1 (43)
δft=f
tct1δct+f
tnt1δnt (44)
δit=i
tztδct+i
tδnt (45)
δzt=itφ(zt)δct (46)
δot=o
tctn1
tδht (47)
δxt=X
g{f,i,z,o}W
gδgt (48)
δR
ht1=X
g{f,i,z,o}R
gδgt (49)
δ
Rg=X
tht1δ
gt, g {i,f,z,o} (50)
δ
Wg=X
txtδ
gt, g {i,f,z,o} (51)
with the derivatives of the respective gate activation function i
t= exp(it) = exp( it) =it,o
t=
σ(ot), andf
t=σ(ft)orf
t=ftdepending on the forget gate activation. φ(z)is the derivative of
the cell input activation function φ(z).
The matrices Rz,Ri,Rf,Roare block-diagonal which is analogous to multiple heads in the
mLSTM. This way, the parameters reduce to d2/(Nh), where Nhis the number of heads, limiting the
cell interactions to individual heads. This parameter efficient formulation of cell interactions together
with the exponential gating is called the new memory mixing. Finally, to stabilize the backward
pass, we clip the magnitude of δR
htto10, as a means to prohibit exploding gradients for long context
lengths.
Proof of Equivalence for sLSTM Stabilized Version. The stabilization state m, see Equation (15)
in the main paper, has no gradient, and hence does not influence the other gradients. We go back to
the scalar version (Equation 8) here for simplicity. We re-define c(s)
tandn(s)
tas stabilized cell and
normalizer states:
ct=c(s)
texp
mt
(52)
nt=n(s)
texp
mt
(53)
Inserting Equation 15 into Equation 8 yields:
h(s)
t=c(s)
t/n(s)
t= (54)
=exp
log (f t) +mt1mt
c(s)
t1+ exp
log (i t)mt
zt
exp
log (f t) +mt1mt
n(s)
t1+ exp
log (i t)mt (55)
=exp
log (f t) +mt1
c(s)
t1+ exp (log (i t))zt
exp
log (f t) +mt1
n(s)
t1+ exp (log (i t))(56)
=exp (log (f t))ct1+ exp (log (i t))zt
exp (log (f t))nt1+ exp (log (i t))(57)
=ftct1+ itzt
ftnt1+ it=ct/nt=ht (58)
24Therefore, since the loss solely depends on ht, theres no dependency on mt, and consequently,
no gradient exists for this stabilization state. Note that mtcan be chosen arbitrarily. We choose
mt= max (log ( ft) +mt1,log (it)), which stabilizes the exponential function. One can even find
mt, such that the normalizer state ntcan be eliminated, but this version was experimentally found to
be numerically unstable in the backward pass.
A.3 mLSTM
Throughout this section, 1RTdenotes a column vector of ones and 1R1Ta row vector of
ones, where Tis the dimension of this vector.
Recurrent mLSTM Backward Pass. The recurrent formulation of the mLSTM cell in Equation 19
yields the following backward pass recurrence, where δaindicates gradients with respect to parameter
or internal variable aandδext
htdenotes gradients from subsequent layers:
δ
ht=otδext
ht(59)
δCt1= ftδCt+qt1δ
ht1
maxn
t1qt1,1	 (60)
δnt1= ftδntq
t1C
t1δht1
maxn
t1qt1,1	2Ω
n
t1qt1
qt1 (61)
δ
vt= itk
tδ
Ct(62)
δ
kt= it
v
tδCt+δ
nt
(63)
δqt=C
tδht
maxn
tqt,1	q
tC
tδht
maxn
tqt,1	2Ω
n
tqt
nt (64)
δxt=X
g{q,k,v}W
gδgt (65)
δ
Wg=X
txtδ
gt, g  {q, k, v} (66)
δbg=X
tδgt, g  {q, k, v} (67)
δft=
1
Ct1δCt1
1+1(nt1δnt)
γ
ft
(68)
δit=
1
vtk
t
δCt1
1+1(nt1δnt)
exp
ft
(69)
δot=htσ( ot)δht (70)
andΩ (z) = Θ ( z1)Θ (z1),Θ (z)being the Heaviside step function. γ(z)is either σ(z)
orexp (z), depending on the forget gate activation.
Parallel mLSTM Forward Pass. The mLSTM recurrence in Equations (19-27) can be reformulated
in a parallel form, which is used to speed up training. After training we can still use the recurrent
formulation for fast text generation.
Instead of processing each input xtRdat time step tsequentially, the parallel version processes
all timesteps of a full sequence XRTdat once, where Tis the sequence length and dis the
head dimension. We present the forward pass of the mLSTM for a single head and drop the head
dimension for simplicity.
25LetfRTbe the forget gate pre-activations and iRTbe the input gate pre-activations for a full
sequence. We construct the forget gate activation matrix FRTTby
Fij=

0 forj > i
1 forj=iQi
k=j+1σ
fk
forj < i, (71)
and the input gate pre-activation matrix IRTTby
Iij=0forj > i
ijforij. (72)
By applying the elementwise exponential input gate activation function naively, we obtain the
unstabilized gate activation matrix DRTTas
D=Fexp(I). (73)
In order to avoid overflow due to the exponential function we apply the same stabilization as in the
recurrent sLSTM, see Equation 15. In the parallel formulation of the mLSTM we get a numerically
stable gate activation matrix DRTTby taking the logarithm of Delement-wise and subtracting
the row-wise maximum value of Dfrom each element:
eD= log D= log
Fexp(I)
= log F+I (74)
D= exp( eDmaxeD) (75)
Given the queries, keys and values Q,K,VRTd, for a full sequence we can compute all hidden
pre-activation states eHRTdin parallel for the un-stabilized version by
eH=C V ,withC=eC
max
|PT
j=1eCij|,1,andeC=QK

dD. (76)
Note that we extract the1
dfactor for Kexplicitly here and further on. For the stabilized version
this yields
eH=C V ,withC=eC
max
|PT
j=1eC
ij|,exp(maxeD),andeC=QK

dD,(77)
where for both versions the hidden pre-activation states eHare identical.
With the output gate pre-activations eORTdwe can compute the hidden states HRTdfor all
timesteps by applying the output gate in parallel for each timestep element-wise:
H=σ(eO)eH. (78)
This gives the parallel forward pass of the mLSTM for a full input sequence XRTd.
Parallel mLSTM Backward Pass. We present the backward pass of the mLSTM for the stabilized
version only. For completeness we summarize the forward pass in the stabilized version before we
present the backward pass.
Given the forget gate matrix FRTT, the logarithm of the forget gate matrix F= log FRTT,
and the input gate matrix IRTTas introduced above, together with the queries, keys and values
26Q,K,VRTd, we can write the forward pass of the mLSTM in the stabilized version as:
eD=F+I (79)
m= max
jeDij, row-wise maximum (80)
D= exp( eDm1) (81)
eC=QK

dD(82)
b=TX
j=1eC
ij=eC1, row-wise sum (83)
n= max ( |b|,exp(m)) (84)
C=eC
n11
(85)
eH=C V (86)
With this forward pass we can compute the gradients δafor all intermediate and input variables to the
mLSTM forward pass in the backward pass. We denote the gradient with respect to variable aasδa.
Given the output gradient δeHRTdwe can compute the backward pass for the intermediate
gradients as:
δ
C=Vδ
eH(87)
δn=
eC
n21
δC
1 (88)
=
eCδC
1
n2(89)
δb=sign(n)δn1if|b|>exp(m)
0otherwise(90)
δeC,C=
n11
δC, column-wise broadcast (91)
δ
eC,b=1δ
b, column-wise broadcast (92)
δeC=δeC,C+δeC,B(93)
δD=QK

dδeC (94)
δeD= exp( eDm)δD=DδD (95)
We do not compute the gradients for mas they cancel out (see the proof in the recurrent sLSTM).
With these intermediate gradients the gradients for the logarithmic forget gate matrix δFRTT,
the input gate matrix δIRTT, and the queries, keys and values δQ, δK, δVRTdare given by
δF=δeD(96)
δI=δeD(97)
δQ=
DδeCK
d(98)
δK=
DδeCQ
d(99)
δV=CδeH(100)
Having computed the gradients for the logarithmic forget gate matrix δF, we can compute the
gradients for the forget gate pre-activations δf=
δf1, δf2, ..., δ fTRT.
27Recall the logarithmic forget gate matrix F= log Fis computed by
Fij= log Fij=

 forj > i
0 forj=iPi
k=j+1logσ
fk
|{z}
=:fk=Pi
k=j+1fkforj < i. (101)
With the substitution f= log σ(f)we compute the gradients for the logarithmic forget gate activations
δf=
δf1, δf2, ..., δfTRTas
δfk=k1X
j=1TX
i=k+1
δF
ij, (102)
δfk=σ(fk)δfk, (103)
where the last equation makes use of the following:
d
dx(logσ(x)) =(1 + exp( x))1exp(x)(1)
=exp(x)
1 + exp( x)=1
1 + exp( x)
=σ(x)(104)
Finally, we compute the input gate pre-activations gradients δi=
δi1, δi2, ..., δ iSRTas the
column-wise sum over the rows of the input gate matrix δI:
δik=TX
i=1(δI)ik (105)
This completes the backward pass of the parallel mLSTM for a full input sequence XRTd.
28A.4 Detailed Block Structure
PF=¾
NH=4 NH=4 NH=4f z o
Conv4Swishi
NH=4NH=4 NH=4 NH=4 NH=4sLSTMGN
LNPF= PF= 3434GeLU
Figure 9: Schematic representation of an sLSTM Block  post up-projection: Embedded in a pre-
LayerNorm residual structure, the input is optionally passed through a causal convolution of window
size4that includes a Swish activation for input and forget gates. Then, for all input, forget and output
gates i,f,o, and the cell update zthe input is fed through a block-diagonal linear layer with four
diagonal blocks or heads. These diagonal blocks coincide with the recurrent gate pre-activations
from the last hidden state, which corresponds to an sLSTM with four heads depicted with the circular
arrows. The resulting hidden state goes through a GroupNorm layer (Wu & He, 2018)  a head-wise
LayerNorm for each of the four heads. Finally, the output is up- and down-projected using a gated
MLP, with GeLU activation function and projection factor 4/3to match parameters.
29PF=2PF=½
PF=2NH=4
BS=4 BS=4 BS=4
Conv4LSkipSwish
Swishq k vi fmLSTMGN
LNFigure 10: Schematic representation of an mLSTM block  pre up-projection: Embedded in a
pre-LayerNorm residual structure, the input is up-projected first with projection factor 2, once for
an externalized output gate and once as input for the mLSTM cells. The mLSTM cell input is
dimension-wise causally convolved (kernel size 4), before entering a learnable skip connection. We
obtain input qandkvia block-diagonal projection matrices of block size 4. The values vare fed
directly, skipping the convolution part. After the mLSTM sequence mixing, outputs are normalized
via GroupNorm (Wu & He, 2018)  a head-wise layer norm for each of the four heads. Finally, the
learnable skip input is added and the result is gated component-wise with the external output gate.
The output is down-projected.
30B Experiments
Training Setup. For all experiments, we use Python13.11 with PyTorch 2.2.02, and CUDA 12.13
on NVIDIA A100 GPUs.
Nearest Neighbor Search Task. For this auxiliary task, we use randomly sampled feature vectors
of dimension 2 and unit norm. The attached value is a uniformly distributed random number from
[0,1], leading to inputs vectors of dimension 3. The first feature vector serves as search key, with the
first value being ignored. Then the model has to predict the value of the nearest neighbor so far in the
sequence. We train on 8192 sequences of context length up to 64 (uniformly sampled) and validate
on 8192 different samples. All models have two blocks and embedding dimension 128. We use a
dropout of 0.1, 10% linear warm-up steps and cosine decay to 1e-7 for 100k total training steps. We
sweep over learning rates 1e-4, 1e-3, 1e-2, 1e-1 and 5 seeds each. The reported values in Figure 2 are
mean values for the best learning rate and 99% confidence intervals. Note that LSTM requires very
high learning rates, whereas Transformers (Llama) perform best at the smallest learning rate. The
xLSTM[0:1] reaches similar performance across all learning rates.
Wikitext-103 Rare Token Prediction. For this exemplary experiment on rare token prediction, we
trained 125M-sized models on Wikitext-103 (Merity et al., 2017). All models have an embedding
dimension of 768 in a post up-projection structure of 12 residual blocks. The Transformer model
(Llama) uses Multi-Head Attention, for what is called LSTM the Multi-Head Attention is replaced by
an LSTM and the xLSTM[1:0] contains mLSTM layers with matrix memory. Models were trained
with maximum learning rate 1e-3, 4k steps linear warm-up and cosine decay for in total 50k steps,
using a batch size of 256 and context length of 512. We use the validation perplexity as a stopping
criterion and evaluate on the test set.
B.1 Synthetic Tasks and Long Range Arena
B.1.1 Test of xLSTMs Exponential Gating with Memory Mixing.
We evaluate xLSTM on a suite of formal language tasks to test its exponential gating and memory
mixing mechanism.
Formal languages provide a framework to probe the generalization capabilities of models. They allow
to specifically test different expressivity levels, e.g. along the Chomsky hierarchy. Typical language
model architectures do not necessarily fit perfectly in these hierarchies (Delétang et al., 2023) 
nevertheless these languages allow to illustrate differences in generalization expressivity between
different architectures. Our evaluation tasks are heavily based on the work of Delétang et al. (2023).
Experiment Setup. The different formal language tasks in the experiment (see individual tasks
description below) encompass different levels of the Chomsky hierarchy as well as additional counting
and memory-focused tasks. We use different lengths per sample, which allows us to validate in a
length extrapolation setting. We train on a varying task length up to 40. The evaluation is done for
task lengths between 40 and 256 as we are only interested in the task generalization capabilities of
the models.
In all experiments, we use two blocks (or layers for the pure LSTM) for all models. We compare
Llama, Mamba, Retention, Hyena, RWKV-4, RWKV-5, RWKV-6, LSTM, xLSTM[0:1], xLSTM[1:0]
and xLSTM[1:1]. The sLSTM block is used without a convolution and with normal weight initializa-
tion. LSTM (Block) refers to an architecture where a vanilla LSTM is used instead of self-attention
inside a Transformer block.
All models are trained with 3 different learning rates (1e-2, 1e-3, 1e-4), each with two seeds.
Batch size is 256  cosine annealing (min lr: 1e-5) with 10% warm-up steps is applied. We use
AdamW (Loshchilov & Hutter, 2019) and a weight decay of 0.1 for training. In each experiment we
train for 100k steps  the samples are generated randomly, however, all experiments are trained and
evaluated on the same samples.
1https://python.org
2https://pytorch.org
3https://docs.nvidia.com/cuda/archive/12.1.0/
31Odds FirstReverse
StringStack
Manipulation Repetition Set
Llama
Retention
RWKV-4
Hyena
RWKV-5
RWKV-6
xLSTM[0:1]
Mamba
LSTM
(Block)
xLSTM[0:1]
xLSTM[1:0]
xLSTM[1:1]0.07
0.00.06
0.00.11
0.010.08
0.00.04
0.0
0.03
0.00.11
0.00.03
0.00.02
0.00.02
0.0
0.08
0.00.12
0.010.2
0.00.1
0.00.1
0.02
0.04
0.00.15
0.00.07
0.00.07
0.00.03
0.0
0.08
0.010.09
0.010.16
0.00.16
0.00.13
0.01
0.13
0.010.11
0.00.23
0.010.15
0.010.19
0.01
0.09
0.010.14
0.030.13
0.010.09
0.010.17
0.01
0.08
0.010.13
0.020.21
0.00.15
0.010.12
0.0
0.08
0.010.17
0.020.25
0.020.15
0.010.18
0.01
0.09
0.010.14
0.030.13
0.010.09
0.010.17
0.01
0.15
0.030.22
0.020.25
0.030.28
0.00.17
0.01
0.08
0.00.2
0.010.17
0.00.09
0.00.15
0.03Context
SentsitiveDeterministic
Context FreeFigure 11: Supplementary results given by scaled accuracy of different models at solving formal
language tasks. Tasks are grouped by the Chomsky hierarchy.
Additional Formal Language Results. Figure 11 showcases supplementary results on formal
language task, detailing tasks where no model attained a minimum scaled accuracy of 0.3. Although
no model achieves proper extrapolation of the task to a larger context length, xLSTM performs best
among the evaluated models.
Individual Task Description. The majority of tasks are based on Delétang et al. (2023). We
provide the vocabulary size |V|and the random accuracy srand (for accuracy scaling), used in the
evaluation. As we evaluate different task lengths each task has a padding token which is used to pad
the sequence to the given context length. In Listing 1 there is an example for each task.
Bucket Sort Given a string of tokens of a sorted alphabet, compute the sorted string.
|V|= 11 srand=1
|V|1
Cycle Nav Given a string of movement tokens ( +1,1, STAY) compute the end position
of the agent with start position 0. The position must be computed modulo the maximum
position.
|V|= 9 srand=1
|V|4
Even Pairs Given a binary string of aandbtokens, compute whether the number of aband
bais even. This task can be solved by checking if the first and last token of the string are
equal.
|V|= 3 srand= 0.5
Majority Given a string of tokens, compute the token that occurred most often in the
sequence.
|V|= 64 srand=1
|V|1
Majority Count Given a string of tokens of an ordered alphabet. Compute the count of
the token that occurred most often in the sequence. If the count exceeds the vocab size, the
highest vocab token should be outputted.
|V|= 64 srand=1
|V|1
Missing Duplicate Given a string of tokens. The string is repeated but one of the tokens is
masked in the repetition. Output the token that is masked.
|V|= 11 srand=1
|V|2
32Mod Arithmetic (w/o Brackets) Calculate the result  modulo the max number  of the
arithmetic operations in the context. The maximum number is the vocabulary size minus the
number of special tokens (+,-,*,=, [PAD]).
|V|= 10 srand=1
|V|5
Mod Arithmetic (w Brackets) Calculate the result  modulo the maximum number  of
the arithmetic operations in the context. The maximum number is vocabulary size minus the
number of special tokens (+,-,*,=,(,), [PAD]).
|V|= 12 srand=1
|V|7
Odds First An string of tokens t1, t2, t3, ...tnis given. Output all tokens with and odd index
(t1, t3, ...) then the token with an even index ( t2,t4,..) . Apart from that keep the ordering of
the initial string.
|V|= 12 srand=1
|V|2
Parity Given a binary string of aandbtokens, compute if the number of bs is even. If
the number is even output aotherwise b. This is equivalent to sequentially calculating the
half-adder sum.
|V|= 3 srand= 0.5
Repetition Given a string of tokens  repeat it.
|V|= 12 srand=1
|V|2
Reverse String Given a string of tokens  repeat it in reverse order.
|V|= 12 srand=1
|V|2
Stack Manipulation An initial stack content is given, followed by a sequence of push and
pop operations. Compute the stack content after the operations
|V|= 11 srand=1
|V|3
2
SetGiven a string of tokens, compute the ordered set of the tokens. Keep the ordering so
that tokens that occurred first are also outputted first.
|V|= 128 srand=1
|V|2
Solve Equation Given is an equation with the operators {+,-,*,=,(,)}, number, and an
unknown variable x. Compute the value of the variable modulo the max number. The
maximum number is vocabulary size minus the number of special tokens (+,-,*,=,(,), [PAD],
[ACT]).
|V|= 14 srand=1
|V|9
33Bucket Sort
Sequence: 1 4 8 6 1 1 1 4 6 8
Cycle Nav
Sequence: STAY +1 -1 +1 STAY +1 +1 +1 -1 P3
Even Pairs
Sequence: a b b a a b a b a a
Majority
Sequence: 1 7 6 4 3 8 1 7 2 1
Majority Count
Sequence: 1 7 6 4 4 8 1 7 2 2
Missing Duplicate
Sequence: 4 8 6 2 5 4 8 6 2 [MIS] 5
Mod Arithmetic (w/o Braces)
Sequence: 0 - 4 + 0 - 2 = 4 [PAD]
Mod Arithmetic (w Braces)
Sequence: ( ( ( 2 ) * - 2 ) - ( - 4 - 2 ) ) = 2
Odds First
Sequence: 2 7 3 2 6 9 [ACT] 2 3 6 7 2 9
Parity:
Sequence: a b b a a b a b
Repetition
Sequence: 2 4 8 6 2 [ACT] 2 4 8 6 2
Reverse String
Sequence: 2 4 8 6 2 [ACT] 2 6 8 4 2
Stack Manipulation
Sequence: ST1 ST1 ST3 POP POP PS3 PS3 [ACT] ST1 ST3 ST3
Set
Sequence: 8 6 6 3 5 4 5 3 [ACT] 8 6 3 5 4
Solve Equation:
Sequence: ( ( ( 2 + 0 ) + - x ) - ( 1 ) ) = 2 [ACT] 2
Listing 1: Examples of the formal language tasks. Red tokens are evaluated for loss and accuracy
metrics, but are padded for the input. The tokens are illustrated in a way that allows easy semantic
interpretation for the given task  hence, some tokens are represented by multiple characters.
B.1.2 Test of xLSTMs Memory Capacities on Associative Recall Tasks.
We test the memory capacity of xLSTM with the Multi-Query Associative Recall task proposed by
Arora et al. (2023). Figure 12 illustrates the basic task setup.
Why Multi-Query Associative Recall for Memory Tests of LLM Architectures. Associative
Recall (AR), the ability to retrieve a specific value (information) associated with a given key (infor-
mation), constitutes a key capability for LLM to perform well (Poli et al., 2024; Arora et al., 2023;
Olsson et al., 2022). Especially its quality of in-context learning seems to be strongly connected to
this capability (Olsson et al., 2022). Arora et al. (2023) attribute performance gaps between early
non-Transformer and Transformer language models specifically to performance gaps in associative
recall. They argue that prior AR evaluations fall short of capturing these differences and propose
MQAR, which can show the AR performance differences that translate to performance differences
in language modeling performance. Hence, MQAR is especially suitable to analyze the memory
capacity of LLM. Transformer (e.g. Llama) models can be seen as the gold standard for this task as
their memory is exponential in the coding dimension (Ramsauer et al., 2021).
Experiment Setup. There are two relevant variables that determine different experimental setups.
(1)Context Length (CL) : Length of the sequence of one sample  this influences the distances
between the key-value definition and the recall. (2) Number Key-Value Pairs (KV) : Influences how
many key-value pairs the model needs to keep track of. The vocabulary size is always 8192.
34In all experiments, we use two blocks (or layers for the pure LSTM) for all models. LSTM (Block)
model refers to an architecture where a vanilla LSTM is used instead of self-attention inside a
Transformer block.
For each task setup, we train each model with 4 different learning rates (batch size > 24: {1e-2,
2.15e-3, 4.6e-4, 1e-4}, batch size 24: {1e-3, 2.2e-4, 5e-5, 1e-5}). The batch size (BS) changes
depending on the context length (CL) (CL=64/128: BS=512; CL=256: BS=256; CL=756: BS=128;
CL=1024: BS=96; CL=2048: BS=24). We vary the embedding dimension (Model Dim) between
different experiments  different numbers of heads are used accordingly. For each experiment, we
generate 100,000 training samples (validation: 3,000 samples) and train for 64 epochs. We apply
cosine annealing (min lr: 1e-4 and 1e-5) with 10% warm-up steps. We use AdamW (Loshchilov &
Hutter, 2019) and a weight decay of 0.1 for training.
We conduct three different experiments:
MQAR-Experiment 1 evaluates, in the same fashion as Arora et al. (2023), a vari-
ety of models (Llama, Mamba, Mamba (noWT) - i.e. without weight tying, Reten-
tion, Hyena, H3, RWKV-4, RWKV-5, RWKV-6, LSTM, LSTM (Block), xLSTM[0:1],
xLSTM[1:0] and xLSTM[1:1]) on increasing task difficulty by increasing the context length
and number of key-value pairs simultaneously. We benchmark three parameter settings:
CL,KV={(64,4),(128,8),(256,16)}.
MQAR-Experiment 2 increases the task difficulty notably and goes beyond previous
evaluations on this task. We individually scale the context length (CL={756, 1024, 2048})
and the key-value pairs (KV={48, 96, 256}) and evaluate all combinations. This experiment
especially probes the memory capacity because the number of key-value pairs is high.
To reduce the computational burden we only evaluate models that perform flawlessly in
Experiment 1  additionally we evaluate Transformer only in the hardest setting (CL=2048)
as sanity check, because no performance decrease is expected.
MQAR-Experiment 3 analyzes whether the AR capability learned on a certain context
length extrapolates to bigger context lengths. For each KV setting of Experiment 2, we use
the models (we select the 3 biggest model dimensions) trained on CL=2048 and evaluate
bigger context lengths (CL={4096, 6144, 8192}).
Extended Results. The result of Experiment 1 can be found in Figure 13. In accordance to the
results of Arora et al. (2023) H3, Hyena, RWKV-4 fail to solve the task with a smaller model
dimension. In contrast, xLSTM[1:1], xLSTM[1:0], Mamba, RWKV-5 and RWKV-6 are able to solve
these settings for all model dimensions. The comparison of xLSTM[0:1] with both original LSTM
variants indicates that the exponential gating mechanism improves the AR capabilities of the model.
However, both fall short because of the reduced memory capacity compared to xLSTM[1:1] and
xLSTM[1:0].
The results of Experiment 2 are presented in Figure 14. Scaling the context length has a low impact
on the performance of the models. However, while xLSTM[1:1] and xLSTM[1:0] show no clear
decay, both RWKV variants slightly, but consistently lose performance with increasing context
lengths. The varying number of key-value pairs, which mainly probes the memory capacity of the
non-Transformer models, has a more notable impact across all models. RWKV-5 seems to outperform
RWKV-6. The latter fails to learn the task at all in some KV=256 settings. Overall xLSTM[1:1] is the
best-performing non-Transformer model  suggesting that it provides enhanced memory capacity,
also in long contexts.
Figure 15 shows the extrapolation results from Experiment 3. For xLSTM[1:1], xLSTM[1:0], and
Mamba the model performance does not change in the extrapolation setting. The RWKV models
(especially RWKV5) degrade slightly with increasing context length. xLSTM[1:1] performs best, as
it maintains its superior performance of Experiment 2.
4The keys are distributed on the evaluation part of the sequence given a power-law distribution. This is
motivated by similar structures in natural language text.
35InputTargetKV = 4 / CL = 18 Figure 12: Illustration of the MQAR task. Color pairs represent key-value pairs (keys have darker
shade). The first part of the sequence defines the key-value pairs for the respective sample. After that,
the keys appear randomly according to a power law distribution4. Grey tokens in the input sequence
represent a zero token. The target sequence contains the value after the respective key appearance
 the rest of the tokens are ignored for the accuracy and loss calculation. The model must predict the
value tokens given the respective key.
B.1.3 Test of xLSTMs Long Range Capabilities on the Long Range Arena.
We assess the performance of xLSTM across tasks in the Long Range Arena benchmark (Tay et al.,
2021), examining its ability to effectively handle longer context lengths and diverse data types.
Our experiments on Long Range Arena benchmark are composed of five tasks:
Retrieval : The task is to predict if two documents have a citation link. The dataset of text
documents is derived from the ACL Anthology Network (Radev et al., 2009).
ListOps : This is a set of modular arithmetic tasks including brackets and lists of numbers,
using the operations MIN,MAX,MEDIAN andSUMMOD (modular sum). A particular example
is:[MAX 4 3 [MIN 2 3 ] 1 0 [MEDIAN 1 5 8 9, 2]]  5
Image : This task is based on a version of the CIFAR dataset (Krizhevsky, 2009), where
images are transformed to a sequence of pixels and this sequence has to be classified into the
usual CIFAR classes. We test both a gray-scale (G-Image) and RGB (RGB-Image) version
of this dataset, as Orvieto et al. (2023) uses colored images contrary to the standard setup.
Pathfinder : The input for this task is a 32x32 gray-scale image, given as pixel sequence,
with two dots and several curved lines on it. The task is to predict if the two dots are
connected by any of the lines (Linsley et al., 2018).
We omit the Text classification task (Maas et al., 2011), as the language modeling experiments already
test this kind of data, and the Pathfinder-X version of Pathfinder .
Experiment Setup. The architectures that are tested in this experiment comprise LLama, Mamba,
LSTM, RWKV-4, and xLSTM. LSTM (Block) refers to an architecture where a vanilla LSTM is used
inside a post up-projection block (like Transformer with attention replaced by LSTM). For xLSTM
we choose the best performing of xLSTM[0:1] or xLSTM[1:0] on the validation set, specifically the
former for the Image tasks and the latter for all other ones.
We use the hyperparameter settings of the S5 model (Smith et al., 2022) and Linear Recurrent Unit
model (Orvieto et al., 2023), with additional hyperparamter search on learning rates and schedulers
for all models. We use two different schedulers: Linear Warm-up Cosine Annealing and Linear
Warm-up Cosine Annealing with Restarts. Both learning rate schedulers were evaluated with learning
rates of 1e-3, 6e-4 and 1e-4. For the second scheduler, the number of restarts ( R) is set to 3. The
model hyperparameters for each dataset are displayed in Table 5.
Results. Table 6 shows the result of experiments on the Long Range Arena benchmark. xLSTM
demonstrates consistent strong performance on all of the tasks, suggesting that the proposed architec-
ture is remarkably efficient in handling different aspects of long context problems.
360.00.20.40.60.81.0Transformer
Accuracy
Context Length = 64
 Context Length = 128
 Context Length = 256
0.00.20.40.60.81.0(x)LSTM-Family
Accuracy
0.00.20.40.60.81.0Mamba
Accuracy
0.00.20.40.60.81.0RWKV
Accuracy
32 64 128 256 512
Model Dim0.00.20.40.60.81.0Others
Accuracy
32 64 128 256 512
Model Dim
32 64 128 256 512
Model Dim
Llama
Mamba
Mamaba (noWT)RWKV-4
RWKV-5
RWKV-6Retention
Hyena
H3xLSTM[0:1]
xLSTM[1:0]
xLSTM[1:1]LSTM (Block)
LSTMFigure 13: Result of MQAR-Experiment 1. The columns show different task settings (context length
and key-value pairs). The rows group related models for better clarity. The x-axis gives the model
size and the y-axis the validation accuracy.
370.00.20.40.60.81.0Context Length = 756
Accuracy
Key-Value Pairs = 48
 Key-Value Pairs = 96
 Key-Value Pairs = 256
0.00.20.40.60.81.0Context Length = 1024
Accuracy
32 64 128 256 512
Model Dim0.00.20.40.60.81.0Context Length = 2048
Accuracy
32 64 128 256 512
Model Dim
32 64 128 256 512
Model Dim
Llama Mamba RWKV-5 RWKV-6 xLSTM[1:0] xLSTM[1:1]Figure 14: Result of MQAR-Experiment 2. The columns and rows correspond to different numbers
of key-value pairs and the context length respectivly. The x-axis gives the model size and the y-axis
the validation accuracy.
Task #BlocksEmbedding
DimBatch
SizeTraining
Steps
Retrieval 6 128 64 100k
ListOps 8 128 32 80k
Pathfinder 6 192 64 500k
G-Image 6 512 64 180k
RGB-Image 6 512 64 180k
Table 5: Long Range Arena model hyperparameters. These are the model hyperparameters used in
each of the Long Range Arena tasks. For each model we used the best learning rate and the better of
the two learning rate schedulers.
380.000.250.500.751.00Context Length = 4096
Accuracy
Key-Value Pairs = 48
 Key-Value Pairs = 96
 Key-Value Pairs = 256
0.000.250.500.751.00Context Length = 6144
Accuracy
128 256 512
Model Dim0.000.250.500.751.00Context Length = 8192
Accuracy
128 256 512
Model Dim
128 256 512
Model Dim
Mamba RWKV-5 RWKV-6 xLSTM[1:0] xLSTM[1:1]Figure 15: Result of MQAR-Experiment 3 (Extrapolation). All evaluated models were trained on
context length 2048 and the number of key-value pairs given by the columns of the plot. The rows
show the different context lengths used in the evaluation. The x-axis gives the model size and the
y-axis the validation accuracy.
Retrieval
accListOps
accPathfinder
accG-Image
accRGB-Image
accRanking
acc
Random Baseline 0.500 0.100 0.500 0.100 0.100
Llama 0.845 0.379 0.887 0.541 0.629 5.2
Mamba 0.902 0.325 0.992 0.689 0.765 2.2
RWKV-4 0.898 0.389 0.914 0.691 0.757 3.0
LSTM X 0.275 X 0.675 0.718 5.4
LSTM (Block) 0.880 0.495 X 0.690 0.756 3.4
xLSTM 0.906 0.411 0.919 0.695 0.761 1.6
Table 6: Long Range Arena test accuracy. Bold highlights the best performing model, underlined
the second best. X denotes models that fail to outperform random baselines. xLSTM is the best of
xLSTM[1:0], xLSTM[0:1] based on validation dataset accuracy.
39B.2 Method Comparison and Ablation Study on SlimPajama (15B)
General Training Procedure. We tokenize our datasets using the HuggingFace GPT-2 tokenizer
(Radford et al., 2019; Brown et al., 2020)5and use this tokenizer for all models in this paper. In
general, we try to follow Brown et al. (2020) for the general training setup, i.e. we choose context
length 2048 and batch sizes 256 or 512 for our models. We use the AdamW (Loshchilov & Hutter,
2019) optimizer with beta parameters ( β1,β2)=(0.9, 0.95) and an epsilon parameter of 1e-5. As
learning rate scheduler we use a linear warm-up with 750 steps and cosine decay to 10% of the
peak learning rate. We apply a weight decay of 0.1 to all our models and always exclude the token
embedding matrix from weight decay. If not specified otherwise, we do not tie the weights of
the token embedding and the language model head. For parallelization, we use PyTorch FSDP in
SHARD_GRAD_OP mode with mixed precision in bfloat16 , where applicable. For small models we
useNO_SHARD . We keep the weights in float32 and reduce the gradients across GPUs in float32 .
We use torch.compile to speed up models, except for Transformer models as their training curves
did not match the non-compiled versions. For xLSTM[7:1], we use positions [3, 5, 7, 40, 42, 44] for
sLSTM-based blocks, except for the 125M size, where we use [3, 20] (this is actually a [11:1] ratio).
Model EmbeddingDim #Blocks #Heads/HeadDim#Params
MPeak LR
(15B)Peak LR
(300B)125MRWKV-4 768 12 - 169.4 3e-3 6e-4
Llama 768 12 12 / 64 162.2 3e-3 3e-3
Mamba 768 24 - 167.8 3e-3 3e-3
xLSTM 768 24 4 / 384 163.8 1e-3 1.5e-3350MRWKV-4 1024 24 - 430.5 1-e3 4e-4
Llama 1024 24 16 / 64 406.6 1.5e-3 1.5e-4
Mamba 1024 48 - 423.1 1.5e-3 1.5e-3
xLSTM 1024 48 4 / 512 409.3 1e-3 7.5e-4760MRWKV-4 1536 24 - 891.0 1e-3 2.5e-4
Llama 1536 24 16 / 96 834.1 1.25e-3 1.25e-4
Mamba 1536 48 - 870.5 1.25e-3 1.25e-3
xLSTM 1536 48 4 / 768 840.4 9e-4 6.25e-41.3BRWKV-4 2048 24 - 1515.2 1e-3 2e-4
Llama 2048 24 32 / 64 1420.4 1e-3 1e-3
Mamba 2048 48 - 1475.3 1e-3 1e-3
xLSTM 2048 48 4 / 1024 1422.6 9e-4 5e-42.7BRWKV-4 2560 32 - 2984.8 8e-4 -
Llama 2560 32 32 / 80 2779.5 8e-4 -
Mamba 2560 64 - 2897.2 8e-4 -
xLSTM 2560 64 4 / 1280 2788.3 8e-4 -
Table 7: Peak learning rates and model dimensions for scaling law plots.
Details on Comparison to Other Methods. For the model comparison on 15B training tokens
of SlimPajama we train all models with context length 2048 and batch size 256. We use a peak
learning rate of 1e-3 for all models for comparability. The learning rate decays over 30k training
steps. The models are compared after one epoch at training step 28170. As model implementations
we use the original repositories code for Mamba (Gu & Dao, 2023)6, RWKV-5, RWKV-6 (Peng
et al., 2024)7. For RWKV-4, we use a cleaned and validated re-implementation based on the
original repo and kernels (Peng et al., 2023). For HGRN (Qin et al., 2023), GLA (Yang et al.,
2023), HGRN2 (Qin et al., 2024) we use the a re-implementation by the authors of GLA (Yang
5https://huggingface.co/docs/transformers/en/model_doc/gpt2
6https://github.com/state-spaces/mamba
7https://github.com/BlinkDL/RWKV-LM/
40et al., 2023; Yang & Zhang, 2024)8. For GPT-3 and Llama-like Transformers, we use our own
implementations based on PyTorch. Note that for all xLSTMs, Transformers, Mamba and RWKV-4,
we use Mixed Precision training with bfloat16 and weights in float32 precision, while resorting
to full bfloat16 precision (weights and operations) for all other models due to their custom kernels
that force one precision internally. Following the general training procedure we use torch.compile
for all models, except for models using the flash-linear-attention (Yang & Zhang, 2024)
library because of compilation problems.
General Details on Ablation Studies. We follow our general training procedure and train all
models with context length 2048, batch size 256 and peak learning rate 1e-3. We report perplexity
values on the validation set.
Additional Ablation Study on Matrix Memory. As default block configuration we use the
mLSTM in the pre up-projection block (see Figure 10) and the sLSTM in the post up-projection block
(see Figure 9). In this experiment we study combination of mLSTM with different block variants
using the xLSTM[1:0] architecture. We compare the mLSTM in a post up-projection block (see
Figure 3 and 9) with ReLU2activation function and non-gated feed-forward network to mLSTM
in a pre up-projection block with and without a dimension-wise causal convolution. Table 8 shows
that the matrix memory benefits from the pre up-projection block structure, and that the convolution
within this block is important.
Model Details #BlocksEmbedding
Dim#Params
MSlimPajama
(15B) ppl 
xLSTM[1:0]Post Up-Projection Block (ReLU2) 24 1024 430.4 13.90
Pre Up-Projection Block, No Convolution 48 1024 408.8 15.41
Pre Up-Projection Block, With Convolution 48 1024 409.3 13.43
Table 8: Matrix Memory variants. We study different configurations for the matrix memory. Matrix
memory in the pre up-projection block performs best and gives xLSTM[1:0]. Notably, it seems that
the dimension-wise causal convolution within the pre up-projection block is important.
Details on new xLSTM Components Ablation Study. In Table 2 (top), we show our modifications
to the vanilla LSTM that transform the vanilla LSTM into the xLSTM. We start with a large
default PyTorch LSTM with 24 layers and 1536 hidden size. Due to a lack of skip-connections
and LayerNorms, vanilla LSTMs of this size are not trainable. We then add skip-connections and
pre-LayerNorms before each LSTM layer corresponding to a residual architecture. This enables
training for LSTMs at this scale. Replacing every second LSTM layer by a non-gated feed-forward
network with GeLU activation function (similar to Vaswani et al.), which corresponds to the post
up-projection backbone (see Figure 3) further boosts performance. Adding Exponential Gating to this
architecture yields the sLSTM as depicted in Figure 9, with another large performance improvement.
Finally, adding the best Matrix Memory variant found in Table 8 by replacing some sLSTM blocks
with the mLSTM (see Figure 10) gives xLSTM[7:1] with the best performance.
Details on Gating Technique Ablation Study. In Table 2 (bottom), we investigate the effect
of trainable and input-dependent gates for mLSTM. The results show that, in contrast to other
methods (Katharopoulos et al., 2020; Sun et al., 2023; Qin et al., 2023; Katsch, 2023; Yang et al.,
2023; Qin et al., 2024; Peng et al., 2024), having the gates both learnable and input dependent gives
the best results.
Details on Scaling Experiments. We follow our general training procedure (see paragraph above)
and train all models, including the 1.3B and 2.7B model sizes, with context length 2048 and batch
size 256. We use the peak learning rates from Table 7.
8https://github.com/sustcsonglin/flash-linear-attention
41B.3 xLSTM Large Language Models  SlimPajama300B
General Training Procedure. We use the same general training procedure as in Section B.2 with
peak learning rates from Table 7. All models are trained with context length 2048. The 125M, 350M
and 760M models are trained with batch size 256 for 600k training steps, whereas the 1.3B models
are trained with batch size 512 for 300k training steps. We keep the same learning rate scheduler
across all models.
Details on Downstream Evaluation. We use the LM Evaluation Harness from
EleutherAI (Sutawika et al., 2023) for evaluating the following tasks that measure common
sense reasoning: LAMBADA (OpenAI version in LM Evaluation Harness) (Paperno et al., 2016),
HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), ARC-challenge, ARC-easy (Clark et al.,
2018), WinoGrande (Sakaguchi et al., 2021). This selection of downstream tasks is inspired by (Gu
& Dao, 2023).
Following Gu & Dao (2023), we report accuracy for LAMADA, WinoGrande, PIQA, and ARC-easy,
and accuracy normalized by sequence length for HellaSwag and ARC-challenge.
We evaluate all models in full float32 , full bfloat16 andbfloat16 Mixed Precision with weights
infloat32 . For each model we select the best value respectively.
Details on PALOMA. We use 16 out of the 18 data sources of the PALOMA dataset (Magnusson
et al., 2023). We use C4 (Raffel et al., 2019), MC4-EN (Xue et al., 2021), Wikitext-103 (Merity
et al., 2017), PennTreebank (Vadas & Curran, 2011), RedPajama (TogetherComputer, 2023), Fal-
con Refinedweb (Refined Web) (Penedo et al., 2023), Dolma v1.5 (Soldaini et al., 2023), M2D2
S2ORC, M2D2 Wikipedia (Reid et al., 2022), C4-100-Domains (C4 Domains) (Chronopoulou et al.,
2022), Dolma-100-Subreddits (Dolma Subreddits) (Soldaini et al., 2023), Dolma-100-Programming
Languages (Dolma Coding) (Soldaini et al., 2023; Kocetkov et al., 2022), TwitterAAE (Blodgett
et al., 2016; Liang et al., 2023), Manosphere Corpus (Ribeiro et al., 2021), GAB Corpus (Zannettou
et al., 2018), 4CHAN Corpus (Papasavva et al., 2020). We leave out ThePile (Gao et al., 2021) and
ICE (Greenbaum & Nelson, 1996) as they are not part of Palomas Huggingface dataset repository9.
A detailed description of these datasets can be found in Magnusson et al. (2023, Table 2). All models
are evaluated in bfloat16 Mixed Precision.
Results on the data sources TwitterAAE, Manosphere, GAB and 4CHAN are reported in Table 9 and
for each individual dataset the results are given in Section C.
Model#Params
MTwitter
AAEManosphere 4CHAN GAB125MRWKV-4 169.4 265.80 39.31 18.48 53.89
Llama 162.2 277.93 32.98 14.03 56.45
Mamba 167.8 258.17 32.14 14.01 51.58
xLSTM[1:0] 163.8 244.53 31.45 13.27 51.00
xLSTM[7:1] 163.7 248.51 30.90 13.45 50.25350MRWKV-4 430.5 216.17 30.25 13.82 42.25
Llama 406.6 231.09 25.90 11.49 43.04
Mamba 423.1 202.88 25.24 11.60 40.78
xLSTM[1:0] 409.3 200.61 24.58 11.20 39.83
xLSTM[7:1] 408.4 206.25 24.73 11.31 39.86760MRWKV-4 891.0 195.27 24.66 12.00 35.73
Llama 834.1 205.50 22.69 10.40 37.68
Mamba 793.2 182.74 22.58 10.47 36.25
xLSTM[1:0] 840.4 179.74 21.66 10.11 35.33
xLSTM[7:1] 839.7 180.19 21.78 10.22 34.891.3BRWKV-4 1515.2 174.87 23.51 11.34 33.18
Llama 1420.4 192.52 20.67 9.67 34.84
Mamba 1475.3 171.38 20.37 9.80 32.01
xLSTM[1:0] 1422.6 166.16 19.94 9.64 31.90
xLSTM[7:1] 1420.1 171.36 20.28 9.64 32.17
Table 9: Perplexity values per domain.
9https://huggingface.co/datasets/allenai/paloma
42In order to evaluate the perplexity values on each data source, we split the text documents into
sequences of length 2048, which corresponds to the pre-training context length of all models. For
documents longer than 2048 tokens we split each document into non-overlapping input sequences. In
this case for the last input sequence, we follow the LM Evaluation Harness and fill up the full 2048
token context window with previous tokens, but compute the perplexity only on the remaining tokens.
We compute the token perplexities per data source in Table 4 as the exponential of the negative
loglikelihoods per domain weighted by the number of tokens per domain in that data source as it is
defined in Magnusson et al. (2023, Equation 1)
43C Detailed Results on PALOMA Language Model Evaluation
We report the perplexity values on each of the 571 subdomains of PALOMA in Table 10. Note that
the aggregated perplexity values in Table 4 are not macro averages of the values shown in Table 10.
Dataset Llama Mamba RWKV-4 xLSTM[7:1] xLSTM[1:0]
#Params (M) 1420 1475 1515 1420 1423
4chan_meta_sep_val-00000000 9.58 9.72 11.37 9.53 9.55
4chan_meta_sep_val-00000001 9.95 10.06 11.57 9.91 9.88
4chan_meta_sep_val-00000002 9.42 9.53 11.00 9.40 9.38
4chan_meta_sep_val-00000003 9.78 9.93 11.48 9.77 9.77
c4_100dom_val_100_www.ign.com 16.22 15.75 17.10 15.67 15.43
c4_100dom_val_10_www.eventbrite.com 12.72 12.33 13.33 12.30 12.12
c4_100dom_val_11_link.springer.com 8.66 8.54 9.31 8.42 8.33
c4_100dom_val_12_www.chicagotribune.com 12.09 11.60 12.49 11.55 11.37
c4_100dom_val_13_www.foxnews.com 9.59 9.21 9.83 9.16 9.08
c4_100dom_val_14_www.aljazeera.com 10.97 10.61 11.31 10.50 10.40
c4_100dom_val_15_www.dailymail.co.uk 12.42 11.97 12.87 11.85 11.69
c4_100dom_val_16_www.ncbi.nlm.nih.gov 7.39 7.31 7.98 7.11 7.07
c4_100dom_val_17_www.express.co.uk 11.57 11.04 11.84 10.99 10.79
c4_100dom_val_18_en.m.wikipedia.org 9.28 8.95 9.52 8.89 8.80
c4_100dom_val_19_www.cnet.com 12.61 12.23 13.12 12.09 11.97
c4_100dom_val_1_www.nytimes.com 13.13 12.66 14.04 12.68 12.44
c4_100dom_val_20_www.telegraph.co.uk 13.71 13.10 14.28 13.06 12.88
c4_100dom_val_21_www.theatlantic.com 14.70 14.17 15.54 14.17 13.97
c4_100dom_val_22_forums.macrumors.com 17.77 17.34 19.15 17.22 16.95
c4_100dom_val_23_www.oreilly.com 13.36 12.99 14.31 13.02 12.88
c4_100dom_val_24_www.washingtonpost.com 12.06 11.58 12.98 11.64 11.41
c4_100dom_val_25_www.zdnet.com 13.22 12.86 13.80 12.78 12.61
c4_100dom_val_26_www.foxbusiness.com 9.32 9.03 9.58 8.92 8.81
c4_100dom_val_27_www.reuters.com 10.67 10.13 11.16 10.13 9.97
c4_100dom_val_28_www.ibtimes.co.uk 11.36 11.01 11.71 10.89 10.76
c4_100dom_val_29_www.rt.com 13.59 12.96 14.24 12.98 12.74
c4_100dom_val_2_en.wikipedia.org 10.75 10.45 11.32 10.32 10.19
c4_100dom_val_30_www.prweb.com 11.18 10.88 11.92 10.83 10.65
c4_100dom_val_31_www.deviantart.com 21.78 21.05 22.78 21.00 20.69
c4_100dom_val_32_www.si.com 11.49 11.00 11.92 10.90 10.76
c4_100dom_val_33_www.bbc.com 9.35 8.91 9.41 8.80 8.70
c4_100dom_val_34_github.com 11.57 11.49 12.94 11.40 11.28
c4_100dom_val_35_nypost.com 14.31 13.41 15.29 13.62 13.31
c4_100dom_val_36_itunes.apple.com 16.49 15.88 17.15 15.98 15.69
c4_100dom_val_37_www.instructables.com 16.75 16.33 17.73 16.28 15.97
c4_100dom_val_38_www.youtube.com 8.42 8.24 8.83 8.22 8.07
c4_100dom_val_39_www.booking.com 8.84 8.49 8.83 8.41 8.32
c4_100dom_val_40_www.etsy.com 11.93 11.66 12.66 11.52 11.43
c4_100dom_val_41_www.marketwired.com 7.66 7.47 7.88 7.33 7.27
c4_100dom_val_42_sites.google.com 14.23 13.81 14.91 13.68 13.51
c4_100dom_val_43_www.baltimoresun.com 11.57 11.16 11.96 11.09 10.95
c4_100dom_val_44_www.agreatertown.com 13.56 12.94 13.57 12.77 12.64
c4_100dom_val_45_www.npr.org 10.59 10.30 11.14 10.19 10.12
c4_100dom_val_46_www.fool.com 11.03 10.63 11.35 10.56 10.42
c4_100dom_val_47_www.tripadvisor.com 15.80 15.26 16.26 15.10 14.93
c4_100dom_val_48_www.bbc.co.uk 12.55 12.10 13.02 12.00 11.85
44Dataset Llama Mamba RWKV-4 xLSTM[7:1] xLSTM[1:0]
c4_100dom_val_49_lists.w3.org 18.75 18.24 19.89 18.05 17.84
c4_100dom_val_4_www.latimes.com 11.88 11.46 12.40 11.39 11.24
c4_100dom_val_50_mashable.com 12.44 11.95 12.85 11.90 11.76
c4_100dom_val_51_disneyparksmomspanel.disney.go.com 11.99 11.29 11.98 11.16 11.00
c4_100dom_val_52_www.cnbc.com 10.65 10.32 10.99 10.24 10.10
c4_100dom_val_53_answers.sap.com 23.59 23.09 25.71 22.99 22.55
c4_100dom_val_54_homestars.com 14.13 13.70 14.51 13.65 13.52
c4_100dom_val_55_www.hindustantimes.com 12.13 11.60 12.74 11.60 11.37
c4_100dom_val_56_www.reference.com 11.57 11.04 11.75 10.92 10.79
c4_100dom_val_57_www.city-data.com 18.38 17.94 19.61 17.73 17.62
c4_100dom_val_58_medium.com 15.50 15.09 16.58 15.18 15.01
c4_100dom_val_59_app-wiringdiagram... 9.74 9.10 9.68 8.88 8.75
c4_100dom_val_5_www.theguardian.com 14.78 14.09 15.47 14.08 13.86
c4_100dom_val_60_www.csmonitor.com 15.35 14.85 15.92 14.75 14.57
c4_100dom_val_61_www.adweek.com 14.55 13.95 15.58 14.09 13.81
c4_100dom_val_62_docs.microsoft.com 7.69 7.79 8.86 7.68 7.58
c4_100dom_val_63_www.yahoo.com 9.29 8.88 9.71 8.89 8.77
c4_100dom_val_64_www.thesun.co.uk 12.18 11.66 12.74 11.59 11.39
c4_100dom_val_65_www.nydailynews.com 12.15 11.60 12.61 11.56 11.36
c4_100dom_val_66_www.dailystar.co.uk 10.65 10.17 11.03 10.09 9.92
c4_100dom_val_67_fineartamerica.com 12.06 11.58 12.29 11.46 11.36
c4_100dom_val_68_www.kickstarter.com 13.85 13.58 15.38 13.55 13.38
c4_100dom_val_69_uk.reuters.com 9.54 9.13 9.90 9.07 8.92
c4_100dom_val_6_www.huffpost.com 13.45 13.03 13.96 12.99 12.83
c4_100dom_val_70_www.insiderpages.com 13.24 12.84 13.55 12.77 12.64
c4_100dom_val_71_www.inquisitr.com 12.12 11.58 12.86 11.71 11.38
c4_100dom_val_72_lists.debian.org 18.18 17.81 19.62 17.67 17.30
c4_100dom_val_73_www.straitstimes.com 11.51 11.06 11.91 10.94 10.79
c4_100dom_val_74_www.cbsnews.com 10.29 9.91 10.60 9.82 9.72
c4_100dom_val_75_simple.wikipedia.org 8.25 7.85 8.37 7.78 7.67
c4_100dom_val_76_deadline.com 14.75 13.83 15.48 13.92 13.51
c4_100dom_val_77_www.androidheadlines.com 11.11 10.74 11.43 10.72 10.59
c4_100dom_val_78_www.wired.com 14.42 13.88 15.14 13.87 13.68
c4_100dom_val_79_www.bustle.com 12.79 12.33 13.19 12.25 12.09
c4_100dom_val_7_patents.google.com 7.59 7.84 9.33 7.72 7.59
c4_100dom_val_80_premium.wpmudev.org 16.86 16.63 18.13 16.50 16.29
c4_100dom_val_81_www.librarything.com 14.36 13.98 15.42 13.91 13.75
c4_100dom_val_82_mail-archives.apache.org 5.67 5.61 6.17 5.56 5.49
c4_100dom_val_83_scholars.duke.edu 8.72 8.43 9.03 8.32 8.21
c4_100dom_val_84_www.glassdoor.com 16.64 15.97 16.99 16.00 15.83
c4_100dom_val_85_www.pcworld.com 12.34 11.95 12.95 11.90 11.72
c4_100dom_val_86_www.shutterstock.com 8.70 8.89 10.75 8.62 8.52
c4_100dom_val_87_myemail.constantcontact.com 14.59 14.24 15.32 14.18 13.98
c4_100dom_val_88_www.eventbrite.co.uk 14.47 13.99 14.89 13.98 13.79
c4_100dom_val_89_www.fastcompany.com 14.24 13.75 15.52 13.82 13.56
c4_100dom_val_8_www.businessinsider.com 10.97 10.69 11.35 10.52 10.46
c4_100dom_val_90_www.firstpost.com 11.71 11.24 12.08 11.12 10.96
c4_100dom_val_91_www.entrepreneur.com 13.10 12.68 13.65 12.72 12.54
c4_100dom_val_92_www.breitbart.com 13.47 12.67 14.29 12.84 12.56
c4_100dom_val_93_techcrunch.com 14.20 13.68 15.18 13.82 13.58
c4_100dom_val_94_www.nme.com 14.12 13.28 15.06 13.43 13.12
c4_100dom_val_95_www.ndtv.com 10.66 10.26 10.90 10.10 10.00
45Dataset Llama Mamba RWKV-4 xLSTM[7:1] xLSTM[1:0]
c4_100dom_val_96_finance.yahoo.com 9.96 9.55 10.22 9.43 9.34
c4_100dom_val_97_archives.lib.state.ma.us 6.53 6.12 7.09 6.27 5.85
c4_100dom_val_98_www.gsmarena.com 23.21 22.15 24.52 22.10 21.76
c4_100dom_val_99_www.lonelyplanet.com 11.33 10.92 12.28 10.84 10.69
c4_100dom_val_9_www.forbes.com 13.72 13.31 14.63 13.34 13.13
c4_en_val-00000000 14.34 13.70 14.87 13.67 13.46
c4_en_val-00000001 14.86 14.28 15.51 14.21 14.09
c4_en_val-00000002 15.29 14.71 15.95 14.71 14.51
c4_en_val-00000003 12.95 12.28 13.32 12.23 12.06
c4_en_val-00000004 12.56 12.13 13.27 12.05 11.87
c4_en_val-00000005 12.77 12.35 13.26 12.32 12.18
dolma-v1_5_val_books 13.00 12.44 13.64 12.44 12.27
dolma-v1_5_val_common-crawl 16.86 16.37 18.00 16.35 16.10
dolma-v1_5_val_pes2o 9.42 9.56 11.25 9.41 9.29
dolma-v1_5_val_reddit_uniform 23.04 21.97 23.84 22.05 21.80
dolma-v1_5_val_stack_uniform 2.30 2.33 2.53 2.30 2.29
dolma-v1_5_val_wiki 10.86 10.48 11.25 10.41 10.31
dolma_100_proglang_val_00_text 5.61 6.30 6.94 5.67 5.69
dolma_100_proglang_val_01_markdown 3.16 3.16 3.56 3.15 3.11
dolma_100_proglang_val_02_c 1.84 1.91 2.23 1.86 1.85
dolma_100_proglang_val_03_php 1.75 1.75 1.83 1.73 1.72
dolma_100_proglang_val_04_java 1.96 1.99 2.18 1.95 1.95
dolma_100_proglang_val_05_c++ 2.19 2.25 2.53 2.21 2.19
dolma_100_proglang_val_06_python 2.35 2.39 2.62 2.36 2.34
dolma_100_proglang_val_07_javascript 2.54 2.59 2.83 2.53 2.53
dolma_100_proglang_val_08_html 1.92 1.94 2.13 1.91 1.91
dolma_100_proglang_val_09_c# 2.23 2.28 2.45 2.19 2.24
dolma_100_proglang_val_10_yaml 2.93 3.01 3.71 2.94 2.92
dolma_100_proglang_val_11_go 1.75 1.78 1.97 1.77 1.75
dolma_100_proglang_val_12_typescript 2.17 2.20 2.41 2.18 2.16
dolma_100_proglang_val_13_xml 2.44 2.50 2.78 2.46 2.48
dolma_100_proglang_val_14_css 2.25 2.25 2.34 2.21 2.20
dolma_100_proglang_val_15_jupyter-nb 1.57 1.60 1.75 1.58 1.58
dolma_100_proglang_val_16_rust 1.96 2.01 2.23 1.97 1.96
dolma_100_proglang_val_17_unity3d-asset 4.01 4.17 4.56 4.10 4.05
dolma_100_proglang_val_18_gettext-catalog 2.84 2.87 3.53 2.86 2.83
dolma_100_proglang_val_19_ruby 2.41 2.44 2.70 2.39 2.38
dolma_100_proglang_val_20_vue 1.95 1.95 2.10 1.94 1.93
dolma_100_proglang_val_21_sql 2.18 2.23 2.46 2.17 2.16
dolma_100_proglang_val_22_swift 1.86 1.88 2.04 1.86 1.84
dolma_100_proglang_val_23_kotlin 2.05 2.07 2.29 2.07 2.04
dolma_100_proglang_val_24_scala 2.24 2.28 2.64 2.25 2.23
dolma_100_proglang_val_25_scss 2.26 2.27 2.38 2.24 2.24
dolma_100_proglang_val_26_tex 4.04 4.21 4.97 4.10 4.04
dolma_100_proglang_val_27_dart 1.79 1.82 2.01 1.80 1.78
dolma_100_proglang_val_28_kicad 2.57 2.79 3.86 2.68 2.67
dolma_100_proglang_val_29_shell 3.71 3.74 4.31 3.69 3.63
dolma_100_proglang_val_30_smali 1.38 1.39 1.45 1.38 1.37
dolma_100_proglang_val_31_lua 5.65 6.01 7.18 5.33 5.45
dolma_100_proglang_val_32_restructuredtext 4.01 4.05 4.66 3.97 3.92
dolma_100_proglang_val_33_perl 2.57 2.62 3.01 2.59 2.55
dolma_100_proglang_val_34_diff 2.87 2.95 3.43 2.89 2.86
46Dataset Llama Mamba RWKV-4 xLSTM[7:1] xLSTM[1:0]
dolma_100_proglang_val_35_ini 3.91 4.16 4.90 4.05 3.98
dolma_100_proglang_val_36_jsx 1.83 1.84 1.95 1.83 1.82
dolma_100_proglang_val_37_haskell 2.94 3.07 3.73 3.02 2.95
dolma_100_proglang_val_38_gnuplot 2.65 2.88 3.36 2.81 2.77
dolma_100_proglang_val_39_postscript 19.09 19.52 19.56 18.66 18.64
dolma_100_proglang_val_40_groff 6.13 6.32 7.45 6.22 6.21
dolma_100_proglang_val_41_turtle 2.35 2.45 3.17 2.39 2.35
dolma_100_proglang_val_42_fortran 2.32 2.39 2.83 2.35 2.31
dolma_100_proglang_val_43_makefile 2.93 3.01 3.51 2.86 2.82
dolma_100_proglang_val_44_mathematica 10.34 11.34 13.24 10.49 10.71
dolma_100_proglang_val_45_pascal 4.18 4.81 5.49 4.17 4.27
dolma_100_proglang_val_46_common-lisp 2.56 2.71 3.32 2.62 2.58
dolma_100_proglang_val_47_gas 2.49 2.73 3.59 2.57 2.53
dolma_100_proglang_val_48_vhdl 3.91 4.06 4.69 3.92 3.90
dolma_100_proglang_val_49_julia 3.25 3.36 4.05 3.30 3.26
dolma_100_proglang_val_50_edn 1.99 2.10 2.67 2.04 2.03
dolma_100_proglang_val_51_visual-basic 2.42 2.49 2.72 2.37 2.38
dolma_100_proglang_val_52_powershell 4.08 4.16 4.50 3.86 3.89
dolma_100_proglang_val_53_g-code 2.26 2.66 3.29 2.44 2.37
dolma_100_proglang_val_54_ocaml 3.06 3.29 4.22 3.19 3.13
dolma_100_proglang_val_55_java-server-p 2.10 2.11 2.31 2.06 2.09
dolma_100_proglang_val_56_solidity 4.09 4.41 5.28 4.05 4.10
dolma_100_proglang_val_57_graphviz-dot 2.17 2.48 3.54 2.32 2.29
dolma_100_proglang_val_58_less 2.24 2.26 2.33 2.22 2.22
dolma_100_proglang_val_59_twig 1.81 1.81 1.91 1.80 1.79
dolma_100_proglang_val_60_asciidoc 5.33 5.50 6.84 5.43 5.34
dolma_100_proglang_val_61_groovy 2.12 2.15 2.41 2.13 2.11
dolma_100_proglang_val_62_llvm 2.26 2.40 3.25 2.31 2.23
dolma_100_proglang_val_63_hcl 2.52 2.56 2.96 2.52 2.48
dolma_100_proglang_val_64_html+erb 2.10 2.09 2.23 2.08 2.07
dolma_100_proglang_val_65_erlang 2.84 2.98 3.87 2.88 2.85
dolma_100_proglang_val_66_elixir 2.93 2.99 3.58 2.91 2.90
dolma_100_proglang_val_67_eagle 5.35 6.90 10.75 5.64 5.76
dolma_100_proglang_val_68_arduino 3.37 3.40 3.81 3.28 3.28
dolma_100_proglang_val_69_coffeescript 2.80 2.85 3.27 2.80 2.77
dolma_100_proglang_val_70_toml 7.76 7.62 8.44 7.53 7.58
dolma_100_proglang_val_71_cuda 2.15 2.21 2.56 2.19 2.16
dolma_100_proglang_val_72_nix 7.80 7.84 9.03 7.88 7.83
dolma_100_proglang_val_73_smalltalk 9.32 9.61 12.60 9.47 9.20
dolma_100_proglang_val_74_cmake 1.87 1.86 2.02 1.84 1.81
dolma_100_proglang_val_75_actionscript 2.45 2.54 2.88 2.46 2.46
dolma_100_proglang_val_76_glsl 2.40 2.42 2.72 2.36 2.32
dolma_100_proglang_val_77_systemverilog 2.53 2.66 3.17 2.58 2.55
dolma_100_proglang_val_78_haxe 2.74 2.81 3.20 2.77 2.76
dolma_100_proglang_val_79_f# 2.89 3.02 3.53 2.93 2.88
dolma_100_proglang_val_80_max 1.59 1.62 1.80 1.61 1.61
dolma_100_proglang_val_81_objective-c++ 2.18 2.19 2.40 2.17 2.16
dolma_100_proglang_val_82_standard-ml 3.57 4.05 4.79 3.81 3.77
dolma_100_proglang_val_83_dockerfile 4.08 4.17 4.37 4.01 4.05
dolma_100_proglang_val_84_emacs-lisp 3.83 3.83 4.44 3.80 3.72
dolma_100_proglang_val_85_scheme 2.78 2.86 3.40 2.84 2.77
dolma_100_proglang_val_86_clojure 3.18 3.30 4.00 3.26 3.17
47Dataset Llama Mamba RWKV-4 xLSTM[7:1] xLSTM[1:0]
dolma_100_proglang_val_87_handlebars 1.79 1.79 1.88 1.78 1.78
dolma_100_proglang_val_88_smarty 2.30 2.35 2.58 2.29 2.30
dolma_100_proglang_val_89_logos 2.37 2.58 2.98 2.46 2.44
dolma_100_proglang_val_90_stata 4.67 5.08 6.85 4.85 4.81
dolma_100_proglang_val_91_yacc 2.42 2.48 2.87 2.44 2.43
dolma_100_proglang_val_92_nimrod 2.75 2.87 3.63 2.81 2.77
dolma_100_proglang_val_93_tcl 3.00 3.16 3.95 3.07 3.02
dolma_100_proglang_val_94_viml 5.56 5.76 7.21 5.59 5.55
dolma_100_proglang_val_95_asp 1.79 1.79 1.90 1.77 1.77
dolma_100_proglang_val_96_protocol-buffer 1.32 1.31 1.38 1.31 1.32
dolma_100_proglang_val_97_r 2.80 2.92 3.66 2.86 2.81
dolma_100_proglang_val_98_cython 2.34 2.39 2.69 2.36 2.35
dolma_100_proglang_val_99_mediawiki 2.01 2.10 2.48 2.12 2.04
dolma_100_subreddits_val_00_AskReddit 20.25 19.29 20.38 19.28 19.14
dolma_100_subreddits_val_01_politics 22.08 20.70 22.07 20.83 20.61
dolma_100_subreddits_val_02_AmItheAsshole 22.49 21.30 22.89 21.60 21.27
dolma_100_subreddits_val_03_worldnews 22.57 21.43 22.77 21.50 21.23
dolma_100_subreddits_val_04_relationships 18.64 17.80 18.89 17.86 17.67
dolma_100_subreddits_val_05_relationship_advice 19.40 18.53 19.68 18.63 18.46
dolma_100_subreddits_val_06_news 22.49 21.25 22.51 21.49 21.17
dolma_100_subreddits_val_07_leagueoflegends 34.45 32.41 35.13 32.46 32.04
dolma_100_subreddits_val_08_todayilearned 22.53 21.30 22.68 21.28 21.10
dolma_100_subreddits_val_09_TwoXChromosomes 20.20 19.16 20.25 19.20 19.02
dolma_100_subreddits_val_10_personalfinance 18.62 17.65 18.82 17.73 17.64
dolma_100_subreddits_val_11_changemyview 20.02 19.10 20.50 19.17 18.99
dolma_100_subreddits_val_12_unpopularopinion 23.39 22.16 23.63 22.32 22.04
dolma_100_subreddits_val_13_movies 21.62 20.52 21.79 20.64 20.35
dolma_100_subreddits_val_14_Games 22.26 21.15 22.52 21.18 20.87
dolma_100_subreddits_val_15_nba 23.28 21.93 23.60 22.10 21.85
dolma_100_subreddits_val_16_pics 21.84 20.56 21.82 20.64 20.47
dolma_100_subreddits_val_17_gaming 24.45 23.13 24.61 23.15 22.86
dolma_100_subreddits_val_18_soccer 23.38 22.12 23.61 22.19 22.03
dolma_100_subreddits_val_19_nfl 19.86 18.76 20.17 18.81 18.62
dolma_100_subreddits_val_20_explainlikeimfive 18.35 17.21 18.59 17.32 17.03
dolma_100_subreddits_val_21_conspiracy 23.86 22.53 24.09 22.67 22.54
dolma_100_subreddits_val_22_atheism 21.23 20.18 21.43 20.23 20.13
dolma_100_subreddits_val_23_AskMen 20.00 19.04 20.11 19.10 18.94
dolma_100_subreddits_val_24_videos 22.26 21.24 22.51 21.29 21.04
dolma_100_subreddits_val_25_sex 21.13 20.13 21.30 20.09 19.98
dolma_100_subreddits_val_26_raisedbynarcissists 22.07 21.08 22.48 21.20 21.02
dolma_100_subreddits_val_27_NoStupidQuestions 19.66 18.59 19.87 18.68 18.52
dolma_100_subreddits_val_28_DestinyTheGame 35.27 33.58 36.13 33.78 33.37
dolma_100_subreddits_val_29_anime 23.21 22.04 23.46 22.12 21.77
dolma_100_subreddits_val_30_DnD 28.22 26.71 28.78 26.72 26.39
dolma_100_subreddits_val_31_ukpolitics 22.35 21.19 22.80 21.31 21.10
dolma_100_subreddits_val_32_funny 20.78 19.45 20.70 19.40 19.23
dolma_100_subreddits_val_33_europe 21.76 20.59 22.10 20.72 20.52
dolma_100_subreddits_val_34_canada 22.44 21.21 22.44 21.30 21.09
dolma_100_subreddits_val_35_Christianity 17.88 17.02 18.10 17.04 16.94
dolma_100_subreddits_val_36_SquaredCircle 25.87 24.31 25.83 24.34 24.03
dolma_100_subreddits_val_37_AskWomen 17.72 16.81 17.77 16.85 16.72
dolma_100_subreddits_val_38_legaladvice 18.66 17.75 18.92 17.74 17.64
48Dataset Llama Mamba RWKV-4 xLSTM[7:1] xLSTM[1:0]
dolma_100_subreddits_val_39_JUSTNOMIL 24.25 23.16 24.86 23.32 23.02
dolma_100_subreddits_val_40_technology 23.39 22.09 23.52 22.21 21.95
dolma_100_subreddits_val_41_IAmA 19.83 18.83 19.86 18.71 18.56
dolma_100_subreddits_val_42_wow 31.26 29.25 31.44 29.39 28.82
dolma_100_subreddits_val_43_Parenting 20.15 19.11 20.43 19.30 19.06
dolma_100_subreddits_val_44_exmormon 23.12 21.90 23.44 21.99 21.84
dolma_100_subreddits_val_45_AdviceAnimals 22.14 20.96 22.14 20.98 20.79
dolma_100_subreddits_val_46_childfree 21.87 20.85 22.13 20.89 20.72
dolma_100_subreddits_val_47_unitedkingdom 23.27 22.00 23.40 22.00 21.85
dolma_100_subreddits_val_48_ffxiv 32.53 30.79 33.33 31.01 30.62
dolma_100_subreddits_val_49_dndnext 29.67 28.03 30.53 28.26 27.63
dolma_100_subreddits_val_50_ADHD 20.75 19.83 21.14 19.95 19.78
dolma_100_subreddits_val_51_loseit 19.36 18.39 19.49 18.52 18.33
dolma_100_subreddits_val_52_asoiaf 25.28 23.99 25.63 23.94 23.69
dolma_100_subreddits_val_53_BabyBumps 20.96 19.82 21.11 19.92 19.76
dolma_100_subreddits_val_54_Advice 19.17 18.29 19.35 18.38 18.19
dolma_100_subreddits_val_55_australia 23.97 22.51 24.06 22.61 22.40
dolma_100_subreddits_val_56_CFB 20.45 19.41 20.92 19.49 19.23
dolma_100_subreddits_val_57_offmychest 19.63 18.79 19.77 18.93 18.77
dolma_100_subreddits_val_58_PublicFreakout 25.96 24.49 26.02 24.65 24.39
dolma_100_subreddits_val_59_TrueOffMyChest 21.53 20.63 21.70 20.73 20.54
dolma_100_subreddits_val_60_science 20.44 19.46 20.64 19.51 19.38
dolma_100_subreddits_val_61_magicTCG 28.82 26.79 28.94 26.69 26.38
dolma_100_subreddits_val_62_asktransgender 20.72 19.86 21.07 19.83 19.62
dolma_100_subreddits_val_63_DotA2 34.35 32.38 34.74 32.57 32.16
dolma_100_subreddits_val_64_neoliberal 21.74 20.59 22.26 20.64 20.45
dolma_100_subreddits_val_65_whowouldwin 29.18 27.81 30.08 27.63 27.30
dolma_100_subreddits_val_66_depression 18.28 17.52 18.31 17.50 17.41
dolma_100_subreddits_val_67_WTF 22.30 21.18 22.38 21.17 20.99
dolma_100_subreddits_val_68_pathofexile 40.48 38.59 41.43 38.75 38.43
dolma_100_subreddits_val_69_PoliticalDiscussion 20.01 18.92 20.16 18.97 18.82
dolma_100_subreddits_val_70_Libertarian 22.97 21.77 23.15 21.87 21.75
dolma_100_subreddits_val_71_PurplePillDebate 24.94 23.66 25.44 23.85 23.55
dolma_100_subreddits_val_72_Fitness 21.57 20.35 21.48 20.34 20.11
dolma_100_subreddits_val_73_books 21.12 20.02 21.31 20.09 19.82
dolma_100_subreddits_val_74_dogs 20.13 19.12 20.32 19.20 18.92
dolma_100_subreddits_val_75_pcmasterrace 23.73 22.49 24.02 22.56 22.21
dolma_100_subreddits_val_76_teenagers 18.37 16.35 16.44 15.56 17.02
dolma_100_subreddits_val_77_stopdrinking 21.08 20.02 21.19 20.17 19.98
dolma_100_subreddits_val_78_Overwatch 30.47 28.77 31.13 29.13 28.57
dolma_100_subreddits_val_79_television 23.97 22.63 24.05 22.75 22.49
dolma_100_subreddits_val_80_buildapc 21.55 20.22 21.78 20.29 19.98
dolma_100_subreddits_val_81_askscience 17.25 16.39 17.52 16.34 16.11
dolma_100_subreddits_val_82_programming 23.66 22.61 24.04 22.55 22.24
dolma_100_subreddits_val_83_Guildwars2 32.98 31.17 33.58 31.39 30.91
dolma_100_subreddits_val_84_cars 22.57 21.41 22.73 21.38 21.15
dolma_100_subreddits_val_85_formula1 23.85 22.65 24.09 22.71 22.49
dolma_100_subreddits_val_86_sysadmin 24.23 22.90 24.41 22.96 22.64
dolma_100_subreddits_val_87_hockey 21.46 20.26 21.74 20.37 20.20
dolma_100_subreddits_val_88_india 24.15 22.92 24.42 23.08 22.68
dolma_100_subreddits_val_89_SubredditDrama 19.14 18.26 19.63 18.29 18.12
dolma_100_subreddits_val_90_DMAcademy 27.77 26.31 28.38 26.41 26.00
49Dataset Llama Mamba RWKV-4 xLSTM[7:1] xLSTM[1:0]
dolma_100_subreddits_val_91_dating_advice 20.18 19.27 20.42 19.40 19.21
dolma_100_subreddits_val_92_Catholicism 19.11 18.22 19.41 18.17 18.03
dolma_100_subreddits_val_93_Drugs 24.50 23.29 24.74 23.32 23.12
dolma_100_subreddits_val_94_trees 23.56 22.38 23.83 22.41 22.25
dolma_100_subreddits_val_95_boardgames 22.69 21.48 23.13 21.61 21.38
dolma_100_subreddits_val_96_Conservative 22.79 21.53 22.97 21.68 21.53
dolma_100_subreddits_val_97_Futurology 23.55 22.36 23.77 22.37 22.17
dolma_100_subreddits_val_98_beyondthebump 21.07 19.89 21.22 20.08 19.83
dolma_100_subreddits_val_99_weddingplanning 20.11 19.01 20.33 19.19 18.96
falcon-refinedweb_val-00000000 15.92 15.46 17.14 15.37 15.22
falcon-refinedweb_val-00000001 18.49 17.91 19.89 17.90 17.71
falcon-refinedweb_val-00000002 18.45 17.90 19.69 17.91 17.68
falcon-refinedweb_val-00000003 16.75 16.23 17.92 16.16 15.89
falcon-refinedweb_val-00000004 16.26 15.66 17.32 15.73 15.41
falcon-refinedweb_val-00000005 15.41 14.96 16.56 14.92 14.74
gab_val-00000000 33.19 30.55 31.57 30.73 30.32
gab_val-00000001 35.64 32.76 33.96 32.80 32.63
gab_val-00000002 34.38 31.68 32.75 31.80 31.65
gab_val-00000003 34.86 32.05 33.26 32.20 32.00
gab_val-00000004 36.20 33.35 34.58 33.42 33.23
gab_val-00000005 33.46 30.82 31.88 31.06 30.72
gab_val-00000006 35.76 32.77 34.26 33.04 32.74
gab_val-00000007 35.54 32.60 33.76 32.78 32.41
gab_val-00000008 35.11 32.03 33.23 32.25 31.86
gab_val-00000009 34.13 31.34 32.36 31.50 31.30
m2d2_s2orc_unsplit_val_Art 20.07 19.80 21.88 19.78 19.44
m2d2_s2orc_unsplit_val_Philosophy 14.80 14.82 16.77 14.69 14.47
m2d2_s2orc_unsplit_val_astro-ph 11.70 11.70 13.18 11.52 11.33
m2d2_s2orc_unsplit_val_astro-ph.CO 11.47 11.49 12.90 11.37 11.15
m2d2_s2orc_unsplit_val_astro-ph.EP 12.76 12.73 14.28 12.60 12.45
m2d2_s2orc_unsplit_val_astro-ph.GA 11.70 11.70 13.18 11.52 11.33
m2d2_s2orc_unsplit_val_astro-ph.HE 11.85 11.77 13.29 11.62 11.46
m2d2_s2orc_unsplit_val_astro-ph.IM 15.36 15.33 17.16 15.21 14.92
m2d2_s2orc_unsplit_val_astro-ph.SR 13.08 13.08 14.89 12.86 12.70
m2d2_s2orc_unsplit_val_astro-ph_l1 15.36 15.33 17.16 15.21 14.92
m2d2_s2orc_unsplit_val_atom-ph 12.74 12.84 14.44 12.75 12.53
m2d2_s2orc_unsplit_val_chem-ph 13.20 13.29 15.22 13.14 12.97
m2d2_s2orc_unsplit_val_cond-mat 11.67 11.78 13.37 11.67 11.50
m2d2_s2orc_unsplit_val_cond-mat.dis-nn 12.54 12.67 14.28 12.58 12.38
m2d2_s2orc_unsplit_val_cond-mat.mes-hall 11.24 11.50 13.19 11.30 11.10
m2d2_s2orc_unsplit_val_cond-mat.mtrl-sci 12.19 12.33 14.09 12.18 11.91
m2d2_s2orc_unsplit_val_cond-mat.other 11.87 11.96 13.55 11.83 11.65
m2d2_s2orc_unsplit_val_cond-mat.quant-gas 11.67 11.78 13.37 11.67 11.50
m2d2_s2orc_unsplit_val_cond-mat.soft 12.18 12.23 13.93 12.18 12.02
m2d2_s2orc_unsplit_val_cond-mat.stat-mech 12.03 12.14 13.60 12.08 11.89
m2d2_s2orc_unsplit_val_cond-mat.str-el 10.39 10.50 11.98 10.41 10.22
m2d2_s2orc_unsplit_val_cond-mat.supr-con 11.57 11.66 13.13 11.53 11.30
m2d2_s2orc_unsplit_val_cond-mat_l1 12.54 12.67 14.28 12.58 12.38
m2d2_s2orc_unsplit_val_cs.AI 11.71 12.09 14.20 12.01 11.79
m2d2_s2orc_unsplit_val_cs.AR 13.09 13.36 15.30 13.18 12.99
m2d2_s2orc_unsplit_val_cs.CC 8.45 8.81 10.46 8.70 8.54
m2d2_s2orc_unsplit_val_cs.CE 13.21 13.31 15.01 13.18 13.02
50Dataset Llama Mamba RWKV-4 xLSTM[7:1] xLSTM[1:0]
m2d2_s2orc_unsplit_val_cs.CG 8.39 8.68 10.12 8.59 8.47
m2d2_s2orc_unsplit_val_cs.CL 14.66 14.75 16.96 14.70 14.47
m2d2_s2orc_unsplit_val_cs.CR 14.63 14.86 16.72 14.74 14.56
m2d2_s2orc_unsplit_val_cs.CV 12.68 12.78 14.38 12.66 12.49
m2d2_s2orc_unsplit_val_cs.CY 16.01 15.93 17.52 15.84 15.67
m2d2_s2orc_unsplit_val_cs.DB 11.86 12.35 14.66 12.27 12.03
m2d2_s2orc_unsplit_val_cs.DC 13.60 14.02 16.20 13.79 13.56
m2d2_s2orc_unsplit_val_cs.DL 14.67 14.83 17.05 14.75 14.50
m2d2_s2orc_unsplit_val_cs.DM 8.11 8.38 9.84 8.27 8.14
m2d2_s2orc_unsplit_val_cs.DS 9.63 9.99 11.76 9.88 9.69
m2d2_s2orc_unsplit_val_cs.ET 14.80 14.95 17.00 14.89 14.67
m2d2_s2orc_unsplit_val_cs.FL 9.51 9.84 11.64 9.74 9.57
m2d2_s2orc_unsplit_val_cs.GL 16.51 16.43 18.18 16.38 16.21
m2d2_s2orc_unsplit_val_cs.GR 13.45 13.60 15.53 13.54 13.29
m2d2_s2orc_unsplit_val_cs.GT 9.25 9.59 11.34 9.49 9.29
m2d2_s2orc_unsplit_val_cs.HC 16.76 16.93 19.08 16.84 16.66
m2d2_s2orc_unsplit_val_cs.IR 13.30 13.46 15.26 13.31 13.21
m2d2_s2orc_unsplit_val_cs.LG 10.39 10.52 12.14 10.44 10.27
m2d2_s2orc_unsplit_val_cs.LO 9.75 10.23 12.50 10.03 9.81
m2d2_s2orc_unsplit_val_cs.MA 11.24 11.65 14.10 11.41 11.19
m2d2_s2orc_unsplit_val_cs.MM 13.12 13.40 15.29 13.25 13.03
m2d2_s2orc_unsplit_val_cs.MS 13.98 14.14 16.27 14.11 13.89
m2d2_s2orc_unsplit_val_cs.NA 10.53 10.80 12.52 10.71 10.47
m2d2_s2orc_unsplit_val_cs.NE 13.76 14.00 16.10 13.89 13.64
m2d2_s2orc_unsplit_val_cs.NI 10.00 10.22 11.61 10.04 9.93
m2d2_s2orc_unsplit_val_cs.OH 15.24 15.43 17.62 15.34 15.10
m2d2_s2orc_unsplit_val_cs.OS 14.61 14.93 17.35 14.80 14.53
m2d2_s2orc_unsplit_val_cs.PF 12.60 12.82 14.71 12.70 12.48
m2d2_s2orc_unsplit_val_cs.PL 15.43 15.74 18.58 15.65 15.40
m2d2_s2orc_unsplit_val_cs.RO 13.04 13.19 14.95 13.12 12.87
m2d2_s2orc_unsplit_val_cs.SC 11.10 11.42 13.33 11.30 11.10
m2d2_s2orc_unsplit_val_cs.SD 13.27 13.42 15.26 13.36 13.13
m2d2_s2orc_unsplit_val_cs.SE 17.72 13.47 15.46 13.40 13.21
m2d2_s2orc_unsplit_val_cs.SI 12.03 12.25 14.03 12.19 11.99
m2d2_s2orc_unsplit_val_cs.SY 11.40 11.79 13.51 11.63 11.39
m2d2_s2orc_unsplit_val_cs_l1 8.39 8.68 10.12 8.59 8.47
m2d2_s2orc_unsplit_val_econ.EM 11.62 11.76 13.73 11.68 11.41
m2d2_s2orc_unsplit_val_econ.TH 9.75 10.16 11.99 9.99 9.88
m2d2_s2orc_unsplit_val_econ_l1 9.75 10.16 11.99 9.99 9.88
m2d2_s2orc_unsplit_val_eess.AS 12.05 12.14 13.88 12.09 11.88
m2d2_s2orc_unsplit_val_eess.IV 13.77 13.89 15.71 13.76 13.54
m2d2_s2orc_unsplit_val_eess.SP 11.29 11.45 12.94 11.28 11.13
m2d2_s2orc_unsplit_val_eess_l1 13.77 13.89 15.71 13.76 13.54
m2d2_s2orc_unsplit_val_gr-qc 12.84 12.99 14.68 12.84 12.71
m2d2_s2orc_unsplit_val_hep-ex 10.47 10.37 11.61 10.13 9.96
m2d2_s2orc_unsplit_val_hep-lat 13.13 13.10 14.57 13.02 12.80
m2d2_s2orc_unsplit_val_hep-ph 11.67 11.81 13.38 11.66 11.45
m2d2_s2orc_unsplit_val_hep-th 11.46 11.49 12.71 11.40 11.24
m2d2_s2orc_unsplit_val_math.AC 7.08 7.37 8.71 7.26 7.13
m2d2_s2orc_unsplit_val_math.AG 8.89 9.27 11.05 9.16 8.95
m2d2_s2orc_unsplit_val_math.AP 9.35 9.53 10.90 9.41 9.35
m2d2_s2orc_unsplit_val_math.AT 8.57 8.77 10.16 8.72 8.53
51Dataset Llama Mamba RWKV-4 xLSTM[7:1] xLSTM[1:0]
m2d2_s2orc_unsplit_val_math.CA 9.18 9.49 11.01 9.36 9.30
m2d2_s2orc_unsplit_val_math.CO 6.99 7.33 8.69 7.21 7.08
m2d2_s2orc_unsplit_val_math.CT 9.78 10.20 12.04 10.12 9.91
m2d2_s2orc_unsplit_val_math.CV 7.81 8.07 9.36 7.99 7.87
m2d2_s2orc_unsplit_val_math.DG 7.96 8.18 9.50 8.08 7.98
m2d2_s2orc_unsplit_val_math.DS 7.88 8.12 9.61 8.08 7.96
m2d2_s2orc_unsplit_val_math.FA 7.71 7.96 9.35 7.88 7.81
m2d2_s2orc_unsplit_val_math.GM 7.85 8.15 9.57 8.07 7.93
m2d2_s2orc_unsplit_val_math.GN 6.27 6.56 7.82 6.45 6.38
m2d2_s2orc_unsplit_val_math.GR 7.39 7.66 9.00 7.51 7.41
m2d2_s2orc_unsplit_val_math.GT 7.47 7.71 9.27 7.62 7.47
m2d2_s2orc_unsplit_val_math.HO 14.52 14.70 16.52 14.51 14.31
m2d2_s2orc_unsplit_val_math.KT 7.54 7.80 9.14 7.70 7.58
m2d2_s2orc_unsplit_val_math.LO 9.84 10.41 12.53 10.13 10.03
m2d2_s2orc_unsplit_val_math.MG 8.25 8.53 9.99 8.42 8.26
m2d2_s2orc_unsplit_val_math.NA 9.85 10.05 11.66 9.95 9.83
m2d2_s2orc_unsplit_val_math.NT 8.26 8.51 9.92 8.43 8.31
m2d2_s2orc_unsplit_val_math.OA 7.21 7.55 9.07 7.47 7.32
m2d2_s2orc_unsplit_val_math.OC 9.70 10.01 11.62 9.85 9.69
m2d2_s2orc_unsplit_val_math.PR 8.91 9.20 10.58 9.04 8.99
m2d2_s2orc_unsplit_val_math.QA 8.09 8.40 9.93 8.28 8.16
m2d2_s2orc_unsplit_val_math.RA 7.18 7.44 8.75 7.39 7.27
m2d2_s2orc_unsplit_val_math.RT 8.39 8.71 10.33 8.65 8.49
m2d2_s2orc_unsplit_val_math.SG 8.63 8.88 10.36 8.76 8.59
m2d2_s2orc_unsplit_val_math.SP 9.39 9.65 11.27 9.52 9.37
m2d2_s2orc_unsplit_val_math_l1 7.81 8.07 9.36 7.99 7.87
m2d2_s2orc_unsplit_val_nlin.AO 11.82 12.01 13.77 11.90 11.75
m2d2_s2orc_unsplit_val_nlin.CD 12.73 12.91 14.88 12.87 12.60
m2d2_s2orc_unsplit_val_nlin.CG 12.43 12.75 14.88 12.61 12.44
m2d2_s2orc_unsplit_val_nlin.PS 11.29 11.44 12.86 11.39 11.22
m2d2_s2orc_unsplit_val_nlin.SI 9.44 9.81 11.28 9.64 9.51
m2d2_s2orc_unsplit_val_nlin_l1 12.43 12.75 14.88 12.61 12.44
m2d2_s2orc_unsplit_val_nucl-ex 13.02 12.94 14.61 12.85 12.63
m2d2_s2orc_unsplit_val_nucl-th 11.65 11.78 13.43 11.68 11.48
m2d2_s2orc_unsplit_val_physics.acc-ph 13.75 14.01 16.17 13.74 13.58
m2d2_s2orc_unsplit_val_physics.ao-ph 13.92 14.04 15.91 13.89 13.68
m2d2_s2orc_unsplit_val_physics.app-ph 13.70 13.81 15.54 13.62 13.43
m2d2_s2orc_unsplit_val_physics.atm-clus 13.00 13.13 15.11 13.00 12.74
m2d2_s2orc_unsplit_val_physics.atom-ph 12.74 12.84 14.44 12.75 12.53
m2d2_s2orc_unsplit_val_physics.bio-ph 13.30 13.42 15.26 13.32 13.08
m2d2_s2orc_unsplit_val_physics.chem-ph 13.20 13.29 15.22 13.14 12.97
m2d2_s2orc_unsplit_val_physics.class-ph 11.01 11.27 12.85 11.12 10.94
m2d2_s2orc_unsplit_val_physics.comp-ph 11.23 11.37 12.88 11.26 11.08
m2d2_s2orc_unsplit_val_physics.data-an 13.18 13.33 14.97 13.25 13.00
m2d2_s2orc_unsplit_val_physics.ed-ph 12.21 12.33 13.88 12.18 12.03
m2d2_s2orc_unsplit_val_physics.flu-dyn 11.81 11.99 13.73 11.81 11.64
m2d2_s2orc_unsplit_val_physics.gen-ph 14.15 14.39 16.76 14.18 14.03
m2d2_s2orc_unsplit_val_physics.geo-ph 14.75 14.86 16.81 14.71 14.57
m2d2_s2orc_unsplit_val_physics.hist-ph 15.57 15.43 16.97 15.40 15.18
m2d2_s2orc_unsplit_val_physics.ins-det 14.01 14.16 16.14 14.07 13.79
m2d2_s2orc_unsplit_val_physics.med-ph 14.34 14.46 16.50 14.29 14.09
m2d2_s2orc_unsplit_val_physics.optics 12.74 12.94 14.64 12.80 12.54
52Dataset Llama Mamba RWKV-4 xLSTM[7:1] xLSTM[1:0]
m2d2_s2orc_unsplit_val_physics.plasm-ph 13.65 13.81 15.77 13.69 13.44
m2d2_s2orc_unsplit_val_physics.pop-ph 13.80 13.67 15.17 13.60 13.41
m2d2_s2orc_unsplit_val_physics.soc-ph 12.79 12.97 14.80 12.83 12.66
m2d2_s2orc_unsplit_val_physics.space-ph 13.00 13.09 14.77 12.94 12.76
m2d2_s2orc_unsplit_val_physics_l1 15.57 15.43 16.97 15.40 15.18
m2d2_s2orc_unsplit_val_plasm-ph 13.65 13.81 15.77 13.69 13.44
m2d2_s2orc_unsplit_val_q-bio 13.69 13.87 15.75 13.75 13.50
m2d2_s2orc_unsplit_val_q-bio.BM 13.28 13.52 15.72 13.41 13.19
m2d2_s2orc_unsplit_val_q-bio.CB 12.06 12.34 14.21 12.19 11.97
m2d2_s2orc_unsplit_val_q-bio.GN 13.21 11.40 12.74 11.32 11.16
m2d2_s2orc_unsplit_val_q-bio.MN 11.96 11.95 13.36 11.90 11.70
m2d2_s2orc_unsplit_val_q-bio.NC 13.69 13.87 15.75 13.75 13.50
m2d2_s2orc_unsplit_val_q-bio.OT 14.90 14.94 17.16 14.92 14.73
m2d2_s2orc_unsplit_val_q-bio.PE 12.57 12.71 14.62 12.69 12.41
m2d2_s2orc_unsplit_val_q-bio.QM 12.49 12.69 14.44 12.56 12.40
m2d2_s2orc_unsplit_val_q-bio.SC 13.68 13.85 15.60 13.75 13.53
m2d2_s2orc_unsplit_val_q-bio.TO 13.49 13.53 15.32 13.48 13.33
m2d2_s2orc_unsplit_val_q-bio_l1 13.69 13.87 15.75 13.75 13.50
m2d2_s2orc_unsplit_val_q-fin.CP 11.37 11.61 13.36 11.41 11.28
m2d2_s2orc_unsplit_val_q-fin.EC 11.72 11.89 13.77 11.77 11.63
m2d2_s2orc_unsplit_val_q-fin.GN 13.79 13.91 15.73 13.83 13.61
m2d2_s2orc_unsplit_val_q-fin.MF 9.91 10.21 11.92 10.04 9.90
m2d2_s2orc_unsplit_val_q-fin.PM 11.00 11.31 13.14 11.14 10.94
m2d2_s2orc_unsplit_val_q-fin.PR 15.87 9.25 10.37 9.20 9.03
m2d2_s2orc_unsplit_val_q-fin.RM 11.35 11.49 13.08 11.41 11.22
m2d2_s2orc_unsplit_val_q-fin.ST 12.43 12.46 14.18 12.43 12.26
m2d2_s2orc_unsplit_val_q-fin.TR 12.79 13.14 15.32 12.89 12.74
m2d2_s2orc_unsplit_val_q-fin_l1 13.79 13.91 15.73 13.83 13.61
m2d2_s2orc_unsplit_val_quant-ph 11.18 11.44 13.18 11.32 11.11
m2d2_s2orc_unsplit_val_stat.AP 13.37 13.56 15.52 13.42 13.15
m2d2_s2orc_unsplit_val_stat.CO 13.07 12.56 14.42 12.46 12.24
m2d2_s2orc_unsplit_val_stat.ME 11.09 11.26 12.91 11.11 10.87
m2d2_s2orc_unsplit_val_stat.ML 11.13 11.39 13.29 11.23 11.06
m2d2_s2orc_unsplit_val_stat.OT 11.31 11.55 13.28 11.45 11.24
m2d2_s2orc_unsplit_val_stat_l1 13.07 12.56 14.42 12.46 12.24
m2d2_s2orc_unsplit_val_supr-con 11.57 11.66 13.13 11.53 11.30
m2d2_wikipedia_unsplit_val_Culture_and_the_arts 12.30 11.90 12.82 11.78 11.66
m2d2_wikipedia_unsplit_val_Culture_and_the_arts__Culture_and_Humanities 12.13 11.74 12.82 11.63 11.48
m2d2_wikipedia_unsplit_val_Culture_and_the_arts__Games_and_Toys 14.06 13.86 15.17 13.79 13.57
m2d2_wikipedia_unsplit_val_Culture_and_the_arts__Mass_media 12.16 11.80 12.74 11.79 11.55
m2d2_wikipedia_unsplit_val_Culture_and_the_arts__Performing_arts 11.75 11.25 12.03 11.17 11.03
m2d2_wikipedia_unsplit_val_Culture_and_the_arts__Sports_and_Recreation 10.01 9.63 10.36 9.58 9.54
m2d2_wikipedia_unsplit_val_Culture_and_the_arts__The_arts_and_Entertainment 12.13 11.85 12.83 11.73 11.58
m2d2_wikipedia_unsplit_val_Culture_and_the_arts__Visual_arts 12.36 12.09 13.05 11.99 11.87
m2d2_wikipedia_unsplit_val_General_referece 11.80 11.46 12.43 11.46 11.30
m2d2_wikipedia_unsplit_val_General_referece__Further_research_tools_and_topics 10.52 10.20 10.96 10.12 9.99
m2d2_wikipedia_unsplit_val_General_referece__Reference_works 11.80 11.46 12.43 11.46 11.30
m2d2_wikipedia_unsplit_val_Health_and_fitness 10.75 10.47 11.14 10.37 10.30
m2d2_wikipedia_unsplit_val_Health_and_fitness__Exercise 9.64 9.29 9.95 9.27 9.16
m2d2_wikipedia_unsplit_val_Health_and_fitness__Health_science 10.10 9.80 10.43 9.71 9.56
m2d2_wikipedia_unsplit_val_Health_and_fitness__Human_medicine 9.14 8.83 9.59 8.63 8.54
m2d2_wikipedia_unsplit_val_Health_and_fitness__Nutrition 8.91 8.68 9.40 8.61 8.47
53Dataset Llama Mamba RWKV-4 xLSTM[7:1] xLSTM[1:0]
m2d2_wikipedia_unsplit_val_Health_and_fitness__Public_health 10.75 10.47 11.14 10.37 10.30
m2d2_wikipedia_unsplit_val_Health_and_fitness__Self_care 12.91 12.49 13.61 12.42 12.28
m2d2_wikipedia_unsplit_val_History_and_events 13.65 13.29 14.48 13.20 13.00
m2d2_wikipedia_unsplit_val_History_and_events__By_continent 11.77 11.44 12.36 11.36 11.26
m2d2_wikipedia_unsplit_val_History_and_events__By_period 12.78 12.41 13.46 12.37 12.12
m2d2_wikipedia_unsplit_val_History_and_events__By_region 12.36 11.88 12.87 11.79 11.64
m2d2_wikipedia_unsplit_val_Human_activites 12.43 12.03 12.98 11.95 11.81
m2d2_wikipedia_unsplit_val_Human_activites__Human_activities 12.43 12.03 12.98 11.95 11.81
m2d2_wikipedia_unsplit_val_Human_activites__Impact_of_human_activity 12.47 12.05 13.12 12.00 11.82
m2d2_wikipedia_unsplit_val_Mathematics_and_logic 12.90 12.51 13.79 12.48 12.29
m2d2_wikipedia_unsplit_val_Mathematics_and_logic__Fields_of_mathematics 8.24 8.26 9.37 8.28 8.06
m2d2_wikipedia_unsplit_val_Mathematics_and_logic__Logic 13.21 12.87 13.90 12.85 12.67
m2d2_wikipedia_unsplit_val_Mathematics_and_logic__Mathematics 12.90 12.51 13.79 12.48 12.29
m2d2_wikipedia_unsplit_val_Natural_and_physical_sciences 9.19 8.22 8.81 7.97 7.96
m2d2_wikipedia_unsplit_val_Natural_and_physical_sciences__Biology 10.97 10.70 11.53 10.64 10.51
m2d2_wikipedia_unsplit_val_Natural_and_physical_sciences__Earth_sciences 11.69 11.36 12.28 11.22 11.05
m2d2_wikipedia_unsplit_val_Natural_and_physical_sciences__Nature 10.43 10.11 10.95 10.00 9.82
m2d2_wikipedia_unsplit_val_Natural_and_physical_sciences__Physical_sciences 11.48 11.09 11.93 10.98 10.90
m2d2_wikipedia_unsplit_val_Philosophy_and_thinking 11.83 11.72 13.04 11.60 11.45
m2d2_wikipedia_unsplit_val_Philosophy_and_thinking__Philosophy 12.00 11.61 12.66 11.57 11.43
m2d2_wikipedia_unsplit_val_Philosophy_and_thinking__Thinking 10.94 10.61 11.34 10.56 10.42
m2d2_wikipedia_unsplit_val_Religion_and_belief_systems 12.81 12.45 13.44 12.38 12.19
m2d2_wikipedia_unsplit_val_Religion_and_belief_systems__Allah 11.11 10.80 11.66 10.71 10.58
m2d2_wikipedia_unsplit_val_Religion_and_belief_systems__Belief_systems 11.46 11.06 11.86 10.95 10.85
m2d2_wikipedia_unsplit_val_Religion_and_belief_systems__Major_beliefs_of_the_world 12.38 12.03 12.94 11.91 11.79
m2d2_wikipedia_unsplit_val_Society_and_social_sciences 10.53 10.24 11.03 10.16 10.05
m2d2_wikipedia_unsplit_val_Society_and_social_sciences__Social_sciences 10.47 10.16 10.95 10.14 10.04
m2d2_wikipedia_unsplit_val_Society_and_social_sciences__Society 12.48 12.13 13.02 12.07 11.93
m2d2_wikipedia_unsplit_val_Technology_and_applied_sciences 8.51 8.18 8.66 7.93 7.88
m2d2_wikipedia_unsplit_val_Technology_and_applied_sciences__Agriculture 12.45 12.07 13.00 12.03 11.88
m2d2_wikipedia_unsplit_val_Technology_and_applied_sciences__Computing 13.62 13.23 14.56 13.18 12.97
m2d2_wikipedia_unsplit_val_Technology_and_applied_sciences__Engineering 13.00 12.72 13.87 12.64 12.43
m2d2_wikipedia_unsplit_val_Technology_and_applied_sciences__Transport 14.34 13.90 15.20 13.94 13.73
manosphere_meta_sep_val_avfm 19.42 19.27 21.88 19.64 19.18
manosphere_meta_sep_val_incels 11.26 12.18 21.40 11.51 11.29
manosphere_meta_sep_val_mgtow 24.83 24.27 27.50 24.12 23.80
manosphere_meta_sep_val_pua_forum 24.22 23.85 26.52 23.86 23.52
manosphere_meta_sep_val_red_pill_talk 34.59 33.90 37.26 33.90 33.27
manosphere_meta_sep_val_reddit 20.63 19.78 21.10 19.94 19.58
manosphere_meta_sep_val_rooshv 22.46 22.17 24.78 22.01 21.69
manosphere_meta_sep_val_the_attraction 20.85 20.57 23.17 20.57 20.20
mc4_val-00000000 8.35 8.41 10.02 8.23 8.15
mc4_val-00000001 12.17 11.97 13.58 11.74 11.64
mc4_val-00000002 9.96 10.06 11.96 9.86 9.67
mc4_val-00000003 11.38 11.29 12.77 11.12 11.00
mc4_val-00000004 11.96 11.64 13.03 11.50 11.35
ptb_val 15.92 16.65 19.37 16.00 15.92
redpajama_val_arxiv 5.15 5.28 5.78 5.12 5.09
redpajama_val_books 12.91 12.71 13.60 12.61 12.50
redpajama_val_c4 13.01 12.51 13.55 12.49 12.27
redpajama_val_commoncrawl 10.90 10.56 11.70 10.52 10.35
redpajama_val_github 1.66 1.66 1.75 1.65 1.64
54Dataset Llama Mamba RWKV-4 xLSTM[7:1] xLSTM[1:0]
redpajama_val_stackexchange 3.73 3.72 4.03 3.68 3.63
redpajama_val_wikipedia 4.64 4.38 4.68 4.35 4.29
twitterAAE_HELM_fixed_val_AA 346.98 302.79 310.30 301.65 289.97
twitterAAE_HELM_fixed_val_white 118.62 107.34 109.13 107.65 105.13
wikitext_103_val 11.74 11.76 13.73 11.32 11.41
Table 10: PPL Evaluations: For the 1.3B sized models trained on 300B SlimPajama tokens, these are
the detailed evaluation results on the respective validation datasets.
55
  Autoregressive Model Beats Diffusion: Llama for
Scalable Image Generation
Peize Sun1Yi Jiang2Shoufa Chen1Shilong Zhang1Bingyue Peng2
Ping Luo1Zehuan Yuan2
1The University of Hong Kong2ByteDance
Codes and models: https://github.com/FoundationVision/LlamaGen
Figure 1: Image generation with vanilla autoregressive models . We show samples from our
class-conditional image (top row) and text-conditional image (bottom row) generation models.
Abstract
We introduce LlamaGen, a new family of image generation models that apply origi-
nal next-token prediction paradigm of large language models to visual generation
domain. It is an affirmative answer to whether vanilla autoregressive models, e.g.,
Llama, without inductive biases on visual signals can achieve state-of-the-art image
generation performance if scaling properly. We reexamine design spaces of image
tokenizers, scalability properties of image generation models, and their training
data quality. The outcome of this exploration consists of: (1) An image tokenizer
with downsample ratio of 16, reconstruction quality of 0.94 rFID and codebook
usage of 97% on ImageNet benchmark. (2) A series of class-conditional image
generation models ranging from 111M to 3.1B parameters, achieving 2.18 FID on
ImageNet 256 256 benchmarks, outperforming the popular diffusion models such
as LDM, DiT. (3) A text-conditional image generation model with 775M parame-
ters, from two-stage training on LAION-COCO and high aesthetics quality images,
demonstrating competitive performance of visual quality and text alignment. (4)
We verify the effectiveness of LLM serving frameworks in optimizing the inference
speed of image generation models and achieve 326% - 414% speedup. We release
all models and codes to facilitate open-source community of visual generation and
multimodal foundation models.
: Corresponding authors, : project leadarXiv:2406.06525v1  [cs.CV]  10 Jun 20241 Introduction
Built upon autoregressive models, large language models (LLMs) [Vaswani et al. 2017; Devlin et al.
2018; Radford et al. 2018; Raffel et al. 2020; Radford et al. 2019; Brown et al. 2020; Zhang et al.
2022] generate the text by predicting the next token in a sequence. This next-token prediction
paradigm presents unprecedented capabilities in solving language tasks in a human-like conversational
manner [Ouyang et al. 2022; OpenAI 2022, 2023b; Google 2023; Anthropic 2023; Workshop et al.
2022; Touvron et al. 2023a,b; Bai et al. 2023a; Yang et al. 2023; Team 2023; Bi et al. 2024] and
incredible scalability [Kaplan et al. 2020; Henighan et al. 2020; Hoffmann et al. 2022; Wei et al. 2022;
Alabdulmohsin et al. 2022; Chowdhery et al. 2023; Anil et al. 2023], demonstrating a promising path
toward general-purpose artificial intelligence models.
Witnessed the scalability of autoregressive models on large language models, pioneering works
attempt to explore autoregressive models in image generation, for example, VQV AE [Van Den Oord
et al. 2017; Razavi et al. 2019], VQGAN [Esser et al. 2021; Lee et al. 2022], DALL-E [Ramesh et al.
2021], Parti [Yu et al. 2021, 2022]. They introduce image tokenizers to convert continuous images to
discrete tokens, and apply autoregressive models to generate image tokens in the way of next-token
prediction. They demonstrate strong performance among their contemporaries [Brock et al. 2018;
Ho et al. 2020; Dhariwal & Nichol 2021] in the year before 2022. However, their open-source
communities are not well developed, which largely limits their further improvements.
At the same period, another image generation method, diffusion models [Song & Ermon 2019; Ho
et al. 2020; Song et al. 2020; Dhariwal & Nichol 2021; Nichol et al. 2021; Lu et al. 2022a; Ho
et al. 2022a; Ho & Salimans 2022; Rombach et al. 2022; Ramesh et al. 2022; Saharia et al. 2022;
Rombach et al. 2022] develop rapidly. Along with their open-source communities, they dominate
the field of visual generation up to today. However, diffusion models share distinct paradigms with
autoregressive language models, which poses a huge challenge to building a unified model between
language and vision.
In this work, we are committed to pushing the envelope of autoregressive models on image generation
further: continuing its research methodology and contributing to open-source community. Reviewing
the literature on image generation in the year before 2024, we identify three keys to existing advanced
models [Peebles & Xie 2023; Podell et al. 2023; Xue et al. 2023; Chen et al. 2023b,c; Betker et al.
2023; Li et al. 2024; Esser et al. 2024]: 1) well-designed image compressors, 2) scalable image
generation models and 3) high-quality training data. Motivated by this, we reexamine the designs of
image tokenizers (image compressors for autoregressive models), the scalability properties of image
generation models, and the effects of training data.
Towards a potential unified model between language and vision, our design is reducing the inductive
biases on visual signals and adopting the same architecture as LLM. This belongs to a different re-
search philosophy with recent works [Chang et al. 2022; Yu et al. 2023b; Tian et al. 2024] that modify
the architectures under the guidance of vision-oriented designs. For example, MaskGIT [Chang et al.
2022], MAGVIT [Yu et al. 2023a,b] adopt the masked image modeling strategy, V AR [Tian et al.
2024] uses hierarchical multi-scale property. Although they have succeeded in achieving leading
image generation performance, and even better than diffusion models, it is still not clear whether
the original language model architectures are capable of this. Instead, our work reveals that vanilla
autoregressive models that apply the exactly same next-token prediction as language models are
also able to achieve state-of-the-art image generation performance. As a bonus, we can leverage the
techniques [Dao et al. 2022; Rasley et al. 2020; Shoeybi et al. 2019; Zhao et al. 2023; Kwon et al.
2023; Chen et al. 2023a; Dettmers 2022] developed in LLM community to optimize the training
recipes and inference speeds of our models.
In summary, our contributions to the community include:
1.Image tokenizer: An image tokenizer with downsample ratio of 16, achieves reconstruction
quality of 0.94 rFID and codebook usage of 97% on ImageNet benchmark. With the downsample
ratio of 8, our tokenizer is competitive or even better than continuous V AE [Rombach et al.
2022; Podell et al. 2023; OpenAI 2023a] used in diffusion models. This shows that discrete
representation in image tokenizers is no longer the bottleneck of the image reconstruction.
2.Scalable image generation model: A series of class-conditional image generation models,
ranging from 111M to 3.1B parameters, are developed based on Llama architecture [Touvron
2et al. 2023a,b]. The largest model realizes 2.18 FID on ImageNet 256 256 benchmarks,
outperforming the popular diffusion models such as LDM [Rombach et al. 2022], DiT [Peebles
& Xie 2023]. This shows that vanilla autoregressive models without inductive biases on visual
signals can serve as the basis of image generation systems.
3.Hiqh-quality training data: A text-conditional image generation model with 775M parameters,
is firstly trained on a 50M subset of LAION-COCO [LAION 2022] and then fine-tuned on
10M internal high aesthetics quality images. It demonstrates competitive performance of visual
quality and text alignment.
4.Optimized inference speed: We adopt vLLM [Kwon et al. 2023], one of the most popular
LLM serving frameworks, to optimize the inference speed of our image generation models, and
remarkable 326% - 414% speedup is achieved.
We release all models and codes to facilitate the open-source community of visual generation and
multimodal foundation models. It is worth noticing that our released models are still behind state-of-
the-art visual generation models based on diffusion models [Alpha-VLLM 2024; Esser et al. 2024;
Brooks et al. 2024]. When more training data and computation resources are available in the future,
large-scale AR-based visual generation models, e.g., above 7B parameters, will be explored.
2 Autoregressive Models for Image Generation
2.1 Overview
Firstly, image pixels xRHW3are quantized into qQhwdiscrete tokens by the image
tokenizer [Van Den Oord et al. 2017; Esser et al. 2021; Yu et al. 2021], where h=H/p,w=W/p ,
pis downsample ratio of the image tokenizer, q(i,j)is indices of the image codebook. Then, these
image tokens are reshaped to a sequence of hwtokens in raster scan ordering and used to train
Transformer [Vaswani et al. 2017]-based autoregressive models.
During image generation, image tokens (q1, q2, . . . , q hw)are generated by autoregressive mod-
els [Radford et al. 2018, 2019; Brown et al. 2020; Touvron et al. 2023a] in the way of next-token
predictionQhw
t=1p(qt|q<t, c), where cis class label embedding or text embedding. Finally, these
image tokens are converted to image pixels by the image tokenizer decoder.
2.2 Image Tokenizer
Quantized-Autoencoder architecture. We use the same architecture as VQGAN [Esser et al.
2021], encoder-quantizer-decoder. The encoder and the decoder are ConvNet with downsample ratio
p. The quantizer contains a codebook ZRKCwithKlearnable vectors. The encoder projects
image pixels xto the feature map f. The quantization process maps each vector f(i,j)in the feature
map to the code index q(i,j)of its nearest vector z(i,j)in the codebook. During decoding, the code
index q(i,j)is remapped to the feature vector z(i,j)and the decoder converts these feature vectors
back to the image pixels ˆx.
The codebook has critical effects on image tokenization performance. Following [Yu et al. 2021], we
useℓ2-normalization to codebook vectors, low codebook vector dimension C, and large codebook
sizeK. These designs significantly improve reconstruction quality and codebook usage. More details
will be discussed in experiments.
Training losses. Since quantization is a non-differentiable operation, a straight-through gradient
estimator [Bengio et al. 2013] is used to preserve the gradient from the decoder to the encoder
z=sg[zf] +f,sg[]is stop-gradient operation. For codebook learning, LVQ=sg[f]z2
2+
βfsg[z]2
2, where the second term is commitment loss [Van Den Oord et al. 2017] to force feature
vectors extracted from the encoder to be close to codebook vectors, βis commitment loss weight.
For simplicity, we dont add entropy loss [Yu et al. 2023a; Chang et al. 2022] in codebook learning.
For image reconstruction training, LAE=ℓ2(x,ˆx)+LP(x,ˆx)+λGLG(ˆx), where ℓ2is a reconstruction
loss on image pixels, LP()is a perceptual loss from LPIPS [Zhang et al. 2018], LG()is an adversarial
loss from a PatchGAN [Isola et al. 2017] discriminator trained at the same time with the image
tokenizer, and λGis adversarial loss weight.
3Model Parameters Layers Hidden Size Heads
LlamaGen-B 111M 12 768 12
LlamaGen-L 343M 24 1024 16
LlamaGen-XL 775M 36 1280 20
LlamaGen-XXL 1.4B 48 1536 24
LlamaGen-3B 3.1B 24 3200 32
Table 1: Model sizes and architecture configurations of LlamaGen. The configurations are
following previous works [Radford et al. 2019; Touvron et al. 2023a; OpenLM-Research 2023].
2.3 Image Generation by Autoregressive Models
Llama architecture. Our model architecture is largely based on Llama [Touvron et al. 2023a,b],
applying pre-normalization using RMSNorm [Zhang & Sennrich 2019], SwiGLU activation func-
tion [Shazeer 2020], and rotary positional embeddings [Su et al. 2024]. Specifically, we use 2D RoPE
in at each layer of our model, following the implementation of [Lu et al. 2023; Fang et al. 2023]. We
do not use the technique of AdaLN [Peebles & Xie 2023] to keep our structure the same as LLM.
Class-conditional image generation. The class embedding is indexed from a set of learnable
embeddings [Peebles & Xie 2023; Esser et al. 2021] and is used as the prefilling token embedding.
Starting from this token embedding, the model generates the sequence of image tokens by next-token
prediction way, and stops at the location of the pre-defined maximum length.
Text-conditional image generation. To integrate the text condition into autoregressive models, we
use FLAN-T5 XL [Chung et al. 2024] as the text encoder, the encoded text feature is projected by an
additional MLP [Chen et al. 2023b,c] and is used as prefilling token embedding in autoregressive
models. We note that this design is not an ultimate design for multimodal foundation models, where
a unified vocabulary is established between language and vision [Lu et al. 2023; Team et al. 2023].
We leave it for future research.
Classifier-free guidance. Developed in the diffusion model community, classifier-free guidance [Ho
& Salimans 2022] is well-known for its improving visual quality and text-image alignment. We adopt
it in our models. During training, the conditional is randomly dropped and is replaced by a null
unconditional embedding [Peebles & Xie 2023; Chen et al. 2023b]. In inference, for each token, its
logitℓgis formed by ℓg=ℓu+s(ℓcℓu), where ℓcis conditional logit, ℓuis unconditional logit,
andsis scale of the classifier-free guidance.
It is worth noting that all design choices discussed so far are largely inspired by previous works, for
example, image tokenizer is borrowed from [Rombach et al. 2022; Yu et al. 2021], image generation
is from [Peebles & Xie 2023; Chen et al. 2023b; Esser et al. 2021]. A large portion of these techniques
are well studied in diffusion models but little in AR models. Our work adapts these advanced designs
collectively to AR-based visual generation models.
2.4 Scale Up
Our model architecture is almost the same as Llama, which allows us to seamlessly adopt optimization
techniques [Zhang & Sennrich 2019; Shazeer 2020; Su et al. 2024] and training recipes [Dao et al.
2022; Rasley et al. 2020; Shoeybi et al. 2019] in LLM community. As shown in Table 1, we scale the
model size up to 3.1B parameters in this work. All models are implemented with PyTorch 2 [Ansel
et al. 2024] and trained on 80GB A100 GPUs. For training the models with parameters below 1.4B,
we directly use DDP, otherwise, we adopt PyTorch FSDP [Zhao et al. 2023] to optimize GPU memory
usage.
2.5 Serving
Autoregressive models have always suffered from its low inference speed. With the rapid development
of large language models, advanced inference techniques [Kwon et al. 2023; Chen et al. 2023a;
Dettmers 2022] are proposed in the LLM community to optimize the inference speed.
4Similar to training, inference techniques developed in the LLM community can also be adopted to
optimize our models. We verify the effectiveness of vLLM [Kwon et al. 2023], one of the most
popular LLM serving frameworks, on our image generation methods. As shown in Table 7, 326% -
414% speedup is achieved compared to the baseline setting.
3 Experiments
3.1 Image Tokenizer
Training setup. The training is on ImageNet [Deng et al. 2009] train set, using the resolution of
256256 and random crop data augmentation. The image tokenizer model size is 72M and 70M
when the downsample ratio is 16 and 8, respectively. All models are trained with the same settings:
constant learning rate of 104, AdamW optimizer with β1= 0.9,β2= 0.95, weight decay = 0.05,
batch size of 128 and training epochs of 40. For the training losses, commitment loss weight is 0.25
and adversarial loss weight is 0.5. The adversarial loss is enabled after 20k training iterations.
Evaluation metrics. We use the popular ImageNet benchmark under the image resolution of 256
256. The image reconstruction quality is measured by r-FID, reconstruction-FID on 256 256
ImageNet 50k validation set. The codebook usage is calculated as the percentage of used codes in the
queue of size 65536 over the whole codebook size. We also report PSNR and SSIM as the metrics of
reconstruction quality, following SDXL [Podell et al. 2023].
dim rFIDPSNRSSIMusage
256 9.21 18.32 0.575 0.29%
32 3.22 19.98 0.646 20.9%
8 2.19 20.79 0.675 97.0%
4 9.88 19.39 0.593 82.0%
(a)Codebook vector dimension. Lower vector di-
mension (from 256 to 8) improves both reconstruction
quality and codebook usage significantly.size rFIDPSNRSSIMusage
4096 3.02 19.99 0.643 100.0%
8192 2.91 20.41 0.654 75.0%
16384 2.19 20.79 0.675 97.0%
32768 2.26 20.59 0.663 85.0%
(b)Codebook size. Larger codebook size (from 4096
to 16384) benefits to the overall performance of image
tokenizers.
Table 2: Ablation studies on codebook designs in image tokenizers. . The evaluations are on
256256 ImageNet 50k validation set. The default setting is codebook vector dimension is 8,
codebook size is 16384, downsample ratio is 16.
ratio img size tokens size rFIDPSNRSSIMusage
256 256 (16 16) 2.19 20.79 0.675 97.0%
16 384 576 (24 24) 0.94 21.94 0.726 97.0%
512 1024 (32 32) 0.70 23.03 0.772 97.0%
256 1024 (32 32) 0.59 24.45 0.813 97.6%
8 384 2304 (48 48) 0.37 25.63 0.852 97.6%
512 4096 (64 64) 0.39 26.98 0.888 97.6%
Table 3: Number of tokens to represent the image. The number of tokens depends on downsample
ratio and input image size. The reconstructed image is always resized to 256 256 when evaluating
on ImageNet 50k validation set. The default setting is codebook vector dimension is 8, codebook
size is 16384.
Effect of image codebook designs. As shown in Table 2, when the codebook vector dimension is
reduced from 256 to 32 to 8, much better reconstruction quality and codebook usage are consistently
achieved. For codebook size, a larger size from 4096 to 16384 benefits the overall performance.
These observations are consistent with previous works [Yu et al. 2021, 2023b].
Effect of number of tokens to represent the image. Table 3 studies the effect of image token
number on image reconstruction quality. Using the same image tokenizer, for example, downsample
ratio as 16, representing an image with only 256 tokens (16 16) is not sufficient for good reconstruc-
tion quality, and increasing the number of tokens to 576 (24 24) could largely improve the image
quality from 2.43 to 0.99 rFID.
5ratio method dim sizeImageNet COCO
rFIDPSNRSSIMrFIDPSNRSSIM
16VQGAN 256 1024 8.30 19.51 0.614 16.95 19.08 0.613
VQGAN 256 16384 4.99 20.00 0.629 12.29 19.57 0.630
MaskGIT 256 1024 2.28 - - - - -
Ours 8 16384 2.19 20.79 0.675 8.11 20.42 0.678
8VQGANoim.4 256 1.44 22.63 0.737 6.58 22.289 0.744
VQGANoim.4 16384 1.19 23.38 0.762 5.89 23.08 0.771
ViT-VQGAN 32 8192 1.28 - - - - -
Ours 8 16384 0.59 24.45 0.813 4.19 24.20 0.822
8SD-V AEukn.4 - 0.74 25.68 0.820 4.45 25.41 0.831
SDXL-V AEukn.4 - 0.68 26.04 0.834 4.07 25.76 0.845
OAI-Decoderukn.4 - 0.81 24.43 0.786 4.59 24.19 0.800
Table 4: Comparisons with other image tokenizers. The evaluations are on 256 256 ImageNet
50k validation set and COCO 5k val2017 set. All models are trained on ImageNet except oim. is
on OpenImage, ukn. is unknown training data.
Comparisons with other image tokenizers. We compare with other image tokenizers, including
VQGAN [Esser et al. 2021], MaskGIT [Chang et al. 2022], ViT-VQGAN [Yu et al. 2021]. As shown
in Table 4, our tokenizer outperforms previous image tokenizers. We also evaluate our tokenizer on
COCO val2017 [Lin et al. 2014] of 256 256 image resolution to verify the image reconstruction
quality, since COCO images contain more complex scenes. The comparison results are consistent
with those in ImageNet validation set. This shows our tokenizer is a generalizable image tokenizer
for both object-centric and scene-centric images.
Importantly, our tokenizer is competitive to continuous latent space representation, such as SD
V AE [Rombach et al. 2022], SDXL V AE [Podell et al. 2023], and Consistency Decoder from
OpenAI [OpenAI 2023a], which are widely used in diffusion models. This shows that discrete
representation in the image tokenizer is no longer the bottleneck of the image reconstruction.
3.2 Class-conditional Image Generation
Training setup. Our benchmark is the popular 256 256 ImageNet. All models are trained with
the similar settings: base learning rate of 104per 256 batch size, AdamW optimizer with β1= 0.9,
β2= 0.95,weight decay = 0.05, gradient clipping of 1.0. The dropout is always 0.1 for input
token embedding, attention module and FFN module. The class condition embedding dropout for
classifier-free guidance is 0.1.
Precomputing image codes. To accelerate the model training, we use the image tokenizer to precom-
pute image codes before training. To achieve the similar effect of random crop data augmentation,
we extract image codes of ten crops of the original image. During training, we randomly select one
copy code from the ten augmentations.
Evaluation metrics. We use Fréchet inception distance (FID) [Heusel et al. 2017] as the main
metric. We also report Inception Score (IS) [Salimans et al. 2016], sFID [Nash et al. 2021] and
Precision/Recall [Kynkäänniemi et al. 2019] as secondary metrics. All evaluations are implemented
using ADMs TensorFlow scripts [Dhariwal & Nichol 2021] for fair comparisons.
Effect of image tokens. Although increasing the image tokens brings better image reconstruction
quality, it is not strongly correlated to image generation quality. As shown in Table 5, when the model
parameter is smaller than 1B, 256 (16 16) tokens bring better image generation performance than
576 (24 24). This shows the synergistic effect of scaling up model parameters and token numbers.
Nevertheless, fewer image tokens would limit the image generation performance, for example, 256
(1616) tokens limit the FID at 3.06 FID, while 576 (24 24) could further improve the FID to a
lower value.
6image token model FID IS Precision Recall
image size: 256 256
tokens: 256 (16 16)
rFID: 2.19B 8.69 124.43 0.78 0.46
L 4.21 200.00 0.82 0.50
XL 3.39 227.08 0.81 0.54
XXL 3.09 253.60 0.82 0.52
3B 3.06 279.71 0.84 0.53
image size: 384 384
tokens: 576 (24 24)
rFID: 0.94B 12.89 92.44 0.73 0.48
L 5.01 167.31 0.78 0.52
XL 3.42 202.93 0.79 0.56
XXL 2.89 236.21 0.80 0.56
3B 2.61 251.90 0.80 0.56
Table 5: The effect of image tokens on image generation. The generated image is always resized to
256256 when evaluating on ImageNet benchmark. We compare all models after training 50 epochs.
The inference setting is cfg = 1.75, top-k = 0 (all), top-p = 1.0, temperature = 1.0 for all experiments.
(a) without classifier-free guidance
 (b) with classifier-free guidance
Figure 2: Scaling model size. We show FID of 256 256 ImageNet benchmark over training epochs.
Scaling model size brings consistent improvement on FID during the whole training process. More
detailed evaluation metrics are in Appendix.
(a) classifier-free guidance
 (b) top-k sampling
Figure 3: The effect of sampling configuration. We show FID and Inception Score of 256 256
ImageNet benchmark over different sampling configurations. The model is LlamaGen-L, and the
default setting is cfg = 2.0, top-k = 0 (all), top-p = 1.0, temperature = 1.0.
Effect of model size. We train our models across five model sizes (B, L, XL, XXL, 3B) and evaluate
their performance with and without classifier-free guidance. Figure 2 illustrates how FID changes as
both the model sizes and the training epochs increase. Notable improvements in FID are observed
when scaling the model from LlamaGen-B to LlamaGen-XXL. Further scaling to 3B yields only
marginal improvements. A plausible explanation for this phenomenon could be the limitation in
dataset size: ImageNet [Deng et al. 2009] comprises approximately only 1 million images, expanding
the dataset or using stronger data augmentation could potentially lead to further improvements.
7Type Model #Para. FID IS Precision Recall
GANBigGAN [Brock et al. 2018] 112M 6.95 224.5 0.89 0.38
GigaGAN [Kang et al. 2023] 569M 3.45 225.5 0.84 0.61
StyleGan-XL [Sauer et al. 2022] 166M 2.30 265.1 0.78 0.53
DiffusionADM [Dhariwal & Nichol 2021] 554M 10.94 101.0 0.69 0.63
CDM [Ho et al. 2022b]  4.88 158.7  
LDM-4 [Rombach et al. 2022] 400M 3.60 247.7  
DiT-XL/2 [Peebles & Xie 2023] 675M 2.27 278.2 0.83 0.57
Mask.MaskGIT [Chang et al. 2022] 227M 6.18 182.1 0.80 0.51
MaskGIT-re [Chang et al. 2022] 227M 4.02 355.6  
ARVQGAN [Esser et al. 2021] 227M 18.65 80.4 0.78 0.26
VQGAN [Esser et al. 2021] 1.4B 15.78 74.3  
VQGAN-re [Esser et al. 2021] 1.4B 5.20 280.3  
ViT-VQGAN [Yu et al. 2021] 1.7B 4.17 175.1  
ViT-VQGAN-re [Yu et al. 2021] 1.7B 3.04 227.4  
RQTran. [Lee et al. 2022] 3.8B 7.55 134.0  
RQTran.-re [Lee et al. 2022] 3.8B 3.80 323.7  
ARLlamaGen-B (cfg=2.00) 111M 5.46 193.61 0.83 0.45
LlamaGen-L (cfg=2.00) 343M 3.07 256.06 0.83 0.52
LlamaGen-XL (cfg=1.75) 775M 2.62 244.08 0.80 0.57
LlamaGen-XXL (cfg=1.75) 1.4B 2.34 253.90 0.80 0.59
LlamaGen-3B (cfg=1.65) 3.1B 2.18 263.33 0.81 0.58
LlamaGen-3B (cfg=1.75) 3.1B 2.32 280.10 0.82 0.56
LlamaGen-3B (cfg=2.00) 3.1B 2.81 311.59 0.84 0.54
Table 6: Model comparisons on class-conditional ImageNet 256 256 benchmark . Metrics include
Fréchet inception distance (FID), inception score (IS), precision and recall.   or  indicate lower
or higher values are better. -re means using rejection sampling. cfg means using classifier-free
guidance. More detailed results are in Appendix.
Effect of classifier-free guidance (CFG). First, as shown in Figure 2, using classifier-free guidance
can significantly enhance the visual quality across all model sizes. Moreover, Figure 3a illustrates
that the model achieves optimal FID at CFG = 2.0 and further increasing CFG would deteriorate FID,
which is consistent with previous findings [Dhariwal & Nichol 2021]. Additionally, the increment in
CFG results in a trade-off between diversity and fidelity, as evidenced by increased precision and
decreased recall, demonstrated in Table 10.
Effect of top-k sampling. As shown in Figure 3b, a small top-k value is not beneficial for FID
and IS. Increasing top-k continuously improves FID but decreases IS, which trades off fidelity for
diversity. We observe a similar trend when changing the parameter of top-p and temperature in
sampling. Since FID is our main metric, we use maximum value as the default top-k value, which is
the whole codebook size.
Comparisons with other image generation methods. In Table 6, we compare with popular image
generation models, including GAN [Brock et al. 2018; Kang et al. 2023; Sauer et al. 2022], Diffusion
models [Dhariwal & Nichol 2021; Ho et al. 2022b; Rombach et al. 2022; Peebles & Xie 2023],
and masked-prediction models [Chang et al. 2022]. Our models exhibit competitive performance
in all metrics of FID, IS, Precision and Recall. Notably, our 3B model outperforms the popular
diffusion models LDM [Rombach et al. 2022], DiT [Peebles & Xie 2023]. This shows that vanilla
autoregressive models can serve as the basis of advanced image generation systems.
When comparing with autoregressive models [Esser et al. 2021; Yu et al. 2021; Lee et al. 2022], our
model outperforms all previous models at different levels of model parameters. This benefits from
better designs of image tokenizers and better scalability of image generation models. We hope our
simple and effective implementation will serve as a solid baseline and help facilitate future research
in autoregressive models for image generations.
8A furry, black bear standing in a rocky, weedy, area in the wild.
A kitchen that is in the process of having the floors done
A cutting board topped with bread, meat and vegetables.
Stage I
Stage II
a big purple bus parked in a parking spot
Figure 4: Visualization of two-stage training of text-conditional image generation models.
Comparisons of generated images by models after stage I training and stage II training. The text
prompts are from COCOPrompts.
3.3 Text-conditional Image Generation
Training setup. We adopt a two-stage training strategy. In stage I, the model is trained on a
50M subset of LAION-COCO [LAION 2022] with the image resolution 256 256. In Stage II,
the model is fine-tuned on 10M internal high aesthetic quality images with the image resolution
512512. Examples of training data are shown in the Appendix. The maximum length of text token
embedding is set to 120, and left padding is used to enable batch processing. The text condition
embedding dropout for classifier-free guidance is 0.1. All models are trained with similar settings:
model parameters of 775M, base learning rate of 104per 256 batch size, AdamW optimizer with
β1= 0.9,β2= 0.95, decay = 0.05, gradient clipping of 1.0.
Precomputing image codes and text embeddings. We use pre-trained FLAN-T5 XL [Chung et al.
2024] to precompute text embedding of the image captions. For image code, we only extract image
codes of the original image center crop in text-conditional models training.
Fine-tune image tokenizer. Before two-stage training for text-conditional image generation models,
we first fine-tune the image tokenizer on the joint of 50M LAION-COCO and 10M internal high
aesthetic quality data.
Visualizations. In Figure 4, we select text prompts from COCOPrompts [Lin et al. 2014] to generate
images using models after stage I training and stage II training. After stage I training, the model
captures the text-image alignment, while its ability to represent image details is not clear. Stage II
training improves the visual aesthetic quality by a significant margin. We explain this improvement
comes from two aspects: high aesthetic quality images shift the domain, and high image resolution
brings better visual details. We notice that further increasing the image resolution to 1024 1024
could bring better visual quality, and we leave it for future research.
More visualizations on PartiPrompts [Yu et al. 2022] are in Appendix. PartiPrompts have more longer
captions than COCOPrompts, and our model demonstrates competitive performance in text-image
alignment for long caption image generation tasks.
Limitation. Due to the training data and model parameters, our text-conditional models have
several limitations, such as text rendering errors, counting errors, and common misconceptions.
These problems are promising to be mitigated when more training data and computation resources
are available in the future.
9model parameters baseline (sec) vllm (sec) speed-up ratio
B 111M 7.80 2.39 326%
L 343M 13.72 3.48 380%
XL 775M 19.76 4.84 408%
XXL 1.4B 26.38 6.36 414%
Table 7: Optimized inference speed by vLLM serving framework. The inference time is for
a batch 16 images (generating 8 images with classifier-free guidance). The image resolution is
384384 for all models.
3.4 Inference Speed
We verify the effectiveness of vLLM [Kwon et al. 2023] serving framework on our methods. Since our
models use the same architecture as Llama, which is already supported by vLLM, we can seamlessly
adopt its implementation. As shown in Table 7, we achieve 326% - 414% speedup compared to the
baseline setting in the models from 111M to 1.4B parameters. Please note that the baseline setting
has already integrated KV-Cache technique. In the 3B model, its head size 100 is not supported by
PagedAttention in vLLM.
4 Related Work
Visual generation. Generative adversarial network (GAN) [Goodfellow et al. 2014; Brock et al.
2018; Karras et al. 2019; Kang et al. 2023] is the first representative visual generation method in deep
learning era. To improve the distribution coverage, several likelihood-based methods are proposed.
Diffusion models [Ho et al. 2020; Song & Ermon 2019; Song et al. 2020; Dhariwal & Nichol 2021]
view image generation as the reverse diffusion process from noises to images. Masked-prediction
models [Chang et al. 2022, 2023; Yu et al. 2023a,b] apply language model BERT-style [Devlin
et al. 2018] by learning to predict masked tokens. Instead, autoregressive models [Esser et al. 2021;
Ramesh et al. 2021; Yu et al. 2022] leverage GPT-style [Radford et al. 2018] to predict the next
token in a sequence. To ease the modeling and improve the generation quality, these methods always
introduce the image tokenization process [Kingma & Welling 2013; Van Den Oord et al. 2017] to
convert pixel space to semantic space.
Multimodal foundation models. Recently, vision-and-language models [Liu et al. 2024; Zhu
et al. 2023; Dai et al. 2024; Peng et al. 2023; Zhang et al. 2023; Ma et al. 2024] have achieved
versatile visual understanding through visual instruction tuning [Liu et al. 2024; Zhu et al. 2023].
However, unifying the understanding and generation in multimodal models is still in its early stages.
Most existing methods [Sun et al. 2023b,a; Dong et al. 2024; Ge et al. 2023] try to collaborate a
pre-trained diffusion model with existing models, rather than utilizing a unified next-token prediction
paradigm. These methods need sophisticated designs to connect the two parts with distinct training
paradigms, which makes scaling up challenging. Pioneering methods [Lu et al. 2022b, 2023; Bai
et al. 2023b; Team et al. 2023; Team 2024] attempt to incorporate image generation into LLM
using an autoregressive approach and achieve promising results. They do not specifically focus on
demonstrating that a plain autoregressive approach can serve as a scalable image generator, which is
our main argument in this work.
5 Conclusion
In this work, we delve into vanilla autoregressive models for scalable image generation. By reexamin-
ing their image tokenizers, image generation models and training data, our class-conditional models
outperform the popular diffusion models, and our text-conditional models demonstrate competitive
performance of visual quality and text alignment.
10References
Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in
language and vision. Advances in Neural Information Processing Systems , 35:2230022312, 2022.
Alpha-VLLM. Large dit. https://github.com/Alpha-VLLM/LLaMA2-Accessory/tree/
main/Large-DiT-ImageNet , 2024.
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv
preprint arXiv:2305.10403 , 2023.
Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael V oznesensky,
Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. Pytorch 2: Faster machine learning
through dynamic python bytecode transformation and graph compilation. In Proceedings of the
29th ACM International Conference on Architectural Support for Programming Languages and
Operating Systems, Volume 2 , pp. 929947, 2024.
Anthropic. Claude. https://www.anthropic.com/index/introducing-claude , 2023.
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,
Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609 , 2023a.
Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra
Malik, and Alexei A Efros. Sequential modeling enables scalable learning for large vision models.
arXiv preprint arXiv:2312.00785 , 2023b.
Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through
stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 , 2013.
James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang
Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer
Science. https://cdn. openai. com/papers/dall-e-3. pdf , 2(3):8, 2023.
Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding,
Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with
longtermism. arXiv preprint arXiv:2401.02954 , 2024.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural
image synthesis. arXiv preprint arXiv:1809.11096 , 2018.
Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe
Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video
generation models as world simulators. OpenAI , 2024. URL https://openai.com/research/
video-generation-models-as-world-simulators .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:18771901, 2020.
Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative
image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 1131511325, 2022.
Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang,
Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation
via masked generative transformers. arXiv preprint arXiv:2301.00704 , 2023.
Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John
Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint
arXiv:2302.01318 , 2023a.
Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James
Kwok, Ping Luo, Huchuan Lu, et al. Pixart: Fast training of diffusion transformer for photorealistic
text-to-image synthesis. arXiv preprint arXiv:2310.00426 , 2023b.
11Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping
Luo, Tao Xiang, and Juan-Manuel Perez-Rua. Gentron: Delving deep into diffusion transformers
for image and video generation. arXiv preprint arXiv:2312.04557 , 2023c.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. Journal of Machine Learning Research , 24(240):1113,
2023.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li,
Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language
models. Journal of Machine Learning Research , 25(70):153, 2024.
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-
language models with instruction tuning. Advances in Neural Information Processing Systems , 36,
2024.
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-
efficient exact attention with io-awareness. Advances in Neural Information Processing Systems ,
35:1634416359, 2022.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,
pp. 248255. Ieee, 2009.
Tim Dettmers. bitsandbytes. https://github.com/TimDettmers/bitsandbytes , 2022.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances
in neural information processing systems , 34:87808794, 2021.
Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian
Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi.
DreamLLM: Synergistic multimodal comprehension and creation. In The Twelfth International
Conference on Learning Representations , 2024.
Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image
synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition ,
pp. 1287312883, 2021.
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam
Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion En-
glish, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow
transformers for high-resolution image synthesis, 2024.
Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva-02: A
visual representation for neon genesis. arXiv preprint arXiv:2303.11331 , 2023.
Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making
llama see and draw with seed tokenizer. arXiv preprint arXiv:2310.01218 , 2023.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
processing systems , 27, 2014.
Google. Bard. https://bard.google.com/ , 2023.
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo
Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative
modeling. arXiv preprint arXiv:2010.14701 , 2020.
12Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural
information processing systems , 30, 2017.
Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 ,
2022.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in
neural information processing systems , 33:68406851, 2020.
Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans.
Cascaded diffusion models for high fidelity image generation. The Journal of Machine Learning
Research , 23(1):22492281, 2022a.
Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans.
Cascaded diffusion models for high fidelity image generation. The Journal of Machine Learning
Research , 23(1):22492281, 2022b.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.
Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pp. 11251134, 2017.
Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung
Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pp. 1012410134, 2023.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.
arXiv preprint arXiv:2001.08361 , 2020.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition , pp. 44014410, 2019.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114 , 2013.
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model
serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating
Systems Principles , 2023.
Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved
precision and recall metric for assessing generative models. Advances in neural information
processing systems , 32, 2019.
LAION. Laion-coco 600m. https://laion.ai/blog/laion-coco , 2022.
Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image
generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 1152311532, 2022.
Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.
5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint
arXiv:2402.17245 , 2024.
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-
training for unified vision-language understanding and generation. In International conference on
machine learning , pp. 1288812900. PMLR, 2022.
13Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision
ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13 , pp. 740755. Springer, 2014.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in
neural information processing systems , 36, 2024.
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast
ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural
Information Processing Systems , 35:57755787, 2022a.
Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-
io: A unified model for vision, language, and multi-modal tasks. arXiv preprint arXiv:2206.08916 ,
2022b.
Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem,
and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision,
language, audio, and action. arXiv preprint arXiv:2312.17172 , 2023.
Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, and Xiaojuan Qi. Groma: Localized visual
tokenization for grounding multimodal large language models. arXiv preprint arXiv:2404.13013 ,
2024.
Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W Battaglia. Generating images with
sparse representations. arXiv preprint arXiv:2103.03841 , 2021.
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,
Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with
text-guided diffusion models. arXiv preprint arXiv:2112.10741 , 2021.
OpenAI. Chatgpt. https://openai.com/blog/chatgpt , 2022.
OpenAI. Consistency decoder. https://github.com/openai/consistencydecoder , 2023a.
OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023b.
OpenLM-Research. Openllama 3b. https://huggingface.co/openlm-research/open_
llama_3b , 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
instructions with human feedback. Advances in Neural Information Processing Systems , 35:
2773027744, 2022.
William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of
the IEEE/CVF International Conference on Computer Vision , pp. 41954205, 2023.
Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu
Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint
arXiv:2306.14824 , 2023.
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe
Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image
synthesis. arXiv preprint arXiv:2307.01952 , 2023.
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language
understanding by generative pre-training. article , 2018.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. The Journal of Machine Learning Research , 21(1):54855551, 2020.
14Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen,
and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine
Learning , pp. 88218831. PMLR, 2021.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-
conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 , 1(2):3, 2022.
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimiza-
tions enable training deep learning models with over 100 billion parameters. In Proceedings of
the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pp.
35053506, 2020.
Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with
vq-vae-2. Advances in neural information processing systems , 32, 2019.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-
ence on computer vision and pattern recognition , pp. 1068410695, 2022.
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar
Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic
text-to-image diffusion models with deep language understanding. Advances in Neural Information
Processing Systems , 35:3647936494, 2022.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. Advances in neural information processing systems , 29,
2016.
Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse
datasets. In ACM SIGGRAPH 2022 conference proceedings , pp. 110, 2022.
Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 , 2020.
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-
zaro. Megatron-lm: Training multi-billion parameter language models using model parallelism.
arXiv preprint arXiv:1909.08053 , 2019.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprint arXiv:2010.02502 , 2020.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
Advances in neural information processing systems , 32, 2019.
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced
transformer with rotary position embedding. Neurocomputing , 568:127063, 2024.
Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao,
Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv
preprint arXiv:2307.05222 , 2023a.
Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao,
Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv
preprint arXiv:2307.05222 , 2023b.
Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint
arXiv:2405.09818 , 2024.
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu
Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable
multimodal models. arXiv preprint arXiv:2312.11805 , 2023.
InternLM Team. Internlm: A multilingual language model with progressively enhanced capabilities,
2023.
15Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling:
Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905 , 2024.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023a.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023b.
Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in
neural information processing systems , 30, 2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing
systems , 30, 2017.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models.
arXiv preprint arXiv:2206.07682 , 2022.
BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili c,
Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. Bloom: A
176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 , 2022.
Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo. Raphael:
Text-to-image generation via large mixture of diffusion paths. arXiv preprint arXiv:2305.18295 ,
2023.
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan,
Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint
arXiv:2309.10305 , 2023.
Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong
Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan.
arXiv preprint arXiv:2110.04627 , 2021.
Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,
Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-
rich text-to-image generation. arXiv preprint arXiv:2206.10789 , 2(3):5, 2022.
Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander G
Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video
transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 1045910469, 2023a.
Lijun Yu, José Lezama, Nitesh B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong
Cheng, Agrim Gupta, Xiuye Gu, Alexander G Hauptmann, et al. Language model beats diffusion
tokenizer is key to visual generation. arXiv preprint arXiv:2310.05737 , 2023b.
Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural
Information Processing Systems , 32, 2019.
Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pp. 586595, 2018.
Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and
Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv preprint
arXiv:2307.03601 , 2023.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher
Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language
models. arXiv preprint arXiv:2205.01068 , 2022.
16Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid
Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data
parallel. arXiv preprint arXiv:2304.11277 , 2023.
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing
vision-language understanding with advanced large language models, 2023.
17An abstract painting on a pillow with pink, yellow and red flowers. The dining room is decorated with elegant decor.The new tab in Powerpointis highlighted.The card is attached to an external PCI.
A large building with columns and a clock tower.Two lions are laying under a tree in the wild.An assortment of party decorations with owls and otheritems.an image of the coordinate of two circles.Figure 5: Examples of stage I training data: 50M subset of LAION-COCO. The short caption is
its original caption (generated from BLIP [Li et al. 2022]).
A large, cartoon-like painting of a smiling Mickey Mouse. Mickey is wearing a red shirt and is holding a pair of white gloves. The painting is displayed on a wall, and the Mickey Mouse character appears to be the main focus of the artwork. There are no geometric patterns or overlays in the image.Acozy bedroom with a large bed situated in the center of the room. The bed is covered with a white comforter and a fur blanket, adding warmth and comfort to the space. A fireplace can be seen in the room, providing additional warmth and ambiance. The bedroom also features a large window, allowing natural light to fill thee room and offering a beautiful view of the snowy landscape outside. There are several candles placed around the room, adding a touch of elegance and creating a serene atmosphere.A beautiful blue flower with a white center, surrounded by a few other blue flowers. The blue flowers are adorned with water droplets, giving them a fresh and vibrant appearance. The scene appears to be set in a forest or a natural environment, with the flowers standing out against the backdrop. The combination of the blue flowers and the water droplets creates a visually appealing and serene atmosphere. A cartoon drawing of a smiling lion, standing on a rock in a grassy field. The lion has a playful expression, and its mouth is open, possibly indicating that it is laughing or making a sound. The lion's mane is prominent, adding to its majestic appearance. The scene captures the lions joyful and carefree demeanor in a natural environment.
Figure 6: Examples of stage II training data: 10M internal high aesthetic quality images. The
long caption is generated from LLaV A.
A Examples of Image-Text Pair Data
Training stage I: 50M subset of LAION-COCO [LAION 2022]. The original dataset has 600M
image-text pair. We filter these images by valid image URL, aesthetic score, watermark score, CLIP
image-text similarity score and image size. The remaining images are about 50M. Some examples
are shown in Figure 5.
Training stage II: 10M internal high aesthetic quality images. Each image is provided a long
caption by LLaV A [Liu et al. 2024] using the prompt of Describe this image in as much detail as
possible. Some examples are shown in Figure 6. We notice that the first sentence of the long caption
is always a summary description of its image, so we use it as the short caption to augment the training
of text-conditional image generation models.
B More Results on ImageNet Benchmark
We provide more detailed performance on ImageNet 256 256 benchmark in Table 8 9 10. The
generated image is always resized to 256 256 when evaluating.
18Model #Para. epochs cfg FID IS sFIDPre.Rec.
B 111M 50 no 31.352 39.576 8.749 0.568 0.614
B 111M 50 1.50 11.984 95.400 7.335 0.738 0.517
B 111M 50 1.75 8.690 124.435 7.165 0.789 0.469
B 111M 50 2.00 7.390 153.974 7.250 0.832 0.417
B 111M 50 2.25 7.220 178.281 7.489 0.861 0.384
B 111M 50 2.50 7.824 197.511 7.857 0.882 0.349
B 111M 300 no 26.262 48.072 9.216 0.593 0.616
B 111M 300 1.50 8.738 120.602 7.668 0.751 0.535
B 111M 300 1.75 6.116 159.123 7.364 0.799 0.492
B 111M 300 2.00 5.464 193.613 7.503 0.839 0.457
B 111M 300 2.25 5.641 220.720 7.668 0.863 0.411
B 111M 300 2.50 6.390 246.565 8.041 0.883 0.382
L 343M 50 no 21.812 59.179 8.772 0.616 0.640
L 343M 50 1.50 5.781 153.792 7.096 0.774 0.555
L 343M 50 1.75 4.218 200.001 7.015 0.824 0.509
L 343M 50 2.00 4.317 242.112 7.077 0.859 0.468
L 343M 300 no 13.452 82.289 8.324 0.656 0.638
L 343M 300 1.50 4.079 198.504 8.157 0.800 0.552
L 343M 300 1.75 3.805 248.280 8.487 0.833 0.515
L 343M 300 2.00 4.407 288.170 8.871 0.858 0.481
XL 775M 50 no 19.417 66.196 8.911 0.610 0.665
XL 775M 50 1.50 4.808 172.170 7.298 0.767 0.585
XL 775M 50 1.75 3.391 227.081 7.022 0.812 0.542
XL 775M 50 2.00 3.642 268.779 7.244 0.846 0.502
XXL 1.4B 50 no 16.822 74.888 9.285 0.628 0.660
XXL 1.4B 50 1.50 3.844 195.527 7.496 0.781 0.577
XXL 1.4B 50 1.75 3.094 253.609 7.305 0.825 0.529
XXL 1.4B 50 2.00 3.644 296.521 7.410 0.857 0.511
3B 3.1B 50 no 13.581 87.902 7.781 0.648 0.666
3B 3.1B 50 1.50 3.050 222.330 6.489 0.801 0.575
3B 3.1B 50 1.75 3.063 279.716 6.686 0.843 0.538
3B 3.1B 50 2.00 4.212 325.150 7.027 0.869 0.492
validation data 1.684 231.811 3.692 0.752 0.671
Table 8: Detailed performance on class-conditional ImageNet 256 256 benchmark . The gener-
ated image is 256 256. All experiments use the sampling configuration of top-k = 0 (all), top-p =
1.0, temperature = 1.0.
...
...
...
19Model #Para. epochs cfg FID sFID IS Pre.Rec.
B 111M 50 no 41.025 30.788 9.825 0.523 0.605
B 111M 50 1.50 18.276 69.337 7.557 0.677 0.534
B 111M 50 1.75 12.899 92.447 6.900 0.738 0.487
B 111M 50 2.00 10.029 116.372 6.562 0.787 0.443
B 111M 50 2.25 8.674 136.621 6.428 0.818 0.413
B 111M 50 2.50 8.309 154.719 6.599 0.843 0.376
B 111M 50 2.75 8.391 168.629 6.708 0.860 0.345
B 111M 100 no 33.442 37.528 9.872 0.536 0.609
B 111M 100 1.50 15.629 77.247 7.632 0.698 0.529
B 111M 100 1.75 10.676 104.581 6.960 0.754 0.490
B 111M 100 2.00 8.298 128.941 6.671 0.795 0.452
B 111M 100 2.25 7.256 152.502 6.510 0.827 0.416
B 111M 100 2.50 7.151 172.677 6.517 0.850 0.390
B 111M 200 no 32.105 37.993 10.144 0.559 0.618
B 111M 200 1.50 12.206 90.783 7.531 0.716 0.534
B 111M 200 1.75 8.535 118.399 7.024 0.766 0.503
B 111M 200 2.00 6.951 146.077 6.784 0.808 0.459
B 111M 200 2.25 6.542 167.825 6.695 0.833 0.428
B 111M 200 2.50 6.632 188.157 6.811 0.853 0.393
B 111M 300 no 32.196 39.877 11.838 0.570 0.611
B 111M 300 1.50 12.012 95.553 8.897 0.725 0.528
B 111M 300 1.75 8.012 127.957 8.088 0.778 0.498
B 111M 300 2.00 6.437 157.173 7.487 0.814 0.456
B 111M 300 2.25 6.092 182.538 7.244 0.845 0.416
B 111M 300 2.50 6.249 203.886 6.981 0.861 0.389
B 111M 300 2.75 6.803 220.708 6.928 0.876 0.357
L 343M 50 no 25.889 48.053 9.612 0.570 0.655
L 343M 50 1.50 7.905 123.830 7.381 0.732 0.569
L 343M 50 1.75 5.018 167.310 6.786 0.784 0.524
L 343M 50 2.00 4.240 206.739 6.483 0.825 0.491
L 343M 50 2.25 4.589 238.890 6.325 0.850 0.451
L 343M 100 no 24.654 53.166 10.497 0.594 0.645
L 343M 100 1.50 6.934 138.852 7.910 0.748 0.569
L 343M 100 1.75 4.321 188.536 7.068 0.802 0.528
L 343M 100 2.00 3.705 228.305 6.701 0.839 0.490
L 343M 100 2.25 4.054 263.864 6.407 0.858 0.460
L 343M 200 no 19.742 61.715 7.286 0.601 0.667
L 343M 200 1.50 4.929 158.546 6.066 0.759 0.588
L 343M 200 1.75 3.249 209.372 5.927 0.805 0.544
L 343M 200 2.00 3.220 250.697 5.879 0.841 0.512
L 343M 200 2.25 3.939 288.217 6.076 0.865 0.479
L 343M 300 no 19.070 64.349 8.668 0.607 0.670
L 343M 300 1.50 4.743 165.381 6.740 0.758 0.596
L 343M 300 1.75 3.151 214.152 6.310 0.803 0.552
L 343M 300 2.00 3.075 256.067 6.088 0.832 0.522
L 343M 300 2.25 3.620 291.695 6.122 0.854 0.493
validation data 1.684 231.811 3.692 0.752 0.671
Table 9: Detailed performance on class-conditional ImageNet 256 256 benchmark . The gener-
ated image is 384 384 and is resized to 256 256 when evaluating on ImageNet. All experiments
use the sampling configuration of top-k = 0 (all), top-p = 1.0, temperature = 1.0.
20Model #Para. epochs cfg FID sFID IS Pre.Rec.
XL 775M 50 no 19.820 61.363 8.067 0.601 0.669
XL 775M 50 1.50 5.231 154.249 6.284 0.746 0.592
XL 775M 50 1.75 3.420 202.939 6.090 0.796 0.560
XL 775M 50 2.00 3.238 245.680 6.023 0.826 0.529
XL 775M 100 no 18.037 69.879 8.388 0.616 0.665
XL 775M 100 1.50 4.563 173.749 6.591 0.759 0.588
XL 775M 100 1.75 3.089 225.856 6.157 0.804 0.551
XL 775M 100 2.00 3.105 267.608 6.001 0.833 0.531
XL 775M 200 no 14.772 80.826 6.840 0.620 0.681
XL 775M 200 1.50 3.388 193.477 5.753 0.771 0.603
XL 775M 200 1.75 2.617 245.465 5.652 0.811 0.566
XL 775M 200 2.00 2.859 285.900 5.758 0.840 0.527
XL 775M 300 no 15.549 79.157 7.049 0.616 0.689
XL 775M 300 1.50 3.479 194.448 5.816 0.763 0.606
XL 775M 300 1.75 2.629 244.085 5.594 0.807 0.579
XL 775M 300 2.00 2.785 286.875 5.567 0.836 0.542
XXL 1.4B 50 no 17.195 74.123 8.689 0.605 0.681
XXL 1.4B 50 1.50 4.363 178.228 6.818 0.758 0.600
XXL 1.4B 50 1.75 2.893 236.210 6.263 0.805 0.564
XXL 1.4B 50 2.00 3.049 285.390 6.053 0.842 0.522
XXL 1.4B 200 no 13.997 86.776 8.178 0.637 0.684
XXL 1.4B 200 1.50 3.137 207.870 6.060 0.774 0.605
XXL 1.4B 200 1.75 2.331 262.995 5.714 0.816 0.579
XXL 1.4B 200 2.00 2.678 304.631 5.587 0.840 0.545
XXL 1.4B 300 no 14.648 86.328 8.687 0.628 0.681
XXL 1.4B 300 1.50 3.295 202.586 6.476 0.770 0.626
XXL 1.4B 300 1.75 2.340 253.906 5.977 0.809 0.596
XXL 1.4B 300 2.00 2.523 295.374 5.736 0.836 0.559
3B 3.1B 50 no 16.431 72.622 7.217 0.611 0.677
3B 3.1B 50 1.50 3.472 191.979 5.955 0.768 0.600
3B 3.1B 50 1.75 2.611 251.903 6.167 0.807 0.568
3B 3.1B 50 2.00 3.222 300.887 5.764 0.847 0.523
3B 3.1B 200 no 9.949 108.083 7.088 0.667 0.672
3B 3.1B 200 1.50 2.400 237.683 5.548 0.794 0.600
3B 3.1B 200 1.65 2.264 268.180 5.426 0.817 0.581
3B 3.1B 200 1.75 2.381 286.091 5.390 0.828 0.569
3B 3.1B 200 2.00 3.011 321.563 5.514 0.851 0.538
3B 3.1B 300 no 9.380 112.877 8.242 0.685 0.668
3B 3.1B 300 1.50 2.388 233.246 6.145 0.798 0.601
3B 3.1B 300 1.60 2.216 251.338 6.002 0.811 0.584
3B 3.1B 300 1.65 2.189 263.334 5.965 0.819 0.581
3B 3.1B 300 1.75 2.329 280.104 5.818 0.828 0.566
3B 3.1B 300 1.80 2.370 287.452 5.825 0.834 0.570
3B 3.1B 300 2.00 2.816 311.597 5.845 0.848 0.544
validation data 1.684 231.811 3.692 0.752 0.671
Table 10: Detailed performance on class-conditional ImageNet 256 256 benchmark . The gener-
ated image is 384 384 and is resized to 256 256 when evaluating on ImageNet. All experiments
use the sampling configuration of top-k = 0 (all), top-p = 1.0, temperature = 1.0.
21Figure 7: 384 384 LlamaGen-3B samples.
Classifier-free guidance scale = 4.0
Class label = golden retriever (207)
Figure 8: 384 384 LlamaGen-3B samples.
Classifier-free guidance scale = 4.0
Class label = husky  (250)
22Figure 9: 384 384 LlamaGen-3B samples.
Classifier-free guidance scale = 4.0
Class label = cliff drop-off (972)
Figure 10: 384 384 LlamaGen-3B samples.
Classifier-free guidance scale = 4.0
Class label = coral reef (973)
23Figure 11: 384 384 LlamaGen-3B samples.
Classifier-free guidance scale = 4.0
Class label = space shuttle (812)
Figure 12: 384 384 LlamaGen-3B samples.
Classifier-free guidance scale = 4.0
Class label = sport car  (817)
24A plate on a wooden table full of bread. A cat resting on an open laptop computer.
Two birds that are sitting in a marsh area.A large green truck on a city street..
A brown cow laying on top of a lush green field.A box of donuts of different colors and varieties.
A large body of water sitting below a mountain range.A few bags laying around in a living room.
A pickup truck driving through a desert environment.
A bathroom with a blue shower curtain and blue walls..Figure 13: (Stage I) Text-conditional 256 256
image generation on COCOPrompts.
a tigeran illustration of a teapot.
a view of the Kremlin with snow fallinga corgi wearing a red bowtie and a purple party hat
the Eiffel Tower in winter
A bare kitchen has wood cabinets and white appliances
A photo of an Athenian vase with a painting of pandas playing soccer in the style of Egyptian hieroglyphics.Golden Gate bridge on the surface of Mars
A photograph of a portrait of a statue of a pharaoh wearing steampunk glasses, white t-shirt and leather jacket.A close-up high-contrast photo of Sydney Opera House sitting next to Eiffel tower, under a blue night sky of roiling energy, exploding yellow stars, and radiating swirls of blue
Figure 14: (Stage I) Text-conditional 256 256
image generation on PartiPrompts.
25an ostricha corgis head depicted as an explosion of a nebula
a sunken ship at the bottom of the ocean
A blue Porsche 356 parked in front of a yellow brick walla gorilla climbing up the side of the Great Pyramida racoon detective using a microscope while riding in a train
a photograph of a squirrel holding an arrow above its head and holding a longbow in its left hand
a photograph of the mona lisadrinking coffee as she has her breakfast. her plate has an omeletteand croissantpanda mad scientist mixing sparkling chemicals, high-contrast painting.
a baby penguina chimpanzee
Dogs sitting around a poker tableA high resolution photo of a chicken working out in a gym.
A photo of a four-leaf clover made of water.
Dogs sitting around a poker table with beer bottles and chips. Their hands are holding cards.
Siberian husky playing the piano.a plant at the bottom of a shallow stream
A photo of an astronaut riding a horse in the forest. There is a river in front of them with water lilies. 
red apples on a tree with green leaves
A portrait of a metal statue of a pharaoh wearing steampunk glasses and a leather jacket over a white t-shirt that has a drawing of a space shuttle on it.Figure 15: (Stage II) Text-conditional 512 512 image generation on PartiPrompts.
26
  High-Resolution Image Synthesis with Latent Diffusion Models
Robin Rombach1* Andreas Blattmann1Dominik Lorenz1Patrick Esser
 Bjorn Ommer1
1Ludwig Maximilian University of Munich & IWR, Heidelberg University, Germany
 Runway ML
https://github.com/CompVis/latent-diffusion
Abstract
By decomposing the image formation process into a se-
quential application of denoising autoencoders, diffusion
models (DMs) achieve state-of-the-art synthesis results on
image data and beyond. Additionally, their formulation al-
lows for a guiding mechanism to control the image gen-
eration process without retraining. However, since these
models typically operate directly in pixel space, optimiza-
tion of powerful DMs often consumes hundreds of GPU
days and inference is expensive due to sequential evalu-
ations. To enable DM training on limited computational
resources while retaining their quality and ﬂexibility, we
apply them in the latent space of powerful pretrained au-
toencoders. In contrast to previous work, training diffusion
models on such a representation allows for the ﬁrst time
to reach a near-optimal point between complexity reduc-
tion and detail preservation, greatly boosting visual ﬁdelity.
By introducing cross-attention layers into the model archi-
tecture, we turn diffusion models into powerful and ﬂexi-
ble generators for general conditioning inputs such as text
or bounding boxes and high-resolution synthesis becomes
possible in a convolutional manner. Our latent diffusion
models (LDMs) achieve new state-of-the-art scores for im-
age inpainting and class-conditional image synthesis and
highly competitive performance on various tasks, includ-
ing text-to-image synthesis, unconditional image generation
and super-resolution, while signiﬁcantly reducing computa-
tional requirements compared to pixel-based DMs.
1. Introduction
Image synthesis is one of the computer vision ﬁelds with
the most spectacular recent development, but also among
those with the greatest computational demands. Espe-
cially high-resolution synthesis of complex, natural scenes
is presently dominated by scaling up likelihood-based mod-
els, potentially containing billions of parameters in autore-
gressive (AR) transformers [66,67]. In contrast, the promis-
ing results of GANs [3, 27, 40] have been revealed to be
mostly conﬁned to data with comparably limited variability
as their adversarial learning procedure does not easily scale
to modeling complex, multi-modal distributions. Recently,
diffusion models [82], which are built from a hierarchy of
denoising autoencoders, have shown to achieve impressive
*The ﬁrst two authors contributed equally to this work.Inputours ( f= 4)
PSNR: 27:4R-FID: 0:58DALL-E ( f= 8)
PSNR: 22:8R-FID: 32:01VQGAN ( f= 16 )
PSNR: 19:9R-FID: 4:98
Figure 1. Boosting the upper bound on achievable quality with
less agressive downsampling. Since diffusion models offer excel-
lent inductive biases for spatial data, we do not need the heavy spa-
tial downsampling of related generative models in latent space, but
can still greatly reduce the dimensionality of the data via suitable
autoencoding models, see Sec. 3. Images are from the DIV2K [1]
validation set, evaluated at 5122px. We denote the spatial down-
sampling factor by f. Reconstruction FIDs [29] and PSNR are
calculated on ImageNet-val. [12]; see also Tab. 8.
results in image synthesis [30,85] and beyond [7,45,48,57],
and deﬁne the state-of-the-art in class-conditional image
synthesis [15,31] and super-resolution [72]. Moreover, even
unconditional DMs can readily be applied to tasks such
as inpainting and colorization [85] or stroke-based syn-
thesis [53], in contrast to other types of generative mod-
els [19,46,69]. Being likelihood-based models, they do not
exhibit mode-collapse and training instabilities as GANs
and, by heavily exploiting parameter sharing, they can
model highly complex distributions of natural images with-
out involving billions of parameters as in AR models [67].
Democratizing High-Resolution Image Synthesis DMs
belong to the class of likelihood-based models, whose
mode-covering behavior makes them prone to spend ex-
cessive amounts of capacity (and thus compute resources)
on modeling imperceptible details of the data [16, 73]. Al-
though the reweighted variational objective [30] aims to ad-
dress this by undersampling the initial denoising steps, DMs
are still computationally demanding, since training and
evaluating such a model requires repeated function evalu-
ations (and gradient computations) in the high-dimensional
space of RGB images. As an example, training the most
powerful DMs often takes hundreds of GPU days ( e.g. 150 -
1000 V100 days in [15]) and repeated evaluations on a noisy
version of the input space render also inference expensive,
1arXiv:2112.10752v2  [cs.CV]  13 Apr 2022so that producing 50k samples takes approximately 5 days
[15] on a single A100 GPU. This has two consequences for
the research community and users in general: Firstly, train-
ing such a model requires massive computational resources
only available to a small fraction of the ﬁeld, and leaves a
huge carbon footprint [65, 86]. Secondly, evaluating an al-
ready trained model is also expensive in time and memory,
since the same model architecture must run sequentially for
a large number of steps ( e.g. 25 - 1000 steps in [15]).
To increase the accessibility of this powerful model class
and at the same time reduce its signiﬁcant resource con-
sumption, a method is needed that reduces the computa-
tional complexity for both training and sampling. Reducing
the computational demands of DMs without impairing their
performance is, therefore, key to enhance their accessibility.
Departure to Latent Space Our approach starts with
the analysis of already trained diffusion models in pixel
space: Fig. 2 shows the rate-distortion trade-off of a trained
model. As with any likelihood-based model, learning can
be roughly divided into two stages: First is a perceptual
compression stage which removes high-frequency details
but still learns little semantic variation. In the second stage,
the actual generative model learns the semantic and concep-
tual composition of the data ( semantic compression ). We
thus aim to ﬁrst ﬁnd a perceptually equivalent, but compu-
tationally more suitable space , in which we will train diffu-
sion models for high-resolution image synthesis.
Following common practice [11, 23, 66, 67, 96], we sep-
arate training into two distinct phases: First, we train
an autoencoder which provides a lower-dimensional (and
thereby efﬁcient) representational space which is perceptu-
ally equivalent to the data space. Importantly, and in con-
trast to previous work [23,66], we do not need to rely on ex-
cessive spatial compression, as we train DMs in the learned
latent space, which exhibits better scaling properties with
respect to the spatial dimensionality. The reduced complex-
ity also provides efﬁcient image generation from the latent
space with a single network pass. We dub the resulting
model class Latent Diffusion Models (LDMs).
A notable advantage of this approach is that we need to
train the universal autoencoding stage only once and can
therefore reuse it for multiple DM trainings or to explore
possibly completely different tasks [81]. This enables efﬁ-
cient exploration of a large number of diffusion models for
various image-to-image and text-to-image tasks. For the lat-
ter, we design an architecture that connects transformers to
the DMs UNet backbone [71] and enables arbitrary types
of token-based conditioning mechanisms, see Sec. 3.3.
In sum, our work makes the following contributions :
(i) In contrast to purely transformer-based approaches
[23, 66], our method scales more graceful to higher dimen-
sional data and can thus (a) work on a compression level
which provides more faithful and detailed reconstructions
than previous work (see Fig. 1) and (b) can be efﬁciently
Figure 2. Illustrating perceptual and semantic compression: Most
bits of a digital image correspond to imperceptible details. While
DMs allow to suppress this semantically meaningless information
by minimizing the responsible loss term, gradients (during train-
ing) and the neural network backbone (training and inference) still
need to be evaluated on all pixels, leading to superﬂuous compu-
tations and unnecessarily expensive optimization and inference.
We propose latent diffusion models (LDMs) as an effective gener-
ative model and a separate mild compression stage that only elim-
inates imperceptible details. Data and images from [30].
applied to high-resolution synthesis of megapixel images.
(ii) We achieve competitive performance on multiple
tasks (unconditional image synthesis, inpainting, stochastic
super-resolution) and datasets while signiﬁcantly lowering
computational costs. Compared to pixel-based diffusion ap-
proaches, we also signiﬁcantly decrease inference costs.
(iii) We show that, in contrast to previous work [93]
which learns both an encoder/decoder architecture and a
score-based prior simultaneously, our approach does not re-
quire a delicate weighting of reconstruction and generative
abilities. This ensures extremely faithful reconstructions
and requires very little regularization of the latent space.
(iv) We ﬁnd that for densely conditioned tasks such
as super-resolution, inpainting and semantic synthesis, our
model can be applied in a convolutional fashion and render
large, consistent images of 10242px.
(v) Moreover, we design a general-purpose conditioning
mechanism based on cross-attention, enabling multi-modal
training. We use it to train class-conditional, text-to-image
and layout-to-image models.
(vi) Finally, we release pretrained latent diffusion
and autoencoding models at https : / / github .
com/CompVis/latent-diffusion which might be
reusable for a various tasks besides training of DMs [81].
2. Related Work
Generative Models for Image Synthesis The high di-
mensional nature of images presents distinct challenges
to generative modeling. Generative Adversarial Networks
(GAN) [27] allow for efﬁcient sampling of high resolution
images with good perceptual quality [3, 42], but are difﬁ-
2cult to optimize [2, 28, 54] and struggle to capture the full
data distribution [55]. In contrast, likelihood-based meth-
ods emphasize good density estimation which renders op-
timization more well-behaved. Variational autoencoders
(V AE) [46] and ﬂow-based models [18, 19] enable efﬁcient
synthesis of high resolution images [9, 44, 92], but sam-
ple quality is not on par with GANs. While autoregressive
models (ARM) [6, 10, 94, 95] achieve strong performance
in density estimation, computationally demanding architec-
tures [97] and a sequential sampling process limit them to
low resolution images. Because pixel based representations
of images contain barely perceptible, high-frequency de-
tails [16,73], maximum-likelihood training spends a dispro-
portionate amount of capacity on modeling them, resulting
in long training times. To scale to higher resolutions, several
two-stage approaches [23,67,101,103] use ARMs to model
a compressed latent image space instead of raw pixels.
Recently, Diffusion Probabilistic Models (DM) [82],
have achieved state-of-the-art results in density estimation
[45] as well as in sample quality [15]. The generative power
of these models stems from a natural ﬁt to the inductive bi-
ases of image-like data when their underlying neural back-
bone is implemented as a UNet [15, 30, 71, 85]. The best
synthesis quality is usually achieved when a reweighted ob-
jective [30] is used for training. In this case, the DM corre-
sponds to a lossy compressor and allow to trade image qual-
ity for compression capabilities. Evaluating and optimizing
these models in pixel space, however, has the downside of
low inference speed and very high training costs. While
the former can be partially adressed by advanced sampling
strategies [47, 75, 84] and hierarchical approaches [31, 93],
training on high-resolution image data always requires to
calculate expensive gradients. We adress both drawbacks
with our proposed LDMs , which work on a compressed la-
tent space of lower dimensionality. This renders training
computationally cheaper and speeds up inference with al-
most no reduction in synthesis quality (see Fig. 1).
Two-Stage Image Synthesis To mitigate the shortcom-
ings of individual generative approaches, a lot of research
[11, 23, 67, 70, 101, 103] has gone into combining the
strengths of different methods into more efﬁcient and per-
formant models via a two stage approach. VQ-V AEs [67,
101] use autoregressive models to learn an expressive prior
over a discretized latent space. [66] extend this approach to
text-to-image generation by learning a joint distributation
over discretized image and text representations. More gen-
erally, [70] uses conditionally invertible networks to pro-
vide a generic transfer between latent spaces of diverse do-
mains. Different from VQ-V AEs, VQGANs [23, 103] em-
ploy a ﬁrst stage with an adversarial and perceptual objec-
tive to scale autoregressive transformers to larger images.
However, the high compression rates required for feasible
ARM training, which introduces billions of trainable pa-
rameters [23, 66], limit the overall performance of such ap-proaches and less compression comes at the price of high
computational cost [23, 66]. Our work prevents such trade-
offs, as our proposed LDMs scale more gently to higher
dimensional latent spaces due to their convolutional back-
bone. Thus, we are free to choose the level of compression
which optimally mediates between learning a powerful ﬁrst
stage, without leaving too much perceptual compression up
to the generative diffusion model while guaranteeing high-
ﬁdelity reconstructions (see Fig. 1).
While approaches to jointly [93] or separately [80] learn
an encoding/decoding model together with a score-based
prior exist, the former still require a difﬁcult weighting be-
tween reconstruction and generative capabilities [11] and
are outperformed by our approach (Sec. 4), and the latter
focus on highly structured images such as human faces.
3. Method
To lower the computational demands of training diffu-
sion models towards high-resolution image synthesis, we
observe that although diffusion models allow to ignore
perceptually irrelevant details by undersampling the corre-
sponding loss terms [30], they still require costly function
evaluations in pixel space, which causes huge demands in
computation time and energy resources.
We propose to circumvent this drawback by introducing
an explicit separation of the compressive from the genera-
tive learning phase (see Fig. 2). To achieve this, we utilize
an autoencoding model which learns a space that is percep-
tually equivalent to the image space, but offers signiﬁcantly
reduced computational complexity.
Such an approach offers several advantages: (i) By leav-
ing the high-dimensional image space, we obtain DMs
which are computationally much more efﬁcient because
sampling is performed on a low-dimensional space. (ii) We
exploit the inductive bias of DMs inherited from their UNet
architecture [71], which makes them particularly effective
for data with spatial structure and therefore alleviates the
need for aggressive, quality-reducing compression levels as
required by previous approaches [23, 66]. (iii) Finally, we
obtain general-purpose compression models whose latent
space can be used to train multiple generative models and
which can also be utilized for other downstream applica-
tions such as single-image CLIP-guided synthesis [25].
3.1. Perceptual Image Compression
Our perceptual compression model is based on previous
work [23] and consists of an autoencoder trained by com-
bination of a perceptual loss [106] and a patch-based [33]
adversarial objective [20, 23, 103]. This ensures that the re-
constructions are conﬁned to the image manifold by enforc-
ing local realism and avoids bluriness introduced by relying
solely on pixel-space losses such as L2orL1objectives.
More precisely, given an image x2RHW3in RGB
space, the encoder Eencodesxinto a latent representa-
3tionz=E(x), and the decoder Dreconstructs the im-
age from the latent, giving ~x=D(z) =D(E(x)), where
z2Rhwc. Importantly, the encoder downsamples the
image by a factor f=H=h =W=w , and we investigate
different downsampling factors f= 2m, withm2N.
In order to avoid arbitrarily high-variance latent spaces,
we experiment with two different kinds of regularizations.
The ﬁrst variant, KL-reg. , imposes a slight KL-penalty to-
wards a standard normal on the learned latent, similar to a
V AE [46, 69], whereas VQ-reg. uses a vector quantization
layer [96] within the decoder. This model can be interpreted
as a VQGAN [23] but with the quantization layer absorbed
by the decoder. Because our subsequent DM is designed
to work with the two-dimensional structure of our learned
latent space z=E(x), we can use relatively mild compres-
sion rates and achieve very good reconstructions. This is
in contrast to previous works [23, 66], which relied on an
arbitrary 1D ordering of the learned space zto model its
distribution autoregressively and thereby ignored much of
the inherent structure of z. Hence, our compression model
preserves details of xbetter (see Tab. 8). The full objective
and training details can be found in the supplement.
3.2. Latent Diffusion Models
Diffusion Models [82] are probabilistic models designed to
learn a data distribution p(x)by gradually denoising a nor-
mally distributed variable, which corresponds to learning
the reverse process of a ﬁxed Markov Chain of length T.
For image synthesis, the most successful models [15,30,72]
rely on a reweighted variant of the variational lower bound
onp(x), which mirrors denoising score-matching [85].
These models can be interpreted as an equally weighted
sequence of denoising autoencoders (xt;t);t= 1:::T ,
which are trained to predict a denoised variant of their input
xt, wherextis a noisy version of the input x. The corre-
sponding objective can be simpliﬁed to (Sec. B)
LDM=Ex;N(0;1);th
k(xt;t)k2
2i
; (1)
withtuniformly sampled from f1;:::;Tg.
Generative Modeling of Latent Representations With
our trained perceptual compression models consisting of E
andD, we now have access to an efﬁcient, low-dimensional
latent space in which high-frequency, imperceptible details
are abstracted away. Compared to the high-dimensional
pixel space, this space is more suitable for likelihood-based
generative models, as they can now (i) focus on the impor-
tant, semantic bits of the data and (ii) train in a lower di-
mensional, computationally much more efﬁcient space.
Unlike previous work that relied on autoregressive,
attention-based transformer models in a highly compressed,
discrete latent space [23,66,103], we can take advantage of
image-speciﬁc inductive biases that our model offers. This
Semantic  
 Map
crossattentionLatent SpaceConditioning  
TextDiffusion Process
denoising step switch skip connectionRepres  
entations
Pixel SpaceImagesDenoising U-Net
concatFigure 3. We condition LDMs either via concatenation or by a
more general cross-attention mechanism. See Sec. 3.3
includes the ability to build the underlying UNet primar-
ily from 2D convolutional layers, and further focusing the
objective on the perceptually most relevant bits using the
reweighted bound, which now reads
LLDM :=EE(x);N(0;1);th
k(zt;t)k2
2i
:(2)
The neural backbone (;t)of our model is realized as a
time-conditional UNet [71]. Since the forward process is
ﬁxed,ztcan be efﬁciently obtained from Eduring training,
and samples from p(z) can be decoded to image space with
a single pass through D.
3.3. Conditioning Mechanisms
Similar to other types of generative models [56, 83],
diffusion models are in principle capable of modeling
conditional distributions of the form p(zjy). This can
be implemented with a conditional denoising autoencoder
(zt;t;y)and paves the way to controlling the synthesis
process through inputs ysuch as text [68], semantic maps
[33, 61] or other image-to-image translation tasks [34].
In the context of image synthesis, however, combining
the generative power of DMs with other types of condition-
ings beyond class-labels [15] or blurred variants of the input
image [72] is so far an under-explored area of research.
We turn DMs into more ﬂexible conditional image gener-
ators by augmenting their underlying UNet backbone with
the cross-attention mechanism [97], which is effective for
learning attention-based models of various input modali-
ties [35,36]. To pre-process yfrom various modalities (such
as language prompts) we introduce a domain speciﬁc en-
coderthat projects yto an intermediate representation
(y)2RMd, which is then mapped to the intermediate
layers of the UNet via a cross-attention layer implementing
Attention (Q;K;V ) =softmax
QKT
p
d
V, with
Q=W(i)
Q'i(zt); K=W(i)
K(y); V=W(i)
V(y):
Here,'i(zt)2RNdi
denotes a (ﬂattened) intermediate
representation of the UNet implementing andW(i)
V2
4CelebAHQ FFHQ LSUN-Churches LSUN-Beds ImageNet
Figure 4. Samples from LDMs trained on CelebAHQ [39], FFHQ [41], LSUN-Churches [102], LSUN-Bedrooms [102] and class-
conditional ImageNet [12], each with a resolution of 256256. Best viewed when zoomed in. For more samples cf. the supplement.
Rddi
,W(i)
Q2Rdd&W(i)
K2Rddare learnable pro-
jection matrices [36, 97]. See Fig. 3 for a visual depiction.
Based on image-conditioning pairs, we then learn the
conditional LDM via
LLDM :=EE(x);y;N(0;1);th
k(zt;t;(y))k2
2i
;(3)
where bothandare jointly optimized via Eq. 3. This
conditioning mechanism is ﬂexible as can be parameter-
ized with domain-speciﬁc experts, e.g. (unmasked) trans-
formers [97] when yare text prompts (see Sec. 4.3.1)
4. Experiments
LDMs provide means to ﬂexible and computationally
tractable diffusion based image synthesis of various image
modalities, which we empirically show in the following.
Firstly, however, we analyze the gains of our models com-
pared to pixel-based diffusion models in both training and
inference. Interestingly, we ﬁnd that LDMs trained in VQ-
regularized latent spaces sometimes achieve better sample
quality, even though the reconstruction capabilities of VQ-
regularized ﬁrst stage models slightly fall behind those of
their continuous counterparts, cf. Tab. 8. A visual compari-
son between the effects of ﬁrst stage regularization schemes
onLDM training and their generalization abilities to resolu-
tions>2562can be found in Appendix D.1. In E.2 we list
details on architecture, implementation, training and evalu-
ation for all results presented in this section.
4.1. On Perceptual Compression Tradeoffs
This section analyzes the behavior of our LDMs with dif-
ferent downsampling factors f2f1;2;4;8;16;32g(abbre-
viated as LDM-f, where LDM-1 corresponds to pixel-based
DMs). To obtain a comparable test-ﬁeld, we ﬁx the com-
putational resources to a single NVIDIA A100 for all ex-
periments in this section and train all models for the same
number of steps and with the same number of parameters.
Tab. 8 shows hyperparameters and reconstruction perfor-
mance of the ﬁrst stage models used for the LDMs com-pared in this section. Fig. 6 shows sample quality as a func-
tion of training progress for 2M steps of class-conditional
models on the ImageNet [12] dataset. We see that, i) small
downsampling factors for LDM-f1,2gresult in slow train-
ing progress, whereas ii) overly large values of fcause stag-
nating ﬁdelity after comparably few training steps. Revis-
iting the analysis above (Fig. 1 and 2) we attribute this to
i) leaving most of perceptual compression to the diffusion
model and ii) too strong ﬁrst stage compression resulting
in information loss and thus limiting the achievable qual-
ity.LDM-f4-16gstrike a good balance between efﬁciency
and perceptually faithful results, which manifests in a sig-
niﬁcant FID [29] gap of 38 between pixel-based diffusion
(LDM-1 ) and LDM-8 after 2M training steps.
In Fig. 7, we compare models trained on CelebA-
HQ [39] and ImageNet in terms sampling speed for differ-
ent numbers of denoising steps with the DDIM sampler [84]
and plot it against FID-scores [29]. LDM-f4-8goutper-
form models with unsuitable ratios of perceptual and con-
ceptual compression. Especially compared to pixel-based
LDM-1 , they achieve much lower FID scores while simulta-
neously signiﬁcantly increasing sample throughput. Com-
plex datasets such as ImageNet require reduced compres-
sion rates to avoid reducing quality. In summary, LDM-4
and-8offer the best conditions for achieving high-quality
synthesis results.
4.2. Image Generation with Latent Diffusion
We train unconditional models of 2562images on
CelebA-HQ [39], FFHQ [41], LSUN-Churches and
-Bedrooms [102] and evaluate the i) sample quality and ii)
their coverage of the data manifold using ii) FID [29] and
ii) Precision-and-Recall [50]. Tab. 1 summarizes our re-
sults. On CelebA-HQ, we report a new state-of-the-art FID
of5:11, outperforming previous likelihood-based models as
well as GANs. We also outperform LSGM [93] where a la-
tent diffusion model is trained jointly together with the ﬁrst
stage. In contrast, we train diffusion models in a ﬁxed space
5Text-to-Image Synthesis on LAION. 1.45B Model.
A street sign that reads
Latent Diffusion A zombie in the
style of PicassoAn image of an animal
half mouse half octopusAn illustration of a slightly
conscious neural networkA painting of a
squirrel eating a burgerA watercolor painting of a
chair that looks like an octopusA shirt with the inscription:
I love generative models! 
Figure 5. Samples for user-deﬁned text prompts from our model for text-to-image synthesis, LDM-8 (KL) , which was trained on the
LAION [78] database. Samples generated with 200 DDIM steps and = 1:0. We use unconditional guidance [32] with s= 10:0.
Figure 6. Analyzing the training of class-conditional LDMs with
different downsampling factors fover 2M train steps on the Im-
ageNet dataset. Pixel-based LDM-1 requires substantially larger
train times compared to models with larger downsampling factors
(LDM-f4-16g). Too much perceptual compression as in LDM-32
limits the overall sample quality. All models are trained on a sin-
gle NVIDIA A100 with the same computational budget. Results
obtained with 100 DDIM steps [84] and = 0.
Figure 7. Comparing LDMs with varying compression on the
CelebA-HQ (left) and ImageNet (right) datasets. Different mark-
ers indicatef10;20;50;100;200gsampling steps using DDIM,
from right to left along each line. The dashed line shows the FID
scores for 200 steps, indicating the strong performance of LDM-
f4-8g. FID scores assessed on 5000 samples. All models were
trained for 500k (CelebA) / 2M (ImageNet) steps on an A100.
and avoid the difﬁculty of weighing reconstruction quality
against learning the prior over the latent space, see Fig. 1-2.
We outperform prior diffusion based approaches on all
but the LSUN-Bedrooms dataset, where our score is close
to ADM [15], despite utilizing half its parameters and re-
quiring 4-times less train resources (see Appendix E.3.5).CelebA-HQ 256256 FFHQ 256256
Method FID# Prec. Recall Method FID# Prec. Recall
DC-V AE [63] 15.8 - - ImageBART [21] 9.57 - -
VQGAN+T. [23] (k=400) 10.2 - - U-Net GAN (+aug) [77] 10.9 (7.6) - -
PGGAN [39] 8.0 - - UDM [43] 5.54 - -
LSGM [93] 7.22 - - StyleGAN [41] 4.16 0.71 0.46
UDM [43] 7.16 - - ProjectedGAN [76] 3.08 0.65 0.46
LDM-4 (ours, 500-sy) 5.11 0.72 0.49 LDM-4 (ours, 200-s) 4.98 0.73 0.50
LSUN-Churches 256256 LSUN-Bedrooms 256256
Method FID# Prec. Recall Method FID# Prec. Recall
DDPM [30] 7.89 - - ImageBART [21] 5.51 - -
ImageBART [21] 7.32 - - DDPM [30] 4.9 - -
PGGAN [39] 6.42 - - UDM [43] 4.57 - -
StyleGAN [41] 4.21 - - StyleGAN [41] 2.35 0.59 0.48
StyleGAN2 [42] 3.86 - - ADM [15] 1.90 0.66 0.51
ProjectedGAN [76] 1.59 0.61 0.44 ProjectedGAN [76] 1.52 0.61 0.34
LDM-8(ours, 200-s) 4.02 0.64 0.52 LDM-4 (ours, 200-s) 2.95 0.66 0.48
Table 1. Evaluation metrics for unconditional image synthesis.
CelebA-HQ results reproduced from [43, 63, 100], FFHQ from
[42, 43].y:N-s refers toNsampling steps with the DDIM [84]
sampler.: trained in KL-regularized latent space. Additional re-
sults can be found in the supplementary.
Text-Conditional Image Synthesis
Method FID# IS Nparams
CogViewy[17] 27.10 18.20 4B self-ranking, rejection rate 0.017
LAFITEy[109] 26.94 26.02 75M
GLIDE[59] 12.24 - 6B 277 DDIM steps, c.f.g. [32] s= 3
Make-A-Scene[26] 11.84 - 4B c.f.g for AR models [98] s= 5
LDM-KL-8 23.31 20.03 0.33 1.45B 250 DDIM steps
LDM-KL-8-G12.63 30.29 0.42 1.45B 250 DDIM steps, c.f.g. [32] s= 1:5
Table 2. Evaluation of text-conditional image synthesis on the
256256-sized MS-COCO [51] dataset: with 250 DDIM [84]
steps our model is on par with the most recent diffusion [59] and
autoregressive [26] methods despite using signiﬁcantly less pa-
rameters.y/:Numbers from [109]/ [26]
Moreover, LDMs consistently improve upon GAN-based
methods in Precision and Recall, thus conﬁrming the ad-
vantages of their mode-covering likelihood-based training
objective over adversarial approaches. In Fig. 4 we also
show qualitative results on each dataset.
6Figure 8. Layout-to-image synthesis with an LDM on COCO [4],
see Sec. 4.3.1. Quantitative evaluation in the supplement D.3.
4.3. Conditional Latent Diffusion
4.3.1 Transformer Encoders for LDMs
By introducing cross-attention based conditioning into
LDMs we open them up for various conditioning modali-
ties previously unexplored for diffusion models. For text-
to-image image modeling, we train a 1.45B parameter
KL-regularized LDM conditioned on language prompts on
LAION-400M [78]. We employ the BERT-tokenizer [14]
and implement as a transformer [97] to infer a latent
code which is mapped into the UNet via (multi-head) cross-
attention (Sec. 3.3). This combination of domain speciﬁc
experts for learning a language representation and visual
synthesis results in a powerful model, which generalizes
well to complex, user-deﬁned text prompts, cf. Fig. 8 and 5.
For quantitative analysis, we follow prior work and evaluate
text-to-image generation on the MS-COCO [51] validation
set, where our model improves upon powerful AR [17, 66]
and GAN-based [109] methods, cf. Tab. 2. We note that ap-
plying classiﬁer-free diffusion guidance [32] greatly boosts
sample quality, such that the guided LDM-KL-8-G is on par
with the recent state-of-the-art AR [26] and diffusion mod-
els [59] for text-to-image synthesis, while substantially re-
ducing parameter count. To further analyze the ﬂexibility of
the cross-attention based conditioning mechanism we also
train models to synthesize images based on semantic lay-
outs on OpenImages [49], and ﬁnetune on COCO [4], see
Fig. 8. See Sec. D.3 for the quantitative evaluation and im-
plementation details.
Lastly, following prior work [3, 15, 21, 23], we evalu-
ate our best-performing class-conditional ImageNet mod-
els withf2 f4;8gfrom Sec. 4.1 in Tab. 3, Fig. 4 and
Sec. D.4. Here we outperform the state of the art diffu-
sion model ADM [15] while signiﬁcantly reducing compu-
tational requirements and parameter count, cf. Tab 18.
4.3.2 Convolutional Sampling Beyond 2562
By concatenating spatially aligned conditioning informa-
tion to the input of ,LDMs can serve as efﬁcient general-Method FID# IS Precision RecallNparams
BigGan-deep [3] 6.95 203.6 2.6 0.87 0.28 340M -
ADM [15] 10.94 100.98 0.69 0.63 554M 250 DDIM steps
ADM-G [15] 4.59 186.7 0.82 0.52 608M 250 DDIM steps
LDM-4 (ours) 10.56 103.49 1.24 0.71 0.62 400M 250 DDIM steps
LDM-4 -G (ours) 3.60 247.67 5.59 0.87 0.48 400M 250 steps, c.f.g [32], s= 1:5
Table 3. Comparison of a class-conditional ImageNet LDM with
recent state-of-the-art methods for class-conditional image gener-
ation on ImageNet [12]. A more detailed comparison with addi-
tional baselines can be found in D.4, Tab. 10 and F. c.f.g. denotes
classiﬁer-free guidance with a scale sas proposed in [32].
purpose image-to-image translation models. We use this
to train models for semantic synthesis, super-resolution
(Sec. 4.4) and inpainting (Sec. 4.5). For semantic synthe-
sis, we use images of landscapes paired with semantic maps
[23, 61] and concatenate downsampled versions of the se-
mantic maps with the latent image representation of a f= 4
model (VQ-reg., see Tab. 8). We train on an input resolution
of2562(crops from 3842) but ﬁnd that our model general-
izes to larger resolutions and can generate images up to the
megapixel regime when evaluated in a convolutional man-
ner (see Fig. 9). We exploit this behavior to also apply the
super-resolution models in Sec. 4.4 and the inpainting mod-
els in Sec. 4.5 to generate large images between 5122and
10242. For this application, the signal-to-noise ratio (in-
duced by the scale of the latent space) signiﬁcantly affects
the results. In Sec. D.1 we illustrate this when learning an
LDM on (i) the latent space as provided by a f= 4 model
(KL-reg., see Tab. 8), and (ii) a rescaled version, scaled by
the component-wise standard deviation.
The latter, in combination with classiﬁer-free guid-
ance [32], also enables the direct synthesis of >2562im-
ages for the text-conditional LDM-KL-8-G as in Fig. 13.
Figure 9. A LDM trained on 2562resolution can generalize to
larger resolution (here: 5121024 ) for spatially conditioned tasks
such as semantic synthesis of landscape images. See Sec. 4.3.2.
4.4. Super-Resolution with Latent Diffusion
LDMs can be efﬁciently trained for super-resolution by
diretly conditioning on low-resolution images via concate-
nation ( cf. Sec. 3.3). In a ﬁrst experiment, we follow SR3
7bicubic LDM -SR SR3
Figure 10. ImageNet 64 !256 super-resolution on ImageNet-Val.
LDM-SR has advantages at rendering realistic textures but SR3
can synthesize more coherent ﬁne structures. See appendix for
additional samples and cropouts. SR3 results from [72].
[72] and ﬁx the image degradation to a bicubic interpola-
tion with 4-downsampling and train on ImageNet follow-
ing SR3s data processing pipeline. We use the f= 4 au-
toencoding model pretrained on OpenImages (VQ-reg., cf.
Tab. 8) and concatenate the low-resolution conditioning y
and the inputs to the UNet, i.e.is the identity. Our quali-
tative and quantitative results (see Fig. 10 and Tab. 5) show
competitive performance and LDM-SR outperforms SR3
in FID while SR3 has a better IS. A simple image regres-
sion model achieves the highest PSNR and SSIM scores;
however these metrics do not align well with human per-
ception [106] and favor blurriness over imperfectly aligned
high frequency details [72]. Further, we conduct a user
study comparing the pixel-baseline with LDM-SR. We fol-
low SR3 [72] where human subjects were shown a low-res
image in between two high-res images and asked for pref-
erence. The results in Tab. 4 afﬁrm the good performance
of LDM-SR. PSNR and SSIM can be pushed by using a
post-hoc guiding mechanism [15] and we implement this
image-based guider via a perceptual loss, see Sec. D.6.
SR on ImageNet Inpainting on Places
User Study Pixel-DM ( f1)LDM-4 LAMA [88] LDM-4
Task 1: Preference vs GT 16.0% 30.4% 13.6% 21.0%
Task 2: Preference Score 29.4% 70.6% 31.9% 68.1%
Table 4. Task 1: Subjects were shown ground truth and generated
image and asked for preference. Task 2: Subjects had to decide
between two generated images. More details in E.3.6
Since the bicubic degradation process does not generalize
well to images which do not follow this pre-processing, we
also train a generic model, LDM-BSR , by using more di-
verse degradation. The results are shown in Sec. D.6.1.Method FID# IS PSNR SSIMNparams [samples
s]()
Image Regression [72] 15.2 121.1 27.9 0.801 625M N/A
SR3 [72] 5.2 180.1 26.4 0.762 625M N/A
LDM-4 (ours, 100 steps) 2.8y/4.8z166.3 24.4 3.8 0.690.14 169M 4.62
emphLDM-4 (ours, big, 100 steps) 2.4y/4.3z174.9 24.74.1 0.710.15 552M 4.5
LDM-4 (ours, 50 steps, guiding) 4.4y/6.4z153.7 25.8 3.7 0.740.12 184M 0.38
Table 5.4upscaling results on ImageNet-Val. ( 2562);y: FID
features computed on validation split,z: FID features computed
on train split;: Assessed on a NVIDIA A100
train throughput sampling throughputytrain+val FID@2k
Model (reg.-type) samples/sec. @256 @512 hours/epoch epoch 6
LDM-1 (no ﬁrst stage) 0.11 0.26 0.07 20.66 24.74
LDM-4 (KL, w/ attn) 0.32 0.97 0.34 7.66 15.21
LDM-4 (VQ, w/ attn) 0.33 0.97 0.34 7.04 14.99
LDM-4 (VQ, w/o attn) 0.35 0.99 0.36 6.66 15.95
Table 6. Assessing inpainting efﬁciency.y: Deviations from Fig. 7
due to varying GPU settings/batch sizes cf. the supplement.
4.5. Inpainting with Latent Diffusion
Inpainting is the task of ﬁlling masked regions of an im-
age with new content either because parts of the image are
are corrupted or to replace existing but undesired content
within the image. We evaluate how our general approach
for conditional image generation compares to more special-
ized, state-of-the-art approaches for this task. Our evalua-
tion follows the protocol of LaMa [88], a recent inpainting
model that introduces a specialized architecture relying on
Fast Fourier Convolutions [8]. The exact training & evalua-
tion protocol on Places [108] is described in Sec. E.2.2.
We ﬁrst analyze the effect of different design choices for
the ﬁrst stage. In particular, we compare the inpainting ef-
ﬁciency of LDM-1 (i.e. a pixel-based conditional DM) with
LDM-4 , for both KLandVQregularizations, as well as VQ-
LDM-4 without any attention in the ﬁrst stage (see Tab. 8),
where the latter reduces GPU memory for decoding at high
resolutions. For comparability, we ﬁx the number of param-
eters for all models. Tab. 6 reports the training and sampling
throughput at resolution 2562and5122, the total training
time in hours per epoch and the FID score on the validation
split after six epochs. Overall, we observe a speed-up of at
least2:7between pixel- and latent-based diffusion models
while improving FID scores by a factor of at least 1:6.
The comparison with other inpainting approaches in
Tab. 7 shows that our model with attention improves the
overall image quality as measured by FID over that of [88].
LPIPS between the unmasked images and our samples is
slightly higher than that of [88]. We attribute this to [88]
only producing a single result which tends to recover more
of an average image compared to the diverse results pro-
duced by our LDM cf. Fig. 21. Additionally in a user study
(Tab. 4) human subjects favor our results over those of [88].
Based on these initial results, we also trained a larger dif-
fusion model ( bigin Tab. 7) in the latent space of the VQ-
regularized ﬁrst stage without attention. Following [15],
the UNet of this diffusion model uses attention layers on
three levels of its feature hierarchy, the BigGAN [3] residual
block for up- and downsampling and has 387M parameters
8input result
Figure 11. Qualitative results on object removal with our big, w/
ftinpainting model. For more results, see Fig. 22.
instead of 215M. After training, we noticed a discrepancy
in the quality of samples produced at resolutions 2562and
5122, which we hypothesize to be caused by the additional
attention modules. However, ﬁne-tuning the model for half
an epoch at resolution 5122allows the model to adjust to
the new feature statistics and sets a new state of the art FID
on image inpainting ( big, w/o attn, w/ ft in Tab. 7, Fig. 11.).
5. Limitations & Societal Impact
Limitations While LDMs signiﬁcantly reduce computa-
tional requirements compared to pixel-based approaches,
their sequential sampling process is still slower than that
of GANs. Moreover, the use of LDMs can be question-
able when high precision is required: although the loss of
image quality is very small in our f= 4autoencoding mod-
els (see Fig. 1), their reconstruction capability can become
a bottleneck for tasks that require ﬁne-grained accuracy in
pixel space. We assume that our superresolution models
(Sec. 4.4) are already somewhat limited in this respect.
Societal Impact Generative models for media like im-
agery are a double-edged sword: On the one hand, they40-50% masked All samples
Method FID# LPIPS# FID# LPIPS#
LDM-4 (ours, big, w/ ft) 9.39 0.246 0.042 1.50 0.137 0.080
LDM-4 (ours, big, w/o ft) 12.89 0.257 0.047 2.40 0.142 0.085
LDM-4 (ours, w/ attn) 11.87 0.257 0.042 2.15 0.144 0.084
LDM-4 (ours, w/o attn) 12.60 0.259 0.041 2.37 0.145 0.084
LaMa [88]y12.31 0.243 0.038 2.23 0.134 0.080
LaMa [88] 12.0 0.24 2.21 0.14
CoModGAN [107] 10.4 0.26 1.82 0.15
RegionWise [52] 21.3 0.27 4.75 0.15
DeepFill v2 [104] 22.1 0.28 5.20 0.16
EdgeConnect [58] 30.5 0.28 8.37 0.16
Table 7. Comparison of inpainting performance on 30k crops of
size512512from test images of Places [108]. The column 40-
50% reports metrics computed over hard examples where 40-50%
of the image region have to be inpainted.yrecomputed on our test
set, since the original test set used in [88] was not available.
enable various creative applications, and in particular ap-
proaches like ours that reduce the cost of training and in-
ference have the potential to facilitate access to this tech-
nology and democratize its exploration. On the other hand,
it also means that it becomes easier to create and dissemi-
nate manipulated data or spread misinformation and spam.
In particular, the deliberate manipulation of images (deep
fakes) is a common problem in this context, and women in
particular are disproportionately affected by it [13, 24].
Generative models can also reveal their training data
[5, 90], which is of great concern when the data contain
sensitive or personal information and were collected with-
out explicit consent. However, the extent to which this also
applies to DMs of images is not yet fully understood.
Finally, deep learning modules tend to reproduce or ex-
acerbate biases that are already present in the data [22, 38,
91]. While diffusion models achieve better coverage of the
data distribution than e.g. GAN-based approaches, the ex-
tent to which our two-stage approach that combines adver-
sarial training and a likelihood-based objective misrepre-
sents the data remains an important research question.
For a more general, detailed discussion of the ethical
considerations of deep generative models, see e.g. [13].
6. Conclusion
We have presented latent diffusion models, a simple and
efﬁcient way to signiﬁcantly improve both the training and
sampling efﬁciency of denoising diffusion models with-
out degrading their quality. Based on this and our cross-
attention conditioning mechanism, our experiments could
demonstrate favorable results compared to state-of-the-art
methods across a wide range of conditional image synthesis
tasks without task-speciﬁc architectures.
This work has been supported by the German Federal Ministry for
Economic Affairs and Energy within the project KI-Absicherung - Safe
AI for automated driving and by the German Research Foundation (DFG)
project 421703927.
9References
[1] Eirikur Agustsson and Radu Timofte. NTIRE 2017 chal-
lenge on single image super-resolution: Dataset and study.
In2017 IEEE Conference on Computer Vision and Pattern
Recognition Workshops, CVPR Workshops 2017, Honolulu,
HI, USA, July 21-26, 2017 , pages 11221131. IEEE Com-
puter Society, 2017. 1
[2] Martin Arjovsky, Soumith Chintala, and L eon Bottou.
Wasserstein gan, 2017. 3
[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale GAN training for high ﬁdelity natural image synthe-
sis. In Int. Conf. Learn. Represent. , 2019. 1, 2, 7, 8, 22,
28
[4] Holger Caesar, Jasper R. R. Uijlings, and Vittorio Ferrari.
Coco-stuff: Thing and stuff classes in context. In 2018
IEEE Conference on Computer Vision and Pattern Recog-
nition, CVPR 2018, Salt Lake City, UT, USA, June 18-
22, 2018 , pages 12091218. Computer Vision Foundation /
IEEE Computer Society, 2018. 7, 20, 22
[5] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew
Jagielski, Ariel Herbert-V oss, Katherine Lee, Adam
Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al.
Extracting training data from large language models. In
30th USENIX Security Symposium (USENIX Security 21) ,
pages 26332650, 2021. 9
[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-
woo Jun, David Luan, and Ilya Sutskever. Generative pre-
training from pixels. In ICML , volume 119 of Proceedings
of Machine Learning Research , pages 16911703. PMLR,
2020. 3
[7] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mo-
hammad Norouzi, and William Chan. Wavegrad: Estimat-
ing gradients for waveform generation. In ICLR . OpenRe-
view.net, 2021. 1
[8] Lu Chi, Borui Jiang, and Yadong Mu. Fast fourier convolu-
tion. In NeurIPS , 2020. 8
[9] Rewon Child. Very deep vaes generalize autoregressive
models and can outperform them on images. CoRR ,
abs/2011.10650, 2020. 3
[10] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
Generating long sequences with sparse transformers.
CoRR , abs/1904.10509, 2019. 3
[11] Bin Dai and David P. Wipf. Diagnosing and enhancing V AE
models. In ICLR (Poster) . OpenReview.net, 2019. 2, 3
[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Fei-Fei Li. Imagenet: A large-scale hierarchical im-
age database. In CVPR , pages 248255. IEEE Computer
Society, 2009. 1, 5, 7, 22
[13] Emily Denton. Ethical considerations of generative ai. AI
for Content Creation Workshop, CVPR, 2021. 9
[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. BERT: pre-training of deep bidirec-
tional transformers for language understanding. CoRR ,
abs/1810.04805, 2018. 7
[15] Prafulla Dhariwal and Alex Nichol. Diffusion models beat
gans on image synthesis. CoRR , abs/2105.05233, 2021. 1,
2, 3, 4, 6, 7, 8, 18, 22, 25, 26, 28[16] Sander Dieleman. Musings on typicality, 2020. 1, 3
[17] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,
Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,
Hongxia Yang, and Jie Tang. Cogview: Mastering text-to-
image generation via transformers. CoRR , abs/2105.13290,
2021. 6, 7
[18] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice:
Non-linear independent components estimation, 2015. 3
[19] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Ben-
gio. Density estimation using real NVP. In 5th Inter-
national Conference on Learning Representations, ICLR
2017, Toulon, France, April 24-26, 2017, Conference Track
Proceedings . OpenReview.net, 2017. 1, 3
[20] Alexey Dosovitskiy and Thomas Brox. Generating images
with perceptual similarity metrics based on deep networks.
In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg,
Isabelle Guyon, and Roman Garnett, editors, Adv. Neural
Inform. Process. Syst. , pages 658666, 2016. 3
[21] Patrick Esser, Robin Rombach, Andreas Blattmann, and
Bjorn Ommer. Imagebart: Bidirectional context with multi-
nomial diffusion for autoregressive image synthesis. CoRR ,
abs/2108.08827, 2021. 6, 7, 22
[22] Patrick Esser, Robin Rombach, and Bj orn Ommer. A
note on data biases in generative models. arXiv preprint
arXiv:2012.02516 , 2020. 9
[23] Patrick Esser, Robin Rombach, and Bj orn Ommer. Taming
transformers for high-resolution image synthesis. CoRR ,
abs/2012.09841, 2020. 2, 3, 4, 6, 7, 21, 22, 29, 34, 36
[24] Mary Anne Franks and Ari Ezra Waldman. Sex, lies, and
videotape: Deep fakes and free speech delusions. Md. L.
Rev., 78:892, 2018. 9
[25] Kevin Frans, Lisa B. Soros, and Olaf Witkowski. Clipdraw:
Exploring text-to-drawing synthesis through language-
image encoders. ArXiv , abs/2106.14843, 2021. 3
[26] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,
Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-
based text-to-image generation with human priors. CoRR ,
abs/2203.13131, 2022. 6, 7, 16
[27] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville,
and Yoshua Bengio. Generative adversarial networks.
CoRR , 2014. 1, 2
[28] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent
Dumoulin, and Aaron Courville. Improved training of
wasserstein gans, 2017. 3
[29] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by
a two time-scale update rule converge to a local nash equi-
librium. In Adv. Neural Inform. Process. Syst. , pages 6626
6637, 2017. 1, 5, 26
[30] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. In NeurIPS , 2020. 1, 2, 3, 4,
6, 17
[31] Jonathan Ho, Chitwan Saharia, William Chan, David J.
Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded
diffusion models for high ﬁdelity image generation. CoRR ,
abs/2106.15282, 2021. 1, 3, 22
10[32] Jonathan Ho and Tim Salimans. Classiﬁer-free diffusion
guidance. In NeurIPS 2021 Workshop on Deep Generative
Models and Downstream Applications , 2021. 6, 7, 16, 22,
28, 37, 38
[33] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A.
Efros. Image-to-image translation with conditional adver-
sarial networks. In CVPR , pages 59675976. IEEE Com-
puter Society, 2017. 3, 4
[34] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A.
Efros. Image-to-image translation with conditional adver-
sarial networks. 2017 IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , pages 59675976,
2017. 4
[35] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste
Alayrac, Carl Doersch, Catalin Ionescu, David Ding,
Skanda Koppula, Daniel Zoran, Andrew Brock, Evan
Shelhamer, Olivier J. H enaff, Matthew M. Botvinick,
Andrew Zisserman, Oriol Vinyals, and Jo ao Carreira.
Perceiver IO: A general architecture for structured inputs
&outputs. CoRR , abs/2107.14795, 2021. 4
[36] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,
Andrew Zisserman, and Jo ao Carreira. Perceiver: General
perception with iterative attention. In Marina Meila and
Tong Zhang, editors, Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-24 July
2021, Virtual Event , volume 139 of Proceedings of Machine
Learning Research , pages 46514664. PMLR, 2021. 4, 5
[37] Manuel Jahn, Robin Rombach, and Bj orn Ommer. High-
resolution complex scene synthesis with transformers.
CoRR , abs/2105.06458, 2021. 20, 22, 27
[38] Niharika Jain, Alberto Olmo, Sailik Sengupta, Lydia
Manikonda, and Subbarao Kambhampati. Imperfect ima-
ganation: Implications of gans exacerbating biases on fa-
cial data augmentation and snapchat selﬁe lenses. arXiv
preprint arXiv:2001.09528 , 2020. 9
[39] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehti-
nen. Progressive growing of gans for improved quality, sta-
bility, and variation. CoRR , abs/1710.10196, 2017. 5, 6
[40] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
InIEEE Conf. Comput. Vis. Pattern Recog. , pages 4401
4410, 2019. 1
[41] T. Karras, S. Laine, and T. Aila. A style-based gener-
ator architecture for generative adversarial networks. In
2019 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , 2019. 5, 6
[42] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improv-
ing the image quality of stylegan. CoRR , abs/1912.04958,
2019. 2, 6, 28
[43] Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo
Kang, and Il-Chul Moon. Score matching model for un-
bounded data score. CoRR , abs/2106.05527, 2021. 6
[44] Durk P Kingma and Prafulla Dhariwal. Glow: Generative
ﬂow with invertible 1x1 convolutions. In S. Bengio, H. Wal-
lach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R.
Garnett, editors, Advances in Neural Information Process-
ing Systems , 2018. 3[45] Diederik P. Kingma, Tim Salimans, Ben Poole, and
Jonathan Ho. Variational diffusion models. CoRR ,
abs/2107.00630, 2021. 1, 3, 16
[46] Diederik P. Kingma and Max Welling. Auto-Encoding Vari-
ational Bayes. In 2nd International Conference on Learn-
ing Representations, ICLR , 2014. 1, 3, 4, 29
[47] Zhifeng Kong and Wei Ping. On fast sampling of diffusion
probabilistic models. CoRR , abs/2106.00132, 2021. 3
[48] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and
Bryan Catanzaro. Diffwave: A versatile diffusion model
for audio synthesis. In ICLR . OpenReview.net, 2021. 1
[49] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper R. R.
Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali,
Stefan Popov, Matteo Malloci, Tom Duerig, and Vittorio
Ferrari. The open images dataset V4: uniﬁed image classi-
ﬁcation, object detection, and visual relationship detection
at scale. CoRR , abs/1811.00982, 2018. 7, 20, 22
[50] Tuomas Kynk aanniemi, Tero Karras, Samuli Laine, Jaakko
Lehtinen, and Timo Aila. Improved precision and re-
call metric for assessing generative models. CoRR ,
abs/1904.06991, 2019. 5, 26
[51] Tsung-Yi Lin, Michael Maire, Serge J. Belongie,
Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro
Perona, Deva Ramanan, Piotr Doll ar, and C. Lawrence Zit-
nick. Microsoft COCO: common objects in context. CoRR ,
abs/1405.0312, 2014. 6, 7, 27
[52] Yuqing Ma, Xianglong Liu, Shihao Bai, Le-Yi Wang, Ais-
han Liu, Dacheng Tao, and Edwin Hancock. Region-wise
generative adversarial imageinpainting for large missing ar-
eas. ArXiv , abs/1909.12507, 2019. 9
[53] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-
Yan Zhu, and Stefano Ermon. Sdedit: Image synthesis
and editing with stochastic differential equations. CoRR ,
abs/2108.01073, 2021. 1
[54] Lars M. Mescheder. On the convergence properties of GAN
training. CoRR , abs/1801.04406, 2018. 3
[55] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-
Dickstein. Unrolled generative adversarial networks. In
5th International Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Conference
Track Proceedings . OpenReview.net, 2017. 3
[56] Mehdi Mirza and Simon Osindero. Conditional generative
adversarial nets. CoRR , abs/1411.1784, 2014. 4
[57] Gautam Mittal, Jesse H. Engel, Curtis Hawthorne, and Ian
Simon. Symbolic music generation with diffusion models.
CoRR , abs/2103.16091, 2021. 1
[58] Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z. Qureshi,
and Mehran Ebrahimi. Edgeconnect: Generative im-
age inpainting with adversarial edge learning. ArXiv ,
abs/1901.00212, 2019. 9
[59] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. GLIDE: towards photorealistic image genera-
tion and editing with text-guided diffusion models. CoRR ,
abs/2112.10741, 2021. 6, 7, 16
[60] Anton Obukhov, Maximilian Seitzer, Po-Wei Wu, Se-
men Zhydenko, Jonathan Kyl, and Elvis Yu-Jing Lin.
11High-ﬁdelity performance metrics for generative models
in pytorch, 2020. Version: 0.3.0, DOI: 10.5281/zen-
odo.4957738. 26, 27
[61] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-
Yan Zhu. Semantic image synthesis with spatially-adaptive
normalization. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , 2019. 4, 7
[62] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-
Yan Zhu. Semantic image synthesis with spatially-adaptive
normalization. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
June 2019. 22
[63] Gaurav Parmar, Dacheng Li, Kwonjoon Lee, and Zhuowen
Tu. Dual contradistinctive generative autoencoder. In IEEE
Conference on Computer Vision and Pattern Recognition,
CVPR 2021, virtual, June 19-25, 2021 , pages 823832.
Computer Vision Foundation / IEEE, 2021. 6
[64] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On
buggy resizing libraries and surprising subtleties in ﬁd cal-
culation. arXiv preprint arXiv:2104.11222 , 2021. 26
[65] David A. Patterson, Joseph Gonzalez, Quoc V . Le, Chen
Liang, Lluis-Miquel Munguia, Daniel Rothchild, David R.
So, Maud Texier, and Jeff Dean. Carbon emissions and
large neural network training. CoRR , abs/2104.10350,
2021. 2
[66] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott
Gray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya
Sutskever. Zero-shot text-to-image generation. CoRR ,
abs/2102.12092, 2021. 1, 2, 3, 4, 7, 21, 27
[67] Ali Razavi, A aron van den Oord, and Oriol Vinyals. Gen-
erating diverse high-ﬁdelity images with VQ-V AE-2. In
NeurIPS , pages 1483714847, 2019. 1, 2, 3, 22
[68] Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-
geswaran, Bernt Schiele, and Honglak Lee. Generative ad-
versarial text to image synthesis. In ICML , 2016. 4
[69] Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Wierstra. Stochastic backpropagation and approximate in-
ference in deep generative models. In Proceedings of the
31st International Conference on International Conference
on Machine Learning, ICML , 2014. 1, 4, 29
[70] Robin Rombach, Patrick Esser, and Bj orn Ommer.
Network-to-network translation with conditional invertible
neural networks. In NeurIPS , 2020. 3
[71] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In MICCAI (3) , volume 9351 of Lecture Notes in
Computer Science , pages 234241. Springer, 2015. 2, 3, 4
[72] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sal-
imans, David J. Fleet, and Mohammad Norouzi. Im-
age super-resolution via iterative reﬁnement. CoRR ,
abs/2104.07636, 2021. 1, 4, 8, 16, 22, 23, 27
[73] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P.
Kingma. Pixelcnn++: Improving the pixelcnn with dis-
cretized logistic mixture likelihood and other modiﬁcations.
CoRR , abs/1701.05517, 2017. 1, 3
[74] Dave Salvator. NVIDIA Developer Blog. https:
/ / developer . nvidia . com / blog / getting -immediate-speedups-with-a100-tf32 , 2020.
28
[75] Robin San-Roman, Eliya Nachmani, and Lior Wolf.
Noise estimation for generative diffusion models. CoRR ,
abs/2104.02600, 2021. 3
[76] Axel Sauer, Kashyap Chitta, Jens M uller, and An-
dreas Geiger. Projected gans converge faster. CoRR ,
abs/2111.01007, 2021. 6
[77] Edgar Sch onfeld, Bernt Schiele, and Anna Khoreva. A u-
net based discriminator for generative adversarial networks.
In2020 IEEE/CVF Conference on Computer Vision and
Pattern Recognition, CVPR 2020, Seattle, WA, USA, June
13-19, 2020 , pages 82048213. Computer Vision Founda-
tion / IEEE, 2020. 6
[78] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-
400m: Open dataset of clip-ﬁltered 400 million image-text
pairs, 2021. 6, 7
[79] Karen Simonyan and Andrew Zisserman. Very deep con-
volutional networks for large-scale image recognition. In
Yoshua Bengio and Yann LeCun, editors, Int. Conf. Learn.
Represent. , 2015. 29, 43, 44, 45
[80] Abhishek Sinha, Jiaming Song, Chenlin Meng, and Stefano
Ermon. D2C: diffusion-denoising models for few-shot con-
ditional generation. CoRR , abs/2106.06819, 2021. 3
[81] Charlie Snell. Alien Dreams: An Emerging Art Scene.
https : / / ml . berkeley . edu / blog / posts /
clip-art/ , 2021. [Online; accessed November-2021].
2
[82] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Mah-
eswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. CoRR ,
abs/1503.03585, 2015. 1, 3, 4, 18
[83] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learn-
ing structured output representation using deep conditional
generative models. In C. Cortes, N. Lawrence, D. Lee,
M. Sugiyama, and R. Garnett, editors, Advances in Neural
Information Processing Systems , volume 28. Curran Asso-
ciates, Inc., 2015. 4
[84] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In ICLR . OpenReview.net,
2021. 3, 5, 6, 22
[85] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma,
Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-
based generative modeling through stochastic differential
equations. CoRR , abs/2011.13456, 2020. 1, 3, 4, 18
[86] Emma Strubell, Ananya Ganesh, and Andrew McCallum.
Energy and policy considerations for modern deep learn-
ing research. In The Thirty-Fourth AAAI Conference on
Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second In-
novative Applications of Artiﬁcial Intelligence Conference,
IAAI 2020, The Tenth AAAI Symposium on Educational
Advances in Artiﬁcial Intelligence, EAAI 2020, New York,
NY, USA, February 7-12, 2020 , pages 1369313696. AAAI
Press, 2020. 2
12[87] Wei Sun and Tianfu Wu. Learning layout and style re-
conﬁgurable gans for controllable image synthesis. CoRR ,
abs/2003.11571, 2020. 22, 27
[88] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,
Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,
Naejin Kong, Harshith Goka, Kiwoong Park, and Victor S.
Lempitsky. Resolution-robust large mask inpainting with
fourier convolutions. ArXiv , abs/2109.07161, 2021. 8, 9,
26, 32
[89] Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R. De-
von Hjelm, and Shikhar Sharma. Object-centric image gen-
eration from layouts. In Thirty-Fifth AAAI Conference on
Artiﬁcial Intelligence, AAAI 2021, Thirty-Third Conference
on Innovative Applications of Artiﬁcial Intelligence, IAAI
2021, The Eleventh Symposium on Educational Advances
in Artiﬁcial Intelligence, EAAI 2021, Virtual Event, Febru-
ary 2-9, 2021 , pages 26472655. AAAI Press, 2021. 20,
22, 27
[90] Patrick Tinsley, Adam Czajka, and Patrick Flynn. This face
does not exist... but it might be yours! identity leakage in
generative models. In Proceedings of the IEEE/CVF Win-
ter Conference on Applications of Computer Vision , pages
13201328, 2021. 9
[91] Antonio Torralba and Alexei A Efros. Unbiased look at
dataset bias. In CVPR 2011 , pages 15211528. IEEE, 2011.
9
[92] Arash Vahdat and Jan Kautz. NV AE: A deep hierarchical
variational autoencoder. In NeurIPS , 2020. 3
[93] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-
based generative modeling in latent space. CoRR ,
abs/2106.05931, 2021. 2, 3, 5, 6
[94] Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt,
koray kavukcuoglu, Oriol Vinyals, and Alex Graves. Con-
ditional image generation with pixelcnn decoders. In Ad-
vances in Neural Information Processing Systems , 2016. 3
[95] A aron van den Oord, Nal Kalchbrenner, and Koray
Kavukcuoglu. Pixel recurrent neural networks. CoRR ,
abs/1601.06759, 2016. 3
[96] A aron van den Oord, Oriol Vinyals, and Koray
Kavukcuoglu. Neural discrete representation learning. In
NIPS , pages 63066315, 2017. 2, 4, 29
[97] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In NIPS ,
pages 59986008, 2017. 3, 4, 5, 7
[98] Rivers Have Wings. Tweet on Classiﬁer-free
guidance for autoregressive models. https :
/ / twitter . com / RiversHaveWings / status /
1478093658716966912 , 2022. 6
[99] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-
mond, Clement Delangue, Anthony Moi, Pierric Cistac,
Tim Rault, R emi Louf, Morgan Funtowicz, and Jamie
Brew. Huggingfaces transformers: State-of-the-art natural
language processing. CoRR , abs/1910.03771, 2019. 26
[100] Zhisheng Xiao, Karsten Kreis, Jan Kautz, and Arash Vah-
dat. V AEBM: A symbiosis between variational autoen-
coders and energy-based models. In 9th International Con-ference on Learning Representations, ICLR 2021, Virtual
Event, Austria, May 3-7, 2021 . OpenReview.net, 2021. 6
[101] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind
Srinivas. Videogpt: Video generation using VQ-V AE and
transformers. CoRR , abs/2104.10157, 2021. 3
[102] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianx-
iong Xiao. LSUN: construction of a large-scale image
dataset using deep learning with humans in the loop. CoRR ,
abs/1506.03365, 2015. 5
[103] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang,
James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge,
and Yonghui Wu. Vector-quantized image modeling with
improved vqgan, 2021. 3, 4
[104] Jiahui Yu, Zhe L. Lin, Jimei Yang, Xiaohui Shen, Xin Lu,
and Thomas S. Huang. Free-form image inpainting with
gated convolution. 2019 IEEE/CVF International Confer-
ence on Computer Vision (ICCV) , pages 44704479, 2019.
9
[105] K. Zhang, Jingyun Liang, Luc Van Gool, and Radu Timo-
fte. Designing a practical degradation model for deep blind
image super-resolution. ArXiv , abs/2103.14006, 2021. 23
[106] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR) , June 2018. 3, 8, 19
[107] Shengyu Zhao, Jianwei Cui, Yilun Sheng, Yue Dong, Xiao
Liang, Eric I-Chao Chang, and Yan Xu. Large scale image
completion via co-modulated generative adversarial net-
works. ArXiv , abs/2103.10428, 2021. 9
[108] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
and Antonio Torralba. Places: A 10 million image database
for scene recognition. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence , 40:14521464, 2018. 8, 9,
26
[109] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li,
Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and
Tong Sun. LAFITE: towards language-free training for
text-to-image generation. CoRR , abs/2111.13792, 2021. 6,
7, 16
13Appendix
Figure 12. Convolutional samples from the semantic landscapes model as in Sec. 4.3.2, ﬁnetuned on 5122images.
14A painting of the last supper by Picasso. 
An oil painting of a latent space. An epic painting of Gandalf the Black
summoning thunder and lightning in the mountains. 
A sunset over a mountain range, vector image. 
Figure 13. Combining classiﬁer free diffusion guidance with the convolutional sampling strategy from Sec. 4.3.2, our 1.45B parameter
text-to-image model can be used for rendering images larger than the native 2562resolution the model was trained on.
15A. Changelog
Here we list changes between this version ( https://arxiv.org/abs/2112.10752v2 ) of the paper and the
previous version, i.e.https://arxiv.org/abs/2112.10752v1 .
 We updated the results on text-to-image synthesis in Sec. 4.3 which were obtained by training a new, larger model (1.45B
parameters). This also includes a new comparison to very recent competing methods on this task that were published on
arXiv at the same time as ( [59, 109]) or after ( [26]) the publication of our work.
 We updated results on class-conditional synthesis on ImageNet in Sec. 4.1, Tab. 3 (see also Sec. D.4) obtained by
retraining the model with a larger batch size. The corresponding qualitative results in Fig. 26 and Fig. 27 were also
updated. Both the updated text-to-image and the class-conditional model now use classiﬁer-free guidance [32] as a
measure to increase visual ﬁdelity.
 We conducted a user study (following the scheme suggested by Saharia et al [72]) which provides additional evaluation
for our inpainting (Sec. 4.5) and superresolution models (Sec. 4.4).
 Added Fig. 5 to the main paper, moved Fig. 18 to the appendix, added Fig. 13 to the appendix.
B. Detailed Information on Denoising Diffusion Models
Diffusion models can be speciﬁed in terms of a signal-to-noise ratio SNR (t) =2
t
2
tconsisting of sequences (t)T
t=1and
(t)T
t=1which, starting from a data sample x0, deﬁne a forward diffusion process qas
q(xtjx0) =N(xtjtx0;2
tI) (4)
with the Markov structure for s<t :
q(xtjxs) =N(xtjtjsxs;2
tjsI) (5)
tjs=t
s(6)
2
tjs=2
t2
tjs2
s (7)
Denoising diffusion models are generative models p(x0)which revert this process with a similar Markov structure running
backward in time, i.e. they are speciﬁed as
p(x0) =Z
zp(xT)TY
t=1p(xt1jxt) (8)
The evidence lower bound (ELBO) associated with this model then decomposes over the discrete time steps as
logp(x0)KL(q(xTjx0)jp(xT)) +TX
t=1Eq(xtjx0)KL(q(xt1jxt;x0)jp(xt1jxt)) (9)
The priorp(xT)is typically choosen as a standard normal distribution and the ﬁrst term of the ELBO then depends only on
the ﬁnal signal-to-noise ratio SNR (T). To minimize the remaining terms, a common choice to parameterize p(xt1jxt)is to
specify it in terms of the true posterior q(xt1jxt;x0)but with the unknown x0replaced by an estimate x(xt;t)based on
the current step xt. This gives [45]
p(xt1jxt):=q(xt1jxt;x(xt;t)) (10)
=N(xt1j(xt;t);2
tjt12
t1
2
tI); (11)
where the mean can be expressed as
(xt;t) =tjt12
t1
2
txt+t12
tjt1
2
tx(xt;t): (12)
16In this case, the sum of the ELBO simplify to
TX
t=1Eq(xtjx0)KL(q(xt1jxt;x0)jp(xt1) =TX
t=1EN(j0;I)1
2(SNR(t1)SNR(t))kx0x(tx0+t;t)k2(13)
Following [30], we use the reparameterization
(xt;t) = (xttx(xt;t))=t (14)
to express the reconstruction term as a denoising objective,
kx0x(tx0+t;t)k2=2
t
2
tk(tx0+t;t)k2(15)
and the reweighting, which assigns each of the terms the same weight and results in Eq. (1).
17C. Image Guiding Mechanisms
Samples 2562Guided Convolutional Samples 5122Convolutional Samples 5122
Figure 14. On landscapes, convolutional sampling with unconditional models can lead to homogeneous and incoherent global structures
(see column 2). L2-guiding with a low resolution image can help to reestablish coherent global structures.
An intriguing feature of diffusion models is that unconditional models can be conditioned at test-time [15, 82, 85]. In
particular, [15] presented an algorithm to guide both unconditional and conditional models trained on the ImageNet dataset
with a classiﬁer logp(yjxt), trained on each xtof the diffusion process. We directly build on this formulation and introduce
post-hoc image-guiding :
For an epsilon-parameterized model with ﬁxed variance, the guiding algorithm as introduced in [15] reads:
^ (zt;t) +q
12
trztlogp(yjzt): (16)
This can be interpreted as an update correcting the score with a conditional distribution logp(yjzt).
So far, this scenario has only been applied to single-class classiﬁcation models. We re-interpret the guiding distribution
p(yjT(D(z0(zt)))) as a general purpose image-to-image translation task given a target image y, whereTcan be any
differentiable transformation adopted to the image-to-image translation task at hand, such as the identity, a downsampling
operation or similar.
18As an example, we can assume a Gaussian guider with ﬁxed variance 2= 1, such that
logp(yjzt) =1
2kyT(D(z0(zt)))k2
2 (17)
becomes aL2regression objective.
Fig. 14 demonstrates how this formulation can serve as an upsampling mechanism of an unconditional model trained on
2562images, where unconditional samples of size 2562guide the convolutional synthesis of 5122images and Tis a2
bicubic downsampling. Following this motivation, we also experiment with a perceptual similarity guiding and replace the
L2objective with the LPIPS [106] metric, see Sec. 4.4.
19D. Additional Results
D.1. Choosing the Signal-to-Noise Ratio for High-Resolution Synthesis
KL-reg, w/o rescaling KL-reg, w/ rescaling VQ-reg, w/o rescaling
Figure 15. Illustrating the effect of latent space rescaling on convolutional sampling, here for semantic image synthesis on landscapes. See
Sec. 4.3.2 and Sec. D.1.
As discussed in Sec. 4.3.2, the signal-to-noise ratio induced by the variance of the latent space ( i.e. Var(z)=2
t) signiﬁcantly
affects the results for convolutional sampling. For example, when training a LDM directly in the latent space of a KL-
regularized model (see Tab. 8), this ratio is very high, such that the model allocates a lot of semantic detail early on in the
reverse denoising process. In contrast, when rescaling the latent space by the component-wise standard deviation of the
latents as described in Sec. G, the SNR is descreased. We illustrate the effect on convolutional sampling for semantic image
synthesis in Fig. 15. Note that the VQ-regularized space has a variance close to 1, such that it does not have to be rescaled.
D.2. Full List of all First Stage Models
We provide a complete list of various autoenconding models trained on the OpenImages dataset in Tab. 8.
D.3. Layout-to-Image Synthesis
Here we provide the quantitative evaluation and additional samples for our layout-to-image models from Sec. 4.3.1. We
train a model on the COCO [4] and one on the OpenImages [49] dataset, which we subsequently additionally ﬁnetune on
COCO. Tab 9 shows the result. Our COCO model reaches the performance of recent state-of-the art models in layout-to-
image synthesis, when following their training and evaluation protocol [89]. When ﬁnetuning from the OpenImages model,
we surpass these works. Our OpenImages model surpasses the results of Jahn et al [37] by a margin of nearly 11 in terms of
FID. In Fig. 16 we show additional samples of the model ﬁnetuned on COCO.
D.4. Class-Conditional Image Synthesis on ImageNet
Tab. 10 contains the results for our class-conditional LDM measured in FID and Inception score (IS). LDM-8 requires
signiﬁcantly fewer parameters and compute requirements (see Tab. 18) to achieve very competitive performance. Similar
to previous work, we can further boost the performance by training a classiﬁer on each noise scale and guiding with it,
20fjZj c R-FID# R-IS PSNR PSIM# SSIM
16VQGAN [23] 16384 256 4.98  19.9 3:4 1.830:42 0.510:18
16VQGAN [23] 1024 256 7.94  19.4 3:3 1.980:43 0.500:18
8DALL-E [66] 8192 - 32.01  22.8 2:1 1.950:51 0.730:13
32 16384 16 31.83 40.40 1:07 17.45 2:90 2.580:48 0.410:18
16 16384 8 5.15 144.55 3:74 20.83 3:61 1.730:43 0.540:18
8 16384 4 1.14 201.92 3:97 23.07 3:99 1.170:36 0.650:16
8 256 4 1.49 194.20 3:87 22.35 3:81 1.260:37 0.620:16
4 8192 3 0.58 224.78 5:35 27.43 4:26 0.530:21 0.820:10
4y8192 3 1.06 221.94 4:58 25.21 4:17 0.720:26 0.760:12
4 256 3 0.47 223.81 4:58 26.43 4:22 0.620:24 0.800:11
2 2048 2 0.16 232.75 5:09 30.85 4:12 0.270:12 0.910:05
2 64 2 0.40 226.62 4:83 29.13 3:46 0.380:13 0.900:05
32 KL 64 2.04 189.53 3:68 22.27 3:93 1.410:40 0.610:17
32 KL 16 7.3 132.75 2:71 20.38 3:56 1.880:45 0.530:18
16 KL 16 0.87 210.31 3:97 24.08 4:22 1.070:36 0.680:15
16 KL 8 2.63 178.68 4:08 21.94 3:92 1.490:42 0.590:17
8 KL 4 0.90 209.90 4:92 24.19 4:19 1.020:35 0.690:15
4 KL 3 0.27 227.57 4:89 27.53 4:54 0.550:24 0.820:11
2 KL 2 0.086 232.66 5:16 32.47 4:19 0.200:09 0.930:04
Table 8. Complete autoencoder zoo trained on OpenImages, evaluated on ImageNet-Val. ydenotes an attention-free autoencoder.
layout-to-image synthesis on the COCO dataset
Figure 16. More samples from our best model for layout-to-image synthesis, LDM-4 , which was trained on the OpenImages dataset and
ﬁnetuned on the COCO dataset. Samples generated with 100 DDIM steps and = 0. Layouts are from the COCO validation set.
see Sec. C. Unlike the pixel-based methods, this classiﬁer is trained very cheaply in latent space. For additional qualitative
results, see Fig. 26 and Fig. 27.
21COCO 256256 OpenImages 256256 OpenImages 512512
Method FID# FID# FID#
LostGAN-V2 [87] 42.55 - -
OC-GAN [89] 41.65 - -
SPADE [62] 41.11 - -
VQGAN+T [37] 56.58 45.33 48.11
LDM-8 (100 steps, ours) 42.06y- -
LDM-4 (200 steps, ours) 40.9132.02 35.80
Table 9. Quantitative comparison of our layout-to-image models on the COCO [4] and OpenImages [49] datasets.y: Training from scratch
on COCO;: Finetuning from OpenImages.
Method FID# IS Precision RecallNparams
SR3 [72] 11.30 - - - 625M -
ImageBART [21] 21.19 - - - 3.5B -
ImageBART [21] 7.44 - - - 3.5B 0.05 acc. rate
VQGAN+T [23] 17.04 70.6 1.8 - - 1.3B -
VQGAN+T [23] 5.88 304.8 3.6 - - 1.3B 0.05 acc. rate
BigGan-deep [3] 6.95 203.6 2.6 0.87 0.28 340M -
ADM [15] 10.94 100.98 0.69 0.63 554M 250 DDIM steps
ADM-G [15] 4.59 186.7 0.82 0.52 608M 250 DDIM steps
ADM-G,ADM-U [15] 3.85 221.72 0.84 0.53 n/a 2 250 DDIM steps
CDM [31] 4.88 158.71 2.26 - - n/a 2 100 DDIM steps
LDM-8 (ours) 17.41 72.92 2.6 0.65 0.62 395M 200 DDIM steps, 2.9M train steps, batch size 64
LDM-8-G (ours) 8.11 190.43 2.60 0.83 0.36 506M 200 DDIM steps, classiﬁer scale 10, 2.9M train steps, batch size 64
LDM-8 (ours) 15.51 79.03 1.03 0.65 0.63 395M 200 DDIM steps, 4.8M train steps, batch size 64
LDM-8-G (ours) 7.76 209.52 4.24 0.84 0.35 506M 200 DDIM steps, classiﬁer scale 10, 4.8M train steps, batch size 64
LDM-4 (ours) 10.56 103.49 1.24 0.71 0.62 400M 250 DDIM steps, 178K train steps, batch size 1200
LDM-4-G (ours) 3.95 178.22 2.43 0.81 0.55 400M 250 DDIM steps, unconditional guidance [32] scale 1.25, 178K train steps, batch size 1200
LDM-4-G (ours) 3.60 247.67 5.59 0.87 0.48 400M 250 DDIM steps, unconditional guidance [32] scale 1.5, 178K train steps, batch size 1200
Table 10. Comparison of a class-conditional ImageNet LDM with recent state-of-the-art methods for class-conditional image generation
on the ImageNet [12] dataset.: Classiﬁer rejection sampling with the given rejection rate as proposed in [67].
D.5. Sample Quality vs. V100 Days (Continued from Sec. 4.1)
Figure 17. For completeness we also report the training progress of class-conditional LDMs on the ImageNet dataset for a ﬁxed number
of 35 V100 days. Results obtained with 100 DDIM steps [84] and = 0. FIDs computed on 5000 samples for efﬁciency reasons.
For the assessment of sample quality over the training progress in Sec. 4.1, we reported FID and IS scores as a function
of train steps. Another possibility is to report these metrics over the used resources in V100 days. Such an analysis is
additionally provided in Fig. 17, showing qualitatively similar results.
22Method FID# IS PSNR SSIM
Image Regression [72] 15.2 121.1 27.9 0.801
SR3 [72] 5.2 180.1 26.4 0.762
LDM-4 (ours, 100 steps) 2.8y/4.8z166.3 24.4 3.8 0.690.14
LDM-4 (ours, 50 steps, guiding) 4.4y/6.4z153.7 25.8 3.7 0.740.12
LDM-4 (ours, 100 steps, guiding) 4.4y/6.4z154.1 25.7 3.7 0.730.12
LDM-4 (ours, 100 steps, +15 ep.) 2.6y/4.6z169.76 5.03 24.43.8 0.690.14
Pixel-DM (100 steps, +15 ep.) 5.1y/ 7.1z163.06 4.67 24.13.3 0.590.12
Table 11.4upscaling results on ImageNet-Val. ( 2562);y: FID features computed on validation split,z: FID features computed on train
split. We also include a pixel-space baseline that receives the same amount of compute as LDM-4 . The last two rows received 15 epochs
of additional training compared to the former results.
D.6. Super-Resolution
For better comparability between LDMs and diffusion models in pixel space, we extend our analysis from Tab. 5 by
comparing a diffusion model trained for the same number of steps and with a comparable number1of parameters to our
LDM. The results of this comparison are shown in the last two rows of Tab. 11 and demonstrate that LDM achieves better
performance while allowing for signiﬁcantly faster sampling. A qualitative comparison is given in Fig. 20 which shows
random samples from both LDM and the diffusion model in pixel space.
D.6.1 LDM-BSR: General Purpose SR Model via Diverse Image Degradation
bicubic LDM-SR LDM-BSR
Figure 18. LDM-BSR generalizes to arbitrary inputs and can be used as a general-purpose upsampler, upscaling samples from a class-
conditional LDM (image cf. Fig. 4) to 10242resolution. In contrast, using a ﬁxed degradation process (see Sec. 4.4) hinders generalization.
To evaluate generalization of our LDM-SR, we apply it both on synthetic LDM samples from a class-conditional ImageNet
model (Sec. 4.1) and images crawled from the internet. Interestingly, we observe that LDM-SR, trained only with a bicubicly
downsampled conditioning as in [72], does not generalize well to images which do not follow this pre-processing. Hence, to
obtain a superresolution model for a wide range of real world images, which can contain complex superpositions of camera
noise, compression artifacts, blurr and interpolations, we replace the bicubic downsampling operation in LDM-SR with the
degration pipeline from [105]. The BSR-degradation process is a degradation pipline which applies JPEG compressions
noise, camera sensor noise, different image interpolations for downsampling, Gaussian blur kernels and Gaussian noise in a
random order to an image. We found that using the bsr-degredation process with the original parameters as in [105] leads to
a very strong degradation process. Since a more moderate degradation process seemed apppropiate for our application, we
adapted the parameters of the bsr-degradation (our adapted degradation process can be found in our code base at https:
//github.com/CompVis/latent-diffusion ). Fig. 18 illustrates the effectiveness of this approach by directly
comparing LDM-SR with LDM-BSR . The latter produces images much sharper than the models conﬁned to a ﬁxed pre-
processing, making it suitable for real-world applications. Further results of LDM-BSR are shown on LSUN-cows in Fig. 19.
1It is not possible to exactly match both architectures since the diffusion model operates in the pixel space
23E. Implementation Details and Hyperparameters
E.1. Hyperparameters
We provide an overview of the hyperparameters of all trained LDM models in Tab. 12, Tab. 13, Tab. 14 and Tab. 15.
CelebA-HQ 256256 FFHQ 256256 LSUN-Churches 256256 LSUN-Bedrooms 256256
f 4 4 8 4
z-shape 64643 64643 - 64643
jZj 8192 8192 - 8192
Diffusion steps 1000 1000 1000 1000
Noise Schedule linear linear linear linear
Nparams 274M 274M 294M 274M
Channels 224 224 192 224
Depth 2 2 2 2
Channel Multiplier 1,2,3,4 1,2,3,4 1,2,2,4,4 1,2,3,4
Attention resolutions 32, 16, 8 32, 16, 8 32, 16, 8, 4 32, 16, 8
Head Channels 32 32 24 32
Batch Size 48 42 96 48
Iterations410k 635k 500k 1.9M
Learning Rate 9.6e-5 8.4e-5 5.e-5 9.6e-5
Table 12. Hyperparameters for the unconditional LDMs producing the numbers shown in Tab. 1. All models trained on a single NVIDIA
A100.
LDM-1 LDM-2 LDM-4 LDM-8 LDM-16 LDM-32
z-shape 2562563 1281282 64643 32324 16168 88832
jZj - 2048 8192 16384 16384 16384
Diffusion steps 1000 1000 1000 1000 1000 1000
Noise Schedule linear linear linear linear linear linear
Model Size 396M 391M 391M 395M 395M 395M
Channels 192 192 192 256 256 256
Depth 2 2 2 2 2 2
Channel Multiplier 1,1,2,2,4,4 1,2,2,4,4 1,2,3,5 1,2,4 1,2,4 1,2,4
Number of Heads 1 1 1 1 1 1
Batch Size 7 9 40 64 112 112
Iterations 2M 2M 2M 2M 2M 2M
Learning Rate 4.9e-5 6.3e-5 8e-5 6.4e-5 4.5e-5 4.5e-5
Conditioning CA CA CA CA CA CA
CA-resolutions 32, 16, 8 32, 16, 8 32, 16, 8 32, 16, 8 16, 8, 4 8, 4, 2
Embedding Dimension 512 512 512 512 512 512
Transformers Depth 1 1 1 1 1 1
Table 13. Hyperparameters for the conditional LDMs trained on the ImageNet dataset for the analysis in Sec. 4.1. All models trained on a
single NVIDIA A100.
E.2. Implementation Details
E.2.1 Implementations of for conditional LDMs
For the experiments on text-to-image and layout-to-image (Sec. 4.3.1) synthesis, we implement the conditioner as an
unmasked transformer which processes a tokenized version of the input yand produces an output :=(y), where2
RMd. More speciﬁcally, the transformer is implemented from Ntransformer blocks consisting of global self-attention
layers, layer-normalization and position-wise MLPs as follows2:
2adapted from https://github.com/lucidrains/x-transformers
24LDM-1 LDM-2 LDM-4 LDM-8 LDM-16 LDM-32
z-shape 2562563 1281282 64643 32324 16168 88832
jZj - 2048 8192 16384 16384 16384
Diffusion steps 1000 1000 1000 1000 1000 1000
Noise Schedule linear linear linear linear linear linear
Model Size 270M 265M 274M 258M 260M 258M
Channels 192 192 224 256 256 256
Depth 2 2 2 2 2 2
Channel Multiplier 1,1,2,2,4,4 1,2,2,4,4 1,2,3,4 1,2,4 1,2,4 1,2,4
Attention resolutions 32, 16, 8 32, 16, 8 32, 16, 8 32, 16, 8 16, 8, 4 8, 4, 2
Head Channels 32 32 32 32 32 32
Batch Size 9 11 48 96 128 128
Iterations500k 500k 500k 500k 500k 500k
Learning Rate 9e-5 1.1e-4 9.6e-5 9.6e-5 1.3e-4 1.3e-4
Table 14. Hyperparameters for the unconditional LDMs trained on the CelebA dataset for the analysis in Fig. 7. All models trained on a
single NVIDIA A100.: All models are trained for 500k iterations. If converging earlier, we used the best checkpoint for assessing the
provided FID scores.
Task Text-to-Image Layout-to-Image Class-Label-to-Image Super Resolution Inpainting Semantic-Map-to-Image
Dataset LAION OpenImages COCO ImageNet ImageNet Places Landscapes
f 8 4 8 4 4 4 8
z-shape 32324 64643 32324 64643 64643 64643 32324
jZj - 8192 16384 8192 8192 8192 16384
Diffusion steps 1000 1000 1000 1000 1000 1000 1000
Noise Schedule linear linear linear linear linear linear linear
Model Size 1.45B 306M 345M 395M 169M 215M 215M
Channels 320 128 192 192 160 128 128
Depth 2 2 2 2 2 2 2
Channel Multiplier 1,2,4,4 1,2,3,4 1,2,4 1,2,3,5 1,2,2,4 1,4,8 1,4,8
Number of Heads 8 1 1 1 1 1 1
Dropout - - 0.1 - - - -
Batch Size 680 24 48 1200 64 128 48
Iterations 390K 4.4M 170K 178K 860K 360K 360K
Learning Rate 1.0e-4 4.8e-5 4.8e-5 1.0e-4 6.4e-5 1.0e-6 4.8e-5
Conditioning CA CA CA CA concat concat concat
(C)A-resolutions 32, 16, 8 32, 16, 8 32, 16, 8 32, 16, 8 - - -
Embedding Dimension 1280 512 512 512 - - -
Transformer Depth 1 3 2 1 - - -
Table 15. Hyperparameters for the conditional LDMs from Sec. 4. All models trained on a single NVIDIA A100 except for the inpainting
model which was trained on eight V100.
 TokEmb (y) +PosEmb(y) (18)
fori= 1;:::;N :
1 LayerNorm () (19)
2 MultiHeadSelfAttention (1) + (20)
3 LayerNorm (2) (21)
 MLP(3) +2 (22)
 LayerNorm () (23)
(24)
Withavailable, the conditioning is mapped into the UNet via the cross-attention mechanism as depicted in Fig. 3. We
modify the ablated UNet [15] architecture and replace the self-attention layer with a shallow (unmasked) transformer
consisting of Tblocks with alternating layers of (i) self-attention, (ii) a position-wise MLP and (iii) a cross-attention layer;
25see Tab. 16. Note that without (ii) and (iii), this architecture is equivalent to the ablated UNet.
While it would be possible to increase the representational power of by additionally conditioning on the time step t, we
do not pursue this choice as it reduces the speed of inference. We leave a more detailed analysis of this modiﬁcation to future
work.
For the text-to-image model, we rely on a publicly available3tokenizer [99]. The layout-to-image model discretizes the
spatial locations of the bounding boxes and encodes each box as a (l;b;c )-tuple, where ldenotes the (discrete) top-left and b
the bottom-right position. Class information is contained in c.
See Tab. 17 for the hyperparameters of and Tab. 13 for those of the UNet for both of the above tasks.
Note that the class-conditional model as described in Sec. 4.1 is also implemented via cross-attention, where is a single
learnable embedding layer with a dimensionality of 512, mapping classes yto2R1512.
input Rhwc
LayerNorm Rhwc
Conv1x1 Rhwdnh
Reshape Rhwdnh
T8
><
>:SelfAttention
MLP
CrossAttentionRhwdnh
Rhwdnh
Rhwdnh
Reshape Rhwdnh
Conv1x1 Rhwc
Table 16. Architecture of a transformer block as described in Sec. E.2.1, replacing the self-attention layer of the standard ablated UNet
architecture [15]. Here, nhdenotes the number of attention heads and dthe dimensionality per head.
Text-to-Image Layout-to-Image
seq-length 77 92
depthN 32 16
dim 1280 512
Table 17. Hyperparameters for the experiments with transformer encoders in Sec. 4.3.
E.2.2 Inpainting
For our experiments on image-inpainting in Sec. 4.5, we used the code of [88] to generate synthetic masks. We use a ﬁxed
set of 2k validation and 30k testing samples from Places [108]. During training, we use random crops of size 256256
and evaluate on crops of size 512512. This follows the training and testing protocol in [88] and reproduces their reported
metrics (seeyin Tab. 7). We include additional qualitative results of LDM-4, w/ attn in Fig. 21 and of LDM-4, w/o attn, big,
w/ ft in Fig. 22.
E.3. Evaluation Details
This section provides additional details on evaluation for the experiments shown in Sec. 4.
E.3.1 Quantitative Results in Unconditional and Class-Conditional Image Synthesis
We follow common practice and estimate the statistics for calculating the FID-, Precision- and Recall-scores [29,50] shown in
Tab. 1 and 10 based on 50k samples from our models and the entire training set of each of the shown datasets. For calculating
FID scores we use the torch-fidelity package [60]. However, since different data processing pipelines might lead to
different results [64], we also evaluate our models with the script provided by Dhariwal and Nichol [15]. We ﬁnd that results
3https://huggingface.co/transformers/model_doc/bert.html#berttokenizerfast
26mainly coincide, except for the ImageNet and LSUN-Bedrooms datasets, where we notice slightly varying scores of 7.76
(torch-fidelity ) vs. 7.77 (Nichol and Dhariwal) and 2.95 vs 3.0. For the future we emphasize the importance of a
uniﬁed procedure for sample quality assessment. Precision and Recall are also computed by using the script provided by
Nichol and Dhariwal.
E.3.2 Text-to-Image Synthesis
Following the evaluation protocol of [66] we compute FID and Inception Score for the Text-to-Image models from Tab. 2 by
comparing generated samples with 30000 samples from the validation set of the MS-COCO dataset [51]. FID and Inception
Scores are computed with torch-fidelity .
E.3.3 Layout-to-Image Synthesis
For assessing the sample quality of our Layout-to-Image models from Tab. 9 on the COCO dataset, we follow common
practice [37, 87, 89] and compute FID scores the 2048 unaugmented examples of the COCO Segmentation Challenge split.
To obtain better comparability, we use the exact same samples as in [37]. For the OpenImages dataset we similarly follow
their protocol and use 2048 center-cropped test images from the validation set.
E.3.4 Super Resolution
We evaluate the super-resolution models on ImageNet following the pipeline suggested in [72], i.e. images with a shorter
size less than 256px are removed (both for training and evaluation). On ImageNet, the low-resolution images are produced
using bicubic interpolation with anti-aliasing. FIDs are evaluated using torch-fidelity [60], and we produce samples
on the validation split. For FID scores, we additionally compare to reference features computed on the train split, see Tab. 5
and Tab. 11.
E.3.5 Efﬁciency Analysis
For efﬁciency reasons we compute the sample quality metrics plotted in Fig. 6, 17 and 7 based on 5k samples. Therefore,
the results might vary from those shown in Tab. 1 and 10. All models have a comparable number of parameters as provided
in Tab. 13 and 14. We maximize the learning rates of the individual models such that they still train stably. Therefore, the
learning rates slightly vary between different runs cf. Tab. 13 and 14.
E.3.6 User Study
For the results of the user study presented in Tab. 4 we followed the protocoll of [72] and and use the 2-alternative force-choice
paradigm to assess human preference scores for two distinct tasks. In Task-1 subjects were shown a low resolution/masked
image between the corresponding ground truth high resolution/unmasked version and a synthesized image, which was gen-
erated by using the middle image as conditioning. For SuperResolution subjects were asked: Which of the two images is a
better high quality version of the low resolution image in the middle? . For Inpainting we asked Which of the two images
contains more realistic inpainted regions of the image in the middle? . In Task-2, humans were similarly shown the low-
res/masked version and asked for preference between two corresponding images generated by the two competing methods.
As in [72] humans viewed the images for 3 seconds before responding.
27F. Computational Requirements
Method Generator Classiﬁer Overall Inference Nparams FID# IS PrecisionRecall
Compute Compute Compute Throughput
LSUN Churches 2562
StyleGAN2 [42]y64 - 64 - 59M 3.86 - - -
LDM-8 (ours, 100 steps, 410K) 18 - 18 6.80 256M 4.02 - 0.64 0.52
LSUN Bedrooms 2562
ADM [15]y(1000 steps) 232 - 232 0.03 552M 1.9 - 0.66 0.51
LDM-4 (ours, 200 steps, 1.9M) 60 - 55 1.07 274M 2.95 - 0.66 0.48
CelebA-HQ 2562
LDM-4 (ours, 500 steps, 410K) 14.4 - 14.4 0.43 274M 5.11 - 0.72 0.49
FFHQ 2562
StyleGAN2 [42] 32.13z- 32.13y- 59M 3.8 - - -
LDM-4 (ours, 200 steps, 635K) 26 - 26 1.07 274M 4.98 - 0.73 0.50
ImageNet 2562
VQGAN-f-4 (ours, ﬁrst stage) 29 - 29 - 55M 0.58yy- - -
VQGAN-f-8 (ours, ﬁrst stage) 66 - 66 - 68M 1.14yy- - -
BigGAN-deep [3]y128-256 128-256 - 340M 6.95 203.6 2.6 0.87 0.28
ADM [15] (250 steps)y916 - 916 0.12 554M 10.94 100.98 0.69 0.63
ADM-G [15] (25 steps)y916 46 962 0.7 608M 5.58 - 0.81 0.49
ADM-G [15] (250 steps)y916 46 962 0.07 608M 4.59 186.7 0.82 0.52
ADM-G,ADM-U [15] (250 steps)y329 30 349 n/a n/a 3.85 221.72 0.84 0.53
LDM-8-G (ours, 100, 2.9M) 79 12 91 1.93 506M 8.11 190.4 2.6 0.83 0.36
LDM-8 (ours, 200 ddim steps 2.9M, batch size 64) 79 - 79 1.9 395M 17.41 72.92 0.65 0.62
LDM-4 (ours, 250 ddim steps 178K, batch size 1200) 271 - 271 0.7 400M 10.56 103.49 1.24 0.71 0.62
LDM-4-G (ours, 250 ddim steps 178K, batch size 1200, classiﬁer-free guidance [32] scale 1.25) 271 - 271 0.4 400M 3.95 178.22 2.43 0.81 0.55
LDM-4-G (ours, 250 ddim steps 178K, batch size 1200, classiﬁer-free guidance [32] scale 1.5) 271 - 271 0.4 400M 3.60 247.67 5.59 0.87 0.48
Table 18. Comparing compute requirements during training and inference throughput with state-of-the-art generative models. Compute
during training in V100-days, numbers of competing methods taken from [15] unless stated differently;: Throughput measured in sam-
ples/sec on a single NVIDIA A100;y: Numbers taken from [15] ;z: Assumed to be trained on 25M train examples;yy: R-FID vs. ImageNet
validation set
In Tab 18 we provide a more detailed analysis on our used compute ressources and compare our best performing models
on the CelebA-HQ, FFHQ, LSUN and ImageNet datasets with the recent state of the art models by using their provided
numbers, cf. [15]. As they report their used compute in V100 days and we train all our models on a single NVIDIA A100
GPU, we convert the A100 days to V100 days by assuming a 2:2speedup of A100 vs V100 [74]4. To assess sample quality,
we additionally report FID scores on the reported datasets. We closely reach the performance of state of the art methods as
StyleGAN2 [42] and ADM [15] while signiﬁcantly reducing the required compute resources.
4This factor corresponds to the speedup of the A100 over the V100 for a U-Net, as deﬁned in Fig. 1 in [74]
28G. Details on Autoencoder Models
We train all our autoencoder models in an adversarial manner following [23], such that a patch-based discriminator D 
is optimized to differentiate original images from reconstructions D(E(x)). To avoid arbitrarily scaled latent spaces, we
regularize the latent zto be zero centered and obtain small variance by introducing an regularizing loss term Lreg.
We investigate two different regularization methods: (i) a low-weighted Kullback-Leibler-term between qE(zjx) =
N(z;E;E2)and a standard normal distribution N(z; 0;1)as in a standard variational autoencoder [46, 69], and, (ii) regu-
larizing the latent space with a vector quantization layer by learning a codebook of jZjdifferent exemplars [96].
To obtain high-ﬁdelity reconstructions we only use a very small regularization for both scenarios, i.e. we either weight the
KLterm by a factor106or choose a high codebook dimensionality jZj.
The full objective to train the autoencoding model (E;D)reads:
LAutoencoder = min
E;Dmax
 
Lrec(x;D(E(x)))Ladv(D(E(x))) + logD (x) +Lreg(x;E;D)
(25)
DM Training in Latent Space Note that for training diffusion models on the learned latent space, we again distinguish two
cases when learning p(z)orp(zjy)(Sec. 4.3): (i) For a KL-regularized latent space, we sample z=E(x)+E(x)=:E(x),
whereN(0;1). When rescaling the latent, we estimate the component-wise variance
^2=1
bchwX
b;c;h;w(zb;c;h;w^)2
from the ﬁrst batch in the data, where ^=1
bchwP
b;c;h;wzb;c;h;w. The output ofEis scaled such that the rescaled latent has
unit standard deviation, i.e.z z
^=E(x)
^. (ii) For a VQ-regularized latent space, we extract zbefore the quantization layer
and absorb the quantization operation into the decoder, i.e. it can be interpreted as the ﬁrst layer of D.
H. Additional Qualitative Results
Finally, we provide additional qualitative results for our landscapes model (Fig. 12, 23, 24 and 25), our class-conditional
ImageNet model (Fig. 26 - 27) and our unconditional models for the CelebA-HQ, FFHQ and LSUN datasets (Fig. 28 - 31).
Similar as for the inpainting model in Sec. 4.5 we also ﬁne-tuned the semantic landscapes model from Sec. 4.3.2 directly on
5122images and depict qualitative results in Fig. 12 and Fig. 23. For our those models trained on comparably small datasets,
we additionally show nearest neighbors in VGG [79] feature space for samples from our models in Fig. 32 - 34.
29bicubic LDM-BSR
Figure 19. LDM-BSR generalizes to arbitrary inputs and can be used as a general-purpose upsampler, upscaling samples from the LSUN-
Cows dataset to 10242resolution.
30input GT Pixel Baseline #1 Pixel Baseline #2 LDM #1 LDM #2
Figure 20. Qualitative superresolution comparison of two random samples between LDM-SR and baseline-diffusionmodel in Pixelspace.
Evaluated on imagenet validation-set after same amount of training steps.
31input GT LaMa [88] LDM #1 LDM #2 LDM #3
Figure 21. Qualitative results on image inpainting. In contrast to [88], our generative approach enables generation of multiple diverse
samples for a given input.
32input result input result
Figure 22. More qualitative results on object removal as in Fig. 11.
33Semantic Synthesis on Flickr-Landscapes [23] ( 5122ﬁnetuning)
Figure 23. Convolutional samples from the semantic landscapes model as in Sec. 4.3.2, ﬁnetuned on 5122images.
34Figure 24. A LDM trained on 2562resolution can generalize to larger resolution for spatially conditioned tasks such as semantic synthesis
of landscape images. See Sec. 4.3.2.
35Semantic Synthesis on Flickr-Landscapes [23]
Figure 25. When provided a semantic map as conditioning, our LDMs generalize to substantially larger resolutions than those seen during
training. Although this model was trained on inputs of size 2562it can be used to create high-resolution samples as the ones shown here,
which are of resolution 1024384. 36Random class conditional samples on the ImageNet dataset
Figure 26. Random samples from LDM-4 trained on the ImageNet dataset. Sampled with classiﬁer-free guidance [32] scale s= 5:0and
200 DDIM steps with = 1:0.
37Random class conditional samples on the ImageNet dataset
Figure 27. Random samples from LDM-4 trained on the ImageNet dataset. Sampled with classiﬁer-free guidance [32] scale s= 3:0and
200 DDIM steps with = 1:0.
38Random samples on the CelebA-HQ dataset
Figure 28. Random samples of our best performing model LDM-4 on the CelebA-HQ dataset. Sampled with 500 DDIM steps and = 0
(FID = 5.15).
39Random samples on the FFHQ dataset
Figure 29. Random samples of our best performing model LDM-4 on the FFHQ dataset. Sampled with 200 DDIM steps and = 1 (FID
= 4.98).
40Random samples on the LSUN-Churches dataset
Figure 30. Random samples of our best performing model LDM-8 on the LSUN-Churches dataset. Sampled with 200 DDIM steps and
= 0(FID = 4.48).
41Random samples on the LSUN-Bedrooms dataset
Figure 31. Random samples of our best performing model LDM-4 on the LSUN-Bedrooms dataset. Sampled with 200 DDIM steps and
= 1(FID = 2.95).
42Nearest Neighbors on the CelebA-HQ dataset
Figure 32. Nearest neighbors of our best CelebA-HQ model, computed in the feature space of a VGG-16 [79]. The leftmost sample is
from our model. The remaining samples in each row are its 10 nearest neighbors.
43Nearest Neighbors on the FFHQ dataset
Figure 33. Nearest neighbors of our best FFHQ model, computed in the feature space of a VGG-16 [79]. The leftmost sample is from our
model. The remaining samples in each row are its 10 nearest neighbors.
44Nearest Neighbors on the LSUN-Churches dataset
Figure 34. Nearest neighbors of our best LSUN-Churches model, computed in the feature space of a VGG-16 [79]. The leftmost sample
is from our model. The remaining samples in each row are its 10 nearest neighbors.
45
  Segment Anything
Alexander Kirillov1;2;4Eric Mintun2Nikhila Ravi1;2Hanzi Mao2Chloe Rolland3Laura Gustafson3
Tete Xiao3Spencer Whitehead Alexander C. Berg Wan-Yen Lo Piotr Doll ar4Ross Girshick4
1project lead2joint ﬁrst author3equal contribution4directional lead
Meta AI Research, FAIR
(b) Model: Segment Anything Model (SAM)promptimagevalid maskimage encoderprompt encoderlightweight mask decoder
(a) Task: promptable segmentationsegmentation promptimagemodelcat withblack earsvalid mask
(c) Data: data engine (top) & dataset (bottom)1+ billion masks11 million images privacy respectinglicensed imagesannotatetraindatamodelSegment Anything 1B (SA-1B):
Figure 1: We aim to build a foundation model for segmentation by introducing three interconnected components: a prompt-
able segmentation task, a segmentation model (SAM) that powers data annotation and enables zero-shot transfer to a range
of tasks via prompt engineering, and a data engine for collecting SA-1B, our dataset of over 1 billion masks.
Abstract
We introduce the Segment Anything (SA) project: a new
task, model, and dataset for image segmentation. Using our
efﬁcient model in a data collection loop, we built the largest
segmentation dataset to date (by far), with over 1 billion
masks on 11M licensed and privacy respecting images. The
model is designed and trained to be promptable, so it can
transfer zero-shot to new image distributions and tasks. We
evaluate its capabilities on numerous tasks and ﬁnd that
its zero-shot performance is impressive  often competitive
with or even superior to prior fully supervised results. We
are releasing the Segment Anything Model (SAM) and cor-
responding dataset (SA-1B) of 1B masks and 11M images at
https://segment-anything.com to foster research into foun-
dation models for computer vision.
1. Introduction
Large language models pre-trained on web-scale datasets
are revolutionizing NLP with strong zero-shot and few-shot
generalization [10]. These foundation models [8] can
generalize to tasks and data distributions beyond those seen
during training. This capability is often implemented with
prompt engineering in which hand-crafted text is used to
prompt the language model to generate a valid textual re-
sponse for the task at hand. When scaled and trained with
abundant text corpora from the web, these models zero and
few-shot performance compares surprisingly well to (evenmatching in some cases) ﬁne-tuned models [10, 21]. Empir-
ical trends show this behavior improving with model scale,
dataset size, and total training compute [56, 10, 21, 51].
Foundation models have also been explored in computer
vision, albeit to a lesser extent. Perhaps the most promi-
nent illustration aligns paired text and images from the web.
For example, CLIP [82] and ALIGN [55] use contrastive
learning to train text and image encoders that align the two
modalities. Once trained, engineered text prompts enable
zero-shot generalization to novel visual concepts and data
distributions. Such encoders also compose effectively with
other modules to enable downstream tasks, such as image
generation ( e.g., DALLE [83]). While much progress has
been made on vision and language encoders, computer vi-
sion includes a wide range of problems beyond this scope,
and for many of these, abundant training data does not exist.
In this work, our goal is to build a foundation model for
image segmentation . That is, we seek to develop a prompt-
able model and pre-train it on a broad dataset using a task
that enables powerful generalization. With this model, we
aim to solve a range of downstream segmentation problems
on new data distributions using prompt engineering.
The success of this plan hinges on three components:
task,model , and data . To develop them, we address the
following questions about image segmentation:
1. What task will enable zero-shot generalization?
2. What is the corresponding model architecture?
3. What data can power this task and model?
1arXiv:2304.02643v1  [cs.CV]  5 Apr 2023These questions are entangled and require a comprehen-
sive solution. We start by deﬁning a promptable segmenta-
tiontask that is general enough to provide a powerful pre-
training objective and to enable a wide range of downstream
applications. This task requires a model that supports ﬂex-
ible prompting and can output segmentation masks in real-
time when prompted to allow for interactive use. To train
our model, we need a diverse, large-scale source of data .
Unfortunately, there is no web-scale data source for seg-
mentation; to address this, we build a data engine, i.e.,
we iterate between using our efﬁcient model to assist in data
collection and using the newly collected data to improve the
model. We introduce each interconnected component next,
followed by the dataset we created and the experiments that
demonstrate the effectiveness of our approach.
Task (2). In NLP and more recently computer vision,
foundation models are a promising development that can
perform zero-shot and few-shot learning for new datasets
and tasks often by using prompting techniques. Inspired
by this line of work, we propose the promptable segmen-
tation task , where the goal is to return a valid segmenta-
tion mask given any segmentation prompt (see Fig. 1a). A
prompt simply speciﬁes what to segment in an image, e.g.,
a prompt can include spatial or text information identifying
an object. The requirement of a valid output mask means
that even when a prompt is ambiguous and could refer to
multiple objects (for example, a point on a shirt may in-
dicate either the shirt or the person wearing it), the output
should be a reasonable mask for at least one of those ob-
jects. We use the promptable segmentation task as both a
pre-training objective and to solve general downstream seg-
mentation tasks via prompt engineering.
Model (3). The promptable segmentation task and the goal
of real-world use impose constraints on the model architec-
ture. In particular, the model must support ﬂexible prompts ,
needs to compute masks in amortized real-time to allow in-
teractive use, and must be ambiguity-aware . Surprisingly,
we ﬁnd that a simple design satisﬁes all three constraints:
a powerful image encoder computes an image embedding,
a prompt encoder embeds prompts, and then the two infor-
mation sources are combined in a lightweight mask decoder
that predicts segmentation masks. We refer to this model as
the Segment Anything Model, or SAM (see Fig. 1b). By
separating SAM into an image encoder and a fast prompt
encoder / mask decoder, the same image embedding can
be reused (and its cost amortized) with different prompts.
Given an image embedding, the prompt encoder and mask
decoder predict a mask from a prompt in 50ms in a web
browser. We focus on point, box, and mask prompts, and
also present initial results with free-form text prompts. To
make SAM ambiguity-aware, we design it to predict mul-
tiple masks for a single prompt allowing SAM to naturally
handle ambiguity, such as the shirt vs. person example.Data engine (4). To achieve strong generalization to new
data distributions, we found it necessary to train SAM on
a large and diverse set of masks, beyond any segmenta-
tion dataset that already exists. While a typical approach
for foundation models is to obtain data online [82], masks
are not naturally abundant and thus we need an alternative
strategy. Our solution is to build a data engine, i.e., we
co-develop our model with model-in-the-loop dataset an-
notation (see Fig. 1c). Our data engine has three stages:
assisted-manual ,semi-automatic , and fully automatic . In
the ﬁrst stage, SAM assists annotators in annotating masks,
similar to a classic interactive segmentation setup. In the
second stage, SAM can automatically generate masks for
a subset of objects by prompting it with likely object lo-
cations and annotators focus on annotating the remaining
objects, helping increase mask diversity. In the ﬁnal stage,
we prompt SAM with a regular grid of foreground points,
yielding on average 100 high-quality masks per image.
Dataset (5). Our ﬁnal dataset, SA-1B, includes more than
1Bmasks from 11M licensed and privacy-preserving im-
ages (see Fig. 2). SA-1B, collected fully automatically us-
ing the ﬁnal stage of our data engine, has 400 more masks
than any existing segmentation dataset [66, 44, 117, 60],
and as we verify extensively, the masks are of high quality
and diversity. Beyond its use in training SAM to be robust
and general, we hope SA-1B becomes a valuable resource
for research aiming to build new foundation models.
Responsible AI (6). We study and report on potential fair-
ness concerns and biases when using SA-1B and SAM. Im-
ages in SA-1B span a geographically and economically di-
verse set of countries and we found that SAM performs sim-
ilarly across different groups of people. Together, we hope
this will make our work more equitable for real-world use
cases. We provide model and dataset cards in the appendix.
Experiments (7). We extensively evaluate SAM. First, us-
ing a diverse new suite of 23 segmentation datasets, we ﬁnd
that SAM produces high-quality masks from a single fore-
ground point, often only slightly below that of the manu-
ally annotated ground truth. Second, we ﬁnd consistently
strong quantitative and qualitative results on a variety of
downstream tasks under a zero-shot transfer protocol using
prompt engineering, including edge detection, object pro-
posal generation, instance segmentation, and a preliminary
exploration of text-to-mask prediction. These results sug-
gest that SAM can be used out-of-the-box with prompt en-
gineering to solve a variety of tasks involving object and
image distributions beyond SAMs training data. Neverthe-
less, room for improvement remains, as we discuss in 8.
Release. We are releasing the SA-1B dataset for research
purposes and making SAM available under a permissive
open license (Apache 2.0) at https://segment-anything.com.
We also showcase SAMs capabilities with an online demo.
2<50 masks
 50-100 masks
 100-200 masks
 200-300 masks
 300-400 masks
 400-500 masks
 >500 masks
Figure 2: Example images with overlaid masks from our newly introduced dataset, SA-1B . SA-1B contains 11M diverse,
high-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks. These masks were
annotated fully automatically by SAM, and as we verify by human ratings and numerous experiments, are of high quality and
diversity. We group images by number of masks per image for visualization (there are 100 masks per image on average).
32. Segment Anything Task
We take inspiration from NLP, where the next token pre-
diction task is used for foundation model pre-training and
to solve diverse downstream tasks via prompt engineer-
ing [10]. To build a foundation model for segmentation,
we aim to deﬁne a task with analogous capabilities.
Task. We start by translating the idea of a prompt from NLP
to segmentation, where a prompt can be a set of foreground
/ background points, a rough box or mask, free-form text,
or, in general, any information indicating what to segment
in an image. The promptable segmentation task , then, is to
return a valid segmentation mask given any prompt . The re-
quirement of a valid mask simply means that even when
a prompt is ambiguous and could refer to multiple objects
(e.g., recall the shirt vs. person example, and see Fig. 3),
the output should be a reasonable mask for at least oneof
those objects. This requirement is similar to expecting a lan-
guage model to output a coherent response to an ambiguous
prompt. We choose this task because it leads to a natural
pre-training algorithm anda general method for zero-shot
transfer to downstream segmentation tasks via prompting.
Pre-training. The promptable segmentation task suggests a
natural pre-training algorithm that simulates a sequence of
prompts ( e.g., points, boxes, masks) for each training sam-
ple and compares the models mask predictions against the
ground truth. We adapt this method from interactive seg-
mentation [109, 70], although unlike interactive segmenta-
tion whose aim is to eventually predict a valid mask after
enough user input, our aim is to always predict a valid mask
forany prompt even when the prompt is ambiguous . This
ensures that a pre-trained model is effective in use cases that
involve ambiguity, including automatic annotation as re-
quired by our data engine 4. We note that performing well
at this task is challenging and requires specialized modeling
and training loss choices, which we discuss in 3.
Zero-shot transfer. Intuitively, our pre-training task en-
dows the model with the ability to respond appropriately to
any prompt at inference time, and thus downstream tasks
can be solved by engineering appropriate prompts. For ex-
ample, if one has a bounding box detector for cats, cat in-
stance segmentation can be solved by providing the detec-
tors box output as a prompt to our model. In general, a wide
array of practical segmentation tasks can be cast as prompt-
ing. In addition to automatic dataset labeling, we explore
ﬁve diverse example tasks in our experiments in 7.
Related tasks. Segmentation is a broad ﬁeld: theres in-
teractive segmentation [57, 109], edge detection [3], su-
per pixelization [85], object proposal generation [2], fore-
ground segmentation [94], semantic segmentation [90], in-
stance segmentation [66], panoptic segmentation [59], etc.
The goal of our promptable segmentation task is to produce
Figure 3: Each column shows 3 valid masks generated by
SAM from a single ambiguous point prompt (green circle).
a broadly capable model that can adapt to many (though
not all) existing and new segmentation tasks via prompt
engineering. This capability is a form of task generaliza-
tion [26]. Note that this is different than previous work on
multi-task segmentation systems. In a multi-task system, a
single model performs a ﬁxed set of tasks, e.g., joint seman-
tic, instance, and panoptic segmentation [114, 19, 54], but
the training and test tasks are the same. An important dis-
tinction in our work is that a model trained for promptable
segmentation can perform a new, different task at inference
time by acting as a component in a larger system, e.g., to
perform instance segmentation, a promptable segmentation
model is combined with an existing object detector.
Discussion. Prompting and composition are powerful tools
that enable a single model to be used in extensible ways, po-
tentially to accomplish tasks unknown at the time of model
design. This approach is analogous to how other founda-
tion models are used, e.g., how CLIP [82] is the text-image
alignment component of the DALL E [83] image generation
system. We anticipate that composable system design, pow-
ered by techniques such as prompt engineering, will enable
a wider variety of applications than systems trained specif-
ically for a ﬁxed set of tasks. Its also interesting to com-
pare promptable and interactive segmentation through the
lens of composition: while interactive segmentation mod-
els are designed with human users in mind, a model trained
for promptable segmentation can also be composed into a
larger algorithmic system as we will demonstrate.
4,score
score
score,,
valid masksimage
image
encoder
image
embeddingmask points box textprompt encodermask decoder
convFigure 4: Segment Anything Model (SAM) overview. A heavyweight image encoder outputs an image embedding that can
then be efﬁciently queried by a variety of input prompts to produce object masks at amortized real-time speed. For ambiguous
prompts corresponding to more than one object, SAM can output multiple valid masks and associated conﬁdence scores.
3. Segment Anything Model
We next describe the Segment Anything Model (SAM)
for promptable segmentation. SAM has three components,
illustrated in Fig. 4: an image encoder, a ﬂexible prompt
encoder, and a fast mask decoder. We build on Transformer
vision models [14, 33, 20, 62] with speciﬁc tradeoffs for
(amortized) real-time performance. We describe these com-
ponents at a high-level here, with details in A.
Image encoder. Motivated by scalability and powerful pre-
training methods, we use an MAE [47] pre-trained Vision
Transformer (ViT) [33] minimally adapted to process high
resolution inputs [62]. The image encoder runs once per
image and can be applied prior to prompting the model.
Prompt encoder. We consider two sets of prompts: sparse
(points, boxes, text) and dense (masks). We represent
points and boxes by positional encodings [95] summed with
learned embeddings for each prompt type and free-form text
with an off-the-shelf text encoder from CLIP [82]. Dense
prompts ( i.e., masks) are embedded using convolutions and
summed element-wise with the image embedding.
Mask decoder. The mask decoder efﬁciently maps the im-
age embedding, prompt embeddings, and an output token
to a mask. This design, inspired by [14, 20], employs a
modiﬁcation of a Transformer decoder block [103] followed
by a dynamic mask prediction head. Our modiﬁed decoder
block uses prompt self-attention and cross-attention in two
directions (prompt-to-image embedding and vice-versa) to
update allembeddings. After running two blocks, we up-
sample the image embedding and an MLP maps the output
token to a dynamic linear classiﬁer, which then computes
the mask foreground probability at each image location.
Resolving ambiguity. With one output, the model will av-
erage multiple valid masks if given an ambiguous prompt.
To address this, we modify the model to predict multiple
output masks for a single prompt (see Fig. 3). We found
3 mask outputs is sufﬁcient to address most common cases
(nested masks are often at most three deep: whole, part, and
subpart). During training, we backprop only the minimumloss [15, 45, 64] over masks. To rank masks, the model pre-
dicts a conﬁdence score ( i.e., estimated IoU) for each mask.
Efﬁciency. The overall model design is largely motivated
by efﬁciency. Given a precomputed image embedding, the
prompt encoder and mask decoder run in a web browser, on
CPU, in 50ms. This runtime performance enables seam-
less, real-time interactive prompting of our model.
Losses and training. We supervise mask prediction with
the linear combination of focal loss [65] and dice loss [73]
used in [14]. We train for the promptable segmentation task
using a mixture of geometric prompts (for text prompts see
7.5). Following [92, 37], we simulate an interactive setup
by randomly sampling prompts in 11 rounds per mask, al-
lowing SAM to integrate seamlessly into our data engine.
4. Segment Anything Data Engine
As segmentation masks are not abundant on the inter-
net, we built a data engine to enable the collection of our
1.1B mask dataset, SA-1B. The data engine has three
stages: (1) a model-assisted manual annotation stage, (2) a
semi-automatic stage with a mix of automatically predicted
masks and model-assisted annotation, and (3) a fully auto-
matic stage in which our model generates masks without
annotator input. We go into details of each next.
Assisted-manual stage. In the ﬁrst stage, resembling clas-
sic interactive segmentation, a team of professional annota-
tors labeled masks by clicking foreground / background ob-
ject points using a browser-based interactive segmentation
tool powered by SAM. Masks could be reﬁned using pixel-
precise brush and eraser tools. Our model-assisted an-
notation runs in real-time directly inside a browser (using
precomputed image embeddings) enabling a truly interac-
tive experience. We did not impose semantic constraints for
labeling objects, and annotators freely labeled both stuff
and things [1]. We suggested annotators label objects
they could name or describe, but did not collect these names
or descriptions. Annotators were asked to label objects in
order of prominence and were encouraged to proceed to the
next image once a mask took over 30 seconds to annotate.
5At the start of this stage, SAM was trained using com-
mon public segmentation datasets. After sufﬁcient data an-
notation, SAM was retrained using only newly annotated
masks. As more masks were collected, the image encoder
was scaled from ViT-B to ViT-H and other architectural de-
tails evolved; in total we retrained our model 6 times. Av-
erage annotation time per mask decreased from 34 to 14
seconds as the model improved. We note that 14 seconds
is 6.5faster than mask annotation for COCO [66] and
only 2slower than bounding-box labeling with extreme
points [76, 71]. As SAM improved, the average number of
masks per image increased from 20 to 44 masks. Overall,
we collected 4.3M masks from 120k images in this stage.
Semi-automatic stage. In this stage, we aimed to increase
thediversity of masks in order to improve our models
ability to segment anything. To focus annotators on less
prominent objects, we ﬁrst automatically detected conﬁdent
masks. Then we presented annotators with images preﬁlled
with these masks and asked them to annotate any additional
unannotated objects. To detect conﬁdent masks, we trained
a bounding box detector [84] on all ﬁrst stage masks using a
generic object category. During this stage we collected an
additional 5.9M masks in 180k images (for a total of 10.2M
masks). As in the ﬁrst stage, we periodically retrained our
model on newly collected data (5 times). Average annota-
tion time per mask went back up to 34 seconds (excluding
the automatic masks) as these objects were more challeng-
ing to label. The average number of masks per image went
from 44 to 72 masks (including the automatic masks).
Fully automatic stage. In the ﬁnal stage, annotation was
fully automatic . This was feasible due to two major en-
hancements to our model. First, at the start of this stage, we
had collected enough masks to greatly improve the model,
including the diverse masks from the previous stage. Sec-
ond, by this stage we had developed the ambiguity-aware
model, which allowed us to predict valid masks even in am-
biguous cases. Speciﬁcally, we prompted the model with a
3232 regular grid of points and for each point predicted
a set of masks that may correspond to valid objects. With
the ambiguity-aware model, if a point lies on a part or sub-
part, our model will return the subpart, part, and whole ob-
ject. The IoU prediction module of our model is used to se-
lectconﬁdent masks; moreover, we identiﬁed and selected
only stable masks (we consider a mask stable if threshold-
ing the probability map at 0:5and0:5 +results in
similar masks). Finally, after selecting the conﬁdent and
stable masks, we applied non-maximal suppression (NMS)
to ﬁlter duplicates. To further improve the quality of smaller
masks, we also processed multiple overlapping zoomed-in
image crops. For further details of this stage, see B. We
applied fully automatic mask generation to all 11M images
in our dataset, producing a total of 1.1B high-quality masks.
We describe and analyze the resulting dataset, SA-1B, next.
Figure 5: Image-size normalized mask center distributions.
5. Segment Anything Dataset
Our dataset, SA-1B, consists of 11M diverse, high-
resolution, licensed, and privacy protecting images and
1.1B high-quality segmentation masks collected with our
data engine. We compare SA-1B with existing datasets
and analyze mask quality and properties. We are releasing
SA-1B to aid future development of foundation models for
computer vision. We note that SA-1B will be released un-
der a favorable license agreement for certain research uses
and with protections for researchers.
Images . We licensed a new set of 11M images from a
provider that works directly with photographers. These im-
ages are high resolution (3300 4950 pixels on average),
and the resulting data size can present accessibility and stor-
age challenges. Therefore, we are releasing downsampled
images with their shortest side set to 1500 pixels. Even af-
ter downsampling, our images are signiﬁcantly higher reso-
lution than many existing vision datasets ( e.g., COCO [66]
images are 480640 pixels). Note that most models today
operate on much lower resolution inputs. Faces and vehicle
license plates have been blurred in the released images.
Masks . Our data engine produced 1.1B masks, 99.1% of
which were generated fully automatically. Therefore, the
quality of the automatic masks is centrally important. We
compare them directly to professional annotations and look
at how various mask properties compare to prominent seg-
mentation datasets. Our main conclusion, as borne out in
the analysis below and the experiments in 7, is that our
automatic masks are high quality and effective for training
models. Motivated by these ﬁndings, SA-1B only includes
automatically generated masks.
Mask quality. To estimate mask quality, we randomly sam-
pled 500 images ( 50k masks) and asked our professional
annotators to improve the quality of all masks in these im-
ages. Annotators did so using our model and pixel-precise
brush and eraser editing tools. This procedure resulted
in pairs of automatically predicted and professionally cor-
rected masks. We computed IoU between each pair and
found that 94% of pairs have greater than 90% IoU (and
97% of pairs have greater than 75% IoU). For comparison,
prior work estimates inter-annotator consistency at 85-91%
IoU [44, 60]. Our experiments in 7 conﬁrm by human rat-
ings that mask quality is high relative to a variety of datasets
and that training our model on automatic masks is nearly as
good as using all masks produced by the data engine.
6SA-1B
11M images
1129M (1.1B) masksLVIS v1
0.120M images
1.5M masksCOCO
0.123M images
0.9M masksADE20K
0.028M images
0.7M masksOpen Images
1M images
2.7M masks
<10 11-50 51-100 101-200 >200
Number of masks per image04080Percent of images
0.00 0.25 0.50 0.75
Relative segmentation mask size100
102Percent of masks
0.0 0.2 0.4 0.6 0.8
Concavity051015Percent of masksFigure 6: Dataset mask properties. The legend references the number of images and masks in each dataset. Note, that SA-1B
has 11more images and 400 more masks than the largest existing segmentation dataset Open Images [60].
Per country
image count
 100k
< 100k
< 10k
< 1k
RUS
THA
USA
ITA
GBR
DEU
ESP
IDN
UKR
FRA
JPN
MYS
TUR
IND
CHN
POL
NLD
VNM
BRA
CAN
GRC
AUS
PRT
CZE
BLR
ROU
KOR
ARE
AUT
SWE
TWN
HKG
CHE
ISR
SGP
HUN
BEL
HRV
BGR
PHL
KAZ
MEX
NOR
MMR
ZAF
SRB
DNK
MAR
FIN
LVA
50 most common countries (ISO codes)0200k400k600k800kNumber of images per countryAsia & Oceania
Africa
Europe
North America
Latin America & Caribbean
Figure 7: Estimated geographic distribution of SA-1B images. Most of the worlds countries have more than 1000 images in
SA-1B, and the three countries with the most images are from different parts of the world.
Mask properties. In Fig. 5 we plot the spatial distribution
of object centers in SA-1B compared to the largest existing
segmentation datasets. Common photographer biases are
present in all datasets. We observe that SA-1B has greater
coverage of image corners compared to LVIS v1 [44] and
ADE20K [117], the two most similarly distributed datasets,
while COCO [66] and Open Images V5 [60] have a more
prominent center bias. In Fig. 6 (legend) we compare these
datasets by size. SA-1B has 11 more images and 400 
more masks than the second largest, Open Images. On av-
erage, it has 36more masks per image than Open Images.
The closest dataset in this respect, ADE20K, still has 3.5 
fewer masks per image. Fig. 6 (left) plots the masks-per-
image distribution. Next, we look at image-relative mask
size (square root of the mask area divided by image area)
in Fig. 6 (middle). As expected, since our dataset has more
masks per image, it also tends to include a greater percent-
age of small and medium relative-size masks. Finally, to
analyze shape complexity, we look at mask concavity (1
minus mask area divided by area of masks convex hull) in
Fig. 6 (right). Since shape complexity is correlated with
mask size, we control for the datasets mask size distribu-
tions by ﬁrst performing stratiﬁed sampling from binned
mask sizes. We observe that the concavity distribution of
our masks is broadly similar to that of other datasets.
6. Segment Anything RAI Analysis
We next perform a Responsible AI (RAI) analysis of our
work by investigating potential fairness concerns and bi-
ases when using SA-1B and SAM. We focus on the geo-
graphic and income distribution of SA-1B and fairness of
SAM across protected attributes of people. We also provide
dataset, data annotation, and model cards in F.SA-1B % images
# countries #imgs #masks SA-1B COCO O.I.
Africa 54 300k 28M 2.8% 3.0% 1.7%
Asia & Oceania 70 3.9M 423M 36.2% 11.4% 14.3%
Europe 47 5.4M 540M 49.8% 34.2% 36.2%
Latin America & Carib. 42 380k 36M 3.5% 3.1% 5.0%
North America 4830k 80M 7.7% 48.3% 42.8%
high income countries 81 5.8M 598M 54.0% 89.1% 87.5%
middle income countries 108 4.9M 499M 45.0% 10.5% 12.0%
low income countries 28 100k 9.4M 0.9% 0.4% 0.5%
Table 1: Comparison of geographic and income representa-
tion. SA-1B has higher representation in Europe and Asia &
Oceania as well as middle income countries. Images from
Africa, Latin America & Caribbean, as well as low income
countries, are underrepresented in all datasets.
Geographic and income representation. We infer the
country images were photographed in using standard meth-
ods (see C). In Fig. 7 we visualize the per-country image
counts in SA-1B (left) and the 50 countries with the most
images (right). We note that the top-three countries are
from different parts of the world. Next, in Table 1 we com-
pare the geographic and income representation of SA-1B,
COCO [66], and Open Images [60]. SA-1B has a substan-
tially higher percentage of images in Europe and Asia &
Oceania as well as in middle income countries. All datasets
underrepresent Africa as well as low income countries. We
note that in SA-1B, all regions, including Africa, have at
least 28 million masks, 10 more than the total number of
masks of any previous dataset. Finally, we observe that the
average number of masks per image (not shown) is fairly
consistent across region and income (94-108 per image).
7mIoU at
1 point 3 points
perceived gender presentation
feminine 54.4 1.7 90.40.6
masculine 55.7 1.7 90.10.6
perceived age group
older 62.9 6.7 92.61.3
middle 54.5 1.3 90.20.5
young 54.2 2.2 91.20.7mIoU at
1 point 3 points
perceived skin tone
1 52.9 2.2 91.00.9
2 51.5 1.4 91.10.5
3 52.2 1.9 91.40.7
4 51.5 2.7 91.71.0
5 52.4 4.2 92.51.4
6 56.7 6.3 91.22.4
Table 2: SAMs performance segmenting people across per-
ceived gender presentation, age group, and skin tone. 95%
conﬁdence intervals are shown. Within each grouping, all
conﬁdence intervals overlap except older vs. middle.
Fairness in segmenting people. We investigate potential
fairness concerns across perceived gender presentation, per-
ceived age group, and perceived skin tone by measuring
the performance discrepancy of SAM between groups. We
use the More Inclusive Annotations for People (MIAP) [87]
dataset for gender presentation and age and a proprietary
dataset for skin tone (see C). Our evaluation uses simu-
lated interactive segmentation with random sampling of 1
and 3 points (see D). Table 2 (top left) shows results for
perceived gender presentation. We note that females have
been shown to be underrepresented in detection and seg-
mentation datasets [115], but observe that SAM performs
similarly across groups. We repeat the analysis for per-
ceived age in Table 2 (bottom left), noting that those who
are perceived to be younger and older have been shown to
be underrepresented in large-scale datasets [110]. SAM per-
forms best on those who are perceived older (although the
conﬁdence interval is large). Finally, we repeat the anal-
ysis for perceived skin tone in Table 2 (right), noting that
those with lighter apparent skin tones have been shown to
be overrepresented and those with darker skin tones under-
represented in large-scale datasets [110]. As MIAP does
not contain perceived skin tone annotations, we use a pro-
prietary dataset that contains annotations for the perceived
Fitzpatrick skin type [36], which ranges from 1 (lightest
skin tone) to 6 (darkest skin tone). While the means vary
somewhat, we do not ﬁnd a signiﬁcant difference across
groups. We believe our ﬁndings stem from the nature of
the task, and acknowledge biases may arise when SAM is
used as a component in larger systems. Finally, in C we
extend the analysis to segmenting clothing where we ﬁnd
an indication of bias across perceived gender presentation.
7. Zero-Shot Transfer Experiments
In this section, we present zero-shot transfer experiments
with SAM, the Segment Anything Model. We consider ﬁve
tasks, four of which differ signiﬁcantly from the promptable
segmentation task used to train SAM. These experiments
evaluate SAM on datasets and tasks that were not seen dur-ing training (our usage of zero-shot transfer follows its
usage in CLIP [82]). The datasets may include novel image
distributions, such as underwater or ego-centric images ( e.g.
Fig. 8) that, to our knowledge, do not appear in SA-1B.
Our experiments begin by testing the core goal of
promptable segmentation: producing a valid mask from any
prompt. We emphasize the challenging scenario of a single
foreground point prompt, since it is more likely to be am-
biguous than other more speciﬁc prompts. Next, we present
a sequence of experiments that traverse low, mid, and high-
level image understanding and roughly parallel the histori-
cal development of the ﬁeld. Speciﬁcally, we prompt SAM
to (1) perform edge detection, (2) segment everything, i.e.
object proposal generation, (3) segment detected objects,
i.e. instance segmentation, and (4), as a proof-of-concept, to
segment objects from free-form text. These four tasks dif-
fer signiﬁcantly from the promptable segmentation task that
SAM was trained on and are implemented via prompt engi-
neering. Our experiments conclude with an ablation study.
Implementation. Unless otherwise speciﬁed: (1) SAM
uses an MAE [47] pre-trained ViT-H [33] image encoder
and (2) SAM was trained on SA-1B, noting that this dataset
includes only automatically generated masks from the ﬁnal
stage of our data engine. For all other model and training
details, such as hyperparameters, refer to A.
7.1. Zero-Shot Single Point Valid Mask Evaluation
Task. We evaluate segmenting an object from a single fore-
ground point. This task is ill-posed as one point can refer
to multiple objects. Ground truth masks in most datasets
do not enumerate allpossible masks, which can make au-
tomatic metrics unreliable. Therefore, we supplement the
standard mIoU metric ( i.e., the mean of all IoUs between
predicted and ground truth masks) with a human study in
which annotators rate mask quality from 1 (nonsense) to 10
(pixel-perfect). See D.1, E, and G for additional details.
By default, we sample points from the center of ground
truth masks (at a maximal value of the masks interior dis-
tance transform), following the standard evaluation proto-
col in interactive segmentation [92]. Since SAM is capable
of predicting multiple masks, we evaluate only the models
most conﬁdent mask by default. The baselines are all
single-mask methods. We compare mainly to RITM [92],
a strong interactive segmenter that performs best on our
benchmark compared to other strong baselines [67, 18].
Datasets. We use a newly compiled suite of 23 datasets
with diverse image distributions. Fig. 8 lists the datasets
and shows a sample from each one (see appendix Table 7 for
more details). We use all 23 datasets for mIoU evaluation.
For the human study, we use the subset listed in Fig. 9b
(due to the resource requirements of such studies). This
subset includes both datasets for which SAM outperforms
and underperforms RITM according to automatic metrics.
8ADE20K [117] BBBC038v1 [12] Cityscapes [25] DOORS [80] DRAM [24] EgoHOS [113] GTEA [34, 63] Hypersim [86]
IBD [17] iShape [111] LVIS [44] NDD20 [100] NDISPark [22, 23] OVIS [81] PPDLS [74] Plittersdorf [46]
STREETS [91] TimberSeg [38] TrashCan [52] VISOR [28, 27] WoodScape [112] PIDRay [104] ZeroWaste-f [6]
Figure 8: Samples from the 23 diverse segmentation datasets used to evaluate SAMs zero-shot transfer capabilities.
-20 0 +20 +40
IoU delta at 1 center pointGTEA [34, 63]TrashCan [52]DRAM [24]PIDRay [104]Cityscapes [25]WoodScape [112]IBD [17]EgoHOS [113]Plittersdorf [46]VISOR [28, 27]NDISPark [22, 23]Hypersim [86]OVIS [81]ADE20K [117]iShape [111]ZeroWaste-f [6]STREETS [91]LVIS [44]NDD20 [100]TimberSeg [38]DOORS [80]BBBC038v1 [12]PPDLS [74]
-21.4-15.0-6.5-5.8-2.0-0.6-0.3+0.8+1.5+1.8+2.7+6.1+7.0+7.8+8.8+9.1+17.3+18.5+21.1+28.9+41.1+44.7+46.9
(a) SAM vs. RITM [92] on 23 datasets
LVIS VISOR DRAM IBD NDD20 OVIS iShape
Datasets579Avg. mask ratingGround Truth
SAM
SAM - single output
RITM
(b) Mask quality ratings by human annotators
123 5 9
Number of points5075mIoU (23 datasets)
SAM (oracle)
SAM
RITM
SimpleClick
FocalClick
123 5 9
Number of points5075mIoU (23 datasets)
SAM (oracle)
(c) Center points (default) (d) Random points
Figure 9: Point to mask evaluation on 23 datasets. (a) Mean IoU of SAM and the strongest single point segmenter, RITM [92].
Due to ambiguity, a single mask may not match ground truth; circles show oracle results of the most relevant of SAMs 3
predictions. (b) Per-dataset comparison of mask quality ratings by annotators from 1 (worst) to 10 (best). All methods use
the ground truth mask center as the prompt. (c, d) mIoU with varying number of points. SAM signiﬁcantly outperforms prior
interactive segmenters with 1 point and is on par with more points. Low absolute mIoU at 1 point is the result of ambiguity.
Results. First, we look at automatic evaluation on the full
suite of 23 datasets using mIoU. We compare per-dataset
results in Fig. 9a against RITM. SAM yields higher re-
sults on 16 of the 23 datasets, by as much as 47 IoU. We
also present an oracle result, in which the most relevant
of SAMs 3 masks is selected by comparing them to the
ground truth, rather than selecting the most conﬁdent mask.
This reveals the impact of ambiguity on automatic evalu-
ation. In particular, with the oracle to perform ambiguity
resolution, SAM outperforms RITM on alldatasets.
Results of the human study are presented in Fig. 9b. Er-
ror bars are 95% conﬁdence intervals for mean mask rat-
ings (all differences are signiﬁcant; see E for details). We
observe that the annotators consistently rate the quality of
SAMs masks substantially higher than the strongest base-
line, RITM. An ablated, ambiguity-unaware version of
SAM with a single output mask has consistently lower rat-
ings, though still higher than RITM. SAMs mean ratingsfall between 7 and 9, which corresponds to the qualitative
rating guideline:  A high score (7-9): The object is identi-
ﬁable and errors are small and rare ( e.g., missing a small,
heavily obscured disconnected component, ...).  These re-
sults indicate that SAM has learned to segment valid masks
from a single point. Note that for datasets like DRAM and
IBD, where SAM is worse on automatic metrics, it receives
consistently higher ratings in the human study .
Fig. 9c shows additional baselines, SimpleClick [67] and
FocalClick [18], which obtain lower single point perfor-
mance than RITM and SAM. As the number of points in-
creases from 1 to 9, we observe that the gap between meth-
ods decreases. This is expected as the task becomes easier;
also, SAM is not optimized for the very high IoU regime.
Finally, in Fig. 9d we replace the default center point sam-
pling with random point sampling. We observe that the gap
between SAM and the baselines grows and SAM is able to
achieve comparable results under either sampling method.
9image ground truth SAM
Figure 10: Zero-shot edge prediction on BSDS500. SAM
was not trained to predict edge maps nor did it have access
to BSDS images or annotations during training.
method year ODS OIS AP R50
HED [108] 2015 .788 .808 .840 .923
EDETR [79] 2022 .840 .858 .896 .930
zero-shot transfer methods:
Sobel ﬁlter 1968 .539 - - -
Canny [13] 1986 .600 .640 .580 -
Felz-Hutt [35] 2004 .610 .640 .560 -
SAM 2023 .768 .786 .794 .928
Table 3: Zero-shot transfer to edge detection on BSDS500.
7.2. Zero-Shot Edge Detection
Approach. We evaluate SAM on the classic low-level task
of edge detection using BSDS500 [72, 3]. We use a sim-
pliﬁed version of our automatic mask generation pipeline.
Speciﬁcally, we prompt SAM with a 16 16 regular grid of
foreground points resulting in 768 predicted masks (3 per
point). Redundant masks are removed by NMS. Then, edge
maps are computed using Sobel ﬁltering of unthresholded
mask probability maps and standard lightweight postpro-
cessing, including edge NMS (see D.2 for details).
Results. We visualize representative edge maps in Fig. 10
(see Fig. 15 for more). Qualitatively, we observe that even
though SAM was not trained for edge detection, it produces
reasonable edge maps. Compared to the ground truth, SAM
predicts more edges, including sensible ones that are not an-
notated in BSDS500. This bias is reﬂected quantitatively in
Table 3: recall at 50% precision (R50) is high, at the cost of
precision. SAM naturally lags behind state-of-the-art meth-
ods that learn the biases of BSDS500, i.e., which edges to
suppress. Nevertheless, SAM performs well compared to
pioneering deep learning methods such as HED [108] (also
trained on BSDS500) and signiﬁcantly better than prior,
though admittedly outdated, zero-shot transfer methods.
7.3. Zero-Shot Object Proposals
Approach. Next, we evaluate SAM on the mid-level task
of object proposal generation [2, 102]. This task has played
an important role in object detection research, serving as anmask AR@1000
method all small med. large freq. com. rare
ViTDet-H [62] 63.0 51.7 80.8 87.0 63.1 63.3 58.3
zero-shot transfer methods:
SAM  single out. 54.9 42.8 76.7 74.4 54.7 59.8 62.0
SAM 59.3 45.5 81.6 86.9 59.1 63.9 65.8
Table 4: Object proposal generation on LVIS v1. SAM is
applied zero-shot, i.e. it was not trained for object proposal
generation nor did it access LVIS images or annotations.
intermediate step in pioneering systems ( e.g., [102, 41, 84]).
To generate object proposals, we run a slightly modiﬁed
version of our automatic mask generation pipeline and out-
put the masks as proposals (see D.3 for details).
We compute the standard average recall (AR) metric on
LVIS v1 [44]. We focus on LVIS because its large number
of categories presents a challenging test. We compare to
astrong baseline implemented as a ViTDet [62] detector
(with cascade Mask R-CNN [48, 11] ViT-H). We note that
this baseline corresponds to the Detector Masquerading
as Proposal generator (DMP) method [16] that was shown
to game AR, making it a truly demanding comparison.
Results. In Table 4 we see unsurprisingly that using the
detections from ViTDet-H as object proposals ( i.e., the
DMP method [16] that games AR) performs the best over-
all. However, SAM does remarkably well on several met-
rics. Notably, it outperforms ViTDet-H on medium and
large objects, as well as rare and common objects. In fact,
SAM only underperforms ViTDet-H on small objects and
frequent objects, where ViTDet-H can easily learn LVIS-
speciﬁc annotation biases since it was trained on LVIS, un-
like SAM. We also compare against an ablated ambiguity-
unaware version of SAM (single out.), which performs
signiﬁcantly worse than SAM on all AR metrics.
7.4. Zero-Shot Instance Segmentation
Approach. Moving to higher-level vision, we use SAM
as the segmentation module of an instance segmenter. The
implementation is simple: we run a object detector (the
ViTDet used before) and prompt SAM with its output
boxes. This illustrates composing SAM in a larger system.
Results. We compare the masks predicted by SAM and
ViTDet on COCO and LVIS in Table 5. Looking at the
mask AP metric we observe gaps on both datasets, where
SAM is reasonably close, though certainly behind ViTDet.
By visualizing outputs, we observed that SAM masks are
often qualitatively better than those of ViTDet, with crisper
boundaries (see D.4 and Fig. 16). To investigate this ob-
servation, we conducted an additional human study asking
annotators to rate the ViTDet masks and SAM masks on the
1 to 10 quality scale used before. In Fig. 11 we observe that
SAM consistently outperforms ViTDet in the human study.
10COCO [66] LVIS v1 [44]
method AP APSAPMAPLAP APSAPMAPL
ViTDet-H [62] 51.0 32.0 54.3 68.9 46.6 35.0 58.0 66.3
zero-shot transfer methods (segmentation module only):
SAM 46.5 30.8 51.0 61.7 44.7 32.5 57.6 65.5
Table 5: Instance segmentation results. SAM is prompted
with ViTDet boxes to do zero-shot segmentation. The fully-
supervised ViTDet outperforms SAM, but the gap shrinks
on the higher-quality LVIS masks. Interestingly, SAM out-
performs ViTDet according to human ratings (see Fig. 11).
1 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings8.6  0.06, LVIS GT
8.1  0.07, SAM
7.9  0.08, ViTDet-H
7.6  0.12, COCO GT
Figure 11: Mask quality rating distribution from our human
study for ViTDet and SAM, both applied to LVIS ground
truth boxes. We also report LVIS and COCO ground truth
quality. The legend shows rating means and 95% conﬁ-
dence intervals. Despite its lower AP (Table 5), SAM has
higher ratings than ViTDet, suggesting that ViTDet exploits
biases in the COCO and LVIS training data.
We hypothesize that on COCO, where the mask AP gap
is larger and the ground truth quality is relatively low (as
borne out by the human study), ViTDet learns the speciﬁc
biases of COCO masks. SAM, being a zero-shot method,
is unable to exploit these (generally undesirable) biases.
The LVIS dataset has higher quality ground truth, but there
are still speciﬁc idiosyncrasies ( e.g., masks do not contain
holes, they are simple polygons by construction) and biases
for modal vs. amodal masks. Again, SAM is not trained to
learn these biases, while ViTDet can exploit them.
7.5. Zero-Shot Text-to-Mask
Approach. Finally, we consider an even higher-level task:
segmenting objects from free-form text. This experiment
is a proof-of-concept of SAMs ability to process text
prompts. While we used the exact same SAM in all prior
experiments, for this one SAMs training procedure is mod-
iﬁed to make it text-aware, but in a way that does not require
new text annotations. Speciﬁcally, for each manually col-
lected mask with area larger than 1002we extract the CLIP
image embedding. Then, during training, we prompt SAM
with the extracted CLIP image embeddings as its ﬁrst in-
teraction. The key observation here is that because CLIPs
image embeddings are trained to align with its textembed-
dings, we can train with image embeddings, but use text
embeddings for inference. That is, at inference time we run
text through CLIPs text encoder and then give the resulting
text embedding as a prompt to SAM (see D.5 for details).
     a wheel
3
     beaver tooth grille 3
     a wiper
7
     a wiper + point
 3
     wipers
7
     wipers + point
 3
Figure 12: Zero-shot text-to-mask. SAM can work with
simple and nuanced text prompts. When SAM fails to make
a correct prediction, an additional point prompt can help.
Results. We show qualitative results in Fig. 12. SAM
can segment objects based on simple text prompts like a
wheel as well as phrases like beaver tooth grille. When
SAM fails to pick the right object from a text prompt only,
an additional point often ﬁxes the prediction, similar to [31].
7.6. Ablations
We perform several ablations on our 23 dataset suite with
the single center point prompt protocol. Recall that a sin-
gle point may be ambiguous and that ambiguity may not
be represented in the ground truth, which contains only a
single mask per point. Since SAM is operating in a zero-
shot transfer setting there can be systematic biases between
SAMs top-ranked mask vs. the masks resulting from data
annotation guidelines. We therefore additionally report the
best mask with respect to the ground truth (oracle).
Fig. 13 (left) plots SAMs performance when trained on
cumulative data from the data engine stages. We observe
that each stage increases mIoU. When training with all three
stages, the automatic masks vastly outnumber the manual
and semi-automatic masks. To address this, we found that
oversampling the manual and semi-automatic masks during
training by 10gave best results. This setup complicates
training. We therefore tested a fourth setup that uses only
the automatically generated masks. With this data, SAM
performs only marginally lower than using all data ( 0.5
mIoU). Therefore, by default we use only the automatically
generated masks to simplify the training setup.
In Fig. 13 (middle) we look at the impact of data volume.
The full SA-1B contains 11M images, which we uniformly
subsample to 1M and 0.1M for this ablation. At 0.1M im-
ages, we observe a large mIoU decline under all settings.
However, with 1M images, about 10% of the full dataset,
we observe results comparable to using the full dataset.
This data regime, which still includes approximately 100M
masks, may be a practical setting for many use cases.
11manual + semi
automatic+ automatic automatic
only
Training data stages506070mIoU (23 datasets)1 point (oracle)
1 point
0.1M 1M 11M
Training images707580mIoU (23 datasets)
1 point (oracle)
2 points3 points5 points
91M
ViT-B308M
ViT-L636M
ViT-H
Number of parameters606570mIoU (23 datasets)
1 point (oracle)
1 point
Figure 13: Ablation studies of our data engine stages, image encoder scaling, and training data scaling. (Left) Each data
engine stage leads to improvements on our 23 dataset suite, and training with only the automatic data (our default) yields
similar results to using data from all three stages. (Middle) SAM trained with 10% of SA-1B and full SA-1B is comparable.
We train with all 11M images by default, but using 1M images is a reasonable practical setting. (Right) Scaling SAMs image
encoder shows meaningful, yet saturating gains. Nevertheless, smaller image encoders may be preferred in certain settings.
Finally, Fig. 13 (right) shows results with ViT-B, ViT-L,
and ViT-H image encoders. ViT-H improves substantially
over ViT-B, but has only marginal gains over ViT-L. Further
image encoder scaling does not appear fruitful at this time.
8. Discussion
Foundation models. Pre-trained models have been adapted
to downstream tasks since the early days of machine learn-
ing [99]. This paradigm has become increasingly impor-
tant in recent years with a growing emphasis on scale, and
such models have recently been (re-)branded as founda-
tion models: i.e. models that are trained on broad data
at scale and are adaptable to a wide range of downstream
tasks [8]. Our work correlates well with this deﬁnition,
though we note that a foundation model for image segmen-
tation is an inherently limited scope, since it represents an
important, yet fractional, subset of computer vision. We
also contrast one aspect of our approach with [8], which
emphasizes the role of self-supervised learning in founda-
tion models. While our model is initialized with a self-
supervised technique (MAE [47]), the vast majority of its
capabilities come from large-scale supervised training. In
cases where data engines can scale available annotations,
like ours, supervised training provides an effective solution.
Compositionality. Pre-trained models can power new ca-
pabilities even beyond ones imagined at the moment of
training. One prominent example is how CLIP [82] is used
as a component in larger systems, such as DALL E [83].
Our goal is to make this kind of composition straightfor-
ward with SAM. We aim to achieve this by requiring SAM
to predict a valid mask for a wide range of segmentation
prompts. The effect is to create a reliable interface between
SAM and other components. For example, MCC [106] can
easily use SAM to segment an object of interest and achieve
strong generalization to unseen objects for 3D reconstruc-
tion from a single RGB-D image. In another example, SAM
can be prompted with gaze points detected by a wearable
device, enabling new applications. Thanks to SAMs abil-
ity to generalize to new domains like ego-centric images,
such systems work without need for additional training.Limitations. While SAM performs well in general, it is
not perfect. It can miss ﬁne structures, hallucinates small
disconnected components at times, and does not produce
boundaries as crisply as more computationally intensive
methods that zoom-in, e.g. [18]. In general, we expect
dedicated interactive segmentation methods to outperform
SAM when many points are provided, e.g. [67]. Unlike
these methods, SAM is designed for generality and breadth
of use rather than high IoU interactive segmentation. More-
over, SAM can process prompts in real-time, but neverthe-
less SAMs overall performance is not real-time when using
a heavy image encoder. Our foray into the text-to-mask task
is exploratory and not entirely robust, although we believe
it can be improved with more effort. While SAM can per-
form many tasks, it is unclear how to design simple prompts
that implement semantic and panoptic segmentation. Fi-
nally, there are domain-speciﬁc tools, such as [7], that we
expect to outperform SAM in their respective domains.
Conclusion. The Segment Anything project is an attempt to
lift image segmentation into the era of foundation models.
Our principal contributions are a new task (promptable seg-
mentation), model (SAM), and dataset (SA-1B) that make
this leap possible. Whether SAM achieves the status of a
foundation model remains to be seen by how it is used in
the community, but regardless we expect the perspective of
this work, the release of over 1B masks, and our promptable
segmentation model will help pave the path ahead.
Acknowledgments. We would like to thank Aaron Ad-
cock and Jitendra Malik for helpful discussion. We thank
Vaibhav Aggarwal and Yanghao Li for help with scal-
ing the model. We thank Cheng-Yang Fu, Jiabo Hu, and
Robert Kuo for help with data annotation platform. We
thank Allen Goodman and Bram Wasti for help in optimiz-
ing web-version of our model. Finally, we thank Morteza
Behrooz, Ashley Gabriel, Ahuva Goldstand, Sumanth Gur-
ram, Somya Jain, Devansh Kukreja, Joshua Lane, Lilian
Luong, Mallika Malhotra, William Ngan, Omkar Parkhi,
Nikhil Raina, Dirk Rowe, Neil Sejoor, Vanessa Stark, Bala
Varadarajan, and Zachary Winstrom for their help in mak-
ing the demo, dataset viewer, and other assets and tooling.
12References
[1] Edward H Adelson. On seeing stuff: the perception of materials by
humans and machines. Human vision and electronic imaging VI ,
2001. 5
[2] Bogdan Alexe, Thomas Deselaers, and Vittorio Ferrari. What is an
object? CVPR , 2010. 4, 10
[3] Pablo Arbel aez, Michael Maire, Charless Fowlkes, and Jitendra
Malik. Contour detection and hierarchical image segmentation.
TPAMI , 2010. 4, 10, 21, 28
[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer
normalization. arXiv:1607.06450 , 2016. 16
[5] Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-training of
image transformers. arXiv:2106.08254 , 2021. 17
[6] Dina Bashkirova, Mohamed Abdelfattah, Ziliang Zhu, James Akl,
Fadi Alladkani, Ping Hu, Vitaly Ablavsky, Berk Calli, Sarah Adel
Bargal, and Kate Saenko. ZeroWaste dataset: Towards deformable
object segmentation in cluttered scenes. CVPR , 2022. 9, 20
[7] Stuart Berg, Dominik Kutra, Thorben Kroeger, Christoph N.
Straehle, Bernhard X. Kausler, Carsten Haubold, Martin Schiegg,
Janez Ales, Thorsten Beier, Markus Rudy, Kemal Eren, Jaime I.
Cervantes, Buote Xu, Fynn Beuttenmueller, Adrian Wolny, Chong
Zhang, Ullrich Koethe, Fred A. Hamprecht, and Anna Kreshuk.
ilastik: interactive machine learning for (bio)image analysis. Na-
ture Methods , 2019. 12
[8] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman,
Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette
Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportu-
nities and risks of foundation models. arXiv:2108.07258 , 2021. 1,
12
[9] Gustav Bredell, Christine Tanner, and Ender Konukoglu. Iterative
interaction training for segmentation editing networks. MICCAI ,
2018. 17
[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav
Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Ben-
jamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei. Language models
are few-shot learners. NeurIPS , 2020. 1, 4
[11] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: Delving into
high quality object detection. CVPR , 2018. 10
[12] Juan C. Caicedo, Allen Goodman, Kyle W. Karhohs, Beth A. Ci-
mini, Jeanelle Ackerman, Marzieh Haghighi, CherKeng Heng, Tim
Becker, Minh Doan, Claire McQuin, Mohammad Rohban, Shan-
tanu Singh, and Anne E. Carpenter. Nucleus segmentation across
imaging experiments: the 2018 data science bowl. Nature Methods ,
2019. 9, 19, 20
[13] John Canny. A computational approach to edge detection. TPAMI ,
1986. 10, 21
[14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end
object detection with Transformers. ECCV , 2020. 5, 16, 17
[15] Guillaume Charpiat, Matthias Hofmann, and Bernhard Sch olkopf.
Automatic image colorization via multimodal predictions. ECCV ,
2008. 5, 17
[16] Neelima Chavali, Harsh Agrawal, Aroma Mahendru, and Dhruv
Batra. Object-proposal evaluation protocol is gameable. CVPR ,
2016. 10, 21
[17] Jiazhou Chen, Yanghui Xu, Shufang Lu, Ronghua Liang, and Lian-
gliang Nan. 3D instance segmentation of MVS buildings. IEEE
Transactions on Geoscience and Remote Sensing , 2022. 9, 19, 20,
23, 24
[18] Xi Chen, Zhiyan Zhao, Yilei Zhang, Manni Duan, Donglian Qi, and
Hengshuang Zhao. FocalClick: towards practical interactive image
segmentation. CVPR , 2022. 8, 9, 12, 19[19] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kir-
illov, and Rohit Girdhar. Masked-attention mask transformer for
universal image segmentation. CVPR , 2022. 4
[20] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-
pixel classiﬁcation is not all you need for semantic segmentation.
NeurIPS , 2021. 5, 16, 17
[21] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won
Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling
language modeling with pathways. arXiv:2204.02311 , 2022. 1
[22] Luca Ciampi, Carlos Santiago, Joao Costeira, Claudio Gennaro, and
Giuseppe Amato. Domain adaptation for trafﬁc density estimation.
International Joint Conference on Computer Vision, Imaging and
Computer Graphics Theory and Applications , 2021. 9, 20
[23] Luca Ciampi, Carlos Santiago, Joao Costeira, Claudio Gennaro, and
Giuseppe Amato. Night and day instance segmented park (NDIS-
Park) dataset: a collection of images taken by day and by night for
vehicle detection, segmentation and counting in parking areas. Zen-
odo, 2022. 9, 20
[24] Nadav Cohen, Yael Newman, and Ariel Shamir. Semantic segmen-
tation in art paintings. Computer Graphics Forum , 2022. 9, 19, 20,
23, 24
[25] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld,
Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth,
and Bernt Schiele. The Cityscapes dataset for semantic urban scene
understanding. CVPR , 2016. 9, 19, 20
[26] Bruno da Silva, George Konidaris, and Andrew Barto. Learning
parameterized skills. ICML , 2012. 4
[27] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino
Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan
Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling
egocentric vision: Collection, pipeline and challenges for EPIC-
KITCHENS-100. IJCV , 2022. 9, 20, 23, 24
[28] Ahmad Darkhalil, Dandan Shan, Bin Zhu, Jian Ma, Amlan Kar,
Richard Higgins, Sanja Fidler, David Fouhey, and Dima Damen.
EPIC-KITCHENS VISOR benchmark: Video segmentations and
object relations. NeurIPS , 2022. 9, 19, 20, 23, 24
[29] Terrance De Vries, Ishan Misra, Changhan Wang, and Laurens
Van der Maaten. Does object recognition work for everyone? CVPR
workshops , 2019. 18
[30] Mark D ıaz, Ian Kivlichan, Rachel Rosen, Dylan Baker, Razvan
Amironesei, Vinodkumar Prabhakaran, and Emily Denton. Crowd-
WorkSheets: Accounting for individual and collective identities un-
derlying crowdsourced dataset annotation. ACM Conference on
Fairness, Accountability, and Transparency , 2022. 25
[31] Henghui Ding, Scott Cohen, Brian Price, and Xudong Jiang.
PhraseClick: toward achieving ﬂexible interactive segmentation by
phrase and click. ECCV , 2020. 11
[32] Piotr Doll ar and C Lawrence Zitnick. Fast edge detection using
structured forests. TPAMI , 2014. 21
[33] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk
Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa De-
hghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob
Uszkoreit, and Neil Houlsby. An image is worth 16x16 words:
Transformers for image recognition at scale. ICLR , 2021. 5, 8,
16
[34] Alireza Fathi, Xiaofeng Ren, and James M. Rehg. Learning to rec-
ognize objects in egocentric activities. CVPR , 2011. 9, 19, 20
[35] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efﬁcient graph-
based image segmentation. IJCV , 2004. 10
[36] Thomas B. Fitzpatrick. The validity and practicality of sun-reactive
skin types i through vi. Archives of Dermatology , 1988. 8
[37] Marco Forte, Brian Price, Scott Cohen, Ning Xu, and Franc ois
Pitie. Getting to 99% accuracy in interactive segmentation.
arXiv:2003.07932 , 2020. 5, 17
[38] Jean-Michel Fortin, Olivier Gamache, Vincent Grondin, Franc ois
Pomerleau, and Philippe Gigu ere. Instance segmentation for au-
tonomous log grasping in forestry operations. IROS , 2022. 9, 20
13[39] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jen-
nifer Wortman Vaughan, Hanna Wallach, Hal Daum e Iii, and Kate
Crawford. Datasheets for datasets. Communications of the ACM ,
2021. 25
[40] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin,
Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple copy-paste is a
strong data augmentation method for instance segmentation. CVPR ,
2021. 16, 18, 22
[41] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.
Rich feature hierarchies for accurate object detection and semantic
segmentation. CVPR , 2014. 10
[42] Priya Goyal, Piotr Doll ar, Ross Girshick, Pieter Noordhuis, Lukasz
Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and
Kaiming He. Accurate, large minibatch SGD: Training ImageNet
in 1 hour. arXiv:1706.02677 , 2017. 17
[43] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary
Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger,
Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Na-
garajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona
Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhong-
cong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Car-
tillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli,
Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Chris-
tian Fuegen, Abrham Gebreselasie, Cristina Gonzalez, James Hillis,
Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Ko-
lar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li,
Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Mod-
hugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will
Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran
Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao,
Minh V o, Yuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu,
Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria
Farinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V . Jawahar,
Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude
Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi,
Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei
Yan, and Jitendra Malik. Ego4D: Around the World in 3,000 Hours
of Egocentric Video. CVPR , 2022. 20
[44] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: A dataset for
large vocabulary instance segmentation. CVPR , 2019. 2, 6, 7, 9, 10,
11, 19, 20, 21, 24
[45] Abner Guzman-Rivera, Dhruv Batra, and Pushmeet Kohli. Multiple
choice learning: Learning to produce multiple structured outputs.
NeurIPS , 2012. 5, 17
[46] Timm Haucke, Hjalmar S. K uhl, and V olker Steinhage.
SOCRATES: Introducing depth in visual wildlife monitoring using
stereo vision. Sensors , 2022. 9, 20
[47] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ar,
and Ross Girshick. Masked autoencoders are scalable vision learn-
ers.CVPR , 2022. 5, 8, 12, 16, 17
[48] Kaiming He, Georgia Gkioxari, Piotr Doll ar, and Ross Girshick.
Mask R-CNN. ICCV , 2017. 10
[49] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
residual learning for image recognition. CVPR , 2016. 16
[50] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units
(gelus). arXiv:1606.08415 , 2016. 16
[51] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena
Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas,
Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training
compute-optimal large language models. arXiv:2203.15556 , 2022.
1
[52] Jungseok Hong, Michael Fulton, and Junaed Sattar. TrashCan: A
semantically-segmented dataset towards visual detection of marine
debris. arXiv:2007.08097 , 2020. 9, 19, 20
[53] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Wein-
berger. Deep networks with stochastic depth. ECCV , 2016. 17
[54] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov,
and Humphrey Shi. Oneformer: One transformer to rule universal
image segmentation. arXiv:2211.06220 , 2022. 4[55] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
Scaling up visual and vision-language representation learning with
noisy text supervision. ICML , 2021. 1
[56] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,
Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey
Wu, and Dario Amodei. Scaling laws for neural language models.
arXiv:2001.08361 , 2020. 1
[57] Michael Kass, Andrew Witkin, and Demetri Terzopoulos. Snakes:
Active contour models. IJCV , 1988. 4
[58] Dahun Kim, Tsung-Yi Lin, Anelia Angelova, In So Kweon, and
Weicheng Kuo. Learning open-world object proposals without
learning to classify. IEEE Robotics and Automation Letters , 2022.
21
[59] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother,
and Piotr Doll ar. Panoptic segmentation. CVPR , 2019. 4
[60] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan
Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo
Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari.
The open images dataset v4: Uniﬁed image classiﬁcation, object
detection, and visual relationship detection at scale. IJCV , 2020. 2,
6, 7, 18, 19
[61] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and
Thomas Dandres. Quantifying the carbon emissions of machine
learning. arXiv:1910.09700 , 2019. 28
[62] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Explor-
ing plain vision transformer backbones for object detection. ECCV ,
2022. 5, 10, 11, 16, 21, 23, 24
[63] Yin Li, Zhefan Ye, and James M. Rehg. Delving into egocentric
actions. CVPR , 2015. 9, 20
[64] Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Interactive image
segmentation with latent diversity. CVPR , 2018. 5, 17, 19
[65] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr
Dollar. Focal loss for dense object detection. ICCV , 2017. 5, 17
[66] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro
Perona, Deva Ramanan, Piotr Doll ar, and C Lawrence Zitnick. Mi-
crosoft COCO: Common objects in context. ECCV , 2014. 2, 4, 6,
7, 11, 18, 19, 20
[67] Qin Liu, Zhenlin Xu, Gedas Bertasius, and Marc Niethammer. Sim-
pleClick: Interactive image segmentation with simple vision trans-
formers. arXiv:2210.11006 , 2022. 8, 9, 12, 19
[68] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regu-
larization. ICLR , 2019. 17
[69] Cathy H Lucas, Daniel OB Jones, Catherine J Hollyhead, Robert H
Condon, Carlos M Duarte, William M Graham, Kelly L Robinson,
Kylie A Pitt, Mark Schildhauer, and Jim Regetz. Gelatinous zoo-
plankton biomass in the global oceans: geographic variation and
environmental drivers. Global Ecology and Biogeography , 2014.
20
[70] Sabarinath Mahadevan, Paul V oigtlaender, and Bastian Leibe. Iter-
atively trained interactive segmentation. BMVC , 2018. 4, 17
[71] Kevis-Kokitsi Maninis, Sergi Caelles, Jordi Pont-Tuset, and Luc
Van Gool. Deep extreme cut: From extreme points to object seg-
mentation. CVPR , 2018. 6
[72] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik.
A database of human segmented natural images and its applica-
tion to evaluating segmentation algorithms and measuring ecologi-
cal statistics. ICCV , 2001. 10, 21, 28
[73] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-Net:
Fully convolutional neural networks for volumetric medical image
segmentation. 3DV, 2016. 5, 17
[74] Massimo Minervini, Andreas Fischbach, Hanno Scharr, and
Sotirios A. Tsaftaris. Finely-grained annotated datasets for image-
based plant phenotyping. Pattern Recognition Letters , 2016. 9, 20
[75] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes,
Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Debo-
rah Raji, and Timnit Gebru. Model cards for model reporting. Pro-
ceedings of the conference on fairness, accountability, and trans-
parency , 2019. 25, 28
14[76] Dim P Papadopoulos, Jasper RR Uijlings, Frank Keller, and Vittorio
Ferrari. Extreme clicking for efﬁcient object annotation. ICCV ,
2017. 6
[77] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-
Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and
Jeff Dean. Carbon emissions and large neural network training.
arXiv:2104.10350 , 2021. 28
[78] Matthew E Peters, Waleed Ammar, Chandra Bhagavatula, and Rus-
sell Power. Semi-supervised sequence tagging with bidirectional
language models. Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics , 2017. 18
[79] Mengyang Pu, Yaping Huang, Yuming Liu, Qingji Guan, and
Haibin Ling. EDTER: Edge detection with transformer. CVPR ,
2022. 10
[80] Mattia Pugliatti and Francesco Topputo. DOORS: Dataset fOr
bOuldeRs Segmentation. Zenodo , 2022. 9, 20
[81] Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu, Xiang
Bai, Serge Belongie, Alan Yuille, Philip Torr, and Song Bai. Oc-
cluded video instance segmentation: A benchmark. ICCV , 2022. 9,
20, 23, 24
[82] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,
Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. ICML , 2021. 1, 2, 4, 5,
8, 12, 16, 22
[83] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea
V oss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-
to-image generation. ICML , 2021. 1, 4, 12
[84] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster
R-CNN: Towards real-time object detection with region proposal
networks. NeurIPS , 2015. 6, 10
[85] Xiaofeng Ren and Jitendra Malik. Learning a classiﬁcation model
for segmentation. ICCV , 2003. 4
[86] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar,
Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M.
Susskind. Hypersim: A photorealistic synthetic dataset for holistic
indoor scene understanding. ICCV , 2021. 9, 19, 20
[87] Candice Schumann, Susanna Ricco, Utsav Prabhu, Vittorio Ferrari,
and Caroline Pantofaru. A step toward more inclusive people anno-
tations for fairness. Proceedings of the 2021 AAAI/ACM Conference
on AI, Ethics, and Society , 2021. 8, 19
[88] Seﬁk Ilkin Serengil and Alper Ozpinar. LightFace: A hybrid deep
face recognition framework. ASYU , 2020. 26
[89] Seﬁk Ilkin Serengil and Alper Ozpinar. HyperExtended LightFace:
A facial attribute analysis framework. ICEET , 2021. 26
[90] Jamie Shotton, John Winn, Carsten Rother, and Antonio Crimin-
isi. TextonBoost: Joint appearance, shape and context modeling for
mulit-class object recognition and segmentation. ECCV , 2006. 4
[91] Corey Snyder and Minh Do. STREETS: A novel camera network
dataset for trafﬁc ﬂow. NeurIPS , 2019. 9, 20
[92] Konstantin Soﬁiuk, Ilya A Petrov, and Anton Konushin. Reviving
iterative training with mask guidance for interactive segmentation.
ICIP , 2022. 5, 8, 9, 17, 19, 23, 24, 28
[93] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to
prevent neural networks from overﬁtting. The Journal of Machine
Learning Research , 2014. 16
[94] Chris Stauffer and W Eric L Grimson. Adaptive background mix-
ture models for real-time tracking. CVPR , 1999. 4
[95] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara
Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-
mamoorthi, Jonathan Barron, and Ren Ng. Fourier features let net-
works learn high frequency functions in low dimensional domains.
NeurIPS , 2020. 5, 16
[96] Yansong Tang, Yi Tian, Jiwen Lu, Jianjiang Feng, and Jie Zhou.
Action recognition in RGB-D egocentric videos. ICIP , 2017. 20[97] Yansong Tang, Zian Wang, Jiwen Lu, Jianjiang Feng, and Jie Zhou.
Multi-stream deep neural networks for RGB-D egocentric action
recognition. IEEE Transactions on Circuits and Systems for Video
Technology , 2019. 20
[98] The World Bank. The world by income and regions,
2022. https://datatopics.worldbank.org/world-development-
indicators/the-world-by-income-and-region.html. 18
[99] Sebastian Thrun. Is learning the n-th thing any easier than learning
the ﬁrst? NeurIPS , 1995. 12
[100] Cameron Trotter, Georgia Atkinson, Matt Sharpe, Kirsten Richard-
son, A. Stephen McGough, Nick Wright, Ben Burville, and Per
Berggren. NDD20: A large-scale few-shot dolphin dataset for
coarse and ﬁne-grained categorisation. arXiv:2005.13359 , 2020.
9, 19, 20, 23, 24
[101] United States Environmental Protection Agency. Greenhouse Gas
Equivalencies Calculator. https://www.epa.gov/energy/greenhouse-
gas-equivalencies-calculator, 2022. 28
[102] Koen EA van de Sande, Jasper RR Uijlings, Theo Gevers, and
Arnold WM Smeulders. Segmentation as selective search for ob-
ject recognition. ICCV , 2011. 10
[103] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.
Attention is all you need. NeurIPS , 2017. 5, 16
[104] Boying Wang, Libo Zhang, Longyin Wen, Xianglong Liu, and Yan-
jun Wu. Towards real-world prohibited item detection: A large-
scale x-ray benchmark. CVPR , 2021. 9, 19, 20
[105] Weiyao Wang, Matt Feiszli, Heng Wang, Jitendra Malik, and
Du Tran. Open-world instance segmentation: Exploiting pseudo
ground truth from learned pairwise afﬁnity. CVPR , 2022. 21
[106] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feicht-
enhofer, and Georgia Gkioxari. Multiview compressive coding for
3D reconstruction. CVPR , 2023. 12
[107] Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and An-
tonio Torralba. SUN database: Large-scale scene recognition from
abbey to zoo. CVPR , 2010. 20
[108] Saining Xie and Zhuowen Tu. Holistically-nested edge detection.
ICCV , 2015. 10
[109] Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and Thomas S
Huang. Deep interactive object selection. CVPR , 2016. 4, 19
[110] Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, and Olga Rus-
sakovsky. Towards fairer datasets: Filtering and balancing the dis-
tribution of the people subtree in the imagenet hierarchy. Proceed-
ings of the 2020 conference on fairness, accountability, and trans-
parency , 2020. 8
[111] Lei Yang, Yan Zi Wei, Yisheng HE, Wei Sun, Zhenhang Huang,
Haibin Huang, and Haoqiang Fan. iShape: A ﬁrst step towards
irregular shape instance segmentation. arXiv:2109.15068 , 2021. 9,
20, 23, 24
[112] Senthil Yogamani, Ciar an Hughes, Jonathan Horgan, Ganesh Sistu,
Padraig Varley, Derek ODea, Michal Uric ar, Stefan Milz, Mar-
tin Simon, Karl Amende, et al. WoodScape: A multi-task, multi-
camera ﬁsheye dataset for autonomous driving. ICCV , 2019. 9,
20
[113] Lingzhi Zhang, Shenghao Zhou, Simon Stent, and Jianbo Shi. Fine-
grained egocentric hand-object segmentation: Dataset, model, and
applications. ECCV , 2022. 9, 19, 20
[114] Wenwei Zhang, Jiangmiao Pang, Kai Chen, and Chen Change Loy.
K-Net: Towards uniﬁed image segmentation. NeurIPS , 2021. 4
[115] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-
Wei Chang. Men also like shopping: Reducing gender bias ampli-
ﬁcation using corpus-level constraints. arXiv:1707.09457 , 2017. 8
[116] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and An-
tonio Torralba. Places: A 10 million image database for scene
recognition. TPAMI , 2017. 20
[117] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler,
Adela Barriuso, and Antonio Torralba. Semantic understanding of
scenes through the ADE20K dataset. IJCV , 2019. 2, 7, 9, 20
15Appendix
Table of contents:
 A: Segment Anything Model and Task Details
 B: Automatic Mask Generation Details
 C: RAI Additional Details
 D: Experiment Implementation Details
 E: Human Study Experimental Design
 F: Dataset, Annotation, and Model Cards
 G: Annotation Guidelines
A. Segment Anything Model and Task Details
Image encoder. In general, the image encoder can be any
network that outputs a CHWimage embedding. Mo-
tivated by scalability and access to strong pre-training, we
use an MAE [47] pre-trained Vision Transformer (ViT) [33]
with minimal adaptations to process high resolution inputs,
speciﬁcally a ViT-H/16 with 14 14 windowed attention
and four equally-spaced global attention blocks, follow-
ing [62]. The image encoders output is a 16 downscaled
embedding of the input image. Since our runtime goal is to
process each prompt in real-time, we can afford a high num-
ber of image encoder FLOPs because they are computed
only once per image, notper prompt.
Following standard practices ( e.g., [40]), we use an in-
put resolution of 1024 1024 obtained by rescaling the im-
age and padding the shorter side. The image embedding
is therefore 6464. To reduce the channel dimension, fol-
lowing [62], we use a 1 1 convolution to get to 256 chan-
nels, followed by a 3 3 convolution also with 256 channels.
Each convolution is followed by a layer normalization [4].
Prompt encoder. Sparse prompts are mapped to 256-
dimensional vectorial embeddings as follows. A point is
represented as the sum of a positional encoding [95] of the
points location and one of two learned embeddings that in-
dicate if the point is either in the foreground or background.
A box is represented by an embedding pair: (1) the posi-
tional encoding of its top-left corner summed with a learned
embedding representing top-left corner and (2) the same
structure but using a learned embedding indicating bottom-
right corner. Finally, to represent free-form text we use the
text encoder from CLIP [82] (any text encoder is possible in
general). We focus on geometric prompts for the remainder
of this section and discuss text prompts in depth in D.5.
Dense prompts ( i.e., masks) have a spatial correspon-
dence with the image. We input masks at a 4 lower res-
olution than the input image, then downscale an additional
4using two 22, stride-2 convolutions with output chan-
nels 4 and 16, respectively. A ﬁnal 1 1 convolution maps
the channel dimension to 256. Each layer is separated by
GELU activations [50] and layer normalization. The mask
image
embedding
(256x64x64)x2
token
to image
attn.2x
conv.
trans.
IoU
scoresmlpmasksdot product
per mask
prompt tokens
(Ntokensx256)output tokens
+output
token
per mask
IoU
output
tokenmlp
mask decoderself attn.token to image attn.mlpimage to token attn.Figure 14: Details of the lightweight mask decoder. A
two-layer decoder updates both the image embedding and
prompt tokens via cross-attention. Then the image embed-
ding is upscaled, from which the updated output tokens are
used to dynamically predict masks. (Not illustrated for ﬁg-
ure clarity: At every attention layer, positional encodings
are added to the image embedding, and the entire original
prompt token (including position encoding) is re-added to
the token queries and keys.)
and image embedding are then added element-wise. If there
is no mask prompt, a learned embedding representing no
mask is added to each image embedding location.
Lightweight mask decoder. This module efﬁciently maps
the image embedding and a set of prompt embeddings to an
output mask. To combine these inputs, we take inspiration
from Transformer segmentation models [14, 20] and modify
a standard Transformer decoder [103]. Before applying our
decoder, we ﬁrst insert into the set of prompt embeddings
a learned output token embedding that will be used at the
decoders output, analogous to the [class] token in [33].
For simplicity, we refer to these embeddings ( notincluding
the image embedding) collectively as tokens.
Our decoder design is shown in Fig. 14. Each decoder
layer performs 4 steps: (1) self-attention on the tokens, (2)
cross-attention from tokens (as queries) to the image em-
bedding, (3) a point-wise MLP updates each token, and (4)
cross-attention from the image embedding (as queries) to
tokens. This last step updates the image embedding with
prompt information. During cross-attention, the image em-
bedding is treated as a set of 642256-dimensional vectors.
Each self/cross-attention and MLP has a residual connec-
tion [49], layer normalization, and a dropout [93] of 0.1 at
training. The next decoder layer takes the updated tokens
and the updated image embedding from the previous layer.
We use a two-layer decoder.
To ensure the decoder has access to critical geometric in-
formation the positional encodings are added to the image
embedding whenever they participate in an attention layer.
Additionally, the entire original prompt tokens (including
their positional encodings) are re-added to the updated to-
kens whenever they participate in an attention layer. This
allows for a strong dependence on both the prompt tokens
geometric location and type.
After running the decoder, we upsample the updated im-
age embedding by 4 with two transposed convolutional
16layers (now its downscaled 4 relative to the input image).
Then, the tokens attend once more to the image embedding
and we pass the updated output token embedding to a small
3-layer MLP that outputs a vector matching the channel di-
mension of the upscaled image embedding. Finally, we pre-
dict a mask with a spatially point-wise product between the
upscaled image embedding and the MLPs output.
The transformer uses an embedding dimension of 256.
The transformer MLP blocks have a large internal dimen-
sion of 2048, but the MLP is applied only to the prompt to-
kens for which there are relatively few (rarely greater than
20). However, in cross-attention layers where we have a
6464 image embedding, we reduce the channel dimension
of the queries, keys, and values by 2 to 128 for computa-
tional efﬁciency. All attention layers use 8 heads.
The transposed convolutions used to upscale the output
image embedding are 2 2, stride 2 with output channel di-
mensions of 64 and 32 and have GELU activations. They
are separated by layer normalization.
Making the model ambiguity-aware. As described, a sin-
gle input prompt may be ambiguous in the sense that it cor-
responds to multiple valid masks, and the model will learn
to average over these masks. We eliminate this problem
with a simple modiﬁcation: instead of predicting a single
mask, we use a small number of output tokens and predict
multiple masks simultaneously. By default we predict three
masks, since we observe that three layers (whole, part, and
subpart) are often enough to describe nested masks. During
training, we compute the loss (described shortly) between
the ground truth and each of the predicted masks, but only
backpropagate from the lowest loss. This is a common tech-
nique used for models with multiple outputs [15, 45, 64].
For use in applications, wed like to rank predicted masks,
so we add a small head (operating on an additional output
token) that estimates the IoU between each predicted mask
and the object it covers.
Ambiguity is much rarer with multiple prompts and the
three output masks will usually become similar. To mini-
mize computation of degenerate losses at training and en-
sure the single unambiguous mask receives a regular gradi-
ent signal, we only predict a single mask when more than
one prompt is given. This is accomplished by adding a
fourth output token for an additional mask prediction. This
fourth mask is never returned for a single prompt and is the
only mask returned for multiple prompts.
Losses. We supervise mask prediction with a linear combi-
nation of focal loss [65] and dice loss [73] in a 20:1 ratio of
focal loss to dice loss, following [20, 14]. Unlike [20, 14],
we observe that auxiliary deep supervision after each de-
coder layer is unhelpful. The IoU prediction head is trained
with mean-square-error loss between the IoU prediction and
the predicted masks IoU with the ground truth mask. It is
added to the mask loss with a constant scaling factor of 1.0.Training algorithm. Following recent approaches [92, 37],
we simulate an interactive segmentation setup during train-
ing. First, with equal probability either a foreground point
or bounding box is selected randomly for the target mask.
Points are sampled uniformly from the ground truth mask.
Boxes are taken as the ground truth masks bounding box,
with random noise added in each coordinate with standard
deviation equal to 10% of the box sidelength, to a maxi-
mum of 20 pixels. This noise proﬁle is a reasonable com-
promise between applications like instance segmentation,
which produce a tight box around the target object, and in-
teractive segmentation, where a user may draw a loose box.
After making a prediction from this ﬁrst prompt, subse-
quent points are selected uniformly from the error region
between the previous mask prediction and the ground truth
mask. Each new point is foreground or background if the er-
ror region is a false negative or false positive, respectively.
We also supply the mask prediction from the previous it-
eration as an additional prompt to our model. To provide
the next iteration with maximal information, we supply the
unthresholded mask logits instead of the binarized mask.
When multiple masks are returned, the mask passed to the
next iteration and used to sample the next point is the one
with the highest predicted IoU.
We ﬁnd diminishing returns after 8 iteratively sampled
points (we have tested up to 16). Additionally, to encour-
age the model to beneﬁt from the supplied mask, we also
use two more iterations where no additional points are sam-
pled. One of these iterations is randomly inserted among the
8 iteratively sampled points, and the other is always at the
end. This gives 11 total iterations: one sampled initial in-
put prompt, 8 iteratively sampled points, and two iterations
where no new external information is supplied to the model
so it can learn to reﬁne its own mask predictions. We note
that using a relatively large number of iterations is possible
because our lightweight mask decoder requires less than 1%
of the image encoders compute and, therefore, each itera-
tion adds only a small overhead. This is unlike previous
interactive methods that perform only one or a few interac-
tive steps per optimizer update [70, 9, 37, 92].
Training recipe. We use the AdamW [68] optimizer ( 1=
0:9,2= 0:999) and a linear learning rate warmup [42] for
250 iterations and a step-wise learning rate decay schedule.
The initial learning rate ( lr), after warmup, is 8e4. We
train for 90k iterations ( 2 SA-1B epochs) and decrease the
lrby a factor of 10 at 60k iterations and again at 86666 it-
erations. The batch size is 256 images. To regularize SAM,
we set weight decay ( wd) to 0.1 and apply drop path [53]
(dp) with a rate of 0.4. We use a layer-wise learning rate
decay [5] ( ld) of 0.8. No data augmentation is applied. We
initialize SAM from an MAE [47] pre-trained ViT-H. We
distribute training across 256 GPUs, due to the large image
encoder and 10241024 input size. To limit GPU mem-
17ory usage, we train with up to 64 randomly sampled masks
per GPU. Additionally, we ﬁnd that lightly ﬁltering SA-1B
masks to discard any that cover more than 90% of the image
qualitatively improves results.
For ablations and others variations on training ( e.g., text-
to-mask D.5), we deviate from the default recipe above as
follows. When training with data from the ﬁrst and sec-
ond data engine stages only, we augment the input with
large-scale jitter [40] with a scale range of [0.1, 2.0]. In-
tuitively, data augmentation may be helpful when training
data is more limited. To train ViT-B and ViT-L, we use
180k iterations with batch size 128 distributed across 128
GPUs. We set lr=8e4/4e4,ld= 0.6/0.8, wd= 0.1, and
dp= 0.6/0.4 for ViT-B/L, respectively.
B. Automatic Mask Generation Details
Here we discuss details of the data engines fully auto-
matic stage that was used to generate the released SA-1B.
Cropping. Masks were generated from a regular grid of
3232 points on the full image and 20 additional zoomed-
in image crops arising from 2 2 and 44 partially over-
lapping windows using 16 16 and 88 regular point grids,
respectively. The original high-resolution images were used
for cropping (this was the only time we used them). We re-
moved masks that touch the inner boundaries of the crops.
We applied standard greedy box-based NMS (boxes were
used for efﬁciency) in two phases: ﬁrst within each crop and
second across crops. When applying NMS within a crop,
we used the models predicted IoU to rank masks. When
applying NMS across crops, we ranked masks from most
zoomed-in ( i.e., from a 44 crop) to least zoomed-in ( i.e.,
the original image), based on their source crop. In both
cases, we used an NMS threshold of 0.7.
Filtering. We used three ﬁlters to increase mask qual-
ity. First, to keep only conﬁdent masks we ﬁltered by the
models predicted IoU score at a threshold of 88.0. Second,
to keep only stable masks we compared two binary masks
resulting from the same underlying soft mask by threshold-
ing it at different values. We kept the prediction ( i.e., the
binary mask resulting from thresholding logits at 0) only if
the IoU between its pair of -1 and +1 thresholded masks was
equal to or greater than 95.0. Third, we noticed that occa-
sionally an automatic mask would cover the entire image.
These masks were generally uninteresting, and we ﬁltered
them by removing masks that covered 95% or more of an
image. All ﬁltering thresholds were selected to achieve both
a large number of masks and high mask quality as judged by
professional annotators using the method described in 5.
Postprocessing. We observed two error types that are eas-
ily mitigated with postprocessing. First, an estimated 4%
of masks include small, spurious components. To address
these, we removed connected components with area lessthan 100 pixels (including removing entire masks if the
largest component is below this threshold). Second, another
estimated 4% of masks include small, spurious holes. To
address these, we ﬁlled holes with area less than 100 pixels.
Holes were identiﬁed as components of inverted masks.
Automatic mask generation model. We trained a special
version of SAM for fully automatic mask generation that
sacriﬁces some inference speed for improved mask gener-
ation properties. We note the differences between our de-
fault SAM and the one used for data generation here: it
was trained on manual and semi-automatic data only, it was
trained for longer (177656 iterations instead of 90k) with
large-scale jitter data augmentation [40], simulated interac-
tive training used only point and mask prompts (no boxes)
and sampled only 4 points per mask during training (reduc-
ing from our default of 9 to 4 sped up training iterations
and had no impact on 1-point performance, though it would
harm mIoU if evaluating with more points), and ﬁnally the
mask decoder used 3 layers instead of 2.
SA-1B examples. We show SA-1B samples in Fig. 2. For
more examples, please see our dataset explorer.
C. RAI Additional Details
Inferring geographic information for SA-1B. While the
images in SA-1B are not geo-tagged, each image has a cap-
tion describing its contents and where it was taken. We infer
approximate image geo-locations from these captions using
an Elmo-based named entity recognition model [78]. Each
extracted location entity is mapped to every matching coun-
try, province, and city. Captions are mapped to a single
country by ﬁrst considering the matching countries, then
provinces, and ﬁnally cities. We note that there are ambigu-
ities and potential for biases with this method ( e.g., Geor-
gia may refer to the country or the US state). As such, we
use the extracted locations to analyze the dataset as a whole,
but do not release the inferred locations. The captions will
not be released publicly as required by the image provider.
Inferring geographic information for COCO and Open
Images. The COCO [66] and Open Images [60] datasets
do not provide geo-locations. Following [29], we retrieve
geographic metadata using the Flickr API. We retrieved
locations for 24% of the COCO training set (19,562 im-
ages) and for Open Images we retrieved 18% of the train-
ing set (493,517 images, after only considering images with
masks). We note that the geographic information is approx-
imate, and the sample of images with this information may
not fully match the full dataset distribution.
Inferring income information. We use each images in-
ferred country to look up its income level using the levels
deﬁned by The World Bank [98]. We collapse the upper-
middle and lower-middle levels into a single middle level.
18mIoU at
1 point 3 points
perceived gender presentation
feminine 76.3 1.1 90.70.5
masculine 81.0 1.2 92.30.4mIoU at
1 point 3 points
perceived age group
older 81.9 3.8 92.81.6
middle 78.2 0.8 91.30.3
young 77.3 2.7 91.50.9
Table 6: SAMs performance segmenting clothing across
perceived gender presentation and age group. The intervals
for perceived gender are disjoint, with mIoU for masculine
being higher. Conﬁdence intervals for age group overlap.
Fairness in segmenting people. To investigate SAMs fair-
ness at segmenting people we use the More Inclusive Anno-
tations for People (MIAP) [87] test set annotations for Open
Images [60], which allows us to compare SAMs perfor-
mance across perceived gender presentation and perceived
age group. MIAP provides box annotations, while we need
ground truth masks for this analysis. To get ground truth
masks, we select each person-category mask from Open
Images if its corresponding bounding box is within a 1%
margin (based on relative box side lengths) of an annotated
bounding box in MIAP, resulting in 3.9k masks.
Fairness in segmenting clothing. We extend our analysis
from 6 to clothing segmentation. We look at SAMs per-
formance on clothing relative to the attributes of those wear-
ing the clothes. We use all 6.5k ground truth masks from
Open Images that have a category under the clothing super-
class and reside within a person box from MIAP. In Table 6
we compare performance across perceived gender presenta-
tion and age group. We ﬁnd that SAM is better at segment-
ing clothing on those who present predominantly mascu-
line, with disjoint 95% conﬁdence intervals. The gap closes
when moving from 1 to 3 point evaluation. Differences for
perceived age group are not signiﬁcant. Our results indicate
there is a bias when segmenting clothing across perceived
gender presentation with a one point prompt, and we en-
courage users of SAM to be mindful of this limitation.
D. Experiment Implementation Details
D.1. Zero-Shot Single Point Valid Mask Evaluation
Datasets. We built a new segmentation benchmark to eval-
uate the zero-shot transfer capabilities of our model using a
suite of 23 diverse segmentation datasets from prior work.
A description of each dataset is given in Table 7. For exam-
ples, see main text Fig. 8. This suite covers a range of do-
mains including egocentric [34, 28, 113], microscopy [12],
X-ray [104], underwater [52, 100], aerial [17], simula-
tion [86], driving [25], and painting [24] images. For ef-
ﬁcient evaluation we subsampled datasets with more than
15k masks. Speciﬁcally, we randomly picked images so
that the total number of masks in the sampled images was
10k. We blurred faces of people in all the datasets.Point sampling. Our default point sampling follows stan-
dard practice in interactive segmentation [109, 64, 92]. The
ﬁrst point is chosen deterministically as the point farthest
from the object boundary. Each subsequent point is the
farthest from the boundary of the error region between
ground truth and the previous prediction. Some experiments
(where speciﬁed) use a more challenging sampling strategy
in which the ﬁrst point is a random point, rather than a deter-
ministically selected center point. Each subsequent point
is selected as described above. This setting better reﬂects
use cases in which the ﬁrst point is not reliably near the
center of the mask, such as prompting from eye gaze.
Evaluation. We measure IoU between a prediction after
Npoint prompts and a ground truth mask, where N=
f1;2;3;5;9gand points are sampled iteratively with either
of the strategies described above. The per-dataset mIoU is
the per-mask IoU averaged across all objects in the dataset.
Finally, we report the top-line metric by averaging the per-
dataset mIoUs across all 23 datasets. Our evaluation differs
from the standard interactive segmentation evaluation pro-
tocol which measures the average number of points needed
to achieveX% IoU, with up to 20 points. We focus on pre-
dictions after just one, or possibly a few points, since many
of our use cases involve a single or very few prompts. Given
our application focus, which requires real-time prompt pro-
cessing, we expect the best interactive segmentation models
to outperform SAM when using a large number of points.
Baselines. We use three recent strong interactive base-
lines: RITM [92], FocalClick [18], and SimpleClick [67].
For each, we use the largest models trained on the broad-
est datasets publicly released by the authors. For RITM,
we use HRNet32 IT-M trained on the combination of
COCO [66] and LVIS [44] introduced by the authors.
For FocalClick, we use SegFormerB3-S2 trained on a
combined dataset that includes 8 different segmentation
datasets [18]. For SimpleClick, we use ViT-H448 trained
on a combination of COCO and LVIS. We follow the sug-
gested default strategies for data pre-processing ( i.e., data
augmentations or image resizing) and do not change or
adapt any parameters for our evaluation. In our experi-
ments, we observe that RITM outperforms other baselines
on our 23 dataset suite with 1 point evaluation. Therefore,
we use RITM as the default baseline. When evaluating with
more points we report results for all baselines.
Single point ambiguity and oracle evaluation. In addition
to IoU after Npoints prompts, we report SAMs oracle
performance at 1 point by evaluating the predicted mask that
best matches ground truth from amongst SAMs three pre-
dictions (rather than using the one that SAM itself ranks
ﬁrst, as we do by default). This protocol addresses possible
single point prompt ambiguity by relaxing the requirement
to guess the one right mask among several valid objects.
19datasetabbreviation
& linkimage
typedescriptionmask
typesource split# images
sampled# masks
sampled
Plant Phenotyping Datasets
Leaf Segmentation [74]PPDLS Plants Leaf segmentation for images of tobacco and ara plants. Instance N/A 182 2347
BBBC038v1 from Broad
Bioimage Benchmark
Collection [12]BBBC038v1 MicroscopyBiological images of cells in a variety of settings testing
robustness in nuclei segmentation.Instance Train 227 10506
Dataset fOr bOuldeRs
Segmentation [80]DOORS BouldersSegmentation masks of single boulders positioned on the
surface of a spherical mesh.Instance DS1 10000 10000
TimberSeg 1.0 [38] TimberSeg LogsSegmentation masks of individual logs in piles of timber in
various environments and conditions. Images are taken from
an operators point-of-view.Instance N/A 220 2487
Northumberland Dolphin
Dataset 2020 [100]NDD20 UnderwaterSegmentation masks of two different dolphin species in
images taken above and under water.Instance N/A 4402 6100
Large V ocabulary Instance
Segmentation [44]LVIS ScenesAdditional annotations for the COCO [66] dataset to enable
the study of long-tailed object detection and segmentation.Instance Validation (v0.5) 945 9642
STREETS [91] STREETSTrafﬁc
cameraSegmentation masks of cars in trafﬁc camera footage. Instance N/A 819 9854
ZeroWaste-f [6] ZeroWaste-f RecyclingSegmentation masks in cluttered scenes of deformed
recycling waste.Instance Train 2947 6155
iShape [111] iShapeIrregular
shapesSegmentation masks of irregular shapes like antennas, logs,
fences, and hangers.Instance Validation 754 9742
ADE20K [117] ADE20K ScenesObject and part segmentation masks for images from
SUN [107] and Places [116] datasets.Instance Validation 302 10128
Occluded Video Instance
Segmentation [81]OVIS OcclusionsInstance segmentation masks in videos, focusing on objects
that are occluded.Instance Train 2044 10011
Hypersim [86] Hypersim SimulationPhotorealistic synthetic dataset of indoor scenes with instance
masks.InstanceEvermotion archinteriors
volumes 1-55 excluding
20,25,40,49338 9445
Night and Day Instance
Segmented Park [22, 23]NDISPark Parking lotsImages of parking lots from video footage taken at day and
night during different weather conditions and camera angles
for vehicle segmentation.Instance Train 111 2577
EPIC-KITCHENS
VISOR [28, 27]VISOR EgocentricSegmentation masks for hands and active objects in
ego-centric video from the cooking dataset
EPIC-KITCHENS [27].Instance Validation 1864 10141
Plittersdorf dataset [46] PlittersdorfStereo
imagesSegmentation masks of wildlife in images taken with the
SOCRATES stereo camera trap.Instance Train, validation, test 187 546
Egocentric Hand-Object
Segmentation [113]EgoHOS EgocentricFine-grained egocentric hand-object segmentation dataset.
Dataset contains mask annotations for existing datasets.InstanceTrain (including only
Ego4D [43] and
THU-READ [97, 96])2940 9961
InstanceBuilding 2D [17] IBD DronesHigh-resolution drone UA V images annotated with roof
instance segmentation masks.Instance Train (2D annotations) 467 11953
WoodScape [112] WoodScapeFisheye
drivingFisheye driving dataset with segmentation masks. Images are
taken from four surround-view cameras.Instance Set 1 107 10266
Cityscapes [25] Cityscapes Driving Stereo video of street scenes with segmentation masks. Panoptic Validation 293 9973
PIDray [104] PIDRay X-raySegmentation masks of prohibited items in X-ray images of
baggage.Instance Test (hard) 3733 8892
Diverse Realism in Art
Movements [24]DRAM PaintingsDomain adaptation dataset for semantic segmentation of art
paintings.Semantic Test 718 1179
TrashCan [52] TrashCan UnderwaterSegmentation masks of trash in images taken by underwater
ROVs. Images are sourced from the J-EDI [69] dataset.Instance Train (instance task) 5936 9540
Georgia Tech Egocentric
Activity Datasets [34, 63]GTEA EgocentricVideos are composed of four different subjects performing
seven types of daily activities with segmentation masks of
hands.InstanceTrain (segmenting hands
task)652 1208
Table 7: Segmentation datasets used to evaluate zero-shot segmentation with point prompts. The 23 datasets cover a broad
range of domains; see column image type. To make our evaluation efﬁcient, we subsample datasets that have more than
15k masks. Speciﬁcally, we randomly sampled images so that the total number of masks in the images is 10k.
20image ground truth SAM image ground truth SAM
Figure 15: Additional visualizations of zero-shot edge predictions on BSDS500. Recall that SAM was not trained to predict
edge maps and did not have access to BSDS images and annotations during training.
D.2. Zero-Shot Edge Detection
Dataset and metrics. We perform zero-shot edge detection
experiments on BSDS500 [72, 3]. The ground truth for each
image comes from the manual annotations of ﬁve different
subjects. We report results on the 200 image test subset
using the four standard metrics for edge detection [3, 32]:
optimal dataset scale (ODS), optimal image scale (OIS), av-
erage precision (AP), and recall at 50% precision (R50).
Method. For zero-shot transfer, we use a simpliﬁed ver-
sion of our automatic mask generation pipeline. We prompt
SAM with a 1616 regular grid of foreground points,
which yields 768 predicted masks (three per point). We do
not ﬁlter by predicted IoU or stability. Redundant masks
are removed by NMS. Then we apply a Sobel ﬁlter to the
remaining masks unthresholded probability maps and set
values to zero if they do not intersect with the outer bound-
ary pixels of a mask. Finally, we take a pixel-wise max over
all the predictions, linearly normalize the result to [0,1], and
apply edge NMS [13] to thin the edges.
Visualizations. In Fig. 15, we show additional examples
of zero-shot edge predictions from SAM. These qualitative
examples further illustrate how SAM tends to output sensi-
ble edge maps, despite not being trained for edge detection.
We see that the edges can align well with the human anno-
tations. Although, as previously mentioned, since SAM is
not trained for edge detection it does not learn the biases of
the BSDS500 dataset and often outputs more edges than are
present in the ground truth annotations.
D.3. Zero-Shot Object Proposals
Dataset and metrics. We report the standard average recall
(AR) metric for masks at 1000 proposals on the LVIS v1
validation set [44]. Since LVIS has high-quality masks for
1203 object classes, it provides a challenging test for ob-
ject proposal generation. We focus on AR@1000 due to the
open-world nature of our model, which will likely produce
many valid masks outside even the 1203 classes in LVIS. To
measure performance on frequent, common, and rare cate-gories, we use AR@1000 but measured against a ground
truth set containing just the corresponding LVIS categories.
Baseline. We use cascade ViTDet-H as a baseline, the
strongest model from [62] by AP on LVIS. As noted in the
main text, an object detector trained in-domain can game
AR [16] and is expected to be a stronger baseline than other
models that focus on open-world proposals or segmenta-
tion [58, 105]. To produce 1000 proposals, we disable score
thresholding in the three cascade stages and as raise the
maximum number of predictions per stage to 1000.
Method. We use a modiﬁed version of SAMs automatic
mask generation pipeline for zero-shot transfer. First, to
make inference time comparable to that of ViTDet we do
not process image crops. Second, we remove ﬁltering by
predicted IoU and stability. This leaves two tunable param-
eters to get 1000 masks per image: the input point grid and
the NMS threshold duplicate mask suppression. We choose
a 6464 point grid and an NMS threshold of 0.9, which
produces 900 masks per image on average. At evaluation,
if greater than 1000 masks have been proposed in an im-
age, they are ranked by the average of their conﬁdence and
stability scores, then truncated to the top 1000 proposals.
We hypothesize that SAMs ability to output multiple
masks is especially valuable for this task, since recall should
beneﬁt from proposals generated at multiple scales from
a single input point. To test this, we compare to an ab-
lated version SAM that only outputs a single mask instead
of three (SAM - single-output). Since this model produces
fewer masks, we further increase the number of points sam-
pled and NMS threshold to 128 128 and 0.95, respectively,
obtaining 950 masks per image on average. Additionally,
single-output SAM does not produce the IoU score used
to rank masks for NMS in the automatic mask generation
pipeline, so instead masks are ranked randomly. Testing
suggests this has similar performance to more sophisticated
methods of ranking masks, such as using the max logit value
of the mask as a proxy for model conﬁdence.
21ground truth ViTDet SAM ground truth ViTDet SAM
Figure 16: Zero-shot instance segmentation on LVIS v1. SAM produces higher quality masks than ViTDet. As a zero-shot
model, SAM does not have the opportunity to learn speciﬁc training data biases; see top-right as an example where SAM
makes a modal prediction, whereas the ground truth in LVIS is amodal given that mask annotations in LVIS have no holes.
D.4. Zero-Shot Instance Segmentation
Method. For zero-shot instance segmentation, we prompt
SAM with the boxes output by a fully-supervised ViTDet-H
on COCO and LVIS v1 validation splits. We apply an ad-
ditional mask reﬁnement iteration by feeding the most con-
ﬁdent predicted mask, together with the box prompt, back
to the mask decoder to produce the ﬁnal prediction. We
show zero-shot instance segmentations predicted on LVIS
in Fig. 16. Compared to ViTDet, SAM tends to produce
higher quality masks with cleaner boundaries. We conﬁrm
this observation with human studies in 7.4. Note that as a
zero-shot model, SAM is not able to learn annotation biases
in a dataset. For instance, we see that SAM makes a valid
modal prediction for the plate, whereas LVIS masks cannot
contain holes by design so the plate is annotated amodally.
D.5. Zero-Shot Text-to-Mask
Model and training. We use the largest publicly available
CLIP model [82] ( ViT-L/14@336px ) to compute text
and image embeddings, which we 2normalize prior to use.
To train SAM, we use masks from the ﬁrst two stages of our
data engine. Moreover, we discard all masks with an area
smaller than 1002pixels. We train this model with large-
scale jitter [40] for 120k iterations with batch size 128. All
other training parameters follow our default settings.
Generating training prompts. To extract an input prompt
we ﬁrst expand the bounding box around each mask by a
random factor from 1 to 2, square-crop the expanded
box to maintain its aspect ratio, and resize it to 336 336
pixels. Before feeding the crop to the CLIP image encoder,
with 50% probability we zero-out pixels outside the mask.
To ensure the embedding focuses on the object, we use
masked attention in the last layer to restrict attention from
the output token to the image positions inside the mask. Fi-
nally, our prompt is the output token embedding. For train-
ing we supply the CLIP-based prompt ﬁrst, followed by ad-
ditional iterative point prompts to reﬁne the prediction.
Figure 17: Visualization of thresholding the similarities of
mask embeddings from SAMs latent space. A query is in-
dicated by the magenta box; top row shows matches at a low
threshold, bottom row at a high threshold. The most similar
mask embeddings in the same image can often be seman-
tically similar to the query mask embedding, even though
SAM is not trained with explicit semantic supervision.
Inference. During inference we use the CLIP text encoder
without any modiﬁcations to create a prompt for SAM. We
rely on the fact that text and image embeddings are aligned
by CLIP, which allows us to train without any explicit text
supervision while using text-based prompts for inference.
D.6. Probing the Latent Space of SAM
Finally, we perform an initial investigation to qualita-
tively probe the latent space learned by SAM. In particu-
lar, we are interested in whether SAM is able to capture any
semantics in its representation even though is not trained
with explicit semantic supervision. To do so, we compute
mask embeddings by extracting an image embedding from
SAM from an image crop around a mask and its horizon-
tally ﬂipped version, multiplying the image embedding by
the binary mask, and averaging over spatial locations. In
Fig. 17, we show 3 examples of a query mask and similar
masks (in the latent space) in the same image. We observe
22that the nearest neighbors for each query show some, albeit
imperfect, shape and semantic similarity. Although these
results are preliminary, they indicate that the representations
from SAM may be useful for a variety of purposes, such as
further data labeling, understanding the contents of datasets,
or as features for downstream tasks.
E. Human Study Experimental Design
Here we describe details of the human study used to eval-
uate mask quality in 7.1 and 7.4. The purpose of the
human study is to address two limitations of using IoU to
ground truth as a measure of predicted mask quality. The
ﬁrst limitation is that, for ambiguous inputs such as a single
point, the model may be strongly penalized for returning a
valid mask of a different object than the ground truth. The
second limitation is that ground truth masks may include
various biases, such as systematic errors in the edge qual-
ity or decisions to modally or amodally segment occluding
objects. A model trained in-domain can learn these biases
and obtain a higher IoU without necessarily producing bet-
ter masks. Human review can obtain a measure of mask
quality independent of an underlying ground truth mask in
order to alleviate these issues.
Models. For single-point evaluation, we use RITM [92],
single-output SAM, and SAM to test two hypotheses. First,
we hypothesize that SAM produces visually higher quality
masks than baseline interactive segmentation models when
given a single point, even when metrics such as IoU with
ground truth do not reveal this. Second, we hypothesize
that SAMs ability to disambiguate masks improves mask
quality for single point inputs, since single output SAM may
return masks that average over ambiguous masks.
For instance segmentation experiments, we evaluate cas-
cade ViTDet-H [62] and SAM in order to test the hypothesis
that SAM produces visually higher quality masks, even if it
obtains a lower AP due to the inability to learn speciﬁc an-
notation biases of the validation dataset.
Datasets. For single-point experiments, we select 7 datasets
from our set of 23 datasets, since the full suite is too large
for human review. We choose LVIS v0.5 [17], VISOR [28,
27], DRAM [24], IBD [17], NDD20 [100], OVIS [81], and
iShape [111], which provide a diverse collection of images,
including scene-level, ego-centric, drawn, overhead, under-
water, and synthetic imagery. Additionally, this set includes
datasets both where SAM outperforms RITM with IoU met-
rics and vice-versa. For instance segmentation experiments,
we use the LVIS v1 validation set, allowing for direct com-
parison to ViTDet, which was trained on LVIS.
Methodology. We presented masks generated by the mod-
els to professional annotators and asked them to rate each
mask using provided guidelines (see G for the complete
guidelines). Annotators were sourced from the same com-pany that collected manually annotated masks for the data
engine. An annotator was provided access to an image, the
predicted mask of a single model, and the input to the model
(either a single point or single box) and asked to judge the
mask on three criterion: Does the mask correspond to a
valid object? Does the mask have a clean boundary? and
Does the mask correspond to the input? They then submit-
ted a rating from 1-10 indicating the overall mask quality.
A score of 1 indicates a mask that corresponds to no ob-
ject at all; a low score (2-4) indicates that the mask has huge
errors, such including huge regions of other objects or hav-
ing large areas of nonsensical boundaries; a middle score
(5-6) indicates masks that are mostly sensible but still have
signiﬁcant semantic or boundary errors; a high score (7-
9) indicates masks with only minor boundary errors; and a
score of 10 is for masks with no visible errors. Annotators
were provided with ﬁve different views, each designed to
help identify different error types.
For single point experiments, 1000 masks per dataset
were selected randomly from the same subsets used for
benchmarking zero-shot interactive segmentation (see D.1
for details on these subsets). The model input was the cen-
termost point, calculated as the largest value of the distance
transform from the edge of the mask. For instance seg-
mentation experiments, 1000 masks were selected from the
LVIS v1 validation set, and the model input was the LVIS
ground truth box. In all experiments, masks with a size
smaller than 242pixels were excluded from sampling, to
prevent showing raters a mask that was too small to judge
accurately. For both memory and display reasons, large im-
ages were rescaled to have a max side-length of 2000 before
predicting a mask. In all experiments, the same inputs were
fed to each model to produce a predicted mask.
For comparison, the ground truth masks from each
dataset were also submitted for rating. For single-point
experiments, this gave 4000 total rating jobs per dataset
(1000 masks each for RITM, SAM single-output, SAM,
and ground truth); for instance segmentation experiments,
it gave 3000 total jobs (ViTDet, SAM, and ground truth).
For each dataset, these jobs were inserted with random
ordering into a queue from which 30 annotators drew jobs.
In initial testing of the review study, we provided each job to
ﬁve different annotators and found reasonable consistency
in scores: the average standard deviation in score over the
ﬁve annotators was 0.83. Additionally, the annotation com-
pany deployed quality assurance testers who spot checked
a fraction of results for extreme departures from the guide-
lines. Thus for our experiments each job ( i.e., rating one
mask in one image) was completed by only a single anno-
tator. Average time spent per annotator per job was 90 sec-
onds, longer than our initial target of 30 seconds, but still
sufﬁciently fast to collect a large number of ratings on each
of the 7 selected datasets.
231 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings6.5  0.15, RITM
7.7  0.12, SAM - single output8.1  0.10, SAM
8.5  0.09, GT(a) LVIS v0.5 [17]
1 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings6.3  0.16, RITM
7.5  0.13, SAM - single output8.3  0.09, SAM
8.5  0.13, GT (b) VISOR [28, 27]
1 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings5.9  0.14, RITM
6.8  0.15, SAM - single output7.7  0.13, SAM
8.0  0.15, GT
(c) DRAM [24]
1 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings7.1  0.12, RITM
7.9  0.11, SAM - single output8.3  0.08, SAM
8.4  0.09, GT (d) IBD [17]
1 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings6.4  0.17, RITM
8.2  0.11, SAM - single output8.6  0.10, SAM
8.9  0.06, GT
(e) NDD20 [100]
1 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings6.1  0.15, RITM
7.7  0.12, SAM - single output7.2  0.13, SAM
8.8  0.09, GT (f) OVIS [81]
1 2 3 4 5 6 7 8 9 10
Mask quality rating02040Percent of ratings4.9  0.16, RITM
6.2  0.17, SAM - single output7.1  0.15, SAM
9.3  0.06, GT
(g) iShape [111]
Figure 18: Mask quality rating distributions by dataset from our human evaluation study.
SAM>baseline SAM>SAM single out.
dataset p-value CI99() p-value CI99()
point input (RITM [92] baseline):
LVIS v0.5 [44] 4e-69 (1.40, 1.84) 2e-11 (0.29, 0.64)
VISOR [28, 27] 7e-98 (1.81, 2.24) 7e-26 (0.58, 0.94)
DRAM [24] 1e-76 (1.54, 2.00) 2e-24 (0.62, 1.03)
IBD [17] 2e-57 (1.03, 1.39) 1e-15 (0.32, 0.62)
NDD20 [100] 2e-86 (1.88, 2.37) 5e-08 (0.19, 0.55)
OVIS [81] 2e-64 (1.38, 1.84) 3e-10 (0.27, 0.63)
iShape [111] 2e-88 (1.97, 2.47) 7e-23 (0.65, 1.10)
box input (ViTDet-H [62] baseline):
LVIS v1 [44] 2e-05 (0.11, 0.42) N/A N/A
Table 8: Statistical tests showing signiﬁcance that SAM has
higher mask quality ratings than baseline and single-output
SAM. P-values are calculated by paired t-test, while conﬁ-
dence intervals for the difference in mean scores are calcu-
lated by paired bootstrap on 10k samples. All p-values are
signiﬁcant, and all conﬁdence intervals exclude zero.
Results. Fig. 18 shows histograms over ratings for each
dataset in the single-point experiments. We run statisticaltests for two hypotheses: (1) that SAM gets higher scores
than the baseline model (RITM or ViTDet) and (2) that
SAM gets higher scores than single-output SAM. P-values
are calculated via a paired t-test on the means of the model
scores, which we supplement with a paired bootstrap test on
10k samples to ﬁnd the 99% conﬁdence interval for the dif-
ference of means. Table 8 shows p-values and conﬁdence
intervals for these tests. All statistical tests are strongly sig-
niﬁcant, and all conﬁdence intervals exclude zero.
For instance segmentation, Fig. 11 of the main text
shows the histogram for ratings. To compare to COCO
ground truth, we additionally include 794 ratings of COCO
ground truth masks that were collected during our testing of
the human review process. These masks were presented to
raters using an identical setup as the LVIS results. For fair
comparison, results for LVIS in Fig. 11 were subsampled
to the same 794 inputs for each model and ground truth.
For Table 8, the full 1000 ratings are used to run statistical
tests, which show that SAMs mask quality improvement
over ViTDet is statistically signiﬁcant.
24F. Dataset, Annotation, and Model Cards
In F.1 we provide a Dataset Card for SA-1B, follow-
ing [39], in a list of questions and answers. Next, we pro-
vide a Data Annotation Card in F.2 for the ﬁrst two stages
of our data engine described in 4, following CrowdWork-
Sheets [30], again as a list of questions and answers. We
provide a Model Card following [75] in Table 9.
F.1. Dataset Card for SA-1B
Motivation
1.For what purpose was the dataset created? Was there a speciﬁc task in
mind? Was there a speciﬁc gap that needed to be ﬁlled? Please provide a
description. The contributions of our dataset to the vision community are
fourfold: (1) We release a dataset of 11M images and 1.1B masks, by far the
largest segmentation dataset to date. (2) The dataset we release is privacy
protecting: we have blurred faces and license plates in all images. (3) The
dataset is licensed under a broad set of terms of use which can be found
at https://ai.facebook.com/datasets/segment-anything. (4) The data is more
geographically diverse than its predecessors, and we hope it will bring the
community one step closer to creating fairer and more equitable models.
2.Who created the dataset ( e.g., which team, research group) and on behalf
of which entity ( e.g., company, institution, organization)? The dataset was
created by the FAIR team of Meta AI. The underlying images were collected
and licensed from a third party photo company.
3.Who funded the creation of the dataset? If there is an associated grant,
please provide the name of the grantor and the grant name and number.
Meta AI funded the creation of the dataset.
4.Any other comments? No.
Composition
1.What do the instances that comprise the dataset represent ( e.g., documents,
photos, people, countries)? Are there multiple types of instances ( e.g.,
movies, users, and ratings; people and interactions between them; nodes
and edges)? Please provide a description. All of the instances in the dataset
are photos. The photos vary in subject matter; common themes of the photo
include: locations, objects, scenes. All of the photos are distinct, however
there are some sets of photos that were taken of the same subject matter.
2.How many instances are there in total (of each type, if appropriate)? There
are 11 million images.
3.Does the dataset contain all possible instances or is it a sample (not nec-
essarily random) of instances from a larger set? If the dataset is a sample,
then what is the larger set? Is the sample representative of the larger set
(e.g., geographic coverage)? If so, please describe how this representa-
tiveness was validated/veriﬁed. If it is not representative of the larger set,
please describe why not ( e.g., to cover a more diverse range of instances,
because instances were withheld or unavailable). The dataset is composed
of images licensed from a photo provider. The dataset contains all instances
licensed. The images are photos, i.e. not artwork, although there are a few
exceptions. The dataset includes all generated masks for each image in the
dataset. We withheld 2k randomly selected images for testing purposes.
4.What data does each instance consist of? Raw data ( e.g., unprocessed
text or images) or features? In either case, please provide a description.
Each instance in the dataset is an image. The images were processed to blur
faces and license plates to protect the identities of those in the image.
5.Is there a label or target associated with each instance? If so, please provide
a description. Each image is annotated with masks. There are no categories
or text associated with the masks. The average image has 100 masks, and
there are 1.1B masks in total.
6.Is any information missing from individual instances? If so, please provide
a description, explaining why this information is missing ( e.g., because it
was unavailable). This does not include intentionally removed information,
but might include, e.g., redacted text. Yes. Each image is accompanied by
a short caption that describes the content and place of the photo in a free
form text. Per our agreement with the photo provider we are not allowed to
release these captions. However, we use them in our paper to analyze the
geographical distribution of the dataset.7.Are relationships between individual instances made explicit ( e.g., users
movie ratings, social network links)? If so, please describe how these rela-
tionships are made explicit. No, there are no known relationships between
instances in the dataset.
8.Are there any errors, sources of noise, or redundancies in the dataset? If
so, please provide a description. Errors: The masks are generated by a
segmentation model, so there may be errors or inconsistencies in the masks.
Redundancies: While no two images are the same, there are instances of
images of the same subject taken close together in time.
9.Is the dataset self-contained, or does it link to or otherwise rely on external
resources ( e.g., websites, tweets, other datasets)? If it links to or relies on
external resources, a) are there guarantees that they will exist, and remain
constant, over time; b) are there ofﬁcial archival versions of the complete
dataset ( i.e., including the external resources as they existed at the time
the dataset was created); c) are there any restrictions ( e.g., licenses, fees)
associated with any of the external resources that might apply to a dataset
consumer? Please provide descriptions of all external resources and any
restrictions associated with them, as well as links or other access points, as
appropriate. The dataset is self-contained.
10. Does the dataset contain data that might be considered conﬁdential ( e.g.,
data that is protected by legal privilege or by doctor-patient conﬁdentiality,
data that includes the content of individuals non-public communications)?
If so, please provide a description. No.
11. Does the dataset contain data that, if viewed directly, might be offensive,
insulting, threatening, or might otherwise cause anxiety? If so, please de-
scribe why. We have two safety measures to prevent objectionable content:
(1) Photos are licensed from a photo provider and had to meet the terms of
service of the photo provider. We requested that all objectionable content
be ﬁltered from the images we licensed. (2) If a user observes objectionable
image(s) in the dataset, we invite them to report the image(s) at segment-
anything@meta.com for removal. Despite the measures taken, we observe
that a small portion of images contains scenes of protests or other gatherings
that focus on a diverse spectrum of religious beliefs or political opinions that
may be offensive. We were not able to produce a ﬁltering strategy that re-
moves all such images and rely on users to report this type of content.
12. Does the dataset identify any subpopulations ( e.g., by age, gender)? If so,
please describe how these subpopulations are identiﬁed and provide a de-
scription of their respective distributions within the dataset. The dataset
does not identify any subpopulations of the people in the photos.
13. Is it possible to identify individuals ( i.e., one or more natural persons), ei-
ther directly or indirectly ( i.e., in combination with other data) from the
dataset? If so, please describe how. No. Images were subjected to a face
blurring model to remove any personally identiﬁable information. If a user
observes any anonymization issue, we invite them to report the issue and
the image id(s) at segment-anything@meta.com.
14. Does the dataset contain data that might be considered sensitive in any way
(e.g., data that reveals race or ethnic origins, sexual orientations, religious
beliefs, political opinions or union memberships, or locations; ﬁnancial or
health data; biometric or genetic data; forms of government identiﬁcation,
such as social security numbers; criminal history)? If so, please provide
a description. The dataset contains scenes of protests, or other gatherings
that may suggest religious beliefs, political opinions or union memberships.
However, the faces of all people in the dataset have been anonymized via
facial blurring, so it is not possible to identify any person in the dataset.
15. Any other comments? No.
Collection Process
1.How was the data associated with each instance acquired? Was the data
directly observable ( e.g., raw text, movie ratings), reported by subjects ( e.g.,
survey responses), or indirectly inferred/derived from other data ( e.g., part-
of-speech tags, model-based guesses for age or language)? If the data was
reported by subjects or indirectly inferred/derived from other data, was the
data validated/veriﬁed? If so, please describe how. The released masks
associated with each image were automatically inferred by our segmentation
model, SAM. The masks that were collected using model-assisted manual
annotation will not be released. Quality was validated as described in 5.
2.What mechanisms or procedures were used to collect the data ( e.g., hard-
ware apparatuses or sensors, manual human curation, software programs,
software APIs)? How were these mechanisms or procedures validated? The
images in the dataset are licensed from an image provider. They are all pho-
tos taken by photographers with different cameras.
253.If the dataset is a sample from a larger set, what was the sampling strategy
(e.g., deterministic, probabilistic with speciﬁc sampling probabilities)? We
withheld 2k randomly selected images for testing purposes. The rest of
the licensed images are included in the dataset.
4.Who was involved in the data collection process ( e.g., students, crowdwork-
ers, contractors) and how were they compensated ( e.g., how much were
crowdworkers paid)? The released masks were automatically inferred by
SAM. For details on our model-assisted manual annotation process see our
Data Annotation Card in F.2. Note these masks will not be released.
5.Over what timeframe was the data collected? Does this timeframe match
the creation timeframe of the data associated with the instances ( e.g., recent
crawl of old news articles)? If not, please describe the timeframe in which
the data associated with the instances was created. The licensed photos
vary in their date taken over a wide range of years up to 2022.
6.Were any ethical review processes conducted ( e.g., by an institutional re-
view board)? If so, please provide a description of these review processes,
including the outcomes, as well as a link or other access point to any sup-
porting documentation. If the dataset does not relate to people, you may skip
the remaining questions in this section. We underwent an internal privacy
review to evaluate and determine how to mitigate any potential risks with
respect to the privacy of people in the photos. Blurring faces and license
plates protects the privacy of the people in the photos.
7.Did you collect the data from the individuals in question directly, or obtain
it via third parties or other sources ( e.g., websites)? We licensed the data
from a third party photo provider.
8.Were the individuals in question notiﬁed about the data collection? If so,
please describe (or show with screenshots or other information) how no-
tice was provided, and provide a link or other access point to, or other-
wise reproduce, the exact language of the notiﬁcation itself. The images
are licensed from a third party who provided appropriate representations
regarding the collection of any notices and consents as required from indi-
viduals. In addition, all identiﬁable information ( e.g. faces, license plates)
was blurred. Under the terms of the dataset license it is prohibited to attempt
to identify or associate an image with a particular individual.
9.Did the individuals in question consent to the collection and use of their
data? If so, please describe (or show with screenshots or other informa-
tion) how consent was requested and provided, and provide a link or other
access point to, or otherwise reproduce, the exact language to which the
individuals consented. The images are licensed from a third party who pro-
vided appropriate representations regarding the collection of any notices and
consents as required from individuals. In addition, all identiﬁable informa-
tion ( e.g. faces, license plates) was blurred from all images. For avoidance
of doubt, under the terms of the dataset license it is prohibited to attempt to
identify or associate an image with a particular individual.
10. If consent was obtained, were the consenting individuals provided with a
mechanism to revoke their consent in the future or for certain uses? If
so, please provide a description, as well as a link or other access point
to the mechanism (if appropriate). We invite users to report at segment-
anything@meta.com for image(s) removal.
11. Has an analysis of the potential impact of the dataset and its use on data
subjects ( e.g., a data protection impact analysis) been conducted? If so,
please provide a description of this analysis, including the outcomes, as
well as a link or other access point to any supporting documentation. To
eliminate any potential impact on people whose photos are included in the
dataset, identiﬁable information (faces, license plates) has been blurred.
12. Any other comments? No.
Preprocessing / Cleaning / Labeling
1.Was any preprocessing / cleaning / labeling of the data done ( e.g., dis-
cretization or bucketing, tokenization, part-of-speech tagging, SIFT fea-
ture extraction, removal of instances, processing of missing values)? If so,
please provide a description. If not, you may skip the remaining questions
in this section. We resized the high-resolution licensed images such that
the shorter side is 1500 pixels and only processed the images to remove any
identiﬁable and personal information from the photos (faces, license plates).
2.Was the raw data saved in addition to the preprocessed/cleaned/labeled
data ( e.g., to support unanticipated future uses)? If so, please provide a link
or other access point to the raw data. No, as we removed the data for
safety reasons and to respect privacy, we do not release the unaltered photos.
3.Is the software that was used to preprocess/clean/label the data avail-
able? If so, please provide a link or other access point. We used theRetinaFace [88, 89] model (https://github.com/serengil/retinaface) to detect
faces. The model used to blur license plates has not been made public.
Uses
1.Has the dataset been used for any tasks already? If so, please provide a
description. The dataset was used to train our segmentation model, SAM.
2.Is there a repository that links to any or all papers or systems that use the
dataset? If so, please provide a link or other access point. No. However, all
users of the dataset must cite it, so its use is trackable via citation explorers.
3.What (other) tasks could the dataset be used for? We intend the dataset
to be a large-scale segmentation dataset. However, we invite the research
community to gather additional annotations for the dataset.
4.Is there anything about the composition of the dataset or the way it was
collected and preprocessed/cleaned/labeled that might impact future uses?
For example, is there anything that a dataset consumer might need to know
to avoid uses that could result in unfair treatment of individuals or groups
(e.g., stereotyping, quality of service issues) or other risks or harms ( e.g.,
legal risks, ﬁnancial harms)? If so, please provide a description. Is there
anything a dataset consumer could do to mitigate these risks or harms? We
have an analysis of the approximate geographic and income level coverage
of our dataset in 6. While we believe our dataset to be more representative
than most of the publicly existing datasets at this time, we acknowledge
that we do not have parity across all groups, and we encourage users to be
mindful of potential biases their models have learned using this dataset.
5.Are there tasks for which the dataset should not be used? If so, please pro-
vide a description. Full terms of use for the dataset including prohibited use
cases can be found at https://ai.facebook.com/datasets/segment-anything.
6.Any other comments? No.
Distribution
1.Will the dataset be distributed to third parties outside of the entity ( e.g.,
company, institution, organization) on behalf of which the dataset was cre-
ated? If so, please provide a description. The dataset will be available for
the research community.
2.How will the dataset will be distributed ( e.g., tarball on website, API,
GitHub)? Does the dataset have a digital object identiﬁer (DOI)? The
dataset is available at https://ai.facebook.com/datasets/segment-anything.
3.When will the dataset be distributed? The dataset will be released in 2023.
4.Will the dataset be distributed under a copyright or other intellectual
property (IP) license, and/or under applicable terms of use (ToU)? If
so, please describe this license and/or ToU, and provide a link or other
access point to, or otherwise reproduce, any relevant licensing terms
or ToU, as well as any fees associated with these restrictions. Yes.
The license agreement and terms of use for the dataset can be found at
https://ai.facebook.com/datasets/segment-anything. Users must agree to the
terms of use before downloading or using the dataset.
5.Have any third parties imposed IP-based or other restrictions on the data
associated with the instances? If so, please describe these restrictions, and
provide a link or other access point to, or otherwise reproduce, any relevant
licensing terms, as well as any fees associated with these restrictions. Full
terms of use and restrictions on use of the SA-1B dataset can be found at
https://ai.facebook.com/datasets/segment-anything.
6.Do any export controls or other regulatory restrictions apply to the dataset
or to individual instances? If so, please describe these restrictions, and pro-
vide a link or other access point to, or otherwise reproduce, any supporting
documentation. The license and restrictions on use of the SA-1B dataset
can be found at https://ai.facebook.com/datasets/segment-anything.
7.Any other comments? No.
Maintenance
1.Who will be supporting/hosting/maintaining the dataset? The dataset will
be hosted at https://ai.facebook.com/datasets/segment-anything and main-
tained by Meta AI.
2.How can the owner/curator/manager of the dataset be contacted ( e.g., email
address)? Please email segment-anything@meta.com.
3.Is there an erratum? If so, please provide a link or other access point. No.
4.Will the dataset be updated ( e.g., to correct labeling errors, add new in-
stances, delete instances)? If so, please describe how often, by whom, and
how updates will be communicated to dataset consumers ( e.g., mailing list,
26GitHub)? To aid reproducibility of research using SA-1B, the only updates
will be to remove reported images.
5.If the dataset relates to people, are there applicable limits on the retention of
the data associated with the instances ( e.g., were the individuals in question
told that their data would be retained for a ﬁxed period of time and then
deleted)? If so, please describe these limits and explain how they will be
enforced. There are no limits on data retention. We took measures to remove
personally identiﬁable information from any images of people. Users may
report content for potential removal here: segment-anything@meta.com.
6.Will older versions of the dataset continue to be sup-
ported/hosted/maintained? If so, please describe how. If not, please
describe how its obsolescence will be communicated to dataset consumers.
No, as the only updates will be to remove potentially harmful content, we
will not keep older versions with the content.
7.If others want to extend/augment/build on/contribute to the dataset, is there
a mechanism for them to do so? If so, please provide a description. Will
these contributions be validated/veriﬁed? If so, please describe how. If not,
why not? Is there a process for communicating/distributing these contribu-
tions to dataset consumers? If so, please provide a description. We encour-
age users to gather further annotations for SA-1B. Any users who generate
annotations will be liable for hosting and distributing their annotations.
8.Any other comments? No.
F.2. Data Annotation Card
Task Formulation
1.At a high level, what are the subjective aspects of your task? Segmenting
objects present in an image is inherently a subjective task. For instance,
one annotator may segment two boots as one mask, whereas another may
segment each boot separately. Depending on annotatorss skills, the quality
of the mask and the number of masks per image are different between an-
notators. Despite these subjective aspects of the task, we believed efﬁcient
annotation was possible as the data was annotated in a per-mask fashion
with the main focus on the diversity of the data rather than completeness.
2.What assumptions do you make about annotators? Our annotators worked
full time on our annotation task with very small attrition rate. This made
it possible to train the annotators providing feedback and answering their
questions on a regular basis. Speciﬁcally: (1) By giving a clear understand-
ing of the goals of this work and providing clear guidelines, including vi-
suals and video recordings of the tasks, annotators had enough context to
understand and perform the tasks reasonably. (2) Sharing objectives and
key results and meeting weekly with annotators increased the likelihood
that annotators improved annotation quality and quantity over time.
3.How did you choose the speciﬁc wording of your task instructions? What
steps, if any, were taken to verify the clarity of task instructions and wording
for annotators? As our task was annotating images, the annotation guide-
lines included visual examples. Our research team completed 30 annotation
tasks to identify any obvious challenges using the annotation tool, collec-
tively decide how to handle complex cases, and reﬁne the guidelines. The
research team met with the annotators weekly for feedback sessions. Videos
of the research team performing the task were shared live with the annota-
tors, followed by Q&A sessions. Annotators were able to give feedback on
unclear aspects, both during the feedback session and asynchronously.
4.What, if any, risks did your task pose for annotators and were they informed
of the risks prior to engagement with the task? No identiﬁed risks. Images
were ﬁltered for objectionable content prior to the annotation phase.
5.What are the precise instructions that were provided to annotators? We
provide only high-level instructions: Given an image, we aim at segment-
ing every possible object. Annotators generate a mask for every potential
object they can identify. An object can be segmented using our interactive
segmentation tool either by using corrective foreground/background clicks
to add/remove parts of the mask or by drawing a bounding box around the
object. Masks can be reﬁned using pixel-precise tools.
Selecting Annotations
1.Are there certain perspectives that should be privileged? If so, how did you
seek these perspectives out? We chose to work with annotators that have
worked on other vision annotation tasks before.
2.Are there certain perspectives that would be harmful to include? If so, how
did you screen these perspectives out? No.3.Were sociodemographic characteristics used to select annotators for your
task? If so, please detail the process. No.
4.If you have any aggregated socio-demographic statistics about your anno-
tator pool, please describe. Do you have reason to believe that sociode-
mographic characteristics of annotators may have impacted how they an-
notated the data? Why or why not? We worked with 130 annotators. The
annotators were all based in Kenya. We do not believe sociodemographic
characteristics of annotators meaningfully impacted the annotated data.
5.Consider the intended context of use of the dataset and the individuals
and communities that may be impacted by a model trained on this dataset.
Are these communities represented in your annotator pool? The Segment
Anything 1B (SA-1B) dataset is to be used for research purposes only.
The SA-1B dataset is one of the most geographically diverse segmentation
dataset, as discussed in 6. In addition, we analyze the responsible AI axes
of a model trained on the dataset in 6.
Platform and Infrastructure Choices
1.What annotation platform did you utilize? At a high level, what considera-
tions informed your decision to choose this platform? Did the chosen plat-
form sufﬁciently meet the requirements you outlined for annotator pools?
Are any aspects not covered? We used a proprietary annotation platform.
2.What, if any, communication channels did your chosen platform offer to
facilitate communication with annotators? How did this channel of com-
munication inﬂuence the annotation process and/or resulting annotations?
We manually reviewed annotations and shared feedback with the annotators
on a weekly basis. We communicated common mistakes or inconsisten-
cies and the corresponding corrections. In addition, the annotators were
given feedback for improvements daily by the annotation QA team. Out-
side the weekly feedback sessions, annotators had access to a spreadsheet
and chat group to facilitate communication with the research team. This
process greatly improved the average speed and quality of the annotations.
3.How much were annotators compensated? Did you consider any partic-
ular pay standards, when determining their compensation? If so, please
describe. Annotators were compensated with an hourly wage set by the
vendor. The vendor is a Certiﬁed B Corporation.
Dataset Analysis and Evaluation
1.How do you deﬁne the quality of annotations in your context, and how did
you assess the quality in the dataset you constructed? Annotators were ﬁrst
placed into training. They followed a 1-day training session led by the ven-
dor and then were asked to annotate a large number of examples from a
training queue. Annotators graduated from training to production after the
vendor QA team, in collaboration with the research team, manually spot-
checked the annotators masks to ensure quality. On average, annotators
spent one week in training before graduating. Production quality assess-
ment followed a similar process: the vendor QA team and the research team
manually reviewed the annotations weekly, sharing feedback weekly.
2.Have you conducted any analysis on disagreement patterns? If so, what
analyses did you use and what were the major ﬁndings? Did you analyze
potential sources of disagreement? We pointed out common mistakes dur-
ing weekly meetings with the annotators.
3.How do the individual annotator responses relate to the ﬁnal labels released
in the dataset? The annotations were only used to train early versions of the
SAM model and we do not currently plan to release them.
Dataset Release and Maintenance
1.Do you have reason to believe the annotations in this dataset may change
over time? Do you plan to update your dataset? No, except to remove
objectionable images.
2.Are there any conditions or deﬁnitions that, if changed, could impact the
utility of your dataset? We do not believe so.
3.Will you attempt to track, impose limitations on, or otherwise inﬂuence how
your dataset is used? If so, how? The SA-1B dataset will be released under
a license agreement allowing use for certain research purposes and protec-
tions for researchers. Researchers must agree to the terms of the license
agreement to access the dataset.
4.Were annotators informed about how the data is externalized? If changes to
the dataset are made, will they be informed? No, we do not plan to release
the manual annotations at the moment.
5.Is there a process by which annotators can later choose to withdraw their
data from the dataset? If so, please detail. No.
27Model Overview
Name SAM or Segment Anything Model
Version 1.0
Date 2023
Organization The FAIR team of Meta AI
Mode type Promptable segmentation model
Architecture See 3
Repository https://github.com/facebookresearch/segment-anything
Citation https://research.facebook.com/publications/segment-anything
License Apache 2.0
Intended Use
Primary intended uses SAM is intended to be used for any prompt-based segmentation task. We explored its use in segmenting objects
from a point (7.1), edge detection (7.2), segmenting all objects (7.3), and segmenting detected objects (7.4).
We explored how SAM can integrate with other vision models to segment objects from text (7.5).
Primary intended users SAM was primarily developed for research. The license for SAM can be found at
https://github.com/facebookresearch/segment-anything.
Out-of-scope use cases See terms of use for SAM found at https://github.com/facebookresearch/segment-anything. See Use Cases under
Ethical Considerations .
Caveats and recommendations SAM has impressive zero-shot performance across a wide range of tasks. We note, however, that in the zero-shot
setting there may be multiple valid ground truth masks for a given input. We recommend users take this into
consideration when using SAM for zero-shot segmentation. SAM can miss ﬁne structures and can hallucinate
small disconnected components. See 8 for a discussion of limitations.
Relevant Factors
Groups SAM was designed to segment any object. This includes stuff andthings .
Instrumentation and environment We benchmarked SAM on a diverse set of datasets and found that SAM can handle a variety of visual data including
simulations, paintings, underwater images, microscopy images, driving data, stereo images, ﬁsh-eye images . See
D.1 and Table 7 for information on the benchmarks used.
Metrics
Model performance measures We evaluated SAM on a variety of metrics based on the downstream task in our experiments.
mIoU : We used the mean intersection-over-union after a given number of prompts to evaluate the segmen-
tation quality of a mask when prompted with points.
Human evaluation : We performed a human study (detailed in E) to evaluate the real world performance
of SAM. We compared the masks generated by SAM to a baseline state-of-the-art interactive segmentation
model, RITM [92], using a perceptual quality scale from 1 to 10.
AP: We used average precision to evaluate instance segmentation for a given box and edge detection.
AR@1000 : We used average recall to evaluate object proposal generation.
ODS, OIS, AP , R50 : We used the standard edge detection evaluation metrics from BSDS500 [72, 3].
Evaluation Data
Data sources See D.1.
Training Data
Data source See Data Card in F.1.
Ethical Considerations
Data We trained SAM on licensed images. The images were ﬁltered for objectionable content by the provider, but we
acknowledge the possibility of false negatives. We performed a geographic analysis of the SA-1B dataset in 6.
While SA-1B is more geographically diverse than many of its predecessors, we acknowledge that some geographic
regions and economic groups are underrepresented.
Cost and impact of compute SAM was trained on 256 A100 GPUS for 68 hours. We acknowledge the environmental impact and cost of training
large scale models. The environmental impact of training the released SAM model is approximately 6963 kWh
resulting in an estimated 2.8 metric tons of carbon dioxide given the speciﬁc data center used, using the calculation
described in [77] and the ML CO 2Impact calculator [61]. This is equivalent to 7k miles driven by the average
gasoline-powered passenger vehicle in the US [101]. We released the SAM models to both reduce the need for
retraining and lower the barrier to entry for large scale vision research.
Risks and harms We evaluated SAM for fairness in 6. Downstream use cases of SAM will create their own potential for biases
and fairness concerns. As such we recommend users run their own fairness evaluation when using SAM for their
speciﬁc use case.
Use cases We implore users to use their best judgement for downstream use of the model.
Table 9: Model Card for SAM, following the procedure detailed in [75].
28We have several models that, when provided with a click or a box as input, output a mask. We would
like to compare the quality of these models by rating the quality of their masks on many examples.
The interface will be different than for regular mask annotation.
 Each job reviews one mask in one image.
 On the right, there will be ﬁve image thumbnails in two rows. Each thumbnail can be moused-
over to show the image at a larger size. Clicking on the thumbnail will make it full screen, and
clicking again will return to the original screen.
 The images show the same mask in ﬁve different views. On the top row: (left) the image
without the mask, (middle) the mask overlaid on the image, and (right) the mask alone. On
the bottom row: (left) a zoomed in view of the object without a mask, and (right) a zoomed
in view of the mask overlaid on the image. These views are provided to make it easy to see
different types of mask errors.
 The mask will be in red when overlaid on the image.
 When shown by itself, the mask is yellow, and the background is purple.
 Each image will include either a blue dot or a blue and white box. This is the input to the
model, as if you had clicked at this location or drawn this box.
 On the left, there are buttons labeled 1-10. This is used to rate the quality of the shown mask.
Objective and Setup
 Example interface page. There will be ﬁve images on the
right and a question box on the left.
Mouse over an image to show the full image.
 Click on an image to make it full screen. The arrows will cy-
cle between images. Click again to return to previous view.
The ﬁrst image on the top row shows the image without a
mask. A blue point will be on the object of interest, or a
blue and white box will surround it.
The second image on the top row shows the mask for the
object in red.
The third image on the top row shows the mask only. The
mask is in yellow and the background is purple.
The ﬁrst image on the bottom row shows a zoomed in view
of the object without a mask.
The second image on the bottom row shows a zoomed in
view of the object with a mask. The mask is in red.
On the left are buttons to rate the mask quality, with selec-
tions 1-10.What we would like you to do for each job:
 Please aim to spend up to 30 seconds per job.
 Mouse-over or click each of the three images of the mask on the right to get a sense of the
quality of the mask. The thumbnail is too small to judge a mask, do not judge a mask by the
thumbnail alone. Each image can provide a different signal on possible mask errors:
 The unzoomed image can give context for the mask: does this mask correspond to an actual
object?
 The mask-only image can show if the mask has small holes or separated, incorrect pixels.
 The zoomed image can show if the mask boundaries make sense.
 Judge the quality of the mask on three criterion. Examples will follow.
 Does the mask correspond to an actual object?
 Does the mask have a good boundary?
 Does the mask correspond to the provided point or box?
 Rate the quality of the mask on a scale of 1-10 using the drop-down box on the left.
 Next are details and examples for judging mask quality according to the three criterion. These
are just examples and other cases may come up, please use your best judgment when deter-
mining if something is a good mask.
TaskDoes the mask correspond to an actual object?
 Valid objects can include:
 Entire single objects (such as a person, shirt, or tree)
 Logical parts of objects (a chair leg, a car door, a tabletop)
 Collections of objects (a stack of books, a crowd of people)
 Stuff (the ground, the sky).
 Example errors a mask may have. The severity of these errors may be minor or major:
 Include a piece of another object (the mask of a person including the arm of a nearby
person)
 Miss part of an object (the mask covers only one part of a building obscured by a tree in
the foreground),
 Combine two unrelated things (a single mask covers both a mug and a pen on a desk)
 Include an arbitrary part of a collection for a point input (a point is on one apple, but
the mask covers three apples in a pile of many apples). If a box surrounds an arbitrary
collection, it is not an error to provide a mask for these objects.
 If you are unsure, a good rule-of-thumb is: can you name the object in question? However,
some things that are hard to name may still be good objects (an unusual component of a
machine, something at the edge of the image for which it is hard to determine what it is).
Judging Mask Quality (1 of 3)
Does the mask have a good boundary?
 Errors in the boundary can include:
 Incorrect holes in the mask
 Incorrect pixels included separated from the main part of the mask
 Poor edge quality, where the mask does not exactly match the edge of the object.
 Failure to consistently handle obscuring foreground objects (a mask that covers obscuring
objects is ﬁne, and a mask that doesnt cover obscuring objects is ﬁne, but one that does
some of both has an error)
 Pixelation of a small mask is not an error, as long as the mask still matches the edges of
the object.
Judging Mask Quality (2 of 3)Does the mask correspond to the provided point or box?
 For points:
 The point needs to be on the mask.
 The size or position of the object with respect to the point does not matter (a point on
someones gloved hand can correspond to the glove or to the entire person, both are valid
masks).
 For boxes:
 The object needs to be the best object that is the size of the box (if a box is around some-
ones entire head but the mask is of their hair, this is an error: their hair is in the box but is
not the correct object).
 If the box clearly corresponds to a given object but is slightly smaller than it, it is okay if
the mask goes slightly outside a box (if a box around a person misses their extended hand,
the mask can still include their hand even if the mask goes outside the box).
Judging Mask Quality (3 of 3)
 Example error of Include a piece of another object: The
elephant mask contains a piece of another nearby elephant.
Example error of Missing a part of an object: the mask is
missing a disconnected part of the object: the back half of
the zebra, and the right portion of the plate.
Example error of Include an arbitrary part of a collection:
In top top image, the point is on one orange rind, but the
mask covers two orange rinds. This is a mask error: the
mask covers an arbitrary number of objects in the collection,
and should either cover one orange rind or all of them. In
the bottom image, the box is around both vegetables. Since
this is the best match to the box, this is not a mask error.
Example error for Incorrect holes in the mask: This mask
has holes in the upper left and on the left sides (black ar-
rows). These holes are much easier to see on the mask
only image.
Example error for Incorrect pixels included separated from
the main part of the mask: The mask only view reveals a
few stray incorrect pixels on the clock face.
Example error for Poor edge quality: The mask has poor
edge quality, both along the edge of the umbrella, as well as
along the thin pole.
Figure 19: Here we provide the complete guidelines given to annotations for the human review of mask quality. Some images
been edited slightly and faces have been blurred to enable release. Best viewed with zoom (part 1 of 2).
G. Annotation Guidelines
We provide the complete guidelines given to annotations
for the human review of mask quality in Fig. 19 and Fig. 20.
29Example for Combine two unrelated things: The point in-
dicates the lizard, but the mask covers both the lizard and a
bird. This is a mask error.
Example error for Failure to consistently handle obscuring
foreground objects: The pole on the right (blue arrow) is
excluded from the mask, while the pole on the left is in-
cluded in the object (black arrow). The mask should either
include or exclude both of these.
Example of Pixelation of a small mask: this mask has an
imperfect boundary, since it extends beyond the object at
the black arrow. However, the blocky pattern of the mask
is not an error, since, when zoomed in this much, the image
is also blocky the same way.
Example error for consistency with the provided point: The
mask does not agree with the blue point, so this is a mask
error.
Example for consistency with the provided point: For this
input point, but the logo (left) and the container (right) are
valid objects, since the blue point lies on both of them. Nei-
ther mask has a mask error.
Example for consistency with a box: The box surrounds the
bowl of oranges, but the mask is only of a single orange.
This is a mask error.
Example for consistency with a box: The boxs shape ﬁts
the zebra. Even though the mask extends slightly outside
the box to include the zebras left leg, this is not an error.Overall mask quality is subjective, each of the above errors may hurt mask quality only a little or a
lot, depending on how large the error is. Please use your best judgment when choosing mask scores,
and try to stay consistent from mask-to-mask. Here are some general guidelines for what different
scores should correspond to:
 A score of 1: It is not possible to tell what object this mask corresponds to. This includes the
case that there is no mask visible at all.
 A low score (2-4): The object is mostly identiﬁable, but the mask quality is extremely poor
(e.g. large regions of the mask cover other objects; large regions of the object missing; ex-
tremely splotchy mask boundaries that cut through the middle of the object).
 A mid score (5-6): The object is identiﬁable and the boundary is mostly correct, but there
are major errors (missing a signiﬁcant disconnected part of the object; containing a signiﬁcant
part of another object; very poor boundary quality in one area of the object but not the entire
object).
 A high score (7-9): The object is identiﬁable and errors are small and rare (missing a small,
heavily obscured disconnected component, having small regions where the mask boundary
does not quite match the object boundary).
 A score of 10: The mask is pixel-perfect; it has no identiﬁable errors at all.
Mask Scoring
Example of a mask with a score of 1: It is not clear what
object this mask corresponds to.
Example of a mask with a low score (2-4): The main ob-
ject is identiﬁable, but the mask includes a large, incorrect
portion of another object.
Example of a mask with a low score (2-4): The main ob-
ject is identiﬁable, but a large, random part of the object is
missing.
Example of a mask with a low-to-medium score (4-5): The
object is identiﬁable and the edges are all correct, but the
mask incorrectly includes the hand of the person on the left.
Example of a mask with a medium score (5-6): The mask
clearly corresponds to the plate, but the boundary with the
wafﬂe is quite poor.
Example of a mask with a medium score (5-6): the object
is easy to identify, and most of the edges make sense. How-
ever, there is a signiﬁcant disconnected part (their arm inside
the frame) that is mostly missing, as well as splotchy pixels
in this region.
Example of a mask with a medium-to-high score (6-8): the
mask has two small-ish regions of poor boundary, at the top
of the mask and on the bottom right.
Example of a mask with a medium-to-high score (6-8): The
wreath is a valid object that is the size of the box (the entire
wreath + clock would also be a valid object). However, there
are incorrect stray mask pixels on the clock.
Example of a mask with a high score (7-9): The boundary of
the horse is almost entirely correct, except for the right side
of its back leg. The mask consistently includes all of the
equipment that horse is wearing, and has logical boundaries.
Example of a mask with a very high score ( 9): There are
only minor errors around the edge of the mask. The blocky
pixelation is not an error, since the image is also blocky at
this scale.
Example of a mask with a very high score (9-10): the mask
has only very minor errors in the edge on the bottom right.
Example of a mask with a very high score (9-10): There are
only minor errors around the edge of the mask.
Figure 20: Here we provide the complete guidelines given to annotations for the human review of mask quality. Some images
been edited slightly and faces have been blurred to enable release. Best viewed with zoom (part 2 of 2).
30
  Denoising Diffusion Probabilistic Models
Jonathan Ho
UC Berkeley
jonathanho@berkeley.eduAjay Jain
UC Berkeley
ajayj@berkeley.eduPieter Abbeel
UC Berkeley
pabbeel@cs.berkeley.edu
Abstract
We present high quality image synthesis results using diffusion probabilistic models,
a class of latent variable models inspired by considerations from nonequilibrium
thermodynamics. Our best results are obtained by training on a weighted variational
bound designed according to a novel connection between diffusion probabilistic
models and denoising score matching with Langevin dynamics, and our models nat-
urally admit a progressive lossy decompression scheme that can be interpreted as a
generalization of autoregressive decoding. On the unconditional CIFAR10 dataset,
we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On
256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our imple-
mentation is available at https://github.com/hojonathanho/diffusion .
1 Introduction
Deep generative models of all kinds have recently exhibited high quality samples in a wide variety
of data modalities. Generative adversarial networks (GANs), autoregressive models, ﬂows, and
variational autoencoders (V AEs) have synthesized striking image and audio samples [ 14,27,3,
58,38,25,10,32,44,57,26,33,45], and there have been remarkable advances in energy-based
modeling and score matching that have produced images comparable to those of GANs [11, 55].
Figure 1: Generated samples on CelebA-HQ 256256(left) and unconditional CIFAR10 (right)
34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.arXiv:2006.11239v2  [cs.LG]  16 Dec 2020!<latexit sha1_base64=7yFrn0YPyuP5dVIvc7Tl2zcbS/g=>AAAB+HicbVBNSwMxEJ2tX7V+dNWjl2ARPJXdKuix6MVjBfsB7VKyaXYbmk2WJKvU0l/ixYMiXv0p3vw3pu0etPXBwOO9GWbmhSln2njet1NYW9/Y3Cpul3Z29/bL7sFhS8tMEdokkkvVCbGmnAnaNMxw2kkVxUnIaTsc3cz89gNVmklxb8YpDRIcCxYxgo2V+m65x6WIFYuHBislH/tuxat6c6BV4uekAjkafferN5AkS6gwhGOtu76XmmCClWGE02mpl2maYjLCMe1aKnBCdTCZHz5Fp1YZoEgqW8Kgufp7YoITrcdJaDsTbIZ62ZuJ/3ndzERXwYSJNDNUkMWiKOPISDRLAQ2YosTwsSWYKGZvRWSIFSbGZlWyIfjLL6+SVq3qn1drdxeV+nUeRxGO4QTOwIdLqMMtNKAJBDJ4hld4c56cF+fd+Vi0Fpx85gj+wPn8AXOGk5o=</latexit>
xT!!xt!xt1!!x0
<latexit sha1_base64=l4LvSgM7PR7I/kkuy5soikK4gpU=>AAAEoXictVLditNAFE7XqGv92a5eejOYLexKLU0VFKRQ9EYvhCrb3YUklOlk2g6dnzBzYrcb8zK+lU/gazhJK6atuiB4YODM+T/n+8YJZwY6nW+1vRvuzVu39+/U7967/+CgcfjwzKhUEzokiit9McaGcibpEBhwepFoisWY0/Px/G3hP/9MtWFKnsIyoZHAU8kmjGCwplHjeygwzAjThNM4Kz/jSXaZj05zFHIlp5pNZ4C1VgsUkliB2TX/oQLYCpe/4rJwZhJM6NPMJyLPt9IM0SwBA0tOUaVGBs/8/J8mWVRH6eSjhtdpd0pBu4q/VjxnLYPR4d7XMFYkFVQC4diYwO8kEGVYA7P183qYGmr3meMpDawqsaAmykpEctS0lhhNlLZPAiqt1YwMC2OWYmwjiynNtq8w/s4XpDB5FWVMJilQSVaNJilHoFABL4qZpgT40irYntTOisgMa0zAkqC+0QbY/MquIfCcYssbsBH1UNIFUUJgGVePGfhR1qyj1YETXAaH/SqAnp836/lGftUfdNcFiqbBT8L2jouQdvE9iVAoVUyDWONFa5XVYlJSjezEPT+BlmCSiVQgw65or2vBaE0Y5z1e4D/VeBmhstwJyo5C0YeZ53vdo/z19lhVjly71+K6xRb/ZbO/rbLCS8HMwmVZ7W9zeFc567b95+3uxxde/82a3/vOY+eJc+z4zkun77xzBs7QIbUPNVP7Ustdz33vDtxPq9C92jrnkbMhbvAD81mObw==</latexit>p(xt1|xt)
<latexit sha1_base64=XVzP503G8Ma8Lkwk3KKGZcZJbZ0=>AAACEnicbVC7SgNBFJ2Nrxhfq5Y2g0FICsNuFEwZsLGMYB6QLMvsZDYZMvtg5q4Y1nyDjb9iY6GIrZWdf+Mk2SImHrhwOOde7r3HiwVXYFk/Rm5tfWNzK79d2Nnd2z8wD49aKkokZU0aiUh2PKKY4CFrAgfBOrFkJPAEa3uj66nfvmdS8Si8g3HMnIAMQu5zSkBLrlmO3R4MGZBSLyAw9Pz0YeKmcG5P8CNekKDsmkWrYs2AV4mdkSLK0HDN714/oknAQqCCKNW1rRiclEjgVLBJoZcoFhM6IgPW1TQkAVNOOntpgs+00sd+JHWFgGfq4kRKAqXGgac7p0eqZW8q/ud1E/BrTsrDOAEW0vkiPxEYIjzNB/e5ZBTEWBNCJde3YjokklDQKRZ0CPbyy6ukVa3YF5Xq7WWxXsviyKMTdIpKyEZXqI5uUAM1EUVP6AW9oXfj2Xg1PozPeWvOyGaO0R8YX7+bCp4F</latexit>q(xt|xt1)
<latexit sha1_base64=eAZ87UuTmAQoJ4u19RGH5tA+bCI=>AAACC3icbVC7TgJBFJ31ifhatbSZQEywkOyiiZQkNpaYyCMBspkdZmHC7MOZu0ay0tv4KzYWGmPrD9j5N87CFgieZJIz59ybe+9xI8EVWNaPsbK6tr6xmdvKb+/s7u2bB4dNFcaSsgYNRSjbLlFM8IA1gINg7Ugy4ruCtdzRVeq37plUPAxuYRyxnk8GAfc4JaAlxyzclbo+gaHrJQ8TB/AjnvsmcGZPTh2zaJWtKfAysTNSRBnqjvnd7Yc09lkAVBClOrYVQS8hEjgVbJLvxopFhI7IgHU0DYjPVC+Z3jLBJ1rpYy+U+gWAp+p8R0J8pca+qyvTRdWil4r/eZ0YvGov4UEUAwvobJAXCwwhToPBfS4ZBTHWhFDJ9a6YDokkFHR8eR2CvXjyMmlWyvZ5uXJzUaxVszhy6BgVUAnZ6BLV0DWqowai6Am9oDf0bjwbr8aH8TkrXTGyniP0B8bXL+1hmu8=</latexit>Figure 2: The directed graphical model considered in this work.
This paper presents progress in diffusion probabilistic models [ 53]. A diffusion probabilistic model
(which we will call a diffusion model for brevity) is a parameterized Markov chain trained using
variational inference to produce samples matching the data after ﬁnite time. Transitions of this chain
are learned to reverse a diffusion process, which is a Markov chain that gradually adds noise to the
data in the opposite direction of sampling until signal is destroyed. When the diffusion consists of
small amounts of Gaussian noise, it is sufﬁcient to set the sampling chain transitions to conditional
Gaussians too, allowing for a particularly simple neural network parameterization.
Diffusion models are straightforward to deﬁne and efﬁcient to train, but to the best of our knowledge,
there has been no demonstration that they are capable of generating high quality samples. We
show that diffusion models actually are capable of generating high quality samples, sometimes
better than the published results on other types of generative models (Section 4). In addition, we
show that a certain parameterization of diffusion models reveals an equivalence with denoising
score matching over multiple noise levels during training and with annealed Langevin dynamics
during sampling (Section 3.2) [ 55,61]. We obtained our best sample quality results using this
parameterization (Section 4.2), so we consider this equivalence to be one of our primary contributions.
Despite their sample quality, our models do not have competitive log likelihoods compared to other
likelihood-based models (our models do, however, have log likelihoods better than the large estimates
annealed importance sampling has been reported to produce for energy based models and score
matching [ 11,55]). We ﬁnd that the majority of our models lossless codelengths are consumed
to describe imperceptible image details (Section 4.3). We present a more reﬁned analysis of this
phenomenon in the language of lossy compression, and we show that the sampling procedure of
diffusion models is a type of progressive decoding that resembles autoregressive decoding along a bit
ordering that vastly generalizes what is normally possible with autoregressive models.
2 Background
Diffusion models [ 53] are latent variable models of the form p(x0):=R
p(x0:T)dx1:T, where
x1;:::;xTare latents of the same dimensionality as the data x0q(x0). The joint distribution
p(x0:T)is called the reverse process , and it is deﬁned as a Markov chain with learned Gaussian
transitions starting at p(xT) =N(xT;0;I):
p(x0:T):=p(xT)TY
t=1p(xt1jxt); p(xt1jxt):=N(xt1;(xt;t);(xt;t)) (1)
What distinguishes diffusion models from other types of latent variable models is that the approximate
posteriorq(x1:Tjx0), called the forward process ordiffusion process , is ﬁxed to a Markov chain that
gradually adds Gaussian noise to the data according to a variance schedule 1;:::;T:
q(x1:Tjx0):=TY
t=1q(xtjxt1); q (xtjxt1):=N(xt;p
1txt1;tI) (2)
Training is performed by optimizing the usual variational bound on negative log likelihood:
E[logp(x0)]Eq
logp(x0:T)
q(x1:Tjx0)
=Eq
logp(xT)X
t1logp(xt1jxt)
q(xtjxt1)
=:L(3)
The forward process variances tcan be learned by reparameterization [ 33] or held constant as
hyperparameters, and expressiveness of the reverse process is ensured in part by the choice of
Gaussian conditionals in p(xt1jxt), because both processes have the same functional form when
tare small [ 53]. A notable property of the forward process is that it admits sampling xtat an
arbitrary timestep tin closed form: using the notation t:= 1tandt:=Qt
s=1s, we have
q(xtjx0) =N(xt;ptx0;(1t)I) (4)
2Efﬁcient training is therefore possible by optimizing random terms of Lwith stochastic gradient
descent. Further improvements come from variance reduction by rewriting L(3) as:
Eq
DKL(q(xTjx0)kp(xT))|{z}
LT+X
t>1DKL(q(xt1jxt;x0)kp(xt1jxt))| {z }
Lt1logp(x0jx1)|{z}
L0
(5)
(See Appendix A for details. The labels on the terms are used in Section 3.) Equation (5) uses KL
divergence to directly compare p(xt1jxt)against forward process posteriors, which are tractable
when conditioned on x0:
q(xt1jxt;x0) =N(xt1;~t(xt;x0);~tI); (6)
where ~t(xt;x0):=pt1t
1tx0+pt(1t1)
1txtand ~t:=1t1
1tt (7)
Consequently, all KL divergences in Eq. (5) are comparisons between Gaussians, so they can be
calculated in a Rao-Blackwellized fashion with closed form expressions instead of high variance
Monte Carlo estimates.
3 Diffusion models and denoising autoencoders
Diffusion models might appear to be a restricted class of latent variable models, but they allow a
large number of degrees of freedom in implementation. One must choose the variances tof the
forward process and the model architecture and Gaussian distribution parameterization of the reverse
process. To guide our choices, we establish a new explicit connection between diffusion models
and denoising score matching (Section 3.2) that leads to a simpliﬁed, weighted variational bound
objective for diffusion models (Section 3.4). Ultimately, our model design is justiﬁed by simplicity
and empirical results (Section 4). Our discussion is categorized by the terms of Eq. (5).
3.1 Forward process and LT
We ignore the fact that the forward process variances tare learnable by reparameterization and
instead ﬁx them to constants (see Section 4 for details). Thus, in our implementation, the approximate
posteriorqhas no learnable parameters, so LTis a constant during training and can be ignored.
3.2 Reverse process and L1:T1
Now we discuss our choices in p(xt1jxt) =N(xt1;(xt;t);(xt;t))for1<tT. First,
we set (xt;t) =2
tIto untrained time dependent constants. Experimentally, both 2
t=tand
2
t=~t=1t1
1tthad similar results. The ﬁrst choice is optimal for x0N (0;I), and the
second is optimal for x0deterministically set to one point. These are the two extreme choices
corresponding to upper and lower bounds on reverse process entropy for data with coordinatewise
unit variance [53].
Second, to represent the mean (xt;t), we propose a speciﬁc parameterization motivated by the
following analysis of Lt. Withp(xt1jxt) =N(xt1;(xt;t);2
tI), we can write:
Lt1=Eq1
22
tk~t(xt;x0)(xt;t)k2
+C (8)
whereCis a constant that does not depend on . So, we see that the most straightforward parameteri-
zation of is a model that predicts ~t, the forward process posterior mean. However, we can expand
Eq. (8) further by reparameterizing Eq. (4) as xt(x0;) =ptx0+p1tforN(0;I)and
applying the forward process posterior formula (7):
Lt1C=Ex0;
1
22
t



~t
xt(x0;);1pt(xt(x0;)p
1t)
(xt(x0;);t)



2#
(9)
=Ex0;
1
22
t



1pt
xt(x0;)tp1t
(xt(x0;);t)



2#
(10)
3Algorithm 1 Training
1:repeat
2:x0q(x0)
3:tUniform(f1;:::;Tg)
4:N(0;I)
5: Take gradient descent step on
r

(ptx0+p1t;t)

2
6:until convergedAlgorithm 2 Sampling
1:xTN(0;I)
2:fort=T;:::; 1do
3:zN(0;I)ift>1, elsez=0
4:xt1=1pt
xt1tp1t(xt;t)
+tz
5:end for
6:return x0
Equation (10) reveals that must predict1pt
xttp1t
given xt. Since xtis available as
input to the model, we may choose the parameterization
(xt;t) =~t
xt;1pt(xtp
1t(xt))
=1pt
xttp1t(xt;t)
(11)
where is a function approximator intended to predict fromxt. To sample xt1p(xt1jxt)is
to compute xt1=1pt
xttp1t(xt;t)
+tz, where zN(0;I). The complete sampling
procedure, Algorithm 2, resembles Langevin dynamics with as a learned gradient of the data
density. Furthermore, with the parameterization (11), Eq. (10) simpliﬁes to:
Ex0;2
t
22
tt(1t)

(ptx0+p
1t;t)

2
(12)
which resembles denoising score matching over multiple noise scales indexed by t[55]. As Eq. (12)
is equal to (one term of) the variational bound for the Langevin-like reverse process (11), we see
that optimizing an objective resembling denoising score matching is equivalent to using variational
inference to ﬁt the ﬁnite-time marginal of a sampling chain resembling Langevin dynamics.
To summarize, we can train the reverse process mean function approximator to predict ~t, or by
modifying its parameterization, we can train it to predict . (There is also the possibility of predicting
x0, but we found this to lead to worse sample quality early in our experiments.) We have shown that
the-prediction parameterization both resembles Langevin dynamics and simpliﬁes the diffusion
models variational bound to an objective that resembles denoising score matching. Nonetheless,
it is just another parameterization of p(xt1jxt), so we verify its effectiveness in Section 4 in an
ablation where we compare predicting against predicting ~t.
3.3 Data scaling, reverse process decoder, and L0
We assume that image data consists of integers in f0;1;:::; 255gscaled linearly to [1;1]. This
ensures that the neural network reverse process operates on consistently scaled inputs starting from
the standard normal prior p(xT). To obtain discrete log likelihoods, we set the last term of the reverse
process to an independent discrete decoder derived from the Gaussian N(x0;(x1;1);2
1I):
p(x0jx1) =DY
i=1Z+(xi
0)
(xi
0)N(x;i
(x1;1);2
1)dx
+(x) =1 ifx= 1
x+1
255ifx<1(x) =1 ifx=1
x1
255ifx>1(13)
whereDis the data dimensionality and the isuperscript indicates extraction of one coordinate.
(It would be straightforward to instead incorporate a more powerful decoder like a conditional
autoregressive model, but we leave that to future work.) Similar to the discretized continuous
distributions used in V AE decoders and autoregressive models [ 34,52], our choice here ensures that
the variational bound is a lossless codelength of discrete data, without need of adding noise to the
data or incorporating the Jacobian of the scaling operation into the log likelihood. At the end of
sampling, we display (x1;1)noiselessly.
3.4 Simpliﬁed training objective
With the reverse process and decoder deﬁned above, the variational bound, consisting of terms derived
from Eqs. (12) and (13), is clearly differentiable with respect to and is ready to be employed for
4Table 1: CIFAR10 results. NLL measured in bits/dim.
Model IS FID NLL Test (Train)
Conditional
EBM [11] 8:30 37 :9
JEM [17] 8:76 38 :4
BigGAN [3] 9:22 14 :73
StyleGAN2 + ADA (v1) [29] 10:06 2:67
Unconditional
Diffusion (original) [53] 5:40
Gated PixelCNN [59] 4:60 65 :93 3:03 (2:90)
Sparse Transformer [7] 2:80
PixelIQN [43] 5:29 49 :46
EBM [11] 6:78 38 :2
NCSNv2 [56] 31:75
NCSN [55] 8:870:12 25:32
SNGAN [39] 8:220:05 21:7
SNGAN-DDLS [4] 9:090:10 15:42
StyleGAN2 + ADA (v1) [29] 9:740:05 3:26
Ours (L, ﬁxed isotropic ) 7:670:13 13:513:70 (3:69)
Ours (Lsimple ) 9:460:11 3:173:75 (3:72)Table 2: Unconditional CIFAR10 reverse
process parameterization and training objec-
tive ablation. Blank entries were unstable to
train and generated poor samples with out-of-
range scores.
Objective IS FID
~prediction (baseline)
L, learned diagonal  7:280:10 23:69
L, ﬁxed isotropic  8:060:09 13:22
k~~k2 
prediction (ours)
L, learned diagonal   
L, ﬁxed isotropic  7:670:13 13:51
k~k2(Lsimple )9:460:11 3:17
training. However, we found it beneﬁcial to sample quality (and simpler to implement) to train on the
following variant of the variational bound:
Lsimple ():=Et;x0;h

(ptx0+p
1t;t)

2i
(14)
wheretis uniform between 1andT. Thet= 1 case corresponds to L0with the integral in the
discrete decoder deﬁnition (13) approximated by the Gaussian probability density function times the
bin width, ignoring 2
1and edge effects. The t>1cases correspond to an unweighted version of
Eq. (12), analogous to the loss weighting used by the NCSN denoising score matching model [ 55].
(LTdoes not appear because the forward process variances tare ﬁxed.) Algorithm 1 displays the
complete training procedure with this simpliﬁed objective.
Since our simpliﬁed objective (14) discards the weighting in Eq. (12), it is a weighted variational
bound that emphasizes different aspects of reconstruction compared to the standard variational
bound [ 18,22]. In particular, our diffusion process setup in Section 4 causes the simpliﬁed objective
to down-weight loss terms corresponding to small t. These terms train the network to denoise data
with very small amounts of noise, so it is beneﬁcial to down-weight them so that the network can
focus on more difﬁcult denoising tasks at larger tterms. We will see in our experiments that this
reweighting leads to better sample quality.
4 Experiments
We setT= 1000 for all experiments so that the number of neural network evaluations needed
during sampling matches previous work [ 53,55]. We set the forward process variances to constants
increasing linearly from 1= 104toT= 0:02. These constants were chosen to be small
relative to data scaled to [1;1], ensuring that reverse and forward processes have approximately
the same functional form while keeping the signal-to-noise ratio at xTas small as possible ( LT=
DKL(q(xTjx0)kN(0;I))105bits per dimension in our experiments).
To represent the reverse process, we use a U-Net backbone similar to an unmasked PixelCNN++ [52,
48] with group normalization throughout [ 66]. Parameters are shared across time, which is speciﬁed
to the network using the Transformer sinusoidal position embedding [ 60]. We use self-attention at
the1616feature map resolution [63, 60]. Details are in Appendix B.
4.1 Sample quality
Table 1 shows Inception scores, FID scores, and negative log likelihoods (lossless codelengths) on
CIFAR10. With our FID score of 3.17, our unconditional model achieves better sample quality than
most models in the literature, including class conditional models. Our FID score is computed with
respect to the training set, as is standard practice; when we compute it with respect to the test set, the
score is 5.24, which is still better than many of the training set FID scores in the literature.
5Figure 3: LSUN Church samples. FID= 7:89
 Figure 4: LSUN Bedroom samples. FID= 4:90
Algorithm 3 Sending x0
1: Send xTq(xTjx0)usingp(xT)
2:fort=T1;:::; 2;1do
3: Send xtq(xtjxt+1;x0)usingp(xtjxt+1)
4:end for
5: Send x0usingp(x0jx1)Algorithm 4 Receiving
1: Receive xTusingp(xT)
2:fort=T1;:::; 1;0do
3: Receive xtusingp(xtjxt+1)
4:end for
5:return x0
We ﬁnd that training our models on the true variational bound yields better codelengths than training
on the simpliﬁed objective, as expected, but the latter yields the best sample quality. See Fig. 1 for
CIFAR10 and CelebA-HQ 256256samples, Fig. 3 and Fig. 4 for LSUN 256256samples [ 71],
and Appendix D for more.
4.2 Reverse process parameterization and training objective ablation
In Table 2, we show the sample quality effects of reverse process parameterizations and training
objectives (Section 3.2). We ﬁnd that the baseline option of predicting ~works well only when
trained on the true variational bound instead of unweighted mean squared error, a simpliﬁed objective
akin to Eq. (14). We also see that learning reverse process variances (by incorporating a parameterized
diagonal (xt)into the variational bound) leads to unstable training and poorer sample quality
compared to ﬁxed variances. Predicting , as we proposed, performs approximately as well as
predicting ~when trained on the variational bound with ﬁxed variances, but much better when trained
with our simpliﬁed objective.
4.3 Progressive coding
Table 1 also shows the codelengths of our CIFAR10 models. The gap between train and test is at
most 0.03 bits per dimension, which is comparable to the gaps reported with other likelihood-based
models and indicates that our diffusion model is not overﬁtting (see Appendix D for nearest neighbor
visualizations). Still, while our lossless codelengths are better than the large estimates reported for
energy based models and score matching using annealed importance sampling [ 11], they are not
competitive with other types of likelihood-based generative models [7].
Since our samples are nonetheless of high quality, we conclude that diffusion models have an inductive
bias that makes them excellent lossy compressors. Treating the variational bound terms L1++LT
as rate andL0as distortion, our CIFAR10 model with the highest quality samples has a rate of 1.78
bits/dim and a distortion of 1.97 bits/dim, which amounts to a root mean squared error of 0.95 on a
scale from 0 to 255. More than half of the lossless codelength describes imperceptible distortions.
Progressive lossy compression We can probe further into the rate-distortion behavior of our model
by introducing a progressive lossy code that mirrors the form of Eq. (5): see Algorithms 3 and 4,
which assume access to a procedure, such as minimal random coding [ 19,20], that can transmit a
sample xq(x)using approximately DKL(q(x)kp(x))bits on average for any distributions pand
q, for which only pis available to the receiver beforehand. When applied to x0q(x0), Algorithms 3
and 4 transmit xT;:::;x0in sequence using a total expected codelength equal to Eq. (5). The receiver,
6at any timet, has the partial information xtfully available and can progressively estimate:
x0^x0=
xtp
1t(xt)
=pt (15)
due to Eq. (4). (A stochastic reconstruction x0p(x0jxt)is also valid, but we do not consider
it here because it makes distortion more difﬁcult to evaluate.) Figure 5 shows the resulting rate-
distortion plot on the CIFAR10 test set. At each time t, the distortion is calculated as the root mean
squared errorp
kx0^x0k2=D, and the rate is calculated as the cumulative number of bits received
so far at time t. The distortion decreases steeply in the low-rate region of the rate-distortion plot,
indicating that the majority of the bits are indeed allocated to imperceptible distortions.
0200 400 600 8001;000020406080
Reverse process steps ( Tt)Distortion (RMSE)
0200 400 600 8001;00000:511:5
Reverse process steps ( Tt)Rate (bits/dim)
0 0:5 1 1:5020406080
Rate (bits/dim)Distortion (RMSE)
Figure 5: Unconditional CIFAR10 test set rate-distortion vs. time. Distortion is measured in root mean squared
error on a [0;255] scale. See Table 4 for details.
Progressive generation We also run a progressive unconditional generation process given by
progressive decompression from random bits. In other words, we predict the result of the reverse
process, ^x0, while sampling from the reverse process using Algorithm 2. Figures 6 and 10 show the
resulting sample quality of ^x0over the course of the reverse process. Large scale image features
appear ﬁrst and details appear last. Figure 7 shows stochastic predictions x0p(x0jxt)withxt
frozen for various t. Whentis small, all but ﬁne details are preserved, and when tis large, only large
scale features are preserved. Perhaps these are hints of conceptual compression [18].
Figure 6: Unconditional CIFAR10 progressive generation ( ^x0over time, from left to right). Extended samples
and sample quality metrics over time in the appendix (Figs. 10 and 14).
Figure 7: When conditioned on the same latent, CelebA-HQ 256256samples share high-level attributes.
Bottom-right quadrants are xt, and other quadrants are samples from p(x0jxt).
Connection to autoregressive decoding Note that the variational bound (5) can be rewritten as:
L=DKL(q(xT)kp(xT)) +EqX
t1DKL(q(xt1jxt)kp(xt1jxt))#
+H(x0) (16)
(See Appendix A for a derivation.) Now consider setting the diffusion process length Tto the
dimensionality of the data, deﬁning the forward process so that q(xtjx0)places all probability mass
onx0with the ﬁrst tcoordinates masked out (i.e. q(xtjxt1)masks out the tthcoordinate), setting
p(xT)to place all mass on a blank image, and, for the sake of argument, taking p(xt1jxt)to
7Figure 8: Interpolations of CelebA-HQ 256x256 images with 500 timesteps of diffusion.
be a fully expressive conditional distribution. With these choices, DKL(q(xT)kp(xT)) = 0 , and
minimizingDKL(q(xt1jxt)kp(xt1jxt))trainspto copy coordinates t+ 1;:::;T unchanged
and to predict the tthcoordinate given t+ 1;:::;T . Thus, training pwith this particular diffusion is
training an autoregressive model.
We can therefore interpret the Gaussian diffusion model (2) as a kind of autoregressive model with
a generalized bit ordering that cannot be expressed by reordering data coordinates. Prior work has
shown that such reorderings introduce inductive biases that have an impact on sample quality [ 38],
so we speculate that the Gaussian diffusion serves a similar purpose, perhaps to greater effect since
Gaussian noise might be more natural to add to images compared to masking noise. Moreover, the
Gaussian diffusion length is not restricted to equal the data dimension; for instance, we use T= 1000 ,
which is less than the dimension of the 32323or2562563images in our experiments.
Gaussian diffusions can be made shorter for fast sampling or longer for model expressiveness.
4.4 Interpolation
We can interpolate source images x0;x0
0q(x0)in latent space using qas a stochastic encoder,
xt;x0
tq(xtjx0), then decoding the linearly interpolated latent xt= (1)x0+x0
0into image
space by the reverse process, x0p(x0jxt). In effect, we use the reverse process to remove
artifacts from linearly interpolating corrupted versions of the source images, as depicted in Fig. 8
(left). We ﬁxed the noise for different values of soxtandx0
tremain the same. Fig. 8 (right)
shows interpolations and reconstructions of original CelebA-HQ 256256images (t= 500 ). The
reverse process produces high-quality reconstructions, and plausible interpolations that smoothly
vary attributes such as pose, skin tone, hairstyle, expression and background, but not eyewear. Larger
tresults in coarser and more varied interpolations, with novel samples at t= 1000 (Appendix Fig. 9).
5 Related Work
While diffusion models might resemble ﬂows [ 9,46,10,32,5,16,23] and V AEs [ 33,47,37],
diffusion models are designed so that qhas no parameters and the top-level latent xThas nearly zero
mutual information with the data x0. Our -prediction reverse process parameterization establishes a
connection between diffusion models and denoising score matching over multiple noise levels with
annealed Langevin dynamics for sampling [ 55,56]. Diffusion models, however, admit straightforward
log likelihood evaluation, and the training procedure explicitly trains the Langevin dynamics sampler
using variational inference (see Appendix C for details). The connection also has the reverse
implication that a certain weighted form of denoising score matching is the same as variational
inference to train a Langevin-like sampler. Other methods for learning transition operators of Markov
chains include infusion training [ 2], variational walkback [ 15], generative stochastic networks [ 1],
and others [50, 54, 36, 42, 35, 65].
By the known connection between score matching and energy-based modeling, our work could have
implications for other recent work on energy-based models [ 6769,12,70,13,11,41,17,8]. Our
rate-distortion curves are computed over time in one evaluation of the variational bound, reminiscent
of how rate-distortion curves can be computed over distortion penalties in one run of annealed
importance sampling [ 24]. Our progressive decoding argument can be seen in convolutional DRAW
and related models [ 18,40] and may also lead to more general designs for subscale orderings or
sampling strategies for autoregressive models [38, 64].
86 Conclusion
We have presented high quality image samples using diffusion models, and we have found connections
among diffusion models and variational inference for training Markov chains, denoising score
matching and annealed Langevin dynamics (and energy-based models by extension), autoregressive
models, and progressive lossy compression. Since diffusion models seem to have excellent inductive
biases for image data, we look forward to investigating their utility in other data modalities and as
components in other types of generative models and machine learning systems.
Broader Impact
Our work on diffusion models takes on a similar scope as existing work on other types of deep
generative models, such as efforts to improve the sample quality of GANs, ﬂows, autoregressive
models, and so forth. Our paper represents progress in making diffusion models a generally useful
tool in this family of techniques, so it may serve to amplify any impacts that generative models have
had (and will have) on the broader world.
Unfortunately, there are numerous well-known malicious uses of generative models. Sample gen-
eration techniques can be employed to produce fake images and videos of high proﬁle ﬁgures for
political purposes. While fake images were manually created long before software tools were avail-
able, generative models such as ours make the process easier. Fortunately, CNN-generated images
currently have subtle ﬂaws that allow detection [ 62], but improvements in generative models may
make this more difﬁcult. Generative models also reﬂect the biases in the datasets on which they
are trained. As many large datasets are collected from the internet by automated systems, it can be
difﬁcult to remove these biases, especially when the images are unlabeled. If samples from generative
models trained on these datasets proliferate throughout the internet, then these biases will only be
reinforced further.
On the other hand, diffusion models may be useful for data compression, which, as data becomes
higher resolution and as global internet trafﬁc increases, might be crucial to ensure accessibility of
the internet to wide audiences. Our work might contribute to representation learning on unlabeled
raw data for a large range of downstream tasks, from image classiﬁcation to reinforcement learning,
and diffusion models might also become viable for creative uses in art, photography, and music.
Acknowledgments and Disclosure of Funding
This work was supported by ONR PECASE and the NSF Graduate Research Fellowship under grant
number DGE-1752814. Googles TensorFlow Research Cloud (TFRC) provided Cloud TPUs.
References
[1]Guillaume Alain, Yoshua Bengio, Li Yao, Jason Yosinski, Eric Thibodeau-Laufer, Saizheng Zhang, and
Pascal Vincent. GSNs: generative stochastic networks. Information and Inference: A Journal of the IMA ,
5(2):210249, 2016.
[2]Florian Bordes, Sina Honari, and Pascal Vincent. Learning to generate samples from noise through infusion
training. In International Conference on Learning Representations , 2017.
[3]Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high ﬁdelity natural
image synthesis. In International Conference on Learning Representations , 2019.
[4] Tong Che, Ruixiang Zhang, Jascha Sohl-Dickstein, Hugo Larochelle, Liam Paull, Yuan Cao, and Yoshua
Bengio. Your GAN is secretly an energy-based model and you should use discriminator driven latent
sampling. arXiv preprint arXiv:2003.06060 , 2020.
[5]Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential
equations. In Advances in Neural Information Processing Systems , pages 65716583, 2018.
[6]Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. PixelSNAIL: An improved autoregres-
sive generative model. In International Conference on Machine Learning , pages 863871, 2018.
[7]Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509 , 2019.
9[8]Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and MarcAurelio Ranzato. Residual energy-based
models for text generation. arXiv preprint arXiv:2004.11714 , 2020.
[9]Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation.
arXiv preprint arXiv:1410.8516 , 2014.
[10] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv
preprint arXiv:1605.08803 , 2016.
[11] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In Advances in
Neural Information Processing Systems , pages 36033613, 2019.
[12] Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, and Ying Nian Wu. Learning generative ConvNets
via multi-grid modeling and sampling. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 91559164, 2018.
[13] Ruiqi Gao, Erik Nijkamp, Diederik P Kingma, Zhen Xu, Andrew M Dai, and Ying Nian Wu. Flow
contrastive estimation of energy-based models. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 75187528, 2020.
[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing
Systems , pages 26722680, 2014.
[15] Anirudh Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. Variational walkback: Learning a
transition operator as a stochastic recurrent net. In Advances in Neural Information Processing Systems ,
pages 43924402, 2017.
[16] Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, and David Duvenaud. FFJORD: Free-form
continuous dynamics for scalable reversible generative models. In International Conference on Learning
Representations , 2019.
[17] Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and
Kevin Swersky. Your classiﬁer is secretly an energy based model and you should treat it like one. In
International Conference on Learning Representations , 2020.
[18] Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards
conceptual compression. In Advances In Neural Information Processing Systems , pages 35493557, 2016.
[19] Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication
complexity of correlation. In Twenty-Second Annual IEEE Conference on Computational Complexity
(CCC07) , pages 1023. IEEE, 2007.
[20] Marton Havasi, Robert Peharz, and José Miguel Hernández-Lobato. Minimal random code learning:
Getting bits back from compressed model parameters. In International Conference on Learning Represen-
tations , 2019.
[21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs
trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances in Neural
Information Processing Systems , pages 66266637, 2017.
[22] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mo-
hamed, and Alexander Lerchner. beta-V AE: Learning basic visual concepts with a constrained variational
framework. In International Conference on Learning Representations , 2017.
[23] Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving ﬂow-based
generative models with variational dequantization and architecture design. In International Conference on
Machine Learning , 2019.
[24] Sicong Huang, Alireza Makhzani, Yanshuai Cao, and Roger Grosse. Evaluating lossy compression rates of
deep generative models. In International Conference on Machine Learning , 2020.
[25] Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, and
Koray Kavukcuoglu. Video pixel networks. In International Conference on Machine Learning , pages
17711779, 2017.
[26] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart,
Florian Stimberg, Aaron van den Oord, Sander Dieleman, and Koray Kavukcuoglu. Efﬁcient neural audio
synthesis. In International Conference on Machine Learning , pages 24102419, 2018.
[27] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved
quality, stability, and variation. In International Conference on Learning Representations , 2018.
[28] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial
networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages
1044014410, 2019.
[29] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training
generative adversarial networks with limited data. arXiv preprint arXiv:2006.06676v1 , 2020.
[30] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and
improving the image quality of StyleGAN. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 81108119, 2020.
[31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations , 2015.
[32] Diederik P Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions. In
Advances in Neural Information Processing Systems , pages 1021510224, 2018.
[33] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114 ,
2013.
[34] Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved
variational inference with inverse autoregressive ﬂow. In Advances in Neural Information Processing
Systems , pages 47434751, 2016.
[35] John Lawson, George Tucker, Bo Dai, and Rajesh Ranganath. Energy-inspired models: Learning with
sampler-induced distributions. In Advances in Neural Information Processing Systems , pages 85018513,
2019.
[36] Daniel Levy, Matt D. Hoffman, and Jascha Sohl-Dickstein. Generalizing Hamiltonian Monte Carlo with
neural networks. In International Conference on Learning Representations , 2018.
[37] Lars Maaløe, Marco Fraccaro, Valentin Liévin, and Ole Winther. BIV A: A very deep hierarchy of
latent variables for generative modeling. In Advances in Neural Information Processing Systems , pages
65486558, 2019.
[38] Jacob Menick and Nal Kalchbrenner. Generating high ﬁdelity images with subscale pixel networks and
multidimensional upscaling. In International Conference on Learning Representations , 2019.
[39] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In International Conference on Learning Representations , 2018.
[40] Alex Nichol. VQ-DRAW: A sequential discrete V AE. arXiv preprint arXiv:2003.01599 , 2020.
[41] Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and Ying Nian Wu. On the anatomy of MCMC-based
maximum likelihood learning of energy-based models. arXiv preprint arXiv:1903.12370 , 2019.
[42] Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-persistent
short-run MCMC toward energy-based model. In Advances in Neural Information Processing Systems ,
pages 52335243, 2019.
[43] Georg Ostrovski, Will Dabney, and Remi Munos. Autoregressive quantile networks for generative modeling.
InInternational Conference on Machine Learning , pages 39363945, 2018.
[44] Ryan Prenger, Rafael Valle, and Bryan Catanzaro. WaveGlow: A ﬂow-based generative network for
speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP) , pages 36173621. IEEE, 2019.
[45] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-ﬁdelity images with VQ-
V AE-2. In Advances in Neural Information Processing Systems , pages 1483714847, 2019.
[46] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. In International
Conference on Machine Learning , pages 15301538, 2015.
[47] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approx-
imate inference in deep generative models. In International Conference on Machine Learning , pages
12781286, 2014.
[48] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical
image segmentation. In International Conference on Medical Image Computing and Computer-Assisted
Intervention , pages 234241. Springer, 2015.
[49] Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. In Advances in Neural Information Processing Systems , pages 901909,
2016.
[50] Tim Salimans, Diederik Kingma, and Max Welling. Markov Chain Monte Carlo and variational inference:
Bridging the gap. In International Conference on Machine Learning , pages 12181226, 2015.
11[51] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved
techniques for training gans. In Advances in Neural Information Processing Systems , pages 22342242,
2016.
[52] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the PixelCNN
with discretized logistic mixture likelihood and other modiﬁcations. In International Conference on
Learning Representations , 2017.
[53] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In International Conference on Machine Learning , pages
22562265, 2015.
[54] Jiaming Song, Shengjia Zhao, and Stefano Ermon. A-NICE-MC: Adversarial training for MCMC. In
Advances in Neural Information Processing Systems , pages 51405150, 2017.
[55] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In
Advances in Neural Information Processing Systems , pages 1189511907, 2019.
[56] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. arXiv
preprint arXiv:2006.09011 , 2020.
[57] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal
Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A generative model for raw audio.
arXiv preprint arXiv:1609.03499 , 2016.
[58] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.
International Conference on Machine Learning , 2016.
[59] Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray
Kavukcuoglu. Conditional image generation with PixelCNN decoders. In Advances in Neural Information
Processing Systems , pages 47904798, 2016.
[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing
Systems , pages 59986008, 2017.
[61] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation ,
23(7):16611674, 2011.
[62] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros. Cnn-generated images
are surprisingly easy to spot...for now. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , 2020.
[63] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 77947803,
2018.
[64] Auke J Wiggers and Emiel Hoogeboom. Predictive sampling with forecasting autoregressive models.
arXiv preprint arXiv:2002.09928 , 2020.
[65] Hao Wu, Jonas Köhler, and Frank Noé. Stochastic normalizing ﬂows. arXiv preprint arXiv:2002.06707 ,
2020.
[66] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on Computer
Vision (ECCV) , pages 319, 2018.
[67] Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In International
Conference on Machine Learning , pages 26352644, 2016.
[68] Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Synthesizing dynamic patterns by spatial-temporal
generative convnet. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ,
pages 70937101, 2017.
[69] Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, and Ying Nian Wu. Learning
descriptor networks for 3d shape synthesis and analysis. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , pages 86298638, 2018.
[70] Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Learning energy-based spatial-temporal generative
convnets for dynamic patterns. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2019.
[71] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: Construction of a large-scale
image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365 , 2015.
[72] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,
2016.
12Extra information
LSUN FID scores for LSUN datasets are included in Table 3. Scores marked withare reported
by StyleGAN2 as baselines, and other scores are reported by their respective authors.
Table 3: FID scores for LSUN 256256datasets
Model LSUN Bedroom LSUN Church LSUN Cat
ProgressiveGAN [27] 8.34 6.42 37.52
StyleGAN [28] 2.65 4.218.53
StyleGAN2 [30] - 3.86 6.93
Ours (Lsimple ) 6.36 7.89 19.75
Ours (Lsimple , large) 4.90 - -
Progressive compression Our lossy compression argument in Section 4.3 is only a proof of concept,
because Algorithms 3 and 4 depend on a procedure such as minimal random coding [ 20], which is
not tractable for high dimensional data. These algorithms serve as a compression interpretation of the
variational bound (5) of Sohl-Dickstein et al. [53], not yet as a practical compression system.
Table 4: Unconditional CIFAR10 test set rate-distortion values (accompanies Fig. 5)
Reverse process time ( Tt+ 1) Rate (bits/dim) Distortion (RMSE [0;255])
1000 1.77581 0.95136
900 0.11994 12.02277
800 0.05415 18.47482
700 0.02866 24.43656
600 0.01507 30.80948
500 0.00716 38.03236
400 0.00282 46.12765
300 0.00081 54.18826
200 0.00013 60.97170
100 0.00000 67.60125
A Extended derivations
Below is a derivation of Eq. (5), the reduced variance variational bound for diffusion models. This
material is from Sohl-Dickstein et al. [53]; we include it here only for completeness.
L=Eq
logp(x0:T)
q(x1:Tjx0)
(17)
=Eq2
4logp(xT)X
t1logp(xt1jxt)
q(xtjxt1)3
5 (18)
=Eq
logp(xT)X
t>1logp(xt1jxt)
q(xtjxt1)logp(x0jx1)
q(x1jx0)#
(19)
=Eq
logp(xT)X
t>1logp(xt1jxt)
q(xt1jxt;x0)q(xt1jx0)
q(xtjx0)logp(x0jx1)
q(x1jx0)#
(20)
=Eq
logp(xT)
q(xTjx0)X
t>1logp(xt1jxt)
q(xt1jxt;x0)logp(x0jx1)#
(21)
13=Eq
DKL(q(xTjx0)kp(xT)) +X
t>1DKL(q(xt1jxt;x0)kp(xt1jxt))logp(x0jx1)#
(22)
The following is an alternate version of L. It is not tractable to estimate, but it is useful for our
discussion in Section 4.3.
L=Eq2
4logp(xT)X
t1logp(xt1jxt)
q(xtjxt1)3
5 (23)
=Eq2
4logp(xT)X
t1logp(xt1jxt)
q(xt1jxt)q(xt1)
q(xt)3
5 (24)
=Eq2
4logp(xT)
q(xT)X
t1logp(xt1jxt)
q(xt1jxt)logq(x0)3
5 (25)
=DKL(q(xT)kp(xT)) +Eq2
4X
t1DKL(q(xt1jxt)kp(xt1jxt))3
5+H(x0) (26)
B Experimental details
Our neural network architecture follows the backbone of PixelCNN++ [ 52], which is a U-Net [ 48]
based on a Wide ResNet [ 72]. We replaced weight normalization [ 49] with group normalization [ 66]
to make the implementation simpler. Our 3232models use four feature map resolutions ( 3232
to44), and our 256256models use six. All models have two convolutional residual blocks
per resolution level and self-attention blocks at the 1616resolution between the convolutional
blocks [ 6]. Diffusion time tis speciﬁed by adding the Transformer sinusoidal position embedding [ 60]
into each residual block. Our CIFAR10 model has 35.7 million parameters, and our LSUN and
CelebA-HQ models have 114 million parameters. We also trained a larger variant of the LSUN
Bedroom model with approximately 256 million parameters by increasing ﬁlter count.
We used TPU v3-8 (similar to 8 V100 GPUs) for all experiments. Our CIFAR model trains at 21
steps per second at batch size 128 (10.6 hours to train to completion at 800k steps), and sampling
a batch of 256 images takes 17 seconds. Our CelebA-HQ/LSUN (2562) models train at 2.2 steps
per second at batch size 64, and sampling a batch of 128 images takes 300 seconds. We trained on
CelebA-HQ for 0.5M steps, LSUN Bedroom for 2.4M steps, LSUN Cat for 1.8M steps, and LSUN
Church for 1.2M steps. The larger LSUN Bedroom model was trained for 1.15M steps.
Apart from an initial choice of hyperparameters early on to make network size ﬁt within memory
constraints, we performed the majority of our hyperparameter search to optimize for CIFAR10 sample
quality, then transferred the resulting settings over to the other datasets:
We chose the tschedule from a set of constant, linear, and quadratic schedules, all
constrained so that LT0. We setT= 1000 without a sweep, and we chose a linear
schedule from 1= 104toT= 0:02.
We set the dropout rate on CIFAR10 to 0:1by sweeping over the values f0:1;0:2;0:3;0:4g.
Without dropout on CIFAR10, we obtained poorer samples reminiscent of the overﬁtting
artifacts in an unregularized PixelCNN++ [ 52]. We set dropout rate on the other datasets to
zero without sweeping.
We used random horizontal ﬂips during training for CIFAR10; we tried training both with
and without ﬂips, and found ﬂips to improve sample quality slightly. We also used random
horizontal ﬂips for all other datasets except LSUN Bedroom.
We tried Adam [ 31] and RMSProp early on in our experimentation process and chose the
former. We left the hyperparameters to their standard values. We set the learning rate to
2104without any sweeping, and we lowered it to 2105for the 256256images,
which seemed unstable to train with the larger learning rate.
14We set the batch size to 128 for CIFAR10 and 64 for larger images. We did not sweep over
these values.
We used EMA on model parameters with a decay factor of 0.9999. We did not sweep over
this value.
Final experiments were trained once and evaluated throughout training for sample quality. Sample
quality scores and log likelihood are reported on the minimum FID value over the course of training.
On CIFAR10, we calculated Inception and FID scores on 50000 samples using the original code
from the OpenAI [ 51] and TTUR [ 21] repositories, respectively. On LSUN, we calculated FID
scores on 50000 samples using code from the StyleGAN2 [ 30] repository. CIFAR10 and CelebA-HQ
were loaded as provided by TensorFlow Datasets ( https://www.tensorflow.org/datasets ),
and LSUN was prepared using code from StyleGAN. Dataset splits (or lack thereof) are standard
from the papers that introduced their usage in a generative modeling context. All details can be found
in the source code release.
C Discussion on related work
Our model architecture, forward process deﬁnition, and prior differ from NCSN [ 55,56] in subtle but
important ways that improve sample quality, and, notably, we directly train our sampler as a latent
variable model rather than adding it after training post-hoc. In greater detail:
1.We use a U-Net with self-attention; NCSN uses a ReﬁneNet with dilated convolutions. We
condition all layers on tby adding in the Transformer sinusoidal position embedding, rather
than only in normalization layers (NCSNv1) or only at the output (v2).
2.Diffusion models scale down the data with each forward process step (by ap1tfactor)
so that variance does not grow when adding noise, thus providing consistently scaled inputs
to the neural net reverse process. NCSN omits this scaling factor.
3.Unlike NCSN, our forward process destroys signal ( DKL(q(xTjx0)kN(0;I))0), ensur-
ing a close match between the prior and aggregate posterior of xT. Also unlike NCSN, our
tare very small, which ensures that the forward process is reversible by a Markov chain
with conditional Gaussians. Both of these factors prevent distribution shift when sampling.
4.Our Langevin-like sampler has coefﬁcients (learning rate, noise scale, etc.) derived rig-
orously from tin the forward process. Thus, our training procedure directly trains our
sampler to match the data distribution after Tsteps: it trains the sampler as a latent variable
model using variational inference. In contrast, NCSNs sampler coefﬁcients are set by hand
post-hoc, and their training procedure is not guaranteed to directly optimize a quality metric
of their sampler.
D Samples
Additional samples Figure 11, 13, 16, 17, 18, and 19 show uncurated samples from the diffusion
models trained on CelebA-HQ, CIFAR10 and LSUN datasets.
Latent structure and reverse process stochasticity During sampling, both the prior xT
N(0;I)and Langevin dynamics are stochastic. To understand the signiﬁcance of the second source
of noise, we sampled multiple images conditioned on the same intermediate latent for the CelebA
256256dataset. Figure 7 shows multiple draws from the reverse process x0p(x0jxt)that
share the latent xtfort2f1000;750;500;250g. To accomplish this, we run a single reverse chain
from an initial draw from the prior. At the intermediate timesteps, the chain is split to sample multiple
images. When the chain is split after the prior draw at xT=1000 , the samples differ signiﬁcantly.
However, when the chain is split after more steps, samples share high-level attributes like gender,
hair color, eyewear, saturation, pose and facial expression. This indicates that intermediate latents
likex750encode these attributes, despite their imperceptibility.
Coarse-to-ﬁne interpolation Figure 9 shows interpolations between a pair of source CelebA
256256images as we vary the number of diffusion steps prior to latent space interpolation.
Increasing the number of diffusion steps destroys more structure in the source images, which the
15model completes during the reverse process. This allows us to interpolate at both ﬁne granularities
and coarse granularities. In the limiting case of 0diffusion steps, the interpolation mixes source
images in pixel space. On the other hand, after 1000 diffusion steps, source information is lost and
interpolations are novel samples.
SourceRec.λ=0.1λ=0.2λ=0.3λ=0.4λ=0.5λ=0.6λ=0.7λ=0.8λ=0.9Rec.Source1000 steps875 steps750 steps625 steps500 steps375 steps250 steps125 steps0 steps
Figure 9: Coarse-to-ﬁne interpolations that vary the number of diffusion steps prior to latent mixing.
0 200 400 600 800 1;000246810
Reverse process steps ( Tt)Inception Score
0 200 400 600 800 1;0000100200300
Reverse process steps ( Tt)FID
Figure 10: Unconditional CIFAR10 progressive sampling quality over time
16Figure 11: CelebA-HQ 256256generated samples
17(a) Pixel space nearest neighbors
(b) Inception feature space nearest neighbors
Figure 12: CelebA-HQ 256256nearest neighbors, computed on a 100100crop surrounding the
faces. Generated samples are in the leftmost column, and training set nearest neighbors are in the
remaining columns.
18Figure 13: Unconditional CIFAR10 generated samples
19Figure 14: Unconditional CIFAR10 progressive generation
20(a) Pixel space nearest neighbors
(b) Inception feature space nearest neighbors
Figure 15: Unconditional CIFAR10 nearest neighbors. Generated samples are in the leftmost column,
and training set nearest neighbors are in the remaining columns.
21Figure 16: LSUN Church generated samples. FID= 7:89
22Figure 17: LSUN Bedroom generated samples, large model. FID= 4:90
23Figure 18: LSUN Bedroom generated samples, small model. FID= 6:36
24Figure 19: LSUN Cat generated samples. FID= 19:75
25
  Neural Discrete Representation Learning
Aaron van den Oord
DeepMind
avdnoord@google.comOriol Vinyals
DeepMind
vinyals@google.comKoray Kavukcuoglu
DeepMind
korayk@google.com
Abstract
Learning useful representations without supervision remains a key challenge in
machine learning. In this paper, we propose a simple yet powerful generative
model that learns such discrete representations. Our model, the Vector Quantised-
Variational AutoEncoder (VQ-V AE), differs from V AEs in two key ways: the
encoder network outputs discrete, rather than continuous, codes; and the prior
is learnt rather than static. In order to learn a discrete latent representation, we
incorporate ideas from vector quantisation (VQ). Using the VQ method allows the
model to circumvent issues of posterior collapse - where the latents are ignored
when they are paired with a powerful autoregressive decoder - typically observed
in the V AE framework. Pairing these representations with an autoregressive prior,
the model can generate high quality images, videos, and speech as well as doing
high quality speaker conversion and unsupervised learning of phonemes, providing
further evidence of the utility of the learnt representations.
1 Introduction
Recent advances in generative modelling of images [ 38,12,13,22,10], audio [ 37,26] and videos
[20,11] have yielded impressive samples and applications [ 24,18]. At the same time, challenging
tasks such as few-shot learning [ 34], domain adaptation [ 17], or reinforcement learning [ 35] heavily
rely on learnt representations from raw data, but the usefulness of generic representations trained in
an unsupervised fashion is still far from being the dominant approach.
Maximum likelihood and reconstruction error are two common objectives used to train unsupervised
models in the pixel domain, however their usefulness depends on the particular application the
features are used in. Our goal is to achieve a model that conserves the important features of the
data in its latent space while optimising for maximum likelihood. As the work in [ 7] suggests, the
best generative models (as measured by log-likelihood) will be those without latents but a powerful
decoder (such as PixelCNN). However, in this paper, we argue for learning discrete and useful latent
variables, which we demonstrate on a variety of domains.
Learning representations with continuous features have been the focus of many previous work
[16,39,6,9] however we concentrate on discrete representations [ 27,33,8,28] which are potentially
a more natural ﬁt for many of the modalities we are interested in. Language is inherently discrete,
similarly speech is typically represented as a sequence of symbols. Images can often be described
concisely by language [ 40]. Furthermore, discrete representations are a natural ﬁt for complex
reasoning, planning and predictive learning (e.g., if it rains, I will use an umbrella). While using
discrete latent variables in deep learning has proven challenging, powerful autoregressive models
have been developed for modelling distributions over discrete variables [37].
In our work, we introduce a new family of generative models succesfully combining the variational
autoencoder (V AE) framework with discrete latent representations through a novel parameterisation
of the posterior distribution of (discrete) latents given an observation. Our model, which relies on
vector quantization (VQ), is simple to train, does not suffer from large variance, and avoids the
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1711.00937v2  [cs.LG]  30 May 2018posterior collapse issue which has been problematic with many V AE models that have a powerful
decoder, often caused by latents being ignored. Additionally, it is the ﬁrst discrete latent V AE model
that get similar performance as its continuous counterparts, while offering the ﬂexibility of discrete
distributions. We term our model the VQ-V AE.
Since VQ-V AE can make effective use of the latent space, it can successfully model important
features that usually span many dimensions in data space (for example objects span many pixels in
images, phonemes in speech, the message in a text fragment, etc.) as opposed to focusing or spending
capacity on noise and imperceptible details which are often local.
Lastly, once a good discrete latent structure of a modality is discovered by the VQ-V AE, we train
a powerful prior over these discrete random variables, yielding interesting samples and useful
applications. For instance, when trained on speech we discover the latent structure of language
without any supervision or prior knowledge about phonemes or words. Furthermore, we can equip
our decoder with the speaker identity, which allows for speaker conversion, i.e., transferring the
voice from one speaker to another without changing the contents. We also show promising results on
learning long term structure of environments for RL.
Our contributions can thus be summarised as:
Introducing the VQ-V AE model, which is simple, uses discrete latents, does not suffer from
posterior collapse and has no variance issues.
We show that a discrete latent model (VQ-V AE) perform as well as its continuous model
counterparts in log-likelihood.
When paired with a powerful prior, our samples are coherent and high quality on a wide
variety of applications such as speech and video generation.
We show evidence of learning language through raw speech, without any supervision, and
show applications of unsupervised speaker conversion.
2 Related Work
In this work we present a new way of training variational autoencoders [ 23,32] with discrete latent
variables [ 27]. Using discrete variables in deep learning has proven challenging, as suggested by
the dominance of continuous latent variables in most of current work  even when the underlying
modality is inherently discrete.
There exist many alternatives for training discrete V AEs. The NVIL [ 27] estimator use a single-sample
objective to optimise the variational lower bound, and uses various variance-reduction techniques to
speed up training. VIMCO [ 28] optimises a multi-sample objective [ 5], which speeds up convergence
further by using multiple samples from the inference network.
Recently a few authors have suggested the use of a new continuous reparemetrisation based on the
so-called Concrete [ 25] or Gumbel-softmax [ 19] distribution, which is a continuous distribution and
has a temperature constant that can be annealed during training to converge to a discrete distribution
in the limit. In the beginning of training the variance of the gradients is low but biased, and towards
the end of training the variance becomes high but unbiased.
None of the above methods, however, close the performance gap of V AEs with continuous latent
variables where one can use the Gaussian reparameterisation trick which beneﬁts from much lower
variance in the gradients. Furthermore, most of these techniques are typically evaluated on relatively
small datasets such as MNIST, and the dimensionality of the latent distributions is small (e.g., below
8). In our work, we use three complex image datasets (CIFAR10, ImageNet, and DeepMind Lab) and
a raw speech dataset (VCTK).
Our work also extends the line of research where autoregressive distributions are used in the decoder
of V AEs and/or in the prior [ 14]. This has been done for language modelling with LSTM decoders [ 4],
and more recently with dilated convolutional decoders [ 42]. PixelCNNs [ 29,38] are convolutional
autoregressive models which have also been used as distribution in the decoder of V AEs [15, 7].
Finally, our approach also relates to work in image compression with neural networks. Theis et. al.
[36] use scalar quantisation to compress activations for lossy image compression before arithmetic
encoding. Other authors [ 1] propose a method for similar compression model with vector quantisation.
2The authors propose a continuous relaxation of vector quantisation which is annealed over time
to obtain a hard clustering. In their experiments they ﬁrst train an autoencoder, afterwards vector
quantisation is applied to the activations of the encoder, and ﬁnally the whole network is ﬁne tuned
using the soft-to-hard relaxation with a small learning rate. In our experiments we were unable to
train using the soft-to-hard relaxation approach from scratch as the decoder was always able to invert
the continuous relaxation during training, so that no actual quantisation took place.
3 VQ-VAE
Perhaps the work most related to our approach are V AEs. V AEs consist of the following parts:
an encoder network which parameterises a posterior distribution q(zjx)of discrete latent random
variableszgiven the input data x, a prior distribution p(z), and a decoder with a distribution p(xjz)
over input data.
Typically, the posteriors and priors in V AEs are assumed normally distributed with diagonal covari-
ance, which allows for the Gaussian reparametrisation trick to be used [ 32,23]. Extensions include
autoregressive prior and posterior models [ 14], normalising ﬂows [ 31,10], and inverse autoregressive
posteriors [22].
In this work we introduce the VQ-V AE where we use discrete latent variables with a new way of
training, inspired by vector quantisation (VQ). The posterior and prior distributions are categorical,
and the samples drawn from these distributions index an embedding table. These embeddings are
then used as input into the decoder network.
3.1 Discrete Latent variables
We deﬁne a latent embedding space e2RKDwhereKis the size of the discrete latent space (i.e.,
aK-way categorical), and Dis the dimensionality of each latent embedding vector ei. Note that
there areKembedding vectors ei2RD,i21;2;:::;K . As shown in Figure 1, the model takes an
inputx, that is passed through an encoder producing output ze(x). The discrete latent variables z
are then calculated by a nearest neighbour look-up using the shared embedding space eas shown in
equation 1. The input to the decoder is the corresponding embedding vector ekas given in equation 2.
One can see this forward computation pipeline as a regular autoencoder with a particular non-linearity
that maps the latents to 1-of-K embedding vectors. The complete set of parameters for the model are
union of parameters of the encoder, decoder, and the embedding space e. For sake of simplicity we
use a single random variable zto represent the discrete latent variables in this Section, however for
speech, image and videos we actually extract a 1D, 2D and 3D latent feature spaces respectively.
The posterior categorical distribution q(zjx)probabilities are deﬁned as one-hot as follows:
q(z=kjx) =1for k = argmin jkze(x)ejk2,
0otherwise; (1)
whereze(x)is the output of the encoder network. We view this model as a V AE in which we
can bound logp(x)with the ELBO. Our proposal distribution q(z=kjx)is deterministic, and by
deﬁning a simple uniform prior over zwe obtain a KL divergence constant and equal to logK.
The representation ze(x)is passed through the discretisation bottleneck followed by mapping onto
the nearest element of embedding eas given in equations 1 and 2.
zq(x) =ek;wherek=argminjkze(x)ejk2 (2)
3.2 Learning
Note that there is no real gradient deﬁned for equation 2, however we approximate the gradient
similar to the straight-through estimator [ 3] and just copy gradients from decoder input zq(x)to
encoder output ze(x). One could also use the subgradient through the quantisation operation, but this
simple estimator worked well for the initial experiments in this paper.
3Figure 1: Left: A ﬁgure describing the VQ-V AE. Right: Visualisation of the embedding space. The
output of the encoder z(x)is mapped to the nearest point e2. The gradientrzL(in red) will push the
encoder to change its output, which could alter the conﬁguration in the next forward pass.
During forward computation the nearest embedding zq(x)(equation 2) is passed to the decoder, and
during the backwards pass the gradient rzLis passed unaltered to the encoder. Since the output
representation of the encoder and the input to the decoder share the same Ddimensional space,
the gradients contain useful information for how the encoder has to change its output to lower the
reconstruction loss.
As seen on Figure 1 (right), the gradient can push the encoders output to be discretised differently in
the next forward pass, because the assignment in equation 1 will be different.
Equation 3 speciﬁes the overall loss function. It is has three components that are used to train
different parts of VQ-V AE. The ﬁrst term is the reconstruction loss (or the data term) which optimizes
the decoder and the encoder (through the estimator explained above). Due to the straight-through
gradient estimation of mapping from ze(x)tozq(x), the embeddings eireceive no gradients from
the reconstruction loss logp(zjzq(x)). Therefore, in order to learn the embedding space, we use one
of the simplest dictionary learning algorithms, Vector Quantisation (VQ). The VQ objective uses
thel2error to move the embedding vectors eitowards the encoder outputs ze(x)as shown in the
second term of equation 3. Because this loss term is only used for updating the dictionary, one can
alternatively also update the dictionary items as function of moving averages of ze(x)(not used for
the experiments in this work). For more details see Appendix A.1.
Finally, since the volume of the embedding space is dimensionless, it can grow arbitrarily if the
embeddings eido not train as fast as the encoder parameters. To make sure the encoder commits to
an embedding and its output does not grow, we add a commitment loss, the third term in equation 3.
Thus, the total training objective becomes:
L= logp(xjzq(x)) +ksg[ze(x)]ek2
2+kze(x)sg[e]k2
2; (3)
where sg stands for the stopgradient operator that is deﬁned as identity at forward computation time
and has zero partial derivatives, thus effectively constraining its operand to be a non-updated constant.
The decoder optimises the ﬁrst loss term only, the encoder optimises the ﬁrst and the last loss terms,
and the embeddings are optimised by the middle loss term. We found the resulting algorithm to be
quite robust to , as the results did not vary for values of ranging from 0:1to2:0. We use= 0:25
in all our experiments, although in general this would depend on the scale of reconstruction loss.
Since we assume a uniform prior for z, the KL term that usually appears in the ELBO is constant
w.r.t. the encoder parameters and can thus be ignored for training.
In our experiments we deﬁne Ndiscrete latents (e.g., we use a ﬁeld of 32 x 32 latents for ImageNet,
or 8 x 8 x 10 for CIFAR10). The resulting loss Lis identical, except that we get an average over N
terms fork-means and commitment loss  one for each latent.
The log-likelihood of the complete model logp(x)can be evaluated as follows:
logp(x) = logX
kp(xjzk)p(zk);
Because the decoder p(xjz)is trained with z=zq(x)from MAP-inference, the decoder should not
allocate any probability mass to p(xjz)forz6=zq(x)once it has fully converged. Thus, we can write
4logp(x)logp(xjzq(x))p(zq(x)). We empirically evaluate this approximation in section 4. From
Jensens inequality, we also can write logp(x)logp(xjzq(x))p(zq(x)).
3.3 Prior
The prior distribution over the discrete latents p(z)is a categorical distribution, and can be made
autoregressive by depending on other zin the feature map. Whilst training the VQ-V AE, the prior is
kept constant and uniform. After training, we ﬁt an autoregressive distribution over z,p(z), so that
we can generate xvia ancestral sampling. We use a PixelCNN over the discrete latents for images,
and a WaveNet for raw audio. Training the prior and the VQ-V AE jointly, which could strengthen our
results, is left as future research.
4 Experiments
4.1 Comparison with continuous variables
As a ﬁrst experiment we compare VQ-V AE with normal V AEs (with continuous variables), as well as
VIMCO [ 28] with independent Gaussian or categorical priors. We train these models using the same
standard V AE architecture on CIFAR10, while varying the latent capacity (number of continuous or
discrete latent variables, as well as the dimensionality of the discrete space K). The encoder consists
of 2 strided convolutional layers with stride 2 and window size 44, followed by two residual
33blocks (implemented as ReLU, 3x3 conv, ReLU, 1x1 conv), all having 256 hidden units. The
decoder similarly has two residual 33blocks, followed by two transposed convolutions with stride
2 and window size 44. We use the ADAM optimiser [ 21] with learning rate 2e-4 and evaluate
the performance after 250,000 steps with batch-size 128. For VIMCO we use 50 samples in the
multi-sample training objective.
The V AE, VQ-V AE and VIMCO models obtain 4.51 bits/dim, 4.67 bits/dim and 5.14 respectively.
All reported likelihoods are lower bounds. Our numbers for the continuous V AE are comparable to
those reported for a Deep convolutional V AE: 4.54 bits/dim [13] on this dataset.
Our model is the ﬁrst among those using discrete latent variables which challenges the performance
of continuous V AEs. Thus, we get very good reconstructions like regular V AEs provide, with the
compressed representation that symbolic representations provide. A few interesting characteristics,
implications and applications of the VQ-V AEs that we train is shown in the next subsections.
4.2 Images
Images contain a lot of redundant information as most of the pixels are correlated and noisy, therefore
learning models at the pixel level could be wasteful.
In this experiment we show that we can model x= 1281283images by compressing them to a
z= 32321discrete space (with K=512) via a purely deconvolutional p(xjz). So a reduction of
12812838
3232942:6in bits. We model images by learning a powerful prior (PixelCNN) over z. This
allows to not only greatly speed up training and sampling, but also to use the PixelCNNs capacity to
capture the global structure instead of the low-level statistics of images.
Figure 2: Left: ImageNet 128x128x3 images, right: reconstructions from a VQ-V AE with a 32x32x1
latent space, with K=512.
5Reconstructions from the 32x32x1 space with discrete latents are shown in Figure 2. Even considering
that we greatly reduce the dimensionality with discrete encoding, the reconstructions look only slightly
blurrier than the originals. It would be possible to use a more perceptual loss function than MSE over
pixels here (e.g., a GAN [12]), but we leave that as future work.
Next, we train a PixelCNN prior on the discretised 32x32x1 latent space. As we only have 1 channel
(not 3 as with colours), we only have to use spatial masking in the PixelCNN. The capacity of the
PixelCNN we used was similar to those used by the authors of the PixelCNN paper [38].
Figure 3: Samples (128x128) from a VQ-V AE with a PixelCNN prior trained on ImageNet images.
From left to right: kit fox, gray whale, brown bear, admiral (butterﬂy), coral reef, alp, microwave,
pickup.
Samples drawn from the PixelCNN were mapped to pixel-space with the decoder of the VQ-V AE
and can be seen in Figure 3.
Figure 4: Samples (128x128) from a VQ-V AE with a PixelCNN prior trained on frames captured
from DeepMind Lab.
We also repeat the same experiment for 84x84x3 frames drawn from the DeepMind Lab environment
[2]. The reconstructions looked nearly identical to their originals. Samples drawn from the PixelCNN
prior trained on the 21x21x1 latent space and decoded to the pixel space using a deconvolutional
model decoder can be seen in Figure 4.
Finally, we train a second VQ-V AE with a PixelCNN decoder on top of the 21x21x1 latent space
from the ﬁrst VQ-V AE on DM-LAB frames. This setup typically breaks V AEs as they suffer from
posterior collapse, i.e., the latents are ignored as the decoder is powerful enough to model x
perfectly. Our model however does not suffer from this, and the latents are meaningfully used. We use
only three latent variables (each with K=512 and their own embedding space e) at the second stage
for modelling the whole image and as such the model cannot reconstruct the image perfectly  which
is consequence of compressing the image onto 3 x 9 bits, i.e. less than a ﬂoat32. Reconstructions
sampled from the discretised global code can be seen in Figure 5.
6Figure 5: Top original images, Bottom: reconstructions from a 2 stage VQ-V AE, with 3 latents to
model the whole image (27 bits), and as such the model cannot reconstruct the images perfectly. The
reconstructions are generated by sampled from the second PixelCNN prior in the 21x21 latent domain
of ﬁrst VQ-V AE, and then decoded with standard VQ-V AE decoder to 84x84. A lot of the original
scene, including textures, room layout and nearby walls remain, but the model does not try to store
the pixel values themselves, which means the textures are generated procedurally by the PixelCNN.
Figure 6: Left: original waveform, middle: reconstructed with same speaker-id, right: reconstructed
with different speaker-id. The contents of the three waveforms are the same.
4.3 Audio
In this set of experiments we evaluate the behaviour of discrete latent variables on models of raw
audio. In all our audio experiments, we train a VQ-V AE that has a dilated convolutional architecture
similar to WaveNet decoder. All samples for this section can be played from the following url:
https://avdnoord.github.io/homepage/vqvae/ .
We ﬁrst consider the VCTK dataset, which has speech recordings of 109 different speakers [ 41].
We train a VQ-V AE where the encoder has 6 strided convolutions with stride 2 and window-size 4.
This yields a latent space 64x smaller than the original waveform. The latents consist of one feature
map and the discrete space is 512-dimensional. The decoder is conditioned on both the latents and a
one-hot embedding for the speaker.
First, we ran an experiment to show that VQ-V AE can extract a latent space that only conserves
long-term relevant information. After training the model, given an audio example, we can encode
it to the discrete latent representation, and reconstruct by sampling from the decoder. Because the
dimensionality of the discrete representation is 64 times smaller, the original sample cannot be
perfectly reconstructed sample by sample. As it can be heard from the provided samples, and as
shown in Figure 7, the reconstruction has the same content (same text contents), but the waveform
is quite different and prosody in the voice is altered. This means that the VQ-V AE has, without
any form of linguistic supervision, learned a high-level abstract space that is invariant to low-level
features and only encodes the content of the speech. This experiment conﬁrms our observations from
before that important features are often those that span many dimensions in the input data space (in
this case phoneme and other high-level content in waveform).
We have then analysed the unconditional samples from the model to understand its capabilities. Given
the compact and abstract latent representation extracted from the audio, we trained the prior on top of
this representation to model the long-term dependencies in the data. For this task we have used a
larger dataset of 460 speakers [ 30] and trained a VQ-V AE model where the resolution of discrete
space is 128 times smaller. Next we trained the prior as usual on top of this representation on chunks
of 40960 timesteps (2.56 seconds), which yields 320 latent timesteps. While samples drawn from even
the best speech models like the original WaveNet [ 37] sound like babbling , samples from VQ-V AE
contain clear words and part-sentences (see samples linked above). We conclude that VQ-V AE was
able to model a rudimentary phoneme-level language model in a completely unsupervised fashion
from raw audio waveforms.
7Next, we attempted the speaker conversion where the latents are extracted from one speaker and then
reconstructed through the decoder using a separate speaker id. As can be heard from the samples,
the synthesised speech has the same content as the original sample, but with the voice from the
second speaker. This experiment again demonstrates that the encoded representation has factored out
speaker-speciﬁc information: the embeddings not only have the same meaning regardless of details
in the waveform, but also across different voice-characteristics.
Finally, in an attempt to better understand the content of the discrete codes we have compared the
latents one-to-one with the ground-truth phoneme-sequence (which was not used any way to train the
VQ-V AE). With a 128-dimensional discrete space that runs at 25Hz (encoder downsampling factor
of640), we mapped every of the 128 possible latent values to one of the 41 possible phoneme values1
(by taking the conditionally most likely phoneme). The accuracy of this 41-way classiﬁcation was
49:3%, while a random latent space would result in an accuracy of 7:2%(prior most likely phoneme).
It is clear that these discrete latent codes obtained in a fully unsupervised way are high-level speech
descriptors that are closely related to phonemes.
4.4 Video
For our ﬁnal experiment we have used the DeepMind Lab [ 2] environment to train a generative model
conditioned on a given action sequence. In Figure 7 we show the initial 6frames that are input to the
model followed by 10frames that are sampled from VQ-V AE with all actions set to forward (top row)
andright (bottom row). Generation of the video sequence with the VQ-V AE model is done purely in
the latent space, ztwithout the need to generate the actual images themselves. Each image in the
sequencextis then created by mapping the latents with a deterministic decoder to the pixel space
after all the latents are generated using only the prior model p(z1;:::;z T). Therefore, VQ-V AE can
be used to imagine long sequences purely in latent space without resorting to pixel space. It can be
seen that the model has learnt to successfully generate a sequence of frames conditioned on given
action without any degradation in the visual quality whilst keeping the local geometry correct. For
completeness, we trained a model without actions and obtained similar results, not shown due to
space constraints.
Figure 7: First 6 frames are provided to the model, following frames are generated conditioned on an
action. Top: repeated action move forward, bottom: repeated action move right.
5 Conclusion
In this work we have introduced VQ-V AE, a new family of models that combine V AEs with vector
quantisation to obtain a discrete latent representation. We have shown that VQ-V AEs are capable of
modelling very long term dependencies through their compressed discrete latent space which we have
demonstrated by generating 128128colour images, sampling action conditional video sequences
and ﬁnally using audio where even an unconditional model can generate surprisingly meaningful
chunks of speech and doing speaker conversion. All these experiments demonstrated that the discrete
latent space learnt by VQ-V AEs capture important features of the data in a completely unsupervised
manner. Moreover, VQ-V AEs achieve likelihoods that are almost as good as their continuous latent
variable counterparts on CIFAR10 data. We believe that this is the ﬁrst discrete latent variable model
that can successfully model long range sequences and fully unsupervisedly learn high-level speech
descriptors that are closely related to phonemes.
1Note that the encoder/decoder pairs could make the meaning of every discrete latent depend on previous
latents in the sequence, e.g.. bi/tri-grams (and thus achieve a higher compression) which means a more advanced
mapping to phonemes would results in higher accuracy.
8References
[1]Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu Timofte, Luca Benini, and
Luc Van Gool. Soft-to-hard vector quantization for end-to-end learned compression of images and neural
networks. arXiv preprint arXiv:1704.00648 , 2017.
[2]Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew
Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801 ,
2016.
[3]Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through
stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 , 2013.
[4]Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio.
Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349 , 2015.
[5]Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv preprint
arXiv:1509.00519 , 2015.
[6]Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. CoRR ,
abs/1606.03657, 2016.
[7]Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever,
and Pieter Abbeel. Variational lossy autoencoder. arXiv preprint arXiv:1611.02731 , 2016.
[8]Aaron Courville, James Bergstra, and Yoshua Bengio. A spike and slab restricted boltzmann machine. In
Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics , pages
233241, 2011.
[9]Emily Denton, Sam Gross, and Rob Fergus. Semi-supervised learning with context-conditional generative
adversarial networks. arXiv preprint arXiv:1611.06430 , 2016.
[10] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv preprint
arXiv:1605.08803 , 2016.
[11] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through
video prediction. In Advances in Neural Information Processing Systems , pages 6472, 2016.
[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing
systems , pages 26722680, 2014.
[13] Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards
conceptual compression. In Advances In Neural Information Processing Systems , pages 35493557, 2016.
[14] Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. Deep autoregressive
networks. arXiv preprint arXiv:1310.8499 , 2013.
[15] Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vázquez, and
Aaron C. Courville. Pixelvae: A latent variable model for natural images. CoRR , abs/1611.05013, 2016.
[16] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks.
science , 313(5786):504507, 2006.
[17] Judy Hoffman, Erik Rodner, Jeff Donahue, Trevor Darrell, and Kate Saenko. Efﬁcient learning of
domain-invariant image representations. arXiv preprint arXiv:1301.3224 , 2013.
[18] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional
adversarial networks. arXiv preprint arXiv:1611.07004 , 2016.
[19] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144 , 2016.
[20] Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, and
Koray Kavukcuoglu. Video pixel networks. arXiv preprint arXiv:1610.00527 , 2016.
[21] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
[22] Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved
variational inference with inverse autoregressive ﬂow. NIPS 2016 , 2016.
[23] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 ,
2013.
[24] Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Cunningham, Alejandro Acosta,
Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-
resolution using a generative adversarial network. arXiv preprint arXiv:1609.04802 , 2016.
9[25] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of
discrete random variables. arXiv preprint arXiv:1611.00712 , 2016.
[26] Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron
Courville, and Yoshua Bengio. Samplernn: An unconditional end-to-end neural audio generation model.
arXiv preprint arXiv:1612.07837 , 2016.
[27] Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. arXiv
preprint arXiv:1402.0030 , 2014.
[28] Andriy Mnih and Danilo Jimenez Rezende. Variational inference for monte carlo objectives. CoRR ,
abs/1602.06725, 2016.
[29] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv
preprint arXiv:1601.06759 , 2016.
[30] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus
based on public domain audio books. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE
International Conference on , pages 52065210. IEEE, 2015.
[31] Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. arXiv
preprint arXiv:1505.05770 , 2015.
[32] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approxi-
mate inference in deep generative models. arXiv preprint arXiv:1401.4082 , 2014.
[33] Ruslan Salakhutdinov and Geoffrey Hinton. Deep boltzmann machines. In Artiﬁcial Intelligence and
Statistics , pages 448455, 2009.
[34] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. One-shot
learning with memory-augmented neural networks. arXiv preprint arXiv:1605.06065 , 2016.
[35] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction , volume 1. MIT press
Cambridge, 1998.
[36] Lucas Theis, Wenzhe Shi, Andrew Cunningham, and Ferenc Huszár. Lossy image compression with
compressive autoencoders. arXiv preprint arXiv:1703.00395 , 2017.
[37] Aäron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal
Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio.
CoRR abs/1609.03499 , 2016.
[38] Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional
image generation with pixelcnn decoders. In Advances in Neural Information Processing Systems , pages
47904798, 2016.
[39] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked
denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.
Journal of Machine Learning Research , 11(Dec):33713408, 2010.
[40] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image
caption generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ,
pages 31563164, 2015.
[41] Junichi Yamagishi. English multi-speaker corpus for cstr voice cloning toolkit. URL http://homepages. inf.
ed. ac. uk/jyamagis/page3/page58/page58. html , 2012.
[42] Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick. Improved variational
autoencoders for text modeling using dilated convolutions. CoRR , abs/1702.08139, 2017.
10A Appendix
A.1 VQ-VAE dictionary updates with Exponential Moving Averages
As mentioned in Section 3.2, one can also use exponential moving averages (EMA) to update the dictionary
items instead of the loss term from Equation 3:
ksg[ze(x)]ek2
2: (4)
Letfzi;1;zi;2;:::;z i;nigbe the set of nioutputs from the encoder that are closest to dictionary item ei, so that
we can write the loss as:niX
jkzi;jeik2
2: (5)
The optimal value for eihas a closed form solution, which is simply the average of elements in the set:
ei=1
niniX
jzi;j:
This update is typically used in algorithms such as K-Means.
However, we cannot use this update directly when working with minibatches. Instead we can use exponential
moving averages as an online version of this update:
N(t)
i:=N(t1)
i
+n(t)
i(1
) (6)
m(t)
i:=m(t1)
i
+X
jz(t)
i;j(1
) (7)
e(t)
i:=m(t)
i
N(t)
i; (8)
with
a value between 0 and 1. We found 
= 0:99to work well in practice.
11
  Retrieval-Augmented Generation for
Knowledge-Intensive NLP Tasks
Patrick Lewisyz, Ethan Perez?,
Aleksandra Piktusy, Fabio Petroniy, Vladimir Karpukhiny, Naman Goyaly, Heinrich Küttlery,
Mike Lewisy, Wen-tau Yihy, Tim Rocktäschelyz, Sebastian Riedelyz, Douwe Kielay
yFacebook AI Research;zUniversity College London;?New York University;
plewis@fb.com
Abstract
Large pre-trained language models have been shown to store factual knowledge
in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-
stream NLP tasks. However, their ability to access and precisely manipulate knowl-
edge is still limited, and hence on knowledge-intensive tasks, their performance
lags behind task-speciﬁc architectures. Additionally, providing provenance for their
decisions and updating their world knowledge remain open research problems. Pre-
trained models with a differentiable access mechanism to explicit non-parametric
memory have so far been only investigated for extractive downstream tasks. We
explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation
(RAG)  models which combine pre-trained parametric and non-parametric mem-
ory for language generation. We introduce RAG models where the parametric
memory is a pre-trained seq2seq model and the non-parametric memory is a dense
vector index of Wikipedia, accessed with a pre-trained neural retriever. We com-
pare two RAG formulations, one which conditions on the same retrieved passages
across the whole generated sequence, and another which can use different passages
per token. We ﬁne-tune and evaluate our models on a wide range of knowledge-
intensive NLP tasks and set the state of the art on three open domain QA tasks,
outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract
architectures. For language generation tasks, we ﬁnd that RAG models generate
more speciﬁc, diverse and factual language than a state-of-the-art parametric-only
seq2seq baseline.
1 Introduction
Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-
edge from data [ 47]. They can do so without any access to an external memory, as a parameterized
implicit knowledge base [ 51,52]. While this development is exciting, such models do have down-
sides: They cannot easily expand or revise their memory, cant straightforwardly provide insight into
their predictions, and may produce hallucinations [ 38]. Hybrid models that combine parametric
memory with non-parametric (i.e., retrieval-based) memories [ 20,26,48] can address some of these
issues because knowledge can be directly revised and expanded, and accessed knowledge can be
inspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that
combine masked language models [ 8] with a differentiable retriever, have shown promising results,arXiv:2005.11401v4  [cs.CL]  12 Apr 2021The	Divine
Comedy	(x) qQuery
Encoder
q(x)
MIPS p θGenerator  pθ
(Parametric)
Margin-
alize
This	14th	century	work
is	divided	into	3
sections:	Inferno,
Purgatorio	&
Paradiso									 (y)End-to-End Backprop through q and  p θ
Barack	Obama	was
born	in	Hawaii. (x)
Fact V eriﬁcation: Fact Querysupports 	(y)
Question GenerationFact V eriﬁcation:
Label GenerationDocument
IndexDefine	middle	ear (x)
Question Answering:
Question QueryThe	middle	ear	includes
the	tympanic	cavity	and
the	three	ossicles.		 (y)
Question Answering:
Answer GenerationRetriever pη
(Non-Parametric)
z 4
z3
z2
z 1d(z)
Jeopardy Question
Generation:
Answer QueryFigure 1: Overview of our approach. We combine a pre-trained retriever ( Query Encoder +Document
Index ) with a pre-trained seq2seq model ( Generator ) and ﬁne-tune end-to-end. For query x, we use
Maximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we
treatzas a latent variable and marginalize over seq2seq predictions given different documents.
but have only explored open-domain extractive question answering. Here, we bring hybrid parametric
and non-parametric memory to the workhorse of NLP, i.e. sequence-to-sequence (seq2seq) models.
We endow pre-trained, parametric-memory generation models with a non-parametric memory through
a general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG).
We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the
non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural
retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The
retriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on
the input, and the seq2seq model (BART [ 32]) then conditions on these latent documents together with
the input to generate the output. We marginalize the latent documents with a top-K approximation,
either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token
basis (where different documents are responsible for different tokens). Like T5 [ 51] or BART, RAG
can be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.
There has been extensive previous work proposing architectures to enrich systems with non-parametric
memory which are trained from scratch for speciﬁc tasks, e.g. memory networks [ 64,55], stack-
augmented networks [ 25] and memory layers [ 30]. In contrast, we explore a setting where both
parametric and non-parametric memory components are pre-trained and pre-loaded with extensive
knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is
present without additional training.
Our results highlight the beneﬁts of combining parametric and non-parametric memory with genera-
tion for knowledge-intensive tasks tasks that humans could not reasonably be expected to perform
without access to an external knowledge source. Our RAG models achieve state-of-the-art results
on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2] and strongly outperform
recent approaches that use specialised pre-training objectives on TriviaQA [ 24]. Despite these being
extractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches.
For knowledge-intensive generation, we experiment with MS-MARCO [ 1] and Jeopardy question
generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and
diverse than a BART baseline. For FEVER [ 56] fact veriﬁcation, we achieve results within 4.3% of
state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that
the non-parametric memory can be replaced to update the models knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequence xto retrieve text documents zand use them
as additional context when generating the target sequence y. As shown in Figure 1, our models
leverage two components: (i) a retriever p(zjx)with parameters that returns (top-K truncated)
distributions over text passages given a query xand (ii) a generator p(yijx;z;y 1:i1)parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [ 66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/ . An interactive demo of RAG models can be found at https://huggingface.co/rag/
2bythat generates a current token based on a context of the previous i1tokensy1:i1, the original
inputxand a retrieved passage z.
To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable.
We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence , the model uses the same document
to predict each target token. The second approach, RAG-Token , can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pandpcomponents, as well as the training and decoding procedure.
2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence . Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(yjx)via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence (yjx)X
z2top-k(p(jx))p(zjx)p(yjx;z) =X
z2top-k(p(jx))p(zjx)NY
ip(yijx;z;y 1:i1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deﬁne:
pRAG-Token (yjx)NY
iX
z2top-k(p(jx))p(zjx)p(yijx;z;y 1:i1)
Finally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.
2.2 Retriever: DPR
The retrieval component p(zjx)is based on DPR [26]. DPR follows a bi-encoder architecture:
p(zjx)/exp
d(z)>q(x)
d(z) =BERTd(z);q(x) =BERTq(x)
where d(z)is a dense representation of a document produced by a BERT BASE document encoder [8],
andq(x)a query representation produced by a query encoder , also based on BERT BASE. Calculating
top-k (p(jx)), the list ofkdocumentszwith highest prior probability p(zjx), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [ 23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This
retriever was trained to retrieve documents which contain answers to TriviaQA [ 24] questions and
Natural Questions [29]. We refer to the document index as the non-parametric memory .
2.3 Generator: BART
The generator component p(yijx;z;y 1:i1)could be modelled using any encoder-decoder. We use
BART-large [ 32], a pre-trained seq2seq transformer [ 58] with 400M parameters. To combine the input
xwith the retrieved content zwhen generating from BART, we simply concatenate them. BART was
pre-trained using a denoising objective and a variety of different noising functions. It has obtained
state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5
models [32]. We refer to the BART generator parameters as the parametric memory henceforth.
2.4 Training
We jointly train the retriever and generator components without any direct supervision on what
document should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs (xj;yj), we
3minimize the negative marginal log-likelihood of each target,P
jlogp(yjjxj)using stochastic
gradient descent with Adam [ 28]. Updating the document encoder BERTdduring training is costly as
it requires the document index to be periodically updated as REALM does during pre-training [ 20].
We do not ﬁnd this step necessary for strong performance, and keep the document encoder (and
index) ﬁxed, only ﬁne-tuning the query encoder BERT qand the BART generator.
2.5 Decoding
At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxyp(yjx).
RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera-
tor with transition probability: p0
(yijx;y 1:i1) =P
z2top-k(p(jx))p(zijx)p(yijx;zi;y1:i1)To
decode, we can plug p0
(yijx;y 1:i1)into a standard beam decoder.
RAG-Sequence For RAG-Sequence, the likelihood p(yjx)does not break into a conventional per-
token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for
each document z, scoring each hypothesis using p(yijx;z;y 1:i1). This yields a set of hypotheses
Y, some of which may not have appeared in the beams of all documents. To estimate the probability
of an hypothesis ywe run an additional forward pass for each document zfor whichydoes not
appear in the beam, multiply generator probability with p(zjx)and then sum the probabilities across
beams for the marginals. We refer to this decoding procedure as Thorough Decoding. For longer
output sequences,jYjcan become large, requiring many forward passes. For more efﬁcient decoding,
we can make a further approximation that p(yjx;zi)0whereywas not generated during beam
search from x;zi. This avoids the need to run additional forward passes once the candidate set Yhas
been generated. We refer to this decoding procedure as Fast Decoding.
3 Experiments
We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use
a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [ 23] with a Hierarchical
Navigable Small World approximation for fast retrieval [ 37]. During training, we retrieve the top
kdocuments for each query. We consider k2f5;10gfor training and set kfor test time using dev
data. We now discuss experimental details for each task.
3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [ 20]. We treat questions and answers as input-output text pairs (x;y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [ 5,7,31,26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to Closed-Book
QA approaches [ 52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [ 29], TriviaQA (TQA) [ 24]. WebQuestions (WQ) [ 3] and CuratedTrec (CT) [ 2]. As
CT and WQ are small, we follow DPR [ 26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31,26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.
3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAGs natural language generation (NLG) in a knowledge-intensive setting,
we use the MSMARCO NLG task v2.1 [ 43]. The task consists of questions, ten gold passages
retrieved from a search engine for each question, and a full sentence answer annotated from the
retrieved passages. We do not use the supplied passages, only the questions and answers, to treat
4MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be
answered in a way that matches the reference answer without access to the gold passages, such as
What is the weather in V olcano, CA? so performance will be lower without using gold passages.
We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here,
RAG can rely on parametric knowledge to generate reasonable responses.
3.3 Jeopardy Question Generation
To evaluate RAGs generation abilities in a non-QA setting, we study open-domain question gen-
eration. Rather than use questions from standard open-domain QA tasks, which typically consist
of short, simple questions, we propose the more demanding task of generating Jeopardy questions.
Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.
For example, The World Cup is the answer to the question In 1986 Mexico scored as the ﬁrst
country to host this international sports competition twice. As Jeopardy questions are precise,
factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a
challenging knowledge-intensive generation task.
We use the splits from SearchQA [ 10], with 100K train, 14K dev, and 27K test examples. As
this is a new task, we train a BART model for comparison. Following [ 67], we evaluate using the
SQuAD-tuned Q-BLEU-1 metric [ 42]. Q-BLEU is a variant of BLEU with a higher weight for
matching entities and has higher correlation with human judgment for question generation than
standard metrics. We also perform two human evaluations, one to assess generation factuality, and
one for speciﬁcity. We deﬁne factuality as whether a statement can be corroborated by trusted external
sources, and speciﬁcity as high mutual dependence between the input and output [ 33]. We follow
best practice and use pairwise comparative evaluation [ 34]. Evaluators are shown an answer and two
generated questions, one from BART and one from RAG. They are then asked to pick one of four
optionsquuestion A is better, question B is better, both are good, or neither is good.
3.4 Fact Veriﬁcation
FEVER [ 56] requires classifying whether a natural language claim is supported or refuted by
Wikipedia, or whether there is not enough information to decide. The task requires retrieving
evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify
whether the claim is true, false, or unveriﬁable from Wikipedia alone. FEVER is a retrieval problem
coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for
exploring the RAG models ability to handle classiﬁcation rather than generation. We map FEVER
class labels (supports, refutes, or not enough info) to single output tokens and directly train with
claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on
retrieved evidence. In many real-world applications, retrieval supervision signals arent available, and
models that do not require such supervision will be applicable to a wider range of tasks. We explore
two variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way
(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.
4 Results
4.1 Open-domain Question Answering
Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA
tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines
the generation ﬂexibility of the closed-book (parametric only) approaches and the performance of
open-book retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results
without expensive, specialized salient span masking pre-training [ 20]. It is worth noting that RAGs
retriever is initialized using DPRs retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based cross-
encoder to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.
There are several advantages to generating answers even when it is possible to extract them. Docu-
ments with clues about the answer but do not contain the answer verbatim can still contribute towards
a correct answer being generated, which is not possible with standard extractive approaches, leading
5Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details.
Model NQ TQA WQ CT
Closed
BookT5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
BookREALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/ 68.0 45.2 52.2Table 2: Generation and classiﬁcation Test Scores.
MS-MARCO SotA is [ 4], FEVER-3 is [ 68] and
FEVER-2 is [ 57] *Uses gold context/evidence.
Best model without gold access underlined.
Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc.
SotA - - 49.8*49.9*76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok. 17.3 22.2 40.1 41.572.5 89.5RAG-Seq. 14.7 21.4 40.8 44.2
to more effective marginalization over documents. Furthermore, RAG can generate correct answers
even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such
cases for NQ, where an extractive model would score 0%.
4.2 Abstractive Question Answering
As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu
points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is
impressive given that (i) those models access gold passages with speciﬁc information required to
generate the reference answer , (ii) many questions are unanswerable without the gold passages, and
(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers
from our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually
correct text more often than BART. Later, we also show that RAG generations are more diverse than
BART generations (see 4.5).
4.3 Jeopardy Question Generation
Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,
with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452
pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual
than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and
BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on
the task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more
speciﬁc by a large margin. Table 3 shows typical generations from each model.
Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform
best because it can generate responses that combine content from several documents. Figure 2 shows
an example. When generating Sun, the posterior is high for document 2 which mentions The
Sun Also Rises. Similarly, document 1 dominates the posterior when A Farewell to Arms is
generated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens.
This observation suggests that the generator can complete the titles without depending on speciﬁc
documents. In other words, the models parametric knowledge is sufﬁcient to complete the titles. We
ﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding The
Sun. BART completes the generation The SunAlso Rises isanovel bythis author ofThe Sun
Also Rises indicating the title The Sun Also Rises is stored in BARTs parameters. Similarly,
BART will complete the partial decoding The SunAlso Rises isanovel bythis author ofA
with The SunAlso Rises isanovel bythis author ofAFarewell toArms . This example shows
how parametric and non-parametric memories work together the non-parametric component helps
to guide the generation, drawing out speciﬁc knowledge stored in the parametric memory.
4.4 Fact Veriﬁcation
Table 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of
state-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and
substantial engineering, trained using intermediate retrieval supervision, which RAG does not require.
6Document 1 : his works are considered classics of American
literature ... His wartime experiences formed the basis for his novel
A Farewell to Arms (1929) ...
Document 2 : ... artists of the 1920s Lost Generation expatriate
community. His debut novel, The Sun Also Rises , was published
in 1926.
BOS
TheSunAlsoRisesisa
novelbythis
authorofA
Farewellto
ArmsDoc 1
Doc 2
Doc 3
Doc 4
Doc 5Figure 2: RAG-Token document posterior p(zijx;yi;yi)for each generated token for input Hem-
ingway for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high
when generating A Farewell to Arms and for document 2 when generating The Sun Also Rises.
Table 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate
responses. ? indicates factually incorrect responses, * indicates partially correct responses.
Task Input Model Generation
MS-
MARCOdeﬁne middle
earBART?The middle ear is the part of the ear between the middle ear and the nose.
RAG-T The middle ear is the portion of the ear internal to the eardrum.
RAG-S The middle ear includes the tympanic cavity and the three ossicles.
what currency
needed in
scotlandBART The currency needed in Scotland is Pound sterling.
RAG-T Pound is the currency needed in Scotland.
RAG-S The currency needed in Scotland is the pound sterling.
Jeopardy
Question
Gener
-ationWashingtonBART?This state has the largest number of counties in the U.S.
RAG-T Its the only U.S. state named for a U.S. president
RAG-S Its the state where youll ﬁnd Mount Rainier National Park
The Divine
ComedyBART*This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio
RAG-T Dantes Inferno is the ﬁrst part of this epic poem
RAG-S This 14th century work is divided into 3 sections: Inferno, Purgatorio & Paradiso
For 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [ 35]
to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy
within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.
We also analyze whether documents retrieved by RAG correspond to documents annotated as gold
evidence in FEVER. We calculate the overlap in article titles between the top kdocuments retrieved
by RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article
in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.
4.5 Additional Results
Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than
BART for Jeopardy question generation. Following recent work on diversity-promoting decoding
[33,59,39], we also investigate generation diversity by calculating the ratio of distinct ngrams to
total ngrams generated by different models. Table 5 shows that RAG-Sequences generations are
more diverse than RAG-Tokens, and both are signiﬁcantly more diverse than BART without needing
any diversity-promoting decoding.
Retrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task.
To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever
during training. As shown in Table 6, learned retrieval improves results for all tasks.
We compare RAGs dense retriever to a word overlap-based BM25 retriever [ 53]. Here, we replace
RAGs retriever with a ﬁxed BM25 system, and use BM25 retrieval scores as logits when calculating
p(zjx). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are
heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval
improves results on all other tasks, especially for Open-Domain QA, where it is crucial.
Index hot-swapping An advantage of non-parametric memory models like RAG is that knowledge
can be easily updated at test time. Parametric-only models like T5 or BART need further training to
update their behavior as the world changes. To demonstrate, we build an index using the DrQA [ 5]
Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer
index from our main results (December 2018). We prepare a list of 82 world leaders who had changed
7Table 4: Human assessments for the Jeopardy
Question Generation Task.
Factuality Speciﬁcity
BART better 7.1% 16.8%
RAG better 42.7% 37.4%
Both good 11.7% 11.8%
Both poor 17.7% 6.9%
No majority 20.8% 20.1%Table 5: Ratio of distinct to total tri-grams for
generation tasks.
MSMARCO Jeopardy QGen
Gold 89.6% 90.0%
BART 70.7% 32.4%
RAG-Token 77.8% 46.8%
RAG-Seq. 83.5% 53.8%
Table 6: Ablations on the dev set. As FEVER is a classiﬁcation task, both RAG models are equivalent.
Model NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 FVR-2
Exact Match B-1 QB-1 R-L B-1 Label Accuracy
RAG-Token-BM25 29.7 41.5 32.1 33.1 17.5 22.3 55.5 48.475.1 91.6RAG-Sequence-BM25 31.8 44.1 36.6 33.8 11.1 19.5 56.5 46.9
RAG-Token-Frozen 37.8 50.1 37.1 51.1 16.7 21.7 55.9 49.472.9 89.4RAG-Sequence-Frozen 41.2 52.1 41.8 52.6 11.8 19.6 56.7 47.3
RAG-Token 43.5 54.8 46.5 51.9 17.9 22.6 56.2 49.474.5 90.6RAG-Sequence 44.0 55.8 44.9 53.4 15.3 21.5 57.2 47.5
between these dates and use a template Who is {position}? (e.g. Who is the President of Peru?)
to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for
2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched
indices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders).
This shows we can update RAGs world knowledge by simply replacing its non-parametric memory.
Effect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent
documents, and we do not observe signiﬁcant differences in performance between them. We have the
ﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and
runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves
Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved
documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for
RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.
10 20 30 40 50
KR e t r i e v e dD o c s394041424344NQ Exact MatchRAG-Tok
RAG-Seq
10 20 30 40 50
KR e t r i e v e dD o c s4050607080NQ Answer Recall @ KRAG-Tok
RAG-Seq
Fixed DPR
BM25
10 20 30 40 50
KR e t r i e v e dD o c s4850525456Bleu-1 / Rouge-L scoreRAG-Tok R-L
RAG-Tok B-1
RAG-Seq R-L
RAG-Seq B-1
Figure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor-
mance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.
5 Related Work
Single-Task Retrieval Prior work has shown that retrieval improves performance across a variety of
NLP tasks when considered in isolation. Such tasks include open-domain question answering [ 5,29],
fact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article
generation [ 36], dialogue [ 41,65,9,13], translation [ 17], and language modeling [ 19,27]. Our
work uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single
retrieval-based architecture is capable of achieving strong performance across several tasks.
8General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP
tasks has shown great success without the use of retrieval. A single, pre-trained language model
has been shown to achieve strong performance on various classiﬁcation tasks in the GLUE bench-
marks [ 60,61] after ﬁne-tuning [ 49,8]. GPT-2 [ 50] later showed that a single, left-to-right, pre-trained
language model could achieve strong performance across both discriminative and generative tasks.
For further improvement, BART [ 32] and T5 [ 51,52] propose a single, pre-trained encoder-decoder
model that leverages bi-directional attention to achieve stronger performance on discriminative
and generative tasks. Our work aims to expand the space of possible tasks with a single, uniﬁed
architecture, by learning a retrieval module to augment pre-trained, generative language models.
Learned Retrieval There is signiﬁcant work on learning to retrieve documents in information
retrieval, more recently with pre-trained, neural language models [ 44,26] similar to ours. Some
work optimizes the retrieval module to aid in a speciﬁc, downstream task such as question answering,
using search [ 46], reinforcement learning [ 6,63,62], or a latent variable approach [ 31,20] as in our
work. These successes leverage different retrieval-based architectures and optimization techniques to
achieve strong performance on a single task, while we show that a single retrieval-based architecture
can be ﬁne-tuned for strong performance on a variety of tasks.
Memory-based Architectures Our document index can be seen as a large external memory for
neural networks to attend to, analogous to memory networks [ 64,55]. Concurrent work [ 14] learns
to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our
work. Other work improves the ability of dialog models to generate factual text by attending over
fact embeddings [ 15,13]. A key feature of our memory is that it is comprised of raw text rather
distributed representations, which makes the memory both (i) human-readable, lending a form of
interpretability to our model, and (ii) human-writable, enabling us to dynamically update the models
memory by editing the document index. This approach has also been used in knowledge-intensive
dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF
rather than end-to-end learnt retrieval [9].
Retrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style
approaches, where a similar training input-output pair is retrieved for a given input, and then edited
to provide a ﬁnal output. These approaches have proved successful in a number of domains including
Machine Translation [ 18,22] and Semantic Parsing [ 21]. Our approach does have several differences,
including less of emphasis on lightly editing a retrieved item, but on aggregating content from several
pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents
rather than related training pairs. This said, RAG techniques may work well in these settings, and
could represent promising future work.
6 Discussion
In this work, we presented hybrid generation models with access to parametric and non-parametric
memory. We showed that our RAG models obtain state of the art results on open-domain QA. We
found that people prefer RAGs generation over purely parametric BART, ﬁnding RAG more factual
and speciﬁc. We conducted an thorough investigation of the learned retrieval component, validating
its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model
without requiring any retraining. In future work, it may be fruitful to investigate if the two components
can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some
another objective. Our work opens up new research directions on how parametric and non-parametric
memories interact and how to most effectively combine them, showing promise in being applied to a
wide variety of NLP tasks.
9Broader Impact
This work offers several positive societal beneﬁts over previous work: the fact that it is more
strongly grounded in real factual knowledge (in this case Wikipedia) makes it hallucinate less
with generations that are more factual, and offers more control and interpretability. RAG could be
employed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it
with a medical index and asking it open-domain questions on that topic, or by helping people be more
effective at their jobs.
With these advantages also come potential downsides: Wikipedia, or any potential external knowledge
source, will probably never be entirely factual and completely devoid of bias. Since RAG can be
employed as a language model, similar concerns as for GPT-2 [ 50] are valid here, although arguably
to a lesser extent, including that it might be used to generate abuse, faked or misleading content in
the news or on social media; to impersonate others; or to automate the production of spam/phishing
content [ 54]. Advanced language models may also lead to the automation of various jobs in the
coming decades [ 16]. In order to mitigate these risks, AI systems could be employed to ﬁght against
misleading content and automated spam/phishing.
Acknowledgments
The authors would like to thank the reviewers for their thoughtful and constructive feedback on this
paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors
would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP
thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD
program.
References
[1]Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan
Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina
Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine
Reading COmprehension Dataset. arXiv:1611.09268 [cs] , November 2016. URL http:
//arxiv.org/abs/1611.09268 . arXiv: 1611.09268.
[2]Petr Baudiš and Jan Šediv y. Modeling of the question answering task in the yodaqa system. In
International Conference of the Cross-Language Evaluation Forum for European Languages ,
pages 222228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%
2F978-3-319-24027-5_20 .
[3]Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase
from Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods
in Natural Language Processing , pages 15331544, Seattle, Washington, USA, October 2013.
Association for Computational Linguistics. URL http://www.aclweb.org/anthology/
D13-1160 .
[4]Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-
ing&autoregressive language model for context-conditioned generation. ArXiv , abs/2004.07159,
2020. URL https://arxiv.org/abs/2004.07159 .
[5]Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer
Open-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , pages 18701879, Vancouver, Canada,
July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL
https://www.aclweb.org/anthology/P17-1171 .
[6]Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and
Jonathan Berant. Coarse-to-ﬁne question answering for long documents. In Proceedings of the
55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,
pages 209220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:
10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020 .
10[7]Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre-
hension. arXiv:1710.10723 [cs] , October 2017. URL http://arxiv.org/abs/1710.10723 .
arXiv: 1710.10723.
[8]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers) , pages 41714186, Minneapolis,
Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.
URL https://www.aclweb.org/anthology/N19-1423 .
[9]Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz-
ard of wikipedia: Knowledge-powered conversational agents. In International Conference on
Learning Representations , 2019. URL https://openreview.net/forum?id=r1l73iRqKm .
[10] Matthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Guney, V olkan Cirik, and Kyunghyun
Cho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.
arXiv:1704.05179 [cs] , April 2017. URL http://arxiv.org/abs/1704.05179 . arXiv:
1704.05179.
[11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-
ings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers) , pages 889898, Melbourne, Australia, July 2018. Association for Computational
Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/
P18-1082 .
[12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:
Long form question answering. In Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics , pages 35583567, Florence, Italy, July 2019. Association for
Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/
anthology/P19-1346 .
[13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers
with KNN-based composite memory, 2020. URL https://openreview.net/forum?id=
H1gx1CNKPH .
[14] Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski.
Entities as experts: Sparse memory access with entity supervision. ArXiv , abs/2004.07202,
2020. URL https://arxiv.org/abs/2004.07202 .
[15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen
tau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI
Conference on Artiﬁcial Intelligence , 2018. URL https://www.aaai.org/ocs/index.php/
AAAI/AAAI18/paper/view/16710 .
[16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI
exceed human performance? evidence from AI experts. CoRR , abs/1705.08807, 2017. URL
http://arxiv.org/abs/1705.08807 .
[17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural
machine translation. In AAAI Conference on Artiﬁcial Intelligence , 2018. URL https:
//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282 .
[18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural
machine translation. In 32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018 , 32nd
AAAI Conference on Artiﬁcial Intelligence, AAAI 2018, pages 51335140. AAAI press, 2018.
32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018 ; Conference date: 02-02-2018
Through 07-02-2018.
[19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by
editing prototypes. Transactions of the Association for Computational Linguistics , 6:437450,
2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031 .
11[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:
Retrieval-augmented language model pre-training. ArXiv , abs/2002.08909, 2020. URL https:
//arxiv.org/abs/2002.08909 .
[21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A
retrieve-and-edit framework for predicting structured outputs. In S. Bengio,
H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-
itors, Advances in Neural Information Processing Systems 31 , pages 10052
10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.
pdf.
[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-
edit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 25322538, Online, July 2020. Association for Computa-
tional Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/
anthology/2020.acl-main.228 .
[23] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. arXiv
preprint arXiv:1702.08734 , 2017. URL https://arxiv.org/abs/1702.08734 .
[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale
Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the
55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,
pages 16011611, Vancouver, Canada, July 2017. Association for Computational Linguistics.
doi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147 .
[25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-
augmented recurrent nets. In Proceedings of the 28th International Conference on
Neural Information Processing Systems - Volume 1 , NIPS15, page 190198, Cam-
bridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/
5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets .
[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint
arXiv:2004.04906 , 2020. URL https://arxiv.org/abs/2004.04906 .
[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza-
tion through memorization: Nearest neighbor language models. In International Conference on
Learning Representations , 2020. URL https://openreview.net/forum?id=HklBjCEKvH .
[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL
http://arxiv.org/abs/1412.6980 .
[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh,
Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken-
ton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Ques-
tion Answering Research. Transactions of the Association of Computational Lin-
guistics , 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/
natural-questions/main-1455-kwiatkowski.pdf .
[30] Guillaume Lample, Alexandre Sablayrolles, Marc Aurelio Ranzato, Ludovic Denoyer, and
Herve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural In-
formation Processing Systems 32 , pages 85488559. Curran Associates, Inc., 2019. URL http:
//papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf .
[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised
open domain question answering. In Proceedings of the 57th Annual Meeting of the Association
12for Computational Linguistics , pages 60866096, Florence, Italy, July 2019. Association for
Computational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/
anthology/P19-1612 .
[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence
pre-training for natural language generation, translation, and comprehension. arXiv preprint
arXiv:1910.13461 , 2019. URL https://arxiv.org/abs/1910.13461 .
[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting
objective function for neural conversation models. In Proceedings of the 2016 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies , pages 110119, San Diego, California, June 2016. Association for Computational
Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/
N16-1014 .
[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation
with optimized questions and multi-turn comparisons. ArXiv , abs/1909.03087, 2019. URL
https://arxiv.org/abs/1909.03087 .
[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine
translation with joint textual and phonetic embedding. In Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics , pages 30443049, Florence, Italy,
July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL
https://www.aclweb.org/anthology/P19-1291 .
[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,
and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International
Conference on Learning Representations , 2018. URL https://openreview.net/forum?
id=Hyg0vbWC- .
[37] Yury A. Malkov and D. A. Yashunin. Efﬁcient and robust approximate nearest neighbor search
using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 42:824836, 2016. URL https://arxiv.org/abs/1603.09320 .
[38] Gary Marcus. The next decade in ai: four steps towards robust artiﬁcial intelligence. arXiv
preprint arXiv:2002.06177 , 2020. URL https://arxiv.org/abs/2002.06177 .
[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis
Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the
veriﬁability of generated text. arXiv preprint arXiv:1911.03587 , 2019. URL https:
//arxiv.org/abs/1911.03587 .
[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,
Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed
precision training. In ICLR , 2018. URL https://openreview.net/forum?id=r1gs9JgRZ .
[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploit-
ing background knowledge for building conversation systems. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing , pages 23222332, Brus-
sels, Belgium, October-November 2018. Association for Computational Linguistics. doi:
10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255 .
[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation
systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing , pages 39503959, Brussels, Belgium, October-November 2018. Association for
Computational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/
anthology/D18-1429 .
[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,
and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In
Tarek Richard Besold, Antoine Bordes, Artur S. dAvila Garcez, and Greg Wayne, editors,
Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic
13approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing
Systems (NIPS 2016), Barcelona, Spain, December 9, 2016 , volume 1773 of CEUR Workshop
Proceedings . CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_
2016_paper9.pdf .
[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint
arXiv:1901.04085 , 2019. URL https://arxiv.org/abs/1901.04085 .
[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings
of the 2019 Conference of the North American Chapter of the Association for Computational
Linguistics (Demonstrations) , pages 4853, Minneapolis, Minnesota, June 2019. Association
for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.
org/anthology/N19-4009 .
[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun
Cho. Finding generalizable evidence by learning to convince q&a models. In Proceedings
of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages
24022411, Hong Kong, China, November 2019. Association for Computational Linguistics.
doi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244 .
[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,
and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 24632473, Hong
Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/
D19-1250. URL https://www.aclweb.org/anthology/D19-1250 .
[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H.
Miller, and Sebastian Riedel. How context affects language models factual predictions. In
Automated Knowledge Base Construction , 2020. URL https://openreview.net/forum?
id=025X0zPfn .
[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Im-
proving Language Understanding by Generative Pre-Training, 2018. URL
https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/
language-unsupervised/language_understanding_paper.pdf .
[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. Language models are unsupervised multitask learners, 2019. URL
https://d4mucfpksywv.cloudfront.net/better-language-models/language_
models_are_unsupervised_multitask_learners.pdf .
[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed
text-to-text transformer. arXiv e-prints , 2019. URL https://arxiv.org/abs/1910.10683 .
[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into
the parameters of a language model? arXiv e-prints , 2020. URL https://arxiv.org/abs/
2002.08910 .
[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and
beyond. Found. Trends Inf. Retr. , 3(4):333389, April 2009. ISSN 1554-0669. doi: 10.1561/
1500000019. URL https://doi.org/10.1561/1500000019 .
[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-V oss, Jeff Wu, Alec
Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models.
ArXiv , abs/1908.09203, 2019.
[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net-
works. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances
in Neural Information Processing Systems 28 , pages 24402448. Curran Associates, Inc., 2015.
URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf .
14[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a
large-scale dataset for fact extraction and VERiﬁcation. In Proceedings of the 2018 Conference
of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers) , pages 809819, New Orleans, Louisiana,
June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL
https://www.aclweb.org/anthology/N18-1074 .
[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model
biases in sentence-pair classiﬁcation with elastic weight consolidation. ArXiv , abs/2004.14366,
2020. URL https://arxiv.org/abs/2004.14366 .
[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural
Information Processing Systems 30 , pages 59986008. Curran Associates, Inc., 2017. URL
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf .
[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David
Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes.
AAAI Conference on Artiﬁcial Intelligence , 2018. URL https://www.aaai.org/ocs/index.
php/AAAI/AAAI18/paper/view/17329 .
[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding.
InProceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting
Neural Networks for NLP , pages 353355, Brussels, Belgium, November 2018. Association for
Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/
anthology/W18-5446 .
[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-
Purpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. dtextquotesingle Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information
Processing Systems 32 , pages 32613275. Curran Associates, Inc., 2019. URL https://
arxiv.org/abs/1905.00537 .
[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang,
Gerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain
question answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of
the Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative
Applications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational
Advances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,
2018 , pages 59815988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.
php/AAAI/AAAI18/paper/view/16712 .
[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang,
Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-
ranking in open-domain question answering. In ICLR , 2018. URL https://openreview.
net/forum?id=rJl3yM-Ab .
[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio
and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL
http://arxiv.org/abs/1410.3916 .
[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and reﬁne: Improved sequence
generation models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd
International Workshop on Search-Oriented Conversational AI , pages 8792, Brussels, Belgium,
October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL
https://www.aclweb.org/anthology/W18-5713 .
15[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony
Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,
Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain
Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingfaces transformers:
State-of-the-art natural language processing. ArXiv , abs/1910.03771, 2019.
[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-
supervised question answering. In Proceedings of the 2019 Conference on Empirical Meth-
ods in Natural Language Processing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP) , pages 24952509, Hong Kong, China, Novem-
ber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL
https://www.aclweb.org/anthology/D19-1253 .
[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and
Jian Yin. Reasoning over semantic-level graph for fact checking. ArXiv , abs/1909.03745, 2019.
URL https://arxiv.org/abs/1909.03745 .
16Appendices for Retrieval-Augmented Generation for
Knowledge-Intensive NLP Tasks
A Implementation Details
For Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models.
For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the
Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as
we did not ﬁnd beam search improved results. For Open-MSMarco and Jeopardy question generation,
we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence,
and we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast
Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.
B Human Evaluation
Figure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions
and a worked example appear when clicking view tool guide.
Figure 4 shows the user interface for human evaluation. To avoid any biases for screen position,
which model corresponded to sentence A and sentence B was randomly selected for each example.
Annotators were encouraged to research the topic using the internet, and were given detailed instruc-
tions and worked examples in a full instructions tab. We included some gold sentences in order to
assess the accuracy of the annotators. Two annotators did not perform well on these examples and
their annotations were removed from the results.
C Training setup Details
We train all RAG models and BART baselines using Fairseq [ 45].2We train with mixed precision
ﬂoating point arithmetic [ 40], distributing training across 8, 32GB NVIDIA V100 GPUs, though
training and inference can be run on one GPU. We ﬁnd that doing Maximum Inner Product Search
with FAISS is sufﬁciently fast on CPU, so we store document index vectors on CPU, requiring 100
GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace
Transformers [ 66]3, which achieves equivalent performance to the previous version but is a cleaner
and easier to use implementation. This version is also open-sourced. We also compress the document
index using FAISSs compression tools, reducing the CPU memory requirement to 36GB. Scripts to
run experiments with RAG can be found at https://github.com/huggingface/transformers/
blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found
athttps://huggingface.co/rag/
2https://github.com/pytorch/fairseq
3https://github.com/huggingface/transformers
17D Further Details on Open-Domain QA
For open-domain QA, multiple answer annotations are often available for a given question. These
answer annotations are exploited by extractive models during training as typically all the answer
annotations are used to ﬁnd matches within documents when preparing training data. For RAG, we
also make use of multiple annotation examples for Natural Questions and WebQuestions by training
the model with each (q;a)pair separately, leading to a small increase in accuracy. For TriviaQA,
there are often many valid answers to a given question, some of which are not suitable training targets,
such as emoji or spelling variants. For TriviaQA, we ﬁlter out answer candidates if they do not occur
in top 1000 documents for the query.
CuratedTrec preprocessing The answers for CuratedTrec are given in the form of regular expres-
sions, which has been suggested as a reason why it is unsuitable for answer-generation models [20].
To overcome this, we use a pre-processing step where we ﬁrst retrieve the top 1000 documents for
each query, and use the answer that most frequently matches the regex pattern as the supervision
target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for
each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.
TriviaQA Evaluation setups The open-domain QA community customarily uses public develop-
ment datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading
compehension purposes. We report our results using the datasets splits used in DPR [ 26], which are
consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public
TriviaQA Web Development split. Roberts et al. [52] used the TriviaQA ofﬁcial Wikipedia test set
instead. Févry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See
appendix of [ 14]). We report results on both test sets to enable fair comparison to both approaches.
We ﬁnd that our performance is much higher using the ofﬁcial Wiki test set, rather than the more
conventional open-domain test set, which we attribute to the ofﬁcial Wiki test set questions being
simpler to answer from Wikipedia.
E Further Details on FEVER
For FEVER classiﬁcation, we follow the practice from [ 32], and ﬁrst re-generate the claim, and
then classify using the representation of the ﬁnal hidden state, before ﬁnally marginalizing across
documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The
ﬁrst is to classify the claim as either Supported, Refuted or Not Enough Info, which is the task
we explore in the main paper. FEVERs other sub-task involves extracting sentences from Wikipedia
as evidence supporting the classiﬁcation prediction. As FEVER uses a different Wikipedia dump to
us, directly tackling this task is not straightforward. We hope to address this in future work.
F Null Document Probabilities
We experimented with adding Null document mechanism to RAG, similar to REALM [ 20] in order
to model cases where no useful information could be retrieved for a given input. Here, if kdocuments
were retrieved, we would additionally retrieve an empty document and predict a logit for the null
document, before marginalizing over k+ 1predictions. We explored modelling this null document
logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or
(iii) a neural network to predict the logit. We did not ﬁnd that these improved performance, so in
the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents
cannot always be retrieved, we observe that the model learns to always retrieve a particular set of
documents for questions that are less likely to beneﬁt from retrieval, suggesting that null document
mechanisms may not be necessary for RAG.
G Parameters
Our RAG models contain the trainable parameters for the BERT-base query and document encoder of
DPR, with 110M parameters each (although we do not train the document encoder ourselves) and
406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable
18Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation
Task Train Development Test
Natural Questions 79169 8758 3611
TriviaQA 78786 8838 11314
WebQuestions 3418 362 2033
CuratedTrec 635 134 635
Jeopardy Question Generation 97392 13714 26849
MS-MARCO 153726 12468 101093*
FEVER-3-way 145450 10000 10000
FEVER-2-way 96966 6666 6666
parameters. The best performing closed-book (parametric only) open-domain QA model is T5-11B
with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our
models is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [ 52],
substantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-
parametric models require far fewer trainable parameters for strong open-domain QA performance.
The non-parametric memory index does not consist of trainable parameters, but does consists of 21M
728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit ﬂoating
point precision to manage memory and disk footprints.
H Retrieval Collapse
In preliminary experiments, we observed that for some tasks such as story generation [ 11], the
retrieval component would collapse and learn to retrieve the same documents regardless of the
input. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,
and the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit
requirement for factual knowledge in some tasks, or the longer target sequences, which could result
in less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results
when optimizing a retrieval component in order to improve performance on downstream tasks.
I Number of instances per dataset
The number of training, development and test datapoints in each of our datasets is shown in Table 7.
19
  Provided proper attribution is provided, Google hereby grants permission to
reproduce the tables and figures in this paper solely for use in journalistic or
scholarly works.
Attention Is All You Need
Ashish Vaswani
Google Brain
avaswani@google.comNoam Shazeer
Google Brain
noam@google.comNiki Parmar
Google Research
nikip@google.comJakob Uszkoreit
Google Research
usz@google.com
Llion Jones
Google Research
llion@google.comAidan N. Gomez 
University of Toronto
aidan@cs.toronto.eduŁukasz Kaiser
Google Brain
lukaszkaiser@google.com
Illia Polosukhin 
illia.polosukhin@gmail.com
Abstract
The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-
to-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature. We show that the Transformer generalizes well to
other tasks by applying it successfully to English constituency parsing both with
large and limited training data.
Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started
the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and
has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head
attention and the parameter-free position representation and became the other person involved in nearly every
detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and
tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and
efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and
implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating
our research.
Work performed while at Google Brain.
Work performed while at Google Research.
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 20231 Introduction
Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks
in particular, have been firmly established as state of the art approaches in sequence modeling and
transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous
efforts have since continued to push the boundaries of recurrent language models and encoder-decoder
architectures [38, 24, 15].
Recurrent models typically factor computation along the symbol positions of the input and output
sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden
states ht, as a function of the previous hidden state ht1and the input for position t. This inherently
sequential nature precludes parallelization within training examples, which becomes critical at longer
sequence lengths, as memory constraints limit batching across examples. Recent work has achieved
significant improvements in computational efficiency through factorization tricks [ 21] and conditional
computation [ 32], while also improving model performance in case of the latter. The fundamental
constraint of sequential computation, however, remains.
Attention mechanisms have become an integral part of compelling sequence modeling and transduc-
tion models in various tasks, allowing modeling of dependencies without regard to their distance in
the input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms
are used in conjunction with a recurrent network.
In this work we propose the Transformer, a model architecture eschewing recurrence and instead
relying entirely on an attention mechanism to draw global dependencies between input and output.
The Transformer allows for significantly more parallelization and can reach a new state of the art in
translation quality after being trained for as little as twelve hours on eight P100 GPUs.
2 Background
The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU
[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building
block, computing hidden representations in parallel for all input and output positions. In these models,
the number of operations required to relate signals from two arbitrary input or output positions grows
in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes
it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is
reduced to a constant number of operations, albeit at the cost of reduced effective resolution due
to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as
described in section 3.2.
Self-attention, sometimes called intra-attention is an attention mechanism relating different positions
of a single sequence in order to compute a representation of the sequence. Self-attention has been
used successfully in a variety of tasks including reading comprehension, abstractive summarization,
textual entailment and learning task-independent sentence representations [4, 27, 28, 22].
End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-
aligned recurrence and have been shown to perform well on simple-language question answering and
language modeling tasks [34].
To the best of our knowledge, however, the Transformer is the first transduction model relying
entirely on self-attention to compute representations of its input and output without using sequence-
aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate
self-attention and discuss its advantages over models such as [17, 18] and [9].
3 Model Architecture
Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].
Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence
of continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output
sequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive
[10], consuming the previously generated symbols as additional input when generating the next.
2Figure 1: The Transformer - model architecture.
The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.
3.1 Encoder and Decoder Stacks
Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two
sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-
wise fully connected feed-forward network. We employ a residual connection [ 11] around each of
the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is
LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer
itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding
layers, produce outputs of dimension dmodel = 512 .
Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two
sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head
attention over the output of the encoder stack. Similar to the encoder, we employ residual connections
around each of the sub-layers, followed by layer normalization. We also modify the self-attention
sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This
masking, combined with fact that the output embeddings are offset by one position, ensures that the
predictions for position ican depend only on the known outputs at positions less than i.
3.2 Attention
An attention function can be described as mapping a query and a set of key-value pairs to an output,
where the query, keys, values, and output are all vectors. The output is computed as a weighted sum
3Scaled Dot-Product Attention
 Multi-Head Attention
Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several
attention layers running in parallel.
of the values, where the weight assigned to each value is computed by a compatibility function of the
query with the corresponding key.
3.2.1 Scaled Dot-Product Attention
We call our particular attention Scaled Dot-Product Attention (Figure 2). The input consists of
queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the
query with all keys, divide each bydk, and apply a softmax function to obtain the weights on the
values.
In practice, we compute the attention function on a set of queries simultaneously, packed together
into a matrix Q. The keys and values are also packed together into matrices KandV. We compute
the matrix of outputs as:
Attention( Q, K, V ) = softmax(QKT
dk)V (1)
The two most commonly used attention functions are additive attention [ 2], and dot-product (multi-
plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor
of1dk. Additive attention computes the compatibility function using a feed-forward network with
a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is
much faster and more space-efficient in practice, since it can be implemented using highly optimized
matrix multiplication code.
While for small values of dkthe two mechanisms perform similarly, additive attention outperforms
dot product attention without scaling for larger values of dk[3]. We suspect that for large values of
dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has
extremely small gradients4. To counteract this effect, we scale the dot products by1dk.
3.2.2 Multi-Head Attention
Instead of performing a single attention function with dmodel-dimensional keys, values and queries,
we found it beneficial to linearly project the queries, keys and values htimes with different, learned
linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of
queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional
4To illustrate why the dot products get large, assume that the components of qandkare independent random
variables with mean 0and variance 1. Then their dot product, qk=Pdk
i=1qiki, has mean 0and variance dk.
4output values. These are concatenated and once again projected, resulting in the final values, as
depicted in Figure 2.
Multi-head attention allows the model to jointly attend to information from different representation
subspaces at different positions. With a single attention head, averaging inhibits this.
MultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO
where head i= Attention( QWQ
i, KWK
i, V WV
i)
Where the projections are parameter matrices WQ
iRdmodeldk,WK
iRdmodeldk,WV
iRdmodeldv
andWORhdvdmodel.
In this work we employ h= 8 parallel attention layers, or heads. For each of these we use
dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost
is similar to that of single-head attention with full dimensionality.
3.2.3 Applications of Attention in our Model
The Transformer uses multi-head attention in three different ways:
In encoder-decoder attention layers, the queries come from the previous decoder layer,
and the memory keys and values come from the output of the encoder. This allows every
position in the decoder to attend over all positions in the input sequence. This mimics the
typical encoder-decoder attention mechanisms in sequence-to-sequence models such as
[38, 2, 9].
The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.
Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property. We implement this
inside of scaled dot-product attention by masking out (setting to ) all values in the input
of the softmax which correspond to illegal connections. See Figure 2.
3.3 Position-wise Feed-Forward Networks
In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully
connected feed-forward network, which is applied to each position separately and identically. This
consists of two linear transformations with a ReLU activation in between.
FFN( x) = max(0 , xW 1+b1)W2+b2 (2)
While the linear transformations are the same across different positions, they use different parameters
from layer to layer. Another way of describing this is as two convolutions with kernel size 1.
The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality
dff= 2048 .
3.4 Embeddings and Softmax
Similarly to other sequence transduction models, we use learned embeddings to convert the input
tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-
mation and softmax function to convert the decoder output to predicted next-token probabilities. In
our model, we share the same weight matrix between the two embedding layers and the pre-softmax
linear transformation, similar to [ 30]. In the embedding layers, we multiply those weights bydmodel.
5Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations
for different layer types. nis the sequence length, dis the representation dimension, kis the kernel
size of convolutions and rthe size of the neighborhood in restricted self-attention.
Layer Type Complexity per Layer Sequential Maximum Path Length
Operations
Self-Attention O(n2d) O(1) O(1)
Recurrent O(nd2) O(n) O(n)
Convolutional O(knd2) O(1) O(logk(n))
Self-Attention (restricted) O(rnd) O(1) O(n/r)
3.5 Positional Encoding
Since our model contains no recurrence and no convolution, in order for the model to make use of the
order of the sequence, we must inject some information about the relative or absolute position of the
tokens in the sequence. To this end, we add positional encodings to the input embeddings at the
bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel
as the embeddings, so that the two can be summed. There are many choices of positional encodings,
learned and fixed [9].
In this work, we use sine and cosine functions of different frequencies:
PE(pos,2i)=sin(pos/100002i/d model)
PE(pos,2i+1)=cos(pos/100002i/d model)
where posis the position and iis the dimension. That is, each dimension of the positional encoding
corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 2π. We
chose this function because we hypothesized it would allow the model to easily learn to attend by
relative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of
PEpos.
We also experimented with using learned positional embeddings [ 9] instead, and found that the two
versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version
because it may allow the model to extrapolate to sequence lengths longer than the ones encountered
during training.
4 Why Self-Attention
In this section we compare various aspects of self-attention layers to the recurrent and convolu-
tional layers commonly used for mapping one variable-length sequence of symbol representations
(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, ziRd, such as a hidden
layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we
consider three desiderata.
One is the total computational complexity per layer. Another is the amount of computation that can
be parallelized, as measured by the minimum number of sequential operations required.
The third is the path length between long-range dependencies in the network. Learning long-range
dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the
ability to learn such dependencies is the length of the paths forward and backward signals have to
traverse in the network. The shorter these paths between any combination of positions in the input
and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare
the maximum path length between any two input and output positions in networks composed of the
different layer types.
As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially
executed operations, whereas a recurrent layer requires O(n)sequential operations. In terms of
computational complexity, self-attention layers are faster than recurrent layers when the sequence
6length nis smaller than the representation dimensionality d, which is most often the case with
sentence representations used by state-of-the-art models in machine translations, such as word-piece
[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving
very long sequences, self-attention could be restricted to considering only a neighborhood of size rin
the input sequence centered around the respective output position. This would increase the maximum
path length to O(n/r). We plan to investigate this approach further in future work.
A single convolutional layer with kernel width k < n does not connect all pairs of input and output
positions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,
orO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths
between any two positions in the network. Convolutional layers are generally more expensive than
recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity
considerably, to O(knd+nd2). Even with k=n, however, the complexity of a separable
convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,
the approach we take in our model.
As side benefit, self-attention could yield more interpretable models. We inspect attention distributions
from our models and present and discuss examples in the appendix. Not only do individual attention
heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic
and semantic structure of the sentences.
5 Training
This section describes the training regime for our models.
5.1 Training Data and Batching
We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million
sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-
target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT
2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece
vocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training
batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000
target tokens.
5.2 Hardware and Schedule
We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using
the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We
trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the
bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps
(3.5 days).
5.3 Optimizer
We used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 109. We varied the learning
rate over the course of training, according to the formula:
lrate =d0.5
modelmin(step_num0.5, step _numwarmup _steps1.5) (3)
This corresponds to increasing the learning rate linearly for the first warmup _steps training steps,
and decreasing it thereafter proportionally to the inverse square root of the step number. We used
warmup _steps = 4000 .
5.4 Regularization
We employ three types of regularization during training:
7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the
English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
ModelBLEU Training Cost (FLOPs)
EN-DE EN-FR EN-DE EN-FR
ByteNet [18] 23.75
Deep-Att + PosUnk [39] 39.2 1.01020
GNMT + RL [38] 24.6 39.92 2.310191.41020
ConvS2S [9] 25.16 40.46 9.610181.51020
MoE [32] 26.03 40.56 2.010191.21020
Deep-Att + PosUnk Ensemble [39] 40.4 8.01020
GNMT + RL Ensemble [38] 26.30 41.16 1.810201.11021
ConvS2S Ensemble [9] 26.36 41.29 7.710191.21021
Transformer (base model) 27.3 38.1 3.31018
Transformer (big) 28.4 41.8 2.31019
Residual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the
sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the
positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of
Pdrop= 0.1.
Label Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This
hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.
6 Results
6.1 Machine Translation
On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)
in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0
BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is
listed in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model
surpasses all previously published models and ensembles, at a fraction of the training cost of any of
the competitive models.
On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,
outperforming all of the previously published single models, at less than 1/4the training cost of the
previous state-of-the-art model. The Transformer (big) model trained for English-to-French used
dropout rate Pdrop= 0.1, instead of 0.3.
For the base models, we used a single model obtained by averaging the last 5 checkpoints, which
were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We
used beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters
were chosen after experimentation on the development set. We set the maximum output length during
inference to input length + 50, but terminate early when possible [38].
Table 2 summarizes our results and compares our translation quality and training costs to other model
architectures from the literature. We estimate the number of floating point operations used to train a
model by multiplying the training time, the number of GPUs used, and an estimate of the sustained
single-precision floating-point capacity of each GPU5.
6.2 Model Variations
To evaluate the importance of different components of the Transformer, we varied our base model
in different ways, measuring the change in performance on English-to-German translation on the
5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.
8Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base
model. All metrics are on the English-to-German translation development set, newstest2013. Listed
perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to
per-word perplexities.
N d model dff h d k dvPdrop ϵlstrain PPL BLEU params
steps (dev) (dev) 106
base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65
(A)1 512 512 5.29 24.9
4 128 128 5.00 25.5
16 32 32 4.91 25.8
32 16 16 5.01 25.4
(B)16 5.16 25.1 58
32 5.01 25.4 60
(C)2 6.11 23.7 36
4 5.19 25.3 50
8 4.88 25.5 80
256 32 32 5.75 24.5 28
1024 128 128 4.66 26.0 168
1024 5.12 25.4 53
4096 4.75 26.2 90
(D)0.0 5.77 24.6
0.2 4.95 25.5
0.0 4.67 25.3
0.2 5.47 25.7
(E) positional embedding instead of sinusoids 4.92 25.7
big 6 1024 4096 16 0.3 300K 4.33 26.4 213
development set, newstest2013. We used beam search as described in the previous section, but no
checkpoint averaging. We present these results in Table 3.
In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,
keeping the amount of computation constant, as described in Section 3.2.2. While single-head
attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.
In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This
suggests that determining compatibility is not easy and that a more sophisticated compatibility
function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,
bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our
sinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical
results to the base model.
6.3 English Constituency Parsing
To evaluate if the Transformer can generalize to other tasks we performed experiments on English
constituency parsing. This task presents specific challenges: the output is subject to strong structural
constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence
models have not been able to attain state-of-the-art results in small-data regimes [37].
We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the
Penn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,
using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences
[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens
for the semi-supervised setting.
We performed only a small number of experiments to select the dropout, both attention and residual
(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters
remained unchanged from the English-to-German base translation model. During inference, we
9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23
of WSJ)
Parser Training WSJ 23 F1
Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3
Petrov et al. (2006) [29] WSJ only, discriminative 90.4
Zhu et al. (2013) [40] WSJ only, discriminative 90.4
Dyer et al. (2016) [8] WSJ only, discriminative 91.7
Transformer (4 layers) WSJ only, discriminative 91.3
Zhu et al. (2013) [40] semi-supervised 91.3
Huang & Harper (2009) [14] semi-supervised 91.3
McClosky et al. (2006) [26] semi-supervised 92.1
Vinyals & Kaiser el al. (2014) [37] semi-supervised 92.1
Transformer (4 layers) semi-supervised 92.7
Luong et al. (2015) [23] multi-task 93.0
Dyer et al. (2016) [8] generative 93.3
increased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3
for both WSJ only and the semi-supervised setting.
Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-
prisingly well, yielding better results than all previously reported models with the exception of the
Recurrent Neural Network Grammar [8].
In contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-
Parser [29] even when training only on the WSJ training set of 40K sentences.
7 Conclusion
In this work, we presented the Transformer, the first sequence transduction model based entirely on
attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with
multi-headed self-attention.
For translation tasks, the Transformer can be trained significantly faster than architectures based
on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014
English-to-French translation tasks, we achieve a new state of the art. In the former task our best
model outperforms even all previously reported ensembles.
We are excited about the future of attention-based models and plan to apply them to other tasks. We
plan to extend the Transformer to problems involving input and output modalities other than text and
to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs
such as images, audio and video. Making generation less sequential is another research goals of ours.
The code we used to train and evaluate our models is available at https://github.com/
tensorflow/tensor2tensor .
Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful
comments, corrections and inspiration.
References
[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450 , 2016.
[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. CoRR , abs/1409.0473, 2014.
[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural
machine translation architectures. CoRR , abs/1703.03906, 2017.
[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine
reading. arXiv preprint arXiv:1601.06733 , 2016.
10[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,
and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical
machine translation. CoRR , abs/1406.1078, 2014.
[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv
preprint arXiv:1610.02357 , 2016.
[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation
of gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.
[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural
network grammars. In Proc. of NAACL , 2016.
[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-
tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.
[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint
arXiv:1308.0850 , 2013.
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-
age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 770778, 2016.
[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in
recurrent nets: the difficulty of learning long-term dependencies, 2001.
[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,
9(8):17351780, 1997.
[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations
across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural
Language Processing , pages 832841. ACL, August 2009.
[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring
the limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.
[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural
Information Processing Systems, (NIPS) , 2016.
[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference
on Learning Representations (ICLR) , 2016.
[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-
ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,
2017.
[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.
InInternational Conference on Learning Representations , 2017.
[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.
[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint
arXiv:1703.10722 , 2017.
[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen
Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint
arXiv:1703.03130 , 2017.
[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task
sequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.
[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-
based neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.
11[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of english: The penn treebank. Computational linguistics , 19(2):313330, 1993.
[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In
Proceedings of the Human Language Technology Conference of the NAACL, Main Conference ,
pages 152159. ACL, June 2006.
[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention
model. In Empirical Methods in Natural Language Processing , 2016.
[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive
summarization. arXiv preprint arXiv:1705.04304 , 2017.
[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,
and interpretable tree annotation. In Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting of the ACL , pages 433440. ACL, July
2006.
[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv
preprint arXiv:1608.05859 , 2016.
[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words
with subword units. arXiv preprint arXiv:1508.07909 , 2015.
[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts
layer. arXiv preprint arXiv:1701.06538 , 2017.
[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine
Learning Research , 15(1):19291958, 2014.
[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory
networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,
Advances in Neural Information Processing Systems 28 , pages 24402448. Curran Associates,
Inc., 2015.
[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural
networks. In Advances in Neural Information Processing Systems , pages 31043112, 2014.
[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.
Rethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.
[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In
Advances in Neural Information Processing Systems , 2015.
[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Googles neural machine
translation system: Bridging the gap between human and machine translation. arXiv preprint
arXiv:1609.08144 , 2016.
[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with
fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.
[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate
shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume
1: Long Papers) , pages 434443. ACL, August 2013.
12Attention Visualizations
Input-Input Layer5
It
is
in
this
spirit
that
a
majority
of
American
governments
have
passed
new
laws
since
2009
making
the
registration
or
voting
process
more
difficult
.
<EOS>
<pad>
<pad>
<pad>
<pad>
<pad>
<pad>
It
is
in
this
spirit
that
a
majority
of
American
governments
have
passed
new
laws
since
2009
making
the
registration
or
voting
process
more
difficult
.
<EOS>
<pad>
<pad>
<pad>
<pad>
<pad>
<pad>
Figure 3: An example of the attention mechanism following long-distance dependencies in the
encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of
the verb making, completing the phrase making...more difficult. Attentions here shown only for
the word making. Different colors represent different heads. Best viewed in color.
13Input-Input Layer5
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
Input-Input Layer5
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:
Full attentions for head 5. Bottom: Isolated attentions from just the word its for attention heads 5
and 6. Note that the attentions are very sharp for this word.
14Input-Input Layer5
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
Input-Input Layer5
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the
sentence. We give two such examples above, from two different heads from the encoder self-attention
at layer 5 of 6. The heads clearly learned to perform different tasks.
15
  Sigmoid Loss for Language Image Pre-Training
Xiaohua Zhai?Basil Mustafa Alexander Kolesnikov Lucas Beyer?
Google DeepMind, Z urich, Switzerland
fxzhai, basilm, akolesnikov, lbeyer g@google.com
Abstract
We propose a simple pairwise Sigmoid loss for
Language-Image Pre-training (SigLIP). Unlike standard
contrastive learning with softmax normalization, the sig-
moid loss operates solely on image-text pairs and does not
require a global view of the pairwise similarities for nor-
malization. The sigmoid loss simultaneously allows fur-
ther scaling up the batch size, while also performing bet-
ter at smaller batch sizes. Combined with Locked-image
Tuning, with only four TPUv4 chips, we train a SigLiT
model that achieves 84.5% ImageNet zero-shot accuracy
in two days. The disentanglement of the batch size from
the loss further allows us to study the impact of exam-
ples vs pairs and negative to positive ratio. Finally, we
push the batch size to the extreme, up to one million, and
ﬁnd that the beneﬁts of growing batch size quickly dimin-
ish, with a more reasonable batch size of 32 k being suf-
ﬁcient. We release our models at https://github.
com/google-research/big_vision and hope our
research motivates further explorations in improving the
quality and efﬁciency of language-image pre-training.
1. Introduction
Contrastive pre-training using weak supervision from
image-text pairs found on the web is becoming the go-to
method for obtaining generic computer vision backbones,
slowly replacing pre-training on large labelled multi-class
datasets. The high-level idea is to simultaneously learn
an aligned representation space for images and texts using
paired data. Seminal works CLIP [36] and ALIGN [23] es-
tablished the viability of this approach at a large scale, and
following their success, many large image-text datasets be-
came available privately [59, 13, 21, 49] and publicly [40,
6, 15, 7, 41].
The standard recipe to pre-train such models leverages
the image-text contrastive objective. It aligns the image and
?equal contributionTable 1: SigLiT and SigLIP results . Sigmoid loss is mem-
ory efﬁcient, allows larger batch sizes (BS) that unlocks
language image pre-training with a small number of chips.
SigLiT model with a frozen public
 B/8 checkpoint [42],
trained on the LiT image-text dataset [59] using four TPU-
v4 chips for one day, achieves 79.7% 0-shot accuracy on
ImageNet. The same setup with a g/14 checkpoint [58]
leads to 84.5% accuracy, trained for two days. With a pub-
lic unlocked
 B/16 image checkpoint [42], trained on the
WebLI dataset [13], SigLIP achieves 71.0% 0-shot accu-
racy using 16 TPU-v4 chips for three days. The last two
rows show results with randomly initialized models.
Image Text BS #TPUv4 Days INet-0
SigLiT
 B/8 L32 k 4 1 79.8
SigLiT
 g/14 L 20 k 4 2 84.5
SigLIP
 B/16 B 16 k 16 3 71.0
SigLIP B/16 B 32 k 32 2 72.1
SigLIP B/16 B 32 k 32 5 73.4
We use a variant of the L model with 12 layers.
text embeddings for matching (positive) image-text pairs
while making sure that unrelated (negative) image-text pairs
are dissimilar in the embedding space. This is achieved via a
batch-level softmax-based contrastive loss, applied twice to
normalize the pairwise similarity scores across all images,
then all texts. A naive implementation of the softmax is
numerically unstable; it is usually stabilized by subtracting
the maximum input value before applying the softmax [18],
which requires another pass over the full batch.
In this paper, we propose a simpler alternative: the sig-
moid loss. It does not require any operation across the full
batch and hence greatly simpliﬁes the distributed loss im-
plementation and boosts efﬁciency. Additionally, it con-
ceptually decouples the batch size from the deﬁnition of
the task. We compare the proposed sigmoid loss with the
standard softmax loss across multiple setups. In partic-
ular, we investigate sigmoid-based loss with two promi-
1nent approaches for image-text learning: CLIP [36] and
LiT [59], which we call sigmoid language image pre-
training ( SigLIP ) and sigmoid LiT ( SigLiT ), respectively.
We ﬁnd that the sigmoid loss performs signiﬁcantly better
than the softmax loss when the batch size is smaller than
16 k. As the train batch size grows, the gap closes. Impor-
tantly, the sigmoid loss is symmetric, requires just a single
pass, and a typical implementation requires less memory
than the softmax loss. This enables successful training of a
SigLiT model at a batch size of one million . However, we
ﬁnd that the performance saturates with growing batch size,
both for softmax and sigmoid. The good news is that a rea-
sonable batch size, i.e. 32 k, is sufﬁcient for image-text pre-
training. This conclusion also holds for multilingual SigLIP
training on over 100 languages.
In Table 1, we present setups for image-text pre-training
that require a moderate amount of TPUv4 chips for training.
SigLiT is surprisingly efﬁcient, reaching 79.7% zero-shot
accuracy on ImageNet in just a single day on four chips.
SigLIPs more demanding from-scratch training reaches
73.4% zero-shot accuracy in 5 days with 32 TPUv4 chips.
This compares favorably to prior works such as FLIP [30]
and CLIP [36], which require approximately 5 and 10 days
respectively on 256 TPUv3 cores. When ﬁne-tuning a pre-
trained vision backbone in SigLIP, denoted as
 in Table 1,
we found that disabling the weight decay on the pre-trained
backbone leads to better results (see Figure 4 for details).
We hope our work paves the way for making the nascent
language-image pre-training ﬁeld more accessible.
2. Related Work
Contrastive learning with the sigmoid loss. One prior
work proposes a similar sigmoid loss for the task of unsu-
pervised dimensionality reduction [19]; in the scope of con-
trastive image-text learning, the vast majority of works rely
on the softmax-based InfoNCE loss as popularized by [46].
In supervised classiﬁcation, the sigmoid loss has already
been shown to be slightly more effective and robust than
the softmax loss [3, 51].
Contrastive language-image pre-training has become
popular since CLIP [36] and ALIGN [23] applied softmax
contrastive learning [60, 46, 10, 24] to large-scale image-
text datasets. Both models perform very well on zero-shot
transfer tasks, including classiﬁcation and retrieval. Follow-
up works show that contrastively pre-trained models pro-
duce good representations for ﬁne-tuning [53, 16], linear
regression [23], object detection [31], semantic segmenta-
tion [33] and video tasks [57].
Generative language-image pre-training Besides soft-
max contrastive pre-training, various alternatives have been
proposed. GIT [49], SimVLM [50], and LEMON [21] suc-
cessfully pre-train models using a generative text decoderAlgorithm 1 Sigmoid loss pseudo-implementation.
1# img_emb : image model embedding [n, dim]
2# txt_emb : text model embedding [n, dim]
3# t_prime, b : learnable temperature and bias
4# n : mini-batch size
5
6t = exp(t_prime)
7zimg = l2_normalize(img_emb)
8ztxt = l2_normalize(txt_emb)
9logits = dot(zimg, ztxt.T) *t + b
10labels = 2 *eye(n) - ones(n) # -1 with diagonal 1
11l = -sum(log_sigmoid(labels *logits)) / n
instead, while CoCa [56] adds such a decoder to the dis-
criminative CLIP/ALIGN setup, thus combining the pros
and cons of both approaches into a single very capable
model. BLIP [28] further proposes CapFilt which uses the
generative decoder to create better captions and the discrim-
inative part of the model to ﬁlter pairs. Language-Image
pre-training is a very active ﬁeld and surveys [8] rapidly be-
come outdated.
Efﬁcient language-image pre-training On the other hand,
few works have tried making language image pre-training
more efﬁcient. LiT [59] and FLIP [30] are notable attempts,
the former requires a pre-trained and locked backbone, and
the latter sacriﬁces quality by randomly dropping visual to-
kens. BASIC [35] and LAION [52] look at scaling batch-
size but only go up to 16 k and 160 k respectively, by using
many hundreds of chips, and for the former also mixing in a
large private classiﬁcation dataset [35, 55]. The recent Lion
optimizer [12] claims to be able to reduce the training cost
to reach similar quality.
3. Method
In this section, we ﬁrst review the widely-used softmax-
based contrastive loss. We then introduce the pairwise sig-
moid loss and discuss its efﬁcient implementation.
Given a mini-batch B=f(I1;T1);(I2;T2);:::gof
image-text pairs, the contrastive learning objective encour-
ages embeddings of matching pairs (Ii;Ti)to align with
each other, while pushing embeddings of unmatched pairs
(Ii;Tj6=i)apart. For practical purposes, it is assumed that
for all images i, the text associated with a different image j
is not related to i, and vice-versa. This assumption is usu-
ally noisy and imperfect.
3.1. Softmax loss for language image pre-training
When using the softmax loss to formalize this objective,
an image model f()and a text model g()are trained to
2Device 1 Device 2 Device 3
I₁I₂I₃I₄I₅I₆I₇I₈I₉I₁₀I₁₁I₁₂Device 1T₁
T₂
T₃
T₄Device 2T₅
T₆
T₇
T₈Device 3T₉
T₁₀
T₁₁
T₁₂(a) Initially each device holds 4
image and 4 text representations.
Each device needs to see the rep-
resentations from other devices
to calculate the full loss.
Device 1 Device 2 Device 3
I₁I₂I₃I₄I₅I₆I₇I₈I₉I₁₀I₁₁I₁₂Device 1T₁ + 
T₂ + 
T₃ + 
T₄ +Device 2T₅ + 
T₆ + 
T₇ + 
T₈ +Device 3T₉ + 
T₁₀ + 
T₁₁ + 
T₁₂ +

loss33% 33% 33% 33% 33% 33% 33% 33% 33% 33% 33% 33%
Device 1 Device 2 Device 3(b) They each compute the com-
ponent of the loss (highlighted)
for their representations, which
includes the positives.
Device 1 Device 2 Device 3
I₁I₂I₃I₄I₅I₆I₇I₈I₉I₁₀I₁₁I₁₂Device 3T₁     
T₂     
T₃     
T₄     Device 1T₅     
T₆     
T₇     
T₈     Device 2T₉     
T₁₀     
T₁₁     
T₁₂     

loss66% 66% 66% 66% 66% 66% 66% 66% 66% 66% 66% 66%
Device 1 Device 2 Device 3(c) Texts are swapped across the
devices, so device 1 now has I1:4
andT5:8etc. The new loss is
computed and accumulated with
the previous.
Device 1 Device 2 Device 3
I₁I₂I₃I₄I₅I₆I₇I₈I₉I₁₀I₁₁I₁₂Device 2T₁         
T₂         
T₃         
T₄         Device 3T₅         
T₆         
T₇         
T₈         Device 1T₉         
T₁₀         
T₁₁         
T₁₂         

loss           
Device 1 Device 2 Device 3
  
Cross Device Σ(d) This repeats till every image
& text pair have interacted, e.g.
device 1 has the loss of I1:4and
T1:12. A ﬁnal cross-device sum
brings everything together.
Figure 1: Efﬁcient loss implementation demonstrated via a mock setup with 3 devices and a global batch size of 12. There
are no all-gathers, and at any point in time only the bright yellow square (size 44) is materialized in memory.
minimize the following objective:
1
2jBjjBjX
i=10
BBB@image!text softmaxz}| {
logetxiyi
PjBj
j=1etxiyj+text!image softmaxz}| {
logetxiyi
PjBj
j=1etxjyi1
CCCA
where xi=f(Ii)
kf(Ii)k2andyi=g(Ti)
kg(Ti)k2. In this paper, we
adopt the vision transformer architecture [17] for images
and the transformer architecture [47] for texts. Note that
due to the asymmetry of the softmax loss, the normalization
is independently performed two times: across images and
across texts [36]. The scalar tis parametrized as exp(t0),
wheret0is a global freely learnable parameter.
3.2. Sigmoid loss for language image pre-training
Instead of the softmax-based contrastive loss, we pro-
pose a simpler alternative that does not require computing
global normalization factors. The sigmoid-based loss pro-
cesses every image-text pair independently, effectively turn-
ing the learning problem into the standard binary classiﬁca-
tion on the dataset of all pair combinations, with a positive
labels for the matching pairs (Ii;Ti)and negative labels for
all other pairs (Ii;Tj6=i). It is deﬁned as follows:
1
jBjjBjX
i=1jBjX
j=1log1
1 +ezij(txiyj+b)
| {z }
Lij
wherezijis the label for a given image and text input, which
equals 1 if they are paired and 1otherwise. At initial-ization, the heavy imbalance coming from the many nega-
tives dominates the loss, leading to large initial optimization
steps attempting to correct this bias. To alleviate this, we
introduce an additional learnable bias term bsimilar to the
temperature t. We initialize t0andbtolog 10 and10re-
spectively. This makes sure the training starts roughly close
to the prior and does not require massive over-correction.
Algorithm 1 presents a pseudocode implementation of the
proposed sigmoid loss for language image pre-training.
3.3. Efﬁcient chunked implementation
Contrastive training typically utilizes data parallelism.
Computing the loss when data is split across Ddevices
necessitates gathering all embeddings [59] with expensive
all-gathers and, more importantly, the materialization of a
memory-intensivejBjjBj matrix of pairwise similarities.
The sigmoid loss, however, is particularly amenable to
a memory efﬁcient, fast, and numerically stable implemen-
tation that ameliorates both these issues. Denoting the per-
device batch size as b=jBj
D, the loss is reformulated as:
1
jBjDX
di=1|{z}
A:8device diB:swap negs
across devicesz}|{
DX
dj=1C:per device
lossz }| {
b(di+1)X
i=bdi|{z}
all local
positivesb(dj+1)X
j=bdj|{z}
negs from
next deviceLij
This is particularly simple for the sigmoid loss as each pair
is an independent term in the loss. Figure 1 illustrates this
32 8 32 262 1024
Batch Size (k)8182838485ImageNet 0-shot
SigLiT
4 8 16 32 98 307
Batch Size (k)6668707274
SigLIP
Sigmoid
Softmax
16 32 65 131 245
Batch Size (k)30313233343536XM TI 36 lang. avg.
mSigLIPFigure 2: The effect of pre-training batch size. Left: SigLiT results , trained for 18B seen examples. Sigmoid loss outper-
forms the softmax loss signiﬁcantly with small batch sizes, and performs similarly at larger batch sizes. We successfully
trained an SigLiT model with up to one million batch size. However, performance for both sigmoid and softmax saturate at
around 32 k batch size. Middle: SigLIP results , trained for 9B seen examples. Both sigmoid loss and softmax loss saturate
at a reasonable batch size, while the peak of the sigmoid loss comes earlier and slightly outperforms the peak of the softmax
loss. A very large batch size hurts both losses. Right: mSigLIP results , trained for 30B seen examples. With a multilingual
setup using over 100 languages, 32 k batch size is surprisingly sufﬁcient and scaling beyond that hurts performance on a
36-language cross-modal retrieval task.
method. In words, we ﬁrst compute the component of the
loss corresponding to the positive pairs, and b1nega-
tive pairs. We then permute representations across devices,
so each device takes negatives from its neighbouring de-
vice (next iteration of sum B). The loss is then calculated
with respect to this chunk (sum C). This is done indepen-
dently in each device, such that each device computes the
loss with respect to its local batch b. Losses can then simply
be summed across all devices (sum A). Individual collec-
tive permutes (for sum B) are fast (and indeed Dcollective
permutes is typically faster than two all-gathers between D
devices), and the memory cost at any given moment is re-
duced fromjBj2tob2(for sum C). Usuallybis constant as
scalingjBjis achieved by increasing the number of accel-
erators. Due to being quadratic with respect to the batch
size, the vanilla loss computation rapidly bottlenecks scal-
ing up. This chunked approach enabled training with batch
sizes over 1 million on relatively few devices.
4. Results
In this section, we evaluate the proposed SigLiT and
SigLIP models across a wide range of batch sizes. We dis-
cuss what can be achieved with a small number of accel-
erator chips, using both SigLiT and SigLIP recipes. We
also brieﬂy discuss the impact of batch size on multilin-
gual language image pre-training. We ablate the importance
of our large-batch stabilization modiﬁcation and the intro-
duced learned bias term and present a study on the effect of
positive and negative pairs ratio in the sigmoid loss. Lastly,we explore SigLIPs data noise robustness.
To validate our models, we report zero-shot transfer re-
sults on the ImageNet dataset [14] and zero-shot retrieval
results across 36 languages on the XM3600 dataset [44].
We use the ScalingViT-Adafactor optimizer [58] by default
for all our experiments.
4.1. SigLiT: Scaling batch size to the limit
Following [59], we use the same precomputed embed-
dings for the images using a ViT-g vision model, and train
a base size text tower from scratch with the same hyperpa-
rameters using the LiT image-text dataset [59].
We perform a study over a wide range of batch sizes,
from 512 to 1M, demonstrating the impact of batch size
for contrastive learning. Results are presented in Figure 2
(left). When the batch size is smaller than 16k, sigmoid loss
outperforms softmax loss by a large margin. With growing
batch sizes, we observe that softmax loss quickly catches
up and potentially slightly underperforms sigmoid loss with
a large enough batch size. Overall, we recommend using
the SigLIP recipe for large batch sizes as well, due to the
simplicity, compute savings, and straightforward memory
efﬁcient implementation.
There is a consensus that contrastive learning beneﬁts
from large batch sizes, while most of the existing studies
stop at 64 k batch size [59, 35, 10]. We successfully trained
an SigLiT model at one million batch size, to explore the
limit of contrastive learning. To our surprise, the perfor-
mance saturates at 32 k batch size, further scaling up the
batch size only gives a minor boost, and the model peaks at
4450 900 3000 18'000
Examples Seen [M]7879808182838485ImageNet 0-shot
    8k
262kSigmoid
SoftmaxFigure 3: SigLiT ImageNet 0-shot transfer results with
different training durations. Large batch size results in a
big performance boost, but needs a sufﬁciently long sched-
ule to ramp up, as for short schedules, very large batch size
results in a small number of gradient update steps.
256 k batch size. Our best SigLiT with a B-sized text mode
achieves 84.7% zero-shot transfer accuracy on ImageNet,
while the original LiT paper reports a slightly better 85.2%
score with a 10 times larger g-sized text model. Figure 3
presents the impact of training duration for different batch
sizes. It demonstrates that large, 262kbatch size signiﬁ-
cantly outperforms smaller 8kbatch size when trained for
a sufﬁciently long time. Note, that for short training dura-
tions, large batch size leads to the fewer absolute number of
update steps and thus needs more time to ramp up.
4.2. SigLIP: Sigmoid loss is beneﬁcial for language-
image pre-training
We pre-train SigLIP models on the WebLI dataset [13],
using only English image and text pairs. We use CLIP (We-
bLI) to denote the CLIP baseline pre-trained on WebLI with
the standard softmax loss. We use moderately-sized mod-
els: B/16 ViT for image embeddings and B-sized trans-
former for text embeddings. The input images are resized to
224224 resolution. The text is tokenized by a 32 k vocab-
ulary sentencepiece tokenizer [27] trained on the English
C4 dataset [37], and a maximum of 16 text tokens are kept.
Figure 2 middle plot shows SigLIP results, With less than
32 k batch size, SigLIP outperforms CLIP (WebLI) base-
lines. On the other end of the scale, the memory efﬁciency
of the sigmoid loss enabled much larger batch sizes. For ex-
ample, with four TPU-v4 chips, we could ﬁt a batch size of
4096 with a Base SigLIP but only 2048 with a correspond-
ing CLIP model. The two advantages together demonstrate
signiﬁcant beneﬁts of the sigmoid loss for language image
pre-training with ﬁxed resources, which will be discussed
in Section 4.5.16 k 32 k 64 k 128 k 240 k
INet-0 71.6 73.2 73.2 73.2 73.1
XM avg 34.8 34.9 34.4 33.6 32.7
XM de 54.7 54.8 55.4 54.3 54.7
XM en 46.5 46.2 46.5 46.6 46.6
XM hi 9.1 8.5 7.9 8.1 7.3
XM ru 50.1 49.9 49.7 48.6 49.3
XM zh 30.7 32.5 32.0 30.6 23.7
Table 2: Multilingual SigLIP results with various batch
sizes, pre-trained for 30 billion seen examples. We report
zero-shot transfer results on ImageNet (INet-0) and aver-
aged text to image retrieval results across 36 languages on
the crossmodal 3600 dataset (XM). The full table on 36 lan-
guages can be found in Appendix.
As batch size increases, the gap between the sigmoid and
the softmax losses diminish. SigLIP performs best at batch
size 32 k, whereas the softmax loss required 98 k for optimal
performance and still didnt outperform the sigmoid based
variant. Scaling further, a larger batch size like 307 k hurts
both losses.
4.3. mSigLIP: Multi-lingual pre-training
We further scale up the training data by keeping all the
100 languages from the WebLI dataset [13]. With multi-
lingual data, one usually needs to use a larger international
vocabulary. We ﬁrst verify the impact of two tokenizers: a
small multilingual vocabulary with 32 k tokens [37], and a
large multilingual vocabulary with 250 k tokens [54]. We
train B-sized ViT and text models for 900Mtotal exam-
ples seen, and observe slightly more than 1% improvement
when using a larger vocabulary.
However, the token embeddings become huge for very
large vocabulary sizes. Following the standard setup, we
would need to store a NWtoken embedding lookup table
to train the multilingual model, where Nis the vocabulary
size mentioned above and Wis the embedding dimension
of the text model. To save memory, we propose to use a
bottlenecked token embedding. We use NKembed-
ding matrix and additional KWprojection, where the
bottleneckKis much smaller than W.
In our experiments, we observed that using a large mul-
tilingual vocabulary with a bottleneck can be scaled up as
efﬁciently as using a small multilingual vocabulary. Specif-
ically, by enabling the bottleneck of size K= 96 for Base
architecture with W= 768 , we only see about a half per-
cent quality drop on ImageNet zero-shot transfer, compared
to using the full 250kvocabulary.
512 4 8 16 24010203040506070INet 0-shot
12 4 8 16 24
Examples Seen [100M]010203040506070INet 10-shotfrom-scratch
fine-tune
fine-tune w/o enc.wdFigure 4: Top: SigLIP with pre-trained encoders ramps up
quickly. However, only disabling weight decay on the pre-
trained encoder weights leads to stable behavior and good
ImageNet 0-shot transfer results. Bottom : ImageNet 10-
shot transfer results, where decaying the pre-trained weights
leads to deterioration of the pre-trained model visual repre-
sentation quality. Disabling weight decay ﬂattens the curve.
With the memory improvements, we train mSigLIP
models for various batch sizes, for a total of 30 billion ex-
amples seen. Table 2 and Figure 2 (right plot) show the
results. We were expecting a large batch size to improve
multilingual pre-training, where the model sees more ex-
amples from the same language as hard negatives in a sin-
gle mini-batch. However, we didnt observe clear improve-
ments with a batch size larger than 32 k. A batch size of
32 k is sufﬁcient for a multilingual setup as well. On the
XM3600 cross-modal retrieval tasks, we found that going
beyond 32 k batch size leads to worse results on average
while on ImageNet zero-shot transfer it stays ﬂat. mSigLIP
sets the new state-of-the-art on XM3600 text to image re-
trieval task, with only a Base size model. Our best result is
34.9%, which is more than 6% higher than the previously
reported result 28.5% [13] with a standard LiT model [59]
using a much larger four billion ViT-e model. We further
scale up mSigLIP training in Section 4.6.
4.4. SigLiT with four TPU-v4 chips
For many practitioners, the important question usually is
what can be trained with a limited amount of resources?
We explore the usage of SigLiT models in this section with
only four TPU-v4 chips, as the memory efﬁcient sigmoid
loss is suitable for this application scenario.
3456Loss Lβ2=0.999
β2=0.95
110||wL||
1B 2B 3B 4B 5B
Examples seen24||Δw||Figure 5: The effect of Adam and AdaFactors 2.As
we increase batch-size, we observe more frequent training
instability. This instability seen in the loss curves (top) is
caused by spikes in gradient norm (middle) leading to large
parameter updates (bottom). Decreasing the 2momentum
stabilizes training. Occasional gradient spikes still happen
(see step at 2B), but do not destabilize the training process.
We follow the same setup as in section 4.1. We use
the publicly available ViT-AugReg-B/8 [42] model as the
frozen (
 ) vision tower, and precompute embeddings to ac-
celerate the training [59]. The text model is a Large Trans-
former, but with a depth of only 12 layers (instead of 24).
It is trained using the LION [12] optimizer with decoupled
weight decay 1107, linearly warm-up of learning rate
over 6.5k steps up to a peak of 1104, followed by a co-
sine decay to 0. We train for a total of 65 000 steps with a
batch size of 32k  this leads to just under one day of train-
ing. Table 1 shows the results when training a model on four
chips for one day, achieving 79.7% 0-shot ImageNet classi-
ﬁcation accuracy; very competitive in this limited resource
regime. With a ViT-g/14 [58] model as the vision tower and
a Large text tower, we can train at 20 k batch size on four
chips for 107 k steps in under two days. This further pushes
the 0-shot ImageNet classiﬁcation accuracy up to 84.5%.
4.5. SigLIP with a small amount of TPU-v4 chips
Its resource demanding to train a CLIP model from-
scratch in general, with SigLIP its possible to ﬁt a larger
train batch size with fewer amount of chips. In this section,
we explore ways to train SigLIP models efﬁciently with pre-
trained weights. We use pre-trained weights to initialize the
image model to accelerate the pre-training, which was orig-
61 : 16k 1 : 1.6k 1 : 164 1 : 16 1 : 1.6607080
ImageNet 0-shot
Random
Hard
Hard, matched pairs
Easy
1 : 16k 1 : 1.6k 1 : 164 1 : 16 1 : 1.6151050
Learned bias
1 : 16k 1 : 1.6k 1 : 164 1 : 16 1 : 1.6201510505
Average logit of pos and negFigure 6: The effect of batch composition. We simulate various batch compositions by masking out negatives, either
randomly, keeping only the hardest, or the easiest. With no masking, we have 16 k negatives for each positive in the batch
(1:16 k) and the strongest masking we apply (1:1.6) results in almost balanced minibatches. In one setting we match total
pairs seen by training for signiﬁcantly longer. We observe ImageNet 0-shot score, the ﬁnal value of the learned bias, and the
average logits of positive and negative pairs. Overall, the imbalance does not seem to be detrimental, but ﬁnding an efﬁcient
way of mining negatives might be beneﬁcial.
inally discussed in [59]. We use the public and unlocked
ViT-AugReg-B/16 [42] model to initialize our vision tower
and ﬁne-tune on the same WebLI English data as used for
SigLIP. In all the experiments, we apply a 0.1 learning rate
multiplier to the pre-trained image tower to make it suitable
for ﬁne-tuning.
Figure 4 presents unlocked
 ﬁne-tuning results along-
side from-scratch randomly initialized baselines. We used
16 TPU-v4 chips and train at 16 k batch size for 2.4 B ex-
amples seen. We found that the ﬁne-tuning setup doesnt
perform well out-of-the-box; this is consistent with prior
works [59] where ﬁnetuning image models degraded visual
representation quality. This is evidenced by ImageNet 10-
shot linear classiﬁcation, where in Figure 4 the ﬁne-tuned
setup is barely better than the from-scratch baseline.
We hypothesize that the default weight decay applied to
the pre-trained weights reduces their effectiveness. Moti-
vated by the ﬁne-tuning recipe from [17, 58, 25], that uses
no weight decay, we also propose disabling weight decay on
the pre-trained weights for SigLIP training. Weight decay
is therefore only applied to the randomly initialized weights
in the text model. This simple modiﬁcation signiﬁcantly
improved SigLIP results. Figure 4 shows that with our im-
proved recipe, SigLIP reaches 71% 0-shot accuracy on Im-
ageNet, using 16kbatch size, trained on 16 chips for three
days. We also present from-scratch results in the bottom
rows of Table 1: with 32 TPUv4 chips for only two days,
SigLIP achieves 72.1% 0-shot accuracy. This presents a
signiﬁcant training cost reduction e.g. compared to CLIP
(approx. 2500 TPUv3-days for 72.6%) reported in [30].4.6. Scaling up SigLIP and mSigLIP
In this section, we scale up SigLIP by overtraining the
model [45, 1]. We present results in Table 3 using ViT-B,
ViT-L or So-400m [1] as the vision encoder, with a text en-
coder of the same size (B, L and So-400m respectively).
Following the recipe described in Section 4.2, we train both
models for 40 billion examples seen at batch size 32 k, but
use(256=16)2= 256 image patches and 64 text tokens (in-
stead of 16). To get SigLIP models for different resolutions,
we train for 5 billion more examples at the target resolution,
with a 100x smaller learning rate and no weight decay. In
Table 3, we report zero-shot classiﬁcation results on Im-
ageNet [14], ObjectNet [2], ImageNet-v2 [39], ImageNet
ReaL [3], and zero-shot image-to-text (I !T) retrieval, text-
to-image (I!T) retrieval results on MSCOCO [11].
We also scale up the multilingual mSigLIP ViT-B model
in the same way. We report image-text retrieval results
across 36 languages on the XM3600 benchmark [44]. The
scaled-up mSigLIP ViT-B model achieves the state-of-the-
art42.6% image retrieval recall@1 and 54.1% text retrieval
recall@1 for a Base model. This is slightly outperformed
by the Large model in [48] getting 42.96% image retrieval
recall@1. Detailed results are provided in Appendix Table 9
and Figure 8, denoted as *32 k.
4.7. Stabilizing large-batch training
As we move to large batch sizes, the language image pre-
training using transformers becomes increasingly more un-
stable, even when using a modestly-sized model (e.g. Base
size). The reason for these instabilities is large spikes in the
7MethodImage Encoder ImageNet-1k COCO R@1
ViT size # Patches Validation v2 ReaL ObjectNet I !T T!I
CLIP B 196 68.3 61.9 - 55.3 52.4 33.1
OpenCLIP B 196 70.2 62.3 - 56.0 59.4 42.3
EV A-CLIP B 196 74.7 67.0 - 62.3 58.7 42.2
SigLIP B 196 76.2 69.6 82.8 70.7 64.4 47.2
SigLIP B 256 76.7 70.0 83.1 71.3 65.1 47.4
SigLIP B 576 78.6 72.1 84.5 73.8 67.5 49.7
SigLIP B 1024 79.2 73.0 84.9 74.7 67.6 50.4
CLIP L 256 75.5 69.0 - 69.9 56.3 36.5
OpenCLIP L 256 74.0 61.1 - 66.4 62.1 46.1
CLIPA-v2 L 256 79.7 72.8 - 71.1 64.1 46.3
EV A-CLIP L 256 79.8 72.9 - 75.3 63.7 47.5
SigLIP L 256 80.5 74.2 85.9 77.9 69.5 51.1
CLIP L 576 76.6 72.0 - 70.9 57.9 37.1
CLIPA-v2 L 576 80.3 73.5 - 73.1 65.5 47.2
EV A-CLIP L 576 80.4 73.8 - 78.4 64.1 47.9
SigLIP L 576 82.1 75.9 87.0 81.0 70.6 52.7
OpenCLIP G (2B) 256 80.1 73.6 - 73.0 67.3 51.4
CLIPA-v2 H (630M) 576 81.8 75.6 - 77.4 67.2 49.2
EV A-CLIP E (5B) 256 82.0 75.7 - 79.6 68.8 51.1
SigLIP SO (400M) 729 83.2 77.2 87.5 82.9 70.2 52.0
Table 3: Comparison with other publicly released models. Our SigLIP models outperform all prior models, e.g. Open-
CLIP [22] and CLIP [36], by a signiﬁcant margin on both zero-shot classiﬁcation and retrieval tasks. Compared to the concur-
rent EV A-CLIP [43] and CLIPA-v2 [29], our SigLIP-L performs better across the board, in both the low and high resolution
cases. Especially noteworthy is the Shape-Optimized 400M parameter ViT [1] architecture, which outperforms all signiﬁ-
cantly larger models. We publicly release our models: https://github.com/google-research/big_vision .
gradient norms, which translate to large-magnitude changes
in the weights that may destabilize the training process,
see Figure 5. We observe that reducing 2in Adam and
AdaFactor from its default 0.999 to 0.95 (which was sug-
gested in [20, 9]) is enough to stabilize the training. Intu-
itively, this allows recovering from gradient spikes quicker.
We opt for setting 2= 0:95for all our experiments.
4.8. Negative ratio in sigmoid loss
One question which arises when shifting the perspective
from the softmaxs pick the right class view to the sig-
moids rate this pair view, is the imbalance in positive
versus negative pairs. For a batch size jBj, the batch con-
tainsjBjpositive pairs, but jBj2jBj negative examples.
In the modest batch-size of 16 k, there are actually 268 M
negative examples for only 16 k positive ones. At the same
time, because the sigmoid loss decomposes into a sum of
per-example losses, we can perform controlled experiments
to study the effect of the mini-batch composition and dis-tribution of examples visited. We run experiments in the
SigLiT setup at batch-size 16 k for 900 M steps and vary
the composition of the batch by masking out ( i.e. ignoring)
enough negative examples to reach a target positive : neg-
ative ratio, masking in the following ways:
Random: Randomly choose negative pairs to mask.
Hard: Keep hardest negative pairs (highest loss).
Easy: Keep easiest negatives pairs (lowest loss).
Hard + matching total pairs seen: Masking exam-
ples while training for a ﬁxed number of steps does
decrease the total number of pairs seen during train-
ing. Hence in the matched pairs setting, we increase
the number of training steps by the masking ratio in
order to keep the number of pairs seen constant.
Figure 6 shows the effect of the various masking strate-
gies. Randomly removing negatives to rebalance does dete-
riorate performance. Keeping the easiest examples does not
work at all, while keeping the hardest negatives does almost
80.0 0.2 0.40.500.520.540.56ImageNet 0shot
Image
0.0 0.2 0.4
Text
Sigmoid
Softmax
0.0 0.2 0.4
p(corruption)
Batch
0.0 0.1 0.2
Image & Text
0.0 0.1 0.2
Image, Text & BatchFigure 7: Sigmoid-training increases robustness to data noise. Titles show the type of corruption applied, and x-axes show
the probability with which they are applied. With increasing corruption severity, M-scale models trained with sigmoid loss
for 3.6 billion examples retain superiority over corresponding softmax baseline.
maintain the quality, indicating that, as could be expected,
a lot of the learning on the negative side comes from the
harder examples. This is further conﬁrmed by the slightly
increased performance of training longer on the hardest ex-
amples in order to match the total pairs seen.
We also look at the value of the learned bias at the end of
training as well as the average logit value for positive and
negative examples across these settings, and ﬁnd the result
mostly follows what one would expect: as fewer negatives
are present, the bias and logits become more positive over-
all. Interestingly, when training with more hard negative
pairs, the average logits of positive pairs stays mostly ﬂat.
This study conﬁrms that (1) the imbalance does not seem
to be a major reason for concern, while at the same time (2)
coming up with an efﬁcient way of including more negative
examples can be promising but is not trivial.
4.9. Bias term in sigmoid loss
We ablate the bias term in the loss function, using the
Base architecture with an 8 k batch size, trained for 900M
examples with the SigLIP setup. Zero-shot transfer results
are reported on ImageNet [14], Oxford-iiit pet [34] and Ci-
far100 [26]. Table 4 presents results with and without a bias
term in the sigmoid loss.
Table 4: Bias (b) and temperature (t0) initialization. Re-
sults are reported using Base architecture, 8 k batch size,
trained for 900M examples. Enabling the bias term b with
10initialization improves results consistently.
b t0INet-0 Pet-0 C100-0
n/a log 10 62.0 81.8 59.9
-10 log 10 63.0 82.4 61.0
-10 log 1 61.0 80.0 60.4
0 log 10 61.7 79.9 59.0
0 log 1 53.7 73.2 53.8Enabling the bias term with a 10initialization consis-
tently improves performance across all tasks. This is be-
cause the bias term ensures that the training starts close to
the prior, preventing dramatic over-correction in early op-
timization. In contrast, a randomly chosen bias term ini-
tialization, such as the 0 initialization in Table 4, fails to
address the over-correction issue, leading to signiﬁcantly
worse results. This effect is particularly noticeable when
using a small temperature t0initialization. We set the bias
and temperature initialization to b=10andt0= log 10
(hencet= 10 ) as the default for all experiments.
4.10. Label noise robustness
Prior works demonstrated improved robustness against
label noise when using the sigmoid loss for classiﬁcation
models [3]. This property would be particularly useful here
in the face of the famously noisy nature of popular large-
scale image-text datasets. In order to study this for SigLIP,
we train M/16 image models alongside an M text model at
batch size 16384 for 3.6 billion seen examples. We corrupt
the training data using one of the following methods:
Image : With probability p, replace the image with uni-
form random noise.
Text: With probability p, replace tokenized text with a
new sequence of randomly sampled tokens, up to some
(sampled) sequence length.
Batch alignment : Randomly shufﬂe the ordering of
p% of the batch.
Image & text : Apply both with probability peach.
Image, text & batch : Alongside (4), also shufﬂe frac-
tionpof alignments.
Results from varying the likelihood of the corruption are
shown in Figure 7. Models trained with sigmoid loss are
increasingly robust to all kinds of added noise.
95. Conclusion
We conducted a study on two language-image pre-
training instances that used the sigmoid loss: SigLiT and
SigLIP. Our results demonstrate that the sigmoid loss per-
forms better than the softmax baseline, particularly for
small train batch sizes. This loss function is also more mem-
ory efﬁcient, which allows larger train batch sizes without
requiring additional resources. We performed a thorough
investigation of the batch size in contrastive learning. Sur-
prisingly, we found that a relatively modest batch size of
32 k yielded nearly optimal performance. Further studies
have been performed to understand better the introduced
bias term in the sigmoid loss, robustness to data noises and
the impact of positive and negative pairs ratio in the sigmoid
loss. We hope this work will facilitate language-image pre-
training research with limited resources.
Acknowledgements. We thank Daniel Keysers, Ilya Tol-
stikhin, Olivier Bousquet and Michael Tschannen for their
valuable feedback and discussions on this paper. We thank
Joan Puigcerver, Josip Djolonga and Black Hechtman for
discussions on efﬁcient implementations of the chunked
contrastive loss. We thank Kaiming He and Xinlei Chen for
the discussion of 2to stabilize the training. We also thank
Ross Wightman for spotting a mistake in the pseudocode in
the ﬁrst version of this paper, Boris Dayma and Krzysztof
Maziarz for spotting typos in the second and third versions
which made tvst0confusing. We thank the Google Deep-
mind team for providing a supportive research environment.
We use the bigvision codebase [5, 4] for all experi-
ments in this project.
10References
[1] Ibrahim Alabdulmohsin, Xiaohua Zhai, Alexander
Kolesnikov, and Lucas Beyer. Getting vit in shape:
Scaling laws for compute-optimal model design. In
NeurIPS , 2023. 7, 8, 17
[2] Andrei Barbu, David Mayo, Julian Alverio, William Luo,
Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and
Boris Katz. ObjectNet: A large-scale bias-controlled dataset
for pushing the limits of object recognition models. In
NeurIPS , 2019. 7, 17
[3] Lucas Beyer, Olivier J. H enaff, Alexander Kolesnikov, Xi-
aohua Zhai, and A aron van den Oord. Are we done with
imagenet? CoRR , abs/2006.07159, 2020. 2, 7, 9, 17
[4] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Bet-
ter plain vit baselines for imagenet-1k, 2022. 10, 17
[5] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Big
vision. https://github.com/google-research/
big_vision , 2022. 10, 17
[6] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun
Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-
700m: Image-text pair dataset. https://github.com/
kakaobrain/coyo-dataset , 2022. 1
[7] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
Soricut. Conceptual 12M: Pushing web-scale image-text
pre-training to recognize long-tail visual concepts. In CVPR ,
2021. 1
[8] Feilong Chen, Duzhen Zhang, Minglun Han, Xiu-Yi Chen,
Jing Shi, Shuang Xu, and Bo Xu. VLP: A survey on vision-
language pre-training. Int. J. Autom. Comput. , 20(1):3856,
2023. 2
[9] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-
woo Jun, David Luan, and Ilya Sutskever. Generative pre-
training from pixels. In Proceedings of the 37th Interna-
tional Conference on Machine Learning, ICML 2020, 13-18
July 2020, Virtual Event , volume 119 of Proceedings of Ma-
chine Learning Research , pages 16911703. PMLR, 2020.
8
[10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey E. Hinton. A simple framework for contrastive learn-
ing of visual representations. In ICML , 2020. 2, 4
[11] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-
tam, Saurabh Gupta, Piotr Doll ar, and C. Lawrence Zitnick.
Microsoft COCO captions: Data collection and evaluation
server. CoRR , abs/1504.00325, 2015. 7, 17
[12] Xiangning Chen, Chen Liang, Da Huang, Esteban Real,
Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang
Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V . Le. Symbolic
discovery of optimization algorithms, 2023. 2, 6
[13] Xi Chen, Xiao Wang, Soravit Changpinyo, A. J. Piergio-
vanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,
Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander
Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Has-
san Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal,
James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini,
Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas
Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, andRadu Soricut. Pali: A jointly-scaled multilingual language-
image model. CoRR , abs/2209.06794, 2022. 1, 5, 6, 17
[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR , 2009. 4, 7, 9, 17
[15] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin John-
son. Redcaps: Web-curated image-text data created by the
people, for the people. In Joaquin Vanschoren and Sai-
Kit Yeung, editors, Proceedings of the Neural Information
Processing Systems Track on Datasets and Benchmarks 1,
NeurIPS Datasets and Benchmarks 2021, December 2021,
virtual , 2021. 1
[16] Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen,
Shuyang Gu, Weiming Zhang, Lu Yuan, Dong Chen, Fang
Wen, and Nenghai Yu. Clip itself is a strong ﬁne-tuner:
Achieving 85.7% and 88.0% top-1 accuracy with vit-b and
vit-l on imagenet. CoRR , abs/2212.06138, 2022. 2
[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 1616 words: Transformers for image recognition at
scale. In ICLR , 2021. 3, 7, 17
[18] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
Deep Learning . MIT Press, 2016. http://www.
deeplearningbook.org . 1
[19] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimension-
ality reduction by learning an invariant mapping. In CVPR ,
volume 2, 2006. 2
[20] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Pi-
otr Doll ar, and Ross B. Girshick. Masked autoencoders
are scalable vision learners. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition, CVPR 2022, New
Orleans, LA, USA, June 18-24, 2022 , pages 1597915988.
IEEE, 2022. 8
[21] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang,
Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up
vision-language pre-training for image captioning. CoRR ,
abs/2111.12233, 2021. 1, 2
[22] Gabriel Ilharco, Mitchell Wortsman, Nicholas Carlini,
Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok
Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi,
and Ludwig Schmidt. OpenCLIP. Zenodo, 2021. 8
[23] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc V . Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representation
learning with noisy text supervision. In ICML , 2021. 1, 2
[24] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,
Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and
Dilip Krishnan. Supervised contrastive learning. In Hugo
Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-
Florina Balcan, and Hsuan-Tien Lin, editors, Advances in
Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual , 2020. 2
[25] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan
Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.
11Big transfer (BiT): General visual representation learning. In
ECCV , 2020. 7
[26] Alex Krizhevsky. Learning multiple layers of features from
tiny images. Technical report, Univ. of Toronto, 2009. 9
[27] Taku Kudo and John Richardson. SentencePiece: A sim-
ple and language independent subword tokenizer and detok-
enizer for neural text processing. In EMNLP , 2018. 5, 14
[28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.
Hoi. BLIP: bootstrapping language-image pre-training for
uniﬁed vision-language understanding and generation. In
Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba
Szepesv ari, Gang Niu, and Sivan Sabato, editors, Interna-
tional Conference on Machine Learning, ICML 2022, 17-
23 July 2022, Baltimore, Maryland, USA , volume 162 of
Proceedings of Machine Learning Research , pages 12888
12900. PMLR, 2022. 2
[29] Xianhang Li, Zeyu Wang, and Cihang Xie. Clipa-v2: Scal-
ing CLIP training with 81.1% zero-shot imagenet accuracy
within a $10, 000 budget; an extra $4, 000 unlocks 81.8%
accuracy. CoRR , abs/2306.15658, 2023. 8
[30] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichten-
hofer, and Kaiming He. Scaling language-image pre-training
via masking. CoRR , abs/2212.00794, 2022. 2, 7
[31] Matthias Minderer, Alexey A. Gritsenko, Austin Stone,
Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy,
Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani,
Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf,
and Neil Houlsby. Simple open-vocabulary object detection.
In Shai Avidan, Gabriel J. Brostow, Moustapha Ciss e, Gio-
vanni Maria Farinella, and Tal Hassner, editors, Computer
Vision - ECCV 2022 - 17th European Conference, Tel Aviv,
Israel, October 23-27, 2022, Proceedings, Part X , volume
13670 of Lecture Notes in Computer Science , pages 728
755. Springer, 2022. 2
[32] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker
Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer,
Inioluwa Deborah Raji, and Timnit Gebru. Model cards
for model reporting. In danah boyd and Jamie H. Morgen-
stern, editors, Proceedings of the Conference on Fairness,
Accountability, and Transparency, FAT* 2019, Atlanta, GA,
USA, January 29-31, 2019 , pages 220229. ACM, 2019. 17
[33] Jishnu Mukhoti, Tsung-Yu Lin, Omid Poursaeed, Rui Wang,
Ashish Shah, Philip H. S. Torr, and Ser-Nam Lim. Open
vocabulary semantic segmentation with patch aligned con-
trastive learning, 2022. 2
[34] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and
C. V . Jawahar. Cats and dogs. In IEEE Conference on Com-
puter Vision and Pattern Recognition , 2012. 9
[35] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu,
Adams Wei Yu, Minh-Thang Luong, Mingxing Tan, and
Quoc V . Le. Combined scaling for zero-shot transfer learn-
ing. CoRR , abs/2111.10050, 2021. 2, 4
[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In ICML , 2021.
1, 2, 3, 8[37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J. Liu. Exploring the limits of transfer learning with a
uniﬁed text-to-text transformer. arXiv e-prints , 2019. 5, 14
[38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J. Liu. Exploring the limits of transfer learning with
a uniﬁed text-to-text transformer. J. Mach. Learn. Res. ,
21:140:1140:67, 2020. 17
[39] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and
Vaishaal Shankar. Do ImageNet classiﬁers generalize to Im-
ageNet? In ICML , 2019. 7, 17
[40] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, Patrick Schramowski, Srivatsa Kundurthy, Katherine
Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia
Jitsev. LAION-5B: an open large-scale dataset for training
next generation image-text models. CoRR , abs/2210.08402,
2022. 1
[41] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael
Bendersky, and Marc Najork. WIT: wikipedia-based image
text dataset for multimodal multilingual machine learning.
CoRR , abs/2103.01913, 2021. 1
[42] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross
Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train
your ViT? Data, augmentation, and regularization in vision
transformers. CoRR , abs/2106.10270, 2021. 1, 6, 7
[43] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue
Cao. EV A-CLIP: improved training techniques for CLIP at
scale. CoRR , abs/2303.15389, 2023. 8
[44] Ashish V . Thapliyal, Jordi Pont-Tuset, Xi Chen, and Radu
Soricut. Crossmodal-3600: A massively multilingual mul-
timodal evaluation dataset. In Yoav Goldberg, Zornitsa
Kozareva, and Yue Zhang, editors, Proceedings of the 2022
Conference on Empirical Methods in Natural Language Pro-
cessing, EMNLP 2022, Abu Dhabi, United Arab Emirates,
December 7-11, 2022 , pages 715729. Association for Com-
putational Linguistics, 2022. 4, 7, 17
[45] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth ee Lacroix, Baptiste
Rozi ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur elien
Rodriguez, Armand Joulin, Edouard Grave, and Guillaume
Lample. Llama: Open and efﬁcient foundation language
models. CoRR , abs/2302.13971, 2023. 7
[46] A aron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-
sentation learning with contrastive predictive coding. CoRR ,
abs/1807.03748, 2018. 2
[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS , 2017. 3,
17
[48] Alexander Visheratin. Nllb-clip  train performant multilin-
gual image retrieval model on a budget, 2023. 7
[49] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,
Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.
GIT: A generative image-to-text transformer for vision and
language. CoRR , abs/2205.14100, 2022. 1, 2
12[50] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia
Tsvetkov, and Yuan Cao. Simvlm: Simple visual language
model pretraining with weak supervision. In The Tenth In-
ternational Conference on Learning Representations, ICLR
2022, Virtual Event, April 25-29, 2022 . OpenReview.net,
2022. 2
[51] Ross Wightman, Hugo Touvron, and Herv e Jegou. Resnet
strikes back: An improved training procedure in timm.
CoRR , abs/2110.00476, 2021. 2
[52] Mitchell Wortsman. Reaching 80% zero-shot accuracy with
OpenCLIP: VIT-G/14 trained on LAION-2B. https:
//web.archive.org/web/20230127012732/
https://laion.ai/blog/giant-openclip/ . 2
[53] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim,
Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gon-
tijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok
Namkoong, and Ludwig Schmidt. Robust ﬁne-tuning of
zero-shot models. In IEEE/CVF Conference on Computer
Vision and Pattern Recognition, CVPR 2022, New Orleans,
LA, USA, June 18-24, 2022 , pages 79497961. IEEE, 2022.
2
[54] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,
Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin
Raffel. mT5: A massively multilingual pre-trained text-to-
text transformer. In NAACL-HLT , 2021. 5, 17
[55] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce
Liu, Lu Yuan, and Jianfeng Gao. Uniﬁed contrastive learn-
ing in image-text-label space. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition, CVPR 2022, New
Orleans, LA, USA, June 18-24, 2022 , pages 1914119151.
IEEE, 2022. 2
[56] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung,
Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Con-
trastive captioners are image-text foundation models. CoRR ,
abs/2205.01917, 2022. 2
[57] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,
Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu,
Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao,
Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and
Pengchuan Zhang. Florence: A new foundation model for
computer vision. CoRR , abs/2111.11432, 2021. 2
[58] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu-
cas Beyer. Scaling vision transformers. CVPR , 2022. 1, 4,
6, 7, 14
[59] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner,
Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer.
Lit: Zero-shot transfer with locked-image text tuning. In
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, CVPR 2022, New Orleans, LA, USA, June 18-
24, 2022 , pages 1810218112. IEEE, 2022. 1, 2, 3, 4, 6, 7,
14
[60] Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D.
Manning, and Curtis P. Langlotz. Contrastive learning of
medical visual representations from paired images and text.
In Zachary C. Lipton, Rajesh Ranganath, Mark P. Sendak,
Michael W. Sjoding, and Serena Yeung, editors, Proceed-
ings of the Machine Learning for Healthcare Conference,MLHC 2022, 5-6 August 2022, Durham, NC, USA , volume
182 of Proceedings of Machine Learning Research , pages
225. PMLR, 2022. 2
13A. More results for SigLiT
In section 4.1, we use the same precomputed embed-
dings for the images using a ViT-g vision model from [59].
Only resize augmentation is applied, to a ﬁxed 288288
resolution. We train a standard base size text tower, using
the ScalingViT-Adafactor optimizer [58] with 1= 0:9and
2= 0:95. We use 0.001 learning rate with a linear warmup
schedule for the ﬁrst 200 M examples seen, and then the
learning rate is decayed to zero with a cosine learning rate
schedule. Weight decay is set to 0.0001 for all the experi-
ments. The text is tokenized by a 32 k vocabulary sentence-
piece tokenizer [27] trained on the English C4 dataset [37],
and a maximum of 16 text tokens are kept. Table 8 shows
results with multiple train examples seen and batch sizes,
for both the sigmoid loss and the softmax loss baseline.
For training SigLiT in under a day with 4 chips (Sec-
tion 4.4), we used the LION optimizer with peak learning
rate1104and weight decay 1107. The learning rate
was warmed linearly to the peak in 6.5 k steps, then cosine
decayed to zero for the remaining 58.5 k steps.
B. More results for SigLIP
In Table 5, we present more results for SigLIP Base with
multiple train examples seen: 3 billion examples and 9 bil-
lion examples respectively.
Batch Size3 B 9 B
sigmoid softmax sigmoid softmax
512 51.5 47.7 - -
1 k 57.3 53.2 - -
2 k 62.1 59.3 - -
4 k 65.3 63.8 68.4 66.6
8 k 68.6 66.6 70.6 69.4
16 k - - 72.3 71.7
32 k 69.9 69.9 73.4 72.9
98 k 69.5 69.7 73.0 73.2
307 k - - 71.6 72.6
Table 5: SigLIP zeor-shot accuracy (%) on the ImageNet
benchmark. Both the sigmoid loss and the softmax loss
baseline are presented. Experiments are performed on mul-
tiple train examples seen (3 B, 9 B) and train batch sizes
(from 512 to 307 k). When trained for 9 B examples, the
peak of the sigmoid loss comes earlier at 32 k than the peak
of the softmax loss at 98 k. Together with the memory ef-
ﬁcient advantage for the sigmoid loss, it allows one to train
the best language-image model with much fewer amount of
accelerators.BS Default Best Best LR Best WD
8 k 70.1 70.1 0.001 0.0001
16 k 70.0 70.0 0.001 0.0001
32 k 68.2 69.0 0.0003 0.00003
Table 6: Default hyperparameters across different batch
sizes, perform either the best or close to the best hyperpa-
rameter from a sweep. Zero-shot accuracy on ImageNet
is reported. BS=batch size, LR=learning rate, WD=weight
decay.
C. Robustness of SigLIP results
Hyperparameters for different batch sizes. Sigmoid
loss doesnt require tuning hyperparameters for different
batch sizes. For example, in both the SigLiP and SigLiT
setup, we only used default 0.001 learning rate and 0.0001
weight decay across a wide range of batch sizes (from 512
to 1024k). We further performed a sweep of 9 hyperparam-
eters across 3 batch sizes on the from-scratch SigLIP setup
for 3B seen examples: learning rate f0.0003, 0.001, 0.003 g
weight decayf0.00003, 0.0001, 0.0003 g batch size
f8 k, 16 k, 32 kg. We observed in Table 6 that the default
LR/WD is either the best or close to the best.
Standard deviation. We repeat SigLIP training ﬁve
times, using the recommended 32k batch size and 3B seen
examples. We report the average and std in Table 7. The std
of the ﬁve runs is very small for both sigmoid and softmax.
Alternative optimizers. We repeat the same experiment
with AdamW optimizer ﬁve times and got very similar re-
sults and std as reported in Table 7. We tested a linear learn-
ing rate scheduler instead of the default cosine learning rate
scheduler, it achieves 69.9% accuracy.
D. More results for mSigLIP
We present the mSigLIP Base crossmodal retrieval re-
sults on the Crossmodal-3600 dataset, across all the 36 lan-
gauges in Figure 8 and Table 9.
Loss Optimizer Results (%)
Softmax ViT-Adafactor 69.9 0.1
Sigmoid ViT-Adafactor 70.1 0.2
Sigmoid AdamW 70.3 0.1
Table 7: Mean and standard deviation of ﬁve repeated ex-
periments. Zero-shot accuracy on ImageNet is reported.
14Batch Size450 M 900 M 3 B 18 B
sigmoid softmax sigmoid softmax sigmoid softmax sigmoid softmax
512 72.5 69.5 75.0 72.8 77.2 74.6 - -
1 k 75.5 73.6 77.2 76.0 79.6 77.9 - -
2 k 77.1 76.3 79.3 78.1 81.3 80.1 82.2 81.2
4 k 79.2 78.3 80.8 79.8 82.4 81.2 83.0 82.0
8 k 80.8 79.7 82.0 81.0 83.1 82.6 83.6 83.1
16 k 81.2 81.2 82.7 82.1 83.8 83.5 84.2 84.1
32 k 81.9 81.4 83.1 82.7 84.2 84.0 84.6 84.4
64 k 81.6 81.6 83.0 82.8 84.3 84.1 84.7 84.4
128 k 80.5 80.0 83.1 83.2 84.2 84.4 84.7 84.6
256 k 72.8 72.2 82.1 81.7 84.3 84.2 84.7 84.6
1024 k - - - - - - 84.7 -
Table 8: SigLiT zero-shot accuracy (%) on the ImageNet benchmark. Both the sigmoid loss and the softmax loss
baseline are presented. Extensive experiments are performed on multiple train examples seen (450 M, 900 M, 3 B, 18 B) and
train batch sizes (from 512 to 1 M).
arbncsdadeelenesfafifilfrhihrhuiditiwjakominlnoplptquz rorusvswtethtrukvizhavg0.00.10.20.30.40.50.60.70.8 16 k 32 k 64 k 128 k 240 k *32 k
arbncsdadeelenesfafifilfrhihrhuiditiwjakominlnoplptquz rorusvswtethtrukvizhavg0.00.10.20.30.40.50.616 k 32 k 64 k 128 k 240 k *32 k
Figure 8: Image-to-text and text-to-image zero-shot retrieval recall@1 results on all 36 languages of Crossmodal-3600 .
Top: Image to text. Bottom: text to image. Colors are batch sizes. *32 k represents the scaled up results as described in
Section 4.6.
E. Label noise experiments
All models had an M/16 image tower and a M text tower.
They were trained from random initialisation for 3.6B ex-
amples seen, with a batch size of 16384. A cosine learning
rate schedule was used, with an initial linear warmup for
10% of steps up to a peak learning rate of 0.001.
15Lang.Image-to-text Text-to-image
16 k 32 k 64 k 128 k 240 k *32 k 16 k 32 k 64 k 128 k 240 k *32 k
ar 52.4 51.3 51.5 51.5 51.1 59.7 37.6 37.4 37.1 36.3 36.0 44.9
bn 11.4 10.8 10.4 10.3 9.9 30.1 5.5 6.2 4.9 5.1 4.4 20.0
cs 54.1 53.7 53.7 52.8 51.8 58.9 41.8 41.6 41.5 39.9 39.4 47.0
da 62.7 62.4 62.0 60.4 59.3 68.4 47.0 47.0 45.6 43.0 43.5 52.9
de 70.3 71.4 71.2 71.1 70.2 79.7 54.7 54.8 55.4 54.3 54.7 65.3
el 36.9 35.8 35.1 34.5 33.8 47.4 22.4 22.8 22.0 21.3 20.8 32.2
en 50.1 50.5 50.2 49.9 50.7 52.5 46.5 46.2 46.5 46.6 46.6 47.6
es 64.7 64.9 67.2 65.3 65.6 66.3 54.8 55.0 55.5 54.5 55.2 57.0
fa 57.0 57.8 56.1 55.3 54.6 66.2 39.6 40.2 38.4 38.4 38.3 50.0
ﬁ 54.9 54.1 53.8 51.7 51.7 59.1 37.7 37.1 36.4 34.0 34.5 44.0
ﬁl 23.2 22.8 22.9 21.4 21.2 29.2 12.8 12.9 12.4 12.2 11.3 20.4
fr 65.7 66.9 67.0 66.1 66.5 71.2 55.9 57.1 55.5 54.4 54.3 61.8
hi 19.9 18.8 19.9 19.5 17.4 32.2 9.1 8.5 7.9 8.1 7.3 17.3
hr 52.7 53.0 53.0 49.9 49.6 62.6 38.2 37.1 36.4 35.2 34.3 47.2
hu 57.0 57.1 56.3 54.8 53.0 62.9 41.4 40.2 40.2 38.6 38.2 51.2
id 64.8 67.1 66.6 65.4 64.7 73.7 48.5 49.4 49.5 47.8 47.3 60.5
it 65.9 66.4 67.1 65.2 66.1 72.3 55.5 56.4 55.8 54.8 54.1 62.3
iw 48.4 47.9 47.7 46.1 45.2 62.2 31.8 31.8 31.9 30.1 30.1 48.0
ja 46.4 45.9 42.9 43.7 30.2 55.1 31.0 31.3 29.2 28.9 18.5 42.3
ko 50.8 49.5 49.4 50.2 46.8 61.4 34.4 34.7 33.2 33.1 31.5 45.9
mi 0.4 0.4 0.6 0.6 0.4 0.3 0.2 0.2 0.2 0.2 0.2 0.3
nl 59.6 60.4 58.9 58.3 57.9 63.6 48.9 49.5 48.9 48.4 47.9 53.6
no 61.4 62.4 62.0 60.9 59.9 65.3 45.3 46.2 45.0 43.5 43.7 50.0
pl 62.2 62.0 62.0 61.1 60.5 67.1 48.8 47.4 48.7 46.8 46.7 56.7
pt 63.1 63.6 64.9 64.3 63.2 65.4 52.4 52.3 52.3 51.9 52.4 57.3
quz 6.8 6.4 6.4 6.6 6.7 6.8 2.7 2.6 2.7 2.7 2.8 2.9
ro 52.1 51.4 51.0 50.6 49.3 61.0 37.2 35.6 34.3 34.5 32.5 49.3
ru 62.2 63.6 63.1 62.7 63.1 68.4 50.1 49.9 49.7 48.6 49.3 59.9
sv 62.3 63.5 63.5 63.1 61.2 67.7 47.9 48.2 47.6 46.2 46.2 52.0
sw 14.8 14.4 14.3 14.2 13.8 17.4 7.8 7.2 7.1 6.9 6.3 10.7
te 1.2 1.2 1.2 1.7 1.1 8.4 0.4 0.3 0.3 0.5 0.3 4.3
th 36.1 35.8 35.6 35.6 28.3 39.0 21.6 23.1 22.2 21.6 16.8 24.6
tr 53.1 54.5 53.7 52.9 51.2 62.0 37.3 37.4 37.8 37.0 36.1 48.1
uk 51.4 51.5 51.2 49.9 49.2 61.2 34.5 33.2 33.8 32.5 32.4 48.3
vi 59.6 59.8 59.5 58.5 58.8 68.4 41.4 41.9 41.9 40.6 40.3 52.3
zh 44.1 45.7 44.1 41.9 36.1 53.9 30.7 32.5 32.0 30.6 23.7 46.8
avg 47.2 47.4 47.1 46.3 45.0 54.1 34.8 34.9 34.4 33.6 32.7 42.6
Table 9: Image-to-text (text retrieval) and text-to-image (image retrieval) zero-shot recall@1 results on all 36 languages
of Crossmodal-3600 , with mSigLIP models trained at different batch sizes for 30 B total examples seen. *32 k represents
the scaled up results as described in Section 4.6.
16F. Model Card
We provide a description of our models following [32].
Model Architecture: The model is trained using the
contrastive pre-training technique with sigmoid loss
as described in this paper. This contrastive model
contains two encoders, i.e. vision transformer en-
coder [17] and language transformer encoder [47]. The
vision and language encoders always have the same
size, one of ViT-B, ViT-L and SoViT-400M [1].
Inputs: The vision encoder takes an image ( 224
2243,2562563,3843843,5125123) as
input. The text encoder takes a tokenized text [38, 54]
cropped to the ﬁrst 64 tokens as input.
Outputs: The vision and text encoders both output a d
dimensional feature vector, where dis 768, 1024 and
1152 for ViT-B, ViT-L and SoViT-400M, respectively.
Intended Use: The models are designed for multi-
modal research purposes. The models can be used
for zero-shot image classiﬁcation and zero-shot image-
text retrieval by comparing both feature vectors. We
provide both en-only and i18n-trained models to en-
courage research on the impact of this choice.
Training Data: The contrastive model is pre-trained
from-scratch using the WebLI [13] dataset. SigLIP
models are pre-trained on a WebLI subset ﬁltered to
contain mostly English. mSigLIP models are pre-
trained on the WebLI dataset without language ﬁlters.
Evaluation Data: Zero-shot classiﬁcation is per-
formed on ImageNet [14], ImageNet v2 [39], Ima-
geNet Real [3], and ObjectNet [2]. Zero-shot re-
trieval is performed on COCO [11] and the multilin-
gual XM3600 dataset [44].
Hardware & Software : The models are developed
in the bigvision codebase [5, 4] and trained on
Google Cloud TPUs.
17
  σ-GPTs: A New Approach to Autoregressive
Models
Arnaud Pannatier1,2, Evann Courdier1,2, and François Fleuret3
1Idiap Research Institute, Martigny, Switzerland
2Ecole Polytechnique Fédérale de Lausanne, Lausanne, Switzerland
3Université de Genève, Geneva, Switzerland
Abstract. Autoregressive models, such as the GPT family, use a fixed
order, usually left-to-right, to generate sequences. However, this is not
a necessity. In this paper, we challenge this assumption and show that
by simply adding a positional encoding for the output, this order can be
modulated on-the-fly per-sample which offers key advantageous proper-
ties. It allows for the sampling of and conditioning on arbitrary subsets
of tokens, and it also allows sampling in one shot multiple tokens dynam-
ically according to a rejection strategy, leading to a sub-linear number
of model evaluations. We evaluate our method across various domains,
including language modeling, path-solving, and aircraft vertical rate pre-
diction, decreasing the number of steps required for generation by an
order of magnitude.
Keywords: Autoregressive models Permutations Transformers Re-
jection Sampling
1 Introduction
Transformers demonstrate exceptional autoregressive capabilities across modal-
ities. The traditional take for autoregression is to follow the natural order of the
data, for example, left-to-right for text. In the case of vision, the usual scheme
is to unfold the images following a raster-scan order and to use transformers
to model the obtained sequence. In this work, we make a distinction between
the order of the input data and the order of autoregression, highlighting that
while they are typically aligned in most applications, they need not be. Our
investigation involves training and generating sequences in a randomly shuffled
order using transformers. While changing the sequence order is more challenging
during training, it also reveals fascinating properties of the models.
By breaking away from the standard autoregression order, one can use the
model to predict the tokens in any particular order. With this scheme, the model
is capable of predicting at any moment of the generation the conditional distri-
bution of the remaining tokens. Having these estimates allows quantifying the
possible outcomes of the generation at any given point. More interestingly, they
can be leveraged to do rejection sampling, allowing to generate sequences by
burst with a dynamical number of steps.arXiv:2404.09562v1  [cs.LG]  15 Apr 20242 A. Pannatier et al.
x1 x2 x3 . . . xT1 xT0
0
ϕ(σ(1))xσ(1)
ϕ(σ(1))
ϕ(σ(2))xσ(2)
ϕ(σ(2))
ϕ(σ(3)). . .xσ(T2)
ϕ(σ(T2))
ϕ(σ(T1))xσ(T1)
ϕ(σ(T1))
ϕ(σ(T))ˆxσ(1) ˆxσ(2) ˆxσ(3) . . . ˆxσ(T1) ˆxσ(T)
GPTˆx1 ˆx2 ˆx3. . . ˆxT1 ˆxT
InputToken content
Token pos. enc.
Target pos. enc.Output
Fig.1: In our σ-GPT, an arbitrary shuffling order σcan be chosen on-the-fly
for every sample. It induces an input order 0, σ(1), σ(2), . . .and an output or-
derσ(1), σ(2), σ(3), . . ., where the input is first padded with a 0to ensure a
consistent number of tokens. Tokens are shuffled accordingly, and these orders
are both encoded separately with two positional encodings concatenated to the
input, allowing the model to sample consistently in the autoregressive process.
The output is finally shuffled back to the true order.
σ-GPT GPTDiffusion
Models
Sample Anywhere   
Conditional Density Estimation   
Arbitrary conditioning   tildelow
Infilling   tildelow
Burst Sampling   
Log-likelihood Training   
Table 1: Comparison between our approach, a standard causal transformer en-
coder (called GPT here), and diffusion models. Our model allows the sampling
of a token at any position in the sequence, to model the remaining density ac-
cording to a partially sampled sequence, naturally supports infilling, and can be
used to sample the sequence by burst allowing faster generation. Compared to
diffusion models, it can be trained easily using cross-entropy.σ-GPTs: A New Approach to Autoregressive Models 3
This work is structured as follows, we first introduce σ-GPTs and shuffled
autoregression, and show that a model trained with this method combined with
a curriculum method can even increase the performance of the underlying model.
We then present the additional properties of σ-GPTs, summarized in Table 1,
in particular for estimating conditional probabilities and we present our token-
based rejection sampling scheme which allows for generating the sequence per
burst and its theoretical properties. We evaluate our model and our scheme
on three main tasks, which are open text generation, path-solving, and aircraft
vertical rate prediction.
Contributions
Introduce σ-GPT,anovelarchitecture,withtwopositionalencodingsrelated
respectively to the input and output order, that allows a causal transformer
to generate sequences in any order which can be modulated on the fly for
any pass through the model.
Demonstrate that our method can reach similar performance as left-to-right
trained autoregressive models when trained with a curriculum scheme.
Demonstrate that our method can be used to generate samples in any or-
der, allowing for the generation of samples conditioned on any part of the
sequence.
Introduce a novel token-based rejection sampling scheme that leads to the
generation of samples per burst.
2 Methodology
2.1 σ-GPTs: Shuffled Autoregression
We propose a novel approach for training autoregressive models, which involves
doing next-token prediction on a shuffled input sequence. We present σ-GPT,
where σdenotes the permutation used to shuffle the sequence, and by GPT we
mean any causal transformer encoder (or causal transformer decoder without
cross-attention) such as [13]. To train such a model, each sequence is shuffled
randomly during training. The model is then tasked to predict the next token
in the shuffled sequence conditioned on all the tokens it has seen before. This
training is done as usual with a standard cross-entropy loss. Besides the ran-
domization of the order of the sequence and the addition of a double positional
encoding, no other changes are needed to the model or training pipelines. For
the rest of the paper, we use left-to-right order to mention the usual order in
which models are trained, even in the case of 2D data which are usually mapped
to a sequence using a raster-scan order. And we use random order to mean that
the input has been shuffled.
2.2 Double Positional Encodings
To be able to model sequences in any order, each token needs to have informa-
tion about its position and the one of the next token in the shuffled sequence.4 A. Pannatier et al.
Specifically, when handling a sequence of tokens alongside a given permutation
σ, every token contains three distinct pieces of information: its value xσ(t), its
current position σ(t), and the position σ(t+ 1)of the subsequent token in the
shuffled sequence, that are all concatenated. The necessity for double positional
encoding arises from the intrinsic characteristics of transformers. Given that
each token attends to every previous token in a position-invariant manner, each
token needs to contain information about its position in the original sequence,
so other tokens can know where they are located. And each token needs to know
the position of the next token in the shuffled sequence as it is the target of
the prediction. The double positional encoding is the only architectural change
needed to train autoregressive models in random order. In this work, we used
the standard sinusoidal positional encoding [15] for both the input and output
positional encodings.
2.3 Conditional Probabilities and Infilling
0 20 40 60 80 100 120 140050100150200250
Infilling
Samples
True
CFL
0 20 40 60 80 100 120050100150200250
Conditional Density Estimation
Fig.2: Left. We can infill the sequence by conditioning on the known part (black
points). Right. We can also have estimates of the density at any point of the
sequence.
Our method allows making conditional density estimation of the rest of the
sequence. It is capable of making predictions all over the task space conditioned
on any known subpart of the task. This can be done by prompting the model
with the known part of the sequence and then decoding, in parallel and in one
pass, the remaining tokens. Such evaluations are not possible with autoregressive
models trained in a left-to-right order, as they need to follow the specific order
theyve been trained in. Examples, showing that the model usually has good
estimates of the unconditioned distribution, can be seen in Figures 2 and 3a.σ-GPTs: A New Approach to Autoregressive Models 5
(a) (Left.) The theoretical density of the
optimal path in the maze. (Right.) The es-
timated probability of the class path at
every position before starting autoregres-
sion. We see that the model has good esti-
mates of the true density.
(b) Two different conditional samplings for
each maze. The known part of the path
(purple) is prompted first, and the rest of
the sequence can be completed coherently.
Fig.3: Conditional density estimation and infilling on the maze path-solving
task.
Directly related to conditional density estimation, is that our method nat-
urally supports infilling, as it is straightforward to prompt the model with the
known part of a signal and to decode auto-regressively or by burst the rest of
the signal. Figures 2 and 3a shows example of such samplings.
2.4 Token-based Rejection Sampling
Autoregressive generation is a slow process as each token has to be gener-
ated sequentially. Even with caching strategies, this still scales linearly with the
sequence length and it becomes prohibitively expensive for long sequences [16].
As our model allows for the generation of tokens in any order, we can leverage
that fact and sample tokens in parallel at every position of the sequence. We can
then evaluate the candidate sequence under different orders and accept multiple
tokens in one pass. This algorithm runs efficiently on GPU as both the sam-
pling at every position and the evaluation under different orders can be made in
parallel, in a forward pass, and using an adapted KV-caching mechanism. We
describe this caching mechanism more in detail in Appendix C of the supple-
mentarymaterial.Whenconditionedonpartiallycompletedsequencesthemodel
outputs distributions that are compatible with different possible outcomes, and
when evaluating under different orders for generation, the distribution of tokens
is constrained to tokens that are compatible with the previous tokens seen in
one given order. As both the sampling and evaluation can be done in parallel,
we can compute the acceptance decision efficiently for every token.
This strategy outputs a decision for each remaining token, but the decisions
made by models become sometimes nonsensical when two mutually exclusive6 A. Pannatier et al.
Algorithm 1 Token-based rejection sampling, following notation of [4]
Given minimum target length T, y trained σ-GPT, and number of orders No
Given a prompt xiXof length t0of initial tokens. ( Xcan be the empty set)
Sett=t0
while t < Tdo
In parallel, compute distribution conditioned on prompt p(xi|X),it, . . . , T
In parallel, sample at every position xip(xi|X),it, . . . , T
Draw Norandom order σand in parallel, compute all logits q(xi|X,xσ<i),i
t, . . . , T
In parallel sample Tt rtU[0,1]from a uniform distribution
In parallel, compute the acceptance decision ai=ri<min(1 ,p(xi|X)
q(xi|X,xσ<i))for
every order
Select the order that accepts the most tokens before seeing a first rejection
Keep that order and add the aaccepted tokens before the first rejection to the
prompt
Sett=t+a
end while
tokens are part of the prompt. Once a rejection is seen, all subsequent accepted
tokens in the order of evaluation should be discarded. Indeed, the scheme rejects
tokens that are incoherent with the ones already seen, and asking a model to
make predictions based on incoherent tokens might lead to incoherent decisions.
Using multiple orders allows keeping the one that accepts the most tokens in its
evaluation. Even if it is dynamic, this algorithm can still easily generate multiple
samplesatonce,byacceptingthesameamountoftokensforeachsequenceinthe
batch. Our rejection sampling algorithm is given in pseudo-code in Algorithm 1.
Other models such as Mask Git [3] or diffusion models [7,1] are doing gen-
eration by burst. However, these models usually require fixing the number of
steps or a masking schedule beforehand. Our method on the other hand adapts
dynamically to the underlying statistics of the data and thus does not require
this extra hyper-parameter. We evaluate it on three synthetic cases to showcase
this dynamic capability, we present the results in Section 3.8.
2.5 Other orders
Ourdoublepositionalencoding scheme allows fortraining andevaluatingmodels
inanyorder.Usingarandomizedorderduringtrainingallowsconditionaldensity
estimation, infilling, and burst sampling at inference time. However the double
positional encoding scheme allows any order to be used, and it can be used to
train models in a deterministic order that is not left-to-right. As an example, we
use a deterministic fractal order to see how it compares to a random or left-to-
right order. This order starts in the middle of the sequence then recursively goes
to the first quarter and three-quarters of the sequence, and goes on recursively
until all the positions have been visited. Such an order is fully deterministic,
yet we make the hypothesis that this order leads to more difficult training forσ-GPTs: A New Approach to Autoregressive Models 7
the model as it cannot rely on the locality of the information. We present the
results in Section 3.5. Note that under perfect models, the order of modeling and
decoding should not matter because of the chain rules of probability. We give
more details about it in Appendix A.1 of the supplementary material.
2.6 Denoising diffusion models
Denoising diffusion models [7] is a family of generative models that can also
be used to generate sequences in a few steps. They are trained to reverse a
diffusion process that is applied to the data. Diffusion processes can be both
continuous and discrete. In this work, we use as a baseline only the discrete
diffusion case, in particular using a uniform diffusion process [1]. To be able to
compare the methods fairly, we use the same transformer architecture for both
σ-GPT and the diffusion model, changing only the training objective. Compared
toσ-GPT, diffusion models are not dynamic and require a fixed number of steps
to generate a sequence, independently of the underlying statistics of the data.
They also dont natively support conditional density estimation and infilling.
3 Results
3.1 General performance
We tested our model across three main distinct tasks: language modeling, maze
path solving, and aircraft vertical-rate prediction.
LanguageModeling:WeusedboththeGPT-2(123M)modelontheWikitext-
103 dataset [11] and GPT-2 (345M) on OpenWeb Text [5].
Maze Path Solving: This task involves determining a valid path between a
starting and ending point in 13 x 21 mazes featuring 15 barriers. Presented
with an image of an empty maze with start and end points, the model is
tasked with producing an image with a legitimate path.
AircraftVertical-RatePrediction:Thistaskusesrealaircrafttrajectorydata,
with its aircraft type. The data represents trajectories conditioned by air
traffic control directives. The models objective is to predict the vertical
trajectory from a planes current altitude to a specified control level.
Additionally to these tasks, we created a synthetic benchmark for evaluating our
burst sampling algorithm.
Product Dataset: This toy example represents a pure product law case and is
made of a sequence of length 100 with two classes (0,1) given by a Bernoulli
law with p = 10%.
Step Dataset: This toy example comprises sequences of two classes (0,1) of
length 100 which are 0 everywhere except on a step of length 10 placed
randomly in the sequence8 A. Pannatier et al.
Joint Law dataset: This toy example represents a pure joint law and consists
of a sequence of length 100 with 100 different classes, the model should
predict a random generation of these different classes.
The general results of our models are presented in Table 2. These results
indicate that training in a random order while requiring more compute-time as
we describe in Section 3.2, reaches similar performances to left-to-right trained
models. For the text modeling, to have a fair comparison during training, we
monitor the validation perplexity of the sequence evaluated in a left-to-right
order. Training in random order for text modeling was plateauing at a higher
left-to-right validation perplexity, but using a curriculum scheme allows reaching
the same performances, as presented in Section 3.3. For the path solving and
the vertical rate prediction, the models were able to reach the same left-to-
right validation loss during training. In inference, we noticed a one percent drop
in accuracy compared to diffusion models and left-to-right trained GPT. For
the vertical rate prediction task, the dataset that we used is limited to around
23.000 different sequences, we noticed that the standard left-to-right GPT was
sometimes stuck repeating the same altitude, we think this is a modeling issue
due to the small data regime. σ-GPT does not seem to suffer as much from this
problem and offers a decrease in MSE. We hypothesize that this behavior comes
from using a random order in inference which forces the model to fix some tokens
over the whole sequence early in the generation. By doing so, the model gains
the advantage of having a sketch of the whole sample and then concentrates on
completing a coherent sample.
Table 2: General results. We report the validation perplexity for text generation,
the test accuracy for the maze solver, and the mean squared error (MSE) for the
vertical rate prediction. σ-GPT reaches a similar performance as GPT in text
generation and maze solving and it outperforms GPT in the case of the vertical
rate prediction. We report the validation perplexity for the text generation. For
the path solver, we report the test accuracy on 1000 novel mazes. For the vertical
prediction task, we report the mean squared error on the test set. We report the
meanandstandarddeviationforthepath-solvingandtheverticalrateprediction
task.Wedonotreportthevalidationerrorforthetextgenerationforthediscrete
diffusion (Dis. Diff) as the training objective is different.
Text-generation Path Solving Vertical Rate
OWT Val Perp. ( ) Wiki-103 Val Perp. ( ) Accuracy ( ) MSE ( )
GPT 18.14 20.30 99.60 0.70 274.8 70.7
σ-GPT 18.64 16.69 98.30 0.67 141.4 4.1
Dis. Diff. - - 99.20 0.67 105.94 1.3σ-GPTs: A New Approach to Autoregressive Models 9
Table 3: Training efficiency. Number of steps/epochs required to reach the same
performance and comparison with as GPT trained causally. As learning to pre-
dict in any order is a more challenging task, it is expected to need more com-
puting time to reach the same accuracy. We dont report the standard deviation
for text generation as we limited the training to one run.
Order Text-generation Maze Solver Climbing Rate
σ-GPT 32500 78.0 6.5 110.7 4.5
GPT 16500 19.3 4.9 25.0 3.6
3.2 Training Efficiency
Modeling sequences in a random order is a more challenging task than modeling
in left-to-right order. We think this is due to two main factors, at the beginning
of the sequences models cannot rely on adjacent tokens to make educated guesses
for the next token. Second some tasks are harder to learn in one direction than
another and by modeling the data in any direction, we are always in the harder
scenario. We give an example of one task that is harder to learn in one direction
in Appendix A of the supplementary material.
This implies that we expect and see an increase in the number of steps or
epochs required to learn a task. As previously mentioned, we dont see experi-
mentally a drop in the validation performance of our model in the case of the
path-finding algorithm or the vertical rate forecasting, but the time to reach the
same performance increased. In the case of text modeling, the models plateaued
before reaching the same accuracy when trained in random order. We treat that
case in the following section. We report in table Table 3, the increase in training
steps or epoch to reach the same accuracy. We see that most of the time, the
number of epochs or steps needed to reach the same performance drastically in-
creases. We think again that this is due to the increased complexity of modeling
the sequence without having to rely on local information.
3.3 Curriculum Learning
For text modeling, we found a gap in validation perplexity in the left-to-right
order between models trained purely in a random order and models trained in a
left-to-right order. We see in Table 4 that σ-GPT is stuck at larger perplexity in
both Open Web Text and WikiText-103 (30.43 vs 18.14 and 39.85 vs 20.30). We
foundthattrainingforlongerandusinglargermodeldidnthelpinreducingthat
gap. To solve that problem, we introduced a curriculum learning scheme where
the model is shown first more sequences in left-to-right order and progressively
learns to model the sequence randomly. Surprisingly using this scheme helped
drastically the model which managed to get even better performance than left-
to-right trained transformers in the Wikitext-103 case and reduce drastically the
gap for models trained on OpenWebText.10 A. Pannatier et al.
Table 4: Curriculum learning. We monitor the Validation Perplexity using a left-
to-right order during training to have a comparable evaluation. We see that there
is a gap between the model trained purely in a left-to-right fashion (GPT) and
others trained in a random order ( σ-GPT). Training for longer and larger models
didnt help in removing that gap. We introduce a curriculum learning scheme
thatstartspresentingthemodelwithsomepercentageofthedata(writteninthe
corresponding label) in a left-to-right order at the beginning of the training and
goes linearly to 100% of sequence in a random order at the end of the training.
We see that training with this scheme removes the gap between σ-GPT and
regular GPT and it reaches even better than left-to-right performance in the
WikiText-103 case.
Text-generation Val Perp. ( )
Min. Left-to-Right
Openweb Text - GPT (345 M)
GPT 18.14
σ-GPT curr. 50% 18.64
σ-GPT no curr. 30.43
WikiText 103 - GPT (128M)
σ-GPT curr. 50% 16.69
σ-GPT curr. 100% 19.38
σ-GPT curr. 10% 19.45
GPT 20.30
σ-GPT no curr. 39.85
3.4 Open Text Generation: t-SNE of generated sequence
To get a qualitative sense of the generated text by the different methods, We
generate 3000 sequences of 1024 tokens with each method, embed each sequence
using an embedding model, and then project the embeddings to 2D using t-SNE.
WepresenttheresultsinFigure4.WeusedOpen-AI text-embedding-3-small [12]
to embed the generated sequences into a single 1536 vector embedding. We rep-
resent as green embeddings of sequences of the validation set, used as reference.
We compute the t-SNE using the whole 15000 embeddings and then plot each
method (blue) and the other considered method (small gray dots). We first see
that embeddings of GPT, σ-GPT, σ-GPT with burst sampling, and diffusion
are spread over the whole space, showing that the model can generate sequences
that are coherent with the validation set.
3.5 Fractal order
We describe here the results that we get when training a GPT using a determin-
istic, but not left-to-right order. We described the order in Section 2.5. We train
a GPT using this specific order for the different tasks and present the resultsσ-GPTs: A New Approach to Autoregressive Models 11
100
 50
 0 50 100100
50
050100-GPT (r.s.)
(a) Rej. Sampling
100
 50
 0 50 100100
50
050100-GPT (ar, random)
 (b)σ-GPT
100
 50
 0 50 100100
50
050100  GPT (ar, causal) (c) GPT
100
 50
 0 50 100100
50
050100Diffusion (d) Diffusion
Fig.4: 2D t-SNE of text-small-3-embeddings of 3000 sequences generated
by each method. We compute the t-SNE of all the embeddings together, and
then we display in each graph the embeddings of the validation set (green),
the embeddings of the corresponding method (blue), and the embeddings of the
other methods (gray). We see that the embeddings of the generated sequences
have the same overall distribution compared to validation sets, which seems to
indicate that GPT,σ-GPT, σ-GPT with burst sampling, and diffusion models
can generate sequences of similar quality.
in Table 5. We found that training in that order was as difficult for the model
as training in a random order, and we noticed a small drop in performance com-
pared to σ-GPT. We suspect that this is due to the high discontinuity of the
order of the sequence, which is such that two consecutive tokens are seen far
away in the sequences. When predicting the first tokens, the model therefore
cannot rely on information contained in neighboring tokens to make its predic-
tion. As the training behavior seen in models trained in random and fractal order
is similar, we think that the drop in training efficiency comes more from the fact
that the model cannot exploit this neighboring information than changing the
order at every batch.
Additionally, models trained in a fractal cannot be used as such for infilling
and conditional density estimation and therefore cannot be used with our rejec-
tion sampling scheme. As the order is fixed for every batch, it might not even
need to have a double positional encoding.
3.6 Memorizing
As learning sequences in any direction is harder than modeling them under a
predefined order, we also expect that the critical dataset size when the model
switches from memorization to generalization will increase. We follow the same
hypotheses than [14], namely that the model has two mechanisms, one general-
izing and one memorizing the data. As the mechanism of generalizing is more
efficient as the dataset grows it will be selected by gradient descent once the size
of the dataset gets beyond a critical size. As learning in a random order is a
more difficult task, we expect that generalization is more difficult in that setup
as well, hence the memorization regime should hold for bigger dataset sizes.
We reduce drastically the training dataset size in the case of the path-finding12 A. Pannatier et al.
Text-generation Path Solver Vertical Rate
Val Perp. ( ), Rand. ord. Test Acc ( ) MSE ( )
σ-GPT 24.46 98.30 0.67 141.4 4.1
Fractal GPT 27.79 98.00 1.94 145.7 2.6
Table 5: Results for GPT trained in a fractal order compared to a standard
left-to-right (GPT) and random ( σ-GPT) order. We found that training models
in this highly non-continuous order is as hard as training them in a random
order, and additionally, models trained in that order cannot be used for condi-
tional density estimation, infilling, or rejection sampling. For text modeling, we
report the model perplexity on the validation set, in a random order for σ-GPT
and in fractal order for the fractal GPT, as left-to-right validation perplexity is
meaningless for fractal GPT which did not see sequences in that order during
training.
1000 10000 100000
N. Training samplesCausal Fractal Randomordermem 
 99.2/3.8gen 
 100.0/91.5gen 
 100.0/99.1
mem 
 98.7/0.3mem 
 100.0/53.8gen 
 99.6/94.0
mem 
 97.3/0.1mem 
 100.0/38.6gen 
 97.2/95.8Maze Memoïzation vs. Generalization
20406080
Fig.5: Number of examples needed to switch from memorization to generaliza-
tion. The model is trained on a restricted dataset size in the path-finding task.
We see that the model trained in a random order needs more examples to switch
from memorization to generalization. At 1k samples both models are fully in a
memorization regime, at 100k both generalize but in between, at 10k, the model
trained in a random order is still in a memorization regime.σ-GPTs: A New Approach to Autoregressive Models 13
task and we present the results in Figure 5. Once it gets to 1000 examples both
models trained in left-to-right, fractal, and random order are in a memorizing
regime, getting perfect accuracy on the training data but very low on the valida-
tion data. Conversely, once the dataset gets bigger than 100k examples models
trained in all the different orders are in a generalization regime. The transition
happens in between and we find that it happens faster in the left-to-right order:
at around 10k samples, the models trained left-to-right can generalize, while
models trained in a random order are still in a memorization regime. We see
also that models trained in a fractal order start generalizing faster than models
trained in a random order, suggesting that the model can rely on seeing always
the same order to generalize more rapidly.
3.7 Infilling and Conditional density estimation
We show in Figure 2 that our model can be used to infill the sequence by con-
ditioning on the known part of the sequence. In this figure, the larger points are
part of the prompt and one can see the generated sequence complete sequence
that matches the prompt. This figure also shows that our model has good esti-
mates of the density at any point of the sequence. We represent at each point
in the sequence the probability of the next sample given the known part of the
sequence as shades of gray, the darker the more probable. We see that during
generation, the model sees multiple possible outcomes that are coherent with
the known part of the sequence. They are then constrained to a single sequence
during the sampling. For left-to-right trained models, we can only have estimates
of the density for the next tokens and we cant know what the model estimates
for the rest of the sequence. In the case of the path-finding task, we show in Fig-
ure 3a that the model conditioned only on an empty maze has good estimates of
the true density of the optimal paths, highlighting that the model has already
partially solved the problem before starting generation. During sampling, the
order of the generation and the sampling procedure influence which path is se-
lected from this joint law. We show as well in Section 2.3, that we can constrain
the generation of mazes and that the model can generate coherent samples based
on some prompted tokens that can be chosen on the fly.
We also give some interactive examples of text generation in the supplemen-
tary material.
3.8 Token-based Rejection Sampling Scheme
We applied our token-based rejection sampling scheme to the problem of text
generation, path-finding, and vertical rate forecasting Figures 6a to 6c. We found
that in the three cases, our method was able to generate samples of comparable
quality with an order of magnitude fewer steps than the autoregressive method.
Wecomparedtodiscretediffusionmodelsaswell,andwecanseethatourmethod
always outputs coherent samples, while the samples generated by the diffusion
model are sometimes incoherent if the number of steps is not high enough. In the14 A. Pannatier et al.
case of path-solving and in the three synthetic tasks, we see that at a comparable
number of steps for both methods, our model shows better generation quality.
We tested our token-based rejection sampling scheme on three synthetic
cases. We estimate the number of steps required to generate the sequence using
a perfect model in Appendix B of the supplementary material. We found that
our scheme was close to the optimal heuristics in all three cases. In the case of
the product dataset, requiring only one step to accept the sequence. In the case
of the step dataset, our scheme required four steps to accept the sequence. In
the case of the joint law dataset, our scheme required a few more steps, which
is expected as the task is more complicated. We see that it manages to generate
valid samples with a number of steps close to the optimal heuristics and with a
large increase in performance compared to diffusion models at the same step.
1 10 100 500
Steps020406080100Error Rate
25500 250167125100502010522
11
(a) Path-Solving
100101102
steps050100150200250300mse1251020
500
250100 50 25 10 51
2
4 (b) Vertical-Rate Pred.
100101102103
Steps103Perplexity
 5
5042
5002
20
2501661
10
1001234 (c) Text Modeling
1 10 100 500
Steps020406080100Error rate
1251020501251020501122 5 10 20 50 100125167250 500Optimal
autoregressive
burst_sample
Diffusion
(d) Product Task
1 10 100 500
Steps020406080100Error Rate
125102050
500 25016712510050201052211 (e) Step Task
1 10 100 500
Steps020406080100Error rate
1251020505002501671251005020 10 5 2211 (f) Permutation Task
Fig.6: We plot the performance vs steps of our σ-GPT used for autoregression
in random order (blue), to σ-GPT with rejection sampling per burst (orange)
against diffusion models (gray). We denote in the text the predefined number of
steps chosen for generation in the diffusion models. For rejection sampling, we
note the number of orders used for the evaluation. We see that increasing the
numberofordersleadstoadecreasednumberofsteps.Forthesynthetictasks,we
also represent heuristics for the optimal number of steps needed to generate the
sequence (gray line), as described in Appendix B of the supplementary material,
and we see that our scheme is close to this heuristics.σ-GPTs: A New Approach to Autoregressive Models 15
4 Related works
4.1 Shuffling in language models
In exploring the landscape of shuffling used by autoregressive models, one finds
noteworthy precedence in the work of [18]. XLNet, which uses randomization of
the sequence order as a pretraining task for sentence encoding in the context
of natural language understanding. Our approach is similar with regards to the
shuffling of the sequence, and the shuffled language modeling but differs in its
implementation: we use double positional encoding and a regular causal mask
instead of relying on two streams and the modification of the attention matrix.
The objective of the two models differs as well, XLNET is used to encode in-
formation similarly to BERT while our approach is generative. A very recent
approach by [6], trains a transformer both on a left-to-right and right-to-left
order which solves a classic problem of the transformer model to understand
the context of the token in both directions. Our approach by default trains the
model in any order, and we can consider that it sees an exponential number of
different sequences during training. However, our section on memoization shows
that this does not serve as a good data augmentation technique and that the
model seems to memorize the data more in that case.
4.2 Burst sampling scheme
Other works are trying to solve the problem of the linear time required by
autoregression using burst sampling. Maskgit [3], for example, uses a BERT-
like masked language model (MLM) and uses a custom decoding scheme, which
samples multiple tokens at the same time to generate an output. The number of
tokens generated at each pass is fixed by the masking schedule and the model
utilizes a confidence-driven decoding scheme to choose which tokens to predict.
In a distinct approach, [10] introduced a method that leverages an auxiliary
model to guide the generation process. Alternatively, the approach outlined by
[9] primarily focuses on generating preliminary drafts of an image, which are
then progressively enhanced in subsequent iterations. Following work in video
generation [2,16], are leveraging a MaskGit [3] like approach for generation as
autoregressive generation of video would be too costly. Our rejection sampling
schemeallowsaswelltogeneratethesequencebyburstbutincontrasttoanother
scheme,thenumberoftokensacceptedisdynamicanddependsonthedatabeing
modelled, which allows faster generation when the underlying data distribution
is simple.
5 Conclusion
Training GPT-like models in different orders offer different desirable properties.
It allows for the conditional prediction based on any subset of the tokens of the
sequence, it naturally can be used for infilling, and as the model can do partial16 A. Pannatier et al.
prediction, we can leverage them to do rejection sampling and accept multiple
tokensatthesametimeduringgeneration.Ourfindingsindicatethatconditional
prediction learned by the models matches the theoretical partial distribution
showing that the model is indeed able to understand and reconstruct the signal
in any order. As the training objective of modeling sequences in any order is
harder than training in a fixed order, it has an impact on the training efficiency,
and in a small dataset size, we show that it leads to more memoïzation. Finally,
we showed that our model was able to generate sequences by burst using a
novel per-token rejection sampling scheme, reaching optimal heuristics in some
cases and decreasing the number of steps needed for generation by an order of
magnitude.
Acknowledgement We thank Youssef Saied for his help and good remarks on
the overall project. We thank Romain Fournier for his precious help on some
theoretical aspects of the analysis. Arnaud Pannatier was supported by SkySoft
ATM for the project MALAT: Machine Learning for Air Traffic and the Swiss
Innovation Agency Innosuisse under grant number 32432.1 IP-ICT.  Evann
Courdier was supported by the Swiss Center for Drones and Robotics - SCDR
of the Swiss Department of Defence, Civil Protection and Sport via armasuisse
S+T under project No 050-38.
References
1. Austin, J., Johnson, D.D., Ho, J., Tarlow, D., Van Den Berg, R.: Structured de-
noising diffusion models in discrete state-spaces. Advances in Neural Information
Processing Systems 34, 1798117993 (2021)
2. Chang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J., Jiang, L., Yang,
M.H., Murphy, K., Freeman, W.T., Rubinstein, M., Li, Y., Krishnan, D.: Muse:
Text-to-image generation via masked generative transformers (2023)
3. Chang, H., Zhang, H., Jiang, L., Liu, C., Freeman, W.T.: Maskgit: Masked genera-
tiveimagetransformer.In:ProceedingsoftheIEEE/CVFConferenceonComputer
Vision and Pattern Recognition (CVPR). pp. 1131511325 (June 2022)
4. Chen, C., Borgeaud, S., Irving, G., Lespiau, J.B., Sifre, L., Jumper, J.: Acceler-
ating large language model decoding with speculative sampling. arXiv preprint
arXiv:2302.01318 (2023)
5. Gokaslan, A., Cohen, V.: Openwebtext corpus. http://Skylion007.github.io/
OpenWebTextCorpus (2019)
6. Golovneva, O., Allen-Zhu, Z., Weston, J., Sukhbaatar, S.: Reverse training to nurse
the reversal curse (2024)
7. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in
neural information processing systems 33, 68406851 (2020)
8. Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language models
are zero-shot reasoners. In: Oh, A.H., Agarwal, A., Belgrave, D., Cho, K. (eds.)
Advances in Neural Information Processing Systems (2022)
9. Lee, D., Kim, C., Kim, S., Cho, M., HAN, W.S.: Draft-and-revise: Effective image
generation with contextual rq-transformer. In: Koyejo, S., Mohamed, S., Agarwal,
A.,Belgrave,D.,Cho,K.,Oh,A.(eds.)AdvancesinNeuralInformationProcessing
Systems. vol. 35, pp. 3012730138. Curran Associates, Inc. (2022)σ-GPTs: A New Approach to Autoregressive Models 17
10. Lezama, J., Chang, H., Jiang, L., Essa, I.: Improved masked image generation with
token-critic. In: Avidan, S., Brostow, G., Cissé, M., Farinella, G.M., Hassner, T.
(eds.) Computer Vision  ECCV 2022. pp. 7086. Springer Nature Switzerland,
Cham (2022)
11. Merity, S., Xiong, C., Bradbury, J., Socher, R.: Pointer sentinel mixture models.
In: International Conference on Learning Representations (2017)
12. OpenAI: New embedding models and api updates. https://openai.com/blog/
new-embedding-models-and-api-updates (2024), accessed: [01.03.2024]
13. Radford,A.,Wu,J.,Child,R.,Luan,D.,Amodei,D.,Sutskever,I.,etal.:Language
models are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019)
14. Varma, V., Shah, R., Kenton, Z., Kramár, J., Kumar, R.: Explaining grokking
through circuit efficiency. arXiv preprint arXiv:2309.02390 (2023)
15. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
Kaiser, L.u., Polosukhin, I.: Attention is all you need. In: Guyon, I., Luxburg,
U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.)
Advances in Neural Information Processing Systems. vol. 30. Curran Associates,
Inc. (2017), https://proceedings.neurips.cc/paper_files/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
16. Villegas, R., Babaeizadeh, M., Kindermans, P.J., Moraldo, H., Zhang, H., Saffar,
M.T., Castro, S., Kunze, J., Erhan, D.: Phenaki: Variable length video generation
from open domain textual descriptions. In: International Conference on Learning
Representations (2023)
17. Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E.H.,
Le, Q.V., Zhou, D.: Chain of thought prompting elicits reasoning in large language
models. In: Oh, A.H., Agarwal, A., Belgrave, D., Cho, K. (eds.) Advances in Neural
Information Processing Systems (2022)
18. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R., Le, Q.V.: Xlnet:
Generalized autoregressive pretraining for language understanding. In: Wallach,
H., Larochelle, H., Beygelzimer, A., d 'Alché-Buc, F., Fox, E., Garnett, R. (eds.)
Advances in Neural Information Processing Systems. vol. 32. Curran Associates,
Inc. (2019)
A Shuffled sequences are harder to learn
Using a random order in the training of a transformer often leads to a harder
task to learn. To showcase this property, we designed the following task. The
model is asked to model a lazy random walk that starts from a multinomial
distribution made of four Dirac distributions (at 100, 120, 130, 140), with equal
probabilities. The random walk evolves with the following law:
p(Xt|Xt1) =

2
5if|XtXt1|= 1
1
5ifXt=Xt1
0otherwise(1)
With p(X0= 100) = p(X0= 120) = p(X0= 130) = p(X0= 140) = 1 /4.
In this setup predicting future tokens conditioned on past tokens is easy and
is given by:
Xt3 N(Xt2,(t3t)
0.8) (2)18 A. Pannatier et al.
with t1< t 2< t 3
note:E(X2) = 20.41 + 0.20 = 0 .8
The exact probability mass function (pmf) can also be computed:
p(Xt3=x|Xt2) =nX
o=0n
ono
u
pu
upd
dpo
o (3)
with n=t3t2,u=no+xXt2,d=nou.
Predicting tokens conditioned on future information is much harder in that
setup as the model has to take into account the initial multinomial distribution
and keep track of all the different possibilities.
In such cases the exact pmf is given by :
p(Xt1=x|Xt2) =(P
iSP(i, x, t 1))P(x, X t2, t2t1)P
iSP(i, Xt2, t2)(4)
Where P(a, b, n ) = #paths abinnsteps =Pn
o=0n
ono
u
. with u=n
o+ba.
0 50 100 150 200 2500.000.020.040.060.080.10(X10|X100=115)
data dist
theory
model
0 100 200 300 400050100150200250Model([X100],[100,x])
Fig.7: Random walks at different altitudes results. ( Left) Comparison with the
theoretical density at position 10, is hard to model as the model needs to take
into account the number of possible paths from each starting point. ( Middle).
Empirical density ( Right). Learned density. The model is capable of modeling
the density in two directions when conditioned on one position even if one order
is more complicated than the other.
When trained with a left-to-right order, the model quickly learns the lazy
random walk distribution but is never asked to learn more than that. However,
when trained with a random order, the model has to learn to compute the more
complex multinomial distribution, which is a much harder task as the model has
to learn the global statistics of the sequence.σ-GPTs: A New Approach to Autoregressive Models 19
A.1 Learning a harder problem: Chain rule of probability
Shuffling the order of the tokens in a sequence has the effect of creating a harder
problem for the model. But if the model has access to enough data and to learn
the distribution perfectly then shuffling should not change the results. If the
model learned the final distribution perfectly, the decoding order should not
matter in theory as stated by the chain rule, indeed
p(X1=x1, X2=x2, . . . , X T=xT) =
p(X1=x1)p(X2=x2|X1=x1). . .
p(XT=xt|X1=x1, . . . X T1=xt1)
=p(Xσ1=xσ1)p(Xσ2=xσ2|Xσ1=xσ1). . .
p(XσT=xσt|Xσ1=xσ1, . . . X σT1=xσt1)(5)
For any permutation σ=1 2T
σ1σ2σT
.
This is not what we see in practice, as models are not perfect and datasets
are finite, so the impact of the order has its importance.
Another demonstration of this effect is the chain of thought step-by-step
prompting strategy [17,8] of large language models. Where adding a few to-
kens between the question and the answer increases the working memory of the
transformer, which seems to help the models make fewer mistakes.
B Estimation of number of steps for burst sampling
B.1 Deterministic case and lucky samplings
In this section, we describe the fully deterministic case and the lucky samplings.
In the deterministic case, and assuming a perfect model, the model will accept
the sequence in one step. Indeed, if the model already knows the sequence, the
probability distribution wont change when it is seen under a different order,
therefore the rejection sampling ratio will always be one and the model will
accept the sequence in one step.
In is the same for lucky samplings, again assuming a perfect model and a
valid sample, the probability of a token given a specific order can only be higher
than the probability of the token conditioned only on the prompt, as the tokens
seen can only remove valid possibilities. Hence q(x)> p(x)and the model will
accept the sequence in one step.
B.2 Product Dataset: Number of passes needed to generate a valid
sequence
In this section, we give the number of steps required to generate a valid sequence
in the case of the product dataset using our rejection sampling scheme under20 A. Pannatier et al.
a perfect model. The product dataset is made of a sequence of length 100 with
two classes (0,1) given by a Bernoulli law with p = 10%. At the beginning of the
generation, a perfect model will predict p(xt= 1|{}) = 0 .1,t {1, . . . , 100}.
As all tokens are independent, the conditioning of the model will not change the
probability of the next token. Therefore, the model will accept all tokens in the
first step of the rejection sampling scheme. Our scheme should generate a valid
sample for this distribution in at most 1 step.
B.3 Step Dataset: Number of passes needed to generate a valid
sequence
In this section, we give the number of steps required to generate a valid sequence
in the case of the step dataset using our rejection sampling scheme under a
perfect model. The step dataset is made of a sequence of length 100 of classes
{0,1}The sequence is 0 everywhere except on a step of ones of length 10 placed
randomly in the sequence. At the beginning of the generation, a perfect model
cannot do better than predicting p(xt= 1|{}) = 0.1,t {1, . . . , 100}.
Sampling with this law will lead to a sequence with 10 ones on average. The
number of accepted tokens in the first round of rejection sampling will then
depend on the random order sampled. The first token given by the order will
always be accepted as q(xt|{}) =p(xt|{}). Now the next tokens are either com-
patible with the tokens already accepted or not. If the next token is compatible
with the already accepted tokens, then it will be accepted with probability 1
asp(xt|{}) =q(xt|xaccepted ). If it is not compatible, then it will be rejected as
q(xt|{xaccepted }) = 0and the rejection sampling stops rejecting all the remaining
tokens.
Now the reason for non-compatibility in the step dataset is either that
a. the model has already seen a 1, and sees a second 1 that is too far away,
b. that the model sees a one and then a zero nearby which would mean the
end of the sequence which is not long enough, or
c. that it has seen only 0s, and no places are remaining for 1s. Therefore it
has to reject the zero that would lead to an empty sequence. 1. In case, a-b.
the model accepts a one, in the first step, and in case c. the start of the step is
confined between the two last zeros where there are still enough places. 2. Once
a token from the step is accepted, or there is only one place for a step, the signal
is deterministic and a perfect model will 3. sample a complete step, Our scheme
should generate a valid sample for this distribution in at most 3 steps.
B.4 Permutation
When wanting to generate a valid permutation per burst assuming a perfect
model, when given a partially completed permutation, a perfect model cannot
do better than predicting a uniform law on the remaining tokens.σ-GPTs: A New Approach to Autoregressive Models 21
There is a well-known formula for the number of different classes that one
gets when sampling with a uniform law, it is given by:
E[# of incoherent tokens ] (6)
=TE[# classes being sampled ] (7)
= (T1)T/TT1(8)
More generally, the probability of sampling exactly k classes is given by
p(sampling exactly kclasses ) =T
k
k!T
k
/TT(9)
(10)
WhereT
k
is the Stirling number of the second kind.
Having a closed formula for the expected number of steps is hard, but we
can estimate it numerically using Equation (9). For a sequence of length 100,
we find that the expected number of steps is 5.2 with a standard deviation of
0.6. Which gives us a lower bound for the number of steps required to generate
a valid permutation.
Note that in this specific case, for our rejection sampling scheme to be able to
generate a valid permutation in this number of steps, the model needs to validate
the partial sequence under an order which is such that each unique classes are
seen first because once a class is repeated the model will have to reject the rest
of the sequence.
C Caching Scheme for Burst rejection sampling
Autoregressive transformers usually rely on a KV-caching mechanism to go from
aO(N3)toaO(N2)costofgeneration.thisKVcachecanbeadaptedtogenerate
a sample by burst. If we already have accepted a set of tokens T1and we have
a KV cache K1, V1that stores the keys and values for this set of tokens, and we
want to generate a prediction for all the remaining set of T2tokens in parallel.
The output of the transformer corresponding to the remaining set of tokens
is
V
2= softmax
QT2KT
1DV1
V2
(11)
With D= diag( qivi)This formula parallelizes easily. We give the corre-
sponding code in pytorch.
1def burst (self , kv_cache , q, k, v):
2 k1 , v1 = kv_cache .get ()
3
4 att1 = q @ k1. transpose (-2, -1)22 A. Pannatier et al.
5 # no masking on KV cache , tokens can see them all
6
7 att2 = (q * k).sum (-1, keepdim = True )
8 att = torch .cat (( att1 , att2 ), dim = -1) * (1.0 / math
. sqrt (k. size ( -1)))
9 att = F. softmax (att , dim = -1)
10 att = self . attn_drop (att)
11
12 att1 = att [... , : -1]
13 att2 = att [... , [ -1]]
14
15 # Works even if the cache is empty
16 y = att1 @ v1 + att2 * v
17 y = rearrange (y, b h t e -> b t (h e))
18
19 y = self . resid_drop ( self . proj (y))
20
21 return y
Listing 1.1: KV Caching scheme of burst sampling
D Additional experiments for the vertical rate forecasting
We present here additional results concerning the vertical rate modeling experi-
ment Table 6. We report the Mean Square Error (MSE) compared to the ground
truth when prompted by partially completed trajectories. The idea is the see
the effect of partial left-to-right conditioning on the quality of the generated
sequences. 0%, 10%, and 50% denote the percentage of the actual flight that is
given as a prompt to the model. In this setup, we see that σ-GPT outperforms
models trained causally for a generation. As the dataset size is small, we noticed
that models trained in a left-to-right manner were suffering from a repetition
problem that arises when the modeling capabilities of the models are insuffi-
cient. As the prompting is given to the model in left-to-right order, we see that
causal models can outperform random models when prompted with half of the
actual sequence.
Weseeaswellthatdiffusionmodelsusuallyoutperformautoregressivemodels
in this task. However, they need to be retrained to be able to generate sequences
conditioned on a partial sequence. We see that the autoregressive models can be
usedinamoreflexiblewayastheycanbeusedtogeneratesequencesconditioned
on a partial sequence without retraining.σ-GPTs: A New Approach to Autoregressive Models 23
Table 6: This table presents the Mean Square Error (MSE) results for the climb-
ingrateforecastingtask.TheMSEiscalculatedontheentiregeneratedsequence,
conditioned on different points during the climb: the start (0%), early stage (10%
of the climb), midway (50% of the climb) and in the middle of the first climb.
For each validation sequence, we generated 20 sequences autoregressively using
the model, following three different schemes: causal scheme, random scheme, and
binary search tree order. We also report the performance of diffusion models, for
comparison, but we only report their performance for the entire sequence as they
need to be retrained to be able to generate sequences conditioned on a partial
sequence.
Size Method 0% 10% 50% Mid Climb
SmallFractal 145.72.6 1278.9 161.9 1466.6 172.2 623.6 84.4
Causal 274.870.7 179.9 48.052.912.746.110.0
Random 141.44.1 123.8 6.0 63.8 1.2 40.1 2.7
Diffusion 105.94 1.3 - - -
BaseFractal 158.15.9 2523.5 13.9 1623.5 283.5 884.4 15.7
Causal 424.710.9 287.7 0.5 70.0 5.7 61.5 6.8
Random 146.36.4 119.3 7.8 65.3 5.1 41.6 0.7
Diffusion 107.96 2.4 - - -
TinyFractal 152.38.7 2553.6 168.3 1508.8 220.3 972.8 44.9
Causal 290.913.0 224.0 1.8 58.9 1.6 53.9 5.7
Random 129.523.7108.1 6.1 72.64.736.70.8
Diffusion 123.28 6.9 - - -
  